{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Image Segmentation Using Text and Image Prompts}\n\n\\begin{document}\n\n\\title{\nImage Segmentation Using Text and Image Prompts\n}\n\n\\date{}\n\n\\author{\nTimo Lüddecke$^{1,\\S}$ and Alexander Ecker$^{1,2}$ \\\\\n\\small $^1$Institute of Computer Science and CIDAS, University of Göttingen \\hspace{1cm}\n\\small$^2$MPI for Dynamics and Self-Organization, Göttingen\n}\n\n\\maketitle\n\n\\definecolor{blue2}{rgb}{0.2,0.6,0.8}\n\\definecolor{cyan2}{rgb}{0.2,0.7,0.7}\n\\definecolor{yellow2}{rgb}{0.7,0.5,0.0}\n\\definecolor{blue3}{rgb}{0.3,0.5,0.7}\n\\definecolor{red2}{rgb}{0.6,0.1,0.2}\n\n\\def\\todo#1{{ \\color{blue}{TODO: #1} } }\n\\def\\new{{ \\color{red}{new} } }\n\\def\\remark#1{{ \\color{cyan}{#1} } }\n\\def\\question#1{{ \\color{orange}{Q: #1} } }\n\n\\def\\conf#1{}\n\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\xv}{\\mathbf{x}}\n\\newcommand{\\yv}{\\mathbf{y}}\n\\newcommand{\\tv}{\\mathbf{t}}\n\\newcommand{\\cv}{\\mathbf{c}}\n\\newcommand{\\pred}{\\mathbf{p}}\n\\newcommand{\\gt}{\\mathbf{g}}\n\\newcommand{\\miou}{\\text{mIoU}}\n\\newcommand{\\ioufg}{\\text{IoU}_{\\text{FG}}}\n\\newcommand{\\ioubin}{\\text{IoU}_{\\text{BIN}}}\n\n\\begin{abstract}\n\nImage segmentation is usually addressed by training a model for a fixed set of object classes. Incorporating additional classes or more complex queries later is expensive as it requires re-training the model on a dataset that encompasses these expressions.\nHere we propose a system that can generate image segmentations based on arbitrary prompts at test time.\nA prompt can be either a text or an image. This approach enables us to create a unified model (trained once) for three common segmentation tasks, which come with distinct challenges: referring expression segmentation, zero-shot segmentation and one-shot segmentation.\nWe build upon the CLIP model as a backbone which we extend with a transformer-based decoder that enables dense prediction. After training on an extended version of the PhraseCut dataset, our system generates a binary segmentation map for an image based on a free-text prompt or on an additional image expressing the query. \nWe analyze different variants of the latter image-based prompts in detail.\nThis novel hybrid input allows for dynamic adaptation not only to the three segmentation tasks mentioned above, but to any binary segmentation task where a text or image query can be formulated.\nFinally, we find our system to adapt well to generalized queries involving affordances or properties.\nCode is available at \\url{https://eckerlab.org/code/clipseg}.\n\n\\end{abstract}\n\n\\section{Introduction}\n\n\\let\\thefootnote\\relax\\footnotetext{\\small$^\\S$timo.lueddecke@uni-goettingen.de}\n\nThe ability to generalize to unseen data is a fundamental problem relevant for a broad range of applications in artificial intelligence. For instance, it is crucial that a household robot understands the prompt of its user, which might involve an unseen object type or an uncommon expression for an object. While humans excel at this task, this form of inference is challenging for computer vision systems. \n\nImage segmentation requires a model to output a prediction for each pixel. Compared to whole-image classification, segmentation requires not only predicting what can be seen but also where it can be found. \nClassical semantic segmentation models are limited to segment the categories they have been trained on. \nDifferent approaches have emerged that extend this fairly constrained setting (see Tab.~\\ref{tab:segmentation_tasks}):\n\\begin{itemize}\n\\setlength\\itemsep{0em}\n    \\item In generalized zero-shot segmentation, seen as well as unseen categories needs to be segmented by putting unseen categories in relation to seen ones, e.g. through word embeddings \\cite{word2vec} or WordNet \\cite{wordnet}. \n    \\item In one-shot segmentation, the desired class is provided in form of an image (and often an associated mask) in addition to the query image to be segmented.\n    \\item In referring expression segmentation, a model is trained on complex text queries but sees all classes during training (i.e. no generalization to unseen classes).\n\\end{itemize}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=8cm]{mini_overview.pdf}\n    \\caption{Our key idea is to use CLIP to build a flexible zero/one-shot segmentation system that addresses multiple tasks at once.}\n    \\label{fig:mini_overview}\n\\end{figure}\n\nTo this work, we introduce the CLIPSeg model (Fig.~\\ref{fig:mini_overview}), which is capable of segmenting based on an arbitrary text query or an example image. CLIPSeg can address all three tasks named above.\nThis multi-modal input format goes beyond existing multi-task benchmarks such as Visual Decathlon \\cite{rebuffi17learning} where input is always provided in form of images. %In our system, training on a single modality and using the other modality at test time is possible, for example, a system trained on images can handle a text query.\nTo realize this system, we employ the pre-trained CLIP model as a backbone and train a thin conditional segmentation layer (decoder) on top. We use the joint text-visual embedding space of CLIP for conditioning our model, which enables us to process prompts in text form as well as images. \nOur idea is to teach the decoder to relate activations inside CLIP with an output segmentation, while permitting as little dataset bias as possible and maintaining the excellent and broad predictive capabilities of CLIP.\n\nWe employ a generic binary prediction setting, where a foreground that matches the prompt has to be differentiated from background. \nThis binary setting can be adapted to multi-label predictions which is needed by Pascal zero-shot segmentation.\nAlthough the focus of our work is on building a versatile model, we find that CLIPSeg achieves competitive performance across three low-shot segmentation tasks. Moreover, it is able to generalize to classes and expressions for which it has never seen a segmentation.\n\n\\begin{table}\n    \\footnotesize\n    \\centering\n    \\setlength{\\tabcolsep}{1mm}\n    \\begin{tabular}{lcccccc}\n    \\toprule\n        & \\multirow{2}{0.9cm}{unseen classes} & \\multirow{2}{1.2cm}{free form prompt} & \\multirow{2}{1.1cm}{no fixed targets} & \\multirow{2}{1cm}{negative samples} \\\\\n        \\\\\n    \\midrule\n        Our setting & \\checkmark & \\checkmark &  \\checkmark & \\checkmark \\\\\n        \\midrule\n        Classic & - & - & - & \\checkmark \\\\\n        Referring Expression  & - & \\checkmark & \\checkmark & -  \\\\\n        Zero-shot & \\checkmark & -  & \\checkmark & \\checkmark  \\\\\n        One-shot & \\checkmark & -  & \\checkmark &  - \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Comparison of different segmentation tasks. \n    Negative means samples that do not contain the target (or one of the targets in multi-label segmentation). All approaches except classic segmentation adapt to new targets dynamically at inference time.\n    }\n    \\label{tab:segmentation_tasks}\n\\end{table}\n\n\\paragraph{Contributions}\n\nOur main technical contribution is the CLIPSeg model, which extends the well-known CLIP transformer for zero-shot and one-shot segmentation tasks by a proposing a lightweight transformer-based decoder. A key novelty of this model is that the segmentation target can be specified by different modalities: through text or an image. \nThis allows us to train a unified model for several benchmarks. \nFor text-based queries, unlike networks trained on PhraseCut, our model is able to generalize to new queries involving unseen words. For image-based queries, we explore various forms of visual prompt engineering -- analogously to text prompt engineering in language modeling.\nFurthermore, we evaluate how our model generalizes to novel forms of prompts involving affordances.\n\n\\section{Related Work}\n\n\\paragraph{Foundation Models and Segmentation}\n\nInstead of learning from scratch, modern vision systems are commonly pre-trained on a large-scale dataset (either supervised \\cite{imagenet} or self-supervised \\cite{chen20, chen21}) and use weight transfer. The term foundation model has been coined for very large pre-training models that are applicable to multiple downstream tasks \\cite{bommasani21foundation}. One of these models is CLIP \\cite{radford20}, which has demonstrated excellent performance on several image classification tasks. In contrast to previous models which rely on ResNet \\cite{he16} backbones, the best-performing CLIP model uses a novel visual transformer \\cite{dosovitskiy20} architecture.\nAnalogously to image classification, there have been efforts to make use of transformers for segmentation: \n\\mbox{TransUNet} \\cite{chen21transunet} and SETR \\cite{zheng21setr} employ a hybrid architecture which combine a visual transformer for encoding with a CNN-based decoder.\nSegformer \\cite{xie21segformer} combines a transformer encoder with an MLP-based decoder.\nThe Segmentor model \\cite{strudel21segmenter} pursues a purely transformer-based approach. To generate a segmentation, either a projection of the patch embeddings or mask transformer are proposed. \nOur CLIPSeg model extends CLIP with a transformer-based decoder, i.e. we do not rely on convolutional layers.\n\n\\paragraph{Referring Expression Segmentation}\n\nIn referring expression segmentation a target is specified in a natural language phrase. The goal is to segment all pixels that match this phrase. Early approaches used recurrent networks in combination with CNNs to address this problem \\cite{hu2016seg_nl, liu17refseg, shi18, li18referring}.\nThe CMSA module, which is central to the approach of \\citet{ye19}, models long-term dependencies between text and image using attention.\nThe more recent HULANet method \\cite{wu20phrasecut} consists of Mask-RCNN backbone and specific modules processing categories, attributes and relations, which are merged to generate a segmentation mask.\nMDETR \\cite{kamath21} is an adaptation of the detection method DETR \\cite{carion20end} to natural language phrase input. It consists of a CNN which extracts features and a transformer which predicts bounding boxes for a set of query prompts. \nNote that referring expression segmentation does not require generalization to unseen object categories or understanding of visual support images.\nSeveral benchmarks \\cite{yu16modeling, mao16generation, wu20phrasecut} were proposed to track progress in referring expression segmentation. We opt for the PhraseCut dataset \\cite{wu20phrasecut} which is substantially larger in terms of images and classes than other datasets. It contains structured text queries involving objects, attributes and relationships. A query can match multiple object instances.\n\n\\paragraph{Zero-Shot Segmentation}\n\nIn zero-shot segmentation the goal is to segment objects of categories that have not been seen during training. Normally, multiple classes need to be segmented in an image at the same time. In the generalized setting, both seen and unseen categories may occur. \nA key problem in zero-shot segmentation addressed by several methods is the bias which favors seen classes.\n\\citet{bucher19} train a DeepLabV3-based network to synthesize artificial, pixel-wise features for unseen classes based on word2vec label embeddings. These features are used to learn a classifier. \nFollow-up work explicitly models the relation between seen and unseen classes  \\cite{li20}.\nOthers add semantic class information into dense prediction models \\cite{xian19}.\nMore recent approaches use a joint space for image features and class prototypes \\cite{baek21}, employ a probabilistic formulation to account for uncertainty \\cite{hu20} or model the detection of unseen objects explicitly \\cite{zhang21}.\n\n\\paragraph{One-Shot Semantic Segmentation}\nIn one-shot semantic segmentation, the model is provided at test time with a single example of a certain class, usually as an image with a corresponding mask. \nOne-shot semantic segmentation is a comparably new task, with the pioneering work being published in 2017 by Shaban et~al. \\cite{shaban17}, which introduced the Pascal-5i dataset based on Pascal images and labels.\nTheir simple model extracts VGG16-features \\cite{simonyan14} from a masked support image to generate regression parameters that are applied per-location on the output of a FCN \\cite{long15} to yield a segmentation. \nLater works introduce more complex mechanisms to handle one-shot segmentation:\nThe pyramid graph network (PGNet) \\cite{zhang19d} generates a set of differently-shaped feature maps obtained through adaptive pooling and processes them by individual graph attention units and passed through an atrous spatial pyramid pooling (ASPP) block \\cite{chen18a}.\nThe CANet network \\cite{zhang19a} first extracts images using a shared encoder. Then predictions are iteratively refined through a sequence of convolutions and ASPP blocks.\nSeveral approaches focus on the modeling of prototypes \\cite{wang19a, yang20, liu20}.\nPFENet \\cite{tian20a} uses a prior computed on high-level CNN-features to provide an auxiliary segmentation that helps further processing.\nA weakly-supervised variant as introduced by Rakelly et~al. \\cite{rakelly18fewshot} requires only sparse annotations in form of a set of points.\nIn one-shot instance segmentation \\cite{michaelis18}, instead of a binary match/non-match prediction, individual object instances are segmented.\n\n\\paragraph{CLIP Extensions}\nDespite CLIP \\cite{radford20} being fairly new, multiple derivative works across different sub-fields have emerged.\nCLIP was combined with a GAN to modify images based on a text prompt \\cite{patashnik21styleclip} and\nin robotics to generalize to unseen objects in manipulations tasks \\cite{shridhar21cliport}.\nOther work focused on understanding CLIP in more detail. In the original CLIP paper \\cite{radford20}, it was found that the design of prompts matters for downstream tasks, i.e. instead of using an object name alone as a prompt, adding the prefix ``a photo of\" increases performance. \\citet{zhou21coop} propose context optimization (CoOp) which automatically learns tokens that perform well for given downstream tasks.\nOther approaches rely on CLIP for open-set object detection \\cite{gu21zero, esmaeilpour21zero}.\n\n\\begin{figure*}\n\\centering\n \\includegraphics[width=0.8\\textwidth]{clipseg.pdf}\n\\caption{Architecture of CLIPSeg: We extend a frozen CLIP model (red and blue) with a transformer that segments the query image based on either a support image or a support prompt. $N$ CLIP activations are extracted after blocks defined by $\\mathcal{S}$. The segmentation transformer and the projections (both green) are trained on PhraseCut or PhraseCut+.}\\label{fig:clipseg}\n\\end{figure*}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.4\\textwidth]{prompt_engineering.pdf}\n\\caption{Different forms of combining an image with the associated object mask to build a visual prompt have a strong effect on CLIP predictions (bar charts). We use the difference in the probability of the target object (orange) in the original image (left column) and the masking methods for our systematic analysis.}\n\\label{fig:prompt_engineering}\n\\end{figure}\n\n\\section{CLIPSeg Method}\n\nWe use the visual transformer-based (ViT-B/16) CLIP \\cite{radford20} model as a backbone (Fig.~\\ref{fig:clipseg}) and extend it with a small, parameter-efficient transformer decoder. The decoder is trained on custom datasets to carry out segmentation, while the CLIP encoder remains frozen.\nA key challenge is to avoid imposing strong biases on predictions during segmentation training and maintaining the versatility of CLIP. We do not use the larger ViT-L/14@336px CLIP variant as its weights were not publicly released as of writing this work.\n\n\\def\\v{\\mathbf{v}}\n\\def\\t{\\mathbf{t}}\n\\def\\y{\\mathbf{y}}\n\\def\\q{\\mathbf{q}}\n\\def\\s{\\mathbf{s}}\n\n\\paragraph{Decoder Architecture}\n\nConsidering these demands, we propose CLIPSeg: A simple, purely-transformer based decoder, which has U-Net-inspired skip connections to the CLIP encoder that allow the decoder to be compact (in terms of parameters). %Conceptually our architecture resembles Segmentor \\cite{strudel21segmentor} but does not use learned class tokens and has a much smaller number of parameters.\nWhile the query image ($\\mathbb{R}^{W \\times H \\times 3}$) is passed through the CLIP visual transformer, activations at certain layers $\\mathcal{S}$ are read out and projected to the token embedding size $D$ of our decoder. \nThen, these extracted activations (including CLS token) are added to the internal activations of our decoder before each transformer block. The decoder has as many transformer blocks as extracted CLIP activations (in our case 3).\nThe decoder generates the binary segmentation by applying a linear projection on the tokens of its transformer (last layer) $\\mathbb{R}^{(1 + \\frac{W}{P} \\times \\frac{H}{P}) \\times D} \\mapsto  \\mathbb{R}^{W \\times H}$, where $P$ is the token patch size of CLIP.\nIn order to inform the decoder about the segmentation target, we modulate the decoder's input activation by a conditional vector using FiLM \\cite{dumoulin18}. \nThis conditional vector can be obtained in two ways: (1) Using the CLIP text-transformer embedding of a text query and (2) using the CLIP visual transformer on a feature engineered prompt image. \nCLIP itself is not trained, but only used as a frozen feature extractor. Due to the compact decoder, CLIPSeg has only 1,122,305 trainable parameters for $D=64$.\n\nThe original CLIP is constrained to a fixed image size due to the learned positional embedding. We enable different image sizes (including larger ones) by interpolating the positional embeddings. To validate the viability of this approach, we compare prediction quality for different image sizes and find that for ViT-B/16 performance only decreases for images larger than 350 pixels (see supplementary for details).\nIn our experiments we use CLIP ViT-B/16 with a patch size $P$ of 16 and use a projection dimension of $D = 64$ if not indicated otherwise. We extract CLIP activations at layers $\\mathcal{S} = [3,7,9]$, consequently our decoder has only three layers.\n\n\\paragraph{Image-Text Interpolation}\n\\label{sec:text_image_interpolation}\nOur model receives information about the segmentation target (``what to segment?'') through a conditional vector. This can be provided either by text or an image (through visual prompt engineering). Since CLIP uses a shared embedding space for images and text captions, we can interpolate between both in the embedding space and condition on the interpolated vector.\nFormally, let $\\s_i$ be the embedding of the support image and $\\t_i$ the text embedding of a sample $i$, we obtain a conditional vector $\\x_i$ by a linear interpolation $\\x_i= a \\s_i + (1-a) \\x_i$, where $a$ is sampled uniformly from $[0, 1]$. \nWe use this randomized interpolation as a data augmentation strategy during training.\n\n\\subsection{PhraseCut + Visual prompts (PC+)}\n\nWe use the PhraseCut dataset \\cite{wu20phrasecut}, which encompasses over 340,000 phrases with corresponding image segmentations.\nOriginally, this dataset does not contain visual support but only phrases and for every phrase a corresponding object exists. We extend this dataset in two ways: visual support samples and negative samples. \nTo add visual support images for a prompt $p$, we randomly draw from the set of all samples $\\mathcal{S}_p$, which share the prompt $p$. In case the prompt is unique ($|\\mathcal{S}_p| = 1$), we rely only on the text prompt. \nAdditionally, we introduce negative samples to the dataset, i.e. samples in which no object matches the prompt. To this end, the sample's phrase is replaced by a different phrase with a probability $q_{neg}$.\nPhrases are augmented randomly using a set of fixed prefixes (as suggested by the CLIP authors). On the images we apply random cropping under consideration of object locations, making sure the object remains at least partially visible.\nIn the remainder of this paper, we call this extended dataset PhraseCut+ (abbreviated by PC+). In contrast to the original PhraseCut dataset, which uses only text to specify the target, PC+ supports training using image-text interpolation. This way, we can train a joint model that operates on text and visual input.\n\n\\section{Visual Prompt Engineering}\n\n\\label{sec:prompt_engineering}\n\n\\begin{table*}\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{lr}\n        \\toprule\n         CLIP modification \\& extras & $\\Delta \\text{P}(\\text{object})$ \\\\\n         \\midrule\nCLIP masking CLS in layer 11 & 1.34 \\\\\nCLIP masking CLS in all layers & 1.71 \\\\\nCLIP masking all in all layers & -14.44 \\\\\ndye object red in grays. image & 1.21 \\\\\nadd red object outline & 2.29 \\\\\n         \\bottomrule\n    \\end{tabular}\n    \\hspace{0.5cm}\n    \\begin{tabular}{lr}\n        \\toprule\n        background modific. & $\\Delta \\text{P}(\\text{object})$ \\\\\n         \\midrule\nBG intensity 50\\% & 3.08 \\\\\nBG intensity 10\\% & 13.85 \\\\\nBG intensity 0\\% & 23.40 \\\\\nBG blur & 13.15 \\\\\n+ intensity 10\\% & 21.73 \\\\\n         \\bottomrule\n    \\end{tabular} \n    \\hspace{0.5cm}\n    \\begin{tabular}{lr}\n        \\toprule\n         cropping \\& combinations & $\\Delta \\text{P}(\\text{object})$ \\\\\n         \\midrule\ncrop large context & 6.27 \\\\\ncrop & 13.60 \\\\\ncrop \\& BG blur & 15.34 \\\\\ncrop \\& BG intensity 10\\% & 21.73 \\\\\n+ BG blur & \\textbf{23.50} \\\\\n         \\bottomrule\n    \\end{tabular}        \n    \\caption{Visual prompt engineering: Average improvement of object probability for different forms of combining image and mask over 1,600 samples. Cropping means cutting the image according to the regions specified by the mask, ``BG'' means background.}\n    \\label{tab:prompt_engineering_short}\n\\end{table*}\n\nIn conventional, CNN-based one-shot semantic segmentation, masked pooling \\cite{shaban17} has emerged as a standard technique to compute a prototype vector for conditioning. The provided support mask is downsampled and multiplied with a late feature map from the CNN along the spatial dimensions and then pooled along the spatial dimensions. This way, only features that pertain to the support object are considered in the prototype vector.\nThis method cannot be applied directly to transformer-based architectures, as semantic information is also accumulated in the CLS token throughout the hierarchy and not only in the feature maps. \nCircumventing the CLS token and deriving the conditional vector directly from masked pooling of the feature maps is not possible either, since it would break the compatibility between text embeddings and visual embeddings of CLIP.\n\nTo learn more about how target information can be incorporated into CLIP, we compare several variants in a simple experiment without segmentation and its confounding effects.\nWe consider the cosine distance (alignment) between visual and text-based embedding and use the original CLIP weights without any additional training.\n\nSpecifically, we use CLIP to compute the text embeddings $\\mathbf{t}_i$ which correspond to object names in the image. We then compare those to (1) the visual embedding of the original image without modifications, $\\mathbf{s}_{\\text{o}}$ and (2) the visual embedding $\\mathbf{s}_{\\text{h}}$ highlighting the target object using a modified RGB image or attention mask (both techniques are described in detail below).\nBy softmax-normalizing the vector of alignments $[\\mathbf{s}_{\\text{h}} \\mathbf{t}_0, \\mathbf{s}_{\\text{h}} \\mathbf{t}_1, \\dots]$ for different highlighting techniques and images, we obtain the distributions shown in Fig.~\\ref{fig:prompt_engineering}.\nFor quantitative scores, we consider only the target object name embedding $\\mathbf{t}_0$, which we expect to have a stronger alignment with the highlighted image embedding $\\mathbf{s}_{\\text{h}}$ than with the original image embedding $\\mathbf{s_0}$ (Fig.~\\ref{fig:prompt_engineering}). This means, if a highlighting technique improves the alignment, the increase in object probability $\\Delta \\text{P}(\\text{object}) = \\mathbf{s}_{\\text{h}} \\mathbf{t}_0  - \\mathbf{s}_{o} \\mathbf{t}_0 $ should be large.\nWe base this analysis on the LVIS dataset \\cite{gupta19lvis} since its images contain multiple objects and a rich set of categories. We sample 1,600 images and mask one target object out of all objects present in this image. \n\n\\paragraph{CLIP-Based Masking}\nThe straightforward equivalent to masked pooling in a visual transformer is to apply the mask on the tokens. Normally, a visual transformer consists of a fixed set of tokens which can interact at every layer through multi-head attention: A CLS token used for read-out and image-region-related tokens which were originally obtained from image patches. \nNow, the mask can be incorporated by constraining the interaction at one (e.g. the last layer 11) or more transformer layers to within-mask patch tokens as well as the CLS token only.\nOur evaluation (Tab.~\\ref{tab:prompt_engineering_short}, left) suggests that this form of introducing the mask does not work well. By constraining the interactions with the CLS token (Tab.~\\ref{tab:prompt_engineering_short}, left, top two rows) only a small improvement is achieved (in last layer or in all layers) while constraining all interactions decreases performance dramatically. From this we conclude that more complex strategies are necessary to combine image and mask internally.\n\n\\paragraph{Visual Prompt Engineering}\nInstead of applying the mask inside the model, we can also combine mask and image to a new image, which can then processed by the visual transformer.\nAnalogous to prompt engineering in NLP (e.g. in GPT-3 \\cite{brown20}), we call this procedure visual prompt engineering. Since this form of prompt design is novel and strategies which perform best in this context are unknown, we conduct an extensive evaluation of different variants of designing visual prompts (Tab.~\\ref{tab:prompt_engineering_short}).\nWe find that the exact form of how the mask and image are combined matters a lot. Generally, we identify three image operations that improve the alignment between the object text prompts and the images: decreasing the background brightness, blurring the background (using a Gaussian filter) and cropping to the object. The combination of all three performs best (Tab.~\\ref{tab:prompt_engineering_short}, last row). We will use this variant in the remainder.\n\n\\section{Experiments}\n\nWe first evaluate our model on three established segmentation benchmarks before demonstrating the main contribution of our work: flexible few-shot segmentation that can be based on either text or image prompts.\n\n\\paragraph{Metrics}\n\nCompared to approaches in zero-shot and one-shot segmentation (e.g. \\cite{bucher19, li20}), the vocabulary we use is open, i.e. the set of classes or expressions is not fixed. Therefore, throughout the experiments, our models are trained to generate binary predictions that indicate where objects matching the query are located. If necessary, this binary setting can be transformed into a multi-label setting (as we do in Section~\\ref{sec:zeroshot}).\n\nIn segmentation, intersection over union (IoU, also Jaccard score) is a common metric to compare predictions with ground truth. \nDue to the diversity of the tasks, we employ different forms of IoU: Foreground IoU ($\\ioufg$) which computes IoU on foreground pixels only, mean IoU, which computes the average over foreground IoUs of different classes and binary IoU ($\\ioubin$) which averages over foreground IoU and background IoU.\nIn binary segmentation, IoU requires a threshold $t$ to be specified. While most of the time the natural choice of 0.5 is used, the optimal values can strongly deviate from 0.5 if the probability that an object matching the query differs between training and inference (the a-priori probability of a query matching one or more objects in the scene depends highly on context and dataset). \nTherefore, we report performance of one-shot segmentation using thresholds $t$ optimized per task and model. Additionally, we adopt the average precision metric (AP) in all our experiments.\nAverage precision measures the area under the recall-precision curve. It measures how well the system can discriminate matches from non-matches, independent of the choice of threshold. %Therefore, no threshold must be selected.\n\n\\paragraph{Models and Baselines}\n\nIn our experiments we differentiate two variants of CLIPSeg: One trained on the original PhraseCut dataset (PC) and one trained on the extended version of PhraseCut which uses 20\\% negative samples, contains visual samples (PC+) and uses image-text interpolation (Sec.~\\ref{sec:text_image_interpolation}). The robust latter version we call the universal model. To put the performance of our models into perspective, we provide two baselines:\n\\begin{itemize}\n\\setlength\\itemsep{0em}\n    \\item \\emph{CLIP-Deconv} encompasses CLIP but uses a very basic decoder, consisting only of the basic parts: FiLM conditioning \\cite{dumoulin18}, a linear projection and a deconvolution. This helps us to estimate to which degree CLIP-alone is responsible for the results.\n    \\item \\emph{ViTSeg} shares the architecture of CLIPSeg, but uses an ImageNet-trained visual transformer as a backbone \\cite{timm}. For encoding text, we use the same text transformer of CLIP. This way we learn to which degree the specific CLIP weights are crucial for good performance.\n\\end{itemize}\nWe rely on PyTorch \\cite{pytorch} for training and use an image size of 352 $\\times$ 352 pixels throughout our experiments (for details see appendix).\n\n\\subsection{Referring Expression Segmentation}\nWe evaluate referring expression segmentation performance (Tab.~\\ref{tab:ref_seg}) on the original PhraseCut dataset and compare to scores reported by \\citet{wu20phrasecut} as well as the concurrently developed transformer-based MDETR method \\cite{kamath21}. \nFor this experiment we trained a version of CLIPSeg on the original PhraseCut dataset (CLIPSeg [PC]) using only text labels in addition to the universal variant which also includes visual samples (CLIPSeg [PC+]).\n\nOur approaches outperform the two-stage HULANet approach by Wu et al. \\cite{wu20phrasecut}. Especially, a high capacity decoder ($D=128$) seems to be beneficial for PhraseCut. However, the performance worse than MDETR \\cite{kamath21}, which operates at full image resolution and received two rounds of fine-tuning on PhraseCut. \nNotably, the ViTSeg baseline performs generally worse than CLIPSeg, which shows that CLIP pre-training is helpful.\n\n\\begin{table}[]\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{llllllll}\n        \\toprule\n        & $t$ &  $\\miou$ & $\\ioufg$ & AP \\\\\n        \\midrule\n        CLIPSeg (PC+)    &  0.3 &  43.4 &  54.7 &  76.7 \\\\\n CLIPSeg (PC, $D=128$) &  0.3 &  48.2 &  \\textbf{56.5} &  \\textbf{78.2} \\\\\n          CLIPSeg (PC)   &  0.3 &  46.1 &  56.2 &  \\textbf{78.2} \\\\\n           CLIP-Deconv   &  0.3 &  37.7 &  49.5 &  71.2 \\\\\n         ViTSeg (PC+)    &  0.1 &  28.4 &  35.4 &  58.3 \\\\\n           ViTSeg (PC)   &  0.3 &  38.9 &  51.2 &  74.4 \\\\\n        \\midrule\n        MDETR \\cite{kamath21} & \\conf{ICCV21} & \\textbf{53.7} & -  & - \\\\\n       HulaNet \\cite{wu20phrasecut} & \\conf{CVPR20}  & 41.3 & 50.8   & -\\\\\n       Mask-RCNN top \\cite{wu20phrasecut} & \\conf{CVPR20} & 39.4 & 47.4  & - \\\\\n       RMI \\cite{wu20phrasecut} & \\conf{CVPR20} & 21.1 & 42.5  & - \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{Referring Expression Segmentation performance on PhraseCut ($t$ refers to the binary threshold).}\n    \\label{tab:ref_seg}\n\\end{table}\n\n\\subsection{Generalized Zero-Shot Segmentation}\n\\label{sec:zeroshot}\n\nIn generalized zero-shot segmentation, test images contain categories that have never been seen before in addition to known categories. \nWe evaluate the model's zero-shot segmentation performance using the established Pascal-VOC benchmark (Tab.~\\ref{tab:zero_shot_performance}). It contains five splits involving 2 to 10 unseen classes (we report only 4 and 10 unseen classes). The latter is the most challenging setting as the set of unseen classes is large.\nSince our model was trained on foreground/background segmentation we cannot directly use it in a multi-label setting. Therefore, we employ a simple adaptation: Our model predicts a binary map independently for each of the 20 Pascal classes. Across all 20 predictions we determine the class with the highest probability for each pixel. \n\nWe train on PhraseCut+ but remove the unseen Pascal classes from the dataset. This is carried out by assigning the Pascal classes to WordNet synsets \\cite{wordnet} and generating a set of invalid words by traversing hyponyms (e.g. different dog breeds for dog). Prompts that contain such a word are removed from the dataset.\n\nThe idea of conducting this experiment is to provide a reference for the zero-shot performance of our universal model. It should not considered as competing in this benchmark as we use a different training (CLIP pre-training, binary segmentation on PhraseCut).\nThe results (Tab.~\\ref{tab:zero_shot_performance}) indicate a major gap between seen and unseen classes in models trained on Pascal-VOC, while our models tend to be more balanced. This is due to other models being trained exclusively on the 10 or 16 seen Pascal classes in contrast to CLIPSeg which can differentiate many more classes (or phrases). \nIn fact, our model performs better on unseen classes than on seen ones. This difference is likely because the seen classes are generally harder to segment: For the unseen-4 setting, the unseen classes are ``airplane'', ``cow'', ``motorbike'' and ``sofa''. All of them are large and comparatively distinct objects.\n\n\\begin{table}[]\n    \\centering\n    \\footnotesize\n    \\setlength{\\tabcolsep}{1.5mm}\n    \\begin{tabular}{ll|rr|rrll}\n         \\toprule\n         && \\multicolumn{2}{c|}{unseen-10} & \\multicolumn{2}{c}{unseen-4} \\\\\n        & pre-train. & mIoU$_S$ & mIoU$_U$ & mIoU$_S$ & mIoU$_U$  \\\\\n        \\midrule\nCLIPSeg (PC+) & CLIP &  \\textbf{35.7} & \\textbf{43.1} & 20.8 & \\textbf{47.3} \\\\\nCLIP-Deconv (PC+) & CLIP &  25.1 & 36.7 & \\textbf{25.9} & 41.9 \\\\\nViTSeg (PC+) & IN &  4.2 & 19.0 & 6.0 & 24.8 \\\\\n        \\midrule\n        \\midrule\n        SPNet \\cite{xian19} & IN & 59.0 & 18.1 & 67.3 & 21.8 \\\\\n        ZS3Net \\cite{bucher19} & IN-seen & 33.9 & 18.1 & 66.4 & 23.2 \\\\\n        CSRL \\cite{li20consistent}  & IN-seen  & 59.2 & 21.0 & 69.8 & 31.7  \\\\\n        CaGNet \\cite{gu2020context} & IN & - & - & 69.5 & 40.2 \\\\\n        OSR \\cite{zhang21} & IN-seen &  \\textbf{72.1} & \\textbf{33.9} & \\textbf{75.0} & \\textbf{44.1} \\\\\n        JoEm \\cite{baek21} & IN-seen & 63.4 & 22.5 & 67.0 & 33.4  \\\\\n        \\bottomrule\n    \\end{tabular}% \n    \\caption{Zero-shot segmentation performance on Pascal-VOC with 10 unseen classes. mIoU$_S$ and mIoU$_U$ indicate performance on seen and unseen classes, respectively. Our model is trained on PhraseCut with the Pascal classes being removed but uses a pre-trained CLIP backbone. IN-seen indicates ImageNet pre-training with unseen classes being removed.}\n    \\label{tab:zero_shot_performance}\n\\end{table}\n\n\\subsection{One-Shot Semantic Segmentation}\n\nIn one-shot semantic segmentation, a single example image along with a mask is presented to the network. Regions that pertain to the class highlighted in the example image must be found in a query image. Compared to previous tasks, we cannot rely on a text label but must understand the provided support image.\nAbove (Sec.~\\ref{sec:prompt_engineering}) we identified the best method for visual prompt design, which we use here: cropping out the target object while blurring and darkening the background. \nTo remove classes that overlap with the respective subset of Pascal during training, we use the same method as in the previous section (Sec.~\\ref{sec:zeroshot}).\nOther than in zero-shot segmentation, in one-shot segmentation, ImageNet pre-trained backbones are common \\cite{tian20a, wang19a}. PFENet particularly leverages pre-training by using high-level feature similarity as a prior. Similarly, HSNet \\cite{min21hsnet} processes correlated activations of query and support image using 4D-convolutions at multiple levels.\n\nOn Pascal-5i we find our universal model CLIPSeg (PC+) to achieve competitive performance (Tab.~\\ref{tab:pas5i_oneshot}) among state-of-the-art methods, with only the very recent HSNet performing better.\nThe results on COCO-20i (Tab.~\\ref{tab:coco20i_oneshot}) show that CLIPSeg also works well when trained on other datasets than PhraseCut(+). Again HSNet performs better. To put this in perspective, it should be considered that HSNet (and PFENet) are explicitly designed for one-shot segmentation, rely on pre-trained CNN activations and cannot handle text by default:\n\\citet{tian20a} extended PFENet to zero-shot segmentation (but used the one-shot protocol) by replacing the visual sample with word vectors \\cite{word2vec, mikolov18} of text labels. In that case, CLIPSeg outperforms their scores by a large margin (Tab.~\\ref{tab:pas5i_zeroshot}).\n\n\\begin{table}[t]\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{llrrrrr}\n         \\toprule\n        & $t$ & vis. backb. & $\\miou$ & $\\ioubin$ &  AP \\\\\n        \\midrule\nCLIPSeg (PC+) & 0.3 & ViT (CLIP) & \\textbf{59.5} & \\textbf{75.0} & \\textbf{82.3} \\\\\nCLIPSeg (PC)  & 0.3 & ViT (CLIP) & 52.3 & 69.5 & 72.4 \\\\\nCLIP-Deconv (PC+) & 0.2 & ViT (CLIP) & 48.0 & 65.8 & 68.0 \\\\\nViTSeg (PC+) & 0.2 & ViT (IN) & 39.0 & 59.0 & 62.4 \\\\\n        \\midrule\n        PPNet  \\cite{liu20} & & RN50 & 52.8  & 69.2  & - \\\\\n        RePRI \\cite{boudiaf20} & & RN50 & 59.7 & - & -  \\\\\n        PFENet \\cite{tian20a} & & RN50 & 60.2  & 73.3 & -  \\\\\n        HSNet \\cite{min21hsnet} & & RN50 & \\textbf{64.0} & \\textbf{76.7} & -\\\\\n        \\midrule\n        PPNet  \\cite{liu20} & & RN101 & 55.2  & 70.9 & -  \\\\\n        RePRI \\cite{boudiaf20} & & RN101  & 59.4  & - & - \\\\\n        PFENet \\cite{tian20a} & & RN101 & 59.6 & 72.9 & - \\\\\n        HSNet \\cite{min21hsnet} & & RN101 & \\textbf{66.2} & \\textbf{77.6} & - \\\\\n        \\bottomrule\n         \\end{tabular} % \n    \\caption{One-shot performance on Pascal-5i (CLIPSeg and ViTSeg trained on PhraseCut+).}\n    \\label{tab:pas5i_oneshot}\n\\end{table}\n\n\\begin{table}[t]\n    \\centering\n    \\footnotesize\n    \\setlength{\\tabcolsep}{1.7mm}\n    \\begin{tabular}{llrrrr}\n        \\midrule\n        & $t$ & vis. backb. & $\\miou$ & $\\ioubin$ &  AP  \\\\\n        \\midrule\nCLIPSeg (COCO) & 0.1 & ViT (CLIP) &  33.2 & 58.4 & 40.5 \\\\\nCLIPSeg (COCO+N)  & 0.1 & ViT (CLIP) &  \\textbf{33.3} & \\textbf{59.1} & \\textbf{41.7} \\\\\nCLIP-Deconv (COCO+N)  & 0.1 & ViT (CLIP) &  29.8 & 56.8 & 40.8 \\\\\nViTSeg (COCO)  & 0.1 & ViT (IN) &  14.4 & 46.1 & 15.7 \\\\\n        \\midrule\n        PPNet \\cite{liu20} & & RN50 & 29.0  & - & -  \\\\\n        RePRI \\cite{boudiaf20} & & RN50  & 34.0  & - & - \\\\\n        PFENet \\cite{tian20a} & & RN50 & 35.8 & - & - \\\\\n        HSNet \\cite{min21hsnet} & & RN50 & \\textbf{39.2} & \\textbf{68.2} & - \\\\\n        \\midrule\n        HSNet \\cite{min21hsnet} & & RN101 & \\textbf{41.2} & \\textbf{69.1} & - \\\\\n        \\bottomrule\n    \\end{tabular} %\n    \\caption{One-shot performance on COCO-20i (CLIPSeg trained on PhraseCut), +N indicates 10\\% negative samples.}\n    \\label{tab:coco20i_oneshot}\n\\end{table}\n\n\\begin{table}[t]\n    \\centering\n    \\footnotesize\n    \\setlength{\\tabcolsep}{1mm}\n    \\begin{tabular}{llrrrrr}\n         \\toprule \n        \\textbf{Pascal-5i} & $t$ & vis. backb. & $\\miou$ & $\\ioubin$ &  AP \\\\\n        \\midrule\nCLIPSeg (PC+) & 0.3 & ViT (CLIP) & \\textbf{72.4} & \\textbf{83.1} & \\textbf{93.5} \\\\\nCLIPSeg (PC) & 0.3 & ViT (CLIP) & 70.3 & 81.6 & 84.8 \\\\\nCLIP-Deconv (PC+) & 0.3 & ViT (CLIP) & 63.2 & 77.3 & 85.3 \\\\\nViTSeg (PC+) & 0.2 & ViT (IN) & 39.0 & 59.0 & 62.4 \\\\\n        \\midrule\n        LSeg  \\cite{li2022languagedriven} & & ViT (CLIP) & 52.3 & 67.0 & -  \\\\\n        PFENet \\cite{tian20a} & & VGG16 & 54.2 & - & - \\\\\n        \\bottomrule\n    \\end{tabular}% \n    \\caption{Zero-shot performance on Pascal-5i. The scores were obtained by following the evaluation protocol of one-shot segmentation but using text input.}\n    \\label{tab:pas5i_zeroshot}\n\\end{table}\n\n\\subsection{One Model For All: Generalized Prompts}\n\n\\label{sec:generalize}\n\nWe have shown that CLIPSeg performs well on a variety of academic segmentation benchmarks. Next, we evaluate its performance ``in the wild\" in unseen situations.\n\n\\paragraph{Qualitative Results}\n\nIn Fig.~\\ref{fig:text_qualitative} we show qualitative results divided into two groups: (1, left) Affordance-like \\cite{gibson66, gibson79} (``generalized'')  prompts that are different from the descriptive prompts of PhraseCut and (2, right) prompts that were taken from the PhraseCut test set. For the latter we add challenging extra prompts involving an existing object but the wrong color (indicated in orange). \nGeneralized prompts, which deviate from the PhraseCut training set by referring to actions (```something to ...'') or rare object classes (```cutlery'') work surprisingly well given that the model was not trained on such cases. It has learned an intuition of stuff that can be stored away in cupboards, where sitting is possible and what ``living creature'' means. Rarely, false positives are generated (the bug in the salad is not a cow).\nDetails in the prompt are reflected by the segmentation (blue boxes) and information about the color influences predicted object probabilities strongly (orange box).\n\n\\begin{figure*}[tb]\n    \\centering\n    \\includegraphics[width=0.98\\textwidth]{qualitative_clipseg_phrasecut3.pdf}\n    \\caption{Qualitative predictions of CLIPSeg (PC+) for various prompts, darkness indicates prediction strength. The generalized prompts (left) deviate from the PhraseCut prompts as they involve action-related properties or new object names.}\n    \\label{fig:text_qualitative}\n\\end{figure*}\n\n\\paragraph{Systematic Analysis}\nTo quantitatively assess the performance for generalized queries, we construct subsets of the LVIS test datasets containing only images of classes that correspond to affordances or attributes.\nThen we ask our model to segment with these affordances or attributes as prompts. For instance, we compute the foreground intersection of union between armchair, sofa and loveseat objects when ``sit on'' is used as prompt.\nA complete list of which affordances or attributes are mapped onto which objects can be found in the appendix.\nWe find (Tab.~\\ref{tab:generalize}) that the CLIPSeg version trained on PC+ performs better than the CLIP-Deconv baseline and the version trained on LVIS, which contains only object labels instead of complex phrases. This result suggests that both dataset variability and model complexity are necessary for generalization. \nViTSeg performs worse, which is expected as it misses the strong CLIP backbone, known for its generalization capabilities.\n\n\\begin{table}\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{llllllll}\n        \\toprule\n         & \\multicolumn{2}{c}{Affordances} & \\multicolumn{2}{c}{Attributes} & \\multicolumn{2}{c}{Meronymy}  \\\\\n        &  $\\miou$ &  AP & $\\miou$ & AP & $\\miou$  & AP \\\\\n        \\midrule\nCLIPSeg (PC+) & 36.9 & \\textbf{50.5} & 26.6 & \\textbf{43.0} & \\textbf{25.7} & \\textbf{29.0} \\\\ \nCLIPSeg (LVIS)  & \\textbf{37.7} & 44.6 & 18.4 & 16.6 & 18.9 & 13.8 \\\\ \nCLIP-Deconv & 32.2 & 43.7 & 23.1 & 35.6 & 21.1 & 27.1 \\\\ \nVITSeg (PC+) & 19.2 & 23.5 & \\textbf{26.8} & 28.0 & 18.4 & 15.9 \\\\\n         \\bottomrule\n    \\end{tabular}% \n    \\caption{Performance for generalized prompts. While the PC+-model has seen prompts during training (colliding prompts with test set were removed), the LVIS version was trained on object classes only and is able to generalize due to the CLIP backbone. We use the best threshold $t$ for each model.}\n    \\label{tab:generalize}\n\\end{table}\n\n\\subsection{Ablation Study}\n\nIn order to identify crucial factors for the performance of CLIPSeg, we conduct an ablation study on PhraseCut (Tab.~\\ref{tab:ablation}). We evaluate text-based and visual prompt-based performance (obtained using our modifications on PhraseCut) separately for a complete picture.\nBoth text-based and visual performance drops when random weights instead of CLIP weights are used (``no CLIP pre-training''). When the number of parameters is reduced to 16 (``$D=16$'') performance decreases substantially, which indicates the importance of the information processing in the decoder.\nUsing an unfavourable visual prompting technique (``highlight mask'') degrades performance on visual input, which supports our findings from Sec.~\\ref{sec:prompt_engineering}. \nUsing only early activations from layer 3 decreases performance (``only layer 3''), from which we conclude that higher level features of CLIP are useful for segmentation.\nTraining without visual samples (``no visual'') decreases the performance on visual samples, which is expected as visual and text vectors do not align perfectly. The gap in text-based performance to the hybrid version (PC+) is negligible.\n\n\\begin{table}[t]\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{llllllll}\n        \\toprule\n        & \\multicolumn{2}{c}{Text-based} & \\multicolumn{2}{c}{Visual-based} \\\\\n         & $\\miou$ &  AP & $\\miou$ &  AP   \\\\\n        \\midrule\n              CLIPSeg (PC+) &  43.6 &  76.7 &  \\textbf{25.4} &  \\textbf{55.6} \\\\\n no CLIP pre-training &  13.1 &  12.6 &  12.7 &   - \\\\\n            no visual &  \\textbf{46.4} &  \\textbf{77.8} &  14.4 &  31.0 \\\\\n                     $D=16$ &  37.4 &  71.5 &  24.7 &  51.2 \\\\\n         only layer 3 &  31.9 &  64.9 &  21.5 &  48.6 \\\\\n       highlight mask &  43.4 &  75.4 &  23.3 &  43.8 \\\\\n        \n        \\bottomrule\n    \\end{tabular}\n    \\caption{Ablation study conducted on PhraseCut, involving text (left) and visual prompts (right) at test time. We use the best threshold $t$ for each model.}\n    \\label{tab:ablation}\n\\end{table}\n\n\\section{Conclusion}\n\nWe presented the CLIPSeg image segmentation approach that can be adapted to new tasks by text or image prompts at inference time instead of expensive training on new data. \nSpecifically, we investigated the novel visual prompt engineering in detail and demonstrated competitive performance on referring expression, zero-shot and one-shot image segmentation tasks.\nBeyond that, we showed -- both qualitatively and quantitatively -- that our model generalizes to novel prompts involving affordances and properties.\nWe expect our method to be useful, especially for inexperienced users for building a segmentation model by specifying prompts and in robotic setups when interaction with humans is desired.\nWe believe that tackling multiple tasks is a promising direction for future research toward more generic and real-world compatible vision systems.\nIn a wider context, our experiments, in particular the comparison to the ImageNet-based ViTSeg baseline, highlight the power of foundation models like CLIP for solving several tasks at once.\n\n\\paragraph{Limitations}\nOur experiments are limited to only a small number of benchmarks, in future work more modalities such as sound and touch could be incorporated. We depend on a large-scale dataset (CLIP) for pre-training. Note, we do not use the best-performing CLIP model ViT-L/14@336px due to weight availability. Furthermore, our model focuses on images, an application to video might suffer from missing temporal consistency. Image size may vary but only within certain limits (for details see supplementary).\n\n\\paragraph{Broader Impact}\nThere is a chance that the model replicates dataset biases from PhraseCut but especially from the unpublished CLIP training dataset. Provided models should be used carefully and not in tasks depicting humans.\nOur approach enables adaptation to new tasks without energy-intensive training. \n\n{\n\\small\n}\n\n\\clearpage\n\n\\section*{Appendix}\n\n\\subsection*{Experimental Setup}\n\n\\label{sec:train_setup}\n\nThroughout our experiments we use PyTorch \\cite{pytorch} with CLIP ViT-B/16 \\cite{radford20}. We train on PhraseCut \\cite{wu20phrasecut} for 20,000 iterations on batches of size 64 with an initial learning rate of 0.001 (for VitSeg 0.0001) which decays following a cosine learning rate schedule to 0.0001 (without warmup). We use automatic mixed precision and binary cross entropy as the only loss function.\n\n\\subsection*{Image-size Dependency of CLIP}\nSince multi-head attention does not require a fixed number of tokens, the visual transformer of CLIP can handle inputs of arbitrary size. However, the publicly available CLIP models (ViT-B/16 and ViT-B/32) were trained on 224 $\\times$ 224 pixel images. In this experiment we investigate how CLIP performance relates to the input image size -- measured in a classification task.\nTo this end, we extract the CLS token vector in the last layer from both CLIP models. Using this feature vector as an input, we train a logistic regression classifier on a subset of ImageNet \\cite{imagenet} classes differentiating 67 classes of vehicles (Fig. \\ref{fig:image_size}).\nOur results indicate that CLIP generally handles large image sizes well, with the 16-px-patch version (ViT-B/16) showing a slightly better performance at an optimal image size of around 350 $\\times$ 350 pixels.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=7cm]{clip_image_dependency.pdf}\n    \\caption{Image classification performance of CLIP over different image sizes.}\n    \\label{fig:image_size}\n\\end{figure}\n\n\\subsection*{Object-mapping for Affordances and Attributes}\n\\label{sec:object_mapping}\n\nFor our systematic analysis on generalization (Section 5.5 in the main paper), we generate samples by replacing the following object categories by affordances (bold). \n\\\\\n\n\\noindent Affordances:\\\\\n\\textbf{{sit on}}: armchair, sofa, loveseat, deck chair, rocking chair, highchair, deck chair, folding chair, chair, recliner, wheelchair\\\\\n\\textbf{{drink from}}: bottle, beer bottle, water bottle, wine bottle, thermos bottle\\\\\n\\textbf{{ride on}}: horse, pony, motorcycle\\\\\n\n\\noindent Attributes:\\\\\n\\textbf{{can fly}}: eagle, jet plane, airplane, fighter jet, bird, duck, gull, owl, seabird, pigeon, goose, parakeet\\\\\n\\textbf{{can be driven}}: minivan, bus (vehicle), cab (taxi), jeep, ambulance, car (automobile)\\\\\n\\textbf{{can swim}}: duck, duckling, water scooter, penguin, boat, kayak, canoe\\\\\n\n\\noindent Meronymy (part-of relations):\\\\\n\\textbf{{has wheels}}: dirt bike, car (automobile), wheelchair, motorcycle, bicycle, cab (taxi), minivan, bus (vehicle), cab (taxi), jeep, ambulance\\\\\n\\textbf{{has legs}}: armchair, sofa, loveseat, deck chair, rocking chair, highchair, deck chair, folding chair, chair, recliner, wheelchair, horse, pony, eagle, bird, duck, gull, owl, seabird, pigeon, goose, parakeet, dog, cat, flamingo, penguin, cow, puppy, sheep, black sheep, ostrich, ram (animal), chicken (animal), person\\\\\n\n\\subsection*{Average Precision Computation}\n\nThe average precision metric has the advantage of not depending on a fixed threshold. This is particularly useful when new classes occur which lead to uncalibrated predictions. \nInstead of operating on bounding boxes as in detection, we compute average precision at the pixel-level. This makes the computation challenging, since AP is normally computed by sorting all predictions (hence all pixels) according their likelihood, which requires keeping them in the working memory. For pixels, this is not possible.\nTo circumvent this, we define a fixed set of thresholds and aggregate statistics (true-positives, etc.) in each image. Finally, we sum up the statistics per threshold level and compute the precision-recall curve. Average precision, which is the area under the precision-recall curve is computed using Simpson integration.\n\n\\subsection*{Qualitative Predictions}\n\nIn Fig.~\\ref{fig:vit_qualitative} we show predictions of ViTSeg (PC), analogous to Fig.~4 of the main paper. In fact, ViTSeg trained with visual samples (PC+) shows worse performance. \nThe predictions clearly indicate the deficits of an ImageNet-trained ViT backbone compared to CLIP: Details in the prompt are not reflected by the segmentation and a large number of false positives occur. \n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.95\\textwidth]{qualitative_clipseg_phrasecut3.pdf}\n    \\includegraphics[width=0.95\\textwidth]{qualitative_clipseg_phrasecut_vit.pdf}\n    \\caption{Qualitative predictions of CLIPSeg (PC+) (top, same as Fig.~4 of main paper for reference) and ViTSeg (PC) (bottom).}\n    \\label{fig:vit_qualitative}\n\\end{figure*}\n\n\\subsection*{Text prompts, object sizes and classes}\n\nTo develop a better understanding of when our model performs well, we compare different text prompts (Fig.~\\ref{fig:text_prompts}), object sizes (Fig.~\\ref{fig:performance_analysis}, left) and object classes (Fig.~\\ref{fig:performance_analysis}, right). This evaluation is conducted on a pre-trained  CLIPSeg (PC+). In all cases we randomly sample different prompt forms during training. \nHere we assess the performance on 5,000 samples of the PhraseCut test set.\n\nWe see a small effect on performance for alternative prompt forms.\nIn terms of object size there is a clear trend towards better performance on larger objects.\nPerformance over different classes is fairly balanced.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[height=3.5cm]{performance_text_prompts.pdf}\n    \\caption{Effect of different text prompts on performance.}\n    \\label{fig:text_prompts}\n\\end{figure}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[height=3.5cm]{performance_object_size.pdf}\n    \\includegraphics[height=3.5cm]{performance_classes.pdf}\n    \\caption{Effect of object size and class on performance.}\n    \\label{fig:performance_analysis}\n    \n\\end{figure}\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Segment Any Anomaly without Training via \\\\  Hybrid Prompt Regularization}\n\n\\begin{document}\n\n\\maketitle\n\n\\renewcommand{\\thefootnote}{\\fnsymbol{footnote}}\n\\footnotetext[1]{Equal Contribution.}\n\\footnotetext[4]{Corresponding Author.}\n\n\\renewcommand{\\thefootnote}{\\arabic{footnote}}\n\n\\begin{abstract}\nWe present a novel framework, \\textit{i.e.}, Segment Any Anomaly + (SAA$+$), for zero-shot anomaly segmentation with hybrid prompt regularization to improve the adaptability of modern foundation models. Existing anomaly segmentation models typically rely on domain-specific fine-tuning, limiting their generalization across countless anomaly patterns. In this work, inspired by the great zero-shot generalization ability of foundation models like Segment Anything, we first explore their assembly to leverage diverse multi-modal prior knowledge for anomaly localization. For non-parameter foundation model adaptation to anomaly segmentation, we further introduce hybrid prompts derived from domain expert knowledge and target image context as regularization. Our proposed SAA$+$ model achieves state-of-the-art performance on several anomaly segmentation benchmarks, including VisA, MVTec-AD, MTD, and KSDD2, in the zero-shot setting. We will release the code at \\href{https://github.com/caoyunkang/Segment-Any-Anomaly}{https://github.com/caoyunkang/Segment-Any-Anomaly}.\n\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\n Anomaly segmentation models~\\cite{cao_collaborative_2023, wan_industrial_2022, roth2022towards} have attracted great interest in various domains, \\textit{e.g,}, industrial quality control~\\cite{bergmann2019mvtec, bergmann2020uninformed} and medical diagnoses~\\cite{baur_autoencoders_2021}. The key to reliable anomaly segmentation is to discriminate the distribution of anomaly data from normal data. Specifically, this paper considers zero-shot anomaly segmentation (ZSAS) on images, which is a promising yet unexplored setting where neither normal nor abnormal image is provided for the target category during  training.\n\nDue to the scarcity of abnormal samples for training, many works are working towards unsupervised or self-supervised anomaly segmentation, which targets learning a representation of the normal samples during training. Then, the anomalies can be segmented by calculating the discrepancy between the test sample and the learned normal distribution. In specific, these models, including auto-encoder-based reconstruction~\\cite{zhou2020encoding,hou2021divide,zavrtanik2021draem,matsubara2020deep,yan2021learning,jiang2022masked}, one-class classification~\\cite{yi2020patch,massoli2021mocca,sohn2020learning}, and memory-based normal distribution~\\cite{roth2022towards, wan_industrial_2022, cao2023complementary, wang_multimodal_nodate, jiang_softpatch_2022} methods, typically require training separate models for certain limited categories. However, in real-world scenarios, there are millions of industrial products, and it is not cost-effective to collect a large training set for individual objects, which hinders their deployment in cases when efficient deployments are required, \\textit{e.g.}, the initial stage of production.\n\nRecently, foundation models, \\textit{e.g.}, SAM~\\cite{kirillov2023segment} and CLIP~\\cite{radford2021learning}, exhibit great zero-shot visual perception abilities by retrieving prior knowledge stored in these models via prompting~\\cite{li2022align, bommasani2021opportunities}.  In this work, we would like to explore how to adapt foundation models to realize anomaly segmentation under the zero-shot setting. To this end, as is shown in Fig.~\\ref{fig:teaser}, we first construct a vanilla baseline, \\textit{i.e.}, Segment Any Anomaly (SAA), by cascading prompt-guided object detection~\\cite{liu2023grounding} and segmentation foundation models~\\cite{kirillov2023segment}, which serve as Anomaly Region Generator and Anomaly Region Refiner, respectively. Following the practice to unlock foundation model knowledge~\\cite{clipseg2022,jeong2023winclip}, naive language prompts, \\textit{e.g.}, ``\\verb|defect|'' or ``\\verb|anomaly|'', are utilized to segment desired anomalies for a target image. In specific, the language prompt is used to prompt the Anomaly Region Generator to generate prompt-conditioned box-level regions for desired anomaly regions. Then these regions are refined in the Anomaly Region Refiner to produce final predictions, \\textit{i.e.}, masks, for anomaly segmentation. \n\nHowever, as is shown in Figure \\ref{fig:teaser}, vanilla foundation model assembly (SAA) tends to cause significant false alarms, \\textit{e.g.}, SAA wrongly refers to all wicks as anomalies whereas only the overlong wick is a real anomaly, which we attribute to the \\textit{ambiguity} brought by naive language prompts. Firstly, conventional language prompts may become ineffective when facing the domain shift between the pre-training data distribution of foundation models and downstream datasets for anomaly segmentation. Secondly, the degree of ``\\verb|anomaly|'' for a target depends on the object context, which is hard for naive coarse-grained language prompts, \\textit{e.g.}, ``\\verb|an anomaly region|'', to express exactly.\n\nThus, going beyond naive language prompts, we incorporate domain expert knowledge and target image context in our revamped framework, \\textit{i.e.}, Segment Any Anomaly + (SAA$+$), respectively.\nOn the one hand, expert knowledge provides detailed descriptions of anomalies that are relevant to the target in open-world scenarios. We utilize more specific descriptions as in-context prompts, effectively aligning the image content in both pre-trained and target datasets. On the other hand, we utilize the target image context to reliably identify and adaptively calibrate anomaly segmentation predictions~\\cite{object_calibration, xu2022reliable}. By leveraging the rich contextual information present in the target image, we can accurately associate the object context with the final anomaly predictions. %In summary, we regularize the foundation models using domain expert knowledge and target image context, enabling them to unleash their knowledge and develop an improved version model, Segment Any Anomaly$+$ (SAA$+$) (see Figure \\ref{fig:teaser}).\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{figures/teaser.png}\n   \\vspace{-5mm}\n   \\caption{Towards segmenting any anomaly without training, we first construct a vanilla baseline (SAA) by prompting into a cascade of anomaly region generator (\\textit{e.g.}, a prompt-guided object detection foundation model~\\cite{liu2023grounding}) and  anomaly region refiner (\\textit{e.g.}, a segmentation foundation model~\\cite{kirillov2023segment}) modules via a naive class-agnostic language prompt (\\textit{e.g.}, ``Anomaly''). However, SAA shows the severe false-alarm problem, which falsely detects all the ``\\texttt{\\textcolor[RGB]{207,63,63}{wick}}'' rather than the ground-truth anomaly region (the ``\\texttt{\\textcolor[RGB]{112,173,71}{overlong wick}}''). Thus, we further strengthen the regularization with hybrid prompts in the revamped model (SAA$+$), which successfully helps identify the anomaly region.}\n  \\label{fig:teaser}\n  \\vspace{-1mm}\n\\end{figure}\n\nTechnically, apart from naive class-agnostic prompts, we leverage domain expert knowledge to construct target-oriented anomaly language prompts, \\textit{i.e.}, class-specific language expressions. Besides, as language can not accurately retrieve regions with certain object characteristics, such as number, size, and location, precisely~\\cite{paiss_count_2023,li2022r}, we introduce object property prompts in the form of thresholding filters. These prompts assist in identifying and removing region candidates that do not satisfy desired properties. Furthermore, to fully exploit the target image context, we suggest utilizing image saliency and region confidence ranking as prompts, which model the anomaly degree of a region by considering the similarities, \\textit{e.g.}, euclidean distance, between it and other regions within the image. Finally, we conduct thorough experiments to confirm the efficacy of our hybrid prompts in adapting foundation models to zero-shot anomaly segmentation. Specifically, our final model (SAA$+$) attains new state-of-the-art performance on various anomaly segmentation datasets under the zero-shot setting.\nTo summarize, our main contributions are:\n\\begin{itemize}\n    \\item We propose the SAA framework for anomaly segmentation, allowing the collaborative assembly of diverse foundation models without the need for training.\n    \\item We introduce hybrid prompts as a regularization technique, leveraging domain expert knowledge and target image context to adapt foundation models for anomaly segmentation. This leads to the development of SAA$+$, an enhanced version of our framework.\n    \\item Our method achieves state-of-the-art performance in zero-shot anomaly segmentation on several benchmark datasets, including VisA, MVTec-AD, KSDD2, and MTD. Notably, SAA/SAA$+$ demonstrates remarkable capability in detecting texture-related anomalies without requiring any annotation.\n\\end{itemize}\n\n\\section{Related work}\n\\label{sec:related_work}\n\n\\noindent\\textbf{Anomaly Segmentation. } Due to the limited availability and high cost of abnormal images in industrial settings, much of the current research on anomaly segmentation focuses on unsupervised methods that rely solely on normal images. Reconstruction-based approaches, such as those proposed in \\cite{zhou2020encoding,hou2021divide,zavrtanik2021draem,matsubara2020deep,yan2021learning,jiang2022masked}, score anomalies with \ntrain an encoder-decoder model to reconstruct images for segmentation purposes. By comparing the input image with the reconstructed version, these methods can predict the location of anomalies. Feature embedding-based methods, on the other hand, typically employ teacher-student architecture \\cite{salehi2021multiresolution,wang2021student,deng2022anomaly,cao2022informative, cao2022semikd, wan_unsupervised_2022, wan_position_2022, cao_collaborative_2023}, one-class classification technology \\cite{yi2020patch,massoli2021mocca,sohn2020learning}, or memory-based normal distribution~\\cite{roth2022towards, wan_industrial_2022, cao2023complementary} to segment anomalies by identifying differences in feature distribution between normal and abnormal images. \n\nRecently, researchers have begun to explore the potential of ZSAS~\\cite{nagy2022zero,liu2021zero,rivera2020anomaly,aota2023zero}, which eliminates the need for either normal or abnormal images during the training process. Among them, WinClip~\\cite{jeong2023winclip} pioneers the potential of foundation models, \\textit{e.g.}, visual-language models, for the ZSAL task. Unlike WinClip~\\cite{jeong2023winclip} that segments anomalies through text-visual similarity, we propose to generate proposals and score their anomaly degree, achieving much better segmentation performance.\n\n\\noindent\\textbf{Foundation Model.} \nFoundation models show an impressive ability to solve diverse vision tasks in a zero-shot manner. Specifically, these models can learn a strong representation by training on large-scale datasets ~\\cite{Laion400}. While early work~\\cite{radford2021learning,li2021align} focus on developing robust image-wise recognition capacity, recent work~\\cite{lu2022unified,wang2022unifying, rao2022denseclip,zhong2022regionclip,zhou2021denseclip,liu2023grounding} introduce foundation models or their applications for dense visual tasks. For instance, Grounding DINO \\cite{liu2023grounding} achieves encouraging open-set object detection ability using arbitrary texts as queries. Recently, SAM \\cite{kirillov2023segment} demonstrates a powerful ability to extract high-quality object segmentation masks in the open world. Impressed by the success of these foundation models, we would like to explore how to adapt these off-the-shelf models to detect anomalies without any training on the downstream datasets for anomaly segmentation.\n\n\\noindent\\textbf{Prompt Engineering. } \nPrompt engineering is a widely employed technique that involves adapting foundation models for downstream tasks. Generally, this approach involves appending a set of learnable tokens to the input.  Prior studies have investigated prompting with text inputs~\\cite{zhou_conditional}, vision inputs~\\cite{ju_prompting_2022, jia_visual_2022, bahng_exploring_2022}, and both text and visual inputs~\\cite{zang_unified_2022, shen_multitask_2022, zhou_learning_2022}. Despite their effectiveness in adapting foundation models to various downstream tasks, prompting methods cannot be employed in ZSAS because they require training data, which is not available in ZSAS. In contrast, some methods employ heuristic prompts~\\cite{shtedritski_what_2023} that do not require any training, making them more feasible for tasks without any data. In this paper, we propose using hybrid prompts derived from domain expert knowledge and target image context for ZSAS. \n\n\\begin{figure*}[!t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{figures/framework.png}\n  \\vspace{-0.15in}\n  \\caption{\\textbf{Overview of the proposed Segment Any Anomaly + (SAA+) framework.} We adapt foundation models to zero-shot anomaly segmentation via hybrid prompt regularization. In specific, apart from naive class-agnostic language prompts, the regularization comes from both domain expert knowledge, including more detailed class-specific language and object property prompts, and target image context, including visual saliency and confidence ranking-related prompts.\n  }  \n  \\label{fig:framework}\n  \n\\end{figure*}\n\n\\section{SAA: Vanilla Foundation Model Assembly for ZSAS}\n\n\\subsection{Problem Definition: Zero-shot Anomaly Segmentation (ZSAS)}\n\nThe goal of ZSAS is to perform anomaly segmentation on new objects without requiring any corresponding object training data. ZSAS seeks to create an anomaly map $\\mathbf{A} \\in [0,1]^{h \\times w \\times 1}$ based on an empty training set $\\emptyset$, in order to identify the anomaly degree for individual pixels in an image $\\mathbf{I} \\in \\mathbb{R}^{h \\times w \\times 3}$ that includes novel objects. The ZSAS task has the potential to significantly reduce the need for training data and lower the costs associated with real-world inspection deployments.\n\n\\subsection{Baseline Model Assembly: Segment Any Anomaly (SAA) }\n\nFor ZSAS, we start by constructing a vanilla foundation model assembly, \\textit{i.e.}, Segment Any Anomaly (SAA), as shown in Fig. \\ref{fig:teaser}. In specific, given a certain query image for anomaly segmentation, we first use languages as the initial prompt to roughly retrieve coarse anomaly region proposals via an Anomaly Region Generator implemented with a language-driven visual grounding foundation model, \\textit{i.e.}, GroundingDINO~\\cite{liu2023grounding}. Afterward, anomaly region proposals are refined into pixel-wise high-quality segmentation masks with the Anomaly Region Refiner in which a prompt-driven segmentation foundation model, \\textit{i.e.}, SAM~\\cite{kirillov2023segment}, is used.\n\n\\subsubsection{Anomaly Region Generator}\n\nWith recent booming development on language-vision models, some foundation models~\\cite{clipseg2022,liu2023grounding,zhong2022regionclip} gradually acquire the ability to retrieve objects in images through language prompts. Given language prompts $\\mathcal{T}$ that describe desired regions to be detected, \\textit{e.g.}, ``\\verb|anomaly|'', foundation models can generate desired regions for  a query image $\\mathbf{I}$. There we base the architecture of the region detector on a text-guided open-set object detection architecture for visual grounding. Specifically, we take a GroundingDINO~\\cite{liu2023grounding} architecture that has been pre-trained on large-scale language-vision datasets~\\cite{Laion400}. Such a network first extracts the features of the language prompt and the query image via text encoder and visual encoder, respectively. Then the rough object regions are generated in the form of bounding boxes with a cross-modality decoder. Given the bounding-box-level region set $\\mathcal{R}^B$, and their corresponding confidence score set $\\mathcal{S}$, the module of anomaly region generator ($\\mathrm{Generator}$) can be formulated as,\n\\begin{equation}\n\\label{eq:det}\n    \\mathcal{R}^B, \\mathcal{S} := \\mathrm{Generator}(\\mathbf{I},\\mathcal{T})\n\\end{equation}\n\n\\subsubsection{Anomaly Region Refiner}\n\nTo generate pixel-wise anomaly segmentation results, we propose Anomaly Region Refiner to refine the bounding-box-level anomaly region candidates into an anomaly segmentation mask set. To this end, we use a sophisticated foundation model for open-world visual segmentation, \\textit{i.e.}, SAM~\\cite{kirillov2023segment}. This model mainly includes a ViT-based~\\cite{dosovitskiy2021an} backbone and a prompt-conditioned mask decoder. In specific, the model is trained on a large-scale image segmentation dataset~\\cite{kirillov2023segment} with one billion fine-grained masks, which enables high-quality mask generation abilities under an open-set segmentation setting. The prompt-conditioned mask decoder accepts various types of prompts as input. We regard the bounding box candidates $\\mathcal{R}^B$ as prompts and obtain pixel-wise segmentation masks $\\mathcal{R}$. The module of the Anomaly Region Refiner ($\\mathrm{Refiner}$) can be formulated as follows,\n\\begin{equation}\n    \\mathcal{R} := \\mathrm{Refiner}(\\mathbf{I}, \\mathcal{R}^B)\n\\end{equation}\n\nTill then, we obtain the set of regions in the form of high-quality segmentation masks $\\mathcal{R}$ with corresponding confidence scores $\\mathcal{S}$. To sum up we summarize framework ($\\mathrm{SAA}$) as follows,\n\\begin{equation}\n    \\mathcal{R}, \\mathcal{S} := \\mathrm{SAA}(\\mathbf{I}, \\mathcal{T}_n)\n\\end{equation}\nwhere $\\mathcal{T}_n$ is a naive class-agnostic language prompt, \\textit{e.g.}, ``\\verb|anomaly|'', utilized in SAA. \n\n\\subsection{Analysis on the ZSAS Performance of Vanilla Foundation Model Assembly}\n\nWe present some preliminary experiments to evaluate the efficacy of vanilla foundation model assembly for ZSAS. Despite the simplicity and intuitiveness of the solution, we observe a \\textit{language ambiguity} issue. Specifically, certain language prompts, such as ``anomaly'', may fail to detect the desired anomaly regions. For instance, as depicted in Fig. \\ref{fig:teaser}, all ``\\verb|wick|'' is erroneously identified as an anomaly by the SAA with the ``\\verb|anomaly|'' prompt. \n\nWe attribute this language ambiguity to the domain gap between the pretraining language-vision datasets and the targeted ZSAS datasets, which means that some language prompts may have different meanings and be associated with different image contents in distinct datasets. In addition, there is hardly any adjective expression like ``\\verb|anomaly|'' in those large-scale datasets, thus making this kind of prompt design poor at understanding what is an anomaly region. Additionally, the exact ``\\verb|anomaly|'' is object-specific and would vary across objects. For example, it denotes the scratches on leather or the crack on hazelnut. The language ambiguity issue leads to severe false alarms in ZSAS datasets. We propose introducing hybrid prompts generated by domain expert knowledge and the target image context to reduce language ambiguity, thereby achieving better ZSAS performance. \n\n\\section{SAA+: Foundation Model Adaption via Hybrid Prompt Regularization}\n\\label{sec:method}  \n\nTo address language ambiguity in SAA and improve its ability on ZSAS, we propose an upgraded version called SAA$+$ that incorporates hybrid prompts, as Fig. \\ref{fig:framework}. In addition to leveraging the knowledge gained from pre-trained foundation models, SAA$+$ utilizes both domain expert knowledge and target image context to generate more accurate anomaly region masks. We provide further details on these hybrid prompts below.\n\n\\subsection{Prompt Generated from Domain Expert Knowledge}\n\nFollowing the trend of prompt learning~\\cite{zhou_conditional,zhou_learning_2022}, we initialize the prompt, which unlocks the knowledge of foundation models, in the form of language. However, the language ambiguity issue caused by the domain gap is particularly severe when using only the naive language prompt ``\\verb|anomaly|''. To address this problem, we leverage domain expert knowledge that contains useful prior information about the target anomaly regions. Specifically, although experts may not provide a comprehensive list of potential open-world anomalies for a new product, they can identify some candidates based on their past experiences with similar products. Domain expert knowledge enables us to refine the naive ``\\verb|anomaly|'' prompt into more specific prompts that describe the anomaly state in greater detail. In addition to language prompts, we introduce property prompts to complement the lack of awareness on specific properties like ``\\verb|count|'' and ``\\verb|area|'' ~\\cite{paiss_count_2023} in existing foundation models~\\cite{paiss_count_2023}.\n\n\\subsubsection{Anomaly Language Expression as Prompt}\n\nTo describe potential open-world anomalies, we propose designing more precise language prompts. These prompts are categorized into two types: class-agnostic and class-specific prompts. \n\n\\noindent \\textbf{Class-agnostic prompts ($\\mathcal{T}_{\\rm a}$}) are general prompts that describe anomalies that are not specific to any particular category, \\textit{e.g.}, ``\\verb|anomaly|'' and ``\\verb|defect|''. Despite the domain gap between the pre-trained datasets and the targeted ZSAS datasets, our empirical analysis (\\ref{sec:ablation}) shows that these generic prompts provide  encouraging initial performance.\n\n\\noindent \\textbf{Class-specific prompts ($\\mathcal{T}_{\\rm s}$}) are designed based on expert knowledge of abnormal patterns with similar products to supplement more specific anomaly details.\nWe use prompts already employed in the pre-trained visual-linguistic dataset, \\textit{e.g.}, ``\\verb|black hole|'' and ``\\verb|white bubble|'', to query the desired regions. This approach reformulates the task of finding an anomaly region into locating objects with a specific anomaly state expression, which is more straightforward to utilize foundation models than identifying ``\\verb|anomaly|'' within an object context.\n\nBy prompting SAA with anomaly language prompts $\\mathcal{P}^L=\\{ \\mathcal{T}_{\\rm a}, \\mathcal{T}_ {\\rm s}   \\}$ derived from domain expert knowledge, we generate finer anomaly region candidates $\\mathcal{R}$ and corresponding confidence scores $\\mathcal{S}$. \n\n\\subsubsection{Anomaly Object Property as Prompt}\n\nCurrent foundation models~\\cite{liu2023grounding, li2021grounded} have limitations when it comes to querying objects with specific property descriptions, such as size or location, which are important for describing anomalies, such as ``\\verb|The small black hole on the left of the cable.|'' To incorporate this critical expert knowledge, we propose using anomaly property prompts formulated as rules rather than language. Specifically, we consider the location and area of anomalies.\n\n\\noindent\\textbf{Anomaly Location.} Accurate localization of anomalies plays a critical role in distinguishing true anomalies from false positives. Typically, anomalies are expected to be located within the objects of interest during inference. However, due to the influence of background context, anomalies may occasionally appear outside the inspected objects. To tackle this challenge, we leverage the open-world detection capability of foundation models to determine the location of the inspected object. Subsequently, we calculate the intersection over union (IoU) between the potential anomaly regions and the inspected object. By applying an expert-derived IoU threshold, denoted as $\\theta_{IoU}$, we filter out anomaly candidates with IoU values below this threshold. This process ensures that the retained anomaly candidates are more likely to represent true anomalies located within the inspected object.\n\n\\noindent\\textbf{Anomaly Area.} The size of an anomaly, as reflected by its area, is also a property that can provide useful information. In general, anomalies should be smaller than the size of the inspected object. Experts can provide a suitable threshold value $\\theta_{area}$ for the specific type of anomaly being considered. Candidates with areas unmatched with $\\theta_{area} \\cdot \\mathrm{Object Area}$ can then be filtered out.\n\nBy combining the two property prompts $\\mathcal{P}^P=\\{ \\theta_{area}, \\theta_{IoU} \\}$, we can filter the set of candidate regions $\\mathcal{R}$ to obtain a subset of selected candidates $\\mathcal{R}^P$ with corresponding confidence scores $\\mathcal{S}^P$ using the filter function ($\\mathrm{Filter}$),\n\\begin{equation}\n    \\mathcal{R}^P, \\mathcal{S}^P := \\mathrm{Filter}(\\mathcal{R}, \\mathcal{P}^P)\n\\end{equation}\n\n\\subsection{Prompts Derived from Target Image Context}\n\nBesides incorporating domain expert knowledge, we can leverage the information provided by the input image itself to improve the accuracy of anomaly region detection. In this regard, we propose two prompts induced by the image context.\n\n\\subsubsection{Anomaly Saliency as Prompt}\nPredictions generated by foundation models like~\\cite{liu2023grounding} using the prompt ``\\verb|defect|'' can be unreliable due to the domain gap between pre-trained language-vision datasets~\\cite{Laion400} and targeted anomaly segmentation datasets~\\cite{bergmann2019mvtec,zou2022spot}. To calibrate the confidence scores of individual predictions, we propose Anomaly Saliency Prompt mimicking human intuition. In specific, humans can recognize anomaly regions by their discrepancy with their surrounding regions \\cite{aota2023zero}, \\textit{i.e.}, visual saliency contains valuable information indicating the anomaly degree. Hence, we calculate a saliency map ($\\mathbf{s}$) for the input image by computing the average distances between the corresponding pixel feature ($\\mathbf{f}$) and its $N$ nearest neighbors, \n\\begin{equation}\n\\label{eq:saliency-map}\n    \\mathbf{s}_{ij} := \\frac{1}{N}\\sum\\limits_{\\mathbf{f}\\in N_p(\\mathbf{f}_{ij})}(1-  \\langle \\mathbf{f}_{ij},\\mathbf{f}  \\rangle)\n\\end{equation}\nwhere $(i,j)$ denotes to the pixel location, $N_p(\\mathbf{f}_{ij})$ denotes to the $N$ nearest neighbors of the corresponding pixel, and $\\langle \\cdot, \\cdot \\rangle$ refers to the cosine similarity. We use pre-trained CNNs from large-scale image datasets~\\cite{hinton2012imagenet} to extract image features, ensuring the descriptiveness of features. The saliency map indicates how different a region is from other regions. The saliency prompts $\\mathcal{P}^{S}$ are defined as the exponential average saliency value within the corresponding region masks, \n\\begin{equation}\n\\label{eq:score_saliency}\n    \\mathcal{P}^{S} := \\left\\{ \\exp(\\frac{\\sum_{i j}\\mathbf{r}_{i j}\\mathbf{s}_{i j}}{\\sum_{i j}\\mathbf{r}_{i j}}) \\quad | \\quad \\mathbf{r} \\in \\mathcal{R}^P \\right\\}\n\\end{equation}\n\nThe saliency prompts provide reliable indications of the confidence of anomaly regions. These prompts are employed to recalibrate the confidence scores generated by the foundation models, yielding new rescaled scores $\\mathcal{S}^{S}$ based on the anomaly saliency prompts $\\mathcal{P}^{S}$. These rescaled scores provide a combined measure that takes into account both the confidence derived from the foundation models and the saliency of the region candidate. The process is formulated as follows,\n\\begin{equation}\n\\label{eq:rescore}\n    \\mathcal{S}^{S} := \\left\\{ p \\cdot s  \\quad | \\quad   p \\in \\mathcal{P}^{S},   s \\in \\mathcal{S}^P \\right\\}\n\\end{equation}\n\n\\subsubsection{Anomaly Confidence as Prompt}\n\nTypically, the number of anomaly regions in an inspected object is limited. Therefore, we propose anomaly confidence prompts $\\mathcal{P}^C$ to identify the $K$ candidates with the highest confidence scores based on the image content and use their average values for final anomaly region detection. This is achieved by selecting the top $K$ candidate regions based on their corresponding confidence scores, as shown in the following,\n\\begin{equation}\n\\label{eq:confidence-prompts}\n    \\mathcal{R}^C, \\mathcal{S}^C := \\mathrm{Top}_K(\\mathcal{R}^P,\\mathcal{S}^S)\n\\end{equation}\n\nDenote a single region and its corresponding score as $\\mathbf{r}^C$ and $s^C$, we then use these $K$ candidate regions to estimate the final anomaly map, \n\\begin{equation}\n\\label{eq:fusion}\n    \\mathbf{A}_{ij} := \\frac{ \\sum_{\\mathbf{r}^C \\in \\mathcal{R}^C}\n    \\mathbf{r}^C_{i j} \\cdot s^C}{\n    \\sum_{\\mathbf{r}^C \\in \\mathcal{R}^C}\n    \\mathbf{r}^C_{i j}}   \n\\end{equation}\n\nWith the proposed hybrid prompts ($ \\mathcal{P}^L, \\mathcal{P}^P,\\mathcal{P}^S$, and $\\mathcal{P}^C$), SAA is regularized in our final framework, \\textit{i.e.}, Segment Any Anomaly $+$ (SAA$+$), which makes more reliable anomaly predictions.\n\n\\section{Experiments}\n\\label{sec:experiment}\nIn this section, we first assess the performance of SAA/SAA$+$ on several anomaly segmentation benchmarks. Then, we extensively study the effectiveness of individual hybrid prompts.\n\n\\subsection{Experimental Setup}\n\n\\noindent\\textbf{Datasets.} We leverage four datasets with pixel-level annotations.: VisA~\\cite{zou2022spot}, MVTec-AD~\\cite{bergmann2019mvtec}, KSDD2~\\cite{bozic_mixed_2021}, and MTD~\\cite{mtd2018}. VisA and MVTec-AD comprise a variety of object subsets, \\textit{e.g.}, circuit boards, while KSDD2 and MTD are comprised of texture anomalies. \nIn summary, we categorize the subsets of all of these datasets into \\textit{texture} which typically exhibit similar patterns within a single image (\\textit{e.g.}, carpets), and \\textit{object}  which includes more diverse distribution (\\textit{e.g.}, candles). \n\n\\noindent\\textbf{Evaluation Metrics.} ZSAS performance is evaluated based on two metrics: \n\\noindent (I) \\textbf{max-F1-pixel} ($\\mathcal{F}_{p}$) ~\\cite{jeong2023winclip}, which measures the F1-score for pixel-wise segmentation at the optimal threshold; \n\\noindent (II) \\textbf{max-F1-region} ($\\mathcal{F}_{r}$), which is proposed in this paper to mitigate the bias towards large defects observed with max-F1-pixel~\\cite{bergmann2019mvtec}. Specifically, we compute the F1-score for region-wise segmentation at the optimal threshold, considering a prediction positive if the overlapping value exceeds $0.6$.\n\n\\vspace{0.05in}\n\n\\noindent\\textbf{Implementation Details.} We adopt the official implementations of GroundingDINO\\footnote{\\url{https://github.com/IDEA-Research/GroundingDINO}} and Segment Anything Model\\footnote{\\url{https://github.com/facebookresearch/segment-anything}} to construct the vanilla baseline (SAA). Details about the prompts derived from domain expert knowledge are explained in the supplementary material. For the saliency prompts induced from image content, we utilize the WideResNet50~\\cite{zagoruyko2016wideresnet} network, pre-trained on ImageNet~\\cite{hinton2012imagenet}, and set $N=400$ in line with prior studies~\\cite{aota2023zero}. For anomaly confidence prompts, we set the hyperparameter $K$ as $5$ by default. Input images are fixed at a resolution of $400 \\times 400$ for evaluation.\n\n\\subsection{Main Results}\n\\label{sec:exp_main}\n\n\\noindent\\textbf{Methods for Comparison.}  We compare our final model, \\textit{i.e.}, Segment Any Anomaly + (SAA$+$) with several concurrent state-of-the-art methods, including WinClip~\\cite{jeong2023winclip}, UTAD~\\cite{aota2023zero}, ClipSeg~\\cite{clipseg2022}, and our vanilla baseline (SAA). For WinClip, we report its official results on VisA and MVTec-AD. For the other three methods, we use official implementations and adapt them to the ZSAS task. Notably, as all methods require no training process, their performance is stable with a variance of $\\pm0.00$. \n\n\\begin{table}[t]\n\\centering\n\\caption{Qualitative comparisons between SAA$+$ and other concurrent methods on zero-shot anomaly segmentation. Best scores are highlighted in \\textbf{bold}. The second best scores are also {\\ul{underlined}}.}\n\\label{tab:exp_main}\n\\setlength{\\tabcolsep}{10.pt}\n\\resizebox{1\\textwidth}{!}{\n\\begin{tabular}{c|l|cccc|cc|c} %@{} @{}\n\\toprule\n\\multirow{2}{*}{Metric} &\n  \\multirow{2}{*}{Method} &\n  \\multicolumn{4}{c|}{Per Dataset} &\n  \\multicolumn{2}{c|}{Per Defect   Type} &\n  \\multirow{2}{*}{Total} \\\\ \\cmidrule(lr){3-6} \\cmidrule(lr){7-8}\n &\n   &\n  \\multicolumn{1}{c|}{VisA} &\n  \\multicolumn{1}{c|}{MVTec-AD} &\n  \\multicolumn{1}{c|}{KSDD2} &\n  MTD &\n  \\multicolumn{1}{c|}{Texture} &\n  Object &\n   \\\\ \\midrule\n\\multirow{3}[10]{*}{$\\mathcal{F}_{p}$} &\n  WinClip~\\cite{jeong2023winclip} &\n  \\multicolumn{1}{c|}{{\\ul {14.82}}} &\n  \\multicolumn{1}{c|}{{\\ul {31.65}}} &\n  \\multicolumn{1}{c|}{-} &\n  - &\n  \\multicolumn{1}{c|}{-} &\n  {\\ul {20.93}} &\n  - \\\\ %\\cmidrule(l){2-9} \n &\n  ClipSeg~\\cite{clipseg2022} &\n  \\multicolumn{1}{c|}{14.32} &\n  \\multicolumn{1}{c|}{25.42} &\n  \\multicolumn{1}{c|}{{\\ul {34.27}}} &\n  9.39 &\n  \\multicolumn{1}{c|}{27.75} &\n  18.30 &\n  {\\ul {20.58}} \\\\ %\\cmidrule(l){2-9} \n &\n  UTAD~\\cite{aota2023zero} &\n  \\multicolumn{1}{c|}{6.95} &\n  \\multicolumn{1}{c|}{23.48} &\n  \\multicolumn{1}{c|}{22.53} &\n  11.37 &\n  \\multicolumn{1}{c|}{{\\ul {29.13}}} &\n  12.07 &\n  16.19 \\\\ \\cmidrule(l){2-9} \n &\n  SAA &\n  \\multicolumn{1}{c|}{12.76} &\n  \\multicolumn{1}{c|}{23.44} &\n  \\multicolumn{1}{c|}{8.79} &\n  {\\ul {14.78}} &\n  \\multicolumn{1}{c|}{20.94} &\n  17.35 &\n  18.22 \\\\ %\\cmidrule(l){2-9} \n &\n  SAA$+$ &\n  \\multicolumn{1}{c|}{\\textbf{27.07}} &\n  \\multicolumn{1}{c|}{\\textbf{39.40}} &\n  \\multicolumn{1}{c|}{\\textbf{59.19}} &\n  \\textbf{35.40} &\n  \\multicolumn{1}{c|}{\\textbf{53.79}} &\n  \\textbf{28.82} &\n  \\textbf{34.85} \\\\ \\midrule\\midrule\n\\multirow{3}[10]{*}{$\\mathcal{F}_{r}$} &\n  ClipSeg~\\cite{clipseg2022} &\n  \\multicolumn{1}{c|}{{\\ul {5.65}}} &\n  \\multicolumn{1}{c|}{19.68} &\n  \\multicolumn{1}{c|}{9.05} &\n  6.55 &\n  \\multicolumn{1}{c|}{21.37} &\n  10.41 &\n  13.06 \\\\ % \\cmidrule(l){2-9} \n &\n  UTAD~\\cite{aota2023zero} &\n  \\multicolumn{1}{c|}{5.32} &\n  \\multicolumn{1}{c|}{17.53} &\n  \\multicolumn{1}{c|}{3.56} &\n  2.95 &\n  \\multicolumn{1}{c|}{16.38} &\n  9.94 &\n  11.49 \\\\ \\cmidrule(l){2-9} \n &\n  SAA &\n  \\multicolumn{1}{c|}{4.83} &\n  \\multicolumn{1}{c|}{{\\ul {32.49}}} &\n  \\multicolumn{1}{c|}{{\\ul {16.40}}} &\n  {\\ul {10.63}} &\n  \\multicolumn{1}{c|}{{\\ul {40.31}}} &\n  {\\ul {13.19}} &\n  {\\ul {19.74}} \\\\% \\cmidrule(l){2-9} \n &\n  SAA$+$ &\n  \\multicolumn{1}{c|}{\\textbf{14.46}} &\n  \\multicolumn{1}{c|}{\\textbf{49.67}} &\n  \\multicolumn{1}{c|}{\\textbf{39.34}} &\n  \\textbf{30.27} &\n  \\multicolumn{1}{c|}{\\textbf{60.40}} &\n  \\textbf{25.70} &\n  \\textbf{34.07} \\\\ \\bottomrule\n\\end{tabular}}\n\\end{table}\n\\noindent\\textbf{Quantitative Results}: As is shown in Table \\ref{tab:exp_main}, SAA$+$ method outperforms other methods in both $\\mathcal{F}_{p}$ and $\\mathcal{F}_{r}$ by a significant margin. Although WinClip~\\cite{jeong2023winclip}, ClipSeg~\\cite{clipseg2022}, and SAA also use foundation models, SAA$+$ better unleash the capacity of foundation models and adapts them to tackle ZSAS. The remarkable performance of SAA$+$ meets the expectation to segment any anomaly without training.\n\n\\noindent\\textbf{Qualitative Results}: Fig. \\ref{fig:qualitative} presents qualitative comparisons between  SAA$+$ and previous competitive methods, where SAA$+$ achieves better performance. Moreover, the visualization shows SAA$+$ is capable of detecting texture anomalies, \\textit{e.g.} small scratches on the leather. \n\n\\begin{figure*}[t]\n\\vspace{-2mm}\n    \\centering\n        \\includegraphics[width=\\linewidth]{figures/exp_qualitative_results.png}\n    \\vspace{-4mm}\\caption{Qualitative comparisons on zero-shot anomaly segmentation  for ClipSeg~\\cite{clipseg2022}, UTAD~\\cite{aota2023zero}, SAA, and SAA$+$ on four datasets, \\textit{i.e.}, VisA~\\cite{zou2022spot}, MVTec-AD~\\cite{bergmann2019mvtec}, KSDD2~\\cite{bozic_mixed_2021}, and MTD~\\cite{mtd2018}}\n    \\label{fig:qualitative}\n\\end{figure*}\n\\begin{figtab}\n\\begin{minipage}{0.48\\linewidth}\n    \\centering\n    \\tabcaption{Ablation study on the proposed hybrid prompts, including language prompt ($\\mathcal{P}^{L}$), object property prompt ($\\mathcal{P}^{P}$), saliency prompt ($\\mathcal{P}^{S}$), and confidence prompt ($\\mathcal{P}^{C}$). The best scores are highlighted in \\textbf{bold}.}\n    \\vspace{4.4mm}\n    \\resizebox{\\textwidth}{!}{\n    \\setlength{\\tabcolsep}{1pt}\n\\begin{tabular}{ccllll}\n\\toprule\n\\multicolumn{1}{c}{Metric} & \\multicolumn{2}{c}{Model Variants}                & Texture        & Object         & Total          \\\\ \\midrule\n\\multirow{7}[10]{*}{$\\mathcal{F}_p$}  & \\multirow{3}{*}{w/o $\\mathcal{P}^L$} & w/o $\\mathcal{T}_{\\rm a}$ \\& $\\mathcal{T}_{\\rm s}$ & 50.30          & 24.79          & 30.95          \\\\\n                     &                         & w/o $\\mathcal{T}_{\\rm a}$       & 51.15          & 25.88          & 31.80          \\\\\n                     &                         & w/o $\\mathcal{T}_{\\rm s}$       & 53.51          & 26.55          & 33.06          \\\\ \\cmidrule(){2-6}\n                     & \\multicolumn{2}{c}{w/o $\\mathcal{P}^P$}             & 21.83         & 21.40          & 21.50          \\\\ \\cmidrule(){2-6}\n                     & \\multicolumn{2}{c}{w/o $\\mathcal{P}^S$}             & 50.58          & 24.72          & 30.96          \\\\ \\cmidrule(){2-6}\n                     & \\multicolumn{2}{c}{w/o $\\mathcal{P}^C$}             & 50.41          & 27.99          & 34.13          \\\\ \\cmidrule(){2-6}\n                     & \\multicolumn{2}{c}{full model (\\textbf{SAA+})}               & \\textbf{53.79} & \\textbf{28.82} & \\textbf{34.85} \\\\ \\midrule \\midrule\n\\multirow{7}[10]{*}{$\\mathcal{F}_r$}  & \\multirow{3}{*}{w/o $\\mathcal{P}^L$} & w/o $\\mathcal{T}_{\\rm a}$ \\& $\\mathcal{T}_{\\rm s}$ & 50.58          & 22.36          & 29.17          \\\\\n                     &                         & w/o $\\mathcal{T}_{\\rm a}$       & 55.26          & 20.28          & 28.72          \\\\\n                     &                         & w/o $\\mathcal{T}_{\\rm s}$       & 54.21          & 23.13          & 30.64          \\\\ \\cmidrule(){2-6}\n                     & \\multicolumn{2}{c}{w/o $\\mathcal{P}^P$}             & 33.94          & 20.99          & 24.11          \\\\ \\cmidrule(){2-6}\n                     & \\multicolumn{2}{c}{w/o $\\mathcal{P}^S$}             & 57.66          & 24.36          & 32.39          \\\\ \\cmidrule(){2-6}\n                     & \\multicolumn{2}{c}{w/o $\\mathcal{P}^C$}             & 53.65          & 25.18          & 32.05          \\\\ \\cmidrule(){2-6}\n                     & \\multicolumn{2}{c}{full model (\\textbf{SAA+})}               & \\textbf{60.40} & \\textbf{25.70} & \\textbf{34.07} \\\\ \\bottomrule\n\\end{tabular}}\\label{tab:ablation}\n  \\end{minipage}\n  \\hfill\n  \\begin{minipage}[t]{0.48\\linewidth}\n      \\vspace{-50mm}\n    \\centering\n    \\includegraphics[width = \\linewidth]{figures/exp_saliency_map.png}\n    \\vspace{-6mm}\n    \\figcaption{Effects of disabling ($w/o$) and abling ($w/$)  prompts ($\\mathcal{P}^{S}$) of saliency maps ($\\mathbf{s}$) on the final anomaly segmentation.}\n  \\label{fig:influence_PS} \n  \\vspace{2mm}\n    \\centering\n    \\includegraphics[width = 0.8\\linewidth]{figures/exp_analysis_k.png}\n        \\vspace{-1mm}\n    \\figcaption{Sensitivity analysis of hyperparameter $K$ of confidence prompts ($\\mathcal{P}^C$).}\n  \\label{fig:influence_K}\n  \\end{minipage}%\\quad\n  \\vspace{-2mm}\n\\end{figtab}\n\n\\subsection{Ablation study}\n\\label{sec:ablation}\nIn Table \\ref{tab:ablation}, we perform component-wise analysis to ablate specific prompt designs in our framework.\n\n\\noindent\\textbf{Language prompt ($\\mathcal{P}^L$).} Table \\ref{tab:ablation} verifies the effectiveness of language prompts derived from domain expert knowledge (+3.90\\% in $\\mathcal{F}_p$ and +4.90\\% in $\\mathcal{F}_r$). Then, we dig into the efficacy of $\\mathcal{T}_{\\rm a}$ and $\\mathcal{T}_ {\\rm s}$, which clearly indicate that both the general description and the specifically designed description for anomalies can achieve reasonable performance. Moreover, their combination can make a synergy, enhancing anomaly segmentation performance. The improvement of $\\mathcal{P}^L$ helps unlock language-driven region detection capacity of current foundation models~\\cite{liu2023grounding,kirillov2023segment}. \n\n\\noindent\\textbf{Property prompt ($\\mathcal{P}^P$).} Apart from the improvement in the overall performance, property prompts bring dramatic improvements (from $21.83\\%$ to $53.79\\%$ in $\\mathcal{F}_p$) on \\textit{texture} categories, thanks to the filtering mechanism which filters out a significant number of falsely detected anomaly region candidates via high-level characteristics, \\textit{e.g.}, location and area of the target image. \n\n\\noindent\\textbf{Saliency prompt ($\\mathcal{P}^S$).} Table \\ref{tab:ablation} provides clear evidence of the efficacy of $\\mathcal{P}^S$ on anomaly segmentation. This is because region saliencies can accurately describe the degree of deviation of a region from its surroundings. In Fig. \\ref{fig:influence_PS}, we showcase the qualitative impact of $\\mathcal{P}^S$ on anomaly segmentation, which illustrates visual saliency maps can help highlight abnormal regions, \\textit{i.e.}, which shows higher saliency values compared to other regions. By incorporating $\\mathcal{P}^S$ to calibrate the confidence scores, more precise segmentation results can be achieved. For example, the use of $\\mathcal{P}^S$ enables the effective localization of the cracked region of hazelnut and the overlong wick on candles. \n\n\\noindent\\textbf{Confidence prompt ($\\mathcal{P}^C$).} \n With the incorporation of anomaly confidence prompts, we limit the number of anomaly regions, which effectively reduces false positives, leading to $0.72\\%$ $\\mathcal{F}_p$ average improvements across all categories, as shown in Table \\ref{tab:ablation}.\nThe influence of the hyperparameter $K$ in $\\mathcal{P}^C$ is illustrated in Fig. \\ref{fig:influence_K}. The figure shows that performance initially increases as $K$ improves, as more anomaly regions are accurately detected. However, when $K$ exceeds a certain threshold (around $K=5$), the performance drops slightly as more regions are wrongly identified as abnormal. The best results are obtained at around $K=5$, with an average $\\mathcal{F}_{p}$ of $34.85\\%$  across all categories.\n\n\\section{Conclusion}\n\nIn this work, we explore how to \\textit{segment any anomaly} without any further training by unleashing the full power of modern foundation models. We owe the struggle of adapting foundation model assembly to anomaly segmentation to the prompt design, which is the key to controlling the function of off-the-shelf foundation models. Thus, we propose a novel framework, \\textit{i.e.}, Segment Any Anomaly $+$, to leverage hybrid prompts derived from both expert knowledge and target image context to regularize foundation models free of training. Finally, we successfully adapt multiple foundation models to tackle zero-shot anomaly segmentation, achieving new SoTA results on several benchmarks. We hope our work can shed light on the design of label-free model adaptation for anomaly segmentation.\n\n\\textbf{Limitations.} Due to the computation restriction, we currently do not test our method on more large-scale foundation models. We have finished the exploration of our methodology with representative foundation models, and we will explore the scaling effect of the models in the future.\n\n{\\small\n\n\\begin{thebibliography}{10}\n\n\\bibitem{cao_collaborative_2023}\nYunkang Cao, Xiaohao Xu, Zhaoge Liu, and Weiming Shen.\n\\newblock Collaborative discrepancy optimization for reliable image anomaly\n  localization.\n\\newblock {\\em IEEE Transactions on Industrial Informatics}, pages 1--10, 2023.\n\n\\bibitem{wan_industrial_2022}\nQian Wan, Liang Gao, Xinyu Li, and Long Wen.\n\\newblock Industrial image anomaly localization based on gaussian clustering of\n  pretrained feature.\n\\newblock {\\em IEEE Transactions on Industrial Electronics}, 69(6):6182--6192,\n  2021.\n\n\\bibitem{roth2022towards}\nKarsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Sch{\\\"o}lkopf, Thomas\n  Brox, and Peter Gehler.\n\\newblock Towards total recall in industrial anomaly detection.\n\\newblock In {\\em Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition}, pages 14318--14328, 2022.\n\n\\bibitem{bergmann2019mvtec}\nPaul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger.\n\\newblock {MVTec AD} -- {A} comprehensive real-world dataset for unsupervised\n  anomaly detection.\n\\newblock In {\\em Proceedings of the IEEE/CVF conference on Computer Vision and\n  Pattern Recognition}, pages 9592--9600, 2019.\n\n\\bibitem{bergmann2020uninformed}\nPaul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger.\n\\newblock Uninformed students: Student-teacher anomaly detection with\n  discriminative latent embeddings.\n\\newblock In {\\em Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition}, pages 4183--4192, 2020.\n\n\\bibitem{baur_autoencoders_2021}\nChristoph Baur, Stefan Denner, Benedikt Wiestler, Nassir Navab, and Shadi\n  Albarqouni.\n\\newblock Autoencoders for unsupervised anomaly segmentation in brain mr\n  images: a comparative study.\n\\newblock {\\em Medical Image Analysis}, 69:101952, 2021.\n\n\\bibitem{zhou2020encoding}\nKang Zhou, Yuting Xiao, Jianlong Yang, Jun Cheng, Wen Liu, Weixin Luo, Zaiwang\n  Gu, Jiang Liu, and Shenghua Gao.\n\\newblock Encoding structure-texture relation with p-net for anomaly detection\n  in retinal images.\n\\newblock In {\\em Computer Vision--ECCV 2020: 16th European Conference,\n  Glasgow, UK, August 23--28, 2020, Proceedings, Part XX 16}, pages 360--377.\n  Springer, 2020.\n\n\\bibitem{hou2021divide}\nJinlei Hou, Yingying Zhang, Qiaoyong Zhong, Di~Xie, Shiliang Pu, and Hong Zhou.\n\\newblock Divide-and-assemble: Learning block-wise memory for unsupervised\n  anomaly detection.\n\\newblock In {\\em Proceedings of the IEEE/CVF International Conference on\n  Computer Vision}, pages 8791--8800, 2021.\n\n\\bibitem{zavrtanik2021draem}\nVitjan Zavrtanik, Matej Kristan, and Danijel Sko{\\v{c}}aj.\n\\newblock {DRAEM} -- {A} discriminatively trained reconstruction embedding for\n  surface anomaly detection.\n\\newblock In {\\em Proceedings of the IEEE/CVF International Conference on\n  Computer Vision}, pages 8330--8339, 2021.\n\n\\bibitem{matsubara2020deep}\nTakashi Matsubara, Kazuki Sato, Kenta Hama, Ryosuke Tachibana, and Kuniaki\n  Uehara.\n\\newblock Deep generative model using unregularized score for anomaly detection\n  with heterogeneous complexity.\n\\newblock {\\em IEEE Transactions on Cybernetics}, 52(6):5161--5173, 2020.\n\n\\bibitem{yan2021learning}\nXudong Yan, Huaidong Zhang, Xuemiao Xu, Xiaowei Hu, and Pheng-Ann Heng.\n\\newblock Learning semantic context from normal samples for unsupervised\n  anomaly detection.\n\\newblock In {\\em Proceedings of the AAAI Conference on Artificial\n  Intelligence}, volume~35, pages 3110--3118, 2021.\n\n\\bibitem{jiang2022masked}\nJielin Jiang, Jiale Zhu, Muhammad Bilal, Yan Cui, Neeraj Kumar, Ruihan Dou,\n  Feng Su, and Xiaolong Xu.\n\\newblock Masked swin transformer unet for industrial anomaly detection.\n\\newblock {\\em IEEE Transactions on Industrial Informatics}, 19(2):2200--2209,\n  2022.\n\n\\bibitem{yi2020patch}\nJihun Yi and Sungroh Yoon.\n\\newblock Patch {SVDD}: Patch-level {SVDD} for anomaly detection and\n  segmentation.\n\\newblock In {\\em Proceedings of the Asian Conference on Computer Vision},\n  2020.\n\n\\bibitem{massoli2021mocca}\nFabio~Valerio Massoli, Fabrizio Falchi, Alperen Kantarci, {\\c{S}}eymanur Akti,\n  Hazim~Kemal Ekenel, and Giuseppe Amato.\n\\newblock Mocca: Multilayer one-class classification for anomaly detection.\n\\newblock {\\em IEEE Transactions on Neural Networks and Learning Systems},\n  33(6):2313--2323, 2021.\n\n\\bibitem{sohn2020learning}\nKihyuk Sohn, Chun-Liang Li, Jinsung Yoon, Minho Jin, and Tomas Pfister.\n\\newblock Learning and evaluating representations for deep one-class\n  classification.\n\\newblock In {\\em International Conference on Learning Representations}, 2020.\n\n\\bibitem{cao2023complementary}\nYunkang Cao, Xiaohao Xu, and Weiming Shen.\n\\newblock Complementary pseudo multimodal feature for point cloud anomaly\n  detection.\n\\newblock {\\em arXiv preprint arXiv:2303.13194}, 2023.\n\n\\bibitem{wang_multimodal_nodate}\nYue Wang, Jinlong Peng, Jiangning Zhang, Ran Yi, Yabiao Wang, and Chengjie\n  Wang.\n\\newblock Multimodal industrial anomaly detection via hybrid fusion.\n\\newblock In {\\em 2023 {IEEE}/{CVF} Conference on Computer Vision and Pattern\n  Recognition ({CVPR})}, 2023.\n\n\\bibitem{jiang_softpatch_2022}\nXi~Jiang, Jianlin Liu, Jinbao Wang, Qiang Nie, Kai Wu, Yong Liu, Chengjie Wang,\n  and Feng Zheng.\n\\newblock {SoftPatch}: Unsupervised anomaly detection with noisy data.\n\\newblock In {\\em Advances in neural information processing systems}, 2022.\n\n\\bibitem{kirillov2023segment}\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura\n  Gustafson, Tete Xiao, Spencer Whitehead, Alexander~C Berg, Wan-Yen Lo, et~al.\n\\newblock Segment anything.\n\\newblock {\\em arXiv preprint arXiv:2304.02643}, 2023.\n\n\\bibitem{radford2021learning}\nAlec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\n  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\n  et~al.\n\\newblock Learning transferable visual models from natural language\n  supervision.\n\\newblock In {\\em International Conference on Machine Learning}, pages\n  8748--8763. PMLR, 2021.\n\n\\bibitem{li2022align}\nDongxu Li, Junnan Li, Hongdong Li, Juan~Carlos Niebles, and Steven~CH Hoi.\n\\newblock Align and prompt: Video-and-language pre-training with entity\n  prompts.\n\\newblock In {\\em Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition}, pages 4953--4963, 2022.\n\n\\bibitem{bommasani2021opportunities}\nRishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney\n  von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma\n  Brunskill, et~al.\n\\newblock On the opportunities and risks of foundation models.\n\\newblock {\\em arXiv preprint arXiv:2108.07258}, 2021.\n\n\\bibitem{liu2023grounding}\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan\n  Li, Jianwei Yang, Hang Su, Jun Zhu, et~al.\n\\newblock Grounding dino: Marrying dino with grounded pre-training for open-set\n  object detection.\n\\newblock {\\em arXiv preprint arXiv:2303.05499}, 2023.\n\n\\bibitem{clipseg2022}\nTimo L{\\\"u}ddecke and Alexander Ecker.\n\\newblock Image segmentation using text and image prompts.\n\\newblock In {\\em Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition}, pages 7086--7096, 2022.\n\n\\bibitem{jeong2023winclip}\nJongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, and\n  Onkar Dabeer.\n\\newblock Winclip: Zero-/few-shot anomaly classification and segmentation.\n\\newblock {\\em arXiv preprint arXiv:2303.14814}, 2023.\n\n\\bibitem{object_calibration}\nXiaohao Xu, Jinglu Wang, Xiang Ming, and Yan Lu.\n\\newblock Towards robust video object segmentation with adaptive object\n  calibration.\n\\newblock In {\\em Proceedings of the 30th ACM International Conference on\n  Multimedia}, pages 1--10, 2022.\n\n\\bibitem{xu2022reliable}\nXiaohao Xu, Jinglu Wang, Xiao Li, and Yan Lu.\n\\newblock Reliable propagation-correction modulation for video object\n  segmentation.\n\\newblock In {\\em Proceedings of the AAAI Conference on Artificial\n  Intelligence}, pages 2946--2954, 2022.\n\n\\bibitem{paiss_count_2023}\nRoni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani,\n  and Tali Dekel.\n\\newblock Teaching clip to count to ten.\n\\newblock {\\em arXiv preprint arXiv:2302.12066}, 2023.\n\n\\bibitem{li2022r}\nXiang Li, Jinglu Wang, Xiaohao Xu, Xiao Li, Yan Lu, and Bhiksha Raj.\n\\newblock R\\^{} 2vos: Robust referring video object segmentation via relational\n  multimodal cycle consistency.\n\\newblock {\\em arXiv preprint arXiv:2207.01203}, 2022.\n\n\\bibitem{salehi2021multiresolution}\nMohammadreza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad~H Rohban,\n  and Hamid~R Rabiee.\n\\newblock Multiresolution knowledge distillation for anomaly detection.\n\\newblock In {\\em Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition}, pages 14902--14912, 2021.\n\n\\bibitem{wang2021student}\nGuodong Wang, Shumin Han, Errui Ding, and Di~Huang.\n\\newblock Student-teacher feature pyramid matching for anomaly detection.\n\\newblock {\\em arXiv preprint arXiv:2103.04257}, 2021.\n\n\\bibitem{deng2022anomaly}\nHanqiu Deng and Xingyu Li.\n\\newblock Anomaly detection via reverse distillation from one-class embedding.\n\\newblock In {\\em Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition}, pages 9737--9746, 2022.\n\n\\bibitem{cao2022informative}\nYunkang Cao, Qian Wan, Weiming Shen, and Liang Gao.\n\\newblock Informative knowledge distillation for image anomaly segmentation.\n\\newblock {\\em Knowledge-Based Systems}, 248:108846, 2022.\n\n\\bibitem{cao2022semikd}\nYunkang Cao, Yanan Song, Xiaohao Xu, Shuya Li, Yuhao Yu, Yiheng Zhang, and\n  Weiming Shen.\n\\newblock Semi-supervised knowledge distillation for tiny defect detection.\n\\newblock In {\\em 2022 IEEE 25th International Conference on Computer Supported\n  Cooperative Work in Design (CSCWD)}, pages 1010--1015, 2022.\n\n\\bibitem{wan_unsupervised_2022}\nQian Wan, Liang Gao, Xinyu Li, and Long Wen.\n\\newblock Unsupervised image anomaly detection and segmentation based on\n  pre-trained feature mapping.\n\\newblock {\\em IEEE Transactions on Industrial Informatics}, 2022.\n\n\\bibitem{wan_position_2022}\nQian Wan, Yunkang Cao, Liang Gao, Weiming Shen, and Xinyu Li.\n\\newblock Position encoding enhanced feature mapping for image anomaly\n  detection.\n\\newblock In {\\em 2022 {IEEE} 18th International Conference on Automation\n  Science and Engineering ({CASE})}, pages 876--881. {IEEE}, 2022.\n\n\\bibitem{nagy2022zero}\nAmr~M Nagy and L{\\'a}szl{\\'o} Cz{\\'u}ni.\n\\newblock Zero-shot learning and classification of steel surface defects.\n\\newblock In {\\em Fourteenth International Conference on Machine Vision (ICMV\n  2021)}, volume 12084, pages 386--394. SPIE, 2022.\n\n\\bibitem{liu2021zero}\nJiahui Liu, Xiaojuan Qi, Songzhi Su, Tony Prescott, and Li~Sun.\n\\newblock Zero-shot anomalous object detection using unsupervised metric\n  learning.\n\\newblock In {\\em 2021 IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS 2021) Proceedings}. Sheffield, 2021.\n\n\\bibitem{rivera2020anomaly}\nAd{\\'\\i}n~Ram{\\'\\i}rez Rivera, Adil Khan, Imad Eddine~Ibrahim Bekkouch, and\n  Taimoor~Shakeel Sheikh.\n\\newblock Anomaly detection based on zero-shot outlier synthesis and\n  hierarchical feature distillation.\n\\newblock {\\em IEEE Transactions on Neural Networks and Learning Systems},\n  33(1):281--291, 2020.\n\n\\bibitem{aota2023zero}\nToshimichi Aota, Lloyd Teh~Tzer Tong, and Takayuki Okatani.\n\\newblock Zero-shot versus many-shot: Unsupervised texture anomaly detection.\n\\newblock In {\\em Proceedings of the IEEE/CVF Winter Conference on Applications\n  of Computer Vision}, pages 5564--5572, 2023.\n\n\\bibitem{Laion400}\nChristoph Schuhmann, Robert Kaczmarczyk, Aran Komatsuzaki, Aarush Katta,\n  Richard Vencu, Romain Beaumont, Jenia Jitsev, Theo Coombes, and Clayton\n  Mullis.\n\\newblock Laion-400m: Open dataset of clip-filtered 400 million image-text\n  pairs.\n\\newblock In {\\em NeurIPS Workshop Datacentric AI}. J{\\\"u}lich Supercomputing\n  Center, 2021.\n\n\\bibitem{li2021align}\nJunnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong,\n  and Steven Chu~Hong Hoi.\n\\newblock Align before fuse: Vision and language representation learning with\n  momentum distillation.\n\\newblock In {\\em Advances in neural information processing systems},\n  volume~34, pages 9694--9705, 2021.\n\n\\bibitem{lu2022unified}\nJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha\n  Kembhavi.\n\\newblock {Unified-IO}: A unified model for vision, language, and multi-modal\n  tasks.\n\\newblock {\\em arXiv preprint arXiv:2206.08916}, 2022.\n\n\\bibitem{wang2022unifying}\nPeng Wang, An~Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma,\n  Chang Zhou, Jingren Zhou, and Hongxia Yang.\n\\newblock Unifying architectures, tasks, and modalities through a simple\n  sequence-to-sequence learning framework.\n\\newblock {\\em arXiv preprint arXiv:2202.03052}, 2022.\n\n\\bibitem{rao2022denseclip}\nYongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang,\n  Jie Zhou, and Jiwen Lu.\n\\newblock Denseclip: Language-guided dense prediction with context-aware\n  prompting.\n\\newblock In {\\em Proceedings of the IEEE Conference on Computer Vision and\n  Pattern Recognition (CVPR)}, 2022.\n\n\\bibitem{zhong2022regionclip}\nYiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella,\n  Liunian~Harold Li, Luowei Zhou, Xiyang Dai, Lu~Yuan, Yin Li, et~al.\n\\newblock Regionclip: Region-based language-image pretraining.\n\\newblock In {\\em Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition}, pages 16793--16803, 2022.\n\n\\bibitem{zhou2021denseclip}\nChong Zhou, Chen~Change Loy, and Bo~Dai.\n\\newblock Extract free dense labels from clip.\n\\newblock In {\\em Computer Vision--ECCV 2022: 17th European Conference, Tel\n  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXVIII}, pages\n  696--712. Springer, 2022.\n\n\\bibitem{zhou_conditional}\nKaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.\n\\newblock Conditional prompt learning for vision-language models.\n\\newblock In {\\em 2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern\n  Recognition ({CVPR})}, 2022.\n\n\\bibitem{ju_prompting_2022}\nChen Ju, Tengda Han, Kunhao Zheng, Ya~Zhang, and Weidi Xie.\n\\newblock Prompting visual-language models for efficient video understanding.\n\\newblock In {\\em Computer Vision--ECCV 2022: 17th European Conference, Tel\n  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXV}, pages 105--124.\n  Springer, 2022.\n\n\\bibitem{jia_visual_2022}\nMenglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath\n  Hariharan, and Ser-Nam Lim.\n\\newblock Visual prompt tuning.\n\\newblock In {\\em Computer Vision--ECCV 2022: 17th European Conference, Tel\n  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXIII}, pages\n  709--727. Springer, 2022.\n\n\\bibitem{bahng_exploring_2022}\nHyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola.\n\\newblock Exploring visual prompts for adapting large-scale models.\n\\newblock {\\em arXiv preprint arXiv:2203.17274}, 1(3):4, 2022.\n\n\\bibitem{zang_unified_2022}\nYuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen~Change Loy.\n\\newblock Unified vision and language prompt learning.\n\\newblock {\\em arXiv preprint arXiv:2210.07225}, 2022.\n\n\\bibitem{shen_multitask_2022}\nSheng Shen, Shijia Yang, Tianjun Zhang, Bohan Zhai, Joseph~E Gonzalez, Kurt\n  Keutzer, and Trevor Darrell.\n\\newblock Multitask vision-language prompt tuning.\n\\newblock {\\em arXiv preprint arXiv:2211.11720}, 2022.\n\n\\bibitem{zhou_learning_2022}\nKaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.\n\\newblock Learning to prompt for vision-language models.\n\\newblock {\\em Int J Comput Vis}, 130(9):2337--2348, 2022.\n\n\\bibitem{shtedritski_what_2023}\nAleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi.\n\\newblock What does clip know about a red circle? visual prompt engineering for\n  vlms.\n\\newblock {\\em arXiv preprint arXiv:2304.06712}, 2023.\n\n\\bibitem{dosovitskiy2021an}\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,\n  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\n  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.\n\\newblock An image is worth 16x16 words: Transformers for image recognition at\n  scale.\n\\newblock In {\\em International Conference on Learning Representations}, 2021.\n\n\\bibitem{li2021grounded}\nLiunian~Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li,\n  Yiwu Zhong, Lijuan Wang, Lu~Yuan, Lei Zhang, Jenq-Neng Hwang, et~al.\n\\newblock Grounded language-image pre-training.\n\\newblock In {\\em Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition}, pages 10965--10975, 2022.\n\n\\bibitem{zou2022spot}\nYang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, and Onkar Dabeer.\n\\newblock {SPot-the-Difference} self-supervised pre-training for anomaly\n  detection and segmentation.\n\\newblock In {\\em Proceedings of the European Conference on Computer Vision},\n  2022.\n\n\\bibitem{hinton2012imagenet}\nGeoffrey~E Hinton, Alex Krizhevsky, and Ilya Sutskever.\n\\newblock Image{N}et classification with deep convolutional neural networks.\n\\newblock {\\em Advances in Neural Information Processing Systems},\n  25(1106-1114):1, 2012.\n\n\\bibitem{bozic_mixed_2021}\nJakob Bo{\\v{z}}i{\\v{c}}, Domen Tabernik, and Danijel Sko{\\v{c}}aj.\n\\newblock Mixed supervision for surface-defect detection: From weakly to fully\n  supervised learning.\n\\newblock {\\em Computers in Industry}, 129:103459, 2021.\n\n\\bibitem{mtd2018}\nYibin Huang, Congying Qiu, Yue Guo, Xiaonan Wang, and Kui Yuan.\n\\newblock Surface defect saliency of magnetic tile.\n\\newblock In {\\em 2018 IEEE 14th International Conference on Automation Science\n  and Engineering (CASE)}, pages 612--617, 2018.\n\n\\bibitem{zagoruyko2016wideresnet}\nSergey Zagoruyko and Nikos Komodakis.\n\\newblock Wide residual networks.\n\\newblock In Edwin R.~Hancock Richard C.~Wilson and William A.~P. Smith,\n  editors, {\\em Proceedings of the British Machine Vision Conference (BMVC)},\n  pages 87.1--87.12. BMVA Press, September 2016.\n\n\\end{thebibliography}\n}\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nWeakly-Supervised Concealed Object Segmentation (WSCOS) aims to  segment objects well blended with surrounding environments using sparsely-annotated data for model training. It remains a challenging task since (1) it is hard to distinguish concealed objects from the background due to the intrinsic similarity and (2) the sparsely-annotated training data only provide weak supervision for model learning. In this paper, we propose a new WSCOS method to address these two challenges. To tackle the intrinsic similarity challenge, we design a multi-scale feature grouping module that first groups features at different granularities and then aggregates these grouping results. By grouping similar features together, it encourages segmentation coherence, helping obtain complete segmentation results for both single and multiple-object images. For the weak supervision challenge, we utilize the recently-proposed vision foundation model, ``\\emph{Segment Anything Model (SAM)}'', and use the provided sparse annotations as prompts to generate segmentation masks, which are used to train the model. To alleviate the impact of low-quality segmentation masks, we further propose a series of strategies, including multi-augmentation result ensemble, entropy-based pixel-level weighting, and entropy-based image-level selection. These strategies help provide more reliable supervision to train the segmentation model. We verify the effectiveness of our method on various WSCOS tasks, and experiments demonstrate that our method achieves state-of-the-art performance on these tasks. \n\\end{abstract}\n\n\\setlength{\\abovedisplayskip}{2pt}\n\\setlength{\\belowdisplayskip}{2pt}\n\n\\section{Introduction}\nConcealed object segmentation (COS) aims to segment objects visually blended with surrounding environments \\cite{fan2021concealed}. COS is a general term with different applications, e.g., camouflaged object detection~\\cite{fan2020camouflaged,yang2021uncertainty}, polyp image segmentation~\\cite{fan2020pranet,zhang2020adaptive}, transparent object detection~\\cite{mei2020don,xie2020segmenting},  etc. \nCOS is a challenging task due to the intrinsic between foreground objects and background, which makes it extremely difficult to identify discriminative clues for accurate foreground-background separation. To address this challenge, existing methods have employed approaches that mimic human vision~\\cite{zhang2022preynet,mei2021camouflaged,pang2022zoom}, introduce frequency information~\\cite{zhong2022detecting,He2023Camouflaged}, or adopt joint modeling in multiple tasks~\\cite{zhai2021mutual,sun2022boundary,zhaiexploring,lv2021simultaneously,jia2022segment,zhu2022can}.\n\nWeakly-Supervised COS (WSCOS) studies an even more challenging yet more  practical problem, involving learning a COS model without relying on pixel-wise fully-annotated training data. \nWSCOS greatly reduces annotation costs by only requiring a few annotated points or scribbles in the foreground or background. However, the sparsity of annotated training data diminishes the limited discrimination capacity of the segmenter during model learning, thus further restricting segmentation performance.\n\nIn this paper, we propose a new algorithm for the challenging WSCOS task. To tackle the intrinsic similarity of foreground and background, we introduce a Multi-scale Feature Grouping (MFG) module that first evacuates discriminative cues at different granularities and then aggregates these cues to handle various concealing scenarios. By performing feature grouping, MFG essentially promotes coherence among features and thus is able to alleviate incomplete segmentation by encouraging local correlation within individual objects, while also facilitating multiple-object segmentation by seeking global coherence across multiple objects.\n\nTo address the challenge of weak supervision, we propose to leverage the recently proposed vision foundation model, \\emph{Segment Anything Model (SAM)}, to generate dense masks by using sparse annotations as prompts, and use the generated masks as pseudo labels to train a segmenter. However, due to the intrinsic similarity between foreground objects and the background, the pseudo labels generated by SAM may not always be reliable. We propose a series of strategies to address this problem. First, we propose to generate multiple augmentation views for each image and fuse the segmentation masks produced from all views. The fused mask can highlight reliable predictions resistant to image augmentations and tend to be more accurate and complete as an effect of ensemble. Second, we propose an entropy-based weighting mechanism that assigns higher weights to predictions of pixels of high certainty. Lastly, to deal with the extreme images that SAM fails to generate reasonably correct masks, we propose an entropy-based image-level selection technique to assess the quality of the generated mask and decide whether to use the masks as pseudo labels for model training. These strategies ensure that only high-quality pseudo labels are used for training the segmenter. For ease of description, we refer to our solution of using SAM to address this task as \\textit{WS-SAM}.\n \n\nOur contributions are summarized as follows: \n\n(1) We propose to leverage SAM for weakly-supervised segmentation by using the provided sparse annotations as prompts to generate dense segmentation masks and train a task segmentation model. To the best of our knowledge, this is the first attempt to leverage the vision foundation model to address the weakly-supervised segmentation task.\n\n(2) We propose a series of strategies for dealing with potentially low segmentation mask quality, including the multi-augmentation result ensemble technique, entropy-based pixel-level weighting technique, and entropy-based image-level selection technique. These techniques help provide reliable guidance to train the model and lead to improved segmentation results.\n\n(3) We introduce a Multi-scale Feature Grouping (MFG) technique to tackle the intrinsic similarity challenge in the WSCOS task. MFG evacuates discriminative cues by performing feature grouping at different granularities. It encourages segmentation coherence, facilitating to obtain complete segmentation results for both single and multiple object images.\n\n(4) We evaluate our method on various WSCOS tasks, and the experiments demonstrate that our method achieves state-of-the-art performance.  \n\n\\section{Related Works}\n\\noindent \\textbf{Segment Anything Model}. SAM~\\cite{kirillov2023segment} is a recently-proposed vision foundation model trained on SA-1B of over 1 billion masks. Its primary objective is to segment any object in any given image without requiring any additional task-specific adaptation. Its outstanding quality in segmentation results and zero-shot generalization to new scenes make SAM a promising candidate for various computer vision tasks. However, recent studies have highlighted that SAM encounters difficulties when segmenting objects with poor visibility, such as camouflaged objects~\\cite{ji2023sam,tang2023can,ji2023segment}, medical polyps~\\cite{mazurowski2023segment,cheng2023sam,mattjie2023exploring,zhou2023can}, and transparent glasses~\\cite{han2023segment,ji2023segment}. These findings suggest that SAM still has limitations in COS tasks.\n\nIn this paper, we propose using SAM to generate dense segmentation masks from sparse annotations and introduce the first SAM-based weakly-supervised framework in COS, termed WS-SAM. To further increase the accuracy of the generated pseudo-labels, we propose a pseudo label refinement strategy. Such a strategy assigns higher weights to those reliable predictions that are resistant to various image augmentation. Therefore, WS-SAM can offer more precise and stable guidance for the learning process, ultimately boosting the segmentation performance of the segmenter.\n\n\\noindent \\textbf{Concealed Object Segmentation}. With the rapid development of deep learning, learning-based segmenters have obtained great achievements in the fully-supervised COS tasks~\\cite{fan2020pranet,fan2020camouflaged,lin2021rich}. PraNet~\\cite{fan2020pranet} proposed a parallel reverse attention network to segment polyps in colonoscopy images. Drawn inspiration from biology, SINet~\\cite{fan2020camouflaged} designed a predator network to discover and locate camouflaged objects. To detect transparent objects, GSDNet~\\cite{lin2021rich} integrated an aggregation module and a reflection refinement module. However, there is limited research on the weakly-supervised COS task. SCOD~\\cite{he2022weakly} introduced the first weakly-supervised COD framework, but it is only supervised with sparse annotations, which greatly restricts its discrimination capacity and inevitably inhibits segmentation performance. To address this problem, we first propose using SAM to generate precise pseudo-labels. Besides, to tackle intrinsic similarity, we introduce the multi-scale feature grouping module to evacuate discriminative cues at different granularities and thus promote feature coherence.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\setlength{\\abovecaptionskip}{-0.1cm}\n\t\\begin{center}\n\t\t\\includegraphics[width=\\linewidth]{Figure/SWSCOS.pdf}\n\t\\end{center}\n\t\\caption{Framework of WS-SAM with scribble supervision. Note that the corresponding masks of the augmented images are inversely transformed so as to be consistent with the original image.\n }\n\t\\label{fig:SWSCOS}\n\t\\vspace{-0.3cm}\n\\end{figure}\n\n\\section{Methodology}\nWeakly-Supervised Concealed Object Segmentation (WSCOS) aims to learn a segmentation model from a sparsely-annotated training dataset $\\mathcal{S} = \\{\\mathbf{X}_i, \\mathbf{Y}_i\\}_{i=1}^{S}$ and test the model on a test dataset $\\mathcal{T} = \\{\\mathbf{T}_i\\}_{i=1}^{T}$, where $\\mathbf{X}_i$ and $\\mathbf{T}_i$ denote the training and test images, respectively; $\\mathbf{Y}_i$ represents the sparse annotations, which could be a few points or scribbles annotated as foreground or background.    \n\nLearning the segmentation model could be a challenging task, as concealed objects usually blend well with their surrounding environment, making it hard to distinguish foreground from background. Besides, the sparse annotations $\\mathbf{Y}_i$ may not provide sufficient supervision to learn the model capable of making accurate dense predictions. To address these challenges, we first propose a strategy of leveraging the recently-proposed vision foundation model, \\emph{Segment Anything Model (SAM)}, to generate high-quality dense masks from sparse annotations and use the dense masks as pseudo labels to train the segmentation model. In addition, we propose a Multi-scale Feature Grouping (MFG) module that groups features at different granularities, encouraging segmentation coherence and facilitating obtain complete segmentation results for various concealing scenarios. \n\n\\subsection{Pseudo Labeling with SAM}\\label{Sec:SWSCOS}\n\nSAM is a recently-released vision foundation model for generic object segmentation \\cite{kirillov2023segment}. It is trained with more than one billion segmentation masks and has shown impressive capabilities of producing precise segmentation masks for a wide range of object categories (so-called ``\\emph{segment anything}''). \nUnlike some enthusiasts who brag SAM has ``killed'' the segment task, we find that SAM is far from reaching that level, at least for the studied concealed object segmentation task. This is first because SAM requires ``prompts'' that provide clues about the objects of interest to produce segmentation results. While the prompts could be in many forms, e.g., points, masks, bounding boxes, etc., they are required to be provided by humans or other external sources (e.g., other algorithms) \\footnote{SAM includes an automatic prompt generation mechanism that first densely selects points across the whole image as prompts, then generates results based on the dense prompts, and last fuse the results. The problem is it segments everything possible in the image, with no distinguishing of objects of interest and others.}.  This requirement of the additional prompt inputs makes SAM unable to be (directly) used for applications where only test images are provided.  In addition, we find that while SAM exhibits impressive performance for general scene images, it still struggles for concealed object images, due to the intrinsic similarity between foreground objects and the background. \n\n\\begin{figure}[t]\n\t\\centering\n\t\\setlength{\\abovecaptionskip}{-0.2cm}\n\t\\begin{center}\n\t\t\\includegraphics[width=0.9\\linewidth]{Figure/ImageAugmentation.pdf}\n\t\\end{center}\n\t\\caption{Masks of SAM with different augmented images. We inversely transform the masks to keep consistent with the original image. It is observed that fused masks contain more accurate and complete segmentation information.\n }\n\t\\label{fig:Augmentation}\n\t\\vspace{-0.6cm}\n\\end{figure}\n\nIn this paper, we introduce SAM for the Weakly-Supervised Concealed Object Segmentation (WSCOS) task. As shown in Fig.~\\ref{fig:SWSCOS}, we use SAM to generate segmentation masks on training images by taking the sparse annotations as prompts, and take the segmentation masks as pseudo labels to train a COS model, which will be used for the test. \nIt is expected that the SAM-generated pseudo labels are not reliable. We address this problem by proposing three techniques, namely, Multi-augmentation result fusion, pixel-level weighting and image-level selection. \n\n\\noindent\\textbf{Multi-augmentation result fusion}. Given a concealed image $(\\mathbf{X}_i, \\mathbf{Y}_i)\\in\\mathcal{S}$, we generate $K$ augmented images $\\{\\mathbf{X}^k_i\\}^K_{k=1}$ by applying stochastic augmentations randomly sampled from image flipping, rotation ($0^{\\circ}$, $90^{\\circ}$, $180^{\\circ}$, $270^{\\circ}$), and scaling ($\\times0.5$, $\\times1.0$, $\\times2.0$). \nWe send $\\{\\mathbf{X}^k_i\\}^K_{k=1}$ to SAM by using the sparse annotations $\\mathbf{Y}_i$ as prompts, and generate segmentation masks $\\{\\mathbf{M}^k_i\\}^K_{k=1}$, where \n\\begin{equation}\n    \\mathbf{M}_i^k=\\mathrm{SAM}\\left(\\mathbf{X}^k_i, \\mathbf{Y}_i \\right).\n\\end{equation}\nNote that $\\mathbf{M}_i^k$ has the same shape as input image $\\mathbf{X}^k_i$, which may differ in shape from $\\mathbf{X}_i$; we perform inverse image transformation to ensure all masks have the same shape as the original image. \n\nAs different segmentation results can be obtained when feeding SAM with different prompts, we expect $\\{\\mathbf{M}^k_i\\}^K_{k=1}$ to vary since different augmented images are used for segmentation. Fig. \\ref{fig:Augmentation} shows some examples. We can see that while these masks vary significantly in shape, they overlap in certain regions, which are reliably predicted by SAM regardless of image transformations and usually correspond to correctly predicted foreground regions. Besides, these masks complement each other, such that some foreground regions missed by one mask can be found in other masks. Based on these observations, we propose to fuse the segmentation masks for different augmented images, as\n\\begin{equation}\n    \\tilde{\\mathbf{M}}_i = \\frac{1}{K}\\sum_{k=1}^K \\mathbf{M}^k_i,\n    \\label{Eq:RefinedPL}\n\\end{equation}\nwhere $\\tilde{\\mathbf{M}}_i$ is the fused mask. We expect $\\tilde{\\mathbf{M}}_i$ to be more reliable than the individual masks as it is an ensemble over various augmented images. \n\n\\noindent\\textbf{Pixel-level weighting}. \nThe prediction reliability of different pixels may vary. To highlight those more reliable ones, we propose to use entropy to weigh the predictions.   \nWe calculate the entropy of each pixel and get an entropy map as  \n\\begin{equation}\n    \\tilde{\\mathbf{E}}_i = -\\tilde{\\mathbf{M}}_i \\log \\tilde{\\mathbf{M}}_i -(1-\\tilde{\\mathbf{M}}_i) \\log (1-\\tilde{\\mathbf{M}}_i).\n    \\label{Eq:entropy}\n\\end{equation}\nAs the entropy map is calculated from the fused mask, it measures the prediction uncertainty of each pixel across all augmentation as a pixel will have low entropy only if it is \\textit{confidently and meanwhile consistently} predicted from all augmented images. Therefore, we can use this entropy map to weigh the fused mask $\\mathbf{M}_i^k$ and assign higher weights to those reliable pixels.  \n\n\\noindent\\textbf{Image-level selection}. We have observed that for some highly challenging concealed images, SAM fails to produce even reasonably correct results with the sparse annotations as the prompts, with whatever types of augmented images. This fundamentally invalidates the above pixel-wise weighting strategy. To deal with this case, we further propose an image-level selection mechanism to selectively choose images for training, further striving to provide reliable supervision to train the segmenter.\n\nSimilar to using entropy to define the pixel-level prediction uncertainty, we propose two entropy-based image-level uncertainty measurements, namely, absolute uncertainty $U_a$ and relative uncertainty $U_r$. Absolute uncertainty $U_a$ refers to the proportion of high-uncertainty pixels among all pixels, while relative uncertainty $U_r$ indicates the proportion of high-uncertainty pixels and foreground pixels with low uncertainty, which is specifically designed to accommodate small-object scenarios. We regard a pixel as a high-uncertainty pixel when its entropy is above 0.9. We define the following indicator function to decide whether to keep an image for training or not:  \n\\begin{equation}\n\\hat{\\mathbf{M}}_i = \\mathbbm{1}[U_a < \\tau_{a}] \\times \\mathbbm{1}[U_r < \\tau_{r}],\n\\end{equation}\nwhere $\\tau_{a}$ and $\\tau_{r}$ are the thresholds that are set as 0.1 and 0.5 in this paper, respectively.  \n\nApplying the entropy weights $\\tilde{\\mathbf{E}}_i$  on the fused mask $\\tilde{\\mathbf{M}}_i$ and the image selection indicator $\\hat{\\mathbf{M}}_i$, we reach our final mask that will be used for training the segmenter as,\n\\begin{equation}\n\\hat{\\mathbf{Y}}_i = (1 - \\tilde{\\mathbf{E}}_i) \\times \\tilde{\\mathbf{M}}_i \\times \\hat{\\mathbf{M}}_i.\n\\end{equation}\nOur technique leverages SAM to generate segmentation masks and further incorporates multi-augmentation result fusion, pixel-level uncertainty weighting, and image-level uncertainty filtering, thus being able to generate reliable pseudo labels to train the segmenter.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\setlength{\\abovecaptionskip}{-0.2cm}\n\t\\begin{center}\n\t\t\\includegraphics[width=\\linewidth]{Figure/FCNet-short.pdf}\n\t\\end{center}\n \\vspace{0.5mm}\n\t\\caption{Architecture of the proposed model. \n $\\Phi_P$ denotes feature grouping with $P$ prototypes. We simplify the broadcast process in $\\Phi_P$ for space limitation.}\n\t\\label{fig:MFG}\n\t\\vspace{-0.6cm}\n\\end{figure}\n\n\\subsection{Multi-scale Feature Grouping}\n\\label{Sec:MFG}\nThe intrinsic similarity in concealed objects may cause incomplete segmentation and partial object localization in multi-object segmentation. Such problems could be further aggravated in weakly supervised scenarios due to the limited discriminative capacity of the segmenter. To address this issue, we propose a Multi-scale Feature Grouping (MFG) module that evacuates discriminative cues at various granularities. MFG achieves this by exploring the coherence of foreground/background regions and performing feature grouping at different levels. By encouraging feature coherence, MFG can alleviate incomplete segmentation by enhancing local correlation within individual objects and further facilitate multiple-object segmentation by seeking global coherence across multiple objects. \nThe architecture of the proposed MFG module is illustrated in Fig.~\\ref{fig:MFG}. \n\n\\noindent\\textbf{Feature grouping}. \nSuppose $\\mathbf{F}\\in \\mathbb{R}^{H\\times W\\times C}$ is the feature representation of an input image. We perform feature grouping by mapping $\\mathbf{F}$ to $N$ learnable cluster prototypes $\\mathbf{P}\\in \\mathbb{R}^{N\\times C}$. These cluster prototypes $\\mathbf{P}$ are randomly initialized. \nWe first append the learnable spatial positional embedding $\\mathbf{P}_e$ to the input feature $\\mathbf{F}$ and get $\\mathbf{F}_p$. Then, we linearly transform the prototypes $\\mathbf{P}$ and the positioned feature $\\mathbf{F}_p$ into $\\mathbf{Q}\\in\\mathbb{R}^{N\\times C}$, $\\mathbf{K}\\in\\mathbb{R}^{H W\\times C}$, and $\\mathbf{V}\\in\\mathbb{R}^{H W\\times C}$:\n\\begin{equation}\\label{eq:prototype_qkv}\n    \\mathbf{Q} = \\mathbf{W}_q \\mathbf{P}, \\ \\ \\ \\mathbf{K} = \\mathbf{W}_k \\mathbf{F}_p, \\ \\ \\ \\mathbf{V} = \\mathbf{W}_v \\mathbf{F}_p, \n\\end{equation}\nwhere $\\mathbf{W}_q, \\mathbf{W}_k, \\mathbf{W}_v \\in \\mathbb{R}^{C\\times C}$ are the learnable weights. \nTo ensure the exclusive assignment of features to the cluster prototypes, \nwe normalize the coefficients over all prototypes, \n\\begin{equation}\n    \\bar{\\mathbf{A}}_{i,j}=\\frac{e^{\\mathbf{A}_{i,j}}}{\\sum_l e^{\\mathbf{A}_{i,l}}}, \\quad \\mathrm{where} \\quad \\mathbf{A}=\\frac{1}{\\sqrt{C}} \\mathbf{K}^{\\top} \\mathbf{Q}.\n\\end{equation}\nWe then calculate the integral value $\\mathbf{U}$ of the input values with respect to  the prototypes as\n\\begin{equation}\n    \\mathbf{U} = \\mathbf{D}^{\\top} \\mathbf{V}, \\quad \\mathrm{where} \\quad \\mathbf{D}_{i,j}= \\frac{\\mathbf{A}_{i,j}}{\\sum_l \\mathbf{A}_{i,l}}, \n\\end{equation}\nand update the prototypes $\\mathbf{P}$ by feeding $\\mathbf{P}$ and $\\mathbf{U}$ into a Gated Recurrent Units $GRU(\\cdot)$:\n\\begin{equation}\\label{eq:prototype_GRU}\n    \\mathbf{P} = GRU\\left(inputs=\\mathbf{U}, states = \\mathbf{P}\\right).\n\\end{equation}\nBy repeating Eqs.~\\eqref{eq:prototype_qkv} - \\eqref{eq:prototype_GRU} for $T$ iterations, the cluster prototypes are iteratively updated and gradually strengthen the association between similar features, where $T=3$ in this paper.\n\nWe broadcast each prototype onto a 2D grid augmented with the learnable spatial position embedding $\\mathbf{P}_e$ to obtain $\\{\\mathbf{F}'_i\\}_{i=1}^N \\in \\mathbb{R}^{H\\times W\\times C}$, and use $1\\times1$ convolution to downsample each prototype, obtaining $\\{\\mathbf{F}''_i\\}_{i=1}^N \\in \\mathbb{R}^{H\\times W\\times C/N}$. We concatenate those prototypes and obtain $\\mathbf{F}_c\\in \\mathbb{R}^{H\\times W\\times C}$. \n\nFor ease of future use, we denote the feature grouping process with $N$ prototypes as $\\mathbf{F}_c= \\Phi_N(\\mathbf{F})$.\n\n\\noindent\\textbf{Multi-scale feature aggregation}. \nThe number of prototypes $N$ in the above feature grouping technique controls the grouping granularity: a smaller value of $N$ facilitates the extraction of global information, while a larger value of $N$ can provide more valuable detailed information. To strike a balance, we propose to aggregate the multi-scale grouping features with different numbers of prototypes. Taking inspiration from the second-order Runge-Kutta (RK2) structure known for its superior numerical solutions compared to the traditional residual structure~\\cite{he2016deep}, we employ RK2 to aggregate those features. Additionally, as shown in Fig.~\\ref{fig:MFG}, we adopt a weighted gate mechanism $\\mathbf{\\alpha}_g$ to adaptively estimate the trade-off parameter rather than using a fixed coefficient. Given the feature $\\mathbf{F}$, the adaptively aggregated feature $\\hat{\\mathbf{F}}$ is formulated as follows:\n\\begin{equation}\n    \\hat{\\mathbf{F}} = \\mathbf{F} +\\alpha_g \\Phi_{N_1}(\\mathbf{F}) +(1-\\alpha_g) \\Phi_{N_2}(\\mathbf{F}+\\Phi_{N_1}(\\mathbf{F})), %\\quad \\mathrm{where} \\quad \\alpha_g=S(\\sigma \\ cat(\\Phi_{N_1}(\\mathbf{F}), \\Phi_{N_2}(\\mathbf{F}))+\\mu), \n\\end{equation}\nwhere $\\alpha_g=S(\\sigma \\ cat(\\Phi_{N_1}(\\mathbf{F}), \\Phi_{N_2}(\\mathbf{F}+\\Phi_{N_1}(\\mathbf{F})))+\\mu)$, $\\sigma$ and $\\mu$ are the learnable parameters in $\\alpha_g$. $N_1$ and $N_2$ are the numbers of groups, which are empirically set as 4 and 2, respectively. \n\nOur multi-scale feature grouping technique is inspired by the slot attention technique~\\cite{locatello2020object}, but we differ from slot attention in the following aspects. Slot attention targets at instance-level grouping in a self-supervised manner, our MFG is proposed to adaptively excavate the feature-level coherence for complete segmentation and accurate multi-object localization. To relax the segmenter and ensure generalization, we remove the auxiliary decoder used in slot attention for image reconstruction, along with the reconstruction constraint. Additionally, we employ an RK2 structure to aggregate the multiscale grouping feature with different numbers of prototypes, which further facilitates the excavation of feature coherence and therefore helps improve segmentation performance.\n\n\\subsection{Weakly-Supervised Concealed Object Segmentation}\nTo use the proposed MFG technique for concealed object segmentation, we integrate MFG with the encoder and decoder architecture utilized in an existing camouflaged object detection model \\cite{He2023Camouflaged} to construct a novel segmenter. The model comprises a ResNet50-backed encoder $E$ that maps an input image $\\mathbf{X}_i$ to a feature space as $\\mathbf{F}_i = E(\\mathbf{X}_i)$. Using the obtained $\\mathbf{F}_i$, we apply MFG to perform multi-scale feature grouping, resulting in $\\hat{\\mathbf{F}}_i=MFG(\\mathbf{F}_i)$. Subsequently, a decoder $D$ maps $\\hat{\\mathbf{F}}_i$ back to the image space, generating the predicted mask $\\mathbf{Y}_i'= D(\\hat{\\mathbf{F}}_i)$. Fig. \\ref{fig:MFG} provides a conceptual illustration of this model, and \\emph{more architecture details can be found in the supplementary materials.} \n\nWe train the whole model jointly with the sparse annotations $\\mathbf{Y}_i$ and the generated segmentation mask $\\hat{\\mathbf{Y}}_i$ by the SAM model, as  \n\\begin{equation}\n    L = \\frac{1}{N_s} \\sum_{(\\mathbf{X}_i, \\mathbf{Y}_i)\\sim\\mathcal{S}} L_{pce}\\left(\\mathbf{Y}_i', \\mathbf{Y}_i\\right)+L_{ce}(\\mathbf{Y}'_i, \\hat{\\mathbf{Y}}_i)+L_{IoU}(\\mathbf{Y}'_i, \\hat{\\mathbf{Y}}_i).\n\\end{equation}\nWhere the first term is the partial cross-entropy loss $L_{pce}$ used to ensure consistency between the prediction maps and the sparse annotations $\\mathbf{Y}_i$ \\cite{he2022weakly}. The second and third terms are the cross-entropy loss $L_{ce}$ and the intersection-over-union loss $L_{IoU}$, both calculated using the pseudo label $\\hat{\\mathbf{Y}}_i$ \\cite{fan2021concealed}.\n\n\\begin{table}[t]\n\\centering\n\\caption{Results on COD with point supervision and scribble supervision. SCOD+ indicates integrating SCOD with our WS-SAM framework. \nThe best two results are in {\\color[HTML]{FF0000} \\textbf{red}} and {\\color[HTML]{00B0F0} \\textbf{blue}} fonts.}\n\\resizebox{\\columnwidth}{!}{\n\\setlength{\\tabcolsep}{1mm}\n\\begin{tabular}{l|c|cccc|cccc|cccc|cccc}\n\\toprule\n\\multicolumn{1}{l|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{4}{c|}{\\textit{CHAMELEON} } & \\multicolumn{4}{c|}{\\textit{CAMO}} & \\multicolumn{4}{c|}{\\textit{COD10K}} & \\multicolumn{4}{c}{\\textit{NC4K}} \\\\ \\cline{3-18}\n\\multicolumn{1}{l|}{\\multirow{-2}{*}{Methods}} & \\multicolumn{1}{c|}{\\multirow{-2}{*}{Pub.}} & {\\cellcolor{gray!40}$M$~$\\downarrow$} &{\\cellcolor{gray!40}$F_\\beta$~$\\uparrow$} &{\\cellcolor{gray!40}$E_\\phi$~$\\uparrow$} & \\multicolumn{1}{c|}{\\cellcolor{gray!40}$S_\\alpha$~$\\uparrow$}& {\\cellcolor{gray!40}$M$~$\\downarrow$} &{\\cellcolor{gray!40}$F_\\beta$~$\\uparrow$} &{\\cellcolor{gray!40}$E_\\phi$~$\\uparrow$} & \\multicolumn{1}{c|}{\\cellcolor{gray!40}$S_\\alpha$~$\\uparrow$}& {\\cellcolor{gray!40}$M$~$\\downarrow$} &{\\cellcolor{gray!40}$F_\\beta$~$\\uparrow$} &{\\cellcolor{gray!40}$E_\\phi$~$\\uparrow$} & \\multicolumn{1}{c|}{\\cellcolor{gray!40}$S_\\alpha$~$\\uparrow$}& {\\cellcolor{gray!40}$M$~$\\downarrow$} &{\\cellcolor{gray!40}$F_\\beta$~$\\uparrow$} &{\\cellcolor{gray!40}$E_\\phi$~$\\uparrow$} & \\multicolumn{1}{c}{\\cellcolor{gray!40}$S_\\alpha$~$\\uparrow$}\\\\ \\midrule\n\\multicolumn{18}{c}{Scribble Supervision} \\\\ \\midrule\nSAM~\\cite{kirillov2023segment}                                         & \\multicolumn{1}{c|}{---}                              & 0.207                                 & 0.595                                 & 0.647                                 & 0.635                                 & 0.160                                 & 0.597                                 & 0.639                                 & 0.643                                 & 0.093                                 & 0.673                                 & 0.737                                 & 0.730                                 & 0.118                                 & 0.675                                 & 0.723                                 & 0.717                                 \\\\\nSAM-S~\\cite{kirillov2023segment}                                         & \\multicolumn{1}{c|}{---}                              & 0.076                                 & 0.729                                 & 0.820                                 & 0.650                                 & 0.105                                 & 0.682                                 & 0.774                                 & 0.731                                 & {\\color[HTML]{00B0F0} \\textbf{0.046}}                                & {\\color[HTML]{00B0F0} \\textbf{0.695}}                                 & 0.828                                 & {\\color[HTML]{00B0F0} \\textbf{0.772}}                                 & 0.071                                 & 0.747                                 & 0.832                                 & 0.763                                 \\\\\nWSSA~\\cite{zhang2020weakly}                                          & CVPR20                                             & 0.067                                 & 0.692                                 & 0.860                                 & 0.782                                 & 0.118                                 & 0.615                                 & 0.786                                 & 0.696                                 & 0.071                                 & 0.536                                 & 0.770                                 & 0.684                                 & 0.091                                 & 0.657                                 & 0.779                                 & 0.761                                 \\\\\nSCWS~\\cite{yu2021structure}                                       & AAAI21                                             & 0.053                                 & 0.758                                 & 0.881                                 & 0.792                                 & 0.102                                 & 0.658                                 & 0.795                                 & 0.713                                 & 0.055                                 & 0.602                                 & 0.805                                 & 0.710                                 & 0.073                                 & 0.723                                 & 0.814                                 & 0.784                                 \\\\\nTEL~\\cite{liang2022tree}                                           & CVPR22                                             & 0.073                                 & 0.708                                 & 0.827                                 & 0.785                                 & 0.104                                 & 0.681                                 & 0.797                                 & 0.717                                 & 0.057                                 & 0.633                                 & 0.826                                 & 0.724                                 & 0.075                                 & 0.754                                 & 0.832                                 & 0.782                                 \\\\\nSCOD~\\cite{he2022weakly}                                         & AAAI23                                             & {\\color[HTML]{FF0000} \\textbf{0.046}} & {\\color[HTML]{00B0F0} \\textbf{0.791}} & {\\color[HTML]{00B0F0} \\textbf{0.897}} & 0.818                                 & {\\color[HTML]{00B0F0} \\textbf{0.092}} & 0.709                                 & 0.815                                 & 0.735                                 & 0.049                                 & 0.637                                 & 0.832                                 & 0.733                                 & 0.064                                 & 0.751                                 & 0.853                                 & 0.779                                 \\\\\n\\rowcolor{c2!20} SCOD+ & \\multicolumn{1}{c|}{---} & {\\color[HTML]{FF0000} \\textbf{0.046}} & {\\color[HTML]{FF0000} \\textbf{0.797}} & {\\color[HTML]{FF0000} \\textbf{0.900}} & {\\color[HTML]{00B0F0} \\textbf{0.820}} & {\\color[HTML]{FF0000} \\textbf{0.090}} &{\\color[HTML]{00B0F0} \\textbf{0.716}} & {\\color[HTML]{FF0000} \\textbf{0.818}} &{\\color[HTML]{00B0F0} \\textbf{0.741}} &0.047 & 0.650 &{\\color[HTML]{00B0F0} \\textbf{0.845}} &0.742 & {\\color[HTML]{00B0F0} \\textbf{0.060}} &{\\color[HTML]{00B0F0} \\textbf{0.766}} &{\\color[HTML]{00B0F0} \\textbf{0.862}} &{\\color[HTML]{00B0F0} \\textbf{0.785}}\\\\\n\\rowcolor{c2!20}Ours & \\multicolumn{1}{c|}{---} & {\\color[HTML]{FF0000} \\textbf{0.046}} & 0.777    & {\\color[HTML]{00B0F0} \\textbf{0.897}} & {\\color[HTML]{FF0000} \\textbf{0.824}} & {\\color[HTML]{00B0F0} \\textbf{0.092}} & {\\color[HTML]{FF0000} \\textbf{0.742}} & {\\color[HTML]{FF0000} \\textbf{0.818}} & {\\color[HTML]{FF0000} \\textbf{0.759}} & {\\color[HTML]{FF0000} \\textbf{0.038}} & {\\color[HTML]{FF0000} \\textbf{0.719}} & {\\color[HTML]{FF0000} \\textbf{0.878}} & {\\color[HTML]{FF0000} \\textbf{0.803}} & {\\color[HTML]{FF0000} \\textbf{0.052}} & {\\color[HTML]{FF0000} \\textbf{0.802}} & {\\color[HTML]{FF0000} \\textbf{0.886}} & {\\color[HTML]{FF0000} \\textbf{0.829}} \\\\ \\midrule\n\\multicolumn{18}{c}{Point Supervision} \\\\ \\midrule\nSAM~\\cite{kirillov2023segment} & \\multicolumn{1}{c|}{---} & 0.207 & 0.595                                 & 0.647                                 & 0.635                                 & 0.160                                 & 0.597                                 & 0.639                                 & 0.643                                 & 0.093                                 & 0.673                                 & 0.737                                 & 0.730                                 & 0.118                                 & 0.675                                 & 0.723                                 & 0.717                                 \\\\\nSAM-P~\\cite{kirillov2023segment} & \\multicolumn{1}{c|}{---} & 0.101 & 0.696                                 & 0.745                                 & 0.697                                 & {\\color[HTML]{00B0F0} \\textbf{0.123}}                                 & 0.649                                 & {\\color[HTML]{00B0F0} \\textbf{0.693}}                                 & 0.677                                 & 0.069                                 & {\\color[HTML]{00B0F0} \\textbf{0.694}}                                 & 0.796                                 & {\\color[HTML]{00B0F0} \\textbf{0.765}}                                 & 0.082                                 & 0.728                                 & 0.786                                 & {\\color[HTML]{00B0F0} \\textbf{0.776}}                                 \\\\\nWSSA~\\cite{zhang2020weakly}                                          & CVPR20                                             & 0.105                                 & 0.660                                 & 0.712                                 & 0.711                                 & 0.148                                 & 0.607                                 & 0.652                                 & 0.649                                 & 0.087                                 & 0.509                                 & 0.733                                 & 0.642                                 & 0.104                                 & 0.688                                 & 0.756                                 & 0.743                                 \\\\\nSCWS~\\cite{yu2021structure}                                       & AAAI21                                             & 0.097                                 & 0.684                                 & 0.739                                 & 0.714                                 & 0.142                                 & 0.624                                 & 0.672                                 & {\\color[HTML]{00B0F0} \\textbf{0.687}}                                 & 0.082                                 & 0.593                                 & 0.777                                 & 0.738                                 & 0.098                                 & 0.695                                 & 0.767                                 & 0.754                                 \\\\\nTEL~\\cite{liang2022tree}                                           & CVPR22                                             & 0.094                                 & {\\color[HTML]{00B0F0} \\textbf{0.712}}                                 & 0.751                                 & {\\color[HTML]{00B0F0} \\textbf{0.746}}                                 & 0.133                                 & {\\color[HTML]{00B0F0} \\textbf{0.662}}                                 & 0.674                                 & 0.645                                 & 0.063                                 & 0.623                                 & 0.803                                 & 0.727                                 & 0.085                                 & 0.725                                 & 0.795                                 & 0.766                                 \\\\\nSCOD~\\cite{he2022weakly}                                         & AAAI23                                             & 0.092 & 0.688                                 & 0.746                                 & 0.725                                 & 0.137                                 & 0.629                                 & 0.688                                 & 0.663                                 & 0.060                                 & 0.607                                 & 0.802                                 & 0.711                                 & 0.080                                 & 0.744                                 & 0.796                                 & 0.758                                 \\\\\n\\rowcolor{c2!20} SCOD+ & \\multicolumn{1}{c|}{---}& {\\color[HTML]{00B0F0} \\textbf{0.089}} & 0.704 & {\\color[HTML]{00B0F0} \\textbf{0.757}} & 0.731 & 0.129 & 0.642  & {\\color[HTML]{00B0F0} \\textbf{0.693}}  & 0.666 & {\\color[HTML]{00B0F0} \\textbf{0.058}} & 0.618 & {\\color[HTML]{00B0F0} \\textbf{0.812}} & 0.719 & {\\color[HTML]{00B0F0} \\textbf{0.075}} & {\\color[HTML]{00B0F0} \\textbf{0.767}} & {\\color[HTML]{00B0F0} \\textbf{0.825}} & 0.771      \\\\\n\\rowcolor{c2!20}Ours & \\multicolumn{1}{c|}{---} & {\\color[HTML]{FF0000} \\textbf{0.056}} & {\\color[HTML]{FF0000} \\textbf{0.767}} & {\\color[HTML]{FF0000} \\textbf{0.868}} & {\\color[HTML]{FF0000} \\textbf{0.805}} & {\\color[HTML]{FF0000} \\textbf{0.102}} & {\\color[HTML]{FF0000} \\textbf{0.703}} & {\\color[HTML]{FF0000} \\textbf{0.757}} & {\\color[HTML]{FF0000} \\textbf{0.718}} & {\\color[HTML]{FF0000} \\textbf{0.039}} & {\\color[HTML]{FF0000} \\textbf{0.698}} & {\\color[HTML]{FF0000} \\textbf{0.856}} & {\\color[HTML]{FF0000} \\textbf{0.790}} & {\\color[HTML]{FF0000} \\textbf{0.057}} & {\\color[HTML]{FF0000} \\textbf{0.801}} & {\\color[HTML]{FF0000} \\textbf{0.859}} & {\\color[HTML]{FF0000} \\textbf{0.813}} \\\\ \n\\bottomrule\n\\end{tabular}}\n\\label{table:CODQuanti}\n\\vspace{-0.5cm}\n\\end{table}\n\n\\begin{table}[t]\n\\centering\n\\caption{Results for PIS and TOD with point supervision. }\n\\resizebox{\\columnwidth}{!}{\n\\setlength{\\tabcolsep}{0.8mm}\n\\begin{tabular}{l|cccc|cccc|cccc|cccc|cccc}\n\\toprule\n& \\multicolumn{12}{c|}{Polyp Image Segmentation (PIS)}& \\multicolumn{8}{c}{Transparant Object Detection (TOD)}\\\\ \\cline{2-21}\n& \\multicolumn{4}{c|}{\\textit{CVC-ColonDB}}& \\multicolumn{4}{c|}{\\textit{ETIS}}& \\multicolumn{4}{c|}{\\textit{Kvasir}}& \\multicolumn{4}{c|}{\\textit{GDD}}& \\multicolumn{4}{c}{\\textit{GSD}}\\\\ \\cline{2-21}\n\\multirow{-3}{*}{Methods} & {\\cellcolor{gray!40}$M$~$\\downarrow$} &{\\cellcolor{gray!40}$F_\\beta$~$\\uparrow$} &{\\cellcolor{gray!40}$E_\\phi$~$\\uparrow$} & \\multicolumn{1}{c|}{\\cellcolor{gray!40}$S_\\alpha$~$\\uparrow$}& {\\cellcolor{gray!40}$M$~$\\downarrow$} &{\\cellcolor{gray!40}$F_\\beta$~$\\uparrow$} &{\\cellcolor{gray!40}$E_\\phi$~$\\uparrow$} & \\multicolumn{1}{c|}{\\cellcolor{gray!40}$S_\\alpha$~$\\uparrow$}& {\\cellcolor{gray!40}$M$~$\\downarrow$} &{\\cellcolor{gray!40}$F_\\beta$~$\\uparrow$} &{\\cellcolor{gray!40}$E_\\phi$~$\\uparrow$} & \\multicolumn{1}{c|}{\\cellcolor{gray!40}$S_\\alpha$~$\\uparrow$}& {\\cellcolor{gray!40}$M$~$\\downarrow$} &{\\cellcolor{gray!40}$F_\\beta$~$\\uparrow$} &{\\cellcolor{gray!40}$E_\\phi$~$\\uparrow$} & \\multicolumn{1}{c|}{\\cellcolor{gray!40}$S_\\alpha$~$\\uparrow$}& {\\cellcolor{gray!40}$M$~$\\downarrow$} &{\\cellcolor{gray!40}$F_\\beta$~$\\uparrow$} &{\\cellcolor{gray!40}$E_\\phi$~$\\uparrow$} & \\multicolumn{1}{c}{\\cellcolor{gray!40}$S_\\alpha$~$\\uparrow$}\\\\ \\midrule\nSAM~\\cite{kirillov2023segment}& 0.479                                 & 0.343                                 & 0.419                                 & 0.427                                 & 0.429                                 & 0.439                                 & 0.512                                 & 0.503                                 & 0.320                                 & 0.545                                 & 0.564                                 & 0.582                                 & 0.245                                 & 0.512                                 & 0.530                                 & 0.551                                 & 0.266                                 & 0.473                                 & 0.501                                 & 0.514                                 \\\\\nSAM-P~\\cite{kirillov2023segment}& 0.194                                 & 0.587                                 & 0.664                                 & 0.671                                 & 0.144                                 & 0.625                                 & 0.719                                 & 0.715                                 & 0.108                                 & 0.793                                 & 0.811                                 & 0.802                                 & 0.164                                 & 0.668                                 & 0.715                                 & 0.625                                 & 0.177                                 & 0.687                                 & 0.730                                 & 0.668                                 \\\\\nWSSA~\\cite{zhang2020weakly} & 0.127                                 & 0.645                                 & 0.732                                 & 0.713                                 & 0.123                                 & 0.647                                 & 0.733                                 & 0.762                                 & 0.082                                 & 0.822                                 & 0.852                                 & 0.828                                 & 0.173                                 & 0.652                                 & 0.710                                 & 0.616                                 & 0.185                                 & 0.661                                 & 0.712                                 & 0.650                                 \\\\\nSCWS~\\cite{yu2021structure}                      & 0.082                                 & 0.674                                 & 0.758                                 & 0.787                                 & 0.085                                 & 0.646                                 & 0.768                                 & 0.731                                 & 0.078                                 & 0.837                                 & 0.860                                 & 0.831                                 & 0.170                                 & 0.631                                 & 0.702                                 & 0.613                                 & 0.172                                 & 0.706                                 & 0.738                                 & 0.673                                 \\\\\nTEL~\\cite{liang2022tree}                       & 0.089                                 & 0.669                                 & 0.743                                 & 0.761                                 & 0.083                                 & 0.639                                 & 0.776                                 & 0.726                                 & 0.091                                 & 0.810                                 & 0.826                                 & 0.804                                 & 0.230                                 & 0.640                                 & 0.586                                 & 0.536                                 & 0.275                                 & 0.571                                 & 0.501                                 & 0.495                                 \\\\\nSCOD~\\cite{he2022weakly}                      & 0.077                                 & 0.691                                 & 0.795                                 & 0.802                                 & 0.071                                 & 0.664                                 & 0.802                                 & 0.766                                 & 0.071                                 & 0.853                                 & 0.877                                 & {\\color[HTML]{00B0F0} \\textbf{0.836}} & 0.146                                 & 0.801                                 & 0.778                                 & 0.723                                 & 0.154                                 & 0.743                                 & 0.751                                 & 0.710                                 \\\\\n\\rowcolor{c2!20}SCOD+              & {\\color[HTML]{00B0F0} \\textbf{0.074}} & {\\color[HTML]{00B0F0} \\textbf{0.702}} & {\\color[HTML]{00B0F0} \\textbf{0.806}} & {\\color[HTML]{00B0F0} \\textbf{0.803}} & {\\color[HTML]{00B0F0} \\textbf{0.066}} & {\\color[HTML]{00B0F0} \\textbf{0.670}} & {\\color[HTML]{00B0F0} \\textbf{0.811}} & {\\color[HTML]{00B0F0} \\textbf{0.769}} & {\\color[HTML]{00B0F0} \\textbf{0.068}} & {\\color[HTML]{00B0F0} \\textbf{0.860}} & {\\color[HTML]{00B0F0} \\textbf{0.880}} & {\\color[HTML]{00B0F0} \\textbf{0.836}} & {\\color[HTML]{00B0F0} \\textbf{0.129}} & {\\color[HTML]{00B0F0} \\textbf{0.818}} & {\\color[HTML]{00B0F0} \\textbf{0.796}} & {\\color[HTML]{00B0F0} \\textbf{0.732}} & {\\color[HTML]{00B0F0} \\textbf{0.145}} & {\\color[HTML]{00B0F0} \\textbf{0.761}} & {\\color[HTML]{00B0F0} \\textbf{0.765}} & {\\color[HTML]{00B0F0} \\textbf{0.720}} \\\\\n\\rowcolor{c2!20}Ours                & {\\color[HTML]{FF0000} \\textbf{0.043}} & {\\color[HTML]{FF0000} \\textbf{0.721}} & {\\color[HTML]{FF0000} \\textbf{0.839}} & {\\color[HTML]{FF0000} \\textbf{0.816}} & {\\color[HTML]{FF0000} \\textbf{0.037}} & {\\color[HTML]{FF0000} \\textbf{0.694}} & {\\color[HTML]{FF0000} \\textbf{0.849}} & {\\color[HTML]{FF0000} \\textbf{0.797}} & {\\color[HTML]{FF0000} \\textbf{0.046}} & {\\color[HTML]{FF0000} \\textbf{0.878}} & {\\color[HTML]{FF0000} \\textbf{0.917}} & {\\color[HTML]{FF0000} \\textbf{0.877}} & {\\color[HTML]{FF0000} \\textbf{0.078}} & {\\color[HTML]{FF0000} \\textbf{0.858}} & {\\color[HTML]{FF0000} \\textbf{0.863}} & {\\color[HTML]{FF0000} \\textbf{0.775}} & {\\color[HTML]{FF0000} \\textbf{0.089}} & {\\color[HTML]{FF0000} \\textbf{0.839}} & {\\color[HTML]{FF0000} \\textbf{0.841}} & {\\color[HTML]{FF0000} \\textbf{0.764}} \\\\\\bottomrule\n\\end{tabular}} \\label{table:MISTOD_Quanti}\n\\vspace{-0.5cm}\n\\end{table}\n\n\\section{Experiments}\n\\subsection{Experimental Setup}\n\\noindent \\textbf{Implementation details}. \nThe image encoder uses ResNet50 as the backbone and is pre-trained on ImageNet~\\cite{deng2009imagenet}. The batch size is 36 and the learning rate is initialized as 0.0001, decreased by 0.1 every 80 epochs. \nFor scribble supervision, we propose a nine-box strategy, namely constructing the minimum outer wrapping rectangle of the foreground/background scribble and dividing it into a nine-box grid, to sample one point in each box and send them to SAM for segmentation mask generation.\nFollowing~\\cite{fan2020camouflaged}, all images are resized as $352\\times352$ in both the training and testing phases. \nFor SAM~\\cite{kirillov2023segment}, we adopt the ViT-H SAM model to generate segmentation masks. \nWe implement our method with PyTorch and run experiments on two RTX3090 GPUs. \n\n\\noindent \\textbf{Baselines}. \nWe explore SAM \\cite{kirillov2023segment} for the WSCOS task by generating segmentation masks with sparse annotations as prompts and using the segmentation masks to train a COS segmenter. However, a more straightforward way to explore SAM for this task is to use the sparse annotation to fine-tune SAM and then directly apply SAM for the test. To verify the advantages of our method over this direct way, we construct two baseline methods, SAM-S and SAM-P, which fine-tune the mask decoder of SAM with scribble and point supervisions, respectively, by the partial cross-entropy loss. We will show the results of these two baselines in our comparative evaluations. For reference, we also report the results of the vanilla SAM. When applying SAM and its variants, SAM-S and SAM-P, on test images, we use the automatic prompt generation strategy and report the results with the highest IoU scores. \n\n\\noindent \\textbf{Metrics}. Following existing methods~\\cite{fan2021concealed,fan2020camouflaged}, we use four common metrics for evaluation, including mean absolute error ($M$), adaptive F-measure ($F_\\beta$)~\\cite{margolin2014evaluate}, mean E-measure ($E_\\phi$)~\\cite{fan2021cognitive}, and structure measure ($S_\\alpha$)~\\cite{fan2017structure}. Smaller $M$, or larger $F_\\beta$, $E_\\phi$, $S_\\alpha$ means better segmentation performance.\n\n\\begin{table}[t]\n\\begin{minipage}[c]{0.443\\textwidth}\n\\centering\n\\setlength{\\abovecaptionskip}{0cm}\n\\caption{Ablations for WS-SAM.}\n\\resizebox{\\columnwidth}{!}{\n\\setlength{\\tabcolsep}{1mm}\n\\begin{tabular}{cccc|cccc}\n\\toprule\nBaseline & MAF & PLW & ILS & $M$~$\\downarrow$ & $F_\\beta$~$\\uparrow$ & $E_\\phi$~$\\uparrow$ & $S_\\alpha$~$\\uparrow$ \\\\ \\midrule\n\\checkmark  &  &   &  & 0.052          & 0.674          & 0.838          & 0.737          \\\\\n\\checkmark & \\checkmark  &   &  & 0.047          & 0.689          & 0.853          & 0.772          \\\\\n\\checkmark &\\checkmark &\\checkmark  & & 0.044          & 0.697          & 0.866          & 0.793          \\\\\n\\rowcolor{c2!20}\\checkmark & \\checkmark&\\checkmark  &\\checkmark & \\textbf{0.038} & \\textbf{0.719} & \\textbf{0.878} & \\textbf{0.803} \\\\ \\bottomrule\n\\end{tabular}}\\label{table:AblationSAM}\n\\vspace{-0.6cm}\n\\end{minipage}\n\\begin{minipage}[c]{0.55\\textwidth}\n\\centering\n\\setlength{\\abovecaptionskip}{0cm}\n\\caption{Ablations for MFG. }\n\\resizebox{\\columnwidth}{!}{\n\\setlength{\\tabcolsep}{1mm}\n\\begin{tabular}{c|ccccc}\n\\toprule\nMetrics & w/o MFG & FG-\\textgreater{}SA & w/o multiscale & WGM-\\textgreater{}FC &\\cellcolor{c2!20} w/ MFG         \\\\ \\midrule\n$M$~$\\downarrow$  & 0.044   & 0.038               & 0.040          & 0.039                &\\cellcolor{c2!20} \\textbf{0.038} \\\\\n$F_\\beta$~$\\uparrow$ & 0.684   & 0.708               & 0.702          & 0.710                &\\cellcolor{c2!20} \\textbf{0.719} \\\\\n$E_\\phi$~$\\uparrow$ & 0.857   & 0.868               & 0.858          & 0.871                &\\cellcolor{c2!20} \\textbf{0.878} \\\\\n$S_\\alpha$~$\\uparrow$  & 0.780   & 0.797               & 0.783          & 0.792                &\\cellcolor{c2!20} \\textbf{0.803} \\\\ \\bottomrule\n\\end{tabular}}\\label{table:AblationMFG}\n\\vspace{-0.6cm}\n\\end{minipage}\n\\end{table}\n\n\\begin{table}[t]\n\\begin{minipage}[c]{0.54\\textwidth}\n\\centering\n\\setlength{\\abovecaptionskip}{0cm}\n\\caption{Results of MFG with full supervision.}\n\\resizebox{\\columnwidth}{!}{\n\\setlength{\\tabcolsep}{1mm}\n\\begin{tabular}{c|ccccc}\n\\toprule\nMetrics & Baseline & SegMaR~\\cite{jia2022segment} & PreyNet~\\cite{zhang2022preynet} & FGANet~\\cite{zhaiexploring} & \\cellcolor{c2!20}Ours   \\\\ \\midrule\n$M$~$\\downarrow$  & 0.035    & 0.035 & 0.034   & \\textbf{0.032}  & \\cellcolor{c2!20}\\textbf{0.032} \\\\\n$F_\\beta$~$\\uparrow$ & 0.688    & 0.699  & \\textbf{0.715}   & 0.708  & \\cellcolor{c2!20}0.706 \\\\\n$E_\\phi$~$\\uparrow$ & 0.879    & 0.890   & 0.894   & 0.894  & \\cellcolor{c2!20}\\textbf{0.897} \\\\\n$S_\\alpha$~$\\uparrow$     & 0.812    & \\textbf{0.813}  & \\textbf{0.813}   & 0.803  & \\cellcolor{c2!20}\\textbf{0.813} \\\\ \\bottomrule\n\\end{tabular}}\\label{table:FullSupervision}\n\\vspace{-0.6cm}\n\\end{minipage}\n\\begin{minipage}[c]{0.42\\textwidth}\n\\centering\n\\setlength{\\abovecaptionskip}{0cm}\n\\caption{Results on multi-object images.\n}\n\\resizebox{\\columnwidth}{!}{\n\\setlength{\\tabcolsep}{1mm}\n\\begin{tabular}{c|cccc}\n\\toprule\nMetrics & SCWS~\\cite{yu2021structure}  & TEL~\\cite{liang2022tree}   & SCOD~\\cite{he2022weakly}  & \\cellcolor{c2!20}Ours           \\\\ \\midrule\n$M$~$\\downarrow$    & 0.094 & 0.101 & 0.084 & \\cellcolor{c2!20}\\textbf{0.070} \\\\\n$F_\\beta$~$\\uparrow$ & 0.378 & 0.350 & 0.381 & \\cellcolor{c2!20}\\textbf{0.452} \\\\\n$E_\\phi$~$\\uparrow$ & 0.740 & 0.726 & 0.718 & \\cellcolor{c2!20}\\textbf{0.772} \\\\\n$S_\\alpha$~$\\uparrow$     & 0.625 & 0.617 & 0.643 & \\cellcolor{c2!20}\\textbf{0.687} \\\\ \\bottomrule\n\\end{tabular}}\\label{table:Multi-object}\n\\vspace{-0.6cm}\n\\end{minipage}\n\\end{table}\n\n\\subsection{Comparative Evaluation}\nWe perform evaluations on the following COS tasks, namely,  Camouflaged Object Detection, Polyp Image Segmentation (PIS), and Transparent Object Detection (TOD). For all the tasks, we evaluate the performance with point annotations. We follow the previous weakly-supervised segmentation method \\cite{gao2022weakly} and randomly select two points (one from the foreground and one from the background) from the training masks as the point annotations. For COD, we additionally evaluate the performance using scribble annotations, using the scribble data provided in \\cite{gao2022weakly}.       \n\n\\noindent\\textbf{Camouflaged object detection}.\nFour datasets are used for experiments, \\textit{i.e.}, \\textit{CHAMELEON}~\\cite{skurowski2018animal}, \\textit{CAMO}~\\cite{le2019anabranch}, \\textit{COD10K}~\\cite{fan2021concealed}, and \\textit{NC4K}~\\cite{lv2021simultaneously}. Table~\\ref{table:CODQuanti} shows that our method reaches the best performance over all competing methods and baselines. Notably, while SAM has shown impressive performance for natural scene images, its performance on the challenging COD task is far from the existing methods particularly designed for this task. We do see performance gains after finetuning SAM with point (SAM-P) and scribble (SAM-S) supervision, but the results are still far below our method. This substantiates the superiority of our way of leveraging SAM to generate segmentation masks with sparse annotations and use the segmentation masks to train the segmenter. To verify our performance improvement over the existing WSCOS methods does not merely come from the usage of SAM, we integrate the most recent WSCOS method, SCOD~\\cite{he2022weakly}, into our WS-SAM framework to also leverage the additional mask supervision. This results in the method, ``SCOD+''. We can see that our method still shows better performance, further verifying our advantages for this task. \n\n\\noindent\\textbf{Polyp image segmentation}.\nThree widely-used Polyp datasets are selected, namely \\textit{CVC-ColonDB}~\\cite{tajbakhsh2015automated}, \\textit{ETIS}~\\cite{silva2014toward}, and \\textit{Kvasir}~\\cite{jha2020kvasir}. Table~\\ref{table:MISTOD_Quanti} shows that our method significantly  surpasses the second-best method, SCOD, with point supervision. SAM and SAM-P do not perform well on this task, further substantiating their weakness in this challenging segmentation task. \nWhile empowering SCOD with the proposed WS-SAM framework indeed improves the performance, the results are still lower than our method. This again verifies our benefit in handling challenging segmentation tasks.  \n\n\\noindent\\textbf{Transparent object detection}.\nTwo datasets, GDD~\\cite{mei2020don} and GSD~\\cite{lin2021rich}, are used for evaluation. As shown in Table~\\ref{table:MISTOD_Quanti}, our method surpasses all baseline methods and existing methods for this task as well. This shows the strong robustness and generalizability of our proposed method. \n\n\\subsection{Ablation Study} \\vspace{-1mm}\nOur method includes two main components, the SAM-based weakly-supervised mask generation framework, WS-SAM, and the multi-scale feature grouping (MFG) module. We conduct ablation studies about these two components on \\textit{COD10K} of the COD task with scribble supervision. \n\n\\noindent \\textbf{Ablation study for WS-SAM}. \nWe establish a baseline by using SAM to generate only one segmentation mask from one training image without augmentations for model training. On top of this baseline, we add multi-augmentation fusion (MAF),  pixel-level weighting (PLW), and image-level selection (ILS) techniques. Table~\\ref{table:AblationSAM} showing adding these components helps improve the performance, thus demonstrating their effectiveness.  \n\n\\noindent \\textbf{Ablation study for MFG}.\nWe examine the effect of MFG by completely removing the MFG module, substituting the proposed feature grouping (FG) with slot attention (SA) \\cite{locatello2020object}, removing the multi-scale strategy, and substituting the weighted gate mechanism (WGM) with a fixed coefficient (FC). Table~\\ref{table:AblationMFG} shows that our designs reach better performance than the alternative ones. \n\n\\subsection{Further Analysis}\\vspace{-1mm}\n\n\\noindent\\textbf{MFG for the \\emph{fully-supervised} setting}.\nThe proposed MFG module is designed to evacuate discriminative cues from concealed images. We have demonstrated its effectiveness with sparse annotations for the weakly-supervised setting. However, it is expected to also work in the fully-supervised setting. To verify this,  we conduct experiments for the standard fully-supervised COD task. Table~\\ref{table:FullSupervision} shows the results on the \\textit{COD10K} dataset. We can see that MFG indeed helps improve the performance of the baseline model, to the level comparative with state-of-the-art methods.  \n\n \n\\noindent \\textbf{Performance on multi-object images}. \nThe proposed MFG module evacuates discriminative cues by performing feature grouping at different granularities, which facilitates discovering multiple objects in images. To verify this, we evaluate the performance on the 186 images with more than one object from $COD10K$. Table~\\ref{table:Multi-object} shows that MFG achieves the best performance, surpassing the second-best method (SCOD) by $10.7\\%$. This gap is large than that with all test images, where the gap is $5.8\\%$. \n\n\\begin{table}[t]\n\\centering\n\\caption{Parameter analysis on $K$, $\\tau_a$, $\\tau_r$, $T$, and $(N_1,N_2)$.}\n\\resizebox{1\\columnwidth}{!}{\n\\setlength{\\tabcolsep}{0.8mm}\n\\begin{tabular}{c|cccc|cccc|cccc|cccc|cccc}\n\\toprule\n\\multicolumn{1}{l|}{\\multirow{2}{*}{Metrics}} & \\multicolumn{4}{c|}{$K$}  & \\multicolumn{4}{c|}{$\\tau_a$} & \\multicolumn{4}{c|}{$\\tau_r$}  & \\multicolumn{4}{c|}{$T$} & \\multicolumn{4}{c}{$(N_1,N_2)$} \\\\\\cline{2-21}\n\\multicolumn{1}{l|}{}  & 1     & 6     & \\cellcolor{c2!20}12             & 18             & 0.05           & \\cellcolor{c2!20}0.1            & 0.2            & 0.3   & 0.3            & \\cellcolor{c2!20}0.5            & 0.7   & 0.9   & 1     & 2     & \\cellcolor{c2!20}3              & 4              & \\cellcolor{c2!20}(2,4)          & (2,8)          & (4,8) & (2,4,8)        \\\\ \\midrule\n$M$~$\\downarrow$ & 0.052 & 0.042 & \\cellcolor{c2!20}\\textbf{0.038} & 0.039          & \\textbf{0.037} & \\cellcolor{c2!20}0.038          & 0.038          & 0.040 & \\textbf{0.038} & \\cellcolor{c2!20}\\textbf{0.038} & 0.039 & 0.040 & 0.039 & 0.039 & \\cellcolor{c2!20}\\textbf{0.038} & \\textbf{0.038} & \\cellcolor{c2!20}\\textbf{0.038} & \\textbf{0.038} & 0.039 & \\textbf{0.038} \\\\\n$F_\\beta$~$\\uparrow$ & 0.674 & 0.697 & \\cellcolor{c2!20}\\textbf{0.719} & 0.718          & 0.706          & \\cellcolor{c2!20}\\textbf{0.719} & 0.716          & 0.704 & \\textbf{0.723} & \\cellcolor{c2!20}0.719          & 0.715 & 0.700 & 0.706 & 0.715 & \\cellcolor{c2!20}0.719          & \\textbf{0.720} & \\cellcolor{c2!20}0.719          & 0.714          & 0.711 & \\textbf{0.721} \\\\\n$E_\\phi$~$\\uparrow$ & 0.838 & 0.857 & \\cellcolor{c2!20}\\textbf{0.878} & \\textbf{0.878} & 0.868          & \\cellcolor{c2!20}\\textbf{0.878} & 0.876          & 0.865 & 0.866          & \\cellcolor{c2!20}\\textbf{0.878} & 0.874 & 0.851 & 0.862 & 0.872 & \\cellcolor{c2!20}\\textbf{0.878} & 0.876          & \\cellcolor{c2!20}\\textbf{0.878} & 0.875          & 0.873 & \\textbf{0.878} \\\\\n$S_\\alpha$~$\\uparrow$ & 0.737 & 0.776 & \\cellcolor{c2!20}0.803          & 0.800          & 0.795          & \\cellcolor{c2!20}0.803          & \\textbf{0.805} & 0.793 & 0.792          & \\cellcolor{c2!20}\\textbf{0.803} & 0.789 & 0.781 & 0.794 & 0.800 & \\cellcolor{c2!20}0.803          & \\textbf{0.805} & \\cellcolor{c2!20}\\textbf{0.803} & 0.802          & 0.799 & 0.802        \\\\ \\bottomrule                        \n\\end{tabular}}\\label{table:ParameterAnalysis}\n\\vspace{-0.3cm}\n\\end{table}\n\n\\begin{figure}[t]\n\t\\centering\n\t\\setlength{\\abovecaptionskip}{-0.2cm}\n\t\\begin{center}\n\t\t\\includegraphics[width=\\linewidth]{Figure/COSQuali.pdf}\n\t\\end{center}\n\t\\caption{Visualized results for the three WSCOS tasks.}\n\t\\label{fig:COSQuali}\n\t\\vspace{-0.5cm}\n\\end{figure}\n\n\\noindent \\textbf{Randomness of point supervision}. \nWe follow the existing point-supervision\nsegmentation methods and randomly select points from ground truth masks as the point annotation.\n\\begin{wrapfigure}[8]{r}{0.3\\textwidth}%{3.3cm}\n\\vspace{-5mm}\n\\centering \n\t\\setlength{\\abovecaptionskip}{-0.2cm}\n\t\\begin{center}\n\t\t\\includegraphics[width=1\\linewidth]{Figure/PointSupervision.pdf}\n\t\\end{center}\n\t\\caption{ Five runs results with varied point annotations. }\n\t\\label{fig:PointSupervision}\n\\end{wrapfigure} To study the variance of the random selection, we repeat the random selection 5 times and calculate the mean and standard deviation of the results. Fig.~\\ref{fig:PointSupervision} shows that our method reaches the best results while having the smallest deviation.  \n\n\\noindent\\textbf{Number of augmented views $K$}.\nTable~\\ref{table:ParameterAnalysis} shows that more augmented views help improve performance in the beginning, but the effect turns weaker when further increasing it. \n\n\\noindent\\textbf{Hyperparameters in image-level selection}.\nTable~\\ref{table:ParameterAnalysis} shows that it is best to set the absolute uncertainty threshold $\\tau_{a}=0.1$ and the relative uncertainty threshold $\\tau_{r}=0.5$, and the proposed method is not sensitive to these two parameters.   \n\n\\noindent\\textbf{Hyperparameters in MFG}. \nTable~\\ref{table:ParameterAnalysis} shows that MFG achieves the best  results when the iteration number $T$ is set as 3, and the groups and scales setting is set as $(N_1, N_2)=(2,4)$. Notice that when adopting $(2,4,8)$, RK2 is replaced with the third-order RK structure, resulting in extra computational burden with limited benefits. Hence, we select the RK2 structure with $(N_1, N_2)=(2,4)$.\n\n\\noindent \\textbf{Result visualization.}\nFig.~\\ref{fig:COSQuali} shows the prediction maps with point supervision. We can see that our method produces more complete results than existing methods and localizes multiple objects more comprehensively. More visualization results can be found in the supplementary materials.  \n\n\\vspace{-2mm}\n\\section{Conclusions} \\vspace{-1mm}\nThis paper proposes a new WSCOS method that includes two key components. The first one is the WS-SAM framework that generates segmentation masks with the recently proposed vision foundation model, SAM, and proposes multi-augmentation result fusion, pixel-level uncertainty weighting, and image-level uncertainty filtration to get reliable pseudo labels to train a segmentation model. The second is the MFG module that leverages the extracted clues for additional nuanced discrimination information. MFG improves feature coherence from a grouping aspect, allowing for alleviating incomplete segmentation and better multiple-object segmentation. Experiments on multiple WSCOS tasks confirm the superiority of our method over the baseline and existing methods.  \n\n\\clearpage\n{\\small\n}\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2112.10003v2.tex",
        "arXiv-2305.10724v1.tex",
        "arXiv-2305.11003v1.tex"
    ],
    "group_id": "group_87",
    "response": "### Summary: Image Segmentation and Anomaly Detection Using Foundation Models\n\n#### Introduction\n\nImage segmentation and anomaly detection are fundamental tasks in computer vision that have seen significant advancements with the advent of deep learning techniques. Historically, these tasks required extensive training datasets and were often constrained to specific object classes or anomaly types. However, recent developments in foundation models, such as CLIP and SAM, have enabled more flexible and generalizable approaches to these problems. Foundation models, like CLIP and SAM, are pre-trained on large-scale datasets and can be adapted to various downstream tasks through prompt-based mechanisms. This has opened up new possibilities for tackling segmentation and anomaly detection tasks without the need for extensive task-specific training data. Despite these advancements, challenges remain, particularly in handling unseen classes, complex queries, and intrinsic similarities between foreground and background, which can lead to incomplete or inaccurate segmentations.\n\nThe field of image segmentation has evolved from traditional methods like Mask-RCNN to more recent transformer-based architectures like TransUNet and Segformer. These models leverage the power of deep learning to generate accurate segmentations but often require re-training for new classes or tasks. Similarly, anomaly detection has traditionally relied on supervised or semi-supervised methods, which can be costly and time-consuming, especially in industrial settings where anomalies are rare and diverse. Recent works have explored unsupervised and weakly-supervised methods to reduce the need for extensive labeled data, but these methods still struggle with generalization to unseen anomalies and the intrinsic similarity between foreground and background.\n\nThis summary focuses on three recent research papers that address these challenges using foundation models. The first paper introduces CLIPSeg, a model that uses CLIP as a backbone to generate segmentations based on text or image prompts. The second paper presents SAA+, a framework for zero-shot anomaly segmentation that incorporates hybrid prompts derived from domain expert knowledge and target image context. The third paper proposes WS-SAM, a method for weakly-supervised concealed object segmentation that leverages SAM to generate pseudo labels and uses a multi-scale feature grouping (MFG) module to improve segmentation coherence.\n\n#### Main Content of Each Paper\n\n**Paper 1: CLIPSeg - Image Segmentation Using Text and Image Prompts**\n\nCLIPSeg is a novel approach to image segmentation that uses CLIP as a backbone and extends it with a transformer-based decoder. CLIP, a vision-language model, is pre-trained on a large dataset and can generate embeddings for both images and text. CLIPSeg leverages these embeddings to condition the segmentation decoder on either text or image prompts, enabling the model to perform segmentation tasks without re-training for new classes or queries. The authors train CLIPSeg on an extended version of the PhraseCut dataset, which includes structured text queries and visual samples, and evaluate its performance on three common segmentation tasks: referring expression segmentation, zero-shot segmentation, and one-shot segmentation. They find that CLIPSeg performs competitively on these tasks, especially when trained on a diverse dataset like PhraseCut. The model is also capable of generalizing to new prompts involving unseen words or properties, demonstrating its versatility.\n\n**Paper 2: SAA+ - Segment Any Anomaly without Training via Hybrid Prompt Regularization**\n\nSAA+ is a framework designed for zero-shot anomaly segmentation, which aims to segment anomalies in images without any training data. The authors construct a baseline model, SAA, by cascading a language-driven object detection model (GroundingDINO) and a segmentation model (SAM). SAA uses naive language prompts, such as \"anomaly,\" to generate bounding box-level anomaly region proposals, which are then refined into pixel-wise segmentation masks. However, SAA suffers from significant false alarms due to language ambiguity, leading to the introduction of SAA+. SAA+ incorporates hybrid prompts derived from domain expert knowledge and target image context to regularize the segmentation process. Domain expert knowledge provides more specific language prompts, while target image context includes saliency maps and confidence rankings to calibrate predictions. The authors evaluate SAA+ on several anomaly segmentation benchmarks, including VisA, MVTec-AD, KSDD2, and MTD, and find that it achieves state-of-the-art performance in the zero-shot setting. This framework demonstrates the potential of foundation models in anomaly detection tasks, especially when combined with domain-specific knowledge.\n\n**Paper 3: WS-SAM - Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping**\n\nWS-SAM is a method for weakly-supervised concealed object segmentation, which aims to segment objects that are visually blended with their surroundings using sparse annotations. The authors propose using SAM to generate dense segmentation masks from sparse annotations and introduce a multi-scale feature grouping (MFG) module to improve segmentation coherence. WS-SAM employs several strategies to refine the pseudo labels generated by SAM, including multi-augmentation result fusion, entropy-based pixel-level weighting, and entropy-based image-level selection. These strategies help alleviate the impact of low-quality segmentation masks and provide reliable supervision for the segmenter. The authors evaluate WS-SAM on various concealed object segmentation tasks, such as camouflaged object detection, polyp image segmentation, and transparent object detection, and find that it achieves state-of-the-art performance. WS-SAM also shows strong robustness and generalizability, performing well on both single and multi-object images.\n\n#### Commonalities and Innovations\n\nAll three papers leverage foundation models to address segmentation and anomaly detection tasks in a flexible and generalizable manner. CLIPSeg uses CLIP embeddings to condition a segmentation decoder on text or image prompts, enabling the model to handle a wide range of segmentation tasks without re-training. SAA+ and WS-SAM both use SAM to generate segmentation masks from sparse annotations, but they differ in their approaches to refining these masks. SAA+ incorporates hybrid prompts derived from domain expert knowledge and target image context to regularize the segmentation process, while WS-SAM uses multi-augmentation result fusion, pixel-level weighting, and image-level selection to refine the pseudo labels generated by SAM. Additionally, WS-SAM introduces a multi-scale feature grouping module to improve segmentation coherence, which is particularly useful for concealed object segmentation tasks.\n\nThe innovations in these papers lie in their ability to adapt foundation models to new tasks without extensive re-training. CLIPSeg demonstrates the versatility of CLIP in handling different types of prompts and generalizing to unseen queries, while SAA+ and WS-SAM showcase the potential of SAM in zero-shot and weakly-supervised settings. These approaches highlight the importance of prompt design and feature coherence in adapting foundation models to specific tasks, providing a promising direction for future research in computer vision.\n\n#### Comparison of Results\n\nThe performance of CLIPSeg, SAA+, and WS-SAM is evaluated on different benchmarks and tasks. CLIPSeg achieves competitive performance on referring expression, zero-shot, and one-shot segmentation tasks, demonstrating its versatility in handling various prompts and unseen classes. SAA+ outperforms existing methods on several anomaly segmentation benchmarks, including VisA, MVTec-AD, KSDD2, and MTD, in the zero-shot setting, highlighting the effectiveness of hybrid prompts in regularizing foundation models. WS-SAM surpasses existing methods on various concealed object segmentation tasks, such as camouflaged object detection, polyp image segmentation, and transparent object detection, with point and scribble supervision, showing strong robustness and generalizability.\n\nWhile CLIPSeg focuses on adapting to new prompts and unseen classes, SAA+ and WS-SAM address the challenges of zero-shot and weakly-supervised anomaly segmentation, respectively. SAA+ achieves state-of-the-art performance by leveraging domain expert knowledge and target image context, while WS-SAM uses SAM to generate high-quality pseudo labels and introduces a multi-scale feature grouping module to improve segmentation coherence. The results indicate that these methods are effective in their respective domains, but they also highlight the limitations of existing foundation models in handling specific tasks, such as concealed object segmentation.\n\n#### Conclusion\n\nThe main findings of these papers are the successful adaptation of foundation models to image segmentation and anomaly detection tasks without extensive re-training. CLIPSeg demonstrates the versatility of CLIP in handling different types of prompts and generalizing to unseen queries, while SAA+ and WS-SAM showcase the potential of SAM in zero-shot and weakly-supervised settings. These approaches provide a promising direction for future research in computer vision, particularly in the areas of prompt design and feature coherence.\n\nFuture research directions include exploring the integration of more modalities, such as sound and touch, into these models, as well as adapting them to video tasks. Additionally, further investigation into the scaling effects of foundation models and the impact of different hyperparameters on performance is needed. The authors also suggest that more extensive evaluations on larger datasets and diverse applications could provide deeper insights into the capabilities and limitations of these methods. Overall, these papers contribute to the growing field of foundation models and their adaptation to specific tasks, paving the way for more flexible and generalizable computer vision systems.\n\n### References\n\n- CLIP: Learning Transferable Visual Models from Natural Language Supervision\n- SAM: Segment Anything\n- GroundingDINO: Marrying DINO with Grounded Pre-training for Open-set Object Detection\n- PraNet: Parallel Reverse Attention Network for Polyp Segmentation\n- SINet: Predator Network for Camouflaged Object Detection\n- GSDNet: Transparent Object Detection with Reflection Refinement Module\n- SCOD: Weakly-Supervised Camouflaged Object Detection\n- MFG: Multi-scale Feature Grouping Module\n- COD: Camouflaged Object Detection\n- PIS: Polyp Image Segmentation\n- TOD: Transparent Object Detection\n- WSCOS: Weakly-Supervised Concealed Object Segmentation\n- WS-SAM: Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping"
}