{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Large-scale Contrastive Language-Audio Pretraining with \\\\ Feature Fusion and Keyword-to-Caption Augmentation}\n\n\\begin{document}\n\n\\ninept \n\n\\maketitle\n\n\\begin{abstract}\nContrastive learning has shown remarkable success in the field of multimodal representation learning.\nIn this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural language descriptions. To accomplish this target, we first release \\texttt{LAION-Audio-630K}, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio encoders and text encoders. We incorporate the feature fusion mechanism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance\\blfootnote{*The first four authors have equal contribution. Marianna Nezhurina was not listed as the author in the paper of the ICASSP 2023 proceedings. We acknowledge her contribution to the LAION-Audio-630K collection.}. Third, we perform comprehensive experiments to evaluate our model across three tasks: text-to-audio retrieval, zero-shot audio classification, and supervised audio classification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zero-shot setting and is able to obtain performance comparable to models' results in the non-zero-shot setting. \\texttt{LAION-Audio-630K}\\footnote{Dataset: \\href{https://github.com/LAION-AI/audio-dataset/}{https://github.com/LAION-AI/audio-dataset/}} and the proposed model\\footnote{Model: \\href{https://github.com/LAION-AI/CLAP}{https://github.com/LAION-AI/CLAP}} are both available to the public. \n\\end{abstract}\n\\begin{keywords}\nContrastive Learning, Representation Learning, Text-to-Audio Retrieval, Audio Classification, Audio Dataset\n\\end{keywords}\n\n\\section{Introduction}\nAudio is one of the most common information types in the world alongside text and image data.\nHowever, different audio tasks typically require finely-annotated data, which limits the amount of available audio data due to the labor-intensive collection procedure. \nConsequently, designing an effective audio representation for many audio tasks without requiring a lot of supervision remains a challenge.\n\nThe contrastive learning paradigm is a successful solution for training a model on large-scale noisy data collected from internet. \nThe recently proposed Contrastive Language-Image Pretraining (CLIP) \\cite{clip} learns the correspondence between text and image by projecting them into a shared latent space. The training is conducted by regarding the ground-truth image-text pair as the positive sample and left as negative. \nIn contrast to training on unimodal data, CLIP is not constrained by data annotation and shows great robustness by achieving high accuracy in a zero-shot setting on out-of-domain variations of ImageNet dataset \\cite{imagenet}. \nAdditionally, CLIP shows great success in downstream tasks such as text-to-image retrieval and text-guided captioning. \nSimilar to vision, audio and natural languages also contain overlapping information.\nIn audio event classification task, for instance, some text descriptions of an event can be mapped to the corresponding audio.\nThese text descriptions share a similar meaning that could be learned together with the related audio to form an audio representation of crossmodal information. Additionally, training such a model requires simply paired audio and text data, which is easy to collect.\n\nSeveral recent studies \\cite{audioclip,clap,clap-retrieval,yuke-dcase,mmt,ml-act,wav2clip} have presented the prototype of the contrastive language-audio pretraining model for the text-to-audio retrieval task. \n\\cite{yuke-dcase} utilizes Pretrained Audio Neural Network (PANN) \\cite{pann} as the audio encoder, BERT \\cite{bert} as the text encoder, and several loss functions to evaluate the text-to-audio retrieval performance.\n\\cite{clap-retrieval} further ensemble HTSAT \\cite{hts-at} and RoBERTa \\cite{roberta} into the encoder list to further enhance performance.\nThen, \\cite{clap} investigates the effectiveness of the learned representation in the downstream task of audio classification.\nSome other studies, such as AudioClip \\cite{audioclip} and WaveCLIP \\cite{wav2clip}, focus more on the contrastive image-audio (or image-audio-language) pretraining model. \nAll these models show great potential for contrastive learning in the audio domain.\n\nNonetheless, current studies have not shown the full strength of the language-audio contrastive learning.\nFirst, the models mentioned above are trained on relatively small datasets, showing that large-scale data collection and augmentation for training are needed. Second, prior work lacks a full investigation of selections and hyperparameter settings of audio/text encoders, which is essential for determining the basic contrastive language-audio architecture.\nThird, the model struggles to accommodate varied audio lengths, particularly for the transformer-based audio encoder.\nThere should be a solution to handle audio inputs of variable-length.\nFinally, the majority of language-audio model studies focuses solely on text-to-audio retrieval without assessing their audio representations in downstream tasks. As a representation model, we expect more discoveries of its generalization ability to more downstream tasks.\n\nIn this paper, we make contributions to improve the dataset, model design and the experiment setting from above concerns:\n\\begin{itemize}[leftmargin=*]\n    \\item We release LAION-Audio-630K, currently the largest public audio caption dataset of 633,526 audio-text pairs. To facilitate the learning process, we employ the keyword-to-caption model to augment labels of AudioSet \\cite{audioset} into corresponding captions. This dataset can also contribute to other audio tasks.  \n    \n    \\item We construct a pipeline of contrastive language-audio pretraining. Two audio encoders and three text encoders are selected for testing. We employ feature fusion mechanisms to enhance the performance and enable our model to handle variable-length inputs.\n\n    \\item We conduct comprehensive experiments on the model, including the text-to-audio retrieval task, as well as zero-shot and supervised audio classification downstream tasks. We demonstrate that scaling of the dataset, keyword-to-caption augmentation, and feature fusion can improve the model's performance in different perspectives. It achieves the state-of-the-art (SOTA) in the text-to-audio retrieval and audio classification tasks, even comparable to the performance of supervised models.\n\\end{itemize}\nWe make both LAION-Audio-630K and the proposed model available to the public.\n\n\\section{LAION-Audio-630K and Training Dataset}\n\\begin{table}[t]\n\\centering\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{lcc}\n\\toprule\nDataset & Pairs & Audio Durations (hrs)   \\\\\n\\midrule \nClotho\\cite{clotho} & 5,929 & 37.00  \\\\\nSoundDescs\\cite{audiotextbenchmark} & 32,979 & 1060.40 \\\\\nAudioCaps\\cite{audiocaps} & 52,904 & 144.94 \\\\\nLAION-Audio-630K & 633,526 & 4325.39 \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{LAION-Audio-630K compared with existing datasets.}\n\\vspace{-0.8cm}\n\\label{tab:LAION-Audio-630K scale comparation}\n\\end{table}\n\\subsection{LAION-Audio-630K}\\label{sec:dataset}\n\nWe collect LAION-Audio-630K, a large-scale audio-text dataset consisting of 633,526 pairs with the total duration of 4,325.39 hours. \nIt contains audios of human activities, natural sounds and audio effects, consisting of 8 data sources from publicly available websites\\footnote{Dataset details are appended at: \\href{https://retrocirce.github.io/appendix/}{https://retrocirce.github.io/appendix/}}. \nWe collect these datasets by downloading audios and relevant text descriptions. Based on our current knowledge, LAION-Audio-630K is the largest audio-text dataset publicly available and a magnitude larger than previous audio-text datasets as shown in Table~\\ref{tab:LAION-Audio-630K scale comparation}.\n\n\\vspace{-0.3cm}\n\\subsection{Training Dataset} \\label{sec:train_dataset}\nTo test how model performance will scale on different sizes and types of dataset, we use three training set setting in the paper, varying from small to large size. These settings employ three datasets: 1) \\textbf{AudioCaps+Clotho} (\\textbf{AC+CL})~\\cite{audiocaps,clotho} contains about 55K training samples of audio-text pairs. 2) LAION-Audio-630K (\\textbf{LA.}) consists of around 630K audio-text pairs. \n3) \\textbf{Audioset}~\\cite{audioset} consists of 1.9 million audio samples with only labels available for each sample. When processing these datasets, we exclude all overlapping data in evaluation sets. More details of the training datasets can be found at the online appendix.\n\n\\vspace{-0.3cm}\n\\subsection{Dataset Format and Preprocessing}\nAll audio files used in this work are preprocessed to mono channel at a sample rate of 48kHz in FLAC format. For datasets with only tags or labels available, we extend labels into captions using the template ``The sound of \\texttt{label-1}, \\texttt{label-2}, ..., and \\texttt{label-n}\" or the keyword-to-caption model (detail in section \\ref{sec:ksa}). As a result, we can leverage more data into the training of the contrastive language-audio pretraining model. Combining all the datasets, we increase the total number of audio samples with text caption to 2.5 million.\n\n\\section{Model Architecture}\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{figure/model-arch-5.pdf}\n    \\caption{The architecture of our proposed model, including audio/text encoders, feature fusion, and keyword-to-caption augmentation.}\n    \\label{fig:audioclip-arch}\n    \\vspace{-0.5cm}\n\\end{figure}\n\n\\subsection{Contrastive Language-Audio Pretraining}\nFigure \\ref{fig:audioclip-arch} depicts the general architecture of our proposed contrastive language-audio encoder model. Similar to CLIP \\cite{clip}, we have two encoders to separately process the input of audio data $X_i^a$ and text data $X_i^t$, where $(X_i^a, X_i^t)$ is one of audio-text pairs indexed by $i$. The audio embedding $E_i^{a}$ and the text embedding $E_i^{t}$ are respectively obtained by the audio encoder $f_{audio}(\\cdot)$ and the text encoder $f_{text}(\\cdot)$, with projection layers:\n\\begin{align}\n    E_i^{a} &= MLP_{audio}(f_{audio}(X_i^a)) \\\\\n    E_i^{t} &= MLP_{text}(f_{text}(X_i^t)) \n\\end{align}\nWhere the audio/text projection layer is a 2-layer multilayer perceptron (MLP) with ReLU \\cite{relu} as the activation function to map the encoder outputs into the same dimension $D$ (i.e., $E_i^{a}, E_i^{t} \\in \\mathbb{R}^D$).\n\nThe model is trained with the contrastive learning paradigm between the audio and text embeddings in pair, following the same loss function in \\cite{clip}:\n\\begin{align}\n\\begin{split}\n\\resizebox{\\columnwidth}{!}{\n    $L = \\frac{1}{2N} \\sum_{i=1}^N (\\log \\frac{\\exp(E_i^a \\cdot E_i^t / \\tau)}{\\sum_{j=1}^N \\exp(E_i^a \\cdot E_j^t / \\tau)} + \\log \\frac{\\exp(E_i^t \\cdot E_i^a / \\tau)}{\\sum_{j=1}^N \\exp(E_i^t \\cdot E_j^a / \\tau)})$\n}\n\\end{split}\n\\end{align}\nWhere $\\tau$ is a learnable temperature parameter for scaling the loss. Two logarithmic terms consider either audio-to-text logits or text-to-audio logits. $N$ is usually the number of data, but during the training phase, $N$ is used as the batch size, as we cannot compute the whole matrix of all data but update the model by batch gradient descent. \n\nAfter we train the model, the embeddings $(E^a, E^b)$ can be used for different tasks as shown in Figure \\ref{fig:audioclip-arch} and listed in the below subsection.\n\n\\subsection{Downstream Tasks in Inference Stage}\n\\noindent\\textbf{Text-to-Audio Retrieval}~ The target audio embedding $E_p^a$ can find the nearest text embedding $E_q^t$ among $M$ texts $E^t=\\{E_1^t, ..., E_M^t\\}$ by the cosine similarity function, determining the best match.\n\n\\vspace{0.1cm}\n\\noindent\\textbf{Zero-shot Audio Classification}~ For $M$ audio classes $C=\\{C_1,...,C_M\\}$, we can construct $M$ prompt texts $X^t=\\{X_1^t,...,X_M^t\\}$ (e.g., ``the sound of \\texttt{class-name}\"). For a given audio $X_p^a$, we determine the best match $X_q^t$ among $X^t$ by the cosine similarity function over their embeddings. One advantage of using the contrastive language-audio pretraining is that the categories of audio are unrestricted (i.e., zero-shot) since the model can convert the classification task into the text-to-audio retrieval task.\n\n\\vspace{0.1cm}\n\\noindent\\textbf{Supervised Audio Classification}~ After training the model, for a given audio $X_p^a$, its embedding $E_p^a$ can be further mapped into a fixed-category classification task by adding a projection layer at the back and finetuning (i.e., the non-zero-shot setting).\n\n\\subsection{Audio Encoders and Text Encoders}\\label{sec:at-encoder}\nWe select two models, PANN \\cite{pann} and HTSAT \\cite{hts-at}, to construct the audio encoder. PANN is a CNN-based \\cite{cnn} audio classification model with 7 downsampling CNN blocks and 7 upsampling blocks. HTSAT is a transformer-based model with 4 groups of swin-transformer blocks \\cite{swintransformer}, which achieves SOTAs on three audio classification datasets. For both of them, we use their penultimate layer's output, a $L$-dimension vector as the output sent to the projection MLP layer, where $L_{PANN}=2048$ and $L_{HTSAT}=768$.\n\nWe select three models, CLIP transformer \\cite{clip} (text encoder of CLIP), BERT \\cite{bert}, and RoBERTa \\cite{roberta}, to construct the text encoder. The output dimension of text encoders is respectively $L_{CLIP}=512$. $L_{BERT}=768$, and $L_{RoBERTa}=768$. We apply both 2-layer MLPs with ReLU activation \\cite{relu} to map both audio and text outputs into 512 dimensions, which is the size of audio/text representations when training with the contrastive learning paradigm.\n\n\\subsection{Feature Fusion for Variable-Length Audio}\nUnlike RGB image data that can be resized to a unified resolution, audio has a nature of variable length. \nConventionally, one would input the full audio into the audio encoder and take the average of per-frame or per-chunk audio embeddings as output (i.e., slice \\& vote). However, the conventional method is computationally inefficient on long audio.\nAs shown in the left of Figure \\ref{fig:audioclip-arch}, we train and inference on different lengths of audio inputs in constant computation time by combining both coarsely global and randomly sampled local information. For an audio in $T$ seconds and a fixed chunk duration $d=10$ seconds:\n\\begin{itemize}[leftmargin=*]\n    \\item $T \\leq d$: we first repeat the input, then pad it with zero values. For example,  a $\\text{3-second}$ input will be repeated as $3\\times3=\\text{9-second}$ and padded with $\\text{1-second}$ zero values.\n    \\item $T > d$: we first downsample the input from $T$ to $d$-second as a global input. Then we randomly slice three $d$-second clips, respectively from the front $\\frac{1}{3}$, middle $\\frac{1}{3}$ and back $\\frac{1}{3}$ of the input, as local inputs. We send these $4 \\times d$ inputs into the first layer of audio encoder to get the initial features, then three local features will be further converted to one feature by another 2D-Convolution layer with 3-stride in the time axis. Finally, the local feature $X^a_{local}$ and the global feature $X^a_{global}$ will be fused as:\n    \\begin{align}\n        X^a_{fusion}= \\alpha X^a_{global}+(1-\\alpha)X^a_{local}\n    \\end{align}\n    Where $\\alpha=f_{AFF}(X^a_{global},X^a_{local})$ is a factor obtained by attention feature fusion (AFF) \\cite{aff}, a two-branch CNN model for learning the fusion factor of two inputs. Comparing with the ``slice \\& vote\" method, the feature fusion also saves the training time as we only process audio slices in the first few layers. \n\\end{itemize}\n\\vspace{-0.4cm}\n\\subsection{Keyword-to-Caption Augmentation} \\label{sec:ksa}\nAs mentioned in section \\ref{sec:dataset}, some datasets contains reasonable labels or tags as keywords of the corresponding audios. As shown in the right of Figure \\ref{fig:audioclip-arch}, we used a pre-trained language model T5 \\cite{t5model} to make captions on top of these keywords. We also de-bias the output sentence as post-processing. For example, we replace ``woman\" and ``man\" with `person' as gender de-biasing. Due to the page limit, we provide examples of the augmentation in the online appendix. \n\n\\vspace{-0.3cm}\n\\section{Experiments} \\label{sec:exp}\n\\vspace{-0.2cm}\nIn this section, we conduct three experiments on our proposed model. First, we train with different audio and text encoders to find the best baseline combination. Then, we train our model on various dataset size, with the feature fusion and keyword-to-caption augmentation to verify the efficacy of the proposed methods.\nFor the first two experiments, we evaluate our model's performance via recall and mean average precision (mAP) on audio-to-text and text-to-audio retrieval. Lastly, we use the best model to conduct zero-shot and supervised audio classification experiments to evaluate the generalization ability to the downstream tasks.\n\n\\begin{table}[t]\n\\centering\n\\resizebox{0.9\\columnwidth}{!}{\n\\begin{tabular}{lcccc}\n\\toprule\n\\multirow{2}{*}{Model} & \\multicolumn{2}{c|}{AudioCaps (mAP@10)}                    & \\multicolumn{2}{c}{Clotho (mAP@10)}   \\\\ \\cline{2-5} \n                       & ~~~A$\\rightarrow$T & \\multicolumn{1}{c|}{T$\\rightarrow$A} & ~~A$\\rightarrow$T & T$\\rightarrow$A \\\\ \\hline\nPANN+CLIP Trans.       & 4.7               & 11.7                                   & 1.9               & 4.4               \\\\\nPANN+BERT              & 34.3              & 44.3                                   & 10.8              & 17.7              \\\\\nPANN+RoBERTa           & 37.5              & 45.3                                   & 11.3              & 18.4              \\\\\nHTSAT+CLIP Trans.      & 2.4               & 6.0                                    & 1.1               & 3.2               \\\\\nHTSAT+BERT             & 43.7              & 49.2                                   & \\textbf{13.8}     & \\textbf{20.8}     \\\\\nHTSAT+RoBERTa          & \\textbf{45.7}     & \\textbf{51.3}                          & \\textbf{13.8}     & 20.4              \\\\ \\hline\n\\end{tabular}\n}\n\\caption{The text-to-audio retrieval result (mAP@10) of using different audio/text encoder on AudioCaps and Clotho.}\n\\vspace{-0.5cm}\n\\label{tab:exp-ta-abalation}\n\\end{table}\n\n\\begin{table*}[t]\n\\vspace{-0.2cm}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{lccccccc|cccccc}\n\\hline\\hline\n\\multicolumn{1}{c}{\\multirow{3}{*}{Model}} &\n  \\multirow{3}{*}{Training Set} &\n  \\multicolumn{6}{c|}{AudioCaps  Eval.} &\n  \\multicolumn{6}{c}{Clotho Eval.} \\\\ \\cline{3-14} \n\\multicolumn{1}{c}{} &\n   &\n  \\multicolumn{3}{c}{T-A Retrieval} &\n  \\multicolumn{3}{c|}{A-T Retrieval} &\n  \\multicolumn{3}{c}{T-A Retrieval} &\n  \\multicolumn{3}{c}{A-T Retrieval} \\\\ \\cline{3-14} \n\\multicolumn{1}{c}{}  &                                              & R@1   & R@5  & R@10 & R@1  & R@5  & R@10 & R@1  & R@5  & R@10 & R@1  & R@5  & R@10 \\\\ \\hline\nMMT \\cite{mmt}                   & AudioCaps or Clotho                             & 36.1  & \\textbf{72.0} & \\textbf{84.5} & 39.6 & 76.8 & 86.7 & 6.7  & 21.6 & 33.2 & 7.0  & 22.7 & 34.6 \\\\\nML-ACT \\cite{ml-act}                & AudioCaps or Clotho                             & 33.9  & 69.7 & 82.6 & 39.4 & 72.0 & 83.9 & 14.4 & 36.6 & 49.9 & 16.2 & 37.6 & 50.2 \\\\\nCLAP-HTSAT \\cite{clap-retrieval} &\n  AudioCaps + Clotho + WT5K & 34.6 & 70.2 & 82.0 & 41.9 & 73.1 & 84.6 & 16.7 & 41.1 & 54.1 & 20.0 & 44.9 & 58.7 \\\\ \\hline\nHTSAT-RoBERTa    & AudioCaps + Clotho                              &  \\textbf{36.7}  &  70.9    & 83.2 &  45.3 &  78.0  &  87.7  & 12.0 & 31.6  &  43.9    &   15.7   &  36.9    &  51.3    \\\\\nHTSAT-RoBERTa    & AudioCaps + Clotho + LA.                       &  32.7    &  68.0  &  81.2   &  43.9   &  77.7    &  87.6    &  15.6    &   38.6  &  52.3    & 23.7    &  48.9   &  59.9    \\\\\nHTSAT-RoBERTa (fusion)    & AudioCaps + Clotho + LA.                       &  36.2   &  70.3  &  82.5   &  45.0   &  76.7    &  88.0    &  \\textbf{17.2}  &  \\textbf{42.9}  &  \\textbf{55.4}    & 24.2    &  \\textbf{51.1}   &  \\textbf{66.9}    \\\\\nHTSAT-RoBERTa    & ACaps. + Clotho + LA. + AudioSet (template) &  34.7  &  70.5 &  83.2 &   45.3   &   79.5  &  89.2  &  16.4    &  39.0    &  51.0    &  21.8   &  44.6   &  60.1    \\\\ \nHTSAT-RoBERTa    & ACaps. + Clotho + LA. + AudioSet (K2C aug.) &  36.1  &  71.8  &  83.9 &   \\textbf{46.8}   &   \\textbf{82.9}   &  \\textbf{90.7}  &  16.1    &  38.3    &  51.1    &  22.7   &  48.5   &  60.8    \\\\ \nHTSAT-RoBERTa (fusion)    & ACaps. + Clotho + LA. + AudioSet (K2C aug.) &  35.1  & 71.9   &  83.7 &   44.2   &  80.8    & 90.3  &  16.9    &  41.6    & 54.4     & \\textbf{24.4}    & 49.3    &  65.7 \\\\\\hline\\hline \n\\end{tabular}}\n\\vspace{-0.2cm}\n\\caption{The text-to-audio retrieval performance on AudioCaps and Clotho datasets, where ``LA.\" refers to LAION-Audio-630K, ``template\" refers to the text prompting by templates, ``K2C aug.\" refers to the keyword-to-caption augmentation, and ``fusion\"  refers to the feature fusion.}\n\\vspace{-0.5cm}\n\\label{tab:exp-t2a-retrieval}\n\\end{table*}\n\n\\vspace{-0.2cm}\n\\subsection{Hyperparameters and Training Details}\nAs mentioned in section \\ref{sec:train_dataset}, we use AudioCaps, Clotho, LAION-Audio-630K, along with the additional dataset --- AudioSet by keyword-to-caption augmentation, to train our model. For the audio data, we use 10-second input length, 480 hop size, 1024 window size, 64 mel-bins to compute STFTs and mel-spectrograms. As the result, each input sent to the audio encoder is of the shape $(T=1024, F=64)$. For the text data, we tokenize the text with a maximum token length of 77. \n\nWhen training the model without the feature fusion, the audio longer than 10-second will be randomly chunked to a 10-second segment. During training, we use the Adam \\cite{kingma2014adam} optimizer with $\\beta_1=0.99$, $\\beta_2=0.9$ with a warm-up \\cite{goyal2017accurate} and cosine learning rate decay at a basic learning rate of $10^{-4}$. We train the model using a batch size of 768 on \\textbf{AudioCaps+Clotho} dataset, 2304 on training dataset containing LAION-Audio-630K, and 4608 on training dataset containing \\textbf{AudioSet}. We train the model for 45 epochs.\n\n\\vspace{-0.2cm}\n\\subsection{Text-to-Audio Retrieval}\n\n\\noindent \\textbf{Audio and Text Encoders} We first conduct experiments to choose the best audio encoder and text encoder for the text-to-audio retrieval task. We combine two audio encoders with three text encoders in section~\\ref{sec:at-encoder} where both are loaded from pretrained checkpoints as the same to \\cite{mmt,ml-act,clap-retrieval}. In this experiment, we only train on AudioCaps and Clotho datasets ($\\sim$55K data), and report the best mAP@10 on audio-to-text (A$\\rightarrow$T) and text-to-audio (T$\\rightarrow$A) perspectives.\n\nAccording to the results in Table \\ref{tab:exp-ta-abalation}, for audio encoder, HTSAT performs better than PANN combined with the RoBERTa or BERT text encoder. For the text encoder, RoBERTa achieves better performance than BERT while the CLIP transformer performs the extremely worst. This coincides with the choice of text encoder in previous works~\\cite{ml-act,clap}. When further analyzing the loss convergence trends of CLIP transformer model, we find that RoBERTa is less over-fitting, while CLIP transformer is of high-over-fitting, thus resulting its low generalization performance. \n\n\\vspace{0.05cm}\n\\noindent \\textbf{Dataset Scale} Consequently, we apply HTSAT-RoBERTa as our best model setting to conduct the text-to-audio retrieval experiments as a comprehensive evaluation in Table~\\ref{tab:exp-t2a-retrieval}. We adopt the same metrics in \\cite{ml-act,mmt} to compute recall scores at different ranks in this task. In the training set, we gradually increase the scale of the dataset.\nWe find that scaling up the dataset from ``AudioCaps + Clotho\" to ``LA.\" does not improve the result on AudioCaps evaluation set but gets better performance on Clotho evaluation set, which is similar to the comparison between MMT \\cite{mmt} and CLAP-HTSAT \\cite{clap-retrieval}. One reason is that AudioCaps contains audios similar to AudioSet on which the audio encoder's loaded checkpoint is pretrained. When the model receives more data from other sources, it increases its generalization but moves the distribution out of AudioSet data. Therefore, the performance on AudioCaps drops but that on Clotho increases a lot, demonstrating a trade-off of the model to keep the performance among different types of audios. \n\n\\vspace{0.05cm}\n\\noindent \\textbf{Keyword-to-Caption and Feature Fusion} When adding the feature fusion mechanism and keyword-to-caption augmentation to the model, we can observe that either of them improves the performance. The feature fusion is effective especially in Clotho dataset because it contains longer audio data ($>10\\text{-second}$). When we add AudioSet into the training set with either template prompting or keyword-to-caption augmentation, we can see the performance increases again on AudioCaps while decreases on Clotho. This further confirms the trade-off performance between AudioCaps and Clotho datasets mentioned above. And the keyword-to-caption augmentation does bring in better performance than the simple template text prompting method on most metrics.\n\nAs the result, our best model outperforms previous methods on most metrics (mainly R@1=36.7\\% on AudioCaps and R@1=18.2\\% on Clotho) in the text-to-audio retrieval tasks. We show that training on large-scale datasets (LAION-Audio-630K and AudioSet with keyword-to caption augmentation), and feature fusion can effectively improve model performance.\n\n\\begin{table}[]\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{lccccc}\n\\hline\\hline\n\\multirow{3}{*}{Model} & \\multicolumn{5}{c}{Audio Classification Dataset \\& Setting}                                                          \\\\ \\cline{2-6} \n                       & \\multicolumn{1}{c|}{ESC-50} & \\multicolumn{1}{c|}{US8K} & \\multicolumn{2}{c|}{VGGSound}                     & FSD50K \\\\ \\cline{2-6} \n                       & \\multicolumn{1}{c|}{ZS.}     & \\multicolumn{1}{c|}{ZS.}   & \\multicolumn{1}{c|}{ZS.} & \\multicolumn{1}{c|}{SV.} & SV.     \\\\ \\hline\nWav2CLIP\\cite{wav2clip}               & 41.4                        & 40.4                      & 10.0                    & 46.6                    & 43.1   \\\\\nAudioClip\\cite{audioclip}              & 69.4                        & 65.3                      & -                       & -                       & -      \\\\\nMicrosoft\\cite{clap-retrieval}                   & 82.6                        & 73.2                      & -                       & -                       & 58.6   \\\\ \\hline\nCLAP                   & 89.1                        & 73.2                      & 29.1                    & \\textbf{75.4}                    & 64.9   \\\\\nCLAP+Fusion            & 88.0                        & 75.8                    & 26.3                  &      75.3                 &     64.4  \\\\\nCLAP+K2C Aug.           & \\textbf{91.0}                        & \\textbf{77.0}                      & \\textbf{46.2}                    &         75.3               &   59.7     \\\\ \\hline\nSoTA*                  & 82.6\\cite{clap-retrieval}                        & 73.2\\cite{clap-retrieval}                      & 10.0\\cite{wav2clip}                    & 64.1\\cite{mbt}                    & \\textbf{65.6}\\cite{passt}   \\\\ \\hline\\hline\n\\end{tabular}}\n\\caption{The zero-shot (ZS.) and supervised (SV.) audio classification results. The SoTA of each dataset/setting is denoted by the reference after the number.}\n\\vspace{-0.6cm}\n\\label{tab:exp-ac}\n\\end{table}\n\n\\vspace{-0.2cm}\n\\subsection{Zero-shot and Supervised Audio Classification}\n\n\\noindent \\textbf{Zero-shot Audio Classification} To study the model generalization and robustness, we conduct zero-shot audio classification experiments on three top-performing models in previous experiments. We evaluate models on three audio classification dataset, namely ESC-50~\\cite{esc50}, VGGSound~\\cite{vggsound}, and Urbansound8K (US8K)~\\cite{us8k}. We use \\textbf{top-1 accuracy} as the metric. We classify audio by performing audio-to-text retrieval with each text corresponds to the text prompt converted from class label via\n``This a sound of \\texttt{label}.\". We noticed a dataset overlap between our training data and the zero-shot dataset we are evaluating on. We \\textbf{excluded all the overlap samples} and perform zero-shot evaluation on the whole remaining dataset.\n\n\\vspace{0.1cm}\n\\noindent \\textbf{Supervised Audio Classification}\nWe perform supervised audio classification by fine-tuning the audio encoder on FSD50K~\\cite{fsd50k} and VGGSounddatasets. We do not conduct this experiment on ESC-50 and Urbansound8K because the potential data leakage issue in those dataset will makes the results incomparable with the previous methods. Specially, \\textbf{mAP} is used as the metric to evaluate FSD50K.\n\nAs shown in the in Table~\\ref{tab:exp-ac}, our models achieves new SoTAs of zero-shot audio classification across all three datasets, demonstrating the high generalization ability of our model to unseen data. Keyword-to-Caption augmentation increases the performance of VGGsound and US8K a lot as it adds more text captions to ``enrich\" the text embedding space. Feature fusion not only enables the model to handle variable-length input, but also achieves better performance than previous models. Our best supervised audio classification result outperforms the current state-of-the-art on VGGSound dataset and is close to state-of-the-art on FSD50K dataset. The results verify that the proposed model also learns efficient audio representation during contrastive learning paradigm. \n\n\\vspace{-0.3cm}\n\\section{Conclusion and Future Work}\n\nIn this paper, we propose a large-scale audio-text dataset and improvements on current language-audio contrastive learning paradigm. We show that LAION-Audio-630, AudioSet with keyword-to-caption augmentation, and feature fusion effectively leads to better audio understanding, task performance, and enables effective learnings on variable-length data. Future works include collecting even larger dataset on training, applying representations into more downstream tasks such as audio synthesis and separation.\n\n\\clearpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Audio Difference Captioning \\\\ Utilizing Similarity-Discrepancy Disentanglement}\n\n\\begin{document}\n\n\\sloppy\n\n\\maketitle\n\\ninept \n\\begin{abstract}\nWe proposed \\textit{Audio Difference Captioning} (ADC) as a new extension task of audio captioning for describing the semantic differences between input pairs of similar but slightly different audio clips.\nThe ADC solves the problem that conventional audio captioning sometimes generates similar captions for similar audio clips, failing to describe the difference in content.\nWe also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference in the latent space.\nTo evaluate the proposed methods, we built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips with human-annotated descriptions of their differences.\nThe experiment with the AudioDiffCaps dataset showed that the proposed methods solve the ADC task effectively and improve the attention weights to extract the difference by visualizing them in the transformer encoder.\n\n\\end{abstract}\n\\noindent\\textbf{Index Terms}: audio difference captioning, contrastive learning, crossmodal representation learning, deep neural network\n\n\\section{Introduction}\n\\label{sec:intro} \n\nAudio captioning is used to generate the caption for an audio clip~\\cite{kim2019audiocaps, drossos2019clotho, takeuchi2020effects, xu2020crnn, mei2021audio, gontier2021automated, koizumi2020audio, xu2022diversity, mei2022diverse, liu2022leveraging}.\nUnlike labels for scenes and events\\cite{piczak2015dataset, barchiesi2015acoustic, mesaros2016tut, gemmeke2017audio, fonseca2020fsd50k}, captions describe the content of the audio clip in detail.\nHowever, conventional audio captioning systems often produce similar captions for similar audio clips, making it challenging to discern their differences solely based on the generated captions.\nFor instance, suppose two audio clips of heavy rain are input into a conventional captioning system. \nThe system will generate a caption describing the content of each, like ``\\textit{It is raining very hard without any break}'' and ``\\textit{Rain falls at a constant and heavy rate}''\\footnote{These captions were taken from the Clotho dataset~\\cite{drossos2019clotho}} as illustrated in Fig.~\\ref{fig:task_illust}(a). \nThe difference, such as which rain sound is louder, is difficult to understand from the generated captions in this case.\n\n\\begin{figure}[t]\n  \\centering\n\\includegraphics[width=0.9\\columnwidth]{task_illustration_v4.pdf} \n\\vspace{-5pt}\n  \\caption{\n  Conceptual diagram of conventional audio captioning and audio difference captioning.\n  Audio difference captioning describes the difference between pair audio clips, while conventional audio captioning describes the contents of each. \n  }\n  \\label{fig:task_illust}\n  \\vspace{-10pt}\n\\end{figure}\n\nTo address this problem, we propose \\textit{Audio Difference Captioning} (ADC) as a new extension task of audio captioning. \nADC takes two audio clips as input and outputs text explaining the difference between two inputs as shown in Fig.~\\ref{fig:task_illust}. \nWe make the ADC clearly describe the difference between the two audio clips,\nsuch as ``Make the rain louder,'' which describes what and how to modify one audio clip to the other in the instruction form, even for audio clips with similar texts. \nPotential real-world applications include machine condition and healthcare monitoring using sound by captioning anomalies that differ from usual sounds.\n\nThe ADC task has two major challenges: different content detection and detection sensitivity. \nSince the difference between a pair of audio clips can be classes of contained events or an attribute, such as loudness, the ADC needs to detect what difference to describe. \nWhen the difference lies in an attribute, the ADC needs to be sensitive enough to detect the magnitude of the attribute, such as rain is hard or moderately shown in the example in Fig.~\\ref{fig:task_illust}.\n\nTo handle these challenges, the ADC should extract features of difference based on the cross-reference of two audio clips. \nThese features should carry enough information to differentiate critical attributes such as loudness. \nA typical choice of a feature extractor could be pre-trained models to classify labels~\\cite{hershey2017cnn, kong2020panns, gong2021ast}. However, these models learn to discriminate sound event classes, learning what is common while ignoring subtle differences such as raining hard or quietly unless the class definition covers that.\n\nTo meet the requirements of the ADC mentioned above, we propose (I) a cross-attention-concentrated~(CAC) transformer encoder and (II) a similarity-discrepancy disentanglement~(SDD). \nThe CAC transformer encoder utilizes the masked multi-head attention layer, which only considers the cross-attention of two audio clips to extract features of difference efficiently. \nThe SDD emphasizes the difference feature in the latent space using contrastive learning based on the assumption that two similar audio clips consist of similar and discrepant parts.\n\nWe demonstrate the effectiveness of our proposals using a newly built dataset, AudioDiffCaps, consisting of two similar but slightly different audio clips synthesized from existing environmental sound datasets~\\cite{fonseca2020fsd50k, piczak2015dataset} and human-annotated difference descriptions. \nExperiments show that the CAC transformer encoder improves the evaluation metric scores by making the attention focus only on cross-references. The SDD also improves the scores by emphasizing the differences between audio clips in the latent space. \nOur contributions are proposals of (i) the ADC task, (ii) the CAC transformer encoder and SDD for solving ADC, (iii) the AudioDiffCaps dataset, and (iv) demonstrating the effectiveness of these proposals.\n\n\\begin{figure*}[t]\n\\vspace{-9pt}\n  \\centering\n\\includegraphics[width=1.95\\columnwidth]{model_arch_dcase23.pdf} \n\\vspace{-5pt}\n  \\caption{\n  Model architecture of our proposed method. \n  The cross-attention-concentrated transformer encoder uses an attention mask illustrated in the upper left. \n  The similarity-discrepancy disentanglement is conducted by symmetric InfoNCE loss and pairwise cosine similarity. The input to them is either the input or output of the cross-attention-concentrated transformer encoder.}\n  \\label{fig:model_arch}\n  \\vspace{-10pt}\n\\end{figure*}\n\n\\section{Audio difference captioning}\nWe propose ADC, a task for generating texts to describe the difference between two audio clips.\nADC estimates a word sequence $\\boldsymbol{w}$ from the two audio clips $\\boldsymbol{x}$ and $\\boldsymbol{y}$. \n\nThe general framework to solve ADC includes three main functions: audio embedding, audio difference encoding, and text decoding.\nAudio embedding calculates two audio embedding vectors from two audio clips, respectively.\nAudio difference encoding captures the difference between two audio embedding vectors.\nText decoding generates a description of the differences from captured differences.\nAudio embedding and audio difference encoding require approaches specific to ADC.\nIn particular, difference encoding is the function unique to audio difference captioning.\nThis function requires a model structure to capture the subtle differences between two audio clips, unlike conventional audio captioning that captures the content of a single audio clip.\nMoreover, the sensitivity to the subtle difference between two similar audio clips is also necessary for audio embedding.\nThe pre-trained audio embedding models widely used for conventional environmental sound analysis tasks are often trained for classification tasks and are suitable for identifying predefined labels.\nConsequently, the outputs of these pre-trained audio embedding models are not sensitive to the subtle differences between audio clips with the same label.\nTherefore, learning to emphasize the differences between similar audio clips in the latent space is necessary when applying pre-trained audio embedding models to the ADC.\n\n\\mysection{Proposed method}\nBased on the above discussion, we propose the ADC system illustrated in Fig.~\\ref{fig:model_arch}.\nOur system consists of an audio feature extractor~(red), difference encoder~(blue), text decoder~(green), and similarity-discrepancy disentanglement~(purple).\n\n\\mysubsection{Audio feature extractor}\nThe audio feature extractor uses a pre-trained audio embedding model to calculate audio embedding vectors.\nTwo audio clips $\\boldsymbol{x}$ and $\\boldsymbol{y}$ are the input, and the audio embedding vectors corresponding to the clips $\\boldsymbol{X} \\in \\mathbb{R}^{H \\times T_x}$ and $\\boldsymbol{Y} \\in \\mathbb{R}^{H \\times T_y}$ are the output, where $H$ is the size of hidden dimension, $T_x$ is the time length of $\\boldsymbol{X}$, and $T_y$ is the time length of $\\boldsymbol{Y}$\n\n\\mysubsection{Difference encoder}\nThe difference encoder extracts information about the differences between the two audio clips from audio embedding vectors $\\boldsymbol{X}$ and $\\boldsymbol{Y}$.\nTo extract difference information efficiently, we utilize a cross-attention-concentrated~(CAC) transformer encoder as the main function of the difference encoder.\nThe CAC transformer encoder utilizes the masked multi-head attention layer, allowing only mutual cross-attention between two audio clips by the attention mask illustrated in the upper right of Fig.~\\ref{fig:model_arch}.\n\nThe detailed procedure is as follows.\nFirst, special tokens that indicate the order of the audio clips $\\mathcal{X} \\in \\mathbb{R}^{H \\times 1}$ and $\\mathcal{Y} \\in \\mathbb{R}^{H \\times 1}$ are concatenated at the beginning of $\\boldsymbol{X}$ and $\\boldsymbol{Y}$, respectively.\nNext, these two sequences are concatenated to make the input of the difference encoder $\\boldsymbol{Z}$ like $\\boldsymbol{Z} = [\\mathcal{X}, \\boldsymbol{X}, \\mathcal{Y}, \\boldsymbol{Y}]$.\nThen, positional encoding $\\mathscr{P}$ is applied to $\\boldsymbol{Z}$.\nFinally, $\\mathscr{P}(\\boldsymbol{Z})$ is input to CAC transformer encoder to obtain the output $\\boldsymbol{\\hat{Z}}= [\\hat{\\mathcal{X}}, \\boldsymbol{\\hat{X}}, \\hat{\\mathcal{Y}}, \\boldsymbol{\\hat{Y}}]$.\n\n\\mysubsection{Text decoder}\nThe transformer decoder is utilized as a text decoder like as \\cite{mei2021audio}.\nThe text decoder calculates word probability from the output of the difference encoder $\\boldsymbol{\\hat{Z}}$.\n\n\\mysubsection{Similarity-discrepancy disentanglement}\nThe similarity-discrepancy disentanglement~(SDD) loss function is an auxiliary loss function aimed at obtaining a difference-emphasized audio representation.\nWhen there is an explainable difference between two audio clips, these clips consist of similar and discrepant parts.\nTo introduce this hypothesis, we design contrastive learning to bring similar parts closer and keep discrepant parts.\nWe propose two types of implementations that apply SDD to the input of the difference encoder $\\boldsymbol{Z}$ or the output of it $\\boldsymbol{\\hat{Z}}$, as shown in Fig.~\\ref{fig:model_arch}, and call the former and latter implementations early and late disentanglement, respectively.\n\nWe explain the procedure in the case of early disentanglement.\nNote that the case of late disentanglement only replaces  $\\boldsymbol{Z}$ with $\\boldsymbol{\\hat{Z}}$.\nFirst, $\\boldsymbol{Z}$ is split along the hidden dimension and assigned to similar and discrepant parts like in the upper left illustration of Fig.~\\ref{fig:model_arch}.\nIf $\\boldsymbol{Z} \\in \\mathbb{R}^{H \\times (T_x+T_y+2)}$, $\\boldsymbol{Z}$ is split into similar part $\\boldsymbol{Z}_{\\rm S}$ and discrepant part $\\boldsymbol{Z}_{\\rm D}$ like\n\\begin{align}\n    \\boldsymbol{Z}_{\\rm S} &= [\\mathcal{X}_{\\rm S}, \\boldsymbol{X}_{\\rm S}, \\mathcal{Y}_{\\rm S}, \\boldsymbol{Y}_{\\rm S}] \\in \\mathbb{R}^{(H/2) \\times (T_x+T_y+2)}, \\\\\n    \\boldsymbol{Z}_{\\rm D} &= [\\mathcal{X}_{\\rm D}, \\boldsymbol{X}_{\\rm D}, \\mathcal{Y}_{\\rm D}, \\boldsymbol{Y}_{\\rm D}] \\in \\mathbb{R}^{(H/2) \\times (T_x+T_y+2)}.\n\\end{align}\nThen, the SDD is performed by $\\mathcal{L}_{\\rm SDD} = \\mathcal{L}_{\\rm S} + \\mathcal{L}_{\\rm D}$, where\n\\begin{align}\n    &\\mathcal{L}_{\\rm S} = {\\rm SymInfoNCE}(\\Phi([\\mathcal{X}_{\\rm S}, \\boldsymbol{X}_{\\rm S}]), \\Phi([\\mathcal{Y}_{\\rm S}, \\boldsymbol{Y}_{\\rm S}])), \\\\\n    &\\mathcal{L}_{\\rm D} = {\\rm PairCosSim}(\\Psi([\\mathcal{X}_{\\rm D}, \\boldsymbol{X}_{\\rm D}]), \\Psi([\\mathcal{Y}_{\\rm D}, \\boldsymbol{Y}_{\\rm D}])), \n\\end{align}\n${\\rm SymInfoNCE}$ is the symmetric version of the InfoNCE loss used in~\\cite{radford2021learning}, ${\\rm PairCosSim}$ is the cosine similarity for each correct data pair,  $\\Phi$ and $\\Psi$ are embedding networks consisting of the bidirectional-LSTM and average pooling, and $\\mathcal{L}_{\\rm SDD}$ is the final value of the SDD loss function.\nThat is, the SDD loss function views $[\\mathcal{X}_{\\rm S}, \\boldsymbol{X}_{\\rm S}]$ and $[\\mathcal{Y}_{\\rm S}, \\boldsymbol{Y}_{\\rm S}]$ as similar parts and brings them closer by using $\\mathcal{L}_{\\rm S}$ and views $[\\mathcal{X}_{\\rm D}, \\boldsymbol{X}_{\\rm D}]$ and $[\\mathcal{Y}_{\\rm D}, \\boldsymbol{Y}_{\\rm D}]$ as discrepant parts and keeps them apart by $\\mathcal{L}_{\\rm D}$.\n\nThe entire loss function $\\mathcal{L}$ is the weighted sum of cross-entropy loss for word prediction $\\mathcal{L}_{\\rm CE}$ and the SDD: $\\mathcal{L} = \\mathcal{L}_{\\rm CE} + \\lambda \\mathcal{L}_{\\rm SDD}$, where $\\lambda$ is a weighting parameter.\n\n\\mysection{Experiment}\nExperiments were conducted to evaluate the proposed CAC transformer encoder and SDD loss function.\nWe constructed the AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips and a human-annotated description of their differences for the experiments.\n\n\\mysubsection{AudioDiffCaps dataset}\nThe constructed AudioDiffCaps dataset consists of\n(i)~pairs of similar but slightly different audio clips and (ii)~human-annotated descriptions of their differences.\n\nThe pairs of audio clips were artificially synthesized by mixing foreground event sounds with background sounds taken from existing environmental sound datasets (FSD50K~\\cite{fonseca2020fsd50k} and ESC-50~\\cite{piczak2015dataset}) using the Scaper library for soundscape synthesis and augmentation~\\cite{salamon2017scaper}. We used the same mixing procedure as our previous work~\\cite{takeuchi2022introducing}.\nData labeled \\myqt{rain} or \\myqt{car\\_passing\\_by} in FSD50K was used as background, and six foreground event classes were taken from ESC-50 (i.e., data labeled \\myqt{dog}, \\myqt{chirping\\_bird}, \\myqt{thunder}, \\myqt{footsteps}, \\myqt{car\\_horn}, and \\myqt{church\\_bells}).\nEach created audio clip was 10 seconds long. The maximum number of events in one  audio clip was two, with 0-100\\% overlap (no overlap-range control applied). \nEach foreground event class had 32 or 8 instances in the development or evaluation set, respectively. \nSimilar to previous work, we focused on the three types of difference: increase/decrease of background sounds, increase/decrease of sound events, and addition/removal of sound events.\nThe development and evaluation sets contained 5996 and 1720 audio clip pairs, respectively. (That is, development and evaluation sets contained 11992 and 3440 audio clips.)\n\nThe human-annotated descriptions were written as instruction forms explaining \"what and how\" to change the first audio clip to create the second audio clip.\nIn the preliminary study, we found that declarative sentences, in some cases, tend to use ordinal numbers such as ``\\textit{First sound} is louder than \\textit{second sound}''. \nSince these cases do not express what the actual difference is, the AudioDiffCaps dataset uses instruction forms with a fixed direction of change from the first audio clip to the second one, e.g., \"Make the rain louder\"\n\\footnote{The dataset is available at \\url{https://github.com/nttcslab/audio-diff-caps}.}\n.\nA wider variety of descriptions explaining the same concept, such as declarative sentences, should be included in future works.\nThe presentation order of the pair to the annotator was randomly selected.\nAnnotators were five na\\\"{i}ve workers remotely supervised by an experienced annotator.\nEach pair of audio clips in the development set had between 1 and 5 descriptions (a total of 28,892) while each pair in the evaluation set had exactly five descriptions assigned to it (a total of 8600).\n\n\\begin{table*}[!t]\n\n\\caption{Results of evaluation metrics}\n\\label{tab:result1}\n\\centering\n\\scriptsize\n\\begin{tabular}{@{}C{0.07}@{}l | cc | ccccccc }\n\\toprule\n\\textbf{ID} & \\textbf{System} &\\textbf{Mask} &\\textbf{Disent.} & \\textbf{BLEU-1} & \\textbf{BLEU-4} & \\textbf{METEOR} & \\textbf{ROUGE-L} & \\textbf{CIDEr} & \\textbf{SPICE} & \\textbf{SPIDEr} \\\\\t\n\\midrule\n(a)& Baseline & N/A & N/A\n& 67.1 & 31.7 & 24.3 & 56.9 & 82.7 & 19.5 & 51.1 \\\\\n(b)& CAC transformer & Cross & N/A\n& 67.0 & 33.4 & 25.2 & 59.5 & 90.2 & 19.5 & 54.9 \\\\\n\n\\midrule\n\\multicolumn{2}{l|}{\\hspace{-3pt}CAC transformer}&&&&&\\\\\n(c)& \\quad w/ Early SDD  ($\\lambda=0.5$) & Cross & Early\n& 67.0 & 33.7 & 25.3 & 59.6 & 91.8 & 19.4 & 55.6 \\\\\n(d)& \\quad w/ Early SDD  ($\\lambda=1.0$) & Cross & Early\n& 66.8 & 32.2 & 25.3 & 59.3 & 91.7 & 19.5 & 55.6 \\\\\n(e)& \\quad w/ Early SDD  ($\\lambda=2.0$) & Cross & Early\n& 66.9 & 33.5 & 25.3 & 59.6 & 92.8 & 18.7 & 55.8 \\\\\n\n&&&&&& \\vspace{-3pt}\\\\\n(f)& \\quad w/ Late SDD ($\\lambda=0.5$) & Cross & Late\n& \\textbf{70.3} & 39.2 & \\textbf{26.4} & \\textbf{61.6} & \\textbf{97.6} & 21.3 & 59.4 \\\\\n(g)& \\quad w/ Late SDD ($\\lambda=1.0$) & Cross & Late\n& 69.9 & 38.3 & 26.3 & 61.5 & 96.3 & 21.2 & 58.7 \\\\\n(h)& \\quad w/ Late SDD ($\\lambda=2.0$) & Cross & Late\n& 69.9 & \\textbf{39.5} & 26.3 & 61.3 & 97.1 & \\textbf{22.6} & \\textbf{59.9} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table*}\n\n\\begin{figure*}[t]\n  \\centering\n\\includegraphics[width=1.9\\columnwidth]{attnw_outcap_cr.pdf} \n  \\vspace{-10pt}\n  \\caption{\n  Examples of output caption and attention weights. \n  The leftmost row was the Mel-spectrogram of two audio clips and one reference caption. \n  The three on the right were the attention weights of the transformer encoder and the output caption.\n  }\n  \\label{fig:output_caps}\n\n\\end{figure*}\n\n\\mysubsection{Experimental conditions}\nWe used 10\\% of the development set for validation.\nThe optimizer was Adam~\\cite{kingma2014adam}.\nThe number of epochs was 100.\nWe used the BLEU-1, BLEU-4, METEOR, ROUGE-L, CIDEr~\\cite{vedantam2015cider}, SPICE~\\cite{anderson2016spice}, and SPIDEr~\\cite{liu2017improved} as evaluation metrics.\nThey were also used for conventional audio captioning\\cite{dcase2022task6}.\n\nWe used BYOL-A~\\cite{niizumi2021byol-a}, a pre-trained audio embedding model, as the audio feature extractor in our ADC implementation, and we fine-tuned the BYOL-A throughout experiments.\nThe transformer encoder and decoder used the official implementation of PyTorch. \nThe number of layers was 1. The hidden size was 768. The number of heads was 4. The activation was RELU. The dimension of the feedforward layer was 512. The dropout rate was 0.1.\nFor the attention mask of the transformer encoder, we compared two types; one with the proposed cross-attention mask and the other without a mask.\nThe text decoder used the teacher forcing algorithm during training and the beam search algorithm~\\cite{koehn2009statistical, koehn2004pharaoh} during inference.\nThe value of $\\lambda$ was empirically set to $0$, $0.5$, $1.0$, or $2.0$. \n\n\\mysubsection{Results}\nThe results of evaluation metrics are shown in Table~\\ref{tab:result1}, where bold font indicates the highest score, ``Mask'' and ``Disent.'' indicate the attention mask utilized in the transformer encoder and input of SDD loss function, respectively.\nWhen the CAC transformer encoder was evaluated by comparing the two lines above, the proposed method had superior or equivalent scores to the conventional method in all evaluation metrics.\nThere was no significant difference in the evaluation metrics related to the degree of matching with single-word references, such as BLEU-1.\nOne likely reason is that the scores above a certain level can be obtained by outputting words in arbitrary sentences, such as ``a'' and ``the'' in these metrics. In contrast, the scores of BLEU-4, ROUGE-L, CIDEr, and SPIDEr, affected by the accuracy of consecutive words, were improved using the proposed cross-attention mask. Therefore, the proposed cross-attention mask was thought to make the feature extraction of differences more efficient and simplify the training of the text decoder. As a result, phrase-level accuracy was improved.\n\nThe effect of SDD was verified from the results of the second to eighth lines.\nThe results in (a) and (b) were the conventional transformer without cross attention mask or SDD loss and the CAC transformer without SDD loss ($\\lambda=0$)\nOnes from (c) to (h) were the result when using early/late disentanglement.\nSince the scores of BLEU-4, ROUGE-L, CIDEr, and SPIDEr improved under all conditions comparing (b) and others, the SDD loss function was effective for the audio difference captioning task.\nThe improvement in the case of late disentanglement (f), (g), and (h) was remarkable, and the results obtained the best scores in all evaluation metrics with late disentanglement.\nIn other words, it was essential to use the information to be compared to decompose the similar part and the different parts in the feature amount space. \nThat corresponds to the difference determined depending on the comparison target.\n\nFig.~\\ref{fig:output_caps} shows one of the evaluation data and estimated caption and attention weight of the transformer encoder from each system. \nThe leftmost colomn is the Mel-spectrogram of the two input audio clips and one of the reference captions. \nThe three on the right are the attention weight of the transformer encoder and output caption, where the attention weight shows the average of multiple heads. \nThe audio clips on the left and above the weights correspond to the input and memory of the transformer, respectively. \nThe area colored pink and yellow on the weights corresponds to the dog barking. \nSince there was a difference in the loudness of the dog barking between the two clips, the attention was expected to focus on areas where pink and yellow overlap to extract the difference. \n\nFirst, in (a), since the attention weight was not constrained, it was also distributed widely to areas other than the above compared with the other two. \nOn the other hand, the attention weights of (b) and (h) concentrated on areas where pink and yellow overlap since the attention of the same input and memory was unavailable. \nComparing (b) and (h), while the attention of the part containing the barking of the dog in the memory was large at any time-frame in (b), more attention was paid to the pink and yellow overlapping areas where both input and the memory contain the barking of the dog in (h). \nSince the late disentanglement required that similar and discrepant parts be retained in the output of the transformer encoder calculated using these attention weights, it was thought that the late disentanglement induced attention to be paid to the part where there was a difference when comparing the two sounds instead of paying attention to the parts that are likely to exist the difference compared with the distribution of training data, such as a dog barking. \n\n\\mysection{Conclusion}\nWe proposed \\textit{Audio Difference Captioning} (ADC) as a new extension task of audio captioning for describing the semantic differences between similar but slightly different audio clips.\nThe ADC solves the problem that conventional audio captioning sometimes generates similar captions for similar but slightly different audio clips, failing to describe the difference in content.\nWe also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference feature in the latent space.\nTo evaluate the proposed methods, we newly built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips and a human-annotated description of their differences.\nWe experimentally showed that since the attention weights of the cross-attention-concentrated transformer encoder are restricted only to the mutual direction of the two inputs, the differences can be efficiently extracted.\nThus, the proposed method solved the ADC task effectively and improved the evaluation metric scores.\n\nFuture work includes utilizing a pre-trained generative language model such as BART~\\cite{lewis-etal-2020-bart} and applying a wider variety of audio events and types of differences.\n\n\\mysection{Acknowledgments}\n\\vspace{-3pt}\nBAOBAB Inc. supported the annotation for the dataset.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nThe auditory system plays a substantial role in shaping the overall human perceptual experience. While prevailing large language models (LLMs) and visual language models (VLMs) have shown their promise in solving a wide variety of vision and language understanding tasks, only a few of them can be generalised to the audio domain without compromising their domain-specific capacity. In this work, we introduce \\textbf{A}coustic \\textbf{P}rompt \\textbf{T}urning (APT), a new adapter extending LLMs and VLMs to the audio domain by soft prompting only. Specifically, APT applies an instruction-aware audio aligner to generate soft prompts, conditioned on both input text and sounds, as language model inputs. To mitigate the data scarcity in the audio domain, a multi-task learning strategy is proposed by formulating diverse audio tasks in a sequence-to-sequence manner. Moreover, we improve the framework of audio language model by using interleaved audio-text embeddings as the input sequence. This improved framework imposes zero constraints on the input format and thus is capable of tackling more understanding tasks, such as few-shot audio classification and audio reasoning. To further evaluate the reasoning ability of audio networks, we propose natural language audio reasoning (NLAR), a new task that analyses across two audio clips by comparison and summarization. Experiments show that APT-enhanced LLMs (namely APT-LLMs) achieve competitive results compared to the expert models (i.e., the networks trained on the targeted datasets) across various tasks. We finally demonstrate the APT's ability in extending frozen VLMs to the audio domain without finetuning, achieving promising results in the audio-visual question and answering task. Our code and model weights are released at \\url{https://github.com/JinhuaLiang/APT}.\\let\\thefootnote\\relax\\footnotetext{\\textsuperscript{*}The work does not relate to H.P.'s position at Amazon.}\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:introduction}\nAuditory stimuli contribute to shaping the overall human perception experience. While visual language models (VLMs)~\\citep{li_blip-2_2023,liu_visual_2023,dai_instructblip_2023,shukor_unified_2023,han_imagebind-llm_2023} that are capable of solving diverse down-streaming tasks have emerged driven by the advent of large language models (LLMs) and massive visual-text pretraining, only few of them~\\citep{shukor_unified_2023,han_imagebind-llm_2023} can be adapted to the audio domain while maintaining their performance in the image/video domain.\n\nTo embrace more than two modalities, few works recently attempted to explore the diversity and heterogeneity of tasks and modalities. UniVAL~\\citep{shukor_unified_2023} unified input/output format, model architecture, and training objective, and therefore, learned a shared encoder-decoder LLM with multi-modal curriculum learning. ImageBind-LLM~\\citep{han_imagebind-llm_2023} adopted ImageBind, a cross-modal encoder bundling six modalities (including images) to a shared embedding space, and adapted the LLM with a frozen image encoder. While both works extended visual LLMs to other domains, in addition to the considerable amount of training data, they are bundled to a specific architecture, hindering the ability to adapt them to a new modality.\n\nMeanwhile, following the VLM framework, a few works proposed audio-only LLMs where a pair of audio clip and text token are used as inputs for text generation. LTU~\\citep{gong_listen_2023} bridged audio with language modalities by end-to-end finetuning on an instruction-based dataset. Pengi~\\citep{deshmukh_pengi_2023} applied multi-task learning to leverage off-the-shelf datasets, alleviating the data-scarcity issue. Still, they are restricted to two domains (i.e., audio and language). They also cannot address tasks beyond the [audio, question, answer] format, e.g., few-shot audio classification~\\citep{liang_adapting_2023}. One question thereby arises: \\textit{Can we adapt LLMs/VLMs to the audio domain by simply encoding sound clips as acoustic prompts?}\n\nIn this work, we introduce APT (\\textbf{A}coustic \\textbf{P}rompt \\textbf{T}uning), an acoustic adapter that extends LLMs and VLMs to audio understanding and reasoning tasks using soft prompts only. Specifically, APT encodes audio clips into audio feature maps and then uses an audio aligner to generate acoustic prompts conditioned on both input instructions and the audio feature maps. When training APTs, a multi-task learning strategy is adapted by formulating diverse audio tasks in a sequence-to-sequence format. Besides popular audio tasks (such as audio tagging and audio captioning), APT makes full use of publicly-available datasets by training on three new tasks, namely query-based sound event detection, temporal event retrieval, and sound event counting, to learn fine-grained audio features. In addition, we improve the audio language model framework by juxtaposing acoustic prompts with text embeddings. Rather than applying soft prompts as a prefix to the input texts, the improved framework exerts no constraints on the format of the input sequence. Therefore, the APT-enhanced LLMs, namely APT-LLM, can analyse multiple audio clips in a single feed-forward process, facilitating more audio understanding tasks, such as few-shot audio classification and audio reasoning. To further evaluate models' reasoning ability, we propose a new task referred to as natural language audio reasoning (NLAR) which is devised to distinguish, compare, and summarise two audio clips. Experiments on existing audio understanding tasks, including audio tagging, audio captioning, and few-shot audio classification, show that APT-LLM achieves performance on par with those obtained by audio language models or even domain-expert models. APT also yields a good performance on the proposed NLAR, indicating its capacity to comprehend over a single audio clip. Finally, quantitative studies are conducted to demonstrate that APT improves the performance of a VLM in the audio-visual question and answering (AVQA) task.\n\nOur contributions are summarized as below:\n\\begin{itemize}\n    \\item An acoustic adapter is introduced to extend LLMs and VLMs to the audio modality by soft prompting. To mitigate data scarcity in the audio domain, we improve the present multi-task training approach by devising new tasks and their corresponding prompts during training. Leveraging the annotations in off-the-shelf databases, APT-LLM learns acoustic embeddings with fine-grained features from task discrepancy.\n    \n    \\item APT formulates diverse audio tasks as a sequence-to-sequence task where generated text is conditioned on interleaved audio-text tokens. Without any constraints on the input format, APT-LLM is not only able to solve different tasks according to the diverse instructions, but also to exploit the correlation among different audio clips in the same sequence. To the best of our knowledge, APT-LLM is the first audio-language model reasoning beyond a single audio clip.\n    \n    \\item Natural language audio reasoning, a new audio comprehension task, is proposed to distinguish, compare, and summarise two audio clips. Compared to existing audio tasks, this new task not only evaluates model ability to understand an audio clip, but also requires models to analyse the content of two recordings by comparison and summarisation. APT-LLM is then benchmarked on this task.\n    \n    \\item BLIP-2~\\citep{li_blip-2_2023} coupled with APT (namely APT-BLIP-2) is studied qualitatively and quantitatively on the audio-visual question and answering task~\\citep{yang_avqa_2022}. Without further finetuning, APT-BLIP-2 can work with the visual modality directly, showcasing an efficient approach for extending multi-modal LLMs to a new modality.\n\n\\end{itemize}\n\\section{Related works} \\label{sec:related_works}\n\\textbf{Multimodal language models.}\nFrom recent advances, LLMs~\\citep{touvron_llama_2023,chiang_vicuna_2023,openai_gpt-4_2023} has exhibited astonishing comprehending and reasoning capacity. Driven by the open-world knowledge in LLMs, a variety of visual language models have been proposed with different alignment methods to integrate image/video data to text tokens~\\citep{alayrac_flamingo_2022,li_blip-2_2023,dai_instructblip_2023,zhang_llama-adapter_2023}. However, most of them are restricted to the visual domain, largely due to the lack of training data in other domains (such as audio) and modality discrepancies. Recently, ImageBind-LLMs~\\citep{han_imagebind-llm_2023} bridged the image encoder of ImageBind~\\citep{girdhar_imagebind_2023}, a six-modality language model, with an LLM and used visual tokens as soft prompts within the language model. UniVAL~\\citep{shukor_unified_2023} uniformed the input/output, the architecture, and the training object of multimodal LLMs and then devised a curriculum learning for gradual exposure to new modality. While both works adapted VLMs to other domains, they demands massive multimodal data to train the overall networks from scratch. Instead, this work investigates a domain-specific adapter that can be applied to extend any existing VLM/LLM to an additional modality (such as audio). \n\n\\textbf{Audio language models.} Following VLM, some works built audio language models for sound-only tasks. SpeechGPT~\\citep{zhang_speechgpt_2023} collected a speech-text instruction dataset, thereby learned to perceive and generating speech content in the audio. LTU~\\citep{gong_listen_2023} rendered an open-end dataset, containing 3.7M [audio, question, answer] tuples, and learned with a perception-to-understanding curriculum. While the aforementioned models achieved a good audio comprehension ability, they required a uniform input format as a triplet tuple. To work around this question, Pengi~\\citep{deshmukh_pengi_2023} proposed a multi-task framework where an audio language model is trained with off-the-shelf audio datasets by prompted with different predefined questions. This work differs from these prior works in three-fold: 1) Rather than an audio-only language model, APT explores how to adapt existing VLMs and LLMs to the sound domain; 2) APT-LLM improves the multi-task framework by designing three new training tasks. By accessing existing datasets from different aspects, APT-LLM learns a fine-grained audio representation, and 3) APT-LLM re-frames the present input format, namely [audio, question, answer], to let audio and text alternate in a sequence. In this way, APT-LLM is able to ingest more than one audio clip in a single feed-forward, unleashing it to more audio tasks. To the best of the knowledge, APT-LLM is the first model that integrates in-context learning with multi-task training.\n\n\\section{Method} \\label{sec:APT LLMs}\nCurrent audio LLMs~\\citep{gong_listen_2023,deshmukh_pengi_2023} learned to bridge audio with language by framing popular audio tasks (e.g., classification and captioning tasks) to the audio-conditioned text generation problem. Going beyond the [audio, question, answer] format, APT-LLM encodes multiple audio clips in one feed-forward process and juxtaposes them with text embeddings without any order constraint. The more flexible training paradigm mitigates the need for high-quality data and massive databases, and thus reduces required computations. Moreover, juxtaposing audio clips with texts enables APT-LLM to address more comprehensive reasoning tasks, such as natural language audio reasoning. We first discuss the overall architecture of APT-LLM in Section~\\ref{subsec:architecture}, and then elaborates APT-LLM learning objective in Section~\\ref{subsec:learning_objective} and the training recipe in Section~\\ref{subsec:multi_task_learning}. In Section~\\ref{subsec:audio_reasoning_task}, we define the natural language audio reasoning task, a new task to evaluate the audio comprehension ability of models.\n \n\\subsection{Architecture} \\label{subsec:architecture}\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[scale=0.8]{src/lam.pdf}\n    \\caption{Illustration of the proposed APT-LLM. APT-LLM is constituted by three components: an audio encoder, an audio aligner, and a LLM. The audio encoder extracts audio feature maps from input spectrograms. The audio aligner then projects each audio feature map to 32 acoustic embeddings according to the input text. These acoustic embeddings, together with the added embeddings of the audio token ``$<$AUDIO$>$'', are juxtaposed with text embeddings. The interleaved audio-text embeddings are fed into the LLM to generate the output text. APT-LLM can ingest multiple audio clips in a sequence and thus benefit from diverse tasks during training.}\n    \\label{fig.:lam}\n    \\vspace{-0.3cm}\n\\end{figure}\n\nThe overall structure of APT-LLM is illustrated in Figure~\\ref{fig.:lam}, with main components of an audio encoder, an audio aligner, and a large language model. APT-LLM alternates audio clips with text tokens without any format constraints and thus benefits from task diversity and large-scale pretraining.\n\n\\textbf{Audio encoder: from spectrograms to feature maps.} We use Audio-MAE~\\citep{huang_masked_2022}, a vanilla 12-layer transformer encoder that learns to reconstruct randomly-masked spectrogram patches during training, as the audio encoder. Rather than using the last layer that finetuned for classification tasks, we apply the output feature map from the penultimate block of an Audio-MAE to encode fine-grained patterns in the sound.\n\n\\textbf{Audio aligner: from 10-second feature maps to a fixed number of audio tokens.} This module connects the audio encoder to the frozen language model as shown in Figure~\\ref{fig.:lam}. It ingests a text prompt together with a variable number of audio feature maps extracted by the audio encoder as input and produces a fixed number of acoustic embeddings. Following the implementation of \\citep{li_blip-2_2023}, four transformer blocks constitute our audio aligner where 32 trainable embeddings attend to the input text tokens and extract the relevant information from the audio feature maps. Resampling a varying-length of audio embeddings to 32 acoustic embeddings, APT aligner reduces the computational complexity in following attention mechanism while filtering out the information irrelevant to the input text. \n\n\\textbf{Large language model: from interleaved audio-text tokens to generated text.} The language model predicts the output text by taking into account the previous generated texts and the input audio-text tokens. We freeze all parameters in the language model during training. In addition to existing works~\\citep{li_blip-2_2023,gong_listen_2023}, we add before each audio clip a learnable audio token, ``$<$AUDIO$>$'', as a special token to indicate the beginning of audio tokens. We find this token helps the language model to distinguish audio tokens from text tokens when interleaving them together.\n\n\\subsection{Learning objective}  \\label{subsec:learning_objective}\nIn order to motivate our training framework, we first present the learning objective used in existing work~\\citep{deshmukh_pengi_2023}. Let an audio-text pair in [audio, question, answer] format be referred to as $(a, t, g)$ where $a$, $t$, $g$ are the audio clip, input text, and output text, respectively, and $\\mathbf{X}$ be input sequential embeddings to the language model. To align the audio modality to the language modality, an audio encoder $\\mathcal{A}$ and an audio aligner $\\mathcal{M}$ project the audio $a$ into a sequence $\\mathbf{X}_{\\mathrm{audio}}$:\n\\begin{equation} \\label{eqn.:extract_audio}\n    \\mathbf{X}_{\\mathrm{audio}} = \\mathcal{M}_{\\theta}(\\mathcal{A}_{\\phi}(a, t)),\n\\end{equation}\nwhere $\\phi$ and $\\theta$ are the parameters of the audio encoder $A$ and the aligner $\\mathcal{M}$. The audio embeddings are used as a prefix and then concatenated with the input text embeddings as\n\\begin{equation} \\label{eqn.:concat}\n    \\mathbf{X}_{\\mathrm{audio;text}} =  \\mathcal{C}(\\mathbf{X}_{\\mathrm{audio}}, \\mathbf{X}_{\\mathrm{text}}) = \\mathcal{C}(\\mathcal{M}_{\\mathcal{\\theta}}(\\mathcal{A}_{\\phi}(a)), \\mathcal{W}_{\\psi}(t)),\n\\end{equation}\nwhere $\\mathcal{C}$ is a concatenating function and $\\psi$ denotes the parameters of the word embedding layer $\\mathcal{W}$ in the language model. Assuming the length of the concatenated embeddings $\\mathbf{X}_{\\mathrm{audio;text}}$ be $L$, the parameters of the audio LLM are optimised by measuring the probability distribution of the next token conditioned on its previous tokens:\n\\begin{equation} \\label{eqn.:next_token_pred}\n    p(\\mathbf{X}_{\\mathrm{pred}}|\\mathbf{X}_{\\mathrm{audio}}; \\mathbf{X}_{\\mathrm{text}}) = \\prod_{i=L+1}^{L+|g|}p_{\\phi,\\theta,\\psi}(\\mathbf{x}_i|X_{\\mathrm{audio;text},<i}; \\mathbf{X}_{\\mathrm{pred},<i}),\n\\end{equation}\nIn this way, prevailing LLMs are able to unify many audio-to-text tasks in a sequence-to-sequence manner. However, not all understanding tasks can be fitted into the format of [audio, question, answer] (e.g., to learn a new concept using a handful of labelled audio examples), calling for a new paradigm that can exploit diverse tasks in a uniform input/output format.\n\nWe thereby propose a new learning framework in which interleaved audio-text embeddings are used as the LLM's input such that the model is able to leverage and learn from more diverse tasks during training. Let $\\mathbf{a}$ and $\\mathbf{t}$ be audio clips and input text, and $g$ still be output text. Assuming both $\\mathbf{a}$ and $\\mathbf{t}$ have $N$ different elements, the input audio-text pairs are denoted as $[(a^i, t^i)]_{i=1}^N$ where $a^i$ and $t^i$ are the $i$-th audio clip and input text, respectively. Eqn. (\\ref{eqn.:concat}) can be re-written as\n\\begin{equation} \\label{eqn.:interleave}\n    \\mathbf{X}_{audio;text}\\!=\\!\\mathcal{I}(\\mathbf{X}_{audio}, \\mathbf{X}_{text}) = [\\mathcal{M}(\\mathcal{A}_{\\phi}(a_{1}, t_{1})), T_{\\psi}(t_{1}), \\ldots, \\mathcal{M}(\\mathcal{A}_{\\phi}(a_{N}, t_{N})), T_{\\psi}(t_{N})],\n\\end{equation}\n\nwhere $\\mathcal{I}$ is the function that alternating acoustic embeddings with text embeddings. In this way, APT-LLM can integrate multiple audio clips in the input sequence, enabling itself to learn from more audio understanding tasks.\n\n\\begin{table}[]\n\\centering\n\\caption{Multi-task learning strategy adopted by APT-LLMs. ``\\#Audio samples'' denote the number of audio clips in the dataset. Stage 0-2 denotes audio-text alignment, learning from single audio clips, and learning from multiple clips, separately.}\n\\label{tab.:multi-task_learning}\n\\resizebox{\\columnwidth}{!}{%\n\\begin{tabular}{@{}lccccccc@{}}\n\\toprule\n\\multirow{2}{*}{Task} &\n  \\multicolumn{3}{c}{Training stages} &\n  \\multirow{2}{*}{Dataset} &\n  \\multirow{2}{*}{\\#Audio samples} &\n  \\multirow{2}{*}{Durations} &\n  \\multirow{2}{*}{Setup} \\\\ \\cmidrule(lr){2-4}\n                                  & 0 & 1 & 2 &             &      &       &            \\\\ \\midrule\nAudio tagging                     & \\ding{51}  & \\ding{51}  & \\ding{51}  & AudioSet    & 2M   & 5.8kh & train/test \\\\ \\midrule\n                                  &   &   &   & Wavcaps     & 400k & 7.6kh &            \\\\\nAudio captioning                  & \\ding{51}  & \\ding{51}  & \\ding{51}  & AudioCaps   & 39k  & 108h  & train/test \\\\\n                                  &   &   &   & Clotho v2   & 7k   & 31h   &            \\\\ \\midrule\nAudio question and answering      &   & \\ding{51}  & \\ding{51}  & Clotho AQA  & 2k   & 12h   & train      \\\\ \\midrule\nQuery-based sound event detection &   & \\ding{51}  & \\ding{51}  & AudioSet-SL & 81k  & 226h  & train      \\\\ \\midrule\nTemporal event retrieval          &   & \\ding{51}  & \\ding{51}  & AudioSet-SL & 81k  & 226h  & train      \\\\ \\midrule\nSound event counting              &   & \\ding{51}  & \\ding{51}  & AudioSet-SL & 81k  & 226h  & train      \\\\ \\midrule\nFew-shot audio classification     &   &   & \\ding{51}  & AudioSet    & 2M   & 5.8kh & train/test \\\\ \\midrule\nNatural language audio reasoning  &   &   & \\ding{51}  & NLAR        & 0.2k & 1.2h  & train/test \\\\ \\bottomrule\n\\end{tabular}%\n}\n\\vspace{-0.3cm}\n\\end{table}\n\n\\subsection{Multi-task learning strategy}  \\label{subsec:multi_task_learning}\nWith the uniform input/output format, APT-LLM is able to learn from a large variety of audio tasks and thus benefiting from the diverse training datasets. As shown in Fig.~\\ref{tab.:multi-task_learning}, instead of passing through all training data directly, APT-LLM is trained through:\n\n\\textbf{Audio-text alignment.} Before coupled with a LLM, we pretrain APT audio aligner to bridge the audio modality and the text modality. To this end, we freeze the other components and optimise parameters of the audio aligner with audio-text pairs from AudioSet~\\citep{gemmeke_audio_2017} and WavCaps~\\citep{mei_wavcaps_2023}. During training, a fixed number of acoustic embeddings are learnt to extract relevant information from the audio feature maps according to the input text tokens. Following~\\cite{li_blip-2_2023}, the audio aligner learns with triplet training objectives: audio-text matching, Audio-grounded text generation, and audio-text contrastive (See more in Appendix~\\ref{appendix:audio-text_alignment})\n\n\\textbf{Learning from single audio clip.} \nAfter APT has extracted acoustic embeddings according to the input text, the following LLM learns to project these tokens to the word embeddings of the targeted LLM. APT-LLM is thus trained with multiple tasks using various prompts (see more in Appendix~\\ref{appendix:multitask_prompt}). In addition to existing audio tasks, namely audio tagging, audio captioning, and audio question and answering, we design three new tasks: (1) \\textit{Query-based sound event detection} that aims to train a model to predict the onset and offset time of a specific sound event; (2) \\textit{Temporal event retrieval} that is to recognise sound events occurred in a specific period, and (3) \\textit{Sound event counting} that requires a model to count the frequency of a specific sound event in a recording. Instead of rendering datasets, we exploit the publicly-available AudioSet with strong labels~\\citep{hershey_benefit_2021} using different prompts (see more in~\\ref{appendix:multitask_prompt}). This multi-task framework facilitates APT-LLM's learning from diverse datasets, including AudioSet~\\citep{gemmeke_audio_2017}, WavCaps~\\citep{mei_wavcaps_2023}, AudioSet with strong labels~\\citep{hershey_benefit_2021}, Clotho~\\citep{drossos_clotho_2020}, AudioCaps~\\citep{kim_audiocaps_2019}, and Clotho-AQA~\\citep{lipping_clotho-aqa_2022}.\n\n\\textbf{Learning from multiple audio clips.}\nIn addition to the aforementioned tasks, APT-LLM learns from two additional tasks by juxtaposing more than one audio clips with input text. Specifically, few-shot audio classification and natural language audio reasoning are added to the multi-task training framework in this stage. On the one hand, for the few-shot audio classification, APT-LLM predicts labels of sound events by exploiting the correlation between input audio clips. On the other hand, APT-LLM is required to compare and summarise two different sounds in the natural language audio reasoning task (see the following Section~\\ref{subsec:audio_reasoning_task}). Trained on these two tasks, APT-LLM learns to analyse beyond a single recording and answer questions as per input questions. We adopts AudioSet~\\citep{gemmeke_audio_2017} and the proposed datasets for few-shot audio classification and natural language audio reasoning, respectively.\n\n\\subsection{Natural language audio reasoning task} \\label{subsec:audio_reasoning_task}\n\n\\begin{table}[t]\n\\centering\n\\caption{An example demonstrating APT-LLM's capacity of audio reasoning. It requires audio networks to comprehend recordings and reasoning across multiple recordings.}\n\\label{tab.:nlar_example}\n\\resizebox{\\columnwidth}{!}{%\n\\begin{tabular}{@{}ll@{}}\n\\toprule\n\\multicolumn{2}{l}{Natural Language Audio Reasoning (NLAR) example: \\textit{``Where is the sudden sound?''}} \\\\ \\midrule\nUser         &  \\\\\n             & \\includegraphics[width=1.0\\columnwidth]{src/wav.pdf} \\\\\n             & Question: Which recording has a more sudden and startling sound event?             \\\\  \\midrule\nAPT-LLM      & First.      \\\\\nGround truth & first       \\\\ \\bottomrule\n\\end{tabular}%\n}\n\\vspace{-0.3cm}\n\\end{table}\n\nOne of the complex reasoning ability of human is to learns across different pieces of sounds, understanding what happening in each audio and analysing the content of different audio by comparison and summarisation. However, existing audio tasks focus on analysing acoustic scenarios in an independent recording by recognising the inside sound events~\\cite{kong_panns_2020} and/or retrieval their spatio-temperal information~\\cite{politis_starss22_2022}. We thus propose natural language audio reasoning (NLAR), a new task where the model is required to answer questions by explicitly comparing or summarising two different audio recordings. Table~\\ref{tab.:nlar_example} showcases an example in the NLAR task. An audio system takes two audio clips together with a free-form text query as input and is expected to answer the question by taking into consideration the both audio. For details of the design process and examples of the proposed natural language audio reasoning task, please refer to Appendix~\\ref{appendix:nlar}. Compared to existing audio tasks, the proposed audio reasoning task features three notable differences:\n\n\\textbf{Comprehension of multiple audio clip}: This task requires a model to answer open-ended questions by comparing or summarising the content of two different audio clips. The model must first comprehend the two audio clips as per the raised question separately and answer the question by taking into account the two audio inputs. An example of the audio reasoning task can be found in Table~\\ref{tab.:nlar_dataset}. \n\n\\textbf{Diverse question types}: Questions for natural language audio reasoning task assess diverse auditory aspects, such as the presence, the frequency, and acoustic features of sound events. Therefore, the model should not only ground the sound events in the recordings, but also retrieve relevant information as per the input question. \n\n\\textbf{Effects of the chronological order}: Compared to existing audio tasks, e.g.,~\\citep{li_blip-2_2023} and~\\citep{gong_listen_2023}, the proposed audio reasoning task emphasises the order of the audio recordings in a sequence. In other word, the answer associated with the audio pair ``[Audio A, Audio B]'' could be different with the answer associated with  ``[Audio B, Audio A]'' when their questions are the same. In this way, we expect audio understanding models to be able to attend to different portions of the input sequence when the question vary. \n\nBy evaluating audio language models on the natural language audio reasoning task, we achieve more comprehensive assessment of audio language models.\n\n\\section{Experiments} \\label{sec:experiments}\nAPT was first coupled with LLMs (i.e., APT-LLM) and evaluated as a general-purposed audio learner on a variety of existing audio-related benchmarks, including audio tagging, audio captioning, and few-shot audio classification. To further assess its ability in comprehending two audio clips of interest, APT-LLM was further benchmarked on the natural language audio reasoning task. In addition to audio comprehension, we also experimented and analysed (quantitatively and qualitatively) APT as an zero-shot adapter to BLIP-2~\\citep{li_blip-2_2023,dai_instructblip_2023}, a state-of-the-art VLM.\n\n\\begin{table}[t]\n\\centering\n\\caption{Zero-shot performance comparison with audio language models. We group the methods in terms of their training strategy. ``\\#Params.'' denotes the number of trainable parameters and ``\\#Pairs'' represents the number of audio-text pairs. $\\uparrow$ indicates the higher number, the better performance.}\n\\vspace{1em}\n\\label{tab.:existing_tasks}\n\\resizebox{\\columnwidth}{!}{%\n\\begin{tabular}{@{}llllcc@{}}\n\\toprule\nModel & \\#Params. & \\#Pairs & AudioSet (mAP$\\uparrow$) & AudioCaps (SPICE$\\uparrow$) & Clotho (SPICE$\\uparrow$) \\\\ \\midrule\n\\textit{Audio-language models trained with the contrastive loss} &        &      &      &      &      \\\\\nAudioCLIP~\\citep{guzhov_audioclip_2022}                 & 30M    & 2M   & 25.9 & -    & -    \\\\\nCLAP~\\citep{elizalde_clap_2023}                         & 190M   & 128k & 5.8  & -    & -    \\\\ \\midrule\n\\textit{One-for-all models for various audio tasks}              &        &      &      &      &      \\\\\nLTU~\\citep{gong_listen_2023}                            & 96M    & 5.7M & 18.5 & 17.0 & 11.9 \\\\\nPengi~\\citep{deshmukh_pengi_2023}                       & $>$191M & 3.4M & -    & 18.2 & 12.6 \\\\\n\\rowcolor{lightgray!60}APT-LLM                          & 101M   & 2.6M & 14.7 & 17.1 & 11.6 \\\\ \\bottomrule\n\\end{tabular}%\n}\n\\end{table}\n\n\\subsection{Experiment setup} \\label{subsec:experiment_setup}\nOur models were implemented relying on the BLIP-2 framework~\\citep{li_blip-2_2023}. We used Audio-MAE~\\citep{huang_masked_2022} as the audio encoder in all APT models we developed. Considering Audio-MAE only contains 100M parameters, we used a two-layer transformer as the aligner to bridge the audio and text domains. Without an explicit statement, we coupled APT with Vicuna 7B v1.1~\\citep{chiang_vicuna_2023} for evaluation. We testified APT-LLM with two close-ended datasets: AudioSet~\\citep{gemmeke_audio_2017} and ESC-50~\\citep{piczak_esc_2015}; and four open-ended datasets: Clotho~\\citep{drossos_clotho_2020}, AudioCaps~\\citep{kim_audiocaps_2019}, natural language audio reasoning (in Section~\\ref{subsec:audio_reasoning_task}), and audio-visual question and answering (AVQA)~\\citep{yang_avqa_2022}.\n\nAdam optimiser was used for model training. We applied Warmup strategy in the first 2K steps and used a cosine linear learning rate in the following steps. We trained the APT models using three NVIDIA A100 (40G) GPUs. The audio-text alignment pretraining and multi-task training took 5 days separately.\n\n\\subsection{Comparison with existing approaches} \\label{subsec:comparison_with_existing_approaches}\nWe compare APT-LLM against the state-of-the-art specialised systems (i.e., the networks trained with task-specific data) and previous audio language models on existing tasks, including audio tagging, audio captioning, and few-shot audio classification.\n\n\\textbf{Audio tagging} requires models to predict classes of test samples from a predefined label set. We evaluated the models on the AudioSet dataset~\\citep{gemmeke_audio_2017}. During inference, APT-LLM was prompted using the sentence ``\\textit{Summarize the audio with key words.}'' Since APT generates free-form texts directly, we used the APT text encoder pretrained in the stage 1 to encode generated answers and the given classes names to text embeddings. Afterwards, cosine similarity is calculated as the classification probably.\nConsistent with the findings in previous work~\\citep{gong_listen_2023}, Table~\\ref{tab.:existing_tasks} shows a performance gap between audio language models and task-specific models. This is expected since the latter addresses the classification task as a close-end problem, with much lower complexity than open-ended problem where models need to search across the entire word embedding space. In addition, we found that the performance of the text encoder greatly impacts the classification result when evaluating the generated answers. This finding can be explained by the fact that  word embeddings of different classes should be sparse enough to when measuring their distance to the embeddings of generated answers.\n\n\\begin{table}[t]\n\\centering\n\\caption{Performance comparison in audio captioning tasks. $\\uparrow$ indicates the higher number, the better performance.}\n\\label{tab.:existing_tasks}\n\\vspace{1em}\n\\resizebox{\\columnwidth}{!}{%\n\\begin{tabular}{@{}lcccc@{}}\n\\toprule\nModel                                                   & \\multicolumn{2}{c}{AudioCaps}   & \\multicolumn{2}{c}{Clotho}      \\\\ \\cmidrule(l){2-5} \n                                                        & SPICE $\\uparrow$ & SPIDEr $\\uparrow$ & SPICE $\\uparrow$ & SPIDEr $\\uparrow$ \\\\ \\midrule\n\\textit{Specialised systems trained with task-specific examples} &                &                &                &                \\\\\nPANNs-BART~\\citep{xu_investigating_2021}                & 0.153          & 0.183          & 0.083          & 0.127          \\\\\nCNN-GPT2~\\citep{kim_prefix_2023}                        & 0.167          & \\textbf{0.438} & 0.111          & 0.215          \\\\\nWSAC+PD~\\citep{kouzelis_weakly-supervised_2023}         & 0.173          & 0.403          & 0.123          & 0.247          \\\\ \\midrule\n\\textit{One-for-all models for various audio tasks}              &                &                &                &                \\\\\n\\rowcolor{lightgray!60}APT-LLM                          & \\textbf{0.191} & 0.402          & \\textbf{0.132} & \\textbf{0.248} \\\\ \\bottomrule\n\\end{tabular}%\n}\n\\vspace{-0.3cm}\n\\end{table}\n\n\\textbf{Audio captioning} is the task where models are supposed to generate free-form description according to an input recording. The sentence ``\\textit{Describe the audio clip concisely}.'' is applied as the input prompt. We finetune APT-LLM two epochs on the training split of AudioCaps~\\citep{kim_audiocaps_2019} and Clotho~\\citep{drossos_clotho_2020} datasets and compare it with the captioning models trained on the both tasks. As shown in Table~\\ref{tab.:existing_tasks}, APT-LLM achieves the best performance on both AudioCaps and Clotho datasets in terms of SPICE and SPIDEr.\n\n\\begin{table}[h]\n\\centering\n\\caption{Accuracy (\\%) of various methods on ESC-50 in the few-shot settings.}\n\\label{tab.:few_shot}\n\\begin{tabular}{llc}\n\\toprule\n                                          & \\multicolumn{2}{c}{Accuracy$\\uparrow$} \\\\ \\hline\n                                          & 5-way          & 12-way         \\\\ \\cline{2-3}\n\\multicolumn{3}{l}{\\textit{Specialised systems trained with task-specific examples}} \\\\\nProtoNet~\\citep{snell_prototypical_2017}   & 88.2          & 77.7          \\\\\nMatchNet~\\citep{vinyals_matching_2016}     & 86.8          & 71.8          \\\\\nHPN~\\citep{liang_leveraging_2022}          & 88.7          & 78.7          \\\\ \\midrule\n\\multicolumn{3}{l}{\\textit{Audio language models trained with constractive learning}} \\\\\nTIP-adapter~\\citep{zhang_tip-adapter_2022} & 97.5          & 95.6          \\\\\nTreff adapter~\\citep{liang_adapting_2023}  & 98.5          & 96.3          \\\\ \\midrule\n\\multicolumn{3}{l}{\\textit{One-for-all models for various audio tasks}} \\\\\n\\rowcolor{lightgray!60}\nAPT-LLM                                & 91.0          & 54.2          \\\\ \\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\textbf{Few-shot audio classification} is to classify test audio clips using labelled audio examples. Models were evaluated in the $N$-way $K$-shot problem where: (1) there are $N$ classes in the classification task, and (2) each class contains $K$ different audio examples. Following previous works~\\citep{liang_adapting_2023} in this task, we tested APT-LLM in the 5/12-way 5-shots settings. In our free-form query design, we prompt the few shot classification question by adding the query audio clip together with the input text ``\\textit{This is a sound of}'' to the end of the sequence of labelled audio examples and their corresponding label texts. We implemented the same evaluation protocol to all few-shot learners for a fair comparison. As shown in Table~\\ref{tab.:few_shot}, APT-LLM outperforms the task-specific models~\\citep{snell_prototypical_2017,vinyals_matching_2016,liang_leveraging_2022} in the 5-way 5-shot setting while having a competitive performance compared to CLAP Adapters~\\citep{zhang_tip-adapter_2022,liang_adapting_2023}. In the 12-way 5-shot problem, however, we can observe a performance degradation of APT. We suspect this may be due to the limitation of attention mechanism in LLMs when addressing very long sequences (12-way 5-shot modelling results in a sequence of roughly 2420 tokens). It should be noted that while APT-LLM was trained with 4-way 1-shot tasks, it can generalise to other few-shot settings, suggesting that APT-LLM learns to act as a few-shot classifier rather than memorising the expected answers.\n\n\\subsection{Evaluation on natural language audio reasoning} \\label{subsec:evaluation_on_nlar}\n\\begin{wraptable}{r}{6.3cm}\n    \\caption{Benchmarking APT on the natural language audio reasoning task.}\n    \\label{tab.:audio_reasoning}\n    \\begin{tabular}{lc}\n    \\toprule\n    Model               & Accuracy$\\uparrow$ (\\%)         \\\\ \\hline\n    the baseline        & 29.9              \\\\\n    APT-Vicuna v1.1 & 62.9              \\\\\n    APT-Vicuna v1.5 & \\textbf{63.8}     \\\\ \\bottomrule\n    \\end{tabular}\n\\end{wraptable}\n\nSince APT-LLM is able to ingest multiple audio clips in a single feed-forward process, we investigated APT-LLM with natural language audio reasoning for which a model is expected to distinguish, compare, and summarise two audio clips (see Appendix~\\ref{appendix:nlar}). To the best of the knowledge, there is no previous work evaluating model ability to comprehend more than one recording. We thus contrast APT-LLM to the baseline where predictions are fixed to a specific answer (we used ``yes'' as the fixed answer after several attempts). Table~\\ref{tab.:audio_reasoning} demonstrates that APT-Vicuna v1.5 achieves 63.78\\% mAP score, outperforming the baseline by a large margin. This result suggests that APT-LLM is able to not only comprehend the content in an audio clip but also analyse more than one audio recordings by comparison and summarisation. It is worth noting that there is marginal improvement when upgrading Vicuna from v1.1 to v1.5, indicating the performance of language models is not the bottleneck in this task, at least for the two used in our study.\n\n\\subsection{Evaluation on zero-shot audio-visual tasks} \\label{subsec:evaluation_on_audio-visual_tasks}\n\n\\begin{wraptable}{r}{7.5cm}\n    \\caption{Performance comparison between different modalities in audio-visual learning.}\n    \\label{tab.:av_learning}\n    \\begin{tabular}{@{}lcc@{}}\n    \\toprule\n    Model                         & Modal       & Accuracy$\\uparrow$         \\\\ \\midrule\n    BLIP-2~\\citep{li_blip-2_2023} & Video-only  & 42.9             \\\\\n    APT-LLM             & Audio-only  & 27.7                  \\\\\n    APT-BLIP-2                  & Audio-video & \\textbf{59.7}    \\\\ \\bottomrule\n    \\end{tabular}\n\\end{wraptable}\n\nAPT was also experimented as an audio adapter for an existing VLM, BLIP-2~\\citep{li_blip-2_2023}. BLIP-2 consists of a frozen image encoder, a Qformer, a projection layer, and a frozen Vicuna v1.1. Therefore, we integrated the APT trained with the same language model to BLIP-2 by interleaving acoustic prompts with text embeddings. We refer to the APT-enhanced BLIP-2 as APT-BLIP-2. Of note, although we selected BLIP-2 as our backbone model, APT can be easily adapted to another language model. APT-BLIP-2, together with other multimodal language models, was investigated on a audio-visual question and answering dataset where models are expected to choose one out of four options by using both audio and video modalities. We experimented APT-BLIP-2 on the subset of the AVQA dataset~\\citep{yang_avqa_2022} as many video links associated with the AVQA test segmentation were no longer available on the internet at the time of the experiment. As shown in Table~\\ref{tab.:av_learning}, APT-BLIP-2 yielded a better performance than video-only and audio-only counterparts, indicating the adaptation to the audio domain benefits models' learning from the content of video. \n\n\\subsection{Limitations} \\label{subsec:limitations}\nIn this work, we devised APT to align acoustic embeddings with text embeddings of language models. Now that the word embeddings change when switching to a different language model, even if their architectures remain the same, each language model calls for a dedicated APT for adaptation. In addition, APT-LLM was not trained with instruction-based datasets, and thus, has limited ability to response to questions excluded from the training set. Finally, we purposely focused APT-LLM training and experimentation on general-purpose audio understanding tasks, therefore, unlikely it can understand speech and music audios.\n\n\\section{Conclusions} \\label{sec:conclusions}\nWe proposed APT, a general-purpose acoustic adapter that extends LLM/VLM to the audio domain. We showed that LLM coupled with APT is a multi-task audio learner that not only achieved a competitive performance across various audio understanding tasks but also be capable of in-context learning when fed with a few labelled examples. We also benchmarked APT-LLM's audio comprehension ability via the natural language audio reasoning task, a new task that requires a model to distinguish, compare, and summarise two different audio clips. Last but not least, it is evident from our study on audio-visual learning that encoding sound clips as word tokens is an efficient approach to adapt LLM/VLM to the audio domain. Future works can extend audio language models into comprehension of music and speech audio, and make them more aligned with human perception via instruction tuning. It is also interesting to investigate how audio language models handle errors in the in-context learning.\n\n\\newpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2211.06687v4.tex",
        "arXiv-2308.11923v1.tex",
        "arXiv-2312.00249v1.tex"
    ],
    "group_id": "group_58",
    "response": "### Summary of Recent Advances in Contrastive Learning for Audio-Text Representation and Audio Difference Captioning\n\n#### Title: Recent Advances in Contrastive Learning for Audio-Text Representation and Audio Difference Captioning\n\n#### Introduction\nThe field of multimodal representation learning, particularly the integration of audio and text data, has seen significant advancements in recent years. This progress is largely driven by the application of contrastive learning techniques, which have proven effective in aligning different modalities within a shared latent space. Contrastive learning, as seen in models like Contrastive Language-Image Pretraining (CLIP) \\cite{radford2021learning}, has shown robustness and high accuracy in zero-shot settings, making it a promising approach for handling large-scale, noisy data. However, the application of contrastive learning in the audio domain is still in its infancy, with several challenges including the need for large-scale datasets, effective handling of variable-length audio inputs, and the generalization of learned representations to diverse downstream tasks.\n\nCurrent research in audio-text contrastive learning is focused on developing models that can effectively capture the semantic and acoustic features of audio and text data. These models aim to improve the performance of audio-text retrieval, audio classification, and other downstream tasks. However, existing models often suffer from limitations such as small dataset sizes, inadequate feature fusion mechanisms, and a lack of exploration into the generalization capabilities of learned representations. To address these challenges, researchers are exploring new methods and datasets that can enhance the performance and robustness of audio-text contrastive learning models.\n\nThis summary will focus on three recent papers that contribute to the field of audio-text contrastive learning and audio difference captioning. The first paper introduces a large-scale audio-text dataset and a contrastive language-audio pretraining model with feature fusion and keyword-to-caption augmentation. The second paper proposes a new task, Audio Difference Captioning (ADC), which aims to describe the semantic differences between similar audio clips using a cross-attention-concentrated transformer encoder and similarity-discrepancy disentanglement. The third paper presents Acoustic Prompt Tuning (APT), a method that extends large language models (LLMs) and visual language models (VLMs) to the audio domain through soft prompting and a multi-task learning strategy.\n\n#### Main Content of Each Paper\n\n**Paper 1: Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation**\n\nThis paper introduces a large-scale audio-text dataset, LAION-Audio-630K, which consists of 633,526 audio-text pairs. The dataset is significantly larger than previous audio-text datasets, making it a valuable resource for training contrastive language-audio models. The authors propose a contrastive language-audio pretraining model that combines different audio and text encoders, including PANN \\cite{kong2020panns} and HTSAT \\cite{gong2021ast} for audio, and CLIP transformer \\cite{radford2021learning}, BERT \\cite{devlin2018bert}, and RoBERTa \\cite{liu2019roberta} for text. The model incorporates a feature fusion mechanism to handle variable-length audio inputs and a keyword-to-caption augmentation technique to enhance the training process. The authors conduct comprehensive experiments on text-to-audio retrieval, zero-shot audio classification, and supervised audio classification tasks, demonstrating the model's superior performance in text-to-audio retrieval and state-of-the-art (SOTA) performance in zero-shot audio classification.\n\n**Paper 2: Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement**\n\nThis work introduces Audio Difference Captioning (ADC), a new task that focuses on describing the semantic differences between similar audio clips. The authors propose a cross-attention-concentrated (CAC) transformer encoder and a similarity-discrepancy disentanglement (SDD) loss function to improve the model's ability to extract and emphasize differences in the latent space. The CAC transformer encoder restricts attention weights to the mutual direction of two audio clips, while the SDD loss function decomposes the feature space into similar and discrepant parts. The authors construct an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips and human-annotated descriptions of their differences. They evaluate the proposed methods on ADC and show that the CAC transformer encoder and SDD loss function significantly improve the model's performance in describing differences between audio clips.\n\n**Paper 3: Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities**\n\nThis paper presents Acoustic Prompt Tuning (APT), a method that extends large language models (LLMs) and visual language models (VLMs) to the audio domain through soft prompting. APT uses an audio encoder to extract feature maps from input spectrograms and an audio aligner to generate acoustic embeddings conditioned on input text. The authors introduce a new task, natural language audio reasoning (NLAR), which requires models to compare and summarize two different audio clips. APT-LLM is trained on a multi-task learning framework that includes audio-text alignment, learning from single audio clips, and learning from multiple audio clips. The authors demonstrate that APT-LLM achieves competitive performance on existing audio-related benchmarks and outperforms baseline models on the NLAR task.\n\n#### Commonalities and Innovations\nAll three papers contribute to the field of audio-text representation learning and audio understanding tasks. They share a common goal of improving the performance of audio models through contrastive learning and multi-task training. However, each paper introduces unique innovations:\n\n- **Paper 1** introduces LAION-Audio-630K, a large-scale audio-text dataset, and a contrastive language-audio pretraining model that incorporates feature fusion and keyword-to-caption augmentation. The feature fusion mechanism enables the model to handle variable-length audio inputs efficiently, while the keyword-to-caption augmentation technique enhances the training process by extending labels into captions.\n- **Paper 2** proposes ADC, a new task that focuses on describing the semantic differences between similar audio clips. The authors introduce a CAC transformer encoder and an SDD loss function to improve the model's ability to extract and emphasize differences in the latent space. This task is particularly useful for applications such as machine condition and healthcare monitoring.\n- **Paper 3** presents APT, a method that extends LLMs and VLMs to the audio domain through soft prompting. APT uses a multi-task learning framework to train on diverse audio tasks, including audio tagging, audio captioning, and few-shot audio classification. The authors introduce NLAR, a new task that requires models to compare and summarize two different audio clips, demonstrating the model's ability to reason across multiple recordings.\n\n#### Comparison of Results\nThe three papers achieve varying levels of success in their respective tasks:\n\n- **Paper 1** demonstrates superior performance in text-to-audio retrieval and state-of-the-art performance in zero-shot audio classification. The model achieves comparable performance to supervised models in audio classification tasks.\n- **Paper 2** shows that the CAC transformer encoder and SDD loss function significantly improve the model's performance in describing differences between audio clips. The authors report improvements in metrics such as BLEU-4, ROUGE-L, CIDEr, and SPIDEr.\n- **Paper 3** achieves competitive performance on existing audio-related benchmarks and outperforms baseline models on the NLAR task. The authors also demonstrate that APT-BLIP-2, a VLM adapted to the audio domain, performs better than video-only and audio-only counterparts in audio-visual learning tasks.\n\n#### Conclusion\nThe three papers collectively advance the field of audio-text representation learning and audio understanding tasks. They introduce new datasets, models, and tasks that enhance the performance and generalization capabilities of audio models. LAION-Audio-630K and the proposed contrastive language-audio pretraining model in Paper 1 provide a robust framework for handling large-scale audio-text data. Paper 2's ADC task and CAC transformer encoder with SDD loss function enable models to describe subtle differences between similar audio clips, while Paper 3's APT method extends LLMs and VLMs to the audio domain through soft prompting and a multi-task learning framework. The introduction of NLAR in Paper 3 further demonstrates the model's ability to reason across multiple audio clips.\n\nFuture research directions include collecting even larger datasets, exploring more downstream tasks such as audio synthesis and separation, and extending audio language models to comprehend music and speech audios. Additionally, further investigation into the generalization and robustness of learned representations in the audio domain is warranted. The combination of contrastive learning, feature fusion, and multi-task training strategies holds promise for developing more effective and versatile audio-text models.\n\n#### Tables\nTo provide a clearer comparison of the results, the following tables summarize the key findings from each paper:\n\n**Table 1: Performance on Text-to-Audio Retrieval**\n\n| Model Setting | AudioCaps (mAP@10) | Clotho (mAP@10) |\n|---------------|-------------------|-----------------|\n| PANN+CLIP Trans. | 4.7 | 1.9 |\n| PANN+BERT | 34.3 | 10.8 |\n| PANN+RoBERTa | 37.5 | 11.3 |\n| HTSAT+CLIP Trans. | 2.4 | 1.1 |\n| HTSAT+BERT | 43.7 | 13.8 |\n| HTSAT+RoBERTa | 45.7 | 18.4 |\n| HTSAT-RoBERTa (fusion) | 36.2 | 17.2 |\n\n**Table 2: Performance on Audio Classification**\n\n| Model | AudioSet (mAP) | ESC-50 (Acc) | US8K (Acc) | VGGSound (Acc) | FSD50K (mAP) |\n|-------|---------------|-------------|------------|----------------|--------------|\n| Wav2CLIP | 41.4 | 82.6 | 73.2 | 10.0 | 43.1 |\n| AudioClip | 69.4 | 65.3 | - | - | - |\n| CLAP | 89.1 | 73.2 | 77.0 | 75.4 | 64.9 |\n| CLAP+Fusion | 88.0 | 75.8 | 75.8 | 75.3 | 64.4 |\n| CLAP+K2C Aug. | 91.0 | 77.0 | 77.0 | 46.2 | 59.7 |\n\n**Table 3: Performance on Audio Captioning**\n\n| Model | AudioCaps (SPICE) | Clotho (SPICE) |\n|-------|-------------------|----------------|\n| PANNs-BART | 0.153 | 0.083 |\n| CNN-GPT2 | 0.167 | 0.111 |\n| WSAC+PD | 0.173 | 0.123 |\n| APT-LLM | 0.191 | 0.132 |\n\n**Table 4: Performance on Few-shot Audio Classification**\n\n| Model | 5-way 5-shot | 12-way 5-shot |\n|-------|--------------|----------------|\n| ProtoNet | 88.2 | 71.8 |\n| MatchNet | 86.8 | 71.8 |\n| HPN | 88.7 | 78.7 |\n| TIP-adapter | 97.5 | 95.6 |\n| Treff adapter | 98.5 | 96.3 |\n| APT-LLM | 91.0 | 54.2 |\n\n**Table 5: Performance on Natural Language Audio Reasoning**\n\n| Model | Accuracy (mAP) |\n|-------|----------------|\n| Baseline | 29.9 |\n| APT-Vicuna v1.1 | 62.9 |\n| APT-Vicuna v1.5 | 63.8 |\n\n**Table 6: Performance on Audio-Visual Question and Answering**\n\n| Model | Modality | Accuracy |\n|-------|----------|----------|\n| BLIP-2 (Video-only) | Video-only | 42.9 |\n| APT-LLM (Audio-only) | Audio-only | 27.7 |\n| APT-BLIP-2 (Audio-video) | Audio-video | 59.7 |\n\n#### Future Research Directions\nFuture research in this field could focus on several key areas:\n\n1. **Collecting Larger Datasets**: The success of contrastive learning models is heavily dependent on the size and diversity of the training dataset. Collecting even larger and more diverse audio-text datasets can further improve the performance and generalization capabilities of these models.\n2. **Exploring New Downstream Tasks**: While the current models excel in tasks like text-to-audio retrieval and audio classification, exploring new downstream tasks such as audio synthesis, audio separation, and speech recognition can provide a more comprehensive evaluation of the learned representations.\n3. **Improving Feature Fusion Mechanisms**: The feature fusion mechanisms used in these models, such as the attention feature fusion (AFF) in Paper 1, can be further refined to handle variable-length audio inputs more efficiently.\n4. **Enhancing Audio Understanding Models**: Extending audio understanding models to comprehend music and speech audios can make them more aligned with human perception. Additionally, instruction tuning can be used to improve the models' ability to respond to specific queries and tasks.\n5. **Investigating Generalization and Robustness**: Further investigation into the generalization and robustness of learned representations in the audio domain can help identify the strengths and limitations of current models and guide future improvements.\n\nThese advancements and future research directions highlight the potential of contrastive learning and multi-task training strategies in developing more effective and versatile audio-text models."
}