{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Loki: Low-rank Keys for Efficient Sparse Attention}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nInference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of {\\em key} vectors computed in the attention block. Our\nanalysis reveals that key vectors lie in a significantly lower-dimensional\nspace, consistently across several datasets and models. Exploiting this\nobservation, we propose \\emph{\\method}, a novel sparse attention method that\nranks and selects tokens in the KV-cache based on attention scores computed in\nlow-dimensional space. Our evaluations show that \\method is able to speed up\nthe attention computation due to reduced data movement (load/store) and compute\ncosts while maintaining the efficacy of the models better than other popular\napproximation methods.\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\nAs large language models (LLMs) grow in size, deploying them for efficient\ninference presents substantial challenges, largely due to computation and\nmemory access bottlenecks in the self-attention block~\\citep{transformer},\nespecially when handling long sequences. These challenges stem from the\nautoregressive nature of attention, which generates the output one token at a\ntime. At each step, the entire preceding state, stored in the key-value (KV)\ncache, must be fetched from memory, which can sometimes exceed the size of the\nmodel parameters itself~\\citep{woosuk2023vllm}. This frequent KV-cache access\nfrom GPU DRAM to registers becomes costly, as it scales quadratically with the\noutput sequence length. In addition, matrix multiplications in the\nattention layers also have a quadratic scaling cost with sequence length,\ncompounding the overall computational burden.\n\nSeveral strategies~\\citep{zhang2023hH2O, ribar2023sparq, liu2023scissorhands}\nhave been proposed to address this challenge by reducing the computational\ncomplexity and/or memory demands associated with the self-attention mechanism.\nOne promising category of approaches focuses on approximating attention,\nemploying techniques such as quantization or using a subset of the tokens in\nthe KV-cache~\\citep{ge2024model} (sparse attention).\n\nIn contrast to other sparse attention approaches that either permanently prune\ntokens from the key-value cache~\\citep{zhang2023hH2O} or impose a fixed\nsparsity pattern~\\citep{xiao2023efficient}, our proposed method dynamically\nselects key tokens at each generation step based on approximate attention\nscores and avoids deletions.  This approach is inspired by a critical\nobservation: across a range of LLMs and datasets, key tensors consistently\noccupy a significantly lower-dimensional space than the full attention head\ndimension. For instance, in Figure~\\ref{fig:overview} (left), we show that\nacross various LLMs~\\citep{dubey2024llama3herdmodels, jiang2024mixtral}, 90\\%\nof the variance explained by PCA is captured at an effective key vector rank of\naround 80, despite the key tensor dimension being much larger (128).\n\nBased on this observation, we introduce \\method, a sparse attention method that\nleverages the low-dimensional structure of key vectors to reduce data movement\nand computation costs without significantly impacting model quality. First, we\napply PCA to keys generated from a calibration dataset, storing all principal\ncomponents but using only the top $d$ (25-50\\%) to compute approximate\nattention scores during inference. This dimensionality reduction, informed by\nour previous observation that key vectors have low effective rank, allows us to\nefficiently identify the top-$k$ (12.5-25\\%) most relevant tokens using the\napproximate scores. For these selected keys, we then revert to the full\ndimensionality to compute the final attention scores, ensuring both efficiency\nand accuracy. Figure~\\ref{fig:overview} (right) illustrates our approach.\n\nOur theoretical complexity analysis demonstrates that \\method can provide\nsignificant speedups in the attention step. However, actually realizing these\ngains requires an efficient implementation of our method to minimize data\nmovement in the additional operations introduced on top of the original self\nattention algorithm. Thus, we implement optimized sparse matrix multiplication kernels\nfor \\method~in Triton, leading to a speedup of up to 45\\% over the \nstandard HuggingFace Transformer's~\\citep{wolf-etal-2020-transformers} attention \nimplementation (\\emph{vanilla} attention) for Llama2-13B.\nFor this setting, the average degradation in model\naccuracy (measured across 6 different benchmarks and 8 different models) is\nonly 6.8\\%.\n\nOur contributions can be summarized as follows:\n\\vspace{-0.1in}\n\\begin{itemize}\n    \\item Detailed analysis showing the intrinsic low-dimensionality of keys in\nself-attention, its variation across layers for different models, and\nconsistency across different datasets.\n    \\item \\method: a sparse attention method that exploits the aforementioned\nlow dimensionality of keys to make the attention computation faster without sacrificing\nmodel quality.\n    \\item Optimized kernels for efficient implementation of \\method~in PyTorch.\n    \\item Evaluation of\n\\method\\footnote{\\url{https://github.com/hpcgroup/loki}}~on multiple LLMs and\ndownstream tasks, showing that it can achieve significant speedups with minimal\ndegradation in model quality.\n\\end{itemize} \n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[height=1.75in]{figs/dimensionality/global_postrotary_variances.pdf}\n  \\hfill\n  \\includegraphics[height=1.75in]{figs/PCA-TopK-Method-Final.pdf}\n  \\caption{Rank at which 90\\% of the variance is explained, averaged across all\nlayers and heads for different models. Full rank is represented by the black\ndashed line (left). Overview of \\method (right).}\n  \\label{fig:overview}\n\\end{figure}\n\n\\section{Background and Related Work}\n\\label{sec:bg}\nThe attention mechanism~\\citep{transformer} is at the core of the transformer\narchitecture. Consider a single attention query head with head dimension\n$D$, processing an input token sequence of length $S$. During auto-regressive\ngeneration, the output of the attention head is calculated as:\n\\begin{equation}\n    \\label{eq:attention}\n    \\boldsymbol{y} = \\mathrm{softmax}\\Bigl(\\dfrac{\\q\\K^\\top}{\\sqrt{D}}\\Bigr) \\cdot \\boldsymbol{V}\n\\end{equation}\nwhere $\\q \\in \\mathbb{R}^{1\\times D}$ is the query, and $\\K \\in \\mathbb{R}^{S\n\\times D}$ and $\\V \\in \\mathbb{R}^{S \\times D}$ are the key and value caches\nrespectively. Additionally, newer transformer models add Rotary Position\nEmbeddings (RoPE)~\\citep{su2023roformer} to the keys and query, before\ncomputing the attention scores.  Since every query attends to all past keys,\nthe mechanism has a quadratic complexity $\\bigO(S^2)$ in number of input $+$\ngenerated tokens.\n\n\\subsection{Related Work}\n\nNumerous studies have explored the low-rank structures in transformers for\nvarious purposes. Linformer~\\citep{wang2020linformer} demonstrated that the\nattention score matrix is low-rank and proposed alternative low-rank attention\nformulations during training for linear computational complexity.\nLoRA~\\citep{lora} showed that parameter updates to a transformer model during\nfine-tuning reside in a low-dimensional subspace. To the best of our knowledge,\nour work is the first to study the intrinsic low dimensionality of the\nattention keys themselves and demonstrate the generalizability of this\nlow-dimensional structure across different models (for natural language data).\n\nSparse-transformers~\\citep{child2019generating} was one of the first works to\nintroduce a sparse-attention method employing strided sparsity patterns in the\nattention mechanism. Reformer~\\citep{kitaev2020reformer} used locally-sensitive\nhashing to compute attention scores in a sparse manner.\nPerformer~\\citep{choromanski2022rethinking} used positive orthogonal random\nfeatures to approximate the attention mechanism. Unlike these methods, which\nrequire training or fine-tuning, our approach operates entirely post-training\nwithout any fine-tuning.\n\nAnother category of sparse attention methods employ token eviction policies to\npermanently delete tokens from the KV-cache based on some heuristic.\nStreamingLLM~\\citep{xiao2023efficient} uses initial tokens and a rolling\nKV-cache for processing infinite-length sequences. Zhang et\nal.~\\citep{zhang2023hH2O} retain only \"Heavy Hitters\" tokens in the KV-cache\nbased on accumulated attention scores. Scissorhands~\\citep{liu2023scissorhands}\nprioritizes important tokens based on the \"Persistence of Importance\nHypothesis\". Ge et al.~\\citep{ge2023model} propose an adaptive eviction policy\nfor each transformer layer. These methods are effective in reducing the memory\nand compute footprint of the attention but suffer from permanent loss of\ninformation leading to a non-trivial degradation in model quality. Our method\ndoes not involve any permanent loss of information with the trade-off of not\nreducing the memory footprint. Quantization-based approximate\napproaches~\\citep{jacob2017quantization, nagel2021white} are complementary to\nour work and can be applied in tandem.\n\nSparQ Attention~\\citep{ribar2023sparq} is a recent work that inspires our\napproach. They use high-magnitude query dimensions and corresponding key\ndimensions for approximate attention scoring, followed by computing the full\nattention scores for the top-$k$ keys. However, their method requires costly\nnon-contiguous column indexing of the key vectors. Further, they store two\ncopies of the past keys for efficiency, increasing memory use by 50\\%. In\ncontrast, \\method~avoids the extra memory and leverages the natural ordering of\nprincipal components, allowing for a more efficient slicing operation.\n\nA concurrent work, InfiniGen~\\citep{lee2024infinigen}, accelerates attention by\npre-fetching top-$k$ keys from CPU to GPU memory, using SVD-based low-rank\napproximation of the attention scores. While their low-rank approximation is\nsimilar to ~\\method, our work provides deeper analysis of the intrinsic\nlow-rank structure of attention keys and focuses on speeding up attention\ncomputation without CPU offloading. Importantly, their results affirm the\nbenefits of the low-dimensional nature of attention keys applied in other\ncontexts.\n\n\\section{Dimensionality Analysis of Attention Keys}\n\\label{sec:dimanalysis}\nAs noted in Section~\\ref{sec:intro}, \\method, our proposed method for\nsparse self-attention, is based on the observation that key tensors\nconsistently reside in a lower-dimensional space than the full attention head\ndimension suggests. Here, we present empirical evidence supporting this claim\nby performing PCA on the keys generated in several language models and\ndatasets. \n\n\\subsection{Models and Datasets Used}\n\nTo investigate the dimensionality of attention keys, we run 11\ntransformer-based models: Llama-2 7B/13B/70B~\\citep{touvron2023llama}, Llama-3\n8B/70B~\\citep{dubey2024llama3herdmodels},\nTinyLlama-1.1B~\\citep{zhang2024tinyllama},\nPythia-6.9B~\\citep{biderman2023pythia}, Mistral-7B~\\citep{jiang2023mistral},\nMixtral-8x7B/8x22B~\\citep{jiang2024mixtral}, and\nPhi3-Mini-4K~\\citep{microsoft2024phi3} on three popular English language\ndatasets: WikiText-2~\\citep{wikitext-103} (Validation Split),\nC4~\\citep{raffel2023exploring} (Custom Split), and\nBookCorpus~\\citep{zhu2015aligning} (Custom Split).  Custom splits are used for\ndatasets where the validation split is not available.  We run perplexity\nevaluation on these datasets and save the generated attention keys, before and\nafter the application of rotary embeddings~\\citep{su2023roformer}, referred to\nas \\emph{pre-rotary} and \\emph{post-rotary} keys, respectively throughout the\npaper. We then perform PCA on all the keys generated for each layer and head\nindividually. \n\nThe metric we use in our analysis is the rank at which $v$\\% of the variance is\nexplained by the principal components. We calculate this metric for each layer\nand head of the models as follows:\n\\begin{equation}\n    Rank_{l,h}@v = \\min \\left\\{ d \\in \\mathbb{Z}^+ : \\sum_{j=1}^{d} \\lambda_{l,h}^j \\geq v/100 \\right\\}\n\\end{equation}\nwhere, $\\lambda_{l,h}^j$ is the $j^{th}$ normalized eigenvalue of the\ncovariance matrix of the keys for layer, $l$ and head, $h$. We\naverage this metric ranks across all heads of layer, $l$ and refer to\nit as $Rank_{l}@v$.\n\n\\subsection{Findings and Discussion}\n\nFigure \\ref{fig:overview} (left) shows the average $Rank_{l}@90$ across all\nlayers for models with full key dimensionality of 128. We can see that the\naverage rank is significantly lower than the full dimensionality of the keys\nfor all models. Diving deeper, we present a layer-wise analysis for a few\nmodels: Llama2-7B, Llama3-70B, Mixtral-8x7B, and Phi3-Mini-4K in\nFigure~\\ref{fig:rank90}. The results for the other models are similar and can\nbe found in Appendix \\ref{appendix_dimanalysis}. \n\n\\begin{figure}[h]\n  \\centering\n    \\includegraphics[width=0.98\\textwidth]{figs/dimensionality/Main_avg_ranks_at_90.pdf}\n\n    \\caption{Rank at which 90\\% of the variance is explained for pre-rotary and\n    post-rotary keys produced by each layer averaged across all heads\n    ($Rank_{l}@90$) for different models. We observe that all models exhibit\n    significantly low rank (full dimensionality is 128 or 96 represented by the black dashed line) consistently across all\n    datasets.} \\label{fig:rank90}\n\\end{figure}\n\nWe observe that the dimensionality of the keys (both pre-rotary and\npost-rotary) is significantly lower than the full dimensionality of the keys\nacross all calibration datasets. Furthermore, the $Rank_{l}@90$ for a\nparticular layer is consistent across datasets, for all combinations of models\nand datasets. This indicates that the lower-dimensional structure of the keys\nis consistent when calculated using different calibration datasets. Another\ntrend we observe is that the initial layers of most models have a very low\nrank, as compared to the later layers, and this trend is particularly prominent\nfor the pre-rotary keys. Lastly, we also observe that for most models, the\naverage of $Rank_{l}@90$ across all layers is lower for pre-rotary keys as\ncompared to post-rotary keys, indicating that the rotary embeddings increase\nthe dimensionality of the keys.  Further analysis on the variation of the rank\nacross different heads within a layer and across different layers within a\nmodel can be found in Appendix \\ref{appendix_dimanalysis}.\n\nThese results indicate the existence of the following properties: (1) The keys\nproduced by the attention layers of transformer models lie in a significantly\nlower-dimensional space. (2) The lower-dimensional structure of the keys is\nconsistent across different calibration datasets. (3) Rotary embeddings increase\nthe dimensionality of the keys for most models. We now use the first two\nproperties to propose \\method, an efficient sparse-attention method. \n\n\\section{\\method: Low-Dimensional Key Representations}\n\\label{sec:loki}\nWe now describe our proposed algorithm for sparse attention -- \\method.\n\\method leverages low dimensional projections of the keys in the KV-cache to\nefficiently and accurately select the top-$k$ (12.5-25\\%) most relevant tokens\nfor self attention.  Before discussing our approach, let us first look at some\ntheoretical properties of attention in the PCA-transformed space of the key\ntensors. \n\n\\subsection{Properties of Attention in the PCA-transformed Space}\n\\label{subsec:properties}\n\nWe begin by proving two lemmas that provide the rationale for our approach to\ncompute attention in the PCA-transformed space. \n\n\\begin{lemma}\n  Let $D$ be the dimension of an attention head and $\\bP \\in \\mathbb{R}^{D \\times D}$ be the PCA projection matrix\n  of key tensors calibrated offline on a dataset. Assuming we are generating the $S^{\\mathit{th}}$ token in the sequence,\n   let $\\q_S \\in \\mathbb{R}^{1 \\times D}$ be the query vector for the $S^{th}$ token,\n  $\\K_{:S} \\in \\mathbb{R}^{S \\times D}$ be the key vectors, including the past\n  $(S-1)$ keys and the current key. Then, the attention scores computed using\n  the PCA-transformed query and keys are equivalent to the attention\n  scores computed using the original query and keys.\n  \\label{lemma:projection}\n\\end{lemma}\n\\vspace{-0.2in}\n\\begin{proof}\n  Let $\\qhat_S = \\q_S\\bP$ and $\\Khat_{:S} = \\K_{:S}\\bP$ be the PCA\n  transformed query and key vectors. \n  Focusing on the dot product term in the attention computation (Equation \\ref{eq:attention}), we have:\n  \\begin{align*}\n    \\q_S\\K_{:S}^{T} &= \\q_S(\\Khat_{:S}\\bP^{T})^{T} \\text{ [inverting the PCA transform]}\\\\\n    &= \\q_S((\\bP^{T})^{T}\\Khat_{:S}^{T}) = (\\q_S\\bP)\\Khat_{:S}^{T} = \\qhat_{S}\\Khat_{:S}^{T}\n  \\end{align*}\nIt is important to note here that Lemma \\ref{lemma:projection} holds for \nany orthogonal $\\bP$.\n  \\end{proof}\n\n\\begin{lemma}\n  Let $\\Khat_{:S,:d} \\in \\mathbb{R}^{S\\times d}$ ($d < D$) be the reduced dimension key vectors \n  obtained by projecting the key vectors onto the first $d$ principal components of $\\bP$ . \n  Then, the attention scores computed using $\\Khat_{:S,:d}$ are a good approximation of the\n  the actual attention scores.\n  \\label{lemma:reconstruction}\n\\end{lemma}\n\\vspace{-0.2in}\n\\begin{proof}\n  Let $\\textbf{R}_{:d} \\in \\mathbb{R}^{d \\times D}$ be an orthogonal transformation \n  that transforms the keys into the reduced dimension space as $\\bL_{:S,:d} = \\K_{:S}\\textbf{R}_{:d}$. \n  Our objective is to minimize the following expression:\n  \\begin{align}\n    \\min_{\\bR_{:d}}||\\q_S\\K_{:S}^{T} - \\q_{S}(\\bL_{:S,:d}\\bR_{:d}^{T})^{T}||_2^2\n  \\end{align}\n  Using Cauchy-Schwarz inequality, we have:\n  \\begin{align}\n    ||\\q_S\\K_{:S}^{T} - \\q_S(\\bL_{:S,:d}\\bR_{:d}^{T})^{T}||_2^2 &\\leq ||\\q_S||_2^2||\\K_{:S}^{T} - (\\bL_{:S,:d}\\bR_{:d}^{T})^{T}||_2^2\n  \\end{align}\n  We change our objective to minimize the upper bound in the RHS instead of the original objective. \n  We know that PCA minimizes the reconstruction error (2nd term in the RHS) among all \n  the orthogonal transformations. Thus, it follows that the optimal value of $\\bR^{*}_{:d} = \\bP_{:d}$, and \n  $\\bL^{*}_{:S,:d}=\\Khat_{:S,:d}$\n\\end{proof}\n\nSince we minimize an upper bound when proving Lemma \\ref{lemma:reconstruction},\nit is possible that some other transformation might give a better approximation\nto the dot product. Thus, in our experiments, we use PCA transforms computed on\nboth the pre-rotary and post-rotary keys as candidate transformations.\n\nBased on these lemmas and the inherent low-dimensional nature of key tensors in\nattention, we now introduce the workings of the \\method~algorithm. \n\n\\subsection{PCA-based Top-K Algorithm}\n\nLoki implements a PCA-based Top-K Attention approach. Previous works have shown\nthat attention scores for a query are highly concentrated on a small subset of\nkeys~\\citep{xiao2024efficient, sun2024massive}.  This observation has motivated\nseveral methods that compute attention using only the top-$k$ most relevant\nkeys. However, these previous works either compute the exact attention scores\nand then select the top-$k$ keys~\\citep{topkattention} or compute non-exact\nscores but have significantly higher memory\nrequirements~\\citep{ribar2023sparq}.  \\method~alleviates these issues by\ncomputing approximate attention scores (for ranking the keys) in the reduced\nlower-dimensional space, without any significant increase in memory\nrequirements. Algorithm \\ref{alg:topkpca} shows our \\method~method. Line 5 of\nthe algorithm computes the approximate attention scores using $d$ principal\ndimensions of the query and key vectors. Lines 6-7 select the top-$k$ keys\nbased on the approximate attention scores. Line 8 computes the exact attention\nscores using the selected top-$k$ keys, directly in the transformed space\n(Lemma \\ref{lemma:projection}).\n\n\\begin{algorithm}[h]\n  \\caption{\\method: PCA-based Top-K Attention}\n  \\label{alg:topkpca}\n  \\begin{algorithmic}[1]\n  \\Require At the $S^{th}$ step - Input: $\\x_S \\in\n  \\mathbb{R}^{1 \\times D}$, KV-cache: $\\Khat_{:S-1}, \\V_{:S-1} \\in \\mathbb{R}^{(S - 1)\n  \\times D}$, Projection Matrix: $\\bP \\in \\mathbb{R}^{D \\times D}$, Configuration parameters (reduced dimensionality, top-$k$): $d$, $k$ \n  \\Function {\\method-Attention}{$\\x_S, \\Khat_{:S-1}, \\V_{:S-1}, \\bP, d, k$} \n  \\State $\\q_S, \\bk_S, \\bv_S \\gets computeQKV(\\x_S)$ \n  \\State $\\qhat_S \\gets \\q_S\\bP$, $\\bkhat_S \\gets \\bk_S\\bP$ \n  \\State $\\Khat_{:S} \\gets concat(\\Khat_{:S-1}, \\bkhat_S)$, $\\V_{:S} \\gets concat(\\V_{:S-1}, \\bv_S)$ \n  \\State $\\ba_{approx} \\gets \\qhat_{S,:d}(\\Khat_{:S,:d})^{T}$ \n  \\State $indices \\gets topk(\\ba_{approx}, k)$ \n  \\State $\\Khat_{:S}^{\\prime} \\gets \\Khat_{:S}[indices]$, $\\V_{:S}^{\\prime} \\gets \\V_{:S}[indices]$\n  \\State $\\ba_{exact} \\gets softmax(\\frac{\\qhat_S\\Khat_{:S}^{T}}{\\sqrt{D}})$ \n  \\State \\Return $\\ba_{exact}\\V_{:S}^{\\prime}$\n  \\EndFunction\n  \\end{algorithmic}\n\\end{algorithm}\n\n\\vspace{0.08in}\n\\noindent \\textbf{Compute and Memory Analysis:} For vanilla attention, the\ncomplexity of computing $\\q_S\\K_{:S}^{T}$ is $\\bigO(DS)$ and the complexity of\nmultiplying the values with the attention scores is $\\bigO(DS)$. For \\method,\nthe complexity of calculating the approximate attention scores (Line 5) is\n$\\bigO(dS)$. The complexity of selecting the top-$k$ keys (Lines 6-7) is\napproximately $\\bigO(Slog(S) + k)$ (sorting followed by selection). The\ncomplexity of calculating the exact attention scores and multiplying with the\nvalues (Line 8-9) is $\\bigO(2Dk)$. Additionally, the complexity of projections\ninto the PCA space (Line 3) is $\\bigO(2D^2)$. Assuming the complexity of\nselecting the top-$k$ keys is small compared to the other operations, the\noverall complexity of the algorithm is $\\bigO(dS + 2Dk + 2D^2)$. Then, we have:\n\\begin{align} speedup = \\frac{2DS}{dS + 2Dk + 2D^2}  = \\frac{1}{d/2D + k/S +\nD/S} \\approx \\frac{1}{d_f/2 + k_f} \\hspace{1em}(\\text{given }D << S)\n\\end{align} where, $d_{f} = d/D$ and $k_{f} = k/S$. The memory requirement of\nthe KV-cache is the same as the original attention, with a small overhead of\nstoring the PCA transformation matrix.\n\n\\subsection{Implementation in Triton} \\label{sec:kernel}\n\nPerforming \\method~efficiently involves complex indexing operations within the\nKV-cache (lines 5 and 7 of Algorithm~\\ref{alg:topkpca}).  Standard PyTorch\noperations create temporary, dense copies of the KV-cache data in memory,\nleading to slowdowns due to expensive memory access. To alleviate this issue,\nwe develop optimized kernels in Triton~\\citep{triton:openai} for the three\nmatrix multiplication operations in \\method.  Our kernels can directly access\nrelevant subsets of the KV-cache (both feature and sequence dimensions) and\nperform computations within GPU registers. This eliminates the need for\ncreating dense copies, significantly improving performance. Our approach builds\non SparQ~\\citep{ribar2023sparq}, which introduced similar kernels for top-$k$\nattention calculations. However, we identified and addressed inefficiencies in\nthe SparQ kernels, which resulted in speedups of nearly $2-3\\times$ in certain\nscenarios.  (see Appendix~\\ref{append:comp_pcatopk}).\n\n\\section{Experimental Setup}\n\\label{sec:setup}\nWe evaluate \\method on the basis of \nperplexity using the WikiText-2~\\citep{wikitext-103} dataset (test split), and on the basis of downstream task performance for  \nshort contexts using the LM-harness\nbenchmark~\\citep{eval-harness} and long contexts\nusing LongBench~\\citep{bai2023longbench}. For the short-context evaluation, we\nchoose the same tasks and associated metrics as the HuggingFace OpenLLM\nleaderboard~\\citep{open-llm-leaderboard}. For the LongBench tasks, we evaluate\non all the English language tasks. \n\nWe compare our method against three methods -- full attention without any\napproximations, the exact TopK approach which computes the exact attention\nscores and then uses the top-$k$ tokens to compute the final output, and\nH$_2$O~\\citep{zhang2023hH2O}, a popular token-eviction method. For these\ncomparisons, we show the results with a budget size of $k_f$ = 0.25 and 0.125.\nFor our method, we additionally use $d_f$ = 0.25 and 0.125. This configuration\nof our represents a 2.6x theoretical speedup. Table \\ref{ref:budget} provides\nan overview of the methods compared and the associated budget terms. H$_2$O's\nbudget was split equally between the heavy hitter and recent tokens, as per the\nauthor's recommendations. For H$_2$O, we were unable to run the GSM8K task as\nthe the author's ML benchmarking code was too memory intensive to run for that\ntask. For the aforementioned experiments, we generate PCA transforms using the\nWikiText-103 dataset. For the LongBench tasks, we compare our method with the\nfull attention baseline as we were unable to run H$_2$O due to memory\nconstraints. \n\nFor the generalizability study, we compare the results of our method with PCA\ntransforms from different calibration datasets:\nWikiText-103~\\citep{wikitext-103}, C4~\\citep{raffel2023exploring}, and\nBookCorpus~\\citep{zhu2015aligning}. Additionally, we also benchmark our triton\nbased implementation of \\method~by running an attention microbenchmark on a\nLlama2-13B-like setup (same hidden size and number of heads) for various prompt\nand generation lengths, and demonstrate speedups over vanilla attention.\n\n\\begin{table}[t]\n  \\small\n  \\centering\n  \\caption{Explanation of key-budget and dimensionality (Dim.) for different approaches, along with the expected speedup and memory savings.}\n  \\begin{adjustbox}{width=\\linewidth}\n  \\begin{tabular}{lccp{7cm}cc}\n      \\toprule\n      Method & Budget & Dim. & Description & Speedup & Memory Savings \\\\\n      \\midrule\n      Exact Top-K & $k_f$ & Full & $k_f$ fraction of keys selected using exact attention scores & No & No \\\\\n      H$_2$O & $k_f$ & Full & $k_f$ fraction of keys \\& values selected using H$_2$O policy& $\\frac{1}{k_f}$ & $\\frac{1}{k_f}$ \\\\\n      \\method~& $k_f$ & $d_f$ & $k_f$ fraction of keys \\&values selected using attention scores computed \n      with $d_f$ fraction of full dimensionality & $\\frac{1}{(d_f/2) + k_f}$ & No \\\\\n      \\bottomrule\n  \\end{tabular}\n  \\end{adjustbox}\n  \\label{ref:budget}\n\\end{table}\n\nAll experiments are run on NVIDIA A100 GPUs with 40 and 80 GB of memory on the\nPerlmutter~\\citep{perlmutter} supercomputer. For larger models, we use\nAxoNN~\\citep{singh:ipdps2022,singh:arxiv2024} to shard the model across\nmultiple GPUs.\n\n\\section{Results}\n\\label{sec:results}\nWe now present the comparisons of \\method with full attention and other sparse\nattention methods, including a comparison of the computation times.\n\n\\subsection{Comparison with Full Attention}\n\nLet us begin our discussion with Figure \\ref{fig:all_model_eval}, showing the\nperplexity (left) and short-context downstream task evaluation (right) results\nfor \\method~on different models.\nWe focus on the Llama2-7B model, comparing pre-rotary (light green/purple)\nand post-rotary (dark green/purple) PCA transforms for different $k_f$ and\n$d_f$ values. For Llama2-7B, we see that the performance of both candidate\ntransforms is similar. This trend is consistent across all the models except\nfor Llama3-8B/70B and Mistral-7B, where the post-rotary PCA transform performs\nsignificantly worse than the pre-rotary one. For Llama3-8B, perplexity jumps\nfrom about 5 for the full attention to over 10, a significant decline not seen\nwith the pre-rotary transform. Mistral-7B shows a similar pattern. This is a\nsurprising observation since attention scores are calculated from post-rotary\nkeys in the original attention mechanism. A possible explanation is that \npost-rotary PCA captures token distributions tied to specific positions in \nthe calibration dataset, while pre-rotary PCA may generalize better by using \nless positional information. Nevertheless, at least one of the PCA\ntransformations performs well for every model. For subsequent results, we only\nshow the better-performing transformation for each model. \n\n\\begin{figure}[h]\n  \\centering\n    \\includegraphics[width=\\textwidth]{figs/results/perplexity_tasks_pca_topk.pdf}\n    \\caption{Evaluation of \\method~on perplexity (left plot) and short-context tasks (right plot) for different models. \n    Task accuracy is an average across all short-context tasks mentioned in \\ref{sec:setup}.}\n    \\label{fig:all_model_eval}\n\\end{figure}\n\nFigure \\ref{fig:long_context_eval} shows the performance of \\method~on the\nLongBench tasks for the Llama2-7B-Chat model. We see that for all tasks, either\none of the two candidate transforms performs similarly to full attention. For\nSummarization, Few Shot Learning, Synthetic, and Code Completion task\ncategories, the best performing \\method~configuration is at par or better than\nthe full attention model. For the Single-Doc QA and Multi-Doc QA task\ncategories, ~\\method~performs slightly worse than the full attention model,\nwith the biggest drop in performance observed for HotpotQA of around 3\\%.\nComparing different $(k_f, d_f)$ settings, we see that using $k_f$ = 0.25 and\n$d_f$ = 0.25 (green), is better than using $k_f$ = 0.125 and $d_f$ = 0.5\n(purple) for all models and tasks (short-context and long-context). These two\nsettings balance speed and performance well, with the first being superior for\naccuracy.\n\n\\begin{figure}[h]\n  \\centering\n    \\includegraphics[width=\\linewidth]{figs/rebuttal/longbench/English_ALL.pdf}\n    \\caption{Evaluation of \\method~on LongBench tasks for the Llama2-7B-Chat model.}\n    \\label{fig:long_context_eval}\n\\end{figure}\n\n\\subsection{Comparison with Other Sparse Attention Methods}\n\\label{subsec:comparison_other_sparse}\n\nNext, we compare the performance of \\method~with other methods, using $k_f$\n= 0.25 for all methods and $d_f$ = 0.25 for ours. Table \\ref{tab:perplexity}\nshows the perplexity results for Llama2-7B/13B, Llama3-8B, and Mistral-7B.\n\\method's perplexity drop is within 0.1 of full attention across all models, a\nthreshold considered acceptable for attention mechanism\napproximations~\\citep{Yao2023ZeroQuantV2EP}. In contrast, H$_2$O's perplexity\ndrop nears 0.2 for all models.  Figure \\ref{fig:tasks_baseline_eval} confirms\nthis trend on short-context evaluation. \\method~performs similar to full\nattention for all models, except Llama3-8B, where the performance is notably\nworse, but still better than H$_2$O.  Importantly, on the challenging MMLU\ntask, \\method~degrades less than H$_2$O.\n\n\\begin{table}[h]\n  \\centering\n  \\caption{Perplexity evaluation of \\method and other approaches for different models (lower is better).}\n  \\begin{adjustbox}{width=\\textwidth, center}\n  \\begin{tabular}{@{}lccccccc@{}}\n      \\toprule\n      Method & $k_f$ & $d_f$ & Speedup & Llama2-7B & Llama2-13B & Llama3-8B & Mistral-7B \\\\\n      \\toprule\n      Full Attention & - & - & No & 5.1101 & 4.5680 & 5.5696 & 4.9140 \\\\\n      Exact-TopK & 0.25 & - & No & 5.1809 & 4.5926 & 5.5716 & 4.9171 \\\\\n      \\midrule\n      H$_2$O & 0.25 & - & Yes & 5.2810 & 4.7009 & 5.7056 & 5.0805 \\\\\n      \\method~& 0.25 & 0.25 & Yes & \\textbf{5.2017} & \\textbf{4.6102} & \\textbf{5.6648} & \\textbf{4.9233} \\\\\n      \\bottomrule\n  \\end{tabular}\n  \\end{adjustbox}\n  \\label{tab:perplexity}\n\\end{table}\n\n\\begin{figure}[h]\n  \\centering\n    \\includegraphics[width=\\textwidth]{figs/results/baselines_pca_topk.pdf}\n    \\caption{Downstream task performance for \\method~and other approaches for different models (higher is better).\n     GSM8K is excluded, as we were unable to run H$_2$O for this task.}\n    \\label{fig:tasks_baseline_eval}\n\\end{figure}\n\nIt is important to note here that \\method~is designed to be compatible with other\nsparse attention methods. For instance, token-eviction methods like H$_2$O delete tokens to\nsave KV-cache memory, whereas \\method~reduces memory bandwidth by selecting the\ntop-$k$ tokens without deletion, making them orthogonal. A combined approach\ncould involve using H$_2$O to delete tokens, then applying \\method~to select\ntop-$k$ tokens from the remaining cache. Similarly, \\method~is theoretically\northogonal to quantization methods.\n\nComparing \\method~with Exact-TopK, we find similar performance for\nLlama2-7B/13B and Mistral-7B. Exact-TopK represents the upper performance bound\nfor \\method~if it could perfectly select the top-$k$ tokens. To understand why\n\\method~works well, we examined the top-$k$ agreement between ~\\method's\nreduced dimensional attention scores and exact attention scores. Figure\n\\ref{fig:topkagreement} shows the Jaccard similarity between the top-$k$ tokens\nselected by both methods across all layers and heads for Llama2-7B. For the\nsettings: ($k_f = 0.25$, $d_f = 0.25$) and ($k_f = 0.125$, $d_f = 0.5$),\nevaluated in Figure \\ref{fig:all_model_eval}, the Jaccard similarity is around\n0.9, validating that the \\method~is able to select the top-$k$ tokens with high\naccuracy.\n\n\\begin{figure}[h]\n  \\centering\n  \\includegraphics[height=1.35in]{figs/topkagreement/jaccard_per_r.pdf}\n  \\hspace{0.1in}\n  \\includegraphics[height=1.35in]{figs/results/best_dataset_generalise.pdf}\n  \\includegraphics[height=1.35in]{figs/results/breakdown.pdf}\n  \\caption{Top-$k$ agreement between \\method~and Exact-TopK methods for Llama2-7B (left plot). \n  Performance of \\method~using transformations derived from different calibration datasets (middle plots).\n  Benchmarking vanilla attention and \\method~for Llama2-13B using huggingface transformers with \n  cache append times (right plot, prompt length = 3072, generation length = 512).\n  }\n  \\label{fig:topkagreement}\n\\end{figure}\n\n\\subsection{Generalizability}\n\nWe now turn our attention to the generalizability of the PCA transformations\nused in our method. Figure \\ref{fig:topkagreement} (middle) shows the\nperformance of \\method~using PCA transformations derived from different\ncalibration datasets ($k_f = 0.25, d_f = 0.25$).  We see that the performance\nof \\method~is consistent across different calibration datasets, indicating that\nthe PCA transformations used in our method are generalizable. This is an\nimportant observation as it shows that the PCA keys can be generated using a\nvariety of calibration datasets and still achieve good performance. \n\n\\subsection{Computational Efficiency}\n\nWe now turn our attention to the computational efficiency of \\method. Analyzing\nLlama2-13B with Hugging Face Transformers exposed an interesting bottleneck\n(Figure~\\ref{fig:topkagreement}, rightmost). Regardless of the attention type\n(vanilla or \\method), more than 80\\% of the time is consumed within the Hugging\nFace framework for appending key-value pairs of the latest token to the\nKV-cache. This shared bottleneck minimizes the overall performance improvement\nof our optimizations. We hypothesize that using a more advanced inference\nsystem like vLLM~\\cite{woosuk2023vllm} could significantly reduce this append\ntime, but leave that exploration for future work. To isolate the impact of our\noptimizations, the plots in Figure~\\ref{fig:compute} focus solely on the\nattention computation time, excluding the KV-cache append time.\n\n\\begin{figure}[h]\n  \\includegraphics[height=1.75in]{figs/results/all_gen_breakdown_without_cache_update_2048_3072_vanilla.pdf}\n  \\includegraphics[height=1.75in]{figs/rebuttal/longbench/longbench_configuration.pdf}\n  \\caption{Time per layer for vanilla attention (V) and \\method (\\mbox{L-A:\n${k_f=0.25,d_f=0.25}$}; \\mbox{L-B: ${k_f=0.125,d_f=0.25}$}) for Llama2-13B\nusing huggingface transformers (left two plots). LongBench average accuracy for\ndifferent \\method configurations, alongside micro-benchmark attention times (right plot, all\nlayers, prompt length = 3500 \\& generation length = 512). We choose the prompt\nlength to match LongBench’s configuration for this model and generation length\nto match the maximum in any LongBench task. For both figures, we use a batch\nsize of 16 and report the average time over 10 trials (std.~dev.~in measured times\nwas less than 0.05 percent of the mean).}\n  \\label{fig:compute}\n\\end{figure}\n\nIn the left plot of Figure~\\ref{fig:compute}, we see that \\method~speeds up the\ntotal attention compute time (excluding KV-cache appends) compared to vanilla\nattention across various prompt and generation lengths. For a prompt length of\n3072 and generation length of 512, \\method~achieves nearly a 45\\% speedup,\ndespite the fact that it incurs an extra matrix multiplication operation.  The\nbreakdowns also show that the top-$k$ operation is nearly as expensive as the\nsmaller matrix multiplications, which is a significant bottleneck. Replacing\nPyTorch’s top-$k$ with a custom kernel could improve this. For the shorter\nprompt length of 2048 we observe a speedup of around 40\\% (generation length =\n512), slightly lower than the speedup at 3072. This trend is expected as larger\nprompts result in a bigger KV-cache, amplifying the impact of our\noptimizations.\n\nFigure~\\ref{fig:compute} (Right) shows the accuracy vs. attention time\ntrade-off across various $k_f, d_f$ settings of \\method, with accuracy measured\non LongBench and attention times from our microbenchmark. The previously\nevaluated settings, $k_f = 0.25, d_f = 0.25$ and $k_f = 0.125, d_f = 0.5$,\nprovide a good balance between performance and accuracy, with $k_f = 0.25, d_f\n= 0.25$ favoring accuracy slightly and $k_f = 0.125, d_f = 0.5$ favoring\nperformance.\n\n\\section{Conclusion}\n\\label{sec:conclusion}\nIn conclusion, we introduced \\method, an algorithm for efficient sparse\nattention that does not compromise the model quality while reducing the\ncomputational complexity of self attention. We made a crucial observation that\nkey vectors in attention lie in a low-dimensional space, across different\nmodels and datasets.\nLeveraging this insight, \\method uses attention scores computed in a\nlower-dimensional space to rank and select the top-$k$ most relevant tokens\nfrom the KV-cache. It then uses the full dimensionality only for the selected\ntokens to compute the final attention. Our theoretical analysis shows that\n\\method can provide significant speedups in the attention step. To implement\nthis efficiently, we develop optimized kernels for the various sparse matrix\nmultiplications in our approach. Our empirical evaluation shows that \\method\nperforms better than popular approximation methods on a variety of models and\ntasks, with respect to preserving model quality. Finally, we show that \\method\ncan provide speedups of up to 45\\% over the vanilla attention empirically,\nmaking it a promising approach to address the computational challenges in\ntransformer inference.\n\n\\vspace{0.08in}\n\\noindent \\textbf{Limitations and Future Work:} \n\\method~does not focus on reducing memory usage of the KV-cache currently. As\nmentioned previously in \\ref{subsec:comparison_other_sparse}, it can potentially\nbe combined with other sparse attention method for improved\nmemory-performance-accuracy trade-offs. Another direction involves storing the\nKV-cache in CPU memory and transferring only the top-$k$ keys and values to the\nGPU~\\citep{lee2024infinigen}.\n\nWhile \\method~outperforms vanilla attention in our benchmarks, practical\ndeployment would require integration with efficient attention kernels like\nFlashAttention~\\citep{dao2022flashattention}. As seen in our compute\nbenchmarking, the top-$k$ selection operation could introduce a bottleneck\ntowards achieving this.  Investigating this bottleneck and integrating\n\\method~with optimized attention kernels is left for future work.\n\nOur finding of the keys’ low intrinsic dimensionality suggests promising\nresearch directions. The variation of this dimensionality across heads and\nlayers could further be explored. We briefly experimented with a variable $d_f$\npolicy per layer (see Appendix \\ref{app:variable_d}), but did not observe\nsignificant significant improvements. A more sophisticated policy could be\nexplored in future work.\n\n\\begin{ack}\nThis research used resources of the National Energy Research Scientific\nComputing Center (NERSC), a Department of Energy Office of Science User\nFacility using NERSC award DDR-ERCAP0029894. Soheil Feizi was supported in\npart by the following grants: NSF CAREER AWARD 1942230, ONR YIP award\nN00014-22-1-2271, ARO’s Early Career Program Award 310902-00001, Army Grant\nNo.~W911NF2120076, NSF award CCF2212458, NSF award 2229885, an Amazon Research\nAward and an award from Capital One.\n\n\\end{ack}\n\n\\clearpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference}\n\n\\begin{document}\n\n\\twocolumn[\n\\icmltitle{\\method{}: Query-Aware Sparsity for Efficient Long-Context LLM Inference}\n\n\\icmlsetsymbol{equal}{*}\n\n\\begin{icmlauthorlist}\n\\icmlauthor{Jiaming Tang}{equal,sjtu,mit}\n\\icmlauthor{Yilong Zhao}{equal,sjtu,UW}\n\\icmlauthor{Kan Zhu}{UW}\n\\icmlauthor{Guangxuan Xiao}{mit}\n\\icmlauthor{Baris Kasikci}{UW}\n\\icmlauthor{Song Han}{mit,nv}\n\\end{icmlauthorlist}\n\n\\icmlaffiliation{sjtu}{Shanghai Jiao Tong University}\n\\icmlaffiliation{mit}{MIT}\n\\icmlaffiliation{UW}{University of Washington}\n\\icmlaffiliation{nv}{NVIDIA}\n\n\\icmlcorrespondingauthor{Song Han}{songhan@mit.edu}\n\\icmlcorrespondingauthor{Baris Kasikci}{baris@cs.washington.edu}\n\n\\icmlkeywords{Machine Learning, ICML}\n\n\\vskip 0.3in\n]\n\n\\printAffiliationsAndNotice{\\icmlEqualContribution} %\n\n\\begin{abstract}\nAs the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. \nPrevious works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose \\method, a query-aware \\kvc{} selection algorithm. \\method keeps track of the minimal and maximal Key values in \\kvc pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, \\method significantly speeds up self-attention without sacrificing accuracy. We show that \\method can achieve up to \\selfspeedup self-attention speedup, which reduces inference latency by \\eespeedup while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at \\url{https://github.com/mit-han-lab/Quest}.\n\\end{abstract}\n\\section{Introduction}\nThe rapid evolution of Large Language Models (LLMs) has shaped our daily lives. With the increasing demand for multi-round conversations and long document queries, the maximum context length of LLMs has dramatically grown from 2K to 1M~\\cite{liu2024world,peng2023yarn,tworkowski2023focused}. The 128k context length GPT-4 model has already been deployed in large-scale serving, which is equivalent to 300 pages of text~\\cite{openaiannouncement}.  \n\nHowever, processing long-context requests is challenging. Due to the auto-regressive nature of LLMs, generating one token would require reading the entire \\kvc{}. For Llama 7B model~\\cite{touvron2023llama} with $32$k context length, the \\kvc{} can occupy $16$GB of space, which requires at least $11$ ms to read, which contributes to more than $50$\\% of the inference latency\\footnote{Tested with FP16 FlashInfer implementation on an RTX4090}, limiting the overall throughput.\n\nDespite the increasingly large size of the \\kvc{}, previous works have shown that a small portion of the tokens can dominate the accuracy of token generation~\\cite{zhang2023h2o, ge2024model}. Therefore, we can dramatically reduce the inference latency by only loading the critical tokens, while still maintaining accuracy. Thus, it is essential to identify critical portions of the \\kvc{}.\n\n\\begin{figure}\n    \\centering\n     \\includegraphics[width=\\linewidth]{figure_text/fig_framework_pdf.pdf}\n    \\caption{Comparison between Dense Attention(a), Query-Agnostic Sparsity (b) and \\method's Query-aware Sparsity (c). \\method significantly speeds up self-attention while maintaining high accuracy by dynamically determining the critical tokens based on the current query. $T$ represents the total sequence length and $L$ represents the number of critical tokens for attention.}\n    \\label{fig:framework}\n\\end{figure}\n\nIn this work, we further observe that the criticality of the tokens can change with different query tokens. As shown in \\fig\\ref{fig:query}, the critical tokens vary a lot with different queries. Therefore, we need a dynamic and efficient approach to determine which portion of the \\kvc{} needs to be attended to. To this end, we propose \\method, a query-aware criticality estimation algorithm for long-context LLM inference that efficiently and effectively identifies critical \\kvc{} tokens and performs self-attention selectively on chosen tokens, as shown in \\fig\\ref{fig:framework}.\n\nTo reduce the overhead of \\kvc{} criticality estimation, \\method{} manages \\kvc{} at page granularity~\\cite{kwon2023efficient}. For each page, \\method{} utilizes maximum and minimum values of each feature dimension of the Key vector as the metadata to represent token information. During inference, \\method{} considers both the Query vector and the metadata to estimate each page's criticality. Given all criticality scores of the pages, \\method{} chooses Top-K pages to perform approximate self-attention, where $K$ is a preset constant (e.g.  128, 256). By reducing the memory movement from the entire \\kvc{} to metadata and constant $K$ pages, \\method{} significantly accelerates inference.\n\nWe evaluate both the accuracy and efficiency of \\method{}. Since \\method{} dynamically decides the criticality of the tokens, \\method{} achieves better accuracy for a given degree of \\kvc{} sparsity than baselines on PG19 dataset~\\cite{raecompressive2019}, passkey retrieval task~\\cite{peng2023yarn}, and LongBench~\\cite{bai2023longbench} with $256$ to $4$K token budgets.  For $32$K context, \\method{} achieves \\selfspeedup self-attention latency reduction compared to FlashInfer~\\cite{flashinfer}. Our end-to-end framework demonstrates that \\method{} can have \\eespeedup inference speedup compared to FlashInfer~\\cite{flashinfer} with 4-bit weight quantization. In summary, we make the following contribution:\n\n\\vspace{-10pt}\n\\begin{itemize}\n    \\setlength{\\itemsep}{-3pt}\n    \\item An analysis of the self-attention mechanism that pinpoints the importance of query-aware sparsity.\n    \\item \\method, an efficient and accurate \\kvc{} acceleration algorithm, which exploits query-aware sparsity by dedicated operator designs and implementations.\n    \\item A comprehensive evaluation of \\method, demonstrating up to \\selfspeedup self-attention latency reduction and \\eespeedup end-to-end latency improvement.\n\\end{itemize}\n\\section{Related Work}\n\n\\subsection{Long-context Model}\n\nAs the demand for long-context models increases, many works have focused on extending the context window of LLMs. Currently, many models utilize Rotary Position Embeddings (RoPE)~\\cite{su2023roformer}, and by different scaling methods of RoPE with fine-tuning, the window size of the original 4k Llama-2 has been expanded to 32k for LongChat~\\cite{longchat2023} and 128k for Yarn-Llama-2~\\cite{peng2023yarn}. Through length extrapolation, the context windows of models reached beyond 1M~\\cite{liu2024scaling}.  Beyond open-source models, GPT-4 Turbo supports lengths of up to 128k, while Claude-2 supports up to 200k~\\cite{gpt4,claude}. With models increasingly capable of handling long input, this poses challenges for inference efficiency. \\method aims to boost long-context inference by exploiting query-aware KV cache sparsity.\n\n\\subsection{KV Cache Eviction Algorithm}\n\nFor long-context LLM inference and serving scenarios, the huge size of the KV cache results in significant time and space overheads. Many previous efforts have been dedicated to compressing the size of the KV cache to accelerate attention and reduce memory usage. H2O~\\cite{zhang2023h2o} retains a limited budget of the important KV cache based on the sum of historical attention scores. FastGen~\\cite{ge2024model} further refines the types of tokens, applying a more sophisticated strategy for selecting the KV cache to keep. TOVA~\\cite{oren2024transformers} simplifies the policy by deciding which tokens to permanently discard based solely on the current query. StreamingLLM~\\cite{xiao2023streamingllm} handles infinitely long texts with attention sinks and a finite KV cache. These methods decide which parts of the KV cache to discard based on historical information or current states, but discarded tokens might be important for future tokens, which may cause the loss of important information. To mitigate this issue, SparQ~\\cite{ribar2023sparq} computes approQximate attention scores by channel pruning and selects important tokens through them. However, this approach has not been widely validated for tasks with long dependencies, and the channel-level sparsity might pose challenges to practical acceleration.\nTherefore, we propose \\method, which retains all of the KV cache and selects part of the KV cache based on the current query to accelerate long-context self-attention without accuracy degradation. \n\\section{Methodlogy}\n\\begin{figure}[t]\n    \\centering\n     \\includegraphics[width=0.45\\textwidth]{figure_text/fig_query_pdf.pdf}\n    \\caption{The attention map of prompt ``A is B. C is D. A is''. Each row represents the attention scores of previous tokens queried by the tokens on the left. When queried with ``D'', token ``B'' has a low attention score, showing ``B'' is not critical for generation. However, the ``is'' strongly attends to ``B''. Therefore, the criticality of tokens strongly correlates with the current query token.}\n    \\label{fig:query}\n\\vspace{-0.1in}\n\\end{figure}\n\nIn this section, we first motivate \\method by analyzing the breakdown of inference cost and self-attention properties. We then present the design of \\method and discuss its benefits.\n\n\\subsection{Long-context Inference Is Costly}\n\nLLM inference contains two stages, namely, the prefill stage and the decode stage. In the prefill stage, all the input tokens are transformed into embeddings and generate the Key ($K$), Query($Q$), and Value($V$) vectors. Both the Key and the Value vectors are saved in the KV cache for future use. The rest of the prefill stage includes self-attention and feed-forward network (FFN) layers, which produce the first response token. \n\n\\begin{figure}\n    \\centering\n     \\includegraphics[width=0.9\\columnwidth]{figure_text/intuition.pdf}\n    \\caption{The query aware sparsity for each layer in LongChat-7B model. We measure the sparsity by eliminating KV cache tokens while making sure the perplexity on PG19 increases less than 0.01. For the first two layers, the sparsity is below 10\\%, while for the rest of the layers, the sparsity is larger than 90\\%, showing great potential for optimization. \\method closely aligns with the oracle. }\n    \\label{fig:qa-sparsity}\n\\end{figure}\n\nIn the decode stage, the model will take the last generated token to calculate its $K,Q,V$. The model uses $Q$ to multiply with every $K$ of previous tokens to generate the \\textit{attention weights}. The attention weights will then get normalized using softmax, where each value $a_i$ represents the attention score between $i$th token and the current token. The self-attention layer will output $\\sum a_i \\cdot V_i$ and send to the FFN.\n\nFor one request, the prefill stage only happens once, while a decoding process is needed for every token in the response. Therefore, the decode stage dominates the inference time. For example, for $16$k token prompts and $512$ token responses, over $86$\\% of the time is spent on decode stages. Therefore, the decode stage performance is crucial for overall latency.\n\nMoreover, a long-context scenario significantly slows down the decode stage. In every decode stage, the $K$ and $V$ of existing tokens must be loaded to perform self-attention, which can easily reach $16$GB for the $32$k context of Llama-7b\\footnote{$\\text{KV cache size} = 2 \\text{\\xspace(both K and V)} * \\text{Num of Layer} * \\text{Sequence length} * \\text{Num of Heads} * \\text{Head Dimensions} * \\text{Size of FP16} = 2 * 32 * 32 * 32 * 128 * 2 =  16 \\text{GB}$}. This memory load operation can take $53$\\% of the time in a decode stage. Therefore, optimizing self-attention becomes a must for efficient long-context inference.\n\\begin{figure}[t]\n    \\centering\n     \\includegraphics[width=0.956\\linewidth]{figure_text/fig_recall_rate_pdf.pdf}\n    \\caption{Recall rate of tokens with Top-10 attention scores. Results are profiled with LongChat-7b-v1.5-32k model in passkey retrieval test of $10$K context length. Recall rate is the ratio of tokens selected by different attention methods to tokens selected by the full attention in each round of decoding. The average rate is shown in the figure, with various token budgets assigned.}\n    \\label{fig:recall}\n    \\vspace{-0.08in}\n\\end{figure}\n\\begin{figure*}[h]\n    \\centering\n     \\includegraphics[width=\\linewidth]{figure_text/fig_select_pdf.pdf}\n    \\caption{\\method performs self-attention in two stages. In stage 1, \\method estimates the criticality of pages by performing element-wise product between the current Query vector and both Min Key and Max Key vectors in each KV cache page. \\method gets the sum of the per-channel maximal value for each page as the page criticality estimation. In stage 2, only Top-K KV cache pages are loaded to perform sparse self-attention with the current Query. }\n    \\label{fig:select}\n\\end{figure*}\n\\subsection{Self-Attention Operation Features High Sparsity}\n\nLuckily, previous research has highlighted the inherent sparsity in self-attention~\\cite{zhang2023h2o, ge2024model}. Due to this property of self-attention, a small portion of tokens in the KV cache, called critical tokens, can accumulate sufficient attention scores, capturing the most important inter-token relationships.\nFor example, as shown in \\fig\\ref{fig:qa-sparsity}, apart from the first two layers, less than 10\\% of the tokens are needed to achieve similar accuracy, which makes the attention on the rest of the tokens unnecessary. \nTherefore, if we can estimate the criticality of the tokens, we can only compute self-attention on critical \\kvc{} tokens to greatly reduce the memory movement and thus improve efficiency.\n\n\\subsection{Critical Tokens Depend on the Query}\nHowever, the criticality of the tokens is dynamic and highly dependent on the query vector $Q$. Assuming the prompt is \"A is B. C is D. A is\", we demonstrate the attention map of a certain head in the 16th layer of Llama-2-7b in \\fig~\\ref{fig:query}. Since the output answer here should be \"B\", the token \"B\" is critical to the current query \"is\". Thus, it has a high attention score. However, before the final token \"is\", \"B\" is not critical for any previous query and has very low attention scores. In other words, the criticality of tokens is tightly related to the query token.\n\nWe quantify this effect by profiling the average recall rate of tokens with Top-10 attention scores along the text generations. The original attention with full \\kvc{} can maintain $100$\\% recall rate. However, \\kvc{} eviction algorithm like H2O~\\cite{zhang2023h2o} which prunes tokens based on history information, suffers from low recall rates since critical tokens are pruned in previous iterations. As shown in \\fig\\ref{fig:recall}, \\method{} maintains recall rate close to full attention, as it estimated critical tokens based on current query. Therefore, pre-determining the criticality is challenging, which motivates query-aware sparsity by considering $Q$ vectors for criticality estimation.\n\n\\subsection{Dynamically Estimating Token Criticality}\n\\label{sec:methodDesp}\nTo efficiently and accurately estimate the criticality of \\kvc{} tokens, we propose \\method, an efficient and accurate algorithm that exploits query-aware context sparsity, which approximately selects the most potentially critical \\kvc{} pages for the current query. We show the workflow of \\method in \\fig\\ref{fig:select}. To manage the overhead, \\method adopts PageAttention~\\cite{kwon2023efficient} and selects the \\kvc{} pages at the granularity of pages. \n\nTo estimate the criticality of the pages, \\method performs an approximate calculation of attention weights before the original attention operation, as shown in Algorithm \\ref{algo:max}.\n\nOur insight is that in order not to miss critical tokens, we should select pages containing the token with the highest attention weights. However, for an efficient selection of pages, we should calculate an approximate attention score following this insight. We found that the upper bound attention weights within a page can be used to approximate the highest attention in the page. The upper bound of the attention weights can be calculated by the channel-wise minimal values ($m_i$) and maximal values ($M_i$) of Key vectors. Given a $Q$ vector, \\method calculates the maximum possible value of the channel $i$ by taking $U_i = \\max(Q_i m_i, Q_i M_i)$. Note that $U_i$ is always greater than any product of $Q_i$ with the Key value $K_i$ for all tokens in this page regardless of the sign of $Q_i$. Therefore, when we add up $U_i$, we get the upper bound of attention weights across all Key vectors on this page.\n\nAfter deriving the upper bound attention weights, we choose the top $K$ pages as critical, where $K$ is an arbitrarily defined hyper-parameter. To demonstrate the feasibility of \\method, we perform actual self-attention and gather Top-K per-page attention scores. As shown in \\fig\\ref{fig:qa-sparsity}, our query-aware sparsity mostly aligns with the oracle sparsity. \\method performs normal self-attention only on selected pages, which greatly reduces memory movement. We define the number of tokens in selected pages as the ``Token Budget''.\n\nDue to the low sparsity ratio for the first two layers (as shown in \\fig\\ref{fig:qa-sparsity}), we only apply \\method and all baselines on later layers to better preserve model accuracy. Note that whether to skip the first two layers or not is orthogonal to the \\kvc{} selection algorithm. \n\\begin{algorithm}[h]\n   \\caption{Token Criticality Estimation}\n\\begin{algorithmic}\n    \\label{algo:max}\n   \n   \\STATE {\\bfseries When inserting new token to KV cache:}\n   \\STATE {\\bfseries Input:} Key vector $K$, Dimension of hidden states $dim$, Current maximal vector $M_i$, Current minimal vector $m_i$\n   \\STATE\n   \\FOR{$i=1$ {\\bfseries to} $dim$}\n        \\STATE $M_i$ = $\\max(M_i, k_{i})$     \n        \\STATE $m_i$ = $\\min(m_i, k_{i})$     \n    \\ENDFOR\n    \\STATE\n   \\STATE {\\bfseries When perform self-attention:}\n   \\STATE {\\bfseries Input:} Query vector $Q$, Dimension of hidden states $dim$, Current maximal vector $M_i$, Current minimal vector $m_i$\n   \\STATE\n   \\STATE Initialize $score = 0$.\n   \\FOR{$i=1$ {\\bfseries to} $dim$}\n   \\STATE $score$ += $MAX( q_i * max, q_i * min)$\n   \\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{\\method Reduces the Memory Movement of Self-Attention}\n\nInstead of loading the whole \\kvc{}, \\method{} only needs to load a fraction of the data, which leverages \\qaware{} sparsity. Assume that every $K$ or $V$ vector is $M$ bytes, the \\kvc{} contains $L$ tokens, and each page contains $S$ KV pairs (Page size). During criticality estimation, \\method will load maximal and minimal vectors of each page, which is approximately $2M*L/S$ bytes. Additionally, \\method performs normal self-attention for top $K$ pages, which is $2M*K*S$ bytes. The whole \\kvc{} is $2M*L$ bytes, which indicates \\method loads $1/S + K*S/L$ of the total KV cache\\footnote{The top-K operator incurs negligible memory loading and execution time (5-10 us). Therefore, we do not include it in efficiency analysis. }, which is equivalent to \n$$\n\\frac{1}{\\text{Page Size}} + \\frac{K}{\\text{Page Num}}\n$$\n\nAssuming that we use $16$ KV pairs per page, context length is 64K, and we choose the top 4K pages, \\method will reduce the memory load by $8\\times$. Note that this memory load reduction is universal across all models and is compatible with existing quantization mechanisms~\\cite{zhao2023atom}.\n\n\\section{Experiments}\n\n\\begin{table}\n\\small\n    \\setlength{\\tabcolsep}{3pt}\n    \n    \\centering\n    \\begin{tabular}{llccccc}\n        \\toprule\n        \\textbf{Method / Budget}  & 32 & 64 & 128 & 256 & 512 \\\\  \\midrule\n        H2O & 0\\% & 1\\% & 1\\% & 1\\% & 3\\% \\\\\n        TOVA & 0\\% & 1\\% & 1\\% & 3\\% & 8\\% \\\\\n        StreamingLLM & 1\\% & 1\\% & 1\\% & 3\\% & 5\\% \\\\\n        \\textbf{\\method (ours)} & \\textbf{65\\%} & \\textbf{99\\%} & \\textbf{99\\%} & \\textbf{99\\%} & \\textbf{100\\%} \\\\\n        \\bottomrule\n    \\vspace{0.03in}\n    \\end{tabular}\n    \\begin{tabular}{llccccc}\n        \\toprule\n        \\textbf{Method / Budget}  & 256 & 512 & 1024 & 2048 & 4096 \\\\  \\midrule\n        H2O & 2\\% & 2\\% & 2\\% & 2\\% & 4\\% \\\\\n        TOVA & 2\\% & 2\\% & 2\\% & 2\\% & 10\\% \\\\\n        StreamingLLM & 1\\% & 1\\% & 1\\% & 2\\% & 4\\% \\\\\n        \\textbf{\\method (ours)} & \\textbf{88\\%} & \\textbf{92\\%} & \\textbf{96\\%} & \\textbf{100\\%} & \\textbf{100\\%} \\\\\n        \\bottomrule\n    \\end{tabular}\n    \n    \\caption{(i) Results of 10k length passkey retrieval test on LongChat-7b-v1.5-32k. (ii) Results of 100k length passkey retrieval test on Yarn-Llama-2-7b-128k. Quest can achieve nearly perfect accuracy with 64 and 1024 tokens KV cache budget, which is about 1\\% of the total sequence length, demonstrating that \\method can effectively preserve the model's ability to handle long-dependency tasks. However, KV cache eviction algorithms such as H2O, TOVA, and StreamingLLM incorrectly discard the KV cache of the answer before receiving the question, thus failing to achieve ideal accuracy. }\n    \\label{tab:passkey}\n\\end{table}\n\n\\subsection{Setting}\nWe evaluate \\method on the language modeling dataset PG19~\\cite{raecompressive2019}, passkey retrieval task~\\cite{peng2023yarn}, and six datasets in LongBench~\\cite{bai2023longbench}: NarrativeQA~\\cite{kocisky-etal-2018-narrativeqa}, HotpotQA~\\cite{yang2018hotpotqa}, Qasper~\\cite{dasigi2021dataset}, TrivialQA~\\cite{joshi-etal-2017-triviaqa}, GovReport~\\cite{huang-etal-2021-efficient}, MultifieldQA~\\cite{bai2023longbench}. We choose two widely used long-context models for our evaluation: LongChat-v1.5-7b-32k~\\cite{longchat2023} and Yarn-Llama-2-7b-128k~\\cite{peng2023yarn}. We compare our method against the KV cache eviction algorithm H2O~\\cite{zhang2023h2o}, TOVA~\\cite{oren2024transformers}, and StreamingLLM~\\cite{xiao2023streamingllm}. Note that we \\textbf{do not} apply any \\method and other baseline algorithms to the first two layers of the model, as our analysis in Sec~\\ref{sec:methodDesp} indicates a low sparsity ratio for these layers.\n\n\\subsection{Accuracy Evaluation}\n\\subsubsection{Language Modeling on PG19}\n\nWe first evaluate the language modeling perplexity on the PG19 test set, which is a dataset comprising 100 books with an average length of 70k tokens. We use the LongChat-7b-v1.5-32k model to test 32k tokens on PG19. We feed the model with various numbers of tokens and evaluate the perplexity of generated tokens. We evaluate H2O, TOVA, and \\method with a token budget of 4096, which is approximately 1/8 of the total token length.  As indicated by the perplexity results in \\fig\\ref{fig:pg19ppl}, \\method's accuracy closely matches the oracle baseline with a full KV cache.\n\n\\subsubsection{Results on long text passkey retrieval task}\nSince language modeling evaluation only involves local dependencies, models can achieve great performance by focusing on recent tokens. However, the ability to handle long-distance dependencies is crucial for long text reasoning. For KV cache eviction algorithms like H2O and TOVA, parts of KV caches that are important for distant future tokens may be discarded, thereby preventing the model from obtaining the correct answer.\n\\begin{figure}\n    \\centering\n     \\includegraphics[width=0.45\\textwidth]{figure_text/pg19_ppl.pdf}\n    \\caption{Language modeling evaluation of \\method on PG19 dataset. We prompt the model with 0 to 32000 tokens from the PG19 test set and measure the perplexity of output tokens. H2O* and TOVA* indicate that for the first two layers of models, we do not apply these two algorithms to prune the KV Cache, as analyzed in Sec~\\ref{sec:methodDesp}, which better preserves the model performance. \\method also uses a full cache in the first two layers of the model. \\method can closely match the performance of the full cache model.}\n    \\label{fig:pg19ppl}\n\n\\end{figure}\nTo show that \\method helps maintain the ability of models to handle longer dependency tasks, we evaluate it on the passkey retrieval task from Yarn~\\cite{peng2023yarn}. This task measures a model's ability to retrieve a simple passkey from a large amount of meaningless text. We put the answer in different depth ratios of the text and evaluate if the model can retrieve the correct answer with different KV cache token budgets. We evaluate LongChat-7b-v1.5-32k on 10k tokens test and Yarn-Llama-2-7b-128k on 100k tokens test.\n\nSince H2O~\\cite{zhang2023h2o} needs to calculate historical attention scores for KV cache pruning, it needs to compute the complete $O(n^2)$ attention map and thus is unable to use Flash-Attention~\\cite{dao2022flashattention} for long-context inference. Therefore, to enable H2O on long-context evaluation, we use Flash-Attention in the context stage for the 100k sequence length passkey retrieval test and start collecting historical attention scores for H2O in the decoding stage. For TOVA~\\cite{oren2024transformers} and StreamingLLM~\\cite{xiao2023streamingllm}, we evaluated them on the 10k and 100k sequence lengths.\n\n\\begin{figure*}[t]\n    \\centering\n    \\subfigure{\n        \\includegraphics[width=0.31\\linewidth]{figure_text/fig_qasper.pdf}\n    }\n    \\subfigure{\n        \\includegraphics[width=0.31\\linewidth]{figure_text/fig_hotpotqa.pdf}\n    }\n    \\subfigure{\n        \\includegraphics[width=0.31\\linewidth]{figure_text/fig_govreport.pdf}\n    }\n    \\subfigure{\n        \\includegraphics[width=0.31\\linewidth]{figure_text/fig_triviaqa.pdf}\n    }\n    \\subfigure{\n        \\includegraphics[width=0.31\\linewidth]{figure_text/fig_narrativeqa.pdf}\n    }\n    \\subfigure{\n        \\includegraphics[width=0.31\\linewidth]{figure_text/fig_multifieldqa.pdf}\n    }\n    \\caption{We evaluate \\method and baselines across six long context datasets with various token budgets. \\method constantly surpassing all baselines at all datasets and all token budgets. For most of the dataset, \\method reaches comparable accuracy with a $1$K token budget. To evaluate the impact of different methods on the model's ability to retrieve long-dependency information, we simulate decoding by feeding the task's question to the model token by token. }\n    \\label{fig:longbench}\n\\end{figure*}\nFor the passkey retrieval test, we directly prefill the input text containing the passkey and texts to the model. However, to evaluate the impact of different methods on the model's ability to handle long-dependency tasks in practical scenarios, we simulate decoding by feeding the task's question and instruction to the model token by token. In this case, H2O and TOVA might mistakenly discard tokens critical for future tokens, such as the passkey that will be queried later. Similarly, StreamingLLM can only focus on the most recent text window, and if the passkey appears outside this window, it cannot provide the correct answer. Therefore, H2O, TOVA, and StreamingLLM cannot achieve ideal accuracy on the 10k and 100k length passkey retrieve test. However, \\method does not discard KV cache but instead uses a query-aware approach to identify critical tokens. As shown in Tab.~\\ref{tab:passkey}, \\method can achieve perfect accuracy with a minimal budget both on 10k and 100k sequence length tests.\n\n\\begin{figure*}[t]\n    \\centering\n    \\subfigure[\\estimate{}]{\n        \\includegraphics[width=0.47\\linewidth]{figure_text/fig_kernel_estimate.pdf}\n        \\label{fig:kernel-estimate}\n    }\n    \\subfigure[\\approxattn{}]{\n        \\includegraphics[width=0.47\\linewidth]{figure_text/fig_kernel_approx_attn.pdf}\n        \\label{fig:kernel-approxattn}\n    }\n    \\caption{We measure the latency of individual kernels in \\method. (a) As sequence length increases, the relative criticality estimation latency decreases to $1/\\text{Page Size}$ of FlashInfer. (b) Approximate attention with token budget K consumes constant time irrelevant to total sequence length and reaches similar performance of FlashInfer at sequence length K.}\n    \\label{fig:kernel_efficiency}\n\\end{figure*}\n\n\\subsubsection{Results on LongBench}\n\nTo validate that \\method can outperform baselines on general long-context datasets, we evaluate our method and baselines on six datasets in LongBench. We evaluate on LongChat-7b-v1.5-32k across a wide range of long-context datasets, including single-document QA: NarrativeQA, Qasper, MultiFieldQA; multi-document QA: HotpotQA; summarization: GovReport; few-shot learning: TriviaQA. We evaluate H2O, TOVA, StreamingLLM, and \\method with different KV cache budgets. For all datasets, we split the input into material and question/instruction. For the material part, we use Flash-Attention~\\cite{dao2022flashattention} with the full KV cache to perform inference. For the question part, we simulate decoding by feeding them to the model token by token. Similar to the passkey retrieval test, to enable H2O to use Flash-Attention, we could not collect H2O's historical attention scores during the context stage, thus starting from the decoding stage.\n\nAs shown in the \\fig\\ref{fig:longbench}, \\method consistently outperforms all baselines across six long-context datasets with various KV cache budgets. \\method with a budget of $1$K tokens can achieve comparable performance as the model with full KV cache, while other baselines still exhibit a notable gap from full cache performance even with a larger budget. After considering the full cache used in the first two layers, \\method can achieve lossless performance on Qasper, HotpotQA, GovReport, TriviaQA, NarrativeQA, and MultifieldQA with KV cache sparsity of 1/6, 1/6, 1/5, 1/10, 1/5, and 1/6, respectively. This demonstrates that \\method is capable of maintaining the model's capabilities across different types of long-context tasks, as it does not lead to the generation of incorrect answers due to improper discarding of KV cache.\n\n\\subsection{Efficiency evaluation}\nTo demonstrate the feasibility of \\method{}, we implement the entire framework with dedicated CUDA kernels based on FlashInfer~\\cite{flashinfer}, a kernel library for LLM inference. We first evaluate \\method{}'s kernel-level efficiency under the configuration of Llama2-7B on an RTX4090 with CUDA 12.2 in Sec~\\ref{sec:eval:kernel}. Besides, we show the end-to-end speedup of \\method{} in text generation as shown in Sec~\\ref{sec:eval:e2e}. We compare \\method{} with a normal attention implementation from the original FlashInfer. To demonstrate the improvement, we qualitatively compare efficiency under the same accuracy between \\method{} and baselines in Sec~\\ref{sec:eval:comparison}. Note that we use an Ada 6000 GPU~\\cite{ada6000} in end-to-end evaluations for longer context length.\n\n\\subsubsection{Kernel evaluation}\n\\label{sec:eval:kernel}\nDue to the memory-bound nature of LLM inference, the speedup of \\method{} is proportional to the sparsity ratio (which is equivalent to memory movement reduction). We quantify this effect in \\fig\\ref{fig:kernel_efficiency}, which evaluates per-kernel performance with NVIDIA's benchmark tool NVBench~\\cite{nvidia_nvbench}.\n\n\\textbf{\\estimate{}}\nWe evaluate the latency of criticality estimation in \\method{} under different sequence lengths and page sizes. At short sequence length, the memory bandwidth utilization of estimation is smaller than that of FlashInfer, as the total memory load size is not enough to fully utilize GPU memory bandwidth. As sequence length grows, the relative performance improves and approaches $1/\\text{Page Size}$ since estimation only consumes one token per page. Note that techniques like quantization or larger page size can further reduce the additional memory usage.\n\n\\begin{figure*} [t]\n    \\centering\n     \\includegraphics[width=\\textwidth]{figure_text/fig_breakdown.pdf}\n    \\caption{\\method self-attention time breakdown compared to FlashInfer. At all sequence lengths, \\method significantly outperforms FlashInfer, as the memory movement is reduced. At sequence length $32$K with token budget $2048$, \\method speeds up self-attention by \\selfspeedup.}\n    \\label{fig:effibreak}\n\\end{figure*}\n\\begin{figure*}\n    \\centering\n     \\includegraphics[width=0.97\\textwidth]{figure_text/fig_e2e_pdf.pdf}\n    \\caption{End-to-end latency of \\method. For all sequence lengths, \\method significantly outperforms FlashInfer. Increasing the sequence lengths only slightly changes the latency of \\method. At a given sequence length, \\method's latency slightly increases as the token budget grows. With sequence length $32$K, token budget $2048$, $4$-bit weight quantization, \\method speedup end-to-end inference by \\eespeedup. }\n    \\label{fig:e2e}\n\\end{figure*}\n\n    \n\n\\textbf{\\topk{}}\nWe enable the \\topk{} in \\method{} with a batched Top-K CUDA operator from a vector search kernel library RAFT~\\cite{topk2023}. We test the latency of \\topk{} under different sequence lengths and token budgets. Since \\estimate{} reduces one entire token into one criticality score, \\topk{} has limited memory movement compared to other operators, thus having a low latency overhead of 5-10 us for sequence length less than $128$k.\n\n\\textbf{\\approxattn{}}\nSince \\method{} is compatible with PageAttention, approximate attention can be easily implemented by feeding Top-K page indices as sparse loading indices. We compare \\method{}'s approximate attention with the original attention of FlashInfer under different sequence lengths and token budgets with a $16$ page size. At a given token budget $B$, the latency of \\approxattn{} is a constant regardless of the sequence length. Since \\approxattn{} introduces minimal overhead, it has a similar latency as FlashInfer at sequence length $B$. \n\nWe further evaluate \\method{}'s attention mechanism, which combines \\estimate{},\\topk{}, and \\approxattn{}, on the Llama2-7B model using the PyTorch profiler. We show the time breakdown of \\method{} in \\fig\\ref{fig:effibreak} on various sequence lengths. \\method{} reduce the self-attention time by \\selfspeedup compared with FlashInfer at $32$K sequence length with $2048$ token budget.\n\n\\begin{figure*}[!h]\n    \\centering\n     \\includegraphics[width=0.99\\textwidth]{figure_text/fig_comparison.pdf}\n    \\caption{Efficiency comparison of \\method{} with baselines under the same accuracy constraint. (a) Tokens budgets needed for comparable accuracy by different attention methods. Full denotes the original attention, which means the average context length of benchmarks. (b) Inference latency of different attention methods for comparable accuracy. \\method{} boosts $3.82\\times$ speed on GovReport compared to TOVA.}\n    \\label{fig:comparision}\n\n\\end{figure*}\n\n\\subsubsection{End-to-End Evaluation}\n\\label{sec:eval:e2e}\nTo show the practical speedup of \\method{}, we deploy the framework into real-world single-batch scenarios. We measure the average latency of generating one token in the decode stage under different sequence lengths and token budgets. Note that we do not measure the sampling process since its execution time is smaller and depends on the setting. We compare \\method{} with a full \\kvc{} baseline which is implemented by FlashInfer. As shown in \\fig\\ref{fig:e2e}, \\method{} outperforms FlashInfer at all sequence lengths. The latency of \\method{} grows significantly slower than FlashInfer when the sequence length increases, as \\method{} maintains similar token budgets. At sequence length $32$K and token budget $2048$, \\method{} boosts inference speed by $1.74\\times$ with FP16 weights and $2.23\\times$ with 4-bit quantized weight. \n\n\\subsubsection{Comparison with Baselines}\n\\label{sec:eval:comparison}\nTo demonstrate the performance improvements of \\method{}, we compare the inference efficiency of different attention mechanisms under the same accuracy constraint, i.e. lossless accuracy of six tasks from LongBench. We show token budgets needed for the lossless accuracy target by different attention mechanisms in Fig~\\ref{fig:comparision}(a). For example, NarrativeQA exhibits an average context length of $24$K tokens. To achieve lossless accuracy, TOVA requires a token budget of $14$K, whereas \\method{} necessitates only $5$K tokens leading to much higher sparsity.\n\nHowever, none of the baselines included a kernel implementation of their proposed method. Consequently, we conduct a qualitative analysis of the baselines' self-attention efficiency by utilizing the inference latency of FlashInfer, disregarding other runtime overheads (e.g., TOVA's requirement to calculate history scores~\\cite{oren2024transformers}). In contrast, \\method{} is evaluated in a practical setting with consideration of all operators. As shown in \\fig\\ref{fig:comparision}(b), \\method{} significantly surpasses all baselines in terms of self-attention latency due to the high query-aware sparsity. For GovReport and TriviaQA, \\method{} boosts the inference by $3.82\\times$ and $4.54\\times$, respectively. Therefore, \\method{} can achieve higher efficiency while maintaining superior accuracy.\n\\section{Conclusion}\nWe present \\method, an efficient and accurate \\kvc{} selection algorithm that exploits \\qaware{} sparsity. \\method{} dynamically estimates the criticality of tokens in \\kvc{} based on the per-page metadata and the current query. It then performs self-attention only on the critical tokens with greatly reduced memory movement, providing high sparsity with negligible accuracy loss. Comprehensive evaluations demonstrate that \\method provides up to \\selfspeedup self-attention speedup, which contributes to \\eespeedup end-to-end latency reduction in the decode phase. Compared to prior baselines, \\method{} reduces up to $4.5\\times$ self-attention latency with the same accuracy target under long-context benchmarks.\n\n\\section*{Acknowledgements}\nWe thank MIT-IBM Watson AI Lab, Amazon and MIT Science Hub, MIT AI Hardware Program, National Science Foundation, and Samsung for supporting this research. We thank NVIDIA for donating the DGX server. \nWe thank Zihao Ye for his insightful discussion, feedback, and useful advice on algorithm design and FlashInfer integration. This work was also supported by\ngenerous gifts from Intel, Google, and the PRISM Research Center, a JUMP Center cosponsored by SRC and DARPA.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{\\textsc{ShadowKV}: KV Cache in Shadows for High-Throughput Long-Context LLM Inference}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nWith the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present \\Sys, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, \\Sys employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating \\Sys on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch sizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at \\url{https://github.com/bytedance/ShadowKV}.\n\\end{abstract}\n\n\\section{Introduction}\nLarge language models (LLMs) have increasingly demonstrated their ability to scale and handle long contexts \\citep{bingchat, liu2024world, achiam2023gpt, team2023gemini}, enabling them to tackle complex tasks like multi-document question answering and information retrieval from extensive contexts of up to 1M tokens \\citep{achiam2023gpt,wang2024loong}. However, efficiently serving these long-context LLMs presents challenges related to the key-value (KV) cache \\citep{liu2024scissorhands, ge2023model}, which stores previous key-value activations to avoid re-computation. As the KV cache scales with sequence length, its growing memory footprint and the need to access it for each token generation lead to low throughput during long-context LLM inference. To address these issues, KV cache eviction or sparse attention methods have been widely explored.\n\nHowever, existing methods face three primary limitations: accuracy degradation, inadequate memory reduction, and significant decoding latency overhead. KV cache eviction strategies \\citep{zhang2024h2o, zhang2024pyramidkv} aim to reduce the memory footprint by discarding KV pairs based on specific policies, but they often result in information loss and accuracy degradation in tasks such as multi-turn conversations \\citep{yang2024no, tang2024razorattention}. Dynamic sparse attention methods \\citep{tang2024quest} preserve all KV pairs on the GPU and accelerate inference by computing attention with selected KV pairs. However, this line of work does not mitigate the memory footprint, thereby limiting the batch size and preventing accommodation of extremely long contexts (e.g., 1M tokens). A naive solution based on sparse attention involves offloading the KV cache to the CPU to reduce memory usage \\citep{lee-osdi24, he2024fastdecode}. Nonetheless, this approach incurs significant overhead due to the latency of fetching the selected sparse KV pairs from the CPU during decoding.\n\nConsequently, an ideal effective system for long-context LLM inference with sparse attention should: (i) reduce GPU memory usage, (ii) minimize inference latency, and (iii) maintain accuracy within limited sparse KV cache budgets. Fortunately, we can potentially overcome these challenges by leveraging our discovery that pre-Rotary Position Embedding \\citep{su2024roformer} (RoPE) keys are exceptionally low-rank compared to the layer inputs, post-RoPE keys, values, key weight matrix, and value weight matrix, as indicated in \\cref{fig:singular_combined}. Furthermore, our analysis in \\cref{fig:similarity} reveals that pre-RoPE keys lack significant similarities in low-rank subspaces across different sequences, while a sequence and its continuation tend to strongly share low-rank subspaces, enabling high compression rates within each sequence. Motivated by these findings, we have developed two key insights that pave the way for  the design of an applicable system, detailed in \\cref{sec:observation}.\n\n\\begin{figure}[t]\n    \\centering\n   \\begin{subfigure}[b]{0.32\\textwidth}\n        \\includegraphics[width=\\linewidth]{imgs/singular_combined.pdf}\n        \\caption{}\n        \\label{fig:singular_combined}\n   \\end{subfigure}\n    \\hfill\n       \\centering\n   \\begin{subfigure}[b]{0.32\\textwidth}\n        \\includegraphics[width=\\linewidth]{imgs/frobs.pdf}\n        \\caption{}\n        \\label{fig:similarity}\n   \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[b]{0.32\\textwidth}\n        \\includegraphics[width=\\linewidth]{imgs/svd_overhead.pdf}\n        \\caption{}\n        \\label{fig:svd_overhead}\n   \\end{subfigure}\n    \\hfill\n    \\caption{(a) For a sample from PG-19 \\citep{rae2019compressive, gao2020pile} fed into Llama-3.1-8B, the pre-RoPE keys are the most low-rank, as indicated by the sharpest decay in singular values. (b) Average similarities, defined in \\cref{sec:lr_compression}, between rank-$256$ truncated SVD projections of pre-RoPE keys from PG-19 sequences using Llama-3.1-8B. Similarity is measured between a length 16K ``Context'' and either a 16K+2K continuation on ``Context'' (``Extended context'') or a new length 16K sequence (``Inter-context''). Pre-RoPE keys within sequences exhibit similar low-rank subspaces, while those between sequences show different patterns. (c) The relative overhead of singular value decomposition (SVD)  decreases as sequence length scales for the pre-filling stage.}\n    \\label{fig:observation}\n\\end{figure}\n\n\\underline{\\textit{Low-rank Keys and Offloaded Values for Storage}}: In long-context LLM inference, the quadratic scaling of attention computation with sequence length makes the linear cost of low-rank decomposition during pre-filling negligible, as illustrated in \\cref{fig:svd_overhead}\\footnote{In practical scenarios, the key cache can be offloaded to the CPU to perform SVD asynchronously or precomputed and stored as part of the prefix cache \\citep{juravsky2024hydragen}.}. To reduce memory footprint, we retain the low-rank pre-RoPE key cache on the GPU and offload the value cache to the CPU since the value cache does not exhibit low-rank properties, minimizing memory footprint without sacrificing accuracy. During decoding with sparse attention, we employ CUDA multi-streams to overlap the recovery of the selected key cache with the fetching of the corresponding value cache. This approach conceals key cache reconstruction and reduces data fetching overhead by $2\\times$ compared to the naive offloading strategy, thereby decreasing the latency of sparse attention during decoding. \n\n\\underline{\\textit{Accurate KV Selection for Fast Decoding}}: To further reduce decoding latency in sparse attention, we propose an accurate KV selection method that maintains accuracy with minimal sparse budgets (1.56\\%). Our analysis reveals that most post-RoPE keys exhibit high cosine similarity with adjacent tokens, enabling chunk-level approximations for selecting important tokens. A minimal number of outlier chunks (0.3\\%), which are more challenging to approximate  (\\cref{fig:OB2}), are stored as static cache on the GPU to preserve accuracy. As shown in \\cref{fig:shadowkv}, our method outperforms the naive sparse attention approach \\citep{tang2024quest} and achieves higher sparsity, accelerating decoding.\n\nBuilding on these insights, we present \\Sys in \\cref{sec:shadowkv}, depicted in \\cref{fig:shadowkv}, a high-throughput system for long-context LLM inference. Specifically, during pre-filling, we offload the value cache to the CPU, retaining only the low-rank pre-RoPE keys, along with compressed landmarks of the key cache and detected outliers for larger batch sizes. During decoding, landmarks are used to select chunk indices for key cache recovery and value cache fetching. We perform accurate sparse attention computation with selected KV pairs and static outliers to achieve high throughput.\n\n \\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{imgs/shadowkv.pdf}\n    \\caption{\\textbf{Left:} \\Sys enhances long-context LLM inference throughput by offloading the value cache to the CPU while maintaining a low-rank key cache, landmarks, and outliers on the GPU. During decoding, it employs landmarks for efficient sparse attention, reducing computation and data movement. \\textbf{Right:} \\Sys effectively utilizes a limited KV budget to achieve high accuracy, theoretically reaching over 7 TB/s equivalent bandwidth on an A100, and empirically boosts generation throughput by 3.04$\\times$ for Llama-3.1-8B with on a batch of 122K contexts.}\n    \\label{fig:shadowkv}\n\\end{figure}\n\nEmpirically, we conduct extensive experiments and ablation studies to demonstrate the effectiveness and efficiency of \\Sys. In \\cref{sec:acc}, we evaluate across various long-context LLMs, such as Llama-3-8B-1M \\citep{gradllama}, Llama-3.1-8B \\citep{meta_llama_3_1}, GLM-4-9B-1M \\citep{glm2024chatglm}, Yi-9B-200K \\citep{ai2024yi}, Phi-3-Mini-128K \\citep{abdin2024phi} and Qwen2-7B-128K \\citep{yang2024qwen2} using benchmarks including RULER \\citep{hsieh2024ruler},  LongBench \\citep{bai2023longbench}, and Needle In A Haystack \\citep{niah} with contexts up to 1M.\n\nIn \\cref{sec:eff}, we demonstrate that \\Sys can support 6$\\times$ larger batch sizes and boost throughput by 3.04$\\times$ compared to small batches on an A100 using Llama-3.1-8B, with each sample having a context length of 122K. We also present results across different models and context lengths, increasing throughput up to 2.97$\\times$ for Llama-3-8B-1M, 2.56$\\times$ for GLM-4-9B-1M, and 2.66$\\times$ for Yi-9B-200K, even surpassing infinite batch size  under the assumption of infinite GPU memory.\\section{Related Works}\n\\paragraph{Token Eviction.} To reduce memory footprint, eviction-based strategies keep a fixed size of KV cache to store the critical token KV pairs and discard unnecessary tokens. StreamingLLM \\citep{xiao2023efficient} addresses the limitations of window attention by retaining attention sinks and recent KV pairs. H$_2$O \\citep{zhang2024h2o} introduces a low-cost eviction policy, updating the KV cache based on cumulative attention scores. LESS \\citep{dong2024get} accumulates evicted token information by a constant-sized low-rank cache, which allows partial access to previously evicted information, along with tokens maintained by a sparse policy. SnapKV \\citep{li2024snapkv} uses the local window of prompts to select important tokens for future generations. However, they suffer from performance degradation and information loss since the evicted tokens will never be recovered.\n\\paragraph{Dynamic Sparse Attention.} This line of work retains all KV cache but performs dynamic sparse attention within selected KV pairs to reduce inference latency. SparQ \\citep{ribar2023sparq} uses the norm of the query to decide an important subset of the key cache's channels to calculate a metric to select relevant tokens. Quest \\citep{tang2024quest} segments tokens into pages and selects pages by approximating the highest attention within each page. Loki \\citep{singhania2024loki} performs principal component analysis on the key cache using a calibration dataset, selecting tokens based on attention scores computed in low-dimensional space. TriForce \\citep{sun2024triforce} combines sparse attention with speculative decoding \\citep{leviathan2023fast} for lossless acceleration. InfiniGen \\citep{lee-osdi24} offloads all KV cache to the CPU and prefetches essential entries during decoding.\n\\paragraph{Quantization.} Several methods have been introduced to optimize KV cache quantization \\citep{hooper2024kvquant, yue2024wkvquant, xiao2023smoothquant}, reducing memory consumption while retaining accuracy. KIVI \\citep{liu2024kivi} applies different quantization strategies for keys and values, quantizing the keys per-channel and the values per-token to 2-bit. Palu \\citep{chang2024palu} decomposes KV weight matrices offline, caching low-rank KV projections to achieve a higher compression rate. Quantization methods reduce the KV cache bit width, which is orthogonal to our approach.\n\\section{Observations}\n\\label{sec:observation}\nWe present two key insights of long-context LLMs that inspire \\Sys's design, as follows.\n\\begin{figure}[t]\n    \\centering\n    \\begin{subfigure}[b]{0.32\\textwidth}\n        \\includegraphics[width=\\linewidth]{imgs/OB1_2.pdf}\n        \\caption{}\n        \\label{fig:OB1_2}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[b]{0.32\\textwidth}\n        \\includegraphics[width=\\linewidth]{imgs/OB2.pdf}\n        \\caption{}\n        \\label{fig:OB2}\n   \\end{subfigure}\n       \\begin{subfigure}[b]{0.32\\textwidth}\n        \\includegraphics[width=\\linewidth]{imgs/hitrate.pdf}\n        \\caption{}\n        \\label{fig:hitrate}\n   \\end{subfigure}\n    \\caption{(a) Accuracy on the needle retrieval task across various ranks shows that the pre-RoPE key cache can be compressed by over 6 times without a drop in accuracy.  (b) The number of notable outlier chunks is small, taking only 0.2-0.3\\%. (c) The KV cache has a high hit rate, reducing computations and data movements by over 60\\% for each decoding step.}\n    \\label{fig:observation}\n\\end{figure}\n\n\\subsection{Low-Rank Keys and Offloaded Values for Storage}\n\\label{sec:lr_compression}\nTo reduce memory footprint, the low-rank nature of the KV cache has been explored by recent studies \\citep{deepseekv2, xu2024think, chang2024palu}. However, these methods focus on data-independent decomposition, either requiring training or achieving limited compression rates.\n\n\\paragraph{Observation.} In our study, by conducting SVD on the model weights $W_k$, $W_v$, the input $X$, the pre-/post-RoPE key cache, and the value cache of Llama-3.1-8B, we visualize the relative singular value distributions in \\cref{fig:singular_combined} together with the accuracy in \\cref{fig:OB1_2}. As we observed, pre-RoPE keys have the lowest rank and can be compressed by 6$\\times$ without performance degradation. \n\nWe also identify striking dynamic and static behaviors in low-rank keys between and within sequences, inspired by a related investigation in FFN layers \\citep{dong2024prompt}. Analogous to cosine similarity, we define $\\mathcal{D}(\\boldsymbol{H}_1, \\boldsymbol{H}_2) = \\langle \\boldsymbol{H}_1, \\boldsymbol{H}_2 \\rangle /r$ to be the similarity metric between low-rank subspaces of two rank-$r$ projection matrices, $\\boldsymbol{H}_1$ and $\\boldsymbol{H}_2$, where $\\langle \\cdot, \\cdot \\rangle$ is the Frobenius inner product\\footnote{Since $\\boldsymbol{H}_1$ and $\\boldsymbol{H}_2$ are projection matrices, their squared Frobenius norms are the sum of their singular values which consist of $r$ 1's and $d-r$ 0's, i.e., $\\|\\boldsymbol{H}_1 \\|^2_F = r$. Thus, by Cauchy-Schwarz, $|\\mathcal{D}(\\boldsymbol{H}_1, \\boldsymbol{H}_2)| \\leq 1$. Additionally, $\\mathcal{D}(\\boldsymbol{H}_1, \\boldsymbol{H}_2) \\geq 0$ by the cyclic property of trace and positive semidefiniteness of projection matrices. Together, this shows $\\mathcal{D}(\\boldsymbol{H}_1, \\boldsymbol{H}_2) \\in [0,1]$, maximized or minimized when the projection matrices project onto identical or orthogonal subspaces, respectively.}. In our case with truncated SVDs of pre-RoPE keys, let $\\boldsymbol{K}_1, \\boldsymbol{K}_2 \\in \\mathbb{R}^{n \\times d}$ have rank-$r$ truncated SVDs, $\\boldsymbol{\\Phi}_1 \\boldsymbol{\\Sigma}_1 \\boldsymbol{\\Psi}^\\top_1$ and $\\boldsymbol{\\Phi}_2 \\boldsymbol{\\Sigma}_2 \\boldsymbol{\\Psi}^\\top_2$, respectively, where $\\boldsymbol{\\Phi}_1 \\in \\mathbb{R}^{n \\times r}, \\boldsymbol{\\Sigma}_1 \\in \\mathbb{R}^{r \\times r}, \\boldsymbol{\\Psi}_1 \\in \\mathbb{R}^{d \\times r}$, and similarly for $\\boldsymbol{\\Phi}_2$, $\\boldsymbol{\\Sigma}_2$, and $\\boldsymbol{\\Psi}_2$. Then, $\\mathcal{D}(\\boldsymbol{\\Psi}_1 \\boldsymbol{\\Psi}_1^\\top, \\boldsymbol{\\Psi}_2 \\boldsymbol{\\Psi}_2^\\top)$ can measure the similarity between the low-rank subspaces of the two right singular matrices. Depicted in \\cref{fig:similarity}, pre-RoPE keys between sequences do not strongly share similar low-rank subspaces, but extensions of the same sequence do.\n\n\\paragraph{Insights.} Our observation of the low-rank nature in the pre-RoPE keys indicates that storing the low-rank projections is sufficient for each sequence. By keeping the low-rank key cache on the GPU and offloading the value cache to the CPU since it is not low-rank, we can largely reduce the memory footprint. During decoding, selected KV pairs can be reconstructed on-the-fly for computation.\n\n\\subsection{Accurate KV Selection for Fast Decoding}\nTo further reduce the latency overhead in sparse attention, including fetching the selected value cache from the CPU and reconstructing the corresponding key cache, an accurate KV selection method is needed to minimize the sparse KV cache budget while maintaining the accuracy.\n\n\\paragraph{Observation.} We found most post-RoPE key cache exhibits spatial locality, with high cosine similarity to adjacent tokens, except for a few outliers. To quantify this, we conducted inference experiments on 128K contexts. We divided the post-RoPE keys into chunks of eight tokens and visualized the minimum cosine similarity between the chunk's mean and its key cache, as shown in \\cref{fig:OB2}. The results indicate that, apart from a few outliers, there is generally high cosine similarity, suggesting the mean values can serve as landmarks to approximate attention well within normal chunks.\n\n\\paragraph{Analysis.} This finding suggests that for the majority of chunks, we can maintain the mean value as compressed landmarks to select minimal important KV pairs (1.56\\%) accurately during decoding. Outlier chunks, which may contain dense or critical information and are difficult to approximate, are retained to ensure accuracy. Given their relatively small number (0.2--0.3\\%), storing them on the GPU is feasible without affecting memory capacity. Furthermore, as shown in \\cref{fig:hitrate}, considering the temporal locality of the KV cache, a cache policy \\citep{zhang2024pqcache} can be leveraged to further reduce the latency overhead by 60\\% during decoding with optimized kernels \\citep{nvidia2020cuda}.\n\n\\section{\\Sys}\n\\label{sec:shadowkv}\nIn this section, we introduce \\Sys, a high-throughput long-context LLM inference system. We first elaborate our algorithm in \\cref{algo}, covering both the pre-filling and decoding phases. Subsequently, in \\cref{anal}, we discuss the concept of theoretical equivalent bandwidth to illustrate the benefits of our approach.\n\n\\begin{figure}[t]\n\\begin{minipage}{0.57\\linewidth}\n\\vspace{3pt}\n\\begin{algorithm}[H]\n\\caption{\\Sys Pre-filling}\n\\label{alg:prefill}\n\\begin{algorithmic}\n\\State \\textbf{Input:} $\\boldsymbol{K},\\boldsymbol{K}^{\\text{RoPE}}, \\boldsymbol{V} \\in \\mathbb{R}^{b\\times h_{kv} \\times s\\times d}$, SVD rank $r$, chunk size $c$, number of outlier chunks $o$\n\n\\State {\\color{commentcolor}{\\textit{$\\triangleright$ Store low-rank projection of pre-RoPE key cache}}}\n\\State $\\boldsymbol{A} \\in \\mathbb{R}^{b\\times s \\times r}$, $\\boldsymbol{B} \\in \\mathbb{R}^{b \\times h_{kv} \\times r \\times d} \\leftarrow \\text{SVD}(\\boldsymbol{K})$\n\n\\State {\\color{commentcolor}{\\textit{$\\triangleright$ Segment post-RoPE key cache into chunks and compute the mean of each chunk}}}\n\\State $\\boldsymbol{C} \\in \\mathbb{R}^{b\\times h_{kv} \\times s/c \\times d} \\leftarrow \\text{Reduce}(\\boldsymbol{K^{\\text{RoPE}}})$\n\n\\State {\\color{commentcolor}{\\textit{$\\triangleright$ Compute cosine similarity within each chunk}}}\n\\State $\\boldsymbol{S} \\in \\mathbb{R}^{b\\times h_{kv} \\times s/c \\times c} \\leftarrow \\text{CosineSimilarity}(\\boldsymbol{C}, \\boldsymbol{K^{\\text{RoPE}}})$\n\n\\State {\\color{commentcolor}{\\textit{$\\triangleright$ Find lowest cosine similarity as outliers}}}\n\\State $\\boldsymbol{I}\\in \\mathbb{R}^{b\\times h_{kv} \\times o} \\leftarrow \\text{ArgTopK}(-\\text{Min}(\\boldsymbol{S}, \\text{dim}=-1), o)$\n\n\\State $\\boldsymbol{K}^{\\text{outlier}}, \\boldsymbol{V}^{\\text{outlier}} \\leftarrow \\text{Gather} (\\boldsymbol{K^{\\text{RoPE}}}, \\boldsymbol{V}, \\boldsymbol{I})$\n\n\\State {\\color{commentcolor}{\\textit{$\\triangleright$ Offload the rest of values to the CPU and store the non-outlier chunks' mean as landmarks}}}\n\\State $\\boldsymbol{V}^{\\text{CPU}} \\leftarrow \\boldsymbol{V} \\setminus \\boldsymbol{V}^{\\text{outlier}}$, $\\boldsymbol{L} \\leftarrow \\boldsymbol{C} \\setminus \\text{Gather} (\\boldsymbol{C}, \\boldsymbol{I})$ \n\n\\end{algorithmic}\n\\end{algorithm}\n\\end{minipage}\n\\hfill\n\\begin{minipage}[c]{0.41\\linewidth}\n\\centering\n\\includegraphics[width=0.8\\textwidth]{imgs/prefill.pdf}\n\\caption{\\Sys pre-filling.}\n\\label{fig:prefill}\n\\end{minipage}\n\\end{figure}\n\n\\subsection{Algorithm}\n\\label{algo}\nThe algorithm of \\Sys is divided into two main phases: pre-filling and decoding. The pre-filling phase involves low-rank decomposition of the post-RoPE key cache, offloading the value cache, and constructing landmarks to facilitate subsequent high-throughput decoding. The decoding phase includes accurate KV selection and efficient sparse KV cache reconstruction.\n\n\\paragraph{Pre-filling.} During the pre-filling phase, we optimize GPU memory usage by performing low-rank compression on the key cache of each layer and offloading values to the CPU. Specifically, as demonstrated in \\cref{alg:prefill} and \\cref{fig:prefill}, we apply SVD on the pre-RoPE key cache and store only the low-rank representations for each layer. Post-RoPE key cache is segmented into chunks, with the mean of each chunk computed as landmarks. By computing the cosine similarity within these chunks, we identify poorly approximated tokens as outliers. This small set of outliers is gathered and stored on the GPU as the static cache, while the remaining key cache is maintained as compact landmarks, with the corresponding values offloaded to the CPU memory.\n\n\\paragraph{High-throughput Decoding.} For incoming queries, we first compute the approximate attention scores using the landmarks. As detailed in \\cref{alg:dec}, by identifying the top-k scoring chunk indices, the corresponding values are retrieved from the CPU, and the key cache is simultaneously reconstructed from low-rank projections, effectively concealing the construction of the key cache. Based on the insight that the KV cache has temporal locality, we build cache-aware CUDA kernels, reducing computation and value fetching by 60\\%. As shown in \\cref{fig:decoding}, we conduct an index scan to detect the missed chunks and only rebuild the necessary KV pairs on-the-fly.\n\n\\begin{figure}[t]\n\\begin{minipage}{0.57\\linewidth}\n\\begin{algorithm}[H]\n\\caption{\\Sys Decoding}\n\\label{alg:dec}\n\\begin{algorithmic}\n\\State \\textbf{Input:} $\\boldsymbol{A}$, $\\boldsymbol{B}$, $\\boldsymbol{L}$, $\\boldsymbol{V}^{\\text{CPU}}$, $\\boldsymbol{Q} \\in \\mathbb{R}^{b\\times h_{q} \\times s_q \\times d}$, $\\boldsymbol{K}^{\\text{outlier}}$, $ \\boldsymbol{V}^{\\text{outlier}}$, $\\boldsymbol{K}, \\boldsymbol{V} \\in \\mathbb{R}^{b\\times h_{kv} \\times s_q \\times d}$, number of chunks $n_c$, number of selected chunk budget $k$\n\n\\State {\\color{commentcolor}{\\textit{$\\triangleright$ Compute chunk attention score}}}\n\\State $\\boldsymbol{P} \\in \\mathbb{R}^{b\\times h_{q} \\times s_q \\times n_c}  \\leftarrow \\text{MatMul}(\\boldsymbol{Q}, \\boldsymbol{L^\\top})$\n\n\\State $\\boldsymbol{S} \\in \\mathbb{R}^{b\\times h_{q} \\times s_q \\times n_c}  \\leftarrow \\text{Softmax}(\\boldsymbol{P} / \\sqrt{d})$\n\n\\State $\\boldsymbol{S}_{1} \\in \\mathbb{R}^{b\\times h_{q} \\times n_c}  \\leftarrow \\text{sum}(\\boldsymbol{S}, \\text{dim}=-2) $\n\\State $\\boldsymbol{S}_{2} \\in \\mathbb{R}^{b\\times h_{kv} \\times n_c}  \\leftarrow \\text{max}_{\\text{kv\\_group}}(\\boldsymbol{S}_{1}) $\n\n\\State {\\color{commentcolor}{\\textit{$\\triangleright$ Select top-k chunks for each KV head}}}\n\\State $\\boldsymbol{I}\\in \\mathbb{R}^{b\\times h_{kv} \\times k} \\leftarrow \\text{ArgTopK}(\\boldsymbol{S}_{2}, k)$\n\n\\SetKwBlock{DoParallel}{In parallel:}{end}\n\\State {\\color{commentcolor}{\\textit{$\\triangleright$ Gather values from CPU}}}\n\\State $\\boldsymbol{V}^{\\text{sparse}} \\leftarrow \\text{Gather}(\\boldsymbol{V}^{\\text{CPU}}, \\boldsymbol{I})$\n\\State $\\boldsymbol{V} \\leftarrow [\\boldsymbol{V}^{\\text{outlier}}; \\boldsymbol{V}^{\\text{sparse}} ;\\boldsymbol{V}]$\n\\State {\\color{commentcolor}{\\textit{$\\triangleright$ Recover keys from low-rank projection}}}\n\\State $\\boldsymbol{K}^{\\text{sparse}} \\leftarrow \\text{MatMul}(\\text{Gather}(\\boldsymbol{A}, \\boldsymbol{I}),\\boldsymbol{B})$\n\\State $\\boldsymbol{K} \\leftarrow [\\boldsymbol{K}^{\\text{outlier}} ;\\text{RoPE}(\\boldsymbol{K}^{\\text{sparse}}); \\boldsymbol{K}]$\n\\end{algorithmic}\n\\end{algorithm}\n\\end{minipage}\n\\hfill\n\\begin{minipage}{0.4\\linewidth}\n\\centering\n\\includegraphics[width=0.7\\textwidth]{imgs/decode.pdf}\n\\caption{\\Sys decoding phase.}\n\\label{fig:decoding}\n\\end{minipage}\n\\end{figure}\n\nBased on our observations in \\cref{sec:lr_compression}, future pre-RoPE keys within a sequence reside in a shared low-rank subspace with the context. As a result, an extension of our algorithm would be to store generated tokens as low-rank states using the same projections obtained from pre-filling to reduce the memory usage for future generations\\footnote{If $\\boldsymbol{\\Psi} \\in \\mathbb{R}^{d \\times r}$ is the right singular matrix calculated from the SVD of pre-RoPE context keys $\\boldsymbol{K} \\in \\mathbb{R}^{s \\times d}$, new pre-RoPE keys $\\boldsymbol{K}' \\in \\mathbb{R}^{s_q \\times d}$ can be stored as $\\boldsymbol{K}' \\boldsymbol{\\Psi}$ and projected back up with $\\boldsymbol{\\Psi}^\\top$ when needed.}. For simplicity, we exclude it from the implementation.\n\n\\subsection{Theoretical Equivalent Bandwidth}\n\\label{anal}\nThe benefit of \\Sys in terms of increasing throughput can be analyzed through the concept of equivalent bandwidth. Consider each K or V vector as being $M$ bytes in size, with a sequence length of $S$, a chunk size of $C$, a selected chunk budget of $K$, $O$ outliers, and hit rate $\\alpha$. During KV selection, \\Sys loads approximately $M \\times S/C$ bytes using the GPU memory bandwidth $B_\\text{GPU}$. For value cache fetching, it loads $M \\times K \\times C$ bytes using the PCIe bandwidth $B_\\text{PCIe}$ \\citep{sheng2023flexgen}. Since value movement and key cache reconstruction can be overlapped, we do not need to count key cache reconstruction here. Following this, \\Sys performs standard attention computation for the top-k chunks and predefined outliers, requiring $2M \\times (K+O) \\times C$ bytes. The equivalent bandwidth of \\Sys can be defined as:\n\n\\begin{equation*}\n    B_\\text{equivalent} = \\frac{2SB_\\text{GPU}}{S/C + 2(K+O)C+ (1-\\alpha) KCB_\\text{GPU}/B_\\text{PCIe}}\n\\end{equation*}\n\nFor example, assuming C=8, S=128K, K=256, O=48, $B_\\text{PCIe}$=31.5 GB/s, and $B_\\text{GPU}$=2 TB/s for A100, the equivalent bandwidth of \\Sys is calculated as 7.2 TB/s, which is 3.6$\\times$ higher than A100 memory bandwidth. This result indicates that \\Sys theoretically achieves a high equivalent bandwidth to accelerate attention computation. System implementation is detailed in \\cref{app:sys}.\n\\begin{table}[h]\n\\centering\n\\caption{Performance of different models and different methods on RULER \\citep{hsieh2024ruler} evaluated at length of 128K. \\Sys outperforms other methods with a 1.56\\% sparse budget. }\n\\setlength{\\tabcolsep}{3.5pt} % 调整列间距\n\\small\n\\begin{tabular}{l|rrrrrrrrrr|r}\n\\toprule\nMethods &  N-S1 & N-S2 & N-MK1&N-MK2&N-MQ&N-MV&QA-1&QA-2&VT&FWE &Avg.\\\\ \\midrule\n\t\\textit{Llama-3-8B-1M} & 100.00&100.00&98.96&98.96&98.96&95.57&75.00&48.96&78.54&71.85 & 86.68\\\\\n         Loki & 18.75 & 1.04 & 2.08 & 0.00 & 1.56 & 0.78 & 4.17 & 13.54 & 26.04 & 25.35 & 9.33\\\\\n         Loki (V only) & 41.67 & 6.25 & 37.50 & 1.04 & 8.07 & 30.73 & 10.42 & 19.79 & 51.67 & 37.50& 24.46 \\\\\n\tQuest & \\textbf{100.00} & \\textbf{100.00}& \\textbf{98.96} & 77.08 & 97.65 & 93.49 & 60.42 & 50.00 & 77.08 & 65.63 & 82.03  \\\\\n    Quest (V only) & \\textbf{100.00} & \\textbf{100.00} &\\textbf{98.96} & 85.42 & \\textbf{97.92} & 95.49 & 70.83 & 46.88 & 78.75 & 65.63&83.99\\\\\n\t\\rowcolor{cyan!10}\n\t\\Sys  & \\textbf{100.00} & \\textbf{100.00} & 97.92&\\textbf{98.96}&96.88&\\textbf{95.83}&\\textbf{72.92}&\\textbf{52.08}&\\textbf{81.67}&\\textbf{72.57}&\\textbf{86.88}\\\\\n\t\\midrule\n        \\textit{GLM-4-9B-1M}&100.00 & 100.00 & 94.79&87.50&99.74&93.75&67.71&55.21&97.29&72.22&86.82 \\\\\n         Loki & 71.88 & 27.08 & 22.92 & 2.08 & 9.90 & 11.46 & 28.13 & 27.08 & 31.04 & 54.17& 28.57\\\\\n         Loki (V only) & 96.88 & 55.21 & 56.25 & 18.75 & 51.04 & 50.52 & 45.83 & 39.58 & 72.71 & 59.72 & 54.65\\\\\n\tQuest &  \\textbf{100.00} & 95.83 & 90.62 & 54.17 & 94.01 & 76.30 & 55.21 & 52.08 & 95.83 & 64.58 & 77.86 \\\\\n        Quest (V only) &  \\textbf{100.00} & 96.88 & 93.75 & 72.92 & 95.83 & 83.07 & 56.25 & 53.13 & 96.88 & 65.97 & 81.47\\\\\n\t\\rowcolor{cyan!10}\n\t\\Sys  & \\textbf{100.00}&\\textbf{100.00}&\\textbf{95.83}&\\textbf{83.33}&\\textbf{98.70}&\\textbf{87.76}&\\textbf{69.79}&\\textbf{55.21}&\\textbf{97.50}&\\textbf{68.06}&\\textbf{85.62}\\\\\n\n\\midrule\n\\textit{Llama-3.1-8B} & 100.00 & 100.00 & 98.96 & 91.67 & 98.96 & 95.31 & 82.29 & 47.92 & 68.96 & 71.18 &85.53\\\\\n         Loki & 68.75 & 32.29 & 32.29 & 20.83 & 42.71 & 28.65 & 41.67 & 33.33 & 24.79 & 29.86&35.52\\\\\n         Loki (V only) & 95.83 & 36.46 & 57.29 & 62.50 & 77.86 & 70.83 & 69.79 & 39.58 & 35.21 & 37.50&58.29\\\\\n\tQuest &  \\textbf{100.00} & 98.96 & 97.92 & 34.38 & 93.49 & 88.54 & 70.83 & 44.79 & 65.63 & \\textbf{68.40} & 76.29  \\\\\n        Quest (V only) & \\textbf{100.00} & 98.96 & 98.96 & 56.25 & 95.83 & 90.63 & 76.04 & 46.88 & 66.25 & 67.36&79.72\\\\\n\t\\rowcolor{cyan!10}\n\t\\Sys  & \\textbf{100.00} & \\textbf{100.00} & \\textbf{100.00} & \\textbf{83.33} & \\textbf{97.92} & \\textbf{92.19} & \\textbf{81.25} & \\textbf{48.96} & \\textbf{67.08} & 64.93 &\\textbf{83.57}\\\\\n\n     \\midrule\n        \\textit{Yi-9B-200K}& 100.00& 100.00 & 86.46&62.50&64.58&32.55&44.79&39.58&36.87&89.93 & 65.73\\\\\n         Loki & 34.38 & 2.08& 2.08 & 0.00 & 0.00 & 0.52 & 22.92 & 21.88 & 0.00 & 25.00& 10.89\\\\\n         Loki (V only) & 59.38 & 11.46 & 18.75 & 5.21 & 4.43 & 2.08 & 22.92 & 31.25 & 0.00 & 35.07 & 19.06\\\\\n\tQuest &  \\textbf{100.00} & 98.96 & 79.17 & 26.04 & 56.51& 31.77 & 32.29 & 31.25 & 51.04 & 71.88 &57.89\\\\\n         Quest (V only) &\\textbf{100.00} & \\textbf{100.00} & 80.21 & 45.83 & 59.37 & \\textbf{31.90} & 36.45 & 34.37 & 53.54 & 71.88 & 61.36\\\\\n\t\\rowcolor{cyan!10}\n\t\\Sys  & \\textbf{100.00} & \\textbf{100.00} & \\textbf{82.29} & \\textbf{67.71} & \\textbf{63.28} & 31.51 & \\textbf{43.75}&3\\textbf{8.54} & \\textbf{56.04}&\\textbf{72.22}&\\textbf{65.53}\\\\\n\n  \\bottomrule\n\\end{tabular}\n\\label{tab:ruler}\n\\end{table}\n\n\\section{Empirical Evaluation}\n\\label{sec:evaluation}\nIn this section, we showcase the effectiveness and efficiency of \\Sys. Specifically,\n\n\\begin{itemize}\n\t\\item In \\cref{sec:acc}, we show that \\Sys can reduce the GPU memory footprint of the KV cache by over $6\\times$ without accuracy degradation on a wide range of models and evaluation benchmarks.\n\t\\item In \\cref{sec:eff}, we demonstrate \\Sys can support up to $6\\times$ larger batch sizes and increase the inference throughput by up to $3.04\\times$ without compromising model quality.\n\t\\item In \\cref{sec:ab}, we present extensive ablation studies that validate the effectiveness of each component of \\Sys in optimizing GPU memory usage and enhancing performance.\n\\end{itemize}\n\nAll details (hyperparameters, datasets, etc.), along with additional experiments, are in \\cref{app:exp_detail}.\n\n\\subsection{Accuracy Evaluation}\n\\label{sec:acc}\nWe demonstrate that \\Sys can reduce the GPU memory usage of the KV cache by $6\\times$ while maintaining accuracy on a range of long-context tasks with a minimal sparse KV cache budget.\n\n\\paragraph{Setup.} We choose four widely used long-context models for our evaluation: Llama-3-8B-1M \\citep{gradllama}, GLM-4-9B-1M \\citep{glm2024chatglm}, Llama-3.1-8B \\citep{meta_llama_3_1}, and Yi-9B-200K \\citep{ai2024yi}. We evaluate our approach on three challenging long-context benchmarks: RULER \\citep{hsieh2024ruler}, LongBench \\citep{bai2023longbench}, and Needle In A Haystack \\citep{niah}, covering QA, multi-hop, reasoning, summarization, code completion. Needle In A Haystack is also tested on Phi-3-Mini-128K \\citep{abdin2024phi} and Qwen2-7B-128K \\citep{yang2024qwen2}. We set the chunk size to 8, the rank to 160, and the number of outliers to 48 for \\Sys.\n\n\\paragraph{Baselines.} We include two dynamic sparse attention methods as baselines: Quest \\citep{tang2024quest} and Loki \\citep{singhania2024loki}. For all methods, we retain exact pre-filling and perform dynamic sparse attention during decoding, where the computation cost is set to 1/16 of full attention for selecting sparse KV pairs. We include two variants for each baseline: one where all the KV cache is offloaded, and another where only the value cache is offloaded. The former has similar latency to \\Sys but a smaller sparse budget since \\Sys only needs to fetch the value cache from the CPU. The latter aligns with the same sparse KV cache budget but significantly increases GPU memory usage. The latter one is marked as ``V only'' in the table.\n\n\\begin{table}[t]\n\\centering\n\\caption{Performance of various methods on different models with LongBench \\citep{bai2023longbench} samples exceeding 4K tokens. \\Sys outperforms other methods and maintains the accuracy.}\n\\setlength{\\tabcolsep}{1.2pt} % 调整列间距\n\\small % 缩小整个表格的字体\n\\begin{tabular}{l|rrrrrrrrr|r}\n\\toprule\nMethods &  NarratQA & MultiFQA & HotpotQA & MuSiQue & DuRead & GovRep & SAMSum &PassRetr & LCC & Avg.\\\\ \\midrule\n\\textit{Llama-3-8B-1M} & 18.98 & 41.84 & 36.79 & 21.47 & 31.93 & 34.18 & 35.96 & 81.50 & 56.07 & 39.86\\\\\nLoki & 2.26 & 10.19 & 5.48 & 3.16 & 12.17 & 28.97 & 7.84 & 40.52 & 31.44 & 15.78\\\\\nLoki (V only) & 3.20 & 21.01 & 12.41 & 3.86 & 17.07 & 31.24 & 16.23 & 52.57 & 38.10&21.74 \\\\\nQuest & \\textbf{20.13} & 36.63 & 35.00 & 18.14 & 24.55 & 27.11&35.63&79.00&53.64&36.65\\\\\nQuest (V only) &17.26&39.51&36.78&18.71 &26.41&29.49&35.80&79.50&60.05&38.17\\\\\n\\rowcolor{cyan!10}\n\\Sys & 17.17 & \\textbf{39.73} & \\textbf{38.29} & \\textbf{21.08} & \\textbf{31.77} & \\textbf{31.62} & \\textbf{35.87} & \\textbf{80.00} & \\textbf{63.93} &\\textbf{39.94}\\\\\n\\midrule\n\n\\textit{GLM-4-9B-1M} & 25.44 & 51.09 & 58.67 & 39.61 & 32.04 & 29.97 & 40.31 & 99.00 & 58.02&48.24\\\\\nLoki &  5.82 & 30.60&22.73 & 9.20 & 30.09 & 30.35 & 22.70& 98.92 &40.77 &32.35\\\\\nLoki (V only) & 10.89 & 44.97 & 45.44 & 23.51 & 32.07 & \\textbf{30.56} & 35.34 & \\textbf{99.50} &50.27 & 41.39\\\\\nQuest &23.81 & 44.53 & 56.41 & 35.49 & 23.54 & 21.73 & 37.39 & 87.00 &43.80 &41.52\\\\\nQuest (V only) & 26.00 & 46.32 & 57.54 & 36.42 & 24.58 & 24.52&37.71 & 93.50&46.52&43.68\\\\\n\\rowcolor{cyan!10}\n\\Sys & \\textbf{26.50} & \\textbf{51.31} & \\textbf{59.09} & \\textbf{38.87} & \\textbf{32.92} & 28.54 & \\textbf{38.70} & 96.50 & \\textbf{58.55}&\\textbf{47.89}\\\\\n\n\\midrule\n\\textit{Llama-3.1-8B}  & 31.56 & 55.10 & 57.65 & 29.46 & 35.26 & 34.45 & 29.84 & 100.00 & 67.31 &48.96 \\\\\nLoki&2.31 & 18.89 & 10.64 & 5.47 & 19.30 & 31.16 & 15.91 & 94.88 & 44.60 & 27.02 \\\\\nLoki (V only) & 3.93 & 38.59 & 22.85 & 12.96 & 27.43 & 32.22 & 26.43 & 98.25 &56.11 &35.42\\\\\nQuest &29.70 & 49.04 & 53.96 & 27.18 & 27.16 & 30.43 & 29.85 & 98.50 & 57.35 & 44.80\\\\\nQuest (V only) & 30.02 & 53.97 & 56.39 & 27.06 & 29.06 & 31.65 &30.23 & 99.00 &63.89&46.81\\\\\n\\rowcolor{cyan!10}\n\\Sys & \\textbf{30.93} & \\textbf{55.20} &\\textbf{57.32} & \\textbf{29.13} & \\textbf{31.85} & \\textbf{32.79} & \\textbf{30.40} & \\textbf{99.50} & \\textbf{66.03}& \\textbf{48.13}\\\\\n\n\\midrule\n\n\\textit{Yi-9B-200K}  & 13.88 & 30.02 & 52.46 & 28.20 & 22.29 & 30.25 & 19.08 & 67.00 & 73.50 & 37.41\\\\\nLoki & 1.63 & 2.73& 16.21 & 4.87 & 4.75 & 2.13& 4.95& 0.00 & 38.72 &8.44\\\\\nLoki (V only) & 1.96 & 10.39 & 21.31 & 7.36 & 6.78 & 9.15 & 10.02 & 4.00 & 58.75 & 14.41\\\\\nQuest &10.57 & 25.83 & 46.06 & 23.04 & 17.09 & 17.11 & 20.59 & 50.50 & 67.70 & 30.94\\\\\nQuest (V only) & \\textbf{14.56} & 25.73 & 48.73& 24.73 & 18.44 & 20.83 & 20.08 & 57.50 & 71.13 & 33.53\\\\\n\\rowcolor{cyan!10}\n\\Sys & 12.44 & \\textbf{30.82} &\\textbf{52.43} & \\textbf{27.73} & \\textbf{20.79} & \\textbf{29.83} & \\textbf{20.73} & \\textbf{64.00} & \\textbf{72.89}&\\textbf{36.85}\\\\\n\n\\bottomrule\n\\end{tabular}\n\\label{tab:longbench}\n\\end{table}\n\n\\paragraph{RULER.} As shown in \\cref{tab:ruler}, \\Sys demonstrates excellent performance on 128K contexts. With a fixed sparse budget of 1.56\\%, Loki and Quest experience performance degradation. In contrast, \\Sys is more robust and even outperforms original full attention on certain tasks, such as variable tracking. For complex tasks like multi-document QA or multi-key needle retrieval, other methods suffer from significant performance degradation while \\Sys does not.\n\n\\paragraph{LongBench.} On LongBench, we evaluate our method with a range of realistic scenarios, including single-/multi-document question-answering, document summarization, code completion, information retrieval, etc. We only test on samples longer than 4K and set the sparse KV cache budget to 256 for this benchmark since it has shorter inputs compared to RULER. As shown in \\cref{tab:longbench}, \\Sys outperforms other methods consistently and maintains the performance.\n\n\\begin{wrapfigure}[8]{r}{0.35\\textwidth}\n    \\centering\n\t\\raisebox{-55pt}[\\dimexpr\\height-5\\baselineskip\\relax]{%\n\t        \\includegraphics[width=0.35\\textwidth]{imgs/needle_Llama-3-8B-Instruct-1M_ours_16K_1000K.pdf}%\n\t    }%\n    \\caption{Needle In A Haystack.}\n    \\label{fig:needle}\n\\end{wrapfigure}\n\n\\paragraph{Needle In A Haystack.} On the Needle In A Haystack dataset, as shown in \\cref{fig:needle}, \\Sys shows the ability to process information at different positions across various context windows, ranging from 16K to 1M tokens. More experiments on a range of models can be found in \\cref{appen:niah}.\n\n\\paragraph{Integrate with Efficient Pre-filling Methods.} We also combined \\Sys with a state-of-the-art efficient pre-filling method MInference \\citep{jiang2024minference}. As shown in \\cref{tab:minference}, following the setting of MInference, we tested it on RULER with contexts scaling from 8K to 256K. This demonstrates that our method is compatible with pre-filling acceleration techniques. For some certain context length settings, we even see a slight performance improvement.\n\n\\begin{table}[h]\n\\centering\n\\caption{Performance of different methods on RULER \\citep{hsieh2024ruler} using MInference \\citep{jiang2024minference} in the pre-filling stage. \\Sys is compatible with MInference.}\n\\begin{tabular}{l|cccccc|c}\n\\toprule\nMethods & 8K& 16K& 32K & 64K & 128K& 256K& Avg.\\\\ \\midrule\n\tLlama-3-8B-1M w/ MInference &89.92&88.02&82.81&\\textbf{78.45}&78.12&\\textbf{74.57}&81.98 \\\\\n\t\\Sys  w/ MInference  &\\textbf{90.47}&\\textbf{88.12}&\\textbf{83.28}&77.71&\\textbf{78.32}& 74.31&\\textbf{82.04} \\\\\n\t\n  \\bottomrule\n\\end{tabular}\n\\label{tab:minference}\n\\end{table}\n\n\\begin{wrapfigure}[10]{r}{0pt}\n    \\centering\n\t\\raisebox{-45pt}[\\dimexpr\\height-5\\baselineskip\\relax]{%\n\t        \\includegraphics[scale=0.36]{imgs/multiniah.pdf}%\n\t    }%\n    \\caption{Multi-turn NIAH.}\n    \\label{fig:multi-turn}\n\\end{wrapfigure}\n\\paragraph{Multi-turn Conversation Capability.} As the input context length scales, pre-filling becomes quite costly due to the quadratic growing computation time for attention, which means multi-turn conversation capability is important. To simulate multi-turn conversations, we challenged \\Sys with a multi-turn needle retrieval task (Multi-turn NIAH). We also test two eviction-based methods in \\cref{fig:multi-turn}, including SnapKV \\citep{li2024snapkv} and StreamingLLM \\citep{xiao2023efficient}. The performance of SnapKV drops significantly from the second round due to the required context information being different from the first round. Since SnapKV inevitably evicted tokens based on the first-turn conversation, it cannot successfully retrieve related information for future queries. In contrast, \\Sys can maintain accuracy in the multi-turn conversation setting.\n\n\\subsection{Efficiency Evaluation}\n\\label{sec:eff}\nTo demonstrate the efficiency of \\Sys, we deploy it into real-world large batch serving scenarios. By measuring the throughput during decoding across different models on A100, we show that \\Sys can support up to $6\\times$ larger batch sizes and boost throughput by up to 3.04$\\times$.\n\n\\paragraph{Baselines.} The baseline selects the largest batch size that can fit entirely on the GPU with full attention. We also include results for the same batch size of \\Sys and the infinite batch size, assuming infinite GPU memory capabilities\\footnote{For the equivalent \\Sys batch size, we evaluate a single Transformer block with FlashAttention and then project the number to the entire model. For the infinite batch size, we leverage A100's theoretical memory bandwidth (2 TB/s) for attention computations.}. We set the sparse budget to 1.56\\% for \\Sys.\n\n\\paragraph{Results.} As shown in \\cref{tab:e2e}, \\Sys demonstrates significant throughput improvements for various models on an A100, surpassing even those with infinite GPU memory. Notably, \\Sys supports batch sizes up to $6\\times$ larger and enhances throughput by up to $3.04\\times$ compared to full attention, even surpassing infinite batch size assuming infinite GPU memory. While the gains for GLM-4-9B-1M and Yi-9B-200K are slightly lower, the improvements still reach up to $2.56\\times$ and $2.66\\times$ respectively, highlighting \\Sys's adaptability even with fewer KV heads.\n\n\\begin{table}[h]\n\\centering\n\\caption{Generation throughput (tokens/s) on an A100. The gray text in brackets denotes batch size.}\n\\setlength{\\tabcolsep}{5.1pt} % 调整列间距\n\\begin{tabular}{lrrrc|r}\n\\toprule\nModel &  Context & Full Attention & \\Sys & Gain & Full Attention (Inf)\\\\ \n\t\\midrule\n\tLlama-3-8B-1M  & 60K & 160.62 \\color{gray}{(8)} & \\textbf{455.14 \\color{gray}{(48)}} & 2.83$\\times$ & 168.72 \\color{gray}{(48)} \\color{black}{/} 273.07 \\color{gray}{(Inf)} \\\\\n\t\t\t(8 KV heads)\t  & 122K &  80.77 \\color{gray}{(4)} & \\textbf{239.51 \\color{gray}{(24)}} & 2.97$\\times$ & 83.05 \\color{gray}{(24)} \\color{black}{/} 134.30 \\color{gray}{(Inf)}\\\\\n\t\t\t\t  & 244K & 40.37 \\color{gray}{(2)}  & \\textbf{119.01 \\color{gray}{(12)}} & 2.95$\\times$ & 52.00 \\color{gray}{(12)} \\color{black}{/} 67.15 \\color{gray}{(Inf)}\\\\\n\t\\midrule\n\tLlama-3.1-8B & 60K & 160.93 \\color{gray}{(8)} & \\textbf{472.77 \\color{gray}{(48)}} & 2.94$\\times$&168.72 \\color{gray}{(48)} \\color{black}{/} 273.07 \\color{gray}{(Inf)}\\\\\n\t\t\t(8 KV heads)\t& 122K & 80.78 \\color{gray}{(4)} & \\textbf{245.90 \\color{gray}{(24)}} & 3.04$\\times$ & 83.05 \\color{gray}{(24)} \\color{black}{/} 134.30 \\color{gray}{(Inf)}\\\\\n\t\\midrule\n\tGLM-4-9B-1M & 60K & 241.05 \\color{gray}{(12)}  & \\textbf{615.89 \\color{gray}{(50)}} & 2.56$\\times$& 266.24 \\color{gray}{(50)} \\color{black}{/}  436.91 \\color{gray}{(Inf)}\\\\\n\t\t(4 KV heads)\t\t& 122K &122.67 \\color{gray}{(6)} & \\textbf{293.40 \\color{gray}{(25)}} & 2.39$\\times$ & 158.83 \\color{gray}{(25)}  \\color{black}{/} 214.87 \\color{gray}{(Inf)}\\\\\n\t\t\t\t& 244K & 61.13 \\color{gray}{(3)} & \\textbf{136.51 \\color{gray}{(12)}}& 2.23$\\times$ & 78.84 \\color{gray}{(12)} \\color{black}{/} 107.44 \\color{gray}{(Inf)}\\\\\n\t\\midrule\n\tYi-9B-200K & 60K & 204.81 \\color{gray}{(10)} & \\textbf{544.36 \\color{gray}{(42)}} & 2.66$\\times$ & 271.21 \\color{gray}{(42)} \\color{black}{/}  364.09 \\color{gray}{(Inf)}\\\\\n\t\t(4 KV heads)\t\t & 122K & 101.44 \\color{gray}{(5)} & \\textbf{260.03 \\color{gray}{(21)}} & 2.56$\\times$ & 133.53 \\color{gray}{(21)}  \\color{black}{/} 179.06 \\color{gray}{(Inf)}\\\\\n\t\t\t\t& 244K &46.74 \\color{gray}{(2)} & \\textbf{118.55 \\color{gray}{(10)}} & 2.54$\\times$ & 65.79 \\color{gray}{(10)} \\color{black}{/} 89.53 \\color{gray}{(Inf)}\\\\\n  \\bottomrule\n\\end{tabular}\n\\label{tab:e2e}\n\\end{table}\n\n\\subsection{Ablation Results}\n\\label{sec:ab}\nWe present extensive ablation studies of \\Sys, focusing on three key points:  (1) sparse KV cache budget variations, (2) chunk size selections, and (3) pre-RoPE key cache rank choices.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{imgs/ablation_budget.pdf}\n    \\caption{Comparison results between the models with full cache, our \\Sys, and Quest.}\n    \\label{fig:ablation_budget}\n\\end{figure}\n\n\\begin{figure}[t]\n    \\centering\n   \\begin{subfigure}[b]{0.32\\textwidth}\n        \\includegraphics[width=\\linewidth]{imgs/chunk_acc.pdf}\n        \\caption{}\n        \\label{fig:chunk_acc}\n   \\end{subfigure}\n   \\hfill\n    \\begin{subfigure}[b]{0.32\\textwidth}\n        \\includegraphics[width=\\linewidth]{imgs/chunk_hitrate.pdf}\n        \\caption{}\n        \\label{fig:chunk_hitrate}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[b]{0.32\\textwidth}\n        \\includegraphics[width=\\linewidth]{imgs/rank.pdf}\n        \\caption{}\n        \\label{fig:ablation_rank}\n   \\end{subfigure}\n    \\caption{(a) Impact of chunk size on batch size and accuracy. (b) Minimal effect of chunk size on hit rate. (c) Accuracy trends across different ranks with Llama-3-8B-1M on different tasks.}\n    \\label{fig:observation}\n\\end{figure}\n\n\\paragraph{Sparse KV Cache Budget.} We examine \\Sys's performance across various tasks with different sparse budgets, as illustrated in \\cref{fig:ablation_budget}. \\Sys consistently surpasses Quest under the same sparse budgets and achieves higher throughput. On most tasks, it maintains accuracy with just a 1.56\\% sparse budget compared to full attention and even improves slightly on some tasks.\n\n\\paragraph{Chunk Size.} As shown in \\cref{fig:chunk_acc}, increasing the chunk size allows for larger batch sizes. However, accuracy declines when the chunk size exceeds eight. Meanwhile, the chunk size choice has minimal impact on the chunk hit rate, which remains around 60\\%, as illustrated in \\cref{fig:chunk_hitrate}.\n\n\\paragraph{Rank of Pre-RoPE Keys.} We assess \\Sys's performance across various tasks using different ranks for pre-RoPE keys. As illustrated in \\cref{fig:ablation_rank}, accuracy increases with the rank up to approximately 160, after which it stabilizes near full-rank performance. Interestingly, the trends vary across tasks, and in some cases, low-rank approximations achieve better performance.\n\n\\section{Conclusion}\nWe present \\Sys, a high-throughput inference system for long-context LLM inference. \\Sys optimizes GPU memory usage through the low-rank key cache and offloaded value cache, allowing for larger batch sizes. It reduces decoding overhead by accurate sparse attention, boosting throughput while maintaining accuracy. Our empirical experiments demonstrate \\Sys can support up to $6\\times$ larger batch sizes and enhance throughput by up to $3.04\\times$ on an A100 across various long-context models, including Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, and Yi-9B-200K. \\Sys holds great promise for improving long-context LLM inference.\n\n\\newpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2406.02542v2.tex",
        "arXiv-2406.10774v2.tex",
        "arXiv-2410.21465v1.tex"
    ],
    "group_id": "group_29",
    "response": "### Title: Efficient Sparse Attention Methods for Large Language Models (LLMs)\n\n### Introduction\nLarge Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment for efficient inference presents significant challenges, particularly with long sequence lengths. The self-attention mechanism, a core component of LLMs, is computationally expensive and memory-intensive, contributing to a substantial portion of the overall computational burden. This has spurred research into approximating self-attention to reduce costs while maintaining model efficacy. The field has seen various approaches, including quantization and token pruning, to alleviate these challenges. However, these methods often introduce trade-offs between computational efficiency and model accuracy. This summary explores three recent research papers that propose novel sparse attention methods: Loki, Quest, and ShadowKV. Each paper contributes unique insights and methodologies to address the computational and memory constraints of LLM inference, aiming to enhance efficiency and throughput without compromising model performance.\n\n### Main Content of Each Paper\n\n#### Loki: Low-Rank Keys for Efficient Sparse Attention\nLoki introduces a novel sparse attention method by analyzing the dimensionality of key vectors in the attention mechanism of LLMs. The authors observe that key vectors consistently occupy a significantly lower-dimensional space than their full attention head dimension, across different models and datasets. Based on this, they propose a method that leverages PCA (Principal Component Analysis) to reduce the dimensionality of key vectors. During inference, they compute approximate attention scores in the reduced dimension space and select the top-$k$ most relevant tokens, which are then used to compute the final attention scores in the full dimension. This approach reduces data movement and computation costs while maintaining model accuracy. The authors also develop optimized kernels for this method in Triton, achieving up to a 45% speedup over standard attention implementations for Llama2-13B. The average degradation in model accuracy across multiple benchmarks and models is only 6.8%.\n\n#### Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference\nQuest focuses on query-aware sparsity to optimize long-context LLM inference. The authors observe that the criticality of tokens in the KV cache (Key-Value cache) varies based on the current query, and propose a method that dynamically selects critical tokens for attention based on the query vector. Quest segments the KV cache into pages and uses metadata (maximum and minimum key values) to estimate the criticality of each page. During inference, it only loads the top-$K$ critical pages, significantly reducing memory movement and improving efficiency. The method is designed to be compatible with existing quantization mechanisms, further enhancing its practical applicability. Quest demonstrates up to a 45% reduction in self-attention latency and a 1.74-2.23 times speedup in end-to-end inference latency, with negligible accuracy loss.\n\n#### ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference\nShadowKV proposes a system that optimizes GPU memory usage by offloading the value cache to the CPU while retaining low-rank projections of the key cache on the GPU. The system leverages the low-rank nature of pre-Rotary Position Embedding (RoPE) keys and the spatial locality of post-RoPE keys to efficiently reconstruct the necessary KV pairs during decoding. By storing only landmarks (mean values of chunks) and a small number of outlier chunks, ShadowKV significantly reduces the memory footprint of the KV cache. This method allows for up to 6 times larger batch sizes and boosts throughput by up to 3.04 times on an A100 GPU, without sacrificing accuracy. ShadowKV also integrates well with efficient pre-filling methods, further enhancing its performance.\n\n### Commonalities and Innovations\nAll three papers address the computational and memory challenges of LLM inference, particularly for long sequence lengths. They share a common goal of reducing the memory footprint and computation costs of the self-attention mechanism while maintaining model accuracy. However, each paper introduces unique innovations:\n\n- **Loki** leverages the low-dimensional structure of key vectors, using PCA to reduce the dimensionality of keys and compute approximate attention scores. This method avoids permanent token deletion from the KV cache, ensuring that no information is lost.\n- **Quest** focuses on query-aware sparsity, dynamically estimating the criticality of tokens based on the current query vector. This approach allows for efficient selection of critical KV pairs, reducing memory movement and improving inference speed.\n- **ShadowKV** combines low-rank key projections and value cache offloading to the CPU, optimizing memory usage and enabling larger batch sizes. It also uses landmarks and outlier chunks to accurately reconstruct KV pairs during decoding, reducing the overhead of fetching data from the CPU.\n\n### Comparison of Results\nThe three papers present different approaches to optimizing LLM inference, and their results highlight the effectiveness of each method:\n\n- **Loki** achieves a 45% speedup in attention computation over standard implementations, with an average accuracy degradation of only 6.8%. This method is particularly effective in reducing the computational complexity of self-attention without permanent token deletion.\n- **Quest** demonstrates up to a 45% reduction in self-attention latency and a 1.74-2.23 times speedup in end-to-end inference latency. Quest's query-aware sparsity ensures that critical tokens are not missed, maintaining high accuracy even with minimal KV cache budgets.\n- **ShadowKV** supports up to 6 times larger batch sizes and boosts throughput by up to 3.04 times on an A100 GPU. It achieves this by offloading the value cache to the CPU and using low-rank key projections, reducing the memory footprint without accuracy degradation.\n\n### Conclusion\nThe three papers present innovative approaches to optimizing LLM inference, each addressing the challenges of long sequence lengths and high computational costs. Loki leverages the low-dimensional structure of key vectors, Quest employs query-aware sparsity to dynamically select critical tokens, and ShadowKV optimizes memory usage by offloading value caches to the CPU. These methods collectively demonstrate significant improvements in computational efficiency and throughput, with minimal impact on model accuracy. Future research could explore integrating these methods with other optimization techniques, such as quantization, to further enhance performance. Additionally, investigating the variability of key vector dimensionality across different layers and models could provide deeper insights into the intrinsic low-rank structures and lead to more sophisticated sparsity policies."
}