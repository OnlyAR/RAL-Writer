{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{AudioLDM: Text-to-Audio Generation with Latent Diffusion Models}\n\n\\begin{document}\n\n\\twocolumn[\n\n\\icmltitle{AudioLDM: Text-to-Audio Generation with Latent Diffusion Models}\n\n\\icmlsetsymbol{equal}{*}\n\n\\begin{icmlauthorlist}\n\\icmlauthor{Haohe Liu}{equal,su}\n\\icmlauthor{Zehua Chen}{equal,im}\n\\icmlauthor{Yi Yuan}{su}\n\\icmlauthor{Xinhao Mei}{su} \n\\icmlauthor{Xubo Liu}{su}\n\\icmlauthor{Danilo Mandic}{im} \\\\\n\\icmlauthor{Wenwu Wang}{su} \n\\icmlauthor{Mark D. Plumbley}{su}\n\n\\end{icmlauthorlist}\n\n\\icmlaffiliation{su}{Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, UK}\n\\icmlaffiliation{im}{Department of Electrical and Electronic Engineering, Imperial College London, London, UK}\n\\icmlcorrespondingauthor{Haohe Liu}{haohe.liu@surrey.ac.uk}\n\n\\icmlkeywords{Machine Learning, ICML}\n\n\\vskip 0.3in\n]\n\n\\printAffiliationsAndNotice{\\icmlEqualContribution} % otherwise use the standard text.\n\n\\begin{abstract}\n\nText-to-audio (TTA) systems have recently gained attention for their ability to synthesize general audio based on text descriptions.\nHowever, previous studies in TTA have limited generation quality with high computational costs.\nIn this study, we propose AudioLDM, a TTA system that is built on a latent space to learn continuous audio representations from contrastive language-audio pretraining (CLAP) embeddings.\nThe pretrained CLAP models enable us to train LDMs with audio embeddings while providing text embeddings as the condition during sampling. \nBy learning the latent representations of audio signals without modelling the cross-modal relationship, AudioLDM improves both generation quality and computational efficiency. \nTrained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance compared to other open-sourced systems, measured by both objective and subjective metrics. AudioLDM is also the first TTA system that enables various text-guided audio manipulations~(e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available at \\url{https://audioldm.github.io}.\n\n\\end{abstract}\n\n\\section{Introduction}\n\\label{Introduction}\n\nGenerating sound effects, music, or speech according to personalized requirements is important for applications such as augmented and virtual reality, game development, and video editing. Traditionally, audio generation has been achieved through signal processing techniques~\\cite{andresen1979new, karplus1983digital}. In recent years, generative models~\\cite{oord2016wavenet, DDPM, SGM, tan2022naturalspeech}, either unconditional or conditioned on other modalities~\\cite{kreuk2022audiogen, zelaszczyk2022audio}, have revolutionized this task. Previous studies primarily worked on the label-to-sound setting with a small set of labels~\\cite{liu2021conditional,pascual2022full} such as the ten sound classes in the UrbanSound8K dataset~\\cite{salamon2014dataset}.\nIn comparison, natural language is considerably more flexible than labels as they can include fine-grained descriptions of audio signals, such as pitch, acoustic environment, and temporal order. \nThe task of generating audio prompted with natural language descriptions is known as \\textit{text-to-audio}~(TTA) generation.\n\nTTA systems are designed to generate a wide range of high-dimensional audio signals. To efficiently model the data, we adopt a similar approach as DiffSound~\\cite{yang2022diffsound} by employing a learned discrete representation to efficiently model high-dimensional audio signals. We also draw inspiration from the recent advancements in autoregressive modelling of discrete representation learnt on the waveform, such as AudioGen~\\cite{kreuk2022audiogen}, which has surpassed the capabilities of DiffSound. Building on the success of StableDiffusion~\\cite{rombach2022high}, which uses latent diffusion models (LDMs) for high-quality image generation, we extend previous TTA approaches to continuous latent representations, instead of learning discrete representations. Additionally, as audio manipulations, such as style transfer~\\cite{engel2020ddsp, pascual2022full}, are desired for some applications such as games, \nwe explore and achieve various zero-shot text-guided audio manipulations with LDMs, which have not been demonstrated before.\n\n\\begin{figure*}\n    \\centerline{\n    \\includegraphics[width=\\linewidth]{pics/main_graph_v2.pdf}}\n    \\caption{Overview of the AudioLDM system for text-to-audio generation~(a). During training, latent diffusion models~(LDMs) are conditioned on an audio embedding $\\boldsymbol{E}^{x}$ and trained in a continuous space $\\boldsymbol{z}_{0}$ learned by VAE. The sampling process uses text embedding $\\boldsymbol{E}^{y}$ as the condition. Given a pretrained LDM, zero-shot audio inpainting~(b) and style transfer~(c) are realized in the reverse diffusion process of LDM. The block \\textit{Forward Diffusion} denotes the process that corrupt data with gaussian noise~(see Equation~\\ref{forwardprocess}).}\n    \\label{fig:overalldesign}\n\\end{figure*}\n\nFor previous TTA works, a potential limitation for generation quality is the requirement of large-scale high-quality audio-text data pairs, which are usually not readily available, and where they are available, are  \nof limited quality and quantity~\\cite{liu2022separate}. To better utilize the low-quality data, several methods for text preprocessing have been proposed~\\cite{kreuk2022audiogen, yang2022diffsound}. However, these preprocessing steps limit generation performances by overlooking the relations of sound events~\n(e.g., \\textit{a dog is barking at the bark} is transformed into \\textit{dog bark park}).\nBy comparison, our proposed method only requires audio data for generative model training, circumvents the challenge of text preprocessing, and performs better than using audio-text paired data, as we will discuss later.  \n\nIn this work, we present a TTA system, AudioLDM, which achieves high generation quality with continuous LDMs, with good computational efficiency and enables text-conditional audio manipulations.\nThe overview of AudioLDM design for TTA generation and text-guided audio manipulation is shown in Figure~\\ref{fig:overalldesign}. \nSpecifically, AudioLDM learns to generate the representation in a latent space encoded by a mel-spectrogram-based variational auto-encoder~(VAE). An LDM conditioned on a contrastive language-audio pretraining~(CLAP) embedding is developed for VAE latent generation. By leveraging the audio-text-aligned embedding space in CLAP, we remove the requirement for paired audio-text data during training LDM, as the condition for VAE latent generation can directly come from the audio itself. We demonstrate that training an LDM with audio only can be even better than training with audio-text data pairs. The proposed AudioLDM achieves leading TTA performance on the AudioCaps dataset with a Freshet distance~(FD) of $23.31$, outperforming the DiffSound baseline~(FD of $47.68$) by a large margin. Our system also enables zero-shot audio manipulations in the sampling process. In summary, our contributions are as follows:\n\\begin{list}{\\labelitemi}{\\leftmargin=1em}\n    \\setlength{\\itemsep}{1pt}\n    \\setlength{\\parskip}{0pt}\n    \\item We demonstrate the first attempt to develop a continuous LDM for TTA generation. Our AudioLDM method outperforms existing methods in both subjective evaluation and objective metrics.\n    \\item We utilize CLAP embeddings to enable TTA generation without using language-audio pairs to train LDMs. \n    \\item We experimentally show that using audio only data in LDM training can obtain a high-quality and computationally efficient TTA system. \n    \\item We show that our proposed TTA system can perform text-guided audio styles manipulation, such as audio style transfer, super-resolution, and inpainting, without fine-tuning the model on a specific task.\n\\end{list}\n\n    \n    \n\n\\section{Related Work}\n\\label{Background}\n\n\\label{TTA}\n\n\\textbf{Text-to-Audio Generation}~has gained a lot of attention recently. Two works~\\cite{yang2022diffsound,kreuk2022audiogen} explore how to learn audio representations in a discrete space given a natural language description, and then decode the representations to the audio waveform. Since both works require audio-text paired data for training the latent generation model, they have both proposed methods to address the issues of low quality and scarcity of paired data.\n\nDiffSound~\\cite{yang2022diffsound} consists of a text encoder, a decoder, a vector-quantized variational autoencoder~(VQ-VAE), and a vocoder. To alleviate the scarcity of audio-text paired data, they propose a mask-based text generation strategy~(MBTG) for generating text descriptions from audio labels. For example, the label \\textit{dog bark, a man speaking} will be represented as \\textit{[M] [M] dog bark [M] man speaking [M]}, where \\textit{[M]} represent the mask token. However, the text generated by MBTG still only includes the label information, which might potentially limit model performance. \n\nAudioGen~\\cite{kreuk2022audiogen} uses a Transformer-based decoder to learn to generate the target discrete tokens that are directly compressed from the waveform. AudioGen is trained on $10$ datasets and proposes data augmentation methods to enhance the diversity of training data. When creating the language-audio pairs, they pre-process the language descriptions to labels to better match the class-label annotation distribution and simplify the task. For example, \nthe text description \\textit{a dog is barking at the park} is transformed to \\textit{dog bark park}. For data augmentation, they mix audio samples according to various signal-to-noise ratios and concatenate the transformed language descriptions. This means that the detailed text descriptions showing the spatial and temporal relationships are discarded.\n\n\\textbf{Diffusion Models}~\\cite{DDPM, SGM} have achieved state-of-the-art sample quality in tasks such as image generation~\\cite{DiffusionBeatsGANs, DALLE2, Imagen}, image restoration~\\cite{ISRIR}, speech generation~\\cite{WaveGrad, DiffWave, leng2022binauralgrad}, and video generation~\\cite{MakeAVideo, ImagenVideo}. For speech or audio synthesis, diffusion models have been studied for both mel-spectrogram generation~\\cite{Grad-TTS, ResGrad} and waveform generation~\\cite{BDDM, PriorGrad, InferGrad}. \n\nA major concern with diffusion models is that the iterative generation process in a high-dimensional data space will result in a low inference speed. One of the solutions is to employ diffusion models in a small latent space, an approach used, for example, in image generation~\\cite{LSGM, D2C, rombach2022high}. For TTA generation, the audio waveform has redundant information~\\cite{liu2022simple, liu2022learning} that increases modeling complexity and decreases inference speed. To overcome this, DiffSound~\\cite{yang2022diffsound} uses text-conditional discrete diffusion models to generate discrete tokens as a compressed representation of mel-spectrograms. However, the quality of the sound generated by their method is limited. In addition, audio manipulation methods are not explored. \n\n\\section{Text-Conditional Audio Generation}\n\\label{AudioLDM}\n\n\\subsection{Contrastive Language-Audio Pretraining}\n\\label{CLAP}\nText-to-image generation models have shown stunning sample quality by utilizing Contrastive Language-Image Pretraining (CLIP)~\\cite{CLIP} for generating the image prior. Inspired by this, we leverage Contrastive Language-Audio Pretraining~(CLAP)~\\cite{wu2022large} to facilitate TTA generation.   \n\nWe denote audio samples as $x$ and the text description as $y$. A text encoder $f_{\\text{text}}(\\cdot)$ and an audio encoder $f_{\\text{audio}}(\\cdot)$ are used to extract a text embedding $\\boldsymbol{E}^{y}\\in \\mathbb{R}^{L}$ and an audio embedding $\\boldsymbol{E}^{x}\\in \\mathbb{R}^{L}$ respectively, where $L$ is the dimension of CLAP embedding. A recent study \\cite{wu2022large} has explored different architectures for both the text encoder and the audio encoder when training the CLAP model. We follow their result to build an audio encoder based on HTSAT~\\cite{HTSAT}, and built a text encoder based on RoBERTa~\\cite{RoBERTa}. We use a symmetric cross-entropy loss as the training objective~\\cite{CLIP,wu2022large}. For details of the training process and the language-audio datasets see Appendix~\\ref{app:CLAP}. \n\nAfter training the CLAP model, an audio sample $x$ can be transformed into an embedding $\\boldsymbol{E}^{x}$ within an aligned audio and text embedding space. The generalization ability of CLAP model has been demonstrated by various downstream tasks such as the zero-shot audio classification~\\cite{wu2022large}. Then, for unseen language or audio samples, CLAP embeddings also provide cross-modal information.\n\n\\subsection{Conditional Latent Diffusion Models}\n\\label{CLDMs}\n\nThe TTA system can generate an audio sample $\\hat{x}$ given text description $y$. With probabilistic generative model LDMs, we estimate the true conditional data distribution $q(\\boldsymbol{z}_{0}|\\boldsymbol{E}^{y})$ with a model distribution $p_{\\theta}(\\boldsymbol{z}_{0}|\\boldsymbol{E}^{y})$, where $\\boldsymbol{z}_{0}\\in \\mathbb{R}^{C\\times \\frac{T}{r}\\times \\frac{F}{r}}$ is the prior of an audio sample $x$ in the space formed from the compressed representation of the mel-spectrogram $\\boldsymbol{X}\\in \\mathbb{R}^{T\\times F}$, and $\\boldsymbol{E}^{y}$ is the text embedding obtained by pretrained text encoder $f_{\\text{text}}(\\cdot)$ in CLAP. Here, $r$ denotes the compression level, $C$ denotes the channel of the compressed representation, $T$ and $F$ denote the time-frequency dimensions in the mel-spectrogram $\\boldsymbol{X}$. With pretrained CLAP to jointly embed the audio and text information, the audio embedding $\\boldsymbol{E}^{x}$ and the text embedding $\\boldsymbol{E}^{y}$ share a joint cross-modal space. This allows us to provide $\\boldsymbol{E}^{x}$ for training the LDMs, while providing $\\boldsymbol{E}^{y}$ for TTA generation.\n\nDiffusion models~\\cite{DDPM, SGM} consist of two processes: i) a forward process to transform the data distribution into a standard Gaussian distribution with a predefined noise schedule $ 0 < \\beta_{1} < \\dots < \\beta_{n} < \\dots \\beta_{N} < 1$, and ii) a reverse process to gradually generate data samples from the noise according to an inference noise schedule. \n\nIn the forward process, at each time step $n\\in [1,\\dots,N]$, the transition probability is given by:\n\\begin{align}\nq(\\boldsymbol{z}_{n}|\\boldsymbol{z}_{n-1})&=\\mathcal{N}(\\boldsymbol{z}_{n};\\sqrt{1-\\beta_{n}}\\boldsymbol{z}_{n-1},\\beta_{n}\\boldsymbol{I}), \\\\\n\\label{forwardprocess}\nq(\\boldsymbol{z}_{n}|\\boldsymbol{z}_{0})&=\\mathcal  N(\\boldsymbol{z}_{n};\\sqrt{\\bar{\\alpha}_{n}}\\boldsymbol{z}_{0},(1-\\bar{\\alpha}_{n})\\boldsymbol{\\epsilon}),\n\\end{align} \nwhere $\\boldsymbol{\\epsilon}\\sim\\mathcal N(\\boldsymbol{0},\\boldsymbol{I})$ denotes injected noise, $\\alpha_{n}$ is a reparameterization of $1-\\beta_{n}$ and $\\bar{\\alpha}_{n}:=\\prod_{s=1}^{n}\\alpha_{s}$ represents the noise level at each step. \nAt the final time step $N$, $\\boldsymbol{z}_{N}\\sim\\mathcal N(\\boldsymbol{0},\\boldsymbol{I})$ has a standard isotropic Gaussian distribution.\nFor model optimization, we employ the reweighted noise estimation training objective \\cite{DDPM,DiffWave,rombach2022high}:\n\\begin{align}\n\\label{trainingobjective}\nL_{n}(\\theta)=\\mathbb{E}_{\\boldsymbol{z}_{0},\\boldsymbol{\\epsilon},n}\\left \\| \\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_{\\theta}(\\boldsymbol{z}_{n},n,\\boldsymbol{E}^{x}) \\right\\|^2_{2},\n\\end{align}\nwhere $\\boldsymbol{E}^{x}$ is the embedding of the audio waveform $x$ produced by the pretrained audio encoder $f_{\\text{audio}}(\\cdot)$ in CLAP. In the reverse process, starting from Gaussian noise distribution $p(\\boldsymbol{z}_{N})\\sim\\mathcal N(\\boldsymbol{0},\\boldsymbol{I})$ and the text embedding $\\boldsymbol{E}^{y}$, a denoising process conditioned on $\\boldsymbol{E}^{y}$ gradually generates the audio prior $\\boldsymbol{z}_{0}$ by the following process:\n\\begin{align}\np_{\\theta}(\\boldsymbol{z}_{0:N}|\\boldsymbol{E}^{y})&=p(\\boldsymbol{z}_{N})\\prod_{t=n}^{N}p_{\\theta}(\\boldsymbol{z}_{n-1}|\\boldsymbol{z}_{n},\\boldsymbol{E}^{y}) \\\\\n\\label{singlereversestep}\np_{\\theta}(\\boldsymbol{z}_{n-1}|\\boldsymbol{z}_{n},\\boldsymbol{E}^{y})&=\\mathcal{N}(\\boldsymbol{z}_{n-1};\\boldsymbol{\\mu}_{\\theta}(\\boldsymbol{z}_{n},n,\\boldsymbol{E}^{y}),\\boldsymbol{\\sigma}_{n}^{2}\\boldsymbol{I}).\n\\end{align}\nThe mean and variance are parameterized as~\\cite{DDPM}: \n\\begin{align}\n\\boldsymbol{\\mu}_{\\theta}(\\boldsymbol{z}_{n},n,\\boldsymbol{E}^{y})&=\\frac{1}{\\sqrt{\\alpha_{n}}}(\\boldsymbol{z}_{n}-\\frac{\\beta_{n}}{\\sqrt{1-\\bar{\\alpha}_{n}}}\\boldsymbol{\\epsilon}_{\\theta}(\\boldsymbol{z}_{n},n,\\boldsymbol{E}^{y})) \\\\\n\\boldsymbol{\\sigma}_{n}^{2}&=\\frac{1-\\bar{\\alpha}_{n-1}}{1-\\bar{\\alpha}_{n}}\\beta_{n}\n\\end{align}\nwhere $\\boldsymbol{\\epsilon}_{\\theta}(\\boldsymbol{z}_{n},n,\\boldsymbol{E}^{y})$ is the predicted generation noise, and $\\boldsymbol{\\sigma}_{1}^{2}=\\beta_{1}$. In the training stage, we learn the generation of an audio prior $\\boldsymbol{z}_{0}$ given the cross-modal representation $\\boldsymbol{E}^{x}$ of an audio sample $x$. Then, in TTA generation, we provide the text embeddings $\\boldsymbol{E}^{y}$ to predict the noise $\\boldsymbol{\\epsilon}_{\\theta}(\\boldsymbol{z}_{n},n,\\boldsymbol{E}^{y})$. Built on the CLAP embeddings, our LDM realizes TTA generation without text supervision in the training stage. We provide the details of network architecture in Appendix~\\ref{app:LDMArchitecture}.\n\n\\subsection{Conditioning Augmentation}\n\\label{CA}\n\nIn text-to-image generation, diffusion-based models have demonstrated an ability to capture the fine-grained details between objects and backgrounds~\\cite{DALLE2,Imagen,CompositionalDDPM}. One of the reasons for this success is the large-scale language-image training pairs, such as $400$ million image-text pairs in the LAION dataset~\\cite{schuhmann2021laion}. For TTA generation, it is also desired to generate compositional audio signals whose relationships are consistent with natural language descriptions. However, the scale of available language-audio datasets is not comparable to that of language-image datasets. For data augmentation, AudioGen~\\cite{kreuk2022audiogen} use a mixup strategy which mixes pairs of audio samples and concatenates their respective processed text captions to form new paired data. In our work, as shown in Equation~\\ref{trainingobjective}, we provide the audio only embedding $\\boldsymbol{E}^{x}$ as conditioning information when training LDMs, we can implement data augmentation on audio only signals instead of needing to augment language-audio pairs. Specifically, we perform mixup augmentation on audio $x_{1}$ and $x_{2}$ by:  \n\\begin{align}\n\\label{mixup}\nx_{1,2}=\\lambda x_{1}+(1-\\lambda)x_{2},\n\\end{align}\nwhere $\\lambda$ is a scaling factor varying between $\\left[0, 1\\right]$ sampled from a Beta distribution $\\mathcal B(5,5)$~\\cite{gong2021psla}. Here we do not need to consider the corresponding text description $y_{1,2}$, since text information is not needed during LDM training. By mixing audio pairs, we increase the number of training data pairs $(\\boldsymbol{z}_{0}, \\boldsymbol{E}^{x})$ for LDMs, which makes LDMs robust to CLAP embeddings. In the sampling process, given the text embedding $\\boldsymbol{E}^{y}$ from unseen language descriptions, LDMs are expected to generate the corresponding audio prior $\\boldsymbol{z}_{0}$.\n\n\\subsection{Classifier-free Guidance}\n\\label{CFG}\n\n\\begin{figure}[htbp]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{pics/guidance_scale_compare.pdf}\n    \\caption{The samples generated with different scales of the classifier-free guidance. The text prompt is \\textit{``A cat is meowing\"}.}\n    \\label{fig:my_label}\n\\end{figure}\n\nFor diffusion models, controllable generation can be achieved by introducing guidance at each sampling step. After classifier guidance~\\cite{SGM,ImprovedDDPM}, classifier-free guidance~\\cite{CFG,Glide} (CFG) has been the state-of-the-art technique for guiding diffusion models. During training, we randomly discard our condition $\\boldsymbol{E}^{x}$ with a fixed probability, e.g., $10\\%$ to train both the conditional LDMs $\\boldsymbol{\\epsilon}_{\\theta}(\\boldsymbol{z}_{n},n,\\boldsymbol{E}^{x})$ and the unconditional LDMs $\\boldsymbol{\\epsilon}_{\\theta}(\\boldsymbol{z}_{n},n)$. In generation, we use text embedding $\\boldsymbol{E}^{y}$ as condition and perform sampling with a modified noise estimation $\\hat{\\boldsymbol{\\epsilon}}_{\\theta}(\\boldsymbol{z}_{n},n,\\boldsymbol{E}^{y})$:\n\\begin{align}\n\\label{mixup}\n\\hat{\\boldsymbol{\\epsilon}}_{\\theta}(\\boldsymbol{z}_{n},n,\\boldsymbol{E}^{y}) = w\\boldsymbol{\\epsilon}_{\\theta}(\\boldsymbol{z}_{n},n)+(1-w)\\boldsymbol{\\epsilon}_{\\theta}(\\boldsymbol{z}_{n},n,\\boldsymbol{E}^{y}),\n\\end{align}\nwhere $w$ determines the guidance scale.\nCompared with AudioGen~\\cite{kreuk2022audiogen}, we have two differences. First, they leverage CFG on a transformer-based auto-regressive model, while our LDMs retain the theoretical formulation behind the CFG~\\cite{CFG}. Second, our text embedding $\\boldsymbol{E}^{y}$ is extracted from unprocessed natural language and therefore enables CFG to make use of the detailed text descriptions as guidance for audio generation. However, AudioGen removed the text details showing spatial or temporal relationships with text preprocessing methods. \n\n\\subsection{Decoder}\n\\label{Decoder}\n\nWe use VAE to compress the mel-spectrogram $\\boldsymbol{X}\\in \\mathbb{R}^{T\\times F}$ into a small latent space $\\boldsymbol{z}\\in \\mathbb{R}^{C\\times \\frac{T}{r}\\times \\frac{F}{r}}$, where $r$ is the compression level of the latent space. Our VAE is composed of an encoder and a decoder with stacked convolutional modules. In the training objective, we adopt a reconstruction loss, an adversarial loss, and a Gaussian constraint loss. We provide the detailed architecture and training methods in Appendix~\\ref{app:VAE}. In the sampling process, the decoder is used to reconstruct the mel-spectrogram $\\boldsymbol{\\hat{X}}$ from the audio prior $\\boldsymbol{\\hat{z}}_{o}$ generated from LDMs. To explore a compression level $r$ that achieves a small latent space for LDMs without sacrificing sample quality, we test a group of values $r\\in \\left[1,2, 4,8,16\\right]$, and take $r\\vequalsignnospace4$ as our default setting because of its high computational efficiency and generation quality. Moreover, as we conduct conditioning augmentation for LDMs, we implement data augmentation with Equation~\\ref{mixup} for VAE as well in order to guarantee the reconstruction quality of generated compositional samples. For vocoder, we employ HiFi-GAN~\\cite{kong2020hifi} to generate the audio sample $\\hat{x}$ from the reconstructed mel-spectrogram $\\boldsymbol{\\hat{X}}$. The training details are shown in Appendix~\\ref{app:HiFi-GAN}.\n\n\\section{Text-Guided Audio Manipulation}\n\\label{AudioLDM}\n\n\\textbf{Style Transfer}\n\\label{AST}\nGiven a source audio sample $x^{src}$, we can calculate its noisy latent representation $\\boldsymbol{z}_{n_{0}}$ with a predefined time step $n_{0}\\leq N$ according to the forward process shown in Equation~\\ref{forwardprocess}. By utilizing $\\boldsymbol{z}_{n_{0}}$ as the starting point of the reverse process of a pretrained AudioLDM model, we enable the manipulation of audio $x^{src}$ with text input $y$ with a shallow reverse process $p_{\\theta}(\\boldsymbol{z}_{0:n_{0}}|\\boldsymbol{E}^{y})$:\n\\begin{align}\n\\label{shallowreverse}\np_{\\theta}(\\boldsymbol{z}_{0:n_{0}}|\\boldsymbol{E}^{y})&=p(\\boldsymbol{z}_{n_{0}})\\prod_{n=1}^{n_{0}}p_{\\theta}(\\boldsymbol{z}_{n-1}|\\boldsymbol{z}_{n},\\boldsymbol{E}^{y}),\n\\end{align}\nwhere $n_{0}$ controls the manipulation results. If we define a $n_{0}\\approx N$, the information provided by source audio will not be retained and the manipulation would be similar to TTA generation. We show the effect of $n_{0}$ in Figure~\\ref{fig:style-transfer-demo}, where larger manipulations can be seen in the setting of $n_{0}=3N/4$.\n\n\\begin{figure}[tbp]\n    \\centering\n    \\includegraphics[width=\\linewidth]{pics/Figure3_23Jan_1.pdf}\n    \\caption{The manipulation result with different starting points $n_0$ of the shallow reverse process. The original signal is \\textit{Trumpet}, and the text prompt for style transfer is \\textit{Children Singing}.}\n    \\label{fig:style-transfer-demo}\n\\end{figure}\n\n\\textbf{Inpainting and Super-Resolution}\n\\label{AI}\nBoth audio inpainting and audio super-resolution refer to generating the missing part given the observed part $x^{ob}$. We explore these tasks by incorporating the observed part in latent representation $\\boldsymbol{z}^{ob}$ into the generated latent representation $\\boldsymbol{z}$. Specifically, in reverse process, starting from $p(\\boldsymbol{z}_{N})\\sim\\mathcal N(\\boldsymbol{0},\\boldsymbol{I})$, after each inference step shown in Equation~\\ref{singlereversestep}, we modify the generated $\\boldsymbol{z}_{n-1}$ with:\n\\begin{align}\n\\label{restoration}\n\\boldsymbol{z}^{\\prime}_{n-1}&=(1-\\boldsymbol{m})\\odot\\boldsymbol{z}_{n-1}+\\boldsymbol{m}\\odot\\boldsymbol{z}^{ob}_{n-1},\n\\end{align}\nwhere $\\boldsymbol{z}^{\\prime}$ is the modified latent representation, $\\boldsymbol{m}\\in \\mathbb{R}^{\\frac{T}{r}\\times \\frac{F}{r}}$ denotes an observation mask in latent space, $\\boldsymbol{z}^{ob}_{n-1}$ is obtained by adding noise on $\\boldsymbol{z}^{ob}$ with the forward process shown by Equation~\\ref{forwardprocess}. \n\nThe values of observation mask $\\boldsymbol{m}$ depend on the observed part of a mel-spectrogram $\\boldsymbol{X}$. As we adopt a convolutional structure in VAE to learn the latent representation $\\boldsymbol{z}$, we can roughly retain the spatial correspondency in mel-spectrogram, as it is shown in Figure~\\ref{fig:demo-spatial-correspondancy} in Appendix~\\ref{app:VAE}. Therefore, if a time-frequency bin $\\boldsymbol{X}_{t,f}$ is observed, we set the observation mask $\\boldsymbol{m}_{\\frac{t}{r},\\frac{f}{r}}$ in latent space as $1$.\nBy using $\\boldsymbol{m}$ to denote the generation part and observation part in $\\boldsymbol{z}$, according to Equation~\\ref{restoration}, we can generate the missing information conditioned on the text prompt with TTA models, while retaining the ground-truth observation $\\boldsymbol{z}^{ob}$.\n\n\\begin{table*}[tbp]\n\\centering\n\\scriptsize\n\\begin{tabular}{ccccc|cccc|cc}\n\\toprule\n    Model    & Text Data & Use CLAP & Params & Duration~(h) & FD~$\\downarrow$  & IS~$\\uparrow$   & KL~$\\downarrow$ & FAD~$\\downarrow$ & OVL~$\\uparrow$ & REL~$\\uparrow$   \\\\\n\\midrule\nGround truth & - & - & - & - & - & - & - & - & $83.61_{\\pm 1.1}$ & $80.11_{\\pm 1.2}$ \\\\\nDiffSound$^{\\dagger}$~\\cite{yang2022diffsound}   & \\cmark & \\xmark           & $400$M  & $5420$ & $47.68$ & $4.01$ & $2.52$ & $7.75$ & $45.00_{\\pm 2.6}$ & $43.83_{\\pm 2.3}$ \\\\\nAudioGen$^{\\dagger}$~\\cite{kreuk2022audiogen}      & \\cmark & \\xmark &$285$M & $8067$  & -    &  -    & $2.09$  & $3.13$ & - & - \\\\\n\\midrule\nAudioLDM-S-Full-RoBERTa   & \\cmark   & \n\\xmark      & $181$M & $145$  & $32.13$  & $4.02$ & $3.25$ & $5.89$ & - & - \\\\\nAudioLDM-S    & \\xmark  & \n\\cmark       & $181$M & $145$  & $29.48$  & $6.90$ & $1.97$ & $2.43$ & $63.41_{\\pm 1.4}$ & $64.83_{\\pm 0.9}$ \\\\\nAudioLDM-L    & \\xmark   & \n\\cmark     & $739$M & $145$ & $27.12$  & $7.51$ & $1.86$ & $2.08$ \n & $64.30_{\\pm 1.6}$ & $64.72_{\\pm 1.6}$\\\\\nAudioLDM-S-Full & \\xmark & \n\\cmark   & $181$M & $8886$ & $23.47$  & $7.57$ & $1.98$ & $2.32$ & - & - \\\\\nAudioLDM-L-Full & \\xmark & \n\\cmark  & $739$M & $8886$ & $\\mathbf{23.31}$  & $\\mathbf{8.13}$ & $\\mathbf{1.59}$ & $\\mathbf{1.96}$ & $\\mathbf{65.91}_{\\pm 1.0}$ & $\\mathbf{65.97}_{\\pm 1.6}$ \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{The comparison between AudioLDM and baseline TTA generation models. Evaluation is conducted on AudioCaps test set. The symbol $^{\\dagger}$ marks industry-level computation. DiffSound is trained on $32$ V$100$ GPUs and AudioGen is trained on $64$ A$100$ GPUs, while AudioLDM models are trained on a single GPU, RTX $3090$ or A$100$. The AS and AC stand for AudioSet and AudioCaps datasets respectively. The results of AudioGen are employed from ~\\citep{kreuk2022audiogen} since their implementation has been not publicly available.}\n\\label{tab: AudioCapResults}\n\\end{table*}\n\n\\section{Experiments}\n\\label{Experiments}\n\n\\textbf{Training dataset} The datasets we used in this paper includes AudioSet~(AS)~\\cite{gemmeke2017audio}, AudioCaps~(AC)~\\cite{kim2019audiocaps}, Freesound~(FS)\\footnote{\\url{https://freesound.org/}}, and BBC Sound Effect library~(SFX)\\footnote{\\url{https://sound-effects.bbcrewind.co.uk/search}}. AS is currently the largest audio dataset, with $527$ labels and over $5,000$ hours of audio data. AC is a much smaller dataset with around $49,000$ audio clips and text descriptions. Most of the data in AudioSet and AudioCaps are in-the-wild audio from YouTube, so the quality of the audio is not guaranteed. To expand the dataset, especially with high-quality audio data, we crawl the data from the FreeSound and BBC SFX datasets, which have a wide range of categories such as music, speech, and sound effects. We show our detailed data processing methods and training configuration in Appendix~\\ref{app:TrainingDetails}.\n\n\\textbf{Evaluation dataset} We evaluate the model on both AC and AS. Each audio clip in AC has $5$ text captions. We generate the evaluation set by randomly selecting one of them as text condition. Because the authors of AC intentionally remove the audio with the label related to music~\\cite{kim2019audiocaps}, to evaluate model performance with a wider range of sound, we randomly select $10\\%$ audio samples from AS as another evaluation set. Since AS does not contain text descriptions, we use the concatenation of labels as text descriptions, such as \\textit{Speech, hip hop music, and crowd cheering}. \n\n\\textbf{Evaluation methods} We perform both objective evaluation and human subjective evaluation. The main metrics we use for objective evaluation include \\textbf{frechet distance~(FD)}, \\textbf{inception score~(IS)}, and \\textbf{kullback–leibler~(KL) divergence}. Similar to the frechet inception distance in image generation, the FD in audio indicates the similarity between generated samples and target samples. IS is effective in evaluating both sample quality and diversity. KL is measured at a paired sample level and averaged as the final result. All of these three metrics are built upon a state-of-the-art audio classifier PANNs~\\cite{kong2020panns}. To compare with~\\cite{kreuk2022audiogen}, we also adopt the frechet audio distance~(FAD)~\\cite{kilgour2019frechet}. FAD has a similar idea to FD but it uses VGGish~\\cite{vggish_hershey2017cnn} as a classifier which may have inferior performance than PANNs. To better measure the generation quality, we choose FD as the main evaluation metric in this paper. For subjective evaluation, we recruit six audio professionals to carry on a rating process following~\\cite{kreuk2022audiogen,yang2022diffsound}. Specifically, the generated samples are rated based on i) \\textbf{overall quality~(OVL)}; and ii) \\textbf{relevance to the input text~(REL)} between a scale of $1$ to $100$. We include the details of human evaluation in Appendix~\\ref{app:TrainingDetails}. We open-source our evaluation pipeline to facilitate reproducibility\\footnote{\\url{https://github.com/haoheliu/audioldm_eval}}.\n\n\\textbf{Models} We employ two recently proposed TTA systems, DiffSound~\\cite{yang2022diffsound} and AudioGen~\\cite{kreuk2022audiogen} as our baseline models. DiffSound is trained on AS and AC datasets with around $400$M parameters. AudioGen is trained on AS, AC, and eight other datasets with around $285$M parameters. Since AudioGen has not released publicly available implementation, we reuse the KL and FAD results reported in their paper. We train two AudioLDM models. One is a small model named AudioLDM-S, which has $181$M parameters, and the other is a large model named AudioLDM-L with $739$M parameters. We describe the details of UNet architecture in Appendix~\\ref{app:LDMArchitecture}. To demonstrate the advantage of our method, we simply train these two models only with the AC dataset. Moreover, to explore the effect of the scale of training data, we develop an AudioLDM-L-Full model which is trained on AC, AS, FreeSound, and BBC SFX datasets. \n\n\\subsection{Results}\n\nWe show the main evaluation results on the AC test set in Table~\\ref{tab: AudioCapResults}. Given the single training dataset AC, AudioLDM-S can achieve better generation results than the baseline models on both objective and subjective evaluations, even with smaller model size. By expanding model capacity with AudioLDM-L, we further improve the overall results. Then, by incorporating AS and the two other datasets into training, our model AudioLDM-L-Full achieves the best quality, with an FD of $23.31$. \nAlthough RoBERTa and CLAP have the same text encoder structure, CLAP has an advantage in that it decouples audio-text relationship learning from generative model training. This decoupling is intuitive as CLAP has already modelled the relationship between audio and text by aligning their embedding spaces. On the other hand, AudioLDM-S-Full-RoBERTa, in which the text encoder only represents textual information, requires the model to learn the text-audio relationships while simultaneously learning the audio generation process. Additionally, our CLAP-based method allows for model training using audio-only data. Therefore, using Roberta without pretraining with CLAP may increase the difficulty of training.\n\n\\begin{figure*}[tbp]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{pics/hist.png}\n    \\caption{The histogram of the human evaluation result. The horizontal axis and vertical axis represent the rating score and frequency, respectively. \\textit{OVL} denotes the overall quality of audio files and \\textit{REL} denotes the relation between text and generated audio. Both OVL and REL are scored on a scale of $1$ to $100$. Scores on each evaluation file are averaged among all the raters.}\n    \\label{fig:hist-human-evaluation}\n\\end{figure*}\n\n\\begin{figure}[tbp]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{pics/training_curve.pdf}\n    \\caption{The comparison of various evaluation metrics evaluated in the training process of i) AudioLDM-S trained with text embedding (S-Text+Audio) ii) AudioLDM-S (S-Audio), and iii) AudioLDM-L (L-Audio).}\n    \\label{fig:training-steps-fd-is-kl}\n\\end{figure}\n\nOur human evaluation shows a similar trend as other evaluation metrics. Our proposed methods have OVL and REL of around $64$, outperforming DiffSound with OVL of $45.00$ and REL of $43.83$ by a large margin. On the AudioLDM model size, we notice that the larger model is advantageous for the overall audio qualities. After scaling up the training data, both OVL and REL show significant improvements.\nFigure~\\ref{fig:hist-human-evaluation} shows the score statistic of different models averaged between all the raters. We notice our model is more concentrated on the higher scores compared with DiffSound. Our spam cases, which are randomly selected real recordings, show high scores, indicating the rating result is reliable.\n\nTo perform the evaluation on audio data that could include music, we further evaluate our model on the AS evaluation set. We compare our method with DiffSound and show the results in Table~\\ref{tab: AudioSetResults}. Our three AudioLDM models show a similar trend as they perform on the AC test set. We can outperform the DiffSound baseline by a large margin on all the metrics. \n\n\\begin{table}[htbp]\n\\small\n\\centering\n\\begin{tabular}{ccccc}\n\\toprule\n       Model            & FD~$\\downarrow$   & IS~$\\uparrow$   & KL~$\\downarrow$  \\\\\n\\midrule\nDiffSound          & $50.40$ & $4.19$  & $3.63$ \\\\\nAudioLDM-S      & $28.08$ & $6.78$ & $2.51$  \\\\\nAudioLDM-L      & $27.51$ & $7.18$ & $2.49$ \\\\\nAudioLDM-L-Full & $\\mathbf{24.26}$ & $\\mathbf{7.67}$ & $\\mathbf{2.07}$ \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{The evaluation results on the AudioSet evaluation set.}\n\\label{tab: AudioSetResults}\n\\end{table}\n\n\\textbf{Conditioning Information}\nAs we train LDMs conditioned on the audio embedding $\\boldsymbol{E}^{x}$ but provide the text embedding $\\boldsymbol{E}^{y}$ to LDMs in TTA generation, a natural concern is that if stronger results could be achieved by directly using the text embedding as training condition. We conduct experiments and show the results in Table~\\ref{tab: conditioning-text-audio}. For a fair comparison, we also conduct data augmentation and we adopt the strategy from AudioGen. Specifically, we use the same mixing method for audio pairs shown in Section~\\ref{CA}, and concatenate two text captions as conditioning information. Table~\\ref{tab: conditioning-text-audio} shows by training LDMs on $\\boldsymbol{E}^{x}$, we can achieve better results than training with $\\boldsymbol{E}^{y}$. \n\n\\begin{table}[htbp]\n\\centering\n\\footnotesize\n\\begin{tabular}{cccccc}\n\\toprule\n    Model     & Text & Audio  & FD~$\\downarrow$   & IS~$\\uparrow$   & KL~$\\downarrow$  \\\\\n\\midrule\nAudioLDM-S & \\cmark & \\cmark & $31.26$ & $6.35$ & $2.01$ \\\\\nAudioLDM-S & \\xmark & \\cmark & $\\mathbf{29.48}$  & $\\mathbf{6.90}$ & $\\mathbf{1.97}$ \\\\\nAudioLDM-S-Full & \\cmark & \\cmark & $27.20$ & $7.52$ & $2.38$ \\\\\nAudioLDM-S-Full & \\xmark & \\cmark & $\\mathbf{23.47}$ & $\\mathbf{7.57}$ & $\\mathbf{1.98}$ \\\\\nAudioLDM-L-Full & \\cmark & \\cmark & $25.79$ & $7.95$ & $2.26$ \\\\\nAudioLDM-L-Full & \\xmark & \\cmark & $\\mathbf{23.31}$ & $\\mathbf{8.13}$ & $\\mathbf{1.59}$ \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{The comparison between text embedding and audio embedding as conditioning information on the training of LDMs.}\n\\label{tab: conditioning-text-audio}\n\\end{table}\n\nWe believe the primary reason for the result in Table~\\ref{tab: conditioning-text-audio} is that text embedding cannot represent the generation target as good as audio embedding. \nFirstly, due to the ambiguity and complexity of sound, the text caption is difficult to be accurate and comprehensive. Different human annotators may have different perceptions and descriptions over the same audio, which make training with text-audio pair less stable than with audio only.\nMoreover, some of the captions are at a highly-abstracted level and cannot correctly describe the audio content. For example, there is an audio in the BBC SFX dataset with caption \\textit{Boats: Battleships-5.25 conveyor space}, which is even difficult for humans to imagine how it sounds. This quality of language-audio pairs may hinder model optimization. \nBy comparison, if we use $\\boldsymbol{E}^{x}$ from CLAP latents as a condition, it is extracted directly from the audio signal and is aligned with ideally the best text caption, which enables us to provide strong conditioning information to LDMs without considering the noisy labeled text description.\nFigure~\\ref{fig:training-steps-fd-is-kl} shows sample quality as a function of training progress. We notice that i) training with audio embedding can lead to significantly better results than text embedding throughout the entire training process; and ii) larger models may converge more slowly but can achieve better final performance.\n\n\\textbf{Compression Level} We study the effect of compression level $r$ on generation quality. Table~\\ref{tab: compressionratio} shows the performance comparison with $r\\vequalsignnospace4,8,16$. We observe a decreasing trend with the increase of compression levels. Nevertheless, in the setting of $r\\vequalsignnospace16$ where we compress the $64$-band mel-spectrogram into only $4$ dimensions in the frequency axis, our performance is still on par with AudioGen on KL, and better than DiffSound on all the metrics.\n\n\\begin{table}[htbp]\n\\centering\n\\small\n\\begin{tabular}{ccccc}\n\\toprule\nModel         & $r$ & FD~$\\downarrow$   & IS~$\\uparrow$   & KL~$\\downarrow$   \\\\\n\\midrule\nAudioLDM-S & $4$          & $\\mathbf{29.48}$ & $\\mathbf{6.90}$ & $\\mathbf{1.97}$ \\\\\nAudioLDM-S & $8$          & $33.50$  & $6.13$ & $2.04$ \\\\\nAudioLDM-S & $16$         & $34.32$  & $5.68$ & $2.09$ \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{The effect of the compression level on AudioLDM.}\n\\label{tab: compressionratio}\n\\end{table}\n\nIf we set the compression level as $r\\vequalsignnospace1$, which means we directly generate mel-spectrogram from CLAP embeddings, the training process is difficult to implement on a single RTX $3090$ GPU. Similar results happen on $r\\vequalsignnospace2$. Moreover, the inference speed will be low with $r\\vequalsignnospace1,2$. In our studies, $r\\vequalsignnospace4$ achieves high generation quality while reducing the computational load to a reasonable level. Hence, we use it as the default setting in our experiments.\n\n\\textbf{Text-Guided Audio Manipulation} We show the performance of our text-guided audio manipulation methods on two tasks: super-resolution and inpainting. Specifically, for super-resolution, we upsample the audio signal from $8$~kHz to $16$~kHz sampling rate. For the inpainting task, we remove the audio signal between $2.5$ and $7.5$ seconds and refill this part by inpainting. Since most studies on audio super-resolution work on speech signal~\\cite{liu2021voicefixer, liu2022neural}, we demonstrate our results on both AudioCaps, and a speech dataset VCTK~\\cite{vctk-yamagishi2019cstr}, which is a multi-speaker speech dataset. For super-resolution, we employ two models AudioUNet~\\cite{kuleshov2017audio} and NVSR~\\cite{liu2022neural} as baseline models, and employ log-spectral distance (LSD)~\\cite{heming-towards-sr-wang2021towards} as the evaluation metric for comparison. For the inpainting task, we use FAD as a metric and establish a baseline for this task. \n\\begin{table}[tbp]\n\\centering\n\\small\n\\begin{tabular}{cccc}\n\\toprule\nTask          & \\multicolumn{2}{c}{Super-resolution}  &  Inpainting     \\\\\n\\midrule\nDataset       & AudioCaps            & VCTK                 & AudioCaps            \\\\\n\\midrule\nUnprocessed   &          $2.76$            &      $2.15$                &   $10.86$                   \\\\\n\\citet{kuleshov2017audio}     & -                    & $1.32$                 & -                    \\\\\n\\citet{liu2022neural}          & -                    & $\\mathbf{0.78}$                 & -                    \\\\\nAudioLDM-S &         $1.59$             &     $1.12$                 &   $2.33$                   \\\\\nAudioLDM-L & $\\mathbf{1.43}$ & $0.98$ & $\\mathbf{1.92}$ \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Performance comparisons on zero-shot super-resolution and inpainting, which are evaluated by LSD and FAD, respectively.}\n\\label{tab: audiomanipulation}\n\\end{table}\nTable~\\ref{tab: audiomanipulation} shows that AudioLDM can outperform the strong AudioUNet baseline, but the result is not as good as NVSR~\\cite{liu2022neural}. \nRecall that AudioLDM is a model trained on a diverse set of audio signals, including those with heavy background noise. This can lead to the presence of white noise or other non-speech sound events in the output of our super-resolution process, potentially reducing performance. \nNevertheless, our contribution could open the door to achieving text-guided audio manipulation with the TTA system in a zero-shot way. Further improvements could be expected based on our benchmark results. We provide several samples of our results in Appendix~\\ref{app:demos}. \n\n\\subsection{Ablation Study}\n\nTable~\\ref{tab: ablationstudy} shows the result of our ablation study on AudioLDM-S. By simplifying the attention mechanism in UNet into a one-layer multi-head self-attention~(\\textit{w.~Simple attn}), the performance in each metric will have a notable decrease, which indicates complex attention mechanism is preferred. Also, we notice the widely used balanced sampling strategy~\\cite{gong2021psla, liu2022ontology} in audio classification does not show improvement in TTA~(\\textit{w.~Balance samp}). Conditional augmentation~(see Section~\\ref{CA}) shows improvement in the subjective evaluation, but it does not show improvement in the objective evaluation metrics~(\\textit{w.~Cond aug}). \nThe reason could be that conditioning augmentation generates training data that is not representative of the AudioCaps dataset, resulting in model outputs that are not well-aligned with the evaluation data, ultimately leading to lower metric scores. Nevertheless, conditioning augmentation can improve two subjective metrics and we still recommend using it as a data augmentation technique.\n\n\\begin{table}[htbp]\n\\small\n\\centering\n\\begin{tabular}{lccc|cc}\n\\toprule\n       Setting            & FD$\\downarrow$   & IS$\\uparrow$   & KL$\\downarrow$  & OVL~$\\uparrow$ & REL~$\\uparrow$ \\\\\n\\midrule\nAudioLDM-S      & $\\mathbf{29.48}$ & $\\mathbf{6.90}$ & $\\mathbf{1.97}$ & $63.41$ & $64.83$ \\\\\n\\textit{w.~Simple attn}             & $33.12$ & $6.15$ & $2.09$ & - & - \\\\\n\\textit{w.~Balance samp} &  $34.05$     &   $6.21$   &  $2.16$   & - & - \\\\\n\\textit{w.~Cond aug}           & $31.88$ & $6.25$ & $2.02$ & $\\mathbf{64.49}$ & $\\mathbf{65.01}$  \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{The ablation study on the attention mechanism, the balance sampling technique for training data, and the conditioning augmentation algorithm.}\n\\label{tab: ablationstudy}\n\\end{table}\n\n\\textbf{DDIM Sampling Step} The number of inference steps in the reverse process of DDPMs can directly affect the generation quality \\cite{DDPM,SGM}. Generally, the sample quality can be improved with an increase in the number of sampling steps and computational load at the same time. We explore the effect of the DDIM~\\cite{song2020denoising} sampling steps on our latent diffusion model. Table~\\ref{tab: DDIMsampling} shows that more sampling steps lead to better quality. With enough sampling steps such as $100$, the gain of adding sampling steps becomes less significant. The result of $200$ steps is only slightly better than that of $100$ steps.\n\n\\begin{figure}[tbp]\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{pics/guidance_scale.png}\n    \\caption{The comparison between different classifier-free guidance scales (on the horizontal axis) for the AudioLDM-S model trained on AudioCaps.}\n    \\label{fig:classifier-free-guidance}\n    \\vspace{-2mm}\n\\end{figure}\n\n\\begin{table}[tbp]\n\\centering\n\\small\n\\begin{tabular}{cccccc}\n\\toprule\n  DDIM steps    & $10$    & $25$      & $50$     & $100$     & $200$   \\\\\n\\midrule\nFD    &   $55.84$    &    $42.84$     &    $35.71$    &    $30.17$     &   $\\mathbf{29.48}$    \\\\\nIS    &   $4.21$    &    $5.91$     &   $6.51$     &    $6.85$     & \n $\\mathbf{6.90} $      \\\\ % 6.11\nKL    &   $2.47$    &    $2.12$     &   $2.01$     &    $\\mathbf{1.94}$     & $1.97$      \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Effect of sampling steps of LDMs with a DDIM sampler.}\n\\label{tab: DDIMsampling}\n\\vspace{-6mm}\n\\end{table}\n\n\\textbf{Guidance Scale} \nrepresents a trade-off between conditional generation quality and sample diversity. A suitable guidance scale can improve the consistency between generated samples and conditioning information at an acceptable cost of generation diversity. We show the effect of guidance scale $w$ on TTA in Figure~\\ref{fig:classifier-free-guidance}. When $w=3$, we achieve the best results in both FD and KL, but not in FAD. We suppose the reason is the audio classifier in FAD is not as good as FD, as mentioned in Section~\\ref{Experiments}. In this case, the improvement in the adherence to detailed language description may become misleading information to the classifier in FAD. Considering previous studies report FAD results instead of FD, we set $w=2$ for comparison, but also provide detailed effects of $w$ on FAD, FD, IS, and KL, respectively. \n\n\\textbf{Case Study}~We conduct case study and show the generated results in Appendix~\\ref{app:demos}, including style transfer~(see Figure~\\ref{fig:demo-style-transfer-1}-\\ref{fig:demo-style-transfer-3}), super-resolution~(see Figure~\\ref{fig:demo-super-resolution}), inpainting~(see Figure~\\ref{fig:demo-inpainting}-\\ref{fig:demo-inpainting-prompt}), and text-to-audio generation~(see Figure~\\ref{fig:demo-control-speech}-\\ref{fig:demo-audioset-music}). Specifically, for text-to-audio, we demonstrate the controllability of AudioLDM, including the control of the acoustic environment, material, sound event, pitch, musical genres, and temporal orders.\n\n\\section{Conclusions}\n\\label{Conclusion}\nWe have presented a new method AudioLDM for text-to-audio~(TTA) generation, with contrastive language-audio pretraining~(CLAP) models and latent diffusion models~(LDMs). Our method is advantageous in generation quality, computational efficiency, and audio manipulations. With a single training dataset AudioCaps and a single GPU, AudioLDM achieves SOTA generation quality evaluated by both subjective and objective metrics. Moreover, AudioLDM enables zero-shot text-guided audio style transfer, super-resolution, and inpainting.\n\n\\section{Acknowledgement}\n\\label{sec:ack}\nWe would like to thank James King and Jinhua Liang for the useful discussion on the latent diffusion model. This research was partly supported by the British Broadcasting Corporation Research and Development~(BBC R\\&D), Engineering and Physical Sciences Research Council (EPSRC) Grant EP/T019751/1 ``AI for Sound'', and a PhD scholarship from the Centre for Vision, Speech and Signal Processing (CVSSP), Faculty of Engineering and Physical Science (FEPS), University of Surrey. For the purpose of open access, the authors have applied a Creative Commons Attribution (CC BY) license to any Author Accepted Manuscript version arising.\n\n\\newpage\n\\onecolumn\n\\section*{Appendix}\n\\label{sec:appendix}\n\n\\renewcommand{\\thesubsection}{\\Alph{subsection}}\n\n\\subsection{Contrastive Language-Audio Pretraining}\n\\label{app:CLAP}\n\nWe follow the pipeline of the contrastive language-audio pretraining (CLAP) models proposed by \\cite{wu2022large} to capture the similarity between text and audio, and project them into joint latent space. The training dataset includes the currently largest public dataset LAION-Audio-$630$K, the AudioSet dataset whose text caption is augmented with keyword-to-caption\\footnote{\\url{https://github.com/gagan3012/keytotext}} by T$5$ model~\\cite{T5}, the AudioCaps dataset and the Clotho dataset~\\cite{Clotho}. The LAION-Audio-$630$K dataset contains $633,526$ language-audio pairs and $4325.39$ hours of audio samples. The AudioSet dataset contains $1,912,024$ pairs and $463.48$ hours of audio samples. The AudioCaps dataset contains $49,274$ pairs and $136.87$ hours of audio samples. The Clotho dataset contains $3,839$ pairs and $23.99$ hours of audio samples. These datasets contain various natural sounds, audio effects, music and human activity.\n\nGiven the audio sample $x$ and the text data $y$, we use an audio encoder and a text encoder to extract their embedding $\\boldsymbol{E}^{x}\\in \\mathbb{R}^{L}$ and $\\boldsymbol{E}^{y}\\in \\mathbb{R}^{L}$ respectively, where $L$ is set as $512$. We build the audio encoder based on HTSAT~\\cite{HTSAT} and the text encoder based on RoBERTa~\\cite{RoBERTa}. The symmetric cross-entropy loss used to train these contrastive encoders is:\n\\begin{align}\nL_{s}&=\\frac{1}{2D}\\sum^{D}_{i=1}(l_{1} + l_{2}),\\\\\nl_{1}&=\\log\\frac{\\exp(\\boldsymbol{E}^{x}_{i}\\cdot \\boldsymbol{E}^{y}_{i}/\\tau)}{\\sum^{N}_{i=1}\\exp(\\boldsymbol{E}^{x}_{i}\\cdot \\boldsymbol{E}^{y}_{j}/\\tau)},\\\\\nl_{2}&=\\log\\frac{\\exp(\\boldsymbol{E}^{y}_{i}\\cdot \\boldsymbol{E}^{x}_{i}/\\tau)}{\\sum^{N}_{i=1}\\exp(\\boldsymbol{E}^{y}_{i}\\cdot \\boldsymbol{E}^{x}_{j}/\\tau)}),\n\\end{align}\nwhere $\\tau$ is a learnable temperature parameter and $D$ is the batch size. \n\n\\subsection{Latent Diffusion Model}\n\\label{app:LDMArchitecture}\n\nWe adopt the UNet backbone of StableDiffusion~\\cite{rombach2022high} as the basic architecture of LDM for AudioLDM. As shown in Equation~\\ref{singlereversestep}, the UNet model is conditioned on both the time step $t$ and the CLAP embedding $\\boldsymbol{E}$. We map the time step into a one-dimensional embedding and then concatenate it with $\\boldsymbol{E}$ as conditioning information. Since our condition vector is only one-dimensional, we do not use the cross-attention mechanism in StableDiffusion for conditioning. Instead, we directly use the feature-wise linear modulation layer~\\cite{perez2018film} to merge conditioning information with the feature map of the UNet convolution block. The UNet backbone we use has four encoder blocks, a middle block, and four decoder blocks. With a basic channel number of $c_{u}$, the channel dimensions of encoder blocks are $[c_{u}, 2c_{u}, 3c_{u}, 5c_{u}]$. The channel dimensions of decoder blocks are the reverse of encoder blocks, and the channel of the middle block has $5c_{u}$ dimensions. We add an attention block in the last three encoder blocks and the first three decoder blocks. Specifically, we add two multi-head self-attention layers with a fully-connected layer in the middle as the attention block. The number of heads is determined by dividing the embedding dimension of the attention block with a parameter $c_h$. We set AudioLDM-S and AudioLDM-L with $c_{u}\\vequalsignnospace128, c_h\\vequalsignnospace32$, and $c_{u}\\vequalsignnospace256, c_h\\vequalsignnospace64$, respectively. In the forward process, we use $N=1000$ steps. A linear noise schedule from $\\beta_{1}=0.0015$ to $\\beta_{N}=0.0195$ is used. In sampling, we employ the DDIM~\\cite{song2020denoising} sampler with $200$ sampling steps. For classifier-free guidance, a guidance scale $w$ of $2.0$ is used in Equation \\ref{mixup}.\n\n\\subsection{Variational Autoencoder}\n\\label{app:VAE}\n\nWe compress the mel-spectrogram $\\boldsymbol{X}\\in \\mathbb{R}^{T\\times F}$ of $x$ into a small continuous space $\\boldsymbol{z}\\in \\mathbb{R}^{C\\times \\frac{T}{r}\\times \\frac{F}{r}}$ with a convolutional VAE, where $T$ and $F$ is the time and frequency dimension size respectively, $C$ is the channel number of the latent encoding, and $r$ is the compression level (downsampling ratio) of latent space. Both the encoder $\\mathcal{E}(\\cdot)$ and the decoder $\\mathcal{D}(\\cdot)$ are composed of stacked convolutional modules. In this way, VAE encoder could preserve the spatial correspondancy between mel-spectrogram and latent space, as it is shown in Figure~\\ref{fig:demo-spatial-correspondancy}. Each module is formed by ResNet blocks~\\cite{kong2021decoupling} which are made up of convolutional layers and residual connections. The encoding $\\boldsymbol{z}$ will be evenly split into two parts, $\\boldsymbol{z}_{\\boldsymbol{\\mu}}$ and $\\boldsymbol{z}_{\\boldsymbol{\\sigma}}$, with shape $(\\frac{C}{2}, \\frac{T}{r}, \\frac{F}{r})$, representing the mean and variance of the VAE latent space. The input of the decoder is a stochastic encoding $\\boldsymbol{\\hat{z}}=\\boldsymbol{\\hat{z}}_{\\boldsymbol{\\mu}}+\\boldsymbol{\\hat{z}}_{\\boldsymbol{\\sigma}} \\cdot \\boldsymbol{\\epsilon}, \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, I)$. During generation, the decoder will be used to reconstruct the mel-spectrogram given the generated latent representations.\n\nWe employ three loss functions in our training objective: the mel-spectrogram reconstruction loss, adversarial losses, and a gaussian constraint loss. The reconstruction loss calculates the mean absolute error between the input sample $\\boldsymbol{X}\\in \\mathbb{R}^{T\\times F}$ and the reconstructed mel-spectrogram $\\hat{\\boldsymbol{X}}\\in \\mathbb{R}^{T\\times F}$. The adversarial losses are employed to enhance the reconstruction quality. Specifically, we adopt the PatchGAN~\\citep{isola2017image} as our discriminator, which will divide the input image into small patches and predict whether each patch is real or fake by outputting a matrix of logits. \nThe PatchGAN discriminator is trained to maximize the logits of correctly identifying real patches while minimizing the logits of incorrectly identifying fake patches. We also apply the gaussian constraint on the latent space of VAE. By enforcing a gaussian constraint on the latent space, the VAE is encouraged to learn a continuous, structured latent space, rather than a disorganized one. This can help the VAE to better capture the underlying structure of the data, which can result in more stabilized and accurate reconstructions~\\cite{kingma2013auto}. \n\nWe train our VAE using the Adam optimizer~\\cite{kingma2014adam} with a learning rate of $4.5\\times 10^{-6}$ and a batch size of six. The audio data we use includes AudioSet, AudioCaps, Freesound, and BBC SFX. We perform experiments with three compression-level settings $r\\vequalsignnospace4,8,16$, for which the latent channels are $C=8,16,32$, respectively.  VAEs in all three settings are trained with at least $1.5$M steps on a single NVIDIA RTX 3090 GPU. To stabilize training, we do not apply the adversarial loss in the first $50$K training steps. We apply the mixup~\\cite{kong2020panns} strategy for data augmentation. \n\nTable~\\ref{tab: vae-reconstruct} shows the reconstruction performance of our VAE model with different values of $r$. All three settings achieve comparable metrics score with the \\textit{GT Mel + Vocoder} setting, indicating the autoencoder can perform reliable mel-spectrogram encoding and decoding.\n\n\\begin{table}[htbp]\n\\small\n\\centering\n\\begin{tabular}{cccccc}\n\\toprule\n        Setting        & PSNR$\\uparrow$  & SSIM$\\uparrow$ & FD$\\downarrow$   & IS$\\uparrow$   & KL$\\downarrow$   \\\\\n\\midrule\nGT Mel + Vocoder & $25.41$ & $0.86$ & $8.76$ & $10.71$ & $0.23$ \\\\\n\\midrule\nCompression$_{r=4}$     & $25.38$ & $0.86$ & $9.02$ & $10.67$ & $0.23$ \\\\\nCompression$_{r=8}$     & $25.14$ & $0.84$ & $9.68$ & $10.50$ & $0.25$ \\\\\nCompression$_{r=16}$    & $24.87$ & $0.82$ & $9.90$ & $9.84$ & $0.29$ \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{The objective metrics of VAE reconstruction performance with different compression level $r$ on the AudioSet evaluation set.}\n\\label{tab: vae-reconstruct}\n\\end{table}\n\n\\begin{figure}[htbp]\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{pics/spatial_correspondancy.pdf}\n    \\caption{Visualization of the time-frequency-masked spectrogram and their corresponding VAE latent. This figure shows VAE encoder roughly preserves the spatial correspondancy between the spectrogram and the latent.}\n    \\label{fig:demo-spatial-correspondancy}\n\\end{figure}\n\n\\subsection{Vocoder}\n\\label{app:HiFi-GAN}\n\nIn this work, we employ HiFi-GAN~\\cite{kong2020hifi} as a vocoder, which is widely used for speech waveform generation. It contains two sets of discriminators, a multi-period discriminator, and a multi-scale discriminator, to enhance the perceptual quality. To synthesize the audio waveform, we train it on the AudioSet dataset. For the input samples at the sampling rate of $16,000$Hz, we extract $64$ bands mel-spectrogram. Then we follow the default settings of HiFi-GAN V$1$. The window, FFT, and hop size are set to $1024$, $1024$, and $160$. The $f_{\\text{min}}$ and $f_{\\text{max}}$ are set as $0$ and $8000$. We use the AdamW optimizer with $0.8$ and $0.99$. The learning rate starts from $2\\times 10^{-4}$ and a learning rate decay of $0.999$ is used. We use a batch size of $96$ and train the model with $6$ NVIDIA $3090$ GPUs. We release this pretrained vocoder in our open-source implementation.\n\n\\subsection{Experiment Details}\n\\label{app:TrainingDetails}\n\n\\textbf{Data Processing} The duration of the audio samples in AudioSet and AudioCaps is $10$ seconds, while it is much longer in FreeSound and BBC SFX datasets. To avoid overusing the data from long audio, which usually have repeated sound, we only use the first thirty seconds of the audio in both the FreeSound and BBC SFX datasets and segment them into ten-second long audio files. Finally, we have in total $3,302,553$ ten-seconds audio samples for model training. It should be noted that even if some datasets, e.g., AudioCaps and BBC SFX, have text captions for the audio, we do not utilize them during the training of LDMs. We only use the audio samples for training. We resample all the datasets into $16$kHz sampling rate and mono format, and all samples are padded to $10$ seconds. \n\n\\textbf{Configuration} For each LDM model, we use the compression level $r\\vequalsignnospace4$ as the default setting. Then, we train AudioLDM-S and AudioLDM-L for $0.6$M steps on a \\textbf{single GPU}, NVIDIA RTX $3090$, with the batch size of $5$ and $8$, respectively. The learning rate is set as $3\\times 10^{-5}$. The AudioLDM-L-Full is trained for $1.5$M steps on one NVIDIA A$100$ with a batch size of $8$. The learning rate is $10^{-5}$. For better performance on AudioCaps, we further fine-tune AudioLDM-L-Full on AudioCaps for $0.25$M steps before evaluation. It should be noted that we limit our batch size because of the scarcity of GPU. However, this potentially restricts the performance of AudioLDM models. In comparison, DiffSound uses $32$ NVIDIA V$100$ GPUs for model training with a batch size of $16$ on each GPU. AudioGen utilizes $64$ A$100$ GPUs with a batch size of $256$. \n\n\\textbf{Human evaluation} We construct the  dataset for human subjective evaluation with $70$ randomly selected samples where $30$ audios are from AudioCaps, $30$ audios are from AudioSet, and $10$ randomly selected real recordings, which we will refer to as spam cases. Therefore, each model should generate $60$ audio samples given the corresponding text descriptions. We gather the output from models in one folder and anonymize them with random identifiers. An example questionnaire is shown in Table~\\ref{tab:questionniare}. The participant will need to fill in the last two columns for each audio file given the text description. Our final result shows that all the human raters have an average score above $90$ on the spam cases. Hence, their evaluation result is considered reliable.\n\n\\begin{table}[htbp]\n\\scriptsize\n\\centering\n\\begin{tabular}{cccc}\n\\toprule\nFile name &\n  Text description &\n  Overall impression (1-100) &\n  Relation to the text description   (1-100) \\\\\n\\midrule\nrandom\\_name\\_108029.wav &\n  A man talking followed by lights scrapping on a wooden surface &\n  80 &\n  90 \\\\\n\\midrule\nrandom\\_name\\_108436.wav & Bicycle Music Skateboard   Vehicle          & 70 & 80 \\\\\n\\midrule\nrandom\\_name\\_116883.wav & A power tool drilling as rock   music plays & 90 & 95 \\\\\n\\midrule\n... & ... & ... & ... \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Example questionnaire for human evaluation. The participant will need to fill in the last two columns.}\n\\label{tab:questionniare}\n\\end{table}\n\n\\subsection{The Effect of Finetuning}\n\n\\begin{table}[tbp]\n\\centering\n\\small\n\\begin{tabular}{cccc|cccc}\n\\toprule\n    Model    & Text Data & Use CLAP  & Finetuned & FD~$\\downarrow$  & IS~$\\uparrow$   & KL~$\\downarrow$ & FAD~$\\downarrow$   \\\\\n\\midrule\nAudioLDM-S-Full-Roberta   & \\cmark   & \\xmark      & \\xmark  & $34.28$  & $3.53$ & $3.44$ & $6.96$ \\\\\nAudioLDM-S-Full-Roberta   & \\cmark   & \\xmark      & \\cmark  & $32.13$  & $4.02$ & $3.25$ & $5.89$ \\\\\nAudioLDM-S-Full & \\xmark & \\cmark  & \\xmark & $24.13$  & $6.68$ & $2.36$ & $4.94$ \\\\\nAudioLDM-S-Full & \\xmark & \\cmark  & \\cmark & $23.47$  & $7.57$ & $1.98$ & $2.32$ \\\\\nAudioLDM-L-Full & \\xmark & \\cmark  & \\xmark & $23.51$  & $7.11$ & $2.19$ & $4.19$  \\\\\nAudioLDM-L-Full & \\xmark & \\cmark  & \\cmark & $23.31$  & $8.13$ & $1.59$ & $1.96$  \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{The comparison between fine-tuned and non-finetuned models on the AudioCaps evaluation set.}\n\\label{tab: finetune-non-finetune-comparison}\n\\end{table}\n\nTable~\\ref{tab: finetune-non-finetune-comparison} compares the results obtained with and without fine-tuning on the evaluation set. We observe an improvement in various evaluation metrics, which is expected since the training set of AudioCaps has a distribution that is similar to the evaluation set. However, it is important to note that higher performance on the limited distribution of the evaluation set may not necessarily indicate better performance overall. A model that can generate broader distributions of audio may perform worse on the evaluation set, even though it may have better generalization capabilities. Future work in audio generation can focus on building an evaluation protocal that is more aligned with human perceptions.\n\n\\subsection{Computation Efficiency Comparison}\n\n\\begin{figure}[htbp]\n  \\centering\n  \\subfigure[Different batch sizes]{\\includegraphics[width=0.3\\textwidth]{pics/AudioLDM-S.png}\\label{fig:sub-audioldm-s}}\n  \\subfigure[Different sampling steps]{\\includegraphics[width=0.3\\textwidth]{pics/AudioLDM-L.png}\\label{fig:sub-audioldm-l}}\n  \\subfigure[With/Without classifier-free guidance]{\\includegraphics[width=0.3\\textwidth]{pics/timing.png}\\label{sub-audioldm-timing}}\n  \\caption{These three figures show the time cost when generating ten seconds of audio, measured on a single A100 GPU.}\n  \\label{fig:speed-comparison}\n\\end{figure}\n\nAs shown in Figure~\\ref{fig:sub-audioldm-s}, AudioLDM-S can generate eight ten-second-long audios within ten seconds without classifier-free guidance. With classifier-free guidance, AudioLDM-Small can generate eight ten-second-long audios with $150$ DDIM steps. Figure~\\ref{sub-audioldm-timing} shows our model is faster than the DiffSound on different batch sizes. Our model can generate eight ten-second-long audios with $20$ seconds while DiffSound needs more than $40$ seconds. Since AudioGen has not been open-sourced yet, we did not perform a speed comparison with AudioGen.\n\n\\subsection{Limitations}\n\nThere are several limitations to our study that warrant further investigation in future work. For example, the sampling rate of our model is still insufficient, especially for the generation of music. Exploring higher-fidelity sampling rates such as 32 kHz or 48 kHz could improve the quality of the generated audio. Also, all the modules in AudioLDM are trained separately, which may result in misalignment between different modules. For instance, the latent space learned by VAE may not be optimal for the latent diffusion model. Future work can explore approaches to better align the different modules, such as end-to-end fine-tuning.\n\nThe possible negative impact of our method might be the abuse of our technology or released models, e.g., generating fake audio effects to provide misleading information. Moreover, sensitive text content should be restricted in future work to prevent the creation of harmful audio content.\n\n\\newpage\n\\subsection{Demos}\n\\label{app:demos}\n\n\\textbf{Audio Style Transfer}\n\nWe show three examples of zero-shot audio style transfer with AudioLDM-S, using the developed shallow reverse process~(see Equation~\\ref{shallowreverse}). In Figure~\\ref{fig:demo-style-transfer-1}, we show the transfer from \\textbf{drum beats} to \\textbf{ambient music}. From left to right, we show the source audio sample drum beats, and the six generated samples guided by text prompt \\textbf{ambient music} with different starting points $n_{0}$. Given a smaller $n_{0}$~(i.e., the left part of the figure), the generated sample is similar to drum beats, while when we set $n_{0}=0.8\\times N$ for the last sample, the generated sample will be aligned with the text input \\textbf{ambient music}. Similarly, we show the source audio \\textbf{trumpt}, and the seven generated samples guided by text prompt \\textbf{children singing} in Figure~\\ref{fig:demo-style-transfer-2}. We show the source audio \\textbf{sheep vocalization}, and the five generated samples guided by text prompt \\textbf{narration, monologue} in Figure~\\ref{fig:demo-style-transfer-3}.\n\n\\vspace{0.5cm}\n\n\\begin{figure}[htbp]\n    \\centering\n    \\includegraphics[width=0.75\\linewidth]{pics/drum_beat_to_ambient_music.png}\n    \\caption{Audio style transfer from \\textbf{drum beats} to \\textbf{ambient music}.}\n    \\label{fig:demo-style-transfer-1}\n\\end{figure}\n\n\\begin{figure}[htbp]\n    \\centering\n    \\includegraphics[width=0.75\\linewidth]{pics/trumpet-to-children-singing.png}\n    \\caption{Audio style transfer from \\textbf{trumpet} to \\textbf{children singing}.}\n    \\label{fig:demo-style-transfer-2}\n\\end{figure}\n\n\\begin{figure}[htbp]\n    \\centering\n    \\includegraphics[width=0.75\\linewidth]{pics/sheep_to_human.000.png}\n    \\caption{Audio style transfer from \\textbf{sheep vocalization} to \\textbf{narration, monologue}.}\n    \\label{fig:demo-style-transfer-3}\n\\end{figure}\n\n\\newpage\n\n\\textbf{Audio Super-Resolution}\n\nIn Figure~\\ref{fig:demo-super-resolution}, we show four cases of zero-shot audio super-resolution with AudioLDM-S: $1$) \\textbf{violin}, $2$) \\textbf{sneezing sound from a woman}, $3$) \\textbf{baby crying}, and $4$) \\textbf{female speech}. The sampling rate of input samples (left) is $8$~kHz, and that of generated samples (middle) and ground-truth samples (right) is $16$~kHz. Our visualization shows we can retain the ground-truth observation in the low-frequency part (below $8$~kHz), while generating the high-frequency missing part (from $8$~kHz to $16$~kHz) with pretrained AudioLDM-S. The generated high-frequency information is consistent with the low-frequency observation.  \n\n\\vspace{0.5cm}\n\n\\begin{figure}[htbp]\n    \\centering\n    \\includegraphics[width=0.85\\linewidth]{pics/super_resolution_2.pdf}\n    \\caption{The examples of zero-shot audio super-resolution with AudioLDM-S.}\n    \\label{fig:demo-super-resolution}\n\\end{figure}\n\n\\newpage\n\n\\textbf{Audio Inpainting}\n\nIn Figure~\\ref{fig:demo-inpainting}, we show four samples of zero-shot audio inpainting with AudioLDM-S. The time length of each audio sample is $10$ seconds. In the \\textbf{unprocessed} part, we remove the content between $2.5$ and $7.5$ seconds from the ground-truth sample as the input of inpainting. In the \\textbf{inpainting result} part, we show the generated samples guided by the same text prompt of the ground-truth sample. In the \\textbf{ground truth} part, we show the ground-truth sample for comparison.\n\n\\vspace{0.5cm}\n\n\\begin{figure}[htbp]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{pics/inpainting.pdf}\n    \\caption{The examples of zero-shot audio inpainting with AudioLDM-S.}\n    \\label{fig:demo-inpainting}\n\\vspace{0.5cm}\n\\end{figure}\n\n\\begin{figure}[htbp]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{pics/inpaint_prompt.pdf}\n    \\caption{The example of zero-shot audio inpainting with AudioLDM-S given different text prompts.}\n    \\label{fig:demo-inpainting-prompt}\n\\vspace{0.5cm}\n\\end{figure}\n\nIn Figure~\\ref{fig:demo-inpainting-prompt}, we use one sample to demonstrate the audio inpainting guided by different text prompts. Given the observed audio signal shown in the top row, we guide the inpainting process with four different text prompts: $1$) \\textbf{ambient music}; $2$) \\textbf{a man is speaking with bird calls in the background}; $3$) \\textbf{a cat is meowing}; $4$) \\textbf{raining with wind blowing}. As can be seen, the observed audio signal is preserved in each generated sample, while the generated content can be controlled by text input.   \n\n\\newpage\n\n\\vspace{0.5cm}\n\n\\textbf{Environment Control}\n\nIn Figure~\\ref{fig:demo-control-speech}, we demonstrate that AudioLDM can control the acoustic environment of generated samples with a text description. The four samples are generated with the same random seed, but with different text prompts. Their common text information is \\textbf{``A man is speaking in''}, while the specific text information describes the acoustic environment as \\textbf{``a small room''}, \\textbf{``a huge room''}, \\textbf{``a huge room without background noise''}, and \\textbf{``a studio''}. These samples show the ability of AudioLDM to capture the fine-grained text description about the acoustic environment, and control the corresponding effects on audio samples, such as reverberation or background noise.   \n\\vspace{0.5cm}\n\n\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{pics/control_speech_2.pdf}\n    \\caption{The examples of controlling acoustic environment with AudioLDM-S.}\n    \\label{fig:demo-control-speech}\n\\end{figure}\n\n\\newpage\n\n\\textbf{Music Control}\n\nIn Figure~\\ref{fig:demo-control-music}, we show the generated music samples when we control the music characteristics with text input. The first sample is generated by \\textbf{``Theme music with bass drum''}. Then, we add specific text information \\textbf{``flute''}, \\textbf{``fast, flute''}, or \\textbf{``flute in the background''}, to change the text input. The corresponding variations can be seen in generated mel-spectrograms. We use these samples to demonstrate the ability of AudioLDM to add new musical instruments to music samples, tune the speed of music, and control the foreground-background relations. \n\n\\vspace{0.8cm}\n\n\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{pics/control_music_2.pdf}\n    \\caption{The examples of controlling music characteristics with AudioLDM-S.}\n    \\label{fig:demo-control-music}\n\\vspace{0.5cm}\n\\end{figure}\n\n\\newpage    \n\n\\textbf{Pitch Control}\n\nIn Figure~\\ref{fig:pitch}, we show the ability of AudioLDM to control the pitch of generated samples. Pitch is an important characteristic of sound effects, music and speech. Here, we set the common text information as \\textbf{``Sine wave with $\\cdots$ pitch''}, and input the specific text information \\textbf{``low''}, \\textbf{``medium''}, and \\textbf{``high''}. The text-controlled pitch variation can be seen from the three generated samples.\n\n\\vspace{0.5cm}\n\n\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=0.5\\linewidth]{pics/pitch_control.pdf}\n    \\caption{The examples of pitch controlling on generating samples with AudioLDM-S.}\n    \\label{fig:pitch}\n\\end{figure}\n\n\\newpage\n\n\\textbf{Material Control}\n\nIn Figure~\\ref{fig:material}, we show the ability of AudioLDM to control the materials which generate audio samples. We show four samples generated by the common action \\textbf{``hit''} between different materials, e.g., \\textbf{wooden object} and \\textbf{wooden environment}, or \\textbf{metal object} and \\textbf{wooden environment}. \n\n\\vspace{0.5cm}\n\n\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{pics/material_control.pdf}\n    \\caption{The examples of controlling the materials of generated audio samples with AudioLDM-S.}\n    \\label{fig:material}\n\\vspace{0.5cm}\n\\end{figure}\n\n\\vspace{0.5cm}\n\\textbf{Temporal Order Control}\n\nIn Figure~\\ref{fig:temporalorder}, we show the ability of AudioLDM to control the temporal order between generated compositional audio signals. When the text description includes multiple sound effects, AudioLDM can generate the audio signals, and the temporal order between them is consistent with the text input.\n\n\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{pics/order_control.pdf}\n    \\caption{The examples of controlling the temporal order between generated compositional audio samples with AudioLDM-S.}\n    \\label{fig:temporalorder}\n\\vspace{0.5cm}\n\\end{figure}\n\n\\textbf{Text-to-Audio Generation}\n\nIn Figure~\\ref{fig:text-to-audio}, we show four text-to-audio generation results with AudioLDM-S. They include sound effects in natural environment, human speech, human activity, and sound from objects interaction.\n\n\\vspace{0.3cm}\n\n\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{pics/text2sound.pdf}\n    \\caption{The examples of text-to-audio generation with AudioLDM-S.}\n    \\label{fig:text-to-audio}\n\\vspace{0.6cm}\n\\end{figure}\n\n\\textbf{Novel Audio Generation}\n\nIn Figure~\\ref{fig:novel-audio}, we show four novel audio samples generated with AudioLDM-S. Their text description is rarely seen, e.g., \\textbf{``A wolf is singing a beautiful song.''}. We use them to exhibit the generalization ability of AudioLDM.\n\\vspace{0.3cm}\n\n\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{pics/novel_sound.pdf}\n    \\caption{The examples of novel audio generation with AudioLDM-S.}\n    \\label{fig:novel-audio}\n\\end{figure}\n\n\\newpage\n\n\\textbf{Music Generation}\n\nIn Figure~\\ref{fig:demo-audioset-music}, we show four music samples generated with AudioLDM-S. Here, we are using the labels of AudioSet as text description for music generation, and we are able to specify the music genres of generated samples such as \\textbf{Classical music}.\n\n\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{pics/label2music_generation.pdf}\n    \\caption{The examples of music generation with AudioLDM-S.}\n    \\label{fig:demo-audioset-music}\n\\end{figure}\n\n\\newpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Text-to-Audio Generation using Instruction-Guided Latent~Diffusion~Model}\n\n\\begin{document}\n\n\\title{\\modeltitle{}: Improving Diffusion-based Text-to-Audio Generation using Direct Preference Optimization}\n\n\\title{\\modeltitle{}: Improving Diffusion-based Text-to-Audio Generation using Direct Preference Optimization based Alignment}\n\n\\title{\\modeltitle{}: Enhancing Diffusion-based Text-to-Audio Generation through Direct Preference Optimization-based Alignment}\n\n\\title{\\modeltitle{}: Aligning Diffusion-based Text-to-Audio Generative Models through Direct Preference Optimization}\n\n\\title{\\modeltitle{}: Aligning Diffusion-based Text-to-Audio Generations through Direct Preference Optimization}\n\n\\author{Navonil Majumder}\n\\affiliation{%\n  \\institution{Singapore University of Technology and Design, Singapore}\n  \\country{}\n}\n\\authornote{Authors contributed equally.}\n\n\\author{Chia-Yu Hung}\n\\affiliation{%\n  \\institution{Singapore University of Technology and Design, Singapore}\n  \\country{}\n}\n\\authornotemark[1]\n\n\\author{Deepanway Ghosal}\n\\affiliation{%\n  \\institution{Singapore University of Technology and Design, Singapore}\n    \\country{}\n}\n\\authornotemark[1]\n\n\\author{Wei-Ning Hsu}\n\\affiliation{%\n  \\institution{Meta AI, USA}\n    \\country{}\n}\n\n\\author{Rada Mihalcea}\n\\affiliation{%\n  \\institution{University of Michigan, USA}\n    \\country{}\n}\n\n\\author{Soujanya Poria}\n\\affiliation{%\n  \\institution{Singapore University of Technology and Design, Singapore}\n  \\country{}\n}\n\n \\begin{teaserfigure}\n \\centering\n\\begin{minipage}[t]{\\linewidth}\n  \\begin{center}\n    \\includegraphics[width=0.9\\linewidth]{images/tango2-teaser.pdf}\n  \\end{center}\n\\end{minipage}\n\\vspace{0.3cm}\n\\begin{minipage}[t]{\\linewidth}\n  \\centering\n  \\faGithub: \\textcolor{red}{\\url{https://github.com/declare-lab/tango}} \\\\\n  \\faDatabase: \\textcolor{blue}{\\url{https://huggingface.co/datasets/declare-lab/audio-alpaca}}\\\\\n  \\faAnchor: \\textcolor{green}{\\url{https://huggingface.co/declare-lab/tango2}}\\\\\n  \\faGlobe : \\textcolor{magenta}{\\url{https://tango2-web.github.io/}}\n\\end{minipage}\\end{teaserfigure}\n\n\\begin{abstract}\nGenerative multimodal content is increasingly prevalent in much of the content creation arena, as it has the potential to allow artists and media personnel to create pre-production mockups by quickly bringing their ideas to life. The generation of audio from text prompts is an important aspect of such processes in the music and film industry. Many of the recent diffusion-based text-to-audio models focus on training increasingly sophisticated diffusion models on a large set of datasets of prompt-audio pairs. These models do not explicitly focus on the presence of concepts or events and their temporal ordering in the output audio with respect to the input prompt. Our hypothesis is focusing on how these aspects of audio generation could improve audio generation performance in the presence of limited data. As such, in this work, using an existing text-to-audio model Tango, we synthetically create a preference dataset where each prompt has a winner audio output and some loser audio outputs for the diffusion model to learn from. The loser outputs, in theory, have some concepts from the prompt missing or in an incorrect order. We fine-tune the publicly available Tango text-to-audio model using diffusion-DPO (direct preference optimization) loss on our preference dataset and show that it leads to improved audio output over Tango and AudioLDM2, in terms of both automatic- and manual-evaluation metrics.\n\\end{abstract}\n\n\\begin{CCSXML}\n<ccs2012>\n   <concept>\n       <concept_id>10010147.10010178.10010179</concept_id>\n       <concept_desc>Computing methodologies~Natural language processing</concept_desc>\n       <concept_significance>500</concept_significance>\n       </concept>\n   <concept>\n       <concept_id>10002951.10003227.10003251</concept_id>\n       <concept_desc>Information systems~Multimedia information systems</concept_desc>\n       <concept_significance>500</concept_significance>\n       </concept>\n </ccs2012>\n\\end{CCSXML}\n\n\\ccsdesc[500]{Computing methodologies~Natural language processing}\n\\ccsdesc[500]{Information systems~Multimedia information systems}\n\n\\keywords{Multimodal AI, Text-to-Audio Generation, Diffusion Models, Large Language Models, Preference Optimization}\n\n\\maketitle\n\\section{Introduction}\n\nGenerative AI is increasingly turning into a mainstay of our daily lives, be it directly through using ChatGPT~\\cite{chatgpt}, GPT-4~\\cite{gpt4} in an assistive capacity, or indirectly by consuming AI-generated memes, generated using models like StableDiffusion~\\cite{rombach2022high}, DALL-E 3~\\cite{dalle2,BetkerImprovingIG}, on social media platforms. Nonetheless, there is a massive demand for AI-generated content across industries, especially in the multimedia sector. Quick creation of audio-visual content or prototypes would require an effective text-to-audio model along with text-to-image and -video models. Thus, improving the fidelity of such models with respect to the input prompts is paramount.\n\nRecently, supervised fine-tuning-based direct preference optimization~\\cite{rafailov2023direct} (DPO) has emerged as a cheaper and more robust alternative to reinforcement learning with human feedback (RLHF) to align LLM responses with human preferences. This idea is subsequently adapted for diffusion models by \\citet{wallace2023diffusion} to align the denoised outputs to human preferences. In this work, we employ this DPO-diffusion approach to improve the semantic alignment between input prompt and output audio of a text-to-audio model. Particularly, we fine-tune the publicly available text-to-audio latent diffusion model \\textsc{Tango}~\\cite{ghosal2023text} on our synthesized preference dataset with DPO-diffusion loss. This preference dataset contains diverse audio descriptions (\\emph{prompts}) with their respective preferred (\\emph{winner}) and undesirable (\\emph{loser}) audios. The preferred audios are supposed to perfectly reflect their respective textual descriptions, whereas the undesirable audios have some flaws, such as some missing concepts from the prompt or in an incorrect temporal order or high noise level. To this end, we perturbed the descriptions to remove or change the order of certain concepts and passed them to \\textsc{Tango} to generate undesirable audios. Another strategy that we adopted for undesirable audio generation was adversarial filtering: generate multiple audios from the original prompt and choose the audio samples with CLAP-score below a certain threshold. We call this preference dataset \\dataset{}. To mitigate the effect of noisy preference pairs stemming from automatic generation, we further choose a subset of samples for DPO fine-tuning based on certain thresholds defined on the CLAP-score differential between preferred and undesirable audios and the CLAP-score of the undesirable audios. This likely ensures a minimal proximity to the input prompt, while guaranteeing a minimum distance between the preference pairs.\n\nWe experimentally show that fine-tuning \\textsc{Tango} on the pruned \\dataset{} yields \\model{} that significantly surpasses \\textsc{Tango} and \\textsc{AudioLDM2} in both objective and human evaluations. Moreover, exposure to the contrast between good and bad audio outputs during DPO fine-tuning likely allows \\model{} to better map the semantics of the input prompt into the audio space, despite relying on the same dataset as \\textsc{Tango} for synthetic preference data-creation.\n\nThe broad contributions of this paper are the following:\n\\begin{enumerate}%[itemsep=0pt, leftmargin=*, wide, labelwidth=0pt, labelindent=0pt, parsep=0pt]\n    \\item We develop a cheap and effective heuristics for semi automatically creating a preference dataset for text-to-audio generation;\n    \\item On the same note, we also share the preference dataset \\dataset{} for text-to-audio generation that may aid in the future development of such models;\n    \\item Despite not sourcing additional out-of-distribution text-audio pairs over \\textsc{Tango}, our model \\model{} outperforms both \\textsc{Tango} and \\textsc{AudioLDM2} on both objective and subjective metrics;\n    \\item \\model{} demonstrates the applicability of diffusion-DPO in audio generation.\n\\end{enumerate}\n\n\\section{Related Work}\n\\label{sec:related-work}\n\nText-to-audio generation has garnered serious attention lately thanks to models like AudioLDM~\\cite{liu2023audioldm}, Make-an-Audio~\\cite{huang2023make}, Tango~\\cite{ghosal2023text}, and Audiogen~\\cite{kreuk2022audiogen}. These models rely on diffusion architectures for audio generation from textual prompts. Recently, AudioLM~\\cite{borsos2023audiolm} was proposed which utilizes the state-of-the-art semantic model w2v-Bert~\\cite{DBLP:conf/asru/ChungZHCQPW21} to generate semantic tokens from audio prompts. These tokens condition the generation of acoustic tokens, which are decoded using the acoustic model SoundStream~\\cite{DBLP:journals/taslp/ZeghidourLOST22} to produce audio. The semantic tokens generated by w2v-Bert are crucial for conditioning the generation of acoustic tokens, subsequently decoded by SoundStream.\n\nAudioLDM~\\cite{liu2023audioldm} is a text-to-audio framework that employs CLAP \\cite{wu2023large}, a joint audio-text representation model, and a latent diffusion model (LDM). Specifically, an LDM is trained to generate latent representations of melspectrograms obtained using a Variational Autoencoder (VAE). During diffusion, CLAP embeddings guide the generation process. \\texttt{Tango}~\\cite{ghosal2023tango} utilizes the pre-trained VAE from AudioLDM and replaces the CLAP model with a fine-tuned large language model: FLAN-T5. This substitution aims to achieve comparable or superior results while training with a significantly smaller dataset.\n\nIn the realm of aligning generated audio with human perception, \\citet{liao2024baton} recently introduced BATON, a framework that initially gathers pairs of audio and textual prompts, followed by annotating them based on human preference. This dataset is subsequently employed to train a reward model. The reward generated by this model is then integrated into the standard diffusion loss to guide the network, leveraging feedback from the reward model. However, our approach significantly diverges from this work in two key aspects: 1) we automatically construct a \\emph{pairwise} preference dataset, referred to as \\dataset{}, utilizing various techniques such as LLM-guided prompt perturbation and re-ranking of generated audio from Tango using CLAP scores, and 2) we then train Tango on \\dataset{} using diffusion-DPO to generate audio samples preferred by human perception.\n\n\\section{Background}\n\n\\subsection{Overview of \\textsc{Tango}}\n\nTango, proposed by \\citet{ghosal2023text}, primarily relies on a latent diffusion model (LDM) and an instruction-tuned LLM for text-to-audio generation. It has three major components: \\begin{enumerate}\n    \\item Textual-prompt encoder\n    \\item Latent diffusion model (LDM)\n    \\item Audio VAE and Vocoder\n  \\end{enumerate}  \nThe textual-prompt encoder encodes the input description of the audio. Subsequently, the textual representation is used to construct a latent representation of the audio or audio prior from standard Gaussian noise, using reverse diffusion. Thereafter, the decoder of the mel-spectrogram VAE constructs a mel-spectrogram from the latent audio representation. This mel-spectrogram is fed to a vocoder to generate the final audio. \n\n\\subsubsection{Textual Prompt Encoder}\nTango utilizes the pre-trained LLM \\textsc{Flan-T5-Large} (780M)~\\cite{https://doi.org/10.48550/arxiv.2210.11416} as the text encoder ($E_{text}$) to acquire text encoding $\\tau\\in \\mathbb{R}^{L\\times d_{text}}$, where $L$ and $d_{text}$ represent the token count and token-embedding size, respectively.\n\n\\subsubsection{Latent Diffusion Model}\nFor ease of understanding, we briefly introduce the LDM of Tango in this section. \nThe latent diffusion model (LDM)~\\cite{rombach2022high} in Tango is derived from the work of \\citet{Liu2023AudioLDMTG}, aiming to construct the audio prior $x_0$ guided by text encoding $\\tau$. This task essentially involves approximating the true prior $q(x_0|\\tau)$ using parameterized $p_\\theta(x_0|\\tau)$.\n\nLDM achieves this objective through forward and reverse diffusion processes. The forward diffusion represents a Markov chain of Gaussian distributions with scheduled noise parameters $0 < \\beta_1 < \\beta_2 < \\cdots < \\beta_N < 1$, facilitating the sampling of noisier versions of $x_0$:\n\\begin{flalign}\nq(x_n|x_{n-1}) &= \\mathcal{N}(\\sqrt{1-\\beta_n} x_{n-1}, \\beta_n \\mathbf{I}),  \\label{eq:forward_diff}\\\\\nq(x_n|x_0) &= \\mathcal{N}(\\sqrt{\\overline\\alpha_n} x_0, (1-\\overline\\alpha_n)\\mathbf{I}) \\label{eq:quick_forward_diff},\n\\end{flalign}\nwhere $N$ is the number of forward diffusion steps, $\\alpha_n = 1 - \\beta_n$, and $\\overline\\alpha_n = \\prod_{i=1}^n \\alpha_n$. \\citet{Song2020DenoisingDI} show that \\cref{eq:quick_forward_diff} conveniently follows from \\cref{eq:forward_diff} through reparametrization trick that allows direct sampling of any $x_n$ from $x_0$ via a non-Markovian process: \n\\begin{equation}\nx_n = \\sqrt{\\overline\\alpha_n} x_0 + (1-\\overline\\alpha_n)\\epsilon, \\label{eq:x_n_sampling}\n\\end{equation}\nwhere the noise term $\\epsilon\\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$. The final step of the forward process yields $x_N\\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$.\n\nThe reverse process denoises and reconstructs $x_0$ through text-guided noise estimation ($\\hat{\\epsilon}_\\theta$) using loss\n\\begin{align}\n    \\mathcal{L}_{LDM} = \\sum_{n=1}^N\\gamma_n \\mathbb{E}_{ \\epsilon_n\\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}), x_0} || \\epsilon_n - \\hat\\epsilon_\\theta^{(n)}(x_n, \\tau) ||_2^2, \\label{eq:LDM-loss}\n\\end{align}\nwhere $x_n$ is sampled according to \\cref{eq:x_n_sampling} using standard normal noise $\\epsilon_n$, $\\tau$ represents the text encoding for guidance, and $\\gamma_n$ denotes the weight of reverse step $n$~\\cite{hang2023efficient}, interpreted as a measure of signal-to-noise ratio (SNR) relative to $\\alpha_{1:N}$. The estimated noise is then employed for the reconstruction of $x_0$:\n\n\\begin{flalign}\n    p_\\theta(x_{0:N}|\\tau) &= p(x_N) \\prod_{n=1}^N p_\\theta(x_{n-1}|x_n, \\tau), \\\\\n    p_\\theta(x_{n-1}|x_n, \\tau) &= \\mathcal{N}(\\mu^{(n)}_\\theta(x_n, \\tau), \\Tilde{\\beta}^{(n)}),\\\\\n    \\mu_\\theta^{(n)}(x_n, \\tau) &= \\frac{1}{\\sqrt{\\alpha_n}}[x_n - \\frac{1 - \\alpha_n}{\\sqrt{1 - \\overline\\alpha_n}}\\hat\\epsilon_\\theta^{(n)}(x_n, \\tau)],\\\\\n    \\Tilde{\\beta}^{(n)} &=\\frac{1 - \\bar{\\alpha}_{n-1}}{1 - \\bar{\\alpha}_n} \\beta_n.\n\\end{flalign}\nThe parameterization of noise estimation $\\hat\\epsilon_\\theta$ involves utilizing U-Net~\\cite{10.1007/978-3-319-24574-4_28}, incorporating a cross-attention component to integrate the textual guidance $\\tau$.\n\\subsubsection{Audio VAE and Vocoder}\nThe audio variational auto-encoder (VAE)~\\cite{Kingma2013AutoEncodingVB} compresses the mel-spectrogram of an audio sample, $m\\in \\mathbb{R}^{T\\times F}$, into an audio prior $x_0\\in \\mathbb{R}^{C\\times T/r\\times F/r}$, where $C$, $T$, $F$, and $r$ denote the number of channels, time-slots, frequency-slots, and compression level, respectively. The latent diffusion model (LDM) reconstructs the audio prior $\\hat x_0$ using input-text guidance $\\tau$. \nBoth the encoder and decoder consist of ResUNet blocks~\\cite{Kong2021DecouplingMA} and are trained by maximizing the evidence lower-bound (ELBO)~\\cite{Kingma2013AutoEncodingVB} and minimizing adversarial loss~\\cite{Isola2016ImagetoImageTW}.\nTango utilizes the checkpoint of the audio VAE provided by \\citet{Liu2023AudioLDMTG}.\n\nAs a vocoder to convert the audio-VAE decoder-generated mel-spectrogram into audio, Tango employs HiFi-GAN~\\cite{kong2020hifi} which is also utilized by \\citet{Liu2023AudioLDMTG}.\n\nFinally, Tango utilizes a data augmentation method that merges two audio signals while considering human auditory perception. This involves computing the pressure level of each audio signal and adjusting the weights of the signals to prevent the dominance of the signal with higher pressure level over the one with lower pressure level. Specifically, when fusing two audio signals, the relative pressure level is computed using the following equation:\n\\begin{flalign}\n    p = (1 + 10^\\frac{G_1 - G_2}{20})^{-1}, \\label{eq:gain}\n\\end{flalign}\nHere $G_1$ and $G_2$ are the pressure levels of signal $x_1$ and $x_2$. Then the audio signals are mixed using the equation below:\n\\begin{flalign}\n    \\text{mix}(x_1, x_2) = \\frac{p x_1 + (1-p) x_2}{\\sqrt{p^2 + (1-p)^2}}.\n\\end{flalign}\nThe denominator is to account for the fact that the energy of a sound wave is proportional to the square of its amplitude as shown in \\citet{DBLP:journals/corr/abs-1711-10282}. Note that in this augmentation, textual prompts are also concatenated. \n\\subsection{Preference Optimization for Language Models}\nTuning Large Language Models (LLMs) to generate responses according to human preference has been a great interest to the ML community. The most popular approach for aligning language models to human preference is reinforcement learning with human feedback (RLHF). It comprises the following steps~\\cite{rafailov2023direct}:\n\n\\paragraph{\\bf Supervised Fine Tuning (SFT)} First, the pre-trained LLM undergoes supervised fine-tuning on high-quality downstream tasks to obtain the fine-tuned model $\\pi^{SFT}$.\n\\begin{figure*}[ht!]\n    \\centering\n    \\includegraphics[width=0.9\\textwidth]{images/tango2.pdf}\n    \\caption{\\small An illustration of our pipeline for text-to-audio alignment. The top part depicts the preference dataset creation where three strategies are deployed to generate the undesirable audio outputs to the input prompts. These samples are further filtered to form \\dataset{}. This preference dataset is finally used to align \\textsc{Tango} using DPO-diffusion loss (\\cref{eq:DPO-Diff}), resulting in \\model{}.}\n    \\label{fig:pipeline}\n\\end{figure*}\n\\paragraph{\\bf Reward Modeling} Next, $\\pi^{SFT}$ is prompted with an input $\\tau$ to generate multiple responses. These responses are then shown to human labelers to rank. Once such a rank is obtained, $x^w \\succ x^l \\mid \\tau $ indicating $x^w$ is preferred over $x^l$, the task is to model these preferences. Among several popular choices of preference modeling, Bradley-Terry (BT) is the most popular one which relies on the equation below:\n\n\\begin{align}\n    p^*(x^w \\succ x^l \\mid \\tau) = \\frac{\\exp(r^*(\\tau, x^w))}{\\exp(r^*(\\tau, x^w)) + \\exp(r^*(\\tau, x^l))} \\label{eqn:BT}\n\\end{align}\nThe overall idea is to learn the human preference distribution $p^*$. $r^*(\\tau, x)$ is a latent reward function that generates the preferences. With a static dataset created by human annotators, $\\mathcal{D} = \\left\\{ \\left(\\tau_{(i)}, x^{w}_{(i)}, x^{l}_{(i)}\\right) \\right\\}_{i=1}^{N}$, one can train a reward model  $r_\\phi(\\tau, x)$ using maximum likelihood estimation. The negative log-likelihood loss of this training can be written as follows:\n\\begin{align}\n    \\mathcal{L}_R(r_{\\phi}, \\mathcal{D}) = -\\mathbb{E}_{(\\tau, x^w, x^l) \\sim \\mathcal{D}} \\left[ \\log \\sigma (r_{\\phi}(\\tau, x^w) - r_{\\phi}(\\tau, x^l)) \\right]  \\label{eq:NLL}\n\\end{align}\nThis formulation considers framing the problem as a binary classification problem. \n\n\\paragraph{\\bf RL Optimization}\nThe final step is to leverage $r_\\phi(\\tau, x)$ to feedback the language model. As explained by \\citet{rafailov2023direct}, this can be embedded into the following learning objective:\n\\begin{align}\n\\label{eq:rlhf}\n    \\max_{\\pi_{\\theta}} \\mathbb{E}_{\\tau \\sim \\mathcal{D}, x \\sim \\pi_{\\theta}(x|\\tau)} \\left[ r_{\\phi}(\\tau, x) \\right] - \\beta D_{KL} \\left[ \\pi_{\\theta}(x|\\tau) \\parallel \\pi_{\\text{ref}}(x|\\tau) \\right]  \n\\end{align}\nHere, $\\pi_{\\text{ref}}$ represents the reference model, which in this context is the supervised fine-tuned model denoted as $\\pi^{\\text{SFT}}$. $\\pi_{\\theta}$ stands for the policy language model, intended for enhancement based on feedback from $r_\\phi(\\tau, x)$. $\\beta$ governs $\\pi_{\\theta}$ to prevent significant divergence from $\\pi_{\\text{ref}}$. This control is crucial as it ensures that the model stays close to the distributions upon which $r_\\phi(\\tau, x)$ was trained. Since the outputs from LLM are discrete, \\cref{eq:rlhf} becomes non-differentiable, necessitating reinforcement learning methods like PPO to address this objective.\n\n\\section{Methodology}\n\nThe two major parts of our approach (i) creation of preference dataset \\dataset{} and (ii) DPO for alignment are outlined in \\cref{fig:pipeline}.\n\n\\subsection{Creation of \\dataset{}}\n\\label{sec:pref-dataset}\n\n\\subsubsection{Audio Generation from Text Prompts}\n\nOur first step is to create audio samples from various text prompts with the pre-trained Tango model. We follow three different strategies as follows:\n\n\\paragraph{\\bf \\textit{Strategy 1}: Multiple Inferences from the same Prompt} In the first setting, we start by selecting a subset of diverse captions from the training split of the \\texttt{AudioCaps} dataset. We use the sentence embedding model \\textsf{gte-large}\\footnote{\\url{hf.co/thenlper/gte-large}} \\cite{li2023towards} to compute dense embedding vectors of all the captions in the training set. We then perform K-Means clustering on the embedded vectors with 200 clusters. Finally, we select 70 samples from each cluster to obtain a total of 14,000 captions. We denote the selected caption set as $\\mathcal{T}_1$.\n\nThe captions selected through the above process constitute the seed caption set. \nNow, we follow two settings to generate audio samples from these captions:\n\n\\begin{enumerate}\n    \\item \\textbf{Strategy 1.1}: Prompt \\textsf{Tango-full-FT} with the caption to generate four different audio samples with 5, 25, 50, and 100 denoising steps. All samples are created with a guidance scale of 3. \n\n    \\item \\textbf{Strategy 1.2}: Prompt \\textsf{Tango-full-FT} with the caption to generate four different audio samples each with 50 denoising steps. All samples are created with a guidance scale of 3.\n    \n\\end{enumerate}\n\nIn summary, we obtain $(\\tau, x_1, x_2, x_3, x_4)$ from \\emph{\\textbf{Strategy 1}}, where $\\tau$ denotes the caption from $\\mathcal{T}_1$ and $x_i$ denotes the audios generated from $\\tau$.\n\n\\begin{table*}[ht!]\n\\centering\n\\small\n\\caption{Prompts used in GPT-4 for creating the perturbations and the corresponding output.}\n \\resizebox{0.85\\linewidth}{!}{\n   \\begin{tabular}{p{5.5cm}|p{8cm}|p{7cm}}\n   \\toprule\n   \\textbf{\\textit{Strategy} and Original Caption} & \\textbf{Prompt to GPT-4} & \\textbf{Output from GPT-4} \\\\\n   \\midrule\n   \\textit{Strategy 2: Perturbed Prompts} & I have an audio clip for which the original caption is as follows: People cheering and race cars racing by. Can you generate five candidate captions that would satisfy the following requirements: & 1. Crowd applauding and bicycles speeding past. \\\\\n   People cheering and race cars racing by. & i) Would be closely related to the audio clip and the original caption. & 2. Spectators clapping and trains rushing by. \\\\\n   & ii) Would contain inaccuracies in terms of describing the audio clip i.e. they would be somewhat wrong captions for the audio clip. & 3. Audience cheering and horses galloping past. \\\\\n   & iii) However, the new captions should not be completely unrelated. Always keep some concepts from the original caption in the new one. & 4. Fans shouting and airplanes flying by. \\\\\n   & iv) Would be of similar length to the original caption.  & 5. Group celebrating and motorcycles revving past. \\\\\n   & Generate only the captions in separate lines so that I can programmatically extract them later. & \\\\\n   \\midrule\n   \\textit{Strategy 3: Temporally-Perturbed Prompts} & I have an audio clip for which the original caption is as follows: A man is speaking then a sewing machine briefly turns on and off. Can you generate five candidate captions that would satisfy the following requirements: & 1. A sewing machine briefly starts, then a man begins speaking. \\\\\n   A man is speaking then a sewing machine briefly turns on and off. & i) Would be closely related to the audio clip and the original caption. & 2. The sound of a sewing machine is heard after a man's voice. \\\\\n   & ii) Would have a change of order of the events described in the original caption. & 3. A man's voice is followed by the noise of a blender. \\\\\n   & iii) Would contain inaccuracies in terms of describing the audio clip i.e. they would be somewhat wrong captions for the audio clip. & 4. A woman speaks and then a sewing machine is turned on. \\\\\n   & iv) However, the new captions should not be completely unrelated. Always keep some concepts from the original caption in the new one. & 5. The noise of a sewing machine is interrupted by a man talking. \\\\\n   & v) Would be of similar length to the original caption. & \\\\\n   & Generate only the captions in separate lines so that I can programmatically extract them later. & \\\\\n   \\bottomrule\n   \\end{tabular}\n   }\n  \\label{tab:data_generation_example}\n\\end{table*}\n\\paragraph{\\bf \\textit{Strategy 2}: Inferences from Perturbed Prompts} We start from the selected set $\\mathcal{T}_1$ and make perturbations of the captions using the GPT-4 language model~\\cite{gpt4}. For a caption $\\tau$ from $\\mathcal{T}_1$, we denote $\\tau_1$ as the perturbed caption generated from GPT-4. We add specific instructions in our input prompt to make sure that  $\\tau_1$ is semantically or conceptually close to $\\tau$. We show an illustration of the process in \\cref{tab:data_generation_example}. In practice, we create five different perturbed $\\tau_1$ for each $\\tau$ from GPT-4, as shown in \\cref{tab:data_generation_example}.\n\nWe then prompt \\textsf{Tango-full-FT} with $\\tau$ and $\\tau_1$ to generate audio samples $x_{\\tau}$ and $x_{\\tau_1}$. We use 50 denoising steps with a guidance scale of 3 to generate these audio samples. \n\nTo summarize, we obtain $(\\tau, x_{\\tau}, x_{\\tau_1})$ from \\emph{\\textbf{Strategy 2}}. Note that, we considered $\\tau_1$ only to generate the audio sample $x_{\\tau_1}$. We do not further consider $\\tau_1$ while creating the preference dataset. \n\n\\paragraph{\\bf \\textit{Strategy 3}: Inferences from Temporally Perturbed Prompts}\n\nThis strategy is aimed at prompts that describe some composition of sequence and simultaneity of events. To identify such prompts in \\texttt{AudioCaps}' training dataset, as a heuristics, we look for the following keywords in a prompt: \\emph{while}, \\emph{before}, \\emph{after}, \\emph{then}, or \\emph{followed}. We denote the set of such prompts as $\\mathcal{T}_2$.\n\nFor each caption $\\tau_2$ in $\\mathcal{T}_2$, we then prompt GPT-4 to create a set of temporal perturbations. The temporal perturbations include changing the order of the events in the original caption, or introducing a new event or removing an existing event, etc. We aim to create these temporal perturbations by providing specific instructions to GPT-4, which we also illustrate in \\cref{tab:data_generation_example}. \n\nWe denote the temporally perturbed caption as $\\tau_2$. We then follow the same process as mentioned earlier in \\textit{Strategy 2} to create the audio samples $x_{\\tau}$ and $x_{\\tau_2}$. Finally, we pair the $(\\tau, x_{\\tau}, x_{\\tau_2})$ samples from this strategy. Analogous to the previous strategy, the $\\tau_2$ is only used to create the $x_{\\tau_2}$, and is not used anywhere else for preference data creation. \n\nWe collect the paired text prompt and audio samples from the three strategies and denote it overall as $(\\tau, \\langle x \\rangle)$, where $\\langle x \\rangle$ indicates the set of 4 or 2 generated audio samples depending upon the corresponding strategy. \n\n\\subsubsection{Ranking and Preference-Data Selection}\n\\label{sec:audio_alpaca_creation}\nWe first create a pool of candidate preference data for the three strategies as follows:\n\n\\paragraph{\\bf \\textit{For Strategy 1}} Let's assume we have an instance $(\\tau, \\langle x \\rangle)$ from Strategy 1. \nWe first compute the CLAP matching score following \\citet{wu2023large} between $\\tau$ and all the four audio samples in $\\langle x \\rangle$. We surmise that the sample in $\\langle x \\rangle$ that has the highest matching score with $\\tau$ is most aligned with $\\tau$, compared to the other three audio samples that have a relatively lower matching score. We consider this audio with the highest matching score as the winning sample $x^w$ and the other three audio samples as the losing sample $x^l$. In this setting, we can thus create a pool of three preference data points: $(\\tau, x^w, x^l)$, for the three losing audio samples $x^l$.\n\n\\paragraph{\\bf \\textit{For Strategy 2 and 3}} Let's assume we have an instance $(\\tau, \\langle x \\rangle)$ from Strategy 2 or 3. We compute the CLAP matching score between i) $\\tau$ with $x_{\\tau}$, and ii) $\\tau$ with the $x_{\\tau_1}$ or $x_{\\tau_2}$, corresponding to the strategy. We consider only those instances where the CLAP score of i) is higher than the CLAP score of ii). For these instances, we use $x_{\\tau}$ as the winning audio $x^w$ and $x_{\\tau_1}$ or $x_{\\tau_2}$ as the losing audio $x^l$ to create the preference data point: $(\\tau, x^w, x^l)$.\n\n\\paragraph{\\bf \\textit{Final Selection}} We want to ensure that the winning audio sample $x^w$ is strongly aligned with the text prompt \n$\\tau$. At the same time, the winning audio sample should have a considerably higher alignment with the text prompt than the losing audio sample. We use the CLAP score as a measurement to fulfill these conditions. The CLAP score is measured using cosine similarity between the text and audio embeddings, where higher scores indicate higher alignment between the text and the audio. We thus use the following conditions to select a subset of instances from the pool of preference data:\n\n\\begin{enumerate}[leftmargin=0.5cm]\n    \\item The winning audio must have a minimum CLAP score of $\\alpha$ with the text prompt to ensure that the winning audio is strongly aligned with the text prompt.\n    \\item The losing audio must have a minimum CLAP score of $\\beta$ with the text prompt to ensure that we have semantically close negatives that are useful for preference modeling.\n    \\item The winning audio must have a higher CLAP score than the losing audio w.r.t to the text prompt.\n    \\item We denote $\\Delta$ to be the difference in CLAP score between the text prompt with the winning audio\\footnote{In our paper, we employ the terms \"winner\" and \"preferred\" interchangeably. Likewise, we use \"loser\" and \"undesirable\" interchangeably throughout the text.} and the text prompt with the losing audio. The $\\Delta$ should lie between certain thresholds, where the lower bound will ensure that the losing audio is not too close to the winning audio, and the upper bound will ensure that the losing audio is not too undesirable.\n\\end{enumerate}\n\nWe use an \\emph{ensemble filtering} strategy based on two different CLAP models: \\textsf{630k-audioset-best} and \\textsf{630k-best} \\cite{wu2023large}. This can reduce the effect of noise from individual CLAP checkpoints and increase the robustness of the selection process. In this strategy, samples are included in our preference dataset if and only if they satisfy all the above conditions based on CLAP scores from both of the models. \nWe denote the conditional scores mentioned above as $\\alpha_1, \\beta_1, \\Delta_1$, and $\\alpha_2, \\beta_2, \\Delta_2$ for the two CLAP models, respectively. Based on our analysis of the distribution of the CLAP scores as shown in \\Cref{fig:alpha-delta}, we choose their values as follows: $\\alpha_1=0.45, \\alpha_2=0.60$, $\\beta_1=0.40, \\beta_2=0.0$, $0.05 \\leq \\Delta_1 \\leq 0.35$, and $0.08 \\leq \\Delta_2 \\leq 0.70$.\n\nFinally, our preference dataset \\dataset{} has a total of $\\approx$ 15k samples after this selection process. We report the distribution of \\dataset{} in \\cref{tab:strategies}. \n\n\\begin{figure}[b]\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{images/alpha1.pdf}\n    \\includegraphics[width=0.8\\linewidth]{images/delta1.pdf}\n    \\caption{\\small{The distribution of $\\alpha_1$ and $\\Delta_1$ scores in the unfiltered dataset. We see that for an unfiltered dataset: i) the winner audio sample is not always strongly aligned to the text prompt in the $\\alpha_1$ plot; ii) winner and loser audio samples can be too close in the $\\Delta_1$ plot. We thus choose the values of our $\\alpha_1$, $\\Delta_1$ and other selection parameters to ensure the filtered dataset is less noisy with more separation between the winner and loser audios.}\n    }\n    \\label{fig:alpha-delta}\n\\end{figure}\n\n\\begin{table*}[h]\n\\centering\n\\caption{Statistics of \\dataset{}.}\n\\label{tab:strategies}\n\\resizebox{0.8\\linewidth}{!}{\n\\begin{tabular}{lcccc}\n\\toprule\n\\textbf{Strategy} & \\textbf{\\# Samples} & \\textbf{Avg. Winner Score} & \\textbf{Avg. Loser Score}&  \\textbf{Avg. Delta}\\\\\n\\midrule\nInference w/ Different Denoising Steps (Strategy 1.1) \n& 3004 & 0.645 & 0.447 &0.198\\\\\nInference w/ Same Denoising Steps (Strategy 1.2)\n& 2725 & 0.647 & 0.494& 0.153 \\\\\nGPT-4 Perturbed Prompts (Strategy 2)\n& 4544 & 0.641 & 0.425 & 0.216 \\\\\nGPT-4 Temporally Perturbed Prompts (Strategy 3)\n& 4752 & 0.649 & 0.458 & 0.191 \\\\\n\\midrule\nOverall & 15025 & 0.645 & 0.452 & 0.193\\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{table*}\n\n\\subsection{DPO for Preference Modeling}\n\nAs opposed to RLHF, recently DPO has emerged as a more robust and often more practical and straightforward alternative for LLM alignment that is based on the very same BT preference model (\\cref{eqn:BT}). In contrast with supervised fine-tuning (SFT) that only optimizes for the desirable (\\emph{winner}) outputs, the DPO objective also allows the model to learn from undesirable (\\emph{loser}) outputs, which is key in the absence of a high-quality reward model, as required for RLHF. To this end, the DPO objective is derived by substituting the globally optimal reward---obtained by solving \\cref{eq:rlhf}---in the negative log-likelihood (NLL) loss (\\cref{eq:NLL}).\n\nThis success spurred on \\citet{wallace2023diffusion} to bring the same benefits of DPO to diffusion networks. However, unlike DPO, the goal for diffusion networks is to maximize the following learning objective (\\cref{eq:diff-obj}) with a reward (\\cref{eq:diff-reward}) defined on the entire diffusion path $x_{0:N}$:\n\\begin{flalign}\n    \\max_{\\pi_\\theta} \\mathbb{E}_{\\tau \\sim \\mathcal{D}, x_{0:N}\\sim \\pi_\\theta(x_{0:N} | \\tau)} &[r(\\tau, x_0)] \\nonumber \\\\\n    - \\beta D_\\text{KL} &[ \\pi_\\theta(x_{0:N} | \\tau) || \\pi_\\text{ref}(x_{0:N}|\\tau)]. \\label{eq:diff-obj}\\\\\n    r(\\tau, x_0) := \\mathbb{E}_{\\pi_\\theta(x_{1:N} | x_0, \\tau)} &[R(\\tau, x_{0:N})], \\label{eq:diff-reward}\n\\end{flalign}\nSolving this objective and substituting the optimal reward in the NLL loss (\\cref{eq:NLL}) yields the following DPO objective for diffusion:\n\\begin{flalign}\n    &\\mathcal{L}_\\text{DPO-Diff} = - \\mathbb{E}_{(\\tau, x_0^w, x_0^l)\\sim \\mathcal{D_\\text{pref}}} \\log \\sigma(\\nonumber \\\\ \n    &\\beta \\mathbb{E}_{x^*_{1:N}\\sim \\pi_\\theta(x^*_{1:N}|x^*_0, \\tau)}[\\log \\frac{\\pi_\\theta(x^w_{0:N}|\\tau)}{\\pi_\\text{ref}(x^w_{0:N}|\\tau)} - \\log \\frac{\\pi_\\theta(x^l_{0:N}|\\tau)}{\\pi_\\text{ref}(x^l_{0:N}|\\tau)}]).\n\\end{flalign}\nNow, applying Jensen's inequality by taking advantage of the convexity of $-\\log \\sigma$ allows the inner expectation to be pushed outside. Subsequently, approximating the denoising process with the forward process yields the following final form in terms of the L2 noise-estimation losses from LDM (\\cref{eq:LDM-loss}):\n\\begin{flalign}\n    \\mathcal{L}_\\text{DPO-Diff} := &- \\mathbb{E}_{n, \\epsilon^w, \\epsilon^l} \\log \\sigma(-\\beta N \\omega(\\lambda_n) (||\\epsilon_n^w - \\hat\\epsilon_\\theta^{(n)}(x_n^w, \\tau)||_2^2 \\nonumber \\\\ &- ||\\epsilon_n^w - \\hat\\epsilon_\\text{ref}^{(n)}(x_n^w, \\tau)||_2^2 \\nonumber \\\\ &- (||\\epsilon_n^l - \\hat\\epsilon_\\theta^{(n)}(x_n^l, \\tau)||_2^2 - ||\\epsilon_n^l - \\hat\\epsilon_\\text{ref}^{(n)}(x_n^l, \\tau)||_2^2)), \\label{eq:DPO-Diff}\n\\end{flalign}\nwhere $\\mathcal{D}_\\text{pref}:= \\{(\\tau, x_0^w, x_0^l)\\}$ is our preference dataset \\dataset{}, $\\tau$, $x^w_0$, and $x^l_0$ being the input prompt, preferred, and undesirable output, respectively. Furthermore, $n\\sim \\mathcal{U}(0, N)$ is the diffusion step, $\\epsilon_n^*\\sim \\mathcal{N}(0, \\mathbb{I})$ and $x_n^*$ are noise and noisy posteriors, respectively, at some step $n$. $\\lambda_n$ is the signal-to-noise ratio (SNR) and $\\omega(\\lambda_n)$ is a weighting function defined on SNR. We use \\textsf{Tango-full-FT} as our reference model through its noise estimation $\\hat\\epsilon_\\text{ref}$. \n\n\\section{Experiments}\n\n\\subsection{Datasets and Training Details}\n\nWe fine-tuned our model starting from the \\textsf{Tango-full-FT} checkpoint on our preference dataset \\dataset{}.\n\nAs mentioned earlier in \\Cref{sec:audio_alpaca_creation}, we have a total of 15,025 preference pairs in \\dataset{}, which we use for fine-tuning. We use AdamW~\\cite{loshchilov2017decoupled} with a learning rate of 9.6e-7 and a linear learning-rate scheduler for fine-tuning. Following \\citet{wallace2023diffusion}, we set the $\\beta$ in DPO loss (\\cref{eq:DPO-Diff}) to 2000. We performed 1 epoch of supervised fine-tuning on the prompt and the preferred audio as training samples, followed by 4 epochs of DPO. The entirety of the fine-tuning was executed on two A100 GPUs which takes about 3.5 hours in total. We use a per GPU batch size of 4 and a gradient accumulation step of 4, resulting in an effective batch size of 32.\n\n\\subsection{Baselines}\nWe primarily compare \\model{} to three strong baselines:\n\n\\begin{enumerate}[leftmargin=0.75cm]\n    \\item \\textsc{\\bf AudioLDM}~\\cite{liu2023audioldm}: A text-to-audio model that uses CLAP~\\cite{wu2023large}, a joint audio-text representation model, and a latent diffusion model (LDM). Specifically, the LDM is trained to generate the latent representations of melspectrograms obtained from a pre-trained Variational Autoencoder (VAE). During diffusion, CLAP text-embeddings guide the generation process.\n    \n    \\item \\textsc{\\bf AudioLDM2}~\\cite{liu2023audioldm2}: An any-to-audio framework which uses language of audio (LOA) as a joint encoding of audio, text, image, video, and other modalities. Audio modality is encoded into LOA using a self-supervised masked auto-encoder. The remaining modalities, including audio again, are mapped to LOA through a composition of GPT-2~\\cite{radford2019language} and ImageBind~\\cite{girdhar2023imagebind}. This joint encoding is used as a conditioning in the diffusion network for audio generation.\n    \n    \\item \\textsc{\\bf Tango}~\\cite{ghosal2023text}: Utilizes the pre-trained VAE from AudioLDM but replaces the CLAP text-encoder with an instruction-tuned large language model: FLAN-T5. As compared to AudioLDM, its data-augmentation strategy is also cognizant of the audio pressure levels of the source audios. These innovations attain comparable or superior results while training on a significantly smaller dataset.\n\\end{enumerate}\n\n\\textsc{Baton}~\\cite{liao2024baton} represents another recent approach in human preference based text-to-audio modeling. It trains a reward model to maximize rewards through supervised fine-tuning, aiming to maximize the probability of generating audio from a textual prompt. As discussed in \\Cref{sec:related-work}, \\textsc{Baton}'s reward model is not trained using the pairwise preference objective presented in \\Cref{eq:NLL}. In this approach, each text ($\\tau$) and audio ($x$) pair is classified as 1 or 0, indicating whether human annotators favored the text-audio pair or not. Subsequently, this reward is incorporated into the generative objective function of the diffusion. This methodology stands in contrast to the prevailing approach in LLM alignment research. As of now, neither the dataset nor the code has been made available for comparison.\n\\begin{table*}[t]\n\\centering\n\\caption{Text-to-audio generation results on AudioCaps evaluation set. Due to time and budget constraints, we could only subjectively evaluate AudioLDM 2-Full-Large and Tango-full-FT. Notably these two models are considered open-sourced SOTA models for text-to-audio generation as reported in \\cite{vyas2023audiobox}.}\n\n\\begin{tabular}{l|c|cccc|cccc|cc}\n\\toprule\n\\multirow{2}{*}{Model} & \\multirow{2}{*}{\\#Parameters} & \\multicolumn{4}{c|}{Objective -- Holistic} & \\multicolumn{4}{c|}{Objective -- Temporal} & \\multicolumn{2}{c}{Subjective} \\\\\n&  & FAD $\\downarrow$ &  KL $\\downarrow$ & IS $\\uparrow$ & CLAP $\\uparrow$ & OER $\\downarrow$ & DUR $\\downarrow$ & FREQ $\\downarrow$ & TIME $\\uparrow$ & OVL $\\uparrow$ & REL $\\uparrow$  \\\\\n\\midrule\n\\textsc{AudioLDM-M-Full-FT}  & $416$M  & $2.57$  & $1.26$ &  $8.34$ & $0.43$\n & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\\\\n\\textsc{AudioLDM-L-Full}  & $739$M  & $4.18$  & $1.76$ &  $7.76$ & $0.43$\n & $-$ & $-$ & $-$ & $-$ &$-$ & $-$\\\\\n\\midrule\n\n\\textsc{AudioLDM 2-Full} & $346$M & \\textbf{$2.18$} & $1.62$ & $6.92$ & $0.43$  & $-$ & $-$ & $-$ & $-$ & $-$ & $-$  \\\\\n\\textsc{AudioLDM 2-Full-Large}  &  $712$M & $\\textbf{2.11}$ & $1.54$ & $8.29$ & $0.44$  & $-$ & $-$ & $-$ & $-$ & $3.56$ & $3.19$  \\\\\n\n\\midrule\n\n\\textsc{Tango-full-FT}  & $866$M & $2.51$ & $1.15$ & $7.87$ & $0.54$ & 0.882 & \\bf 3.535 & 1.611 & 0.577 & $3.81$ & $3.77$  \\\\\n\\model{}  & $866$M & $2.69$ & $\\textbf{1.12}$ & $\\textbf{9.09}$ & $\\textbf{0.57}$ & \\bf 0.87 & 3.586 & \\bf 1.548 & \\bf 0.61 & $\\textbf{3.99}$ & $\\textbf{4.07}$  \\\\\n\\quad w/o Strategy 2 \\& 3   & $866$M & $2.64$ & $1.13$ & $8.06$ & $0.54$  & $-$ & $-$ & $-$ & $-$ & $-$ & $-$  \\\\\n\\quad w/o Strategy 1   & $866M$ & $2.47$ & $1.13$ & $8.58$ & $0.56$  & $-$ & $-$ & $-$ & $-$ & $-$ & $-$  \\\\\n\\quad w/o Strategy 2   & $866$M & $2.28$ & $\\textbf{1.12}$ & $8.38$ & $0.55$  & $-$ & $-$ & $-$ & $-$ & $-$ & $-$  \\\\\n\\quad w/o Strategy 3   & $866$M & $2.46$ & $1.13$ & $8.63$ & $0.56$  & $0.88$ & $3.63$ & $1.577$ & $0.588$ & $-$ & $-$  \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:main-results}\n\\end{table*}\n\n\\subsection{Evaluation Metrics}\n\n\\paragraph{\\bf Holistic Objective Metrics} We evaluate the generated audio samples in a holistic fashion using the standard Frechet Audio Distance (FAD), KL divergence, Inception score (IS), and CLAP score \\cite{liu2023audioldm}. \\emph{FAD} is adapted from Frechet Inception Distance (FID) and measures the distribution-level gap between generated and reference audio samples. \\emph{KL divergence} is an instance-level reference-dependent metric that measures the divergence between the acoustic event posteriors of the ground truth and the generated audio sample. FAD and KL are computed using PANN, an audio-event tagger. \\emph{IS} evaluates the specificity and coverage of a set of samples, not needing reference audios. IS is inversely proportional to the entropy of the instance posteriors and directly proportional to the entropy of the marginal posteriors. \\emph{CLAP score} is defined as the cosine similarity between the CLAP encodings of an audio and its textual description. We borrowed the AudioLDM evaluation toolkit~\\cite{liu2023audioldm} for the computation of FAD, IS, and KL scores.\n\n\\paragraph{\\bf Temporal Objective Metrics} To specifically evaluate the temporal controllability of the text-to-audio models, we employ the recently-proposed STEAM~\\cite{audiotime} metrics measured on the AudioTime~\\cite{audiotime} benchmark dataset containing temporally-aligned audio-text pairs. STEAM constitutes four temporal metrics: (i) \\emph{Ordering Error Rate} (OER) -- if a pair of events in the generated audio matches their order in the text, (ii) \\emph{Duration} (DUR) / (iii) \\emph{Frequency} (FREQ) -- if the duration/frequency of an event in the generated audio matches matches the given text, (iv) \\emph{Timestamp} (TIME) -- if the on- and off-set timings of an event in the generated audio match the given text.\n\n\\paragraph{\\bf Subjective Metrics} Our subjective assessment examines two key aspects of the generated audio: overall audio quality (OVL) and relevance to the text input (REL), mirroring the approach outlined in the previous works, such as, \\cite{ghosal2023text,vyas2023audiobox}. The OVL metric primarily gauges the general sound quality, clarity, and naturalness irrespective of its alignment with the input prompt. Conversely, the REL metric assesses how well the generated audio corresponds to the given text input. Annotators were tasked with rating each audio sample on a scale from 1 (least) to 5 (highest) for both OVL and REL. This evaluation was conducted on a subset of 50 randomly-selected prompts from the AudioCaps test set, with each sample being independently reviewed by at least four annotators. Please refer to the supplementary material for more details on the evaluation instructions and evaluators.\n\n\\begin{table*}[t]\n\\centering\n\\caption{Objective evaluation results for audio generation in the presence of multiple concepts or a single concept in the text prompt in the \\texttt{AudioCaps} test set. }\n\\resizebox{0.8\\linewidth}{!}{\n\\begin{tabular}{l|cccc|cc|cccc|cc}\n\\toprule\n\\multirow{3}{*}{Model} & \\multicolumn{6}{c|}{Multiple Events/Concepts} & \\multicolumn{6}{c}{Single Event/Concept} \\\\\n & \\multicolumn{4}{c|}{Objective -- Holistic} & \\multicolumn{2}{c|}{Subjective} & \\multicolumn{4}{c|}{Objective -- Holistic} & \\multicolumn{2}{c}{Subjective} \\\\\n& FAD $\\downarrow$ &  KL $\\downarrow$ & IS $\\uparrow$ & CLAP $\\uparrow$ & OVL $\\uparrow$ & REL $\\uparrow$ & FAD $\\downarrow$ &  KL $\\downarrow$ & IS $\\uparrow$ & CLAP $\\uparrow$ & OVL $\\uparrow$ & REL $\\uparrow$ \\\\\n\\midrule\nAudioLDM 2-Full  & \\textbf{2.03} & 1.64 & 7.88 & 0.43 &$-$&$-$\n& 7.93 & 1.24 & 4.50 & 0.47 &$-$&$-$ \\\\\nAudioLDM 2-Full-Large & 2.33 & 1.58 & 8.14 & 0.44&3.54&3.16\n& 5.82 & 1.09 & 4.60 & 0.49 &3.65&3.41 \\\\\nTango-full-FT & 2.69 & 1.16 & 7.85 & 0.54 &3.83&3.80\n& 7.52 & 1.01 &  4.87 & 0.51 &3.67&3.49 \\\\\n\\model{}& 2.60 & \\textbf{1.11} & \\textbf{8.98} & \\textbf{0.57 } &\\textbf{3.99}&\\textbf{4.07}\n& \\textbf{5.48} & \\textbf{1.00} &  \\textbf{4.95} & \\textbf{0.52} &\\textbf{3.95}&\\textbf{4.10} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\label{tab:event-analysis}\n\\end{table*}\n\n\\begin{table*}[t]\n\\centering\n\\caption{Objective evaluation results for audio generation in the presence of temporal events or non-temporal events in the text prompt in the AudioCaps test set. }\n\\resizebox{0.8\\linewidth}{!}{\n\\begin{tabular}{l|cccc|cc|cccc|cc}\n\\toprule\n\\multirow{3}{*}{Model} & \\multicolumn{6}{c|}{Temporal Events} & \\multicolumn{6}{c}{Non Temporal Events} \\\\\n & \\multicolumn{4}{c|}{Objective -- Holistic} & \\multicolumn{2}{c|}{Subjective} & \\multicolumn{4}{c|}{Objective -- Holistic} & \\multicolumn{2}{c}{Subjective} \\\\\n & FAD $\\downarrow$ &  KL $\\downarrow$ & IS $\\uparrow$ & CLAP $\\uparrow$ & OVL $\\uparrow$ & REL $\\uparrow$ & FAD $\\downarrow$ &  KL $\\downarrow$ & IS $\\uparrow$ & CLAP $\\uparrow$ & OVL $\\uparrow$ & REL $\\uparrow$ \\\\\n\\midrule\nAudioLDM 2-Full  & \\textbf{1.95} & 1.71 & 6.37 & 0.41 & $-$ & $-$\n& \\textbf{2.38} & 1.56 & 7.38 & 0.44 & $-$ & $-$  \\\\\nAudioLDM 2-Full-Large & 2.39 & 1.65 & 6.10 & 0.42  & 3.35 & 2.82\n& 2.68 & 1.46 & 8.12 & 0.46 & 3.79 & 3.62 \\\\\nTango-full-FT & 2.55 & 1.16 & 5.82 & 0.55 & 3.83 & 3.67 \n& 3.04 & \\textbf{1.15} &  7.70 & 0.53 & 3.78 & 3.88 \\\\\n\\model{}& 3.29 & \\textbf{1.07} & \\textbf{6.88} &\\textbf{0.58} & \\textbf{3.92} & \\textbf{3.99}\n& 2.84 & 1.16 &  \\textbf{8.62} & \\textbf{0.55}  & \\textbf{4.05} & \\textbf{4.16} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\label{tab:temporal-analysis}\n\\end{table*}\n\\subsection{Main Results}\n\nWe report the comparative evaluations of \\model{} against the baselines \\textsc{Tango}~\\cite{ghosal2023text} and \\textsc{AudioLDM2}~\\cite{liu2023audioldm2} in \\cref{tab:main-results}. For a fair comparison, we used exactly 200 inference steps in all our experiments. \\textsc{Tango} and \\model{} were evaluated with a classifier-free guidance scale of 3 while AudioLDM2 uses a default guidance scale of 3.5. We generate only one sample per text prompt.\n\n\\paragraph{\\bf Objective Evaluations}\n\n\\model{} achieves notable improvements in objective metrics, with scores of 2.69 for FAD, 1.12 for KL, 9.09 for IS, and 0.57 for CLAP. While FAD, KL, and IS assess general naturalness, diversity, and audio quality, CLAP measures the semantic alignment between the input prompt and the generated audio. As documented in \\citet{melechovsky2024mustango}, enhancing audio quality typically relies on improving the pre-training process of the backbone, either through architectural modifications or by leveraging larger or refined datasets. However, in our experiments, we observe enhanced audio quality in two out of three metrics, specifically KL and IS. Notably, \\model{} also significantly outperforms various versions of \\textsc{AudioLDM} and \\textsc{AudioLDM2} on these two metrics. \n\nOn the other hand, we note a substantial enhancement in the CLAP score. CLAP score is particularly crucial in our experimental setup as it directly measures the semantic alignment between the textual prompt and the generated audio. This outcome suggests that DPO-based fine-tuning on the preference data from \\dataset{} yields superior audio generation to \\textsc{Tango} and \\textsc{AudioLDM2}.\n\nA major enhancement in \\model{} is evident in the temporal objective metrics, as measured by STEAM. With the exception of \\emph{Duration}, \\model{} shows consistent superiority over \\textsc{Tango} across all other temporal measurements. The implementation of Strategy 3 in \\dataset{} plays a crucial role in temporal data augmentation. Our findings reveal that the absence of this augmentation leads to a decline in \\model{}'s temporal performance, thus highlighting the effectiveness of Strategy 3-based data augmentation technique.\n\n\\paragraph{\\bf Subjective Evaluations}\nIn our subjective evaluation, \\model{} achieves high ratings of 3.99 in OVL (overall quality) and 4.07 in REL (relevance), surpassing both \\textsc{Tango} and \\textsc{AudioLDM2}. This suggests that \\model{} significantly benefits from preference modeling on \\dataset{}. Interestingly, our subjective findings diverge from those reported by \\citet{melechovsky2024mustango}. In their study, the authors observed lower audio quality when \\textsc{Tango} was fine-tuned on music data. However, in our experiments, the objective of preference modeling enhances both overall sound quality and the relevance of generated audio to the input prompts.\nNotably, in our experiments, \\textsc{AudioLDM2} performed the worst, with the scores of only 3.56 in OVL and 3.19 in REL, significantly lower than both \\textsc{Tango} and \\model{}.\n\n\\begin{table*}[ht!]\n    \\centering\n    \\rowcolors{1}{gray!20}{gray!10} % Set background colors for odd and even rows\n    \\caption{GPT-4 prompt used to extract events or concepts from audio prompts.}\n    \\begin{tabular}{|p{15cm}|} % Define table with one centered column\n        \\hline\n        You are to extract all the indivisible events in the given text, labeled as input. Imagine experiencing the events in the input as you are reading it and write down the indivisible events one by one. After writing your experience, list all the events in the sequence you observed them as a python list. Think step-by-step. Do not directly give the answer. Please refer to these following examples as refernce for input and output:\\\\\n    \\textcolor{blue}{\\textit{Example 1 -}}\\\\\n    \\textcolor{blue}{\\textbf{Input}: An aircraft engine runs and vibrates, metal spinning and grinding occur, and the engine accelerates and fades into the distance}\\\\\n    \\textcolor{blue}{\\textbf{Output}: Firstly, an aircraft engine runs and vibrates. Then, I hear metal spinning and grinding. Then, the aircraft engine accelerates. Finally, the aircraft fades into the distance.}\\\\\n    \\textcolor{blue}{So, here is the list of events that I observed:}\\\\\n    \\textcolor{blue}{\\texttt{[\"aircraft engine runs\", \"aircraft engine vibrates\", \"metal spinning\", \"metal grinding\", \"aircraft engine acclerates\", \"aircraft fades into the distance\"]}}\\\\\n    \\textcolor{purple}{\\textit{Example 2 -}}\\\\\n    \\textcolor{purple}{\\textbf{Input}: Bubbles gurgling and water spraying as a man speaks softly while crowd of people talk in the background}\\\\\n    \\textcolor{purple}{\\textbf{Output}: Firstly, I hear bubble gurgling. Also, I hear water spraying. Simultaneously, a man is speaking softly. Also, a crowd of people are talking in the background. }\\\\\n    \\textcolor{purple}{So, here is the list of events that I observed:}\\\\\n    \\textcolor{purple}{\\texttt{[\"bubble gurgling\", \"water spraying\", \"a man is speaking softly\", \"crowd talking\"]}}\\\\\n   \\textcolor{red}{\\textit{Example 3 -}}\\\\\n    \\textcolor{red}{\\textbf{Input}: A man talking then meowing and hissing}\\\\\n    \\textcolor{red}{\\textbf{Output}: Firstly, I hear a man talking. Subsequently, I hear meowing. I also hear hissing.}\\\\\n    \\textcolor{red}{So, here is the list of events that I observed:}\\\\\n    \\textcolor{red}{\\texttt{[\"a man talking\", \"meowing\", \"hissing\"]}}\\\\\n    **** Examples end here\\\\\n    Now, given the input text below extract all the indivisible events one by one as explained above with examples. Also, remember to follow the exact format of the examples.\\\\\n    \\textbf{Input}: \\textcolor{red}{\\{PROMPT\\}}\\\\\n    \\textbf{Output}: \\\\\n        \\hline\n    \\end{tabular}\n    \\label{tab:gpt4-event}\n\\end{table*}\n\nAdditionally, we categorize prompts based on the presence of multiple concepts or events, exemplified by phrases like \\textit{``\\ul{A woman speaks} while \\ul{cooking}''}. As underlined, this prompt contains two distinct events i.e., \\emph{``sound of a woman speaking''} and \\emph{``sound of cooking''}. Through manual scrutiny, we discovered that pinpointing prompts with such multi-concepts is challenging using basic parts-of-speech or named entity-based rules. Consequently, we task GPT-4 with extracting the various concepts or events from the prompts using in-context exemplars. The specific prompt is displayed in \\Cref{tab:gpt4-event}. To evaluate GPT-4's performance on this task, we randomly selected 30 unique prompts and manually verified their annotations from GPT-4's. No errors attributable to GPT-4 were found. In general, \\model{} outperforms \\textsc{AudioLDM2} and \\textsc{Tango} across most objective and subjective metrics, following \\Cref{tab:event-analysis}. We proceed to visualize the CLAP scores of the models in \\Cref{fig:event-analysis}. This visualization illustrates that \\model{} consistently outperforms the baselines as the number of events or concepts per prompt increases. In particular, specifically, \\textsc{Tango} closely matches the performance of \\model{} only when the textual prompt contains a single concept. However, the disparity between these two models widens as the complexity of the prompt increases with multiple concepts.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{images/tango2-events.png}\n    \\caption{CLAP score of the models vs the number of events or concepts in the textual prompt.\n    }\n    \\label{fig:event-analysis}\n\\end{figure}\nThe supremacy of \\model{} over the baselines in both of these cases can be strongly ascribed to DPO training the diffusion model the differences between generating a preferred and an undesirable audio output. Particularly, the undesirable audio outputs with missing concepts and wrong temporal orderings of events are penalized. Conversely, the preferred audio samples with the correct event orderings and presence are promoted by the noise-estimation terms in the DPO-diffusion loss (\\cref{eq:DPO-Diff}).\n\n\\paragraph{\\bf Ablations on \\dataset{}.} We conducted an ablation study on \\dataset{} to gauge the impact of different negative data construction strategies. As shown in \\Cref{tab:main-results}, excluding the data samples from by \\emph{strategies 2 and 3} notably diminishes the performance of \\model{}. This underscores the significance of event and temporal prompt perturbations.\n\n\\paragraph{\\bf The Effect of Filtering.}\nIn our experiments, we noticed that filtering to create different \\dataset{} can impact the performance (refer to \\Cref{sec:pref-dataset}).  \\Cref{fig:filtering} depicts the impact of this filtering process. We found setting $\\Delta_2 \\ge 0.08$, and $\\alpha_2 \\ge 0.6$ gives the best results. \n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.95\\linewidth]{images/audio-alpaca-filtering.pdf}\n    \\caption{The impact of filtering \\dataset{} on performance observed through $\\Delta_2$, and $\\alpha_2$. The CLAP score of the winning audio must be at least $\\alpha_2$ and $\\Delta_2$ represents the difference in CLAP scores between the winning audio $x^w$ and the losing audio $x^l$ given a prompt $\\tau$. %We denote the set of winning audios as $\\{x^w\\}$, defined as $\\{x^w \\mid \\text{CLAP}(x^w) \\ge \\alpha_2\\}$ where $\\text{CLAP}(x^w)$ indicates the CLAP score of the winning sample $x^w$.\n    }\n    \\label{fig:filtering}\n\\end{figure}\n\n\\section{Conclusion}\nIn this work, we propose aligning text-to-audio generative models through direct preference optimization. To the best of our knowledge, this represents the first attempt to advance text-to-audio generation through preference optimization. We achieve this by automatically generating a preference dataset using a combination of Large Language Models (LLMs) and adversarial filtering. Our preference dataset, \\dataset{}, comprises diverse audio descriptions (prompts) paired with their respective preferred (winner) and undesirable (loser) audios. The preferred audios are expected to accurately reflect their corresponding textual descriptions, while the undesirable audios exhibit flaws such as missing concepts, incorrect temporal order, or high noise levels. To generate undesirable audios, we perturb the descriptions by removing or rearranging certain concepts and feeding them to Tango. Additionally, we employ adversarial filtering, generating multiple audios from the original prompt and selecting those with CLAP scores below a specified threshold. Subsequently, we align a diffusion-based text-to-audio model, \\textsc{Tango}, on \\dataset{} using DPO-diffusion loss. Our results demonstrate significant performance leap over the previous models, both in terms of objective and subjective metrics. We anticipate that our dataset, \\dataset{}, and the proposed model, \\model{}, will pave the way for further advancements in alignment techniques for text-to-audio generation.\n\\section*{Acknowledgements}\nThis research is supported by the Ministry of Education, Singapore,\nunder its AcRF Tier-2 grant (Project no. T2MOE2008, and Grantor\nreference no. MOE-T2EP20220-0017).\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{RiTTA: Modeling Event Relations in \\\\Text-to-Audio Generation}\n\n\\begin{document}\n\n\\maketitle\n\\vspace{-3mm}\n\\begin{abstract}\nDespite significant advancements in Text-to-Audio (TTA) generation models achieving high-fidelity audio with fine-grained context understanding, they struggle to model the relations between audio events described in the input text. However, previous TTA methods have not systematically explored audio event relation modeling, nor have they proposed frameworks to enhance this capability. In this work, we systematically study audio event relation modeling in TTA generation models. We first establish a benchmark for this task by: (1) proposing a comprehensive relation corpus covering all potential relations in real-world scenarios; (2) introducing a new audio event corpus encompassing commonly heard audios; and (3) proposing new evaluation metrics to assess audio event relation modeling from various perspectives. Furthermore, we propose a finetuning framework to enhance existing TTA models' ability to model audio events relation.\n\\end{abstract}\n\\vspace{-2mm}\n\\section{Introduction}\n\\vspace{-2mm}\n\\label{sec:intro}\n\nText-based crossmodal content generation has gained significant attention in recent years as it opens up new possibilities for even amateur users to create professional content. Typical such methods include text-to-image~(TTI)~\\citep{ho2020denoising}, text-to-music~(TTM)~\\citep{musicGEN}, text-to-point~(TTP)~\\citep{point_E}, text-to-speech~(TTS)~\\citep{fast_speech} text-to-audio~(TTA)~\\citep{audioldm2,makeanaudio}. Among all of them, text-to-audio~(TTA) generation stands out as a particularly promising area, enabling the synthesis of complex acoustic environments or soundscapes directly from textual descriptions. Recent advances in TTA have demonstrated impressive progress in generating high-quality, detail-rich audio described in the input text prompt~\\citep{audioldm2,liu2023audioldm,makeanaudio,makeanaudio_v2,ghosal2023tango,ghosal2023tango2,kreuk2022audiogen}.\n\n\\begin{wraptable}{r}{7.0cm}\n\\scriptsize\n\\begin{tabular}{l|cl}\n\\hline\n\\multicolumn{3}{c}{\\textit{\\makecell[c]{\\textbf{Text Prompt}: generate dog barking audio, \\\\followed by cat meowing audio}}}\\\\\n\\hline\nMethod & Relation? & Remark \\\\\n\\hline\nAudioLDM~(\\citeyear{liu2023audioldm}) & \\xmark & \\makecell[c]{just cat meow, low-fidelity}\\\\\nAudioLDM 2~(\\citeyear{audioldm2}) & \\xmark & output dog barking\\\\\nMakeAnAudio~(\\citeyear{makeanaudio}) & \\xmark & just cat meow, low-fidelity \\\\\nAudioGen~(\\citeyear{kreuk2022audiogen}) & \\xmark & output wrong audios \\\\\nTango~(\\citeyear{ghosal2023tango2}) & \\xmark & \\makecell[l]{two audios, low fidelity}\\\\\nTango 2~(\\citeyear{ghosal2023tango2}) & \\xmark & \\makecell[l]{can output two audios}\\\\\n\\hline\n\\end{tabular}\n\\vspace{-3mm}\n\\caption{\\small A case study on relation of TTA methods. Listenable audios are provided in suppplementary material.}\n\\label{tab:relation_test}\n\\end{wraptable} \n\nWhen perceiving the physical world acoustically, whether through text or audio, the fundamental unit is the audio event, a distinct acoustic signal representing an independent source. The essence of perception lies in understanding the relationships emerging from events. Audio events are spatiotemporally distributed in the physical world. Together with relation, they contribute for holistic acoustic scene understanding~\\citep{acoustic_scene_classify}. Studies in psychology~\\citep{Zacks_Speer_Swallow_Braver_Reynolds_2007} and neuroscience~\\citep{human_level_concept,hirsh1967brain} show that the human brain perceives the environment through discrete events and the relations between them. Humans are adept at using rich language to describe both audio events and their intricate relationships. While current TTA models can generate audios with high fidelity, their ability to generate audios that not only includes audio events but also preserves the text-informed relationships between them remains unexplored.\n\nAs a primary study, we prompt the latest six TTA models with an exemplar text with explicit audio events and their relation \\textit{generate dog barking audio, followed by cat meowing audio}. Next we check if the specified audio events are present and if so, their relations are correct in the generated audios. As is shown in Table~\\ref{tab:relation_test}, all existing TTA models fail to properly model temporal relationships in the generated audio, even when they succeed in generating the correct audio events. The generated audio waveform, spectrum and another case study with a much complex text are shown in Fig.~\\ref{fig:teasing_fig}. The poor performance of current TTA models in modeling audio events relation, along with the lack of systematic discussion on this topic, motivates us to explore \\emph{Relation in TTA}~(dubbed \\emph{RiTTA}) in depth in this work. We visualize the motivation in Fig.~\\ref{fig:teasing_fig}.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.98\\linewidth]{figs/ritta_teasing_fig.pdf}\n    \\vspace{-2mm}\n    \\caption{\\small \\emph{RiTTA} Motivation: The acoustic world is rich with diverse audio events that exhibit various relationships. While text can precisely describe these relationships~(Fig.~A), current TTA models struggle to capture both the audio events and the relations conveyed by the text~(Fig.~B). This challenge motivates us to systematically study \\emph{RiTTA}.}\n    \\label{fig:teasing_fig}\n    \\vspace{-3mm}\n\\end{figure}\n\nTo systematically study \\emph{RiTTA}, we first benchmark it from four key perspectives: 1. we construct a comprehensive audio event relation corpus that captures common relationships found in the physical world. Unlike visual relations in cross-modal image tasks, which mainly focus on spatial aspects~(\\textit{e.g.}, left, bottom)~\\citep{gokhale2022benchmarking}, audio events exhibit far more complex relationships spanning spatial, temporal, and compositional dimensions. Consequently, we define four primary relation categories: \\emph{Temporal Order}, \\emph{Spatial Distance}, \\emph{Count}, and \\emph{Compositionality}. 2. Accompanying the relation corpus, we build an audio event category corpus derived from five main sources, each of which is further linked to multiple seed audios. 3. devise a \\texttt{<textprompt,audio>} pair generation strategy emphasising both text prompt and audio diversity. 4. propose a new relation aware evaluation framework that assesses the relation in a multi-stage manner. The proposed benchmark will benefit the community to explore \\emph{RiTTA} in greater depth. Additionally, we introduce a fine-tuning strategy based on the latest state-of-the-art~(SoTA) TTA model and demonstrate its effectivenss in relation modelling. In summary, we make the following four main contributions:\n\n\\vspace{-2mm}\n\\begin{enumerate}[leftmargin=*]\n    \\item We conduct an extensive evaluation of existing TTA models in modeling the audio events relations and demonstrate their inability to capture these relations in the generated audios.\n    \\item We benchmark \\emph{RiTTA} by constructing complete relation corpus, audio event category corpus, seed audio corpus. Combined with the \\texttt{<textprompt,audio>} pair generation strategy, researchers can create large, diverse dataset to further investigate the \\emph{RiTTA} task.\n    \\item We propose a new multi-stage relation aware evaluation framework, called \\emph{MSR-RiTTA}, which offers a more nuanced evaluation compared to existing TTA metrics, allowing researchers to quantitatively assess their methods from multiple angles.\n    \\item We introduce a fine-tuning strategy leveraging the new dataset, demonstrating improvement over current SoTA methods. \n\\end{enumerate}\\vspace{-3mm}\n\\section{Related Work}\n\\vspace{-3mm}\n\n\\textbf{Audio Generation} has received lots of attention and made significant progress in recent years, advanced by fast-progressing generative AI technologies~\\citep{ho2020denoising,stable_diffusion}. Audio generation encompasses sub-tasks such as text-to-speech~(TTS) that focuses on generating speech from text transcription~(\\textit{e.g.}, FastSpeech~\\citep{fast_speech} and GradTTS~\\citep{grad_tts}), text-to-music~(TTM) that generates music from text input~(\\textit{e.g.}, MusicLM~\\citep{musicLM}, MusicGen~\\citep{musicGEN}) and Image-to-Audio~(ITA) generation that generates audio from image input~(\\textit{e.g.}, Img2Wav~\\citep{img2wav}, SpecVQGAN~\\citep{SpecVQGAN_Iashin_2021}, RegNet~\\citep{chen2020regnet}) and Text-to-Audio~(TTA) generation aiming to generate corresponding audio described by text~(\\textit{e.g.}, AudioLDM~\\citep{audioldm2,liu2023audioldm,yang2022diffsound}, DiffSound~\\citep{yang2022diffsound}).\n\n\\textbf{Text-to-Audio~(TTA) Generation} involves producing audio that faithfully reflects the acoustic content or behavior described by the input text. Recent advancements have significantly improved the quality and intelligibility of generated audio~\\citep{audioldm2,liu2023audioldm,kreuk2022audiogen,yang2022diffsound,ghosal2023tango,liao2024baton}. Despite improvements in audio quality and intelligibility, existing TTA methods still lag significantly in their ability to model relationships between audio events in the generated audio. AudioLDM~\\citep{liu2023audioldm} builds on latent space~\\citep{stable_diffusion} to learn continuous representation.\n\n\\textbf{Audio Events Relation Modeling}. In the context of environmental audio, a set of audio events exhibit relationships that are crucial for holistic acoustic scene understanding. \nBased on how audio interact with the physical world in space, time and perceptual aspects, the resulting audio events exhibit complex relationships in spatial, temporal and compositional aspects. Prior work has partially addressed modeling certain temporal relations (\\textit{e.g.}, order) in TTA~\\citep{audiotime} and compositional reasoning~\\citep{ghosh2024compa} for discriminative tasks, such as audio classification and audio-text retrieval. WavJourney~\\citep{liu2023wavjourney} leverages a large language model alongside multiple audio generation models to achieve compositional audio generation. However, its limitations include an artificial post-mixing process, which may result in generated audio lacking smooth transitions across event boundaries and inefficiencies in inference. While prior research has touched on modeling audio event relations, their potential in TTA remains largely underexplored. If we analogize an audio event to an object in image, the corresponding relationships exhibited in an image are mainly limited to 2D spatial relationship~(\\textit{e.g.}, before, bottom, left). Despite object of interest spatial relationship learning and evaluation have received lots of attention in recent years~\\citep{Krishna2016VisualGC,gokhale2022benchmarking,compos_ability}, the research on audio event relation modeling has been almost ignored.\\section{Benchmark TTA Audio Events Relation}\n\\vspace{-3mm}\n\nIn this section, we sequentially present audio events relation corpus in Sec.~\\ref{sec:relation_corpus}, audio event category corpus in Sec.~\\ref{sec:audio_event_category}, seed audio corpus and \\texttt{<textprompt,audio>} pair generation strategy in Sec.~\\ref{sec:seed_audio_corpus}. Finally, the relation aware evaluation framework \\emph{MSR-RiTTA} is presented in Sec.~\\ref{sec:relation_eval}.\n\\vspace{-2mm}\n\n\\begin{figure}[t]\n\\begin{minipage}[t]{.5\\linewidth}\n\\centering\n\\small\n    \\renewcommand{\\arraystretch}{0.5}\n    \\begin{tabular}{m{1.2cm}|m{1.4cm}|m{3.5cm}}\n    \\hline\n    \\makecell[c]{Main\\\\ Relation} & \\makecell[c]{Sub-\\\\Relation} & \\makecell[c]{Sample Text Prompt} \\\\\n    \\hline\n       \\makecell[c]{Temporal \\\\ Order}  &  \\makecell[c]{before;\\\\ after;\\\\ simultaneity} & \\makecell[c]{generate dog barking audio,\\\\ followed by cat meowing;}\\\\\n       \\hline\n       \\makecell[c]{Spatial \\\\Distance} & \\makecell[c]{close first;\\\\far first;\\\\ equal dist.} & \\makecell[c]{generate dog barking audio\\\\ that is 1 meter away, follow-\\\\ed by another 5 meters away.}\\\\\n       \\hline\n      \\makecell[c]{Count}& \\makecell[c]{count} & \\makecell[c]{produce 3 audios: dog bark-\\\\ing, cat meowing and talking.}\\\\\n       \\hline\n       \\makecell{Composit\\\\ionality} & \\makecell[c]{and; or;\\\\ not;\\\\ if-then-else} & \\makecell[c]{create dog barking audio\\\\ or cat meowing audio.}\\\\\n       \\hline\n    \\end{tabular}\n    \\vspace{-1mm}\n    \\captionof{table}{\\small Audio Events Relation Corpus.}\n    \\label{tab:relation_corpus}\n\\end{minipage}\\quad\n\\hspace{2mm}\n\\begin{minipage}[t]{.4\\linewidth}\n\\centering\n\\small\n    \\begin{tabular}{m{1.7cm}|m{4.cm}}\n    \\hline\n    \\makecell[c]{Main\\\\Category} & \\makecell[c]{Sub-Category} \\\\ \n    \\hline\n       \\makecell[c]{Human\\\\ Audio}  &  baby crying; talking; laughing; coughing; whistling \\\\\n       \\hline\n       \\makecell[c]{Animal\\\\ Audio} & cat meowing; bird chirping; dog barking; rooster crowing; sheep bleating\\\\\n       \\hline\n       \\makecell[c]{Machinery} & boat horn; car horn; door bell; paper shredder; telephone ring\\\\\n       \\hline\n       \\makecell[c]{Human-Object\\\\ Interaction} &vegetable chopping; door slam; footstep; keyboard typing; toilet flush\\\\\n       \\hline\n        \\makecell[c]{Object-Object\\\\ Interaction} & emergent brake; glass drop; hammer nailing; key jingling; wood sawing\\\\\n       \\hline\n    \\end{tabular}\n    \\vspace{-1mm}\n    \\captionof{table}{\\small Audio Events Category Corpus.}\n    \\label{tab:event_corpus}\n\\end{minipage}\n\\vspace{-4mm}\n\\end{figure}\n\n\\vspace{-2mm}\n\\subsection{Audio Event Relation Corpus}\n\\vspace{-2mm}\n\\label{sec:relation_corpus}\n\nAn audio event refers to a distinct acoustic signal occurrence with specific frequency, duration and context characteristics that can be attributed to distinguish an independent sound source~\\citep{sounddet} in an environment. Audio event is ubiquitous in the physical world and serves as the fundamental entity to analyze and interpret the acoustic scene. We embrace the audio event as the fundamental element to construct the relation corpus.\n\nWe construct the audio events relation corpus based on two key aspects. First, we consider relations commonly found in the physical world, such as those arising from spatial and temporal variations, which test TTA models' ability to replicate audio events' interactions in real-world scenarios. Second, we focus on relations that challenge TTA models' logical reasoning, evaluating their ability to determine both which audio events to generate and how to generate them. These two aspects partially overlap. Specifically, we define five main audio event categories, each associated with five subcategories of audio events. The detailed relation corpus is provided in Table~\\ref{tab:relation_corpus}, including,\n\n\\vspace{-2mm}\n\\begin{enumerate}[leftmargin=*]\n    \\item \\textbf{Number Count}: The number of audio events included in the generated audio, testing TTA models' ability to address acoustic polyphony challenge.\n    \\item \\textbf{Temporal Order}: Temporal order refers to the sequence of audio events in the generated audio. We include three basic temporal relations for two audio events: \\texttt{before}, \\texttt{after}, and \\texttt{simultaneity}, testing the TTA models' ability to distinguish and generate the correct event order as specified in the input text prompt.\n    \\item \\textbf{Spatial Distance}: Spatial distance refers to the variation in relative spatial distances inferred from the generated audio. It evaluates the TTA models' ability to capture the spatial distance differences specified in the text prompt. Since we focus on mono-channel audio, obtaining the absolute distance for each audio event is nearly impossible~\\citep{sounddet}. Therefore, we rely on loudness differences within intra-class audio events to verify their spatial distance variations.\n    \\item \\textbf{Compositionality}: Compositionality relation describes how multiple individual audio events are integrated together to form a complex auditory structure that specified in the input text prompt. It tests TTA models' logical reasoning capability in determining which audio events to generate and how to structure them, by following the compositional guidance illustrated in the input text prompt. Specifically, we incorporate four main compositionality relations: Conjunction~(\\texttt{And}, \\textit{e.g.}, generate audio A and audio B together); Disjunction~(\\texttt{Or}, \\textit{e.g.}, generate audio A or Audio B, not both); Negation~(\\texttt{Not}, exclude one particular audio event, \\textit{e.g.}, do not generate dog barking audio); Condition~(\\texttt{if-then-else}, either generate two audio events if the condition is met, otherwise generate the third audio if the condition is not met). \n\\end{enumerate}\n\nMost of the relations relate to two audio events~(see Table~\\ref{tab:event_corpus} for more detail). Expanding the corpus to include more complex relations with a greater number of audio events is left for future work.\n\n\\subsection{Audio Event Category Corpus}\n\\vspace{-2mm}\n\\label{sec:audio_event_category}\n\nAlongside the relation corpus presented in Sec.~\\ref{sec:relation_corpus}, we further construct a comprehensive audio event category corpus. The two corpora serve as fundamental dataset for constructing text prompts for TTA models. Since different audio event signals are generated from various sources or through different interactions, we first establish four main audio source categories, further detailing each category with five sub-categories. These constructed audio categories encompass the majority of ubiquitous audio events encountered in our daily lives. Specifically, the audio event category corpus contain,\n\n\\vspace{-2mm}\n\\begin{enumerate}[leftmargin=*]\n    \\item \\textbf{Human Audio}: the audio generated by human beings in our daily life, including \\textit{baby crying}, \\textit{coughing}, \\textit{laughing}, \\textit{whistling}, \\textit{female speech} and \\textit{male speech}.\n    \\item \\textbf{Animal Audio}: the audio generated by animals, including \\textit{cat meowing}, \\textit{dog barking}, \\textit{bird chirping}, \\textit{horse neighing}, \\textit{rooster crowing}, \\textit{sheep bleating} and \\textit{pig oinking}.\n    \\item \\textbf{Machinery Audio}: the audio generated by various machinery devices while they are working, including \\textit{car horn}, \\textit{doorbell}, \\textit{telephone ring}, \\textit{paper shredder} and \\textit{boat horn}.\n    \\item \\textbf{Human-Object Interaction Audio}: human-object interaction audios include \\textit{vegetable chopping}, \\textit{keyboard typing}, \\textit{toilet flushing}, \\textit{door slamming} and \\textit{foot step}.\n    \\item \\textbf{Object-Object Interaction Audio}: we further incorporate object-object interaction audios, including \\textit{glass dropping}, \\textit{car emergency brake}, \\textit{hammering nail}, \\textit{wood sawing} and \\textit{keys jingling}.\n\\end{enumerate}\nThe detailed audio event corpus is given in Table~\\ref{tab:event_corpus}. With the constructed relation and audio event corpus, we can create relation aware text prompts for TTA models.\n\n\\subsection{Seed Audio Corpus and Text-Audio Pair Creation Strategy}\n\\vspace{-2mm}\n\\label{sec:seed_audio_corpus}\n\nIn order to create the corresponding audio for any constructed text prompt, we instantiate each audio event presented in Sec.~\\ref{sec:audio_event_category} with five exemplar seed audios collected from \\texttt{freesound.org}~\\footnote{since \\texttt{freesound.org} does not contain meaningful people talking audio, we collect people talking audio from VCTK~\\citep{yamagishi2019vctk}}. Since most audio files on \\texttt{freesound.org} are uploaded by volunteers who recorded them in their daily lives, incorporating five exemplar audios for each individual audio event category enhances both the diversity and realism of the seed audio. For instance, in the case of the \\texttt{dog barking} audio event, the five selected audios vary in terms of dog breeds and barking styles. To further enhance an audio event's temporal length diversity, we randomly slice each seed audio into non-overlapping clips ranging from 1~sec to 5~secs. In summary, we have constructed 11 relations~(see Table~\\ref{tab:relation_corpus} Sub-Relation column), and 25 audio events across five main audio events categories. Each audio event has been associated with 5 diverse audio clips ranging from 1~sec to 5 secs collected from \\texttt{freesound.org}.\n\n\\begin{wrapfigure}{r}{0.52\\textwidth}\n\\begin{mdframed}[style=prompt]\n\\small\n1.~generate audio A succeeded by B;\\\\\n2.~start with A, followed by B;\\\\\n3.~play A initially, B afterwards;\\\\\n4.~generate A preceded by B;\\\\\n5.~A in the beginning, B coming next;\n\\end{mdframed}\n\\vspace{-2mm}\n\\caption{\\small GPT-4 augmented prompts~(\\texttt{before} relation).}\n\\label{gpt-4-text}\n\\vspace{-3mm}\n\\end{wrapfigure}\n\n\\textbf{Text Prompt Generation}: a proper audio events relation aware text prompt comprises of two parts: a relation~(\\textit{e.g.}, \\texttt{<before>}) and audio events categories. The audio event categories can be either intra-class or inter-class, and the audio event number depends on the relation. We first instantiate an initial text prompt describing this relation. For example, for the temporal order \\texttt{before} relation, the initial text prompt can be like: \\textit{generate audio A, followed by audio B}. To enrich the text prompts, we further use the initial text prompt to query LLM~(in our case GPT-4) to provide more text prompts with diverse descriptive language for the same relation. \nOne such GPT-4 augmented text prompts is shown in Fig.~\\ref{gpt-4-text}, which illustrates that the same relation can be exactly expressed by multiple different text prompts. By incorporating GPT-4, we create 5 text prompts for each individual relation.\n\n\\begin{figure}[t]\n    \\centering\n    \\begin{minipage}{0.6\\textwidth}\n        \\centering\n        \\includegraphics[width=0.98\\textwidth]{figs/relation_datagen_pipeline.pdf}\n        \\caption{\\small Relation aware \\texttt{<textprompt,audio>} pair creation pipeline. It introduces large diversity in both text prompt and audio.}\n        \\label{fig:relation_datagen_pipeline}\n    \\end{minipage}\\hfill\n    \\begin{minipage}{0.4\\textwidth}\n        \\centering\n        \\scriptsize\n        \\begin{tabular}{p{1.3cm}<{\\centering}|p{3cm}<{\\centering}}\n        \\hline\n        Entry & Highlight\\\\\n        \\hline\n        seed audio & \\makecell[c]{one event has 5 audios\\\\\n                                  each has 1~s-5~s audio clips}\\\\\n        \\hline\n        \\makecell[c]{audio categ-\\\\ory corpus} & \\makecell[c]{5 main categories\\\\25 sub-categories}\\\\\n        \\hline\n        \\makecell[c]{relation\\\\corpus} & 4 main 11 sub relations\\\\\n        \\hline\n        \\makecell[c]{relation -\\\\event number} & \\makecell[c]{\\texttt{count}: 2-5 events;\\\\\n                                                               \\texttt{Not}: 1 event;\\\\\n                                                               \\texttt{if-then-else}: 3 events\\\\\n                                                               others: 2 events.}\\\\\n                                                               \n        \\hline\n        \\makecell[c]{train and \\\\ test data info} & \\makecell[c]{each audio is 10~s long\\\\sampling rate 16~kHz\\\\\n                                                                 train: 44~hrs, 1.6~k pairs\\\\\n                                                                 test: 22~hrs, 0.8~k pairs} \\\\\n        \\hline\n        \\makecell[c]{data creation\\\\constraint} & \\makecell[c]{\\texttt{count} inter-category audio\\\\ \\emph{SpatialDist} intra-category\\\\and require temporder}\\\\\n        \\hline\n        \\makecell[c]{audio\\\\diversity} & \\makecell[c]{one event $\\rightarrow$ multi-audios;\\\\\n                                                    seed audio $\\rightarrow$ multi time len;\\\\\n                                  seed audios various start time \\\\}\\\\\n        \\hline\n        \\makecell[c]{text prompts \\\\diversity} & \\makecell[c]{GPT-4 augmented prompts;\\\\\n                                  one template $\\rightarrow$ multi-events.}\\\\\n        \\bottomrule\n        \\end{tabular}\n        \\vspace{-2mm}\n        \\captionof{table}{\\small \\emph{RiTTA} benchmark highlights.}\n        \\label{tab:ritta_datacreate}\n    \\end{minipage}\n    \\vspace{-5mm}\n\\end{figure}\n\n\\textbf{Audio Generation}: Given the aforementioned audio events categories and relation, we randomly select an exemplar seed audio for each audio event and further linearly blend them together by satisfying the specified relation. For example, the relation \\texttt{<before>} requires two audio events, the two selected audios can be blended together to form the final audio as long as the two seed audios satisfy the \\texttt{<before>} relation~(Fig.~\\ref{fig:relation_datagen_pipeline}, D). Notably, unlike blending two objects in an image that requires careful consideration of factors like occlusion and viewing angle, combining two audio signals simply involves linearly adding them together~\\citep{wavetheory}. This offers an advantage for audio generation, as it eliminates the need for additional operations beyond the specified relation.\n\nThe generation of the \\texttt{<textprompt,audio>} pair is further illustrated in Fig.~\\ref{fig:relation_datagen_pipeline}. With the proposed \\texttt{<textprompt,audio>} pair generation strategy, we can create massive diverse pairs even for the same audio events and the same relation, significantly enhancing the diversity and generalization capability of our generated dataset.\n\n\\subsection{Relation Aware Evaluation Metric MSR-RiTTA}\n\\label{sec:relation_eval}\n\nExisting TTA methods adopt general evaluation metrics to asses the similarity between generated audio and reference audio, including Fr\\'{e}chet Audio Distance~(FAD), Fr\\'{e}chet Distance (FD)~\\citep{fid_distance}, Kullback–Leibler~(KL) divergence, Fr\\'{e}chet Inception Distance~(FID) \\textit{etc.}, among others. While those general evaluation metrics give an overall estimation of the similarity between the two comparing audios, they do not offer direct relation-aware evaluations. In addition to incorporating general evaluation metrics, we further propose multi-stage relation-aware evaluation metrics, with which we can gain insight on how the method performs w.r.t. difference relations.\n\n\\textbf{General Evaluation Metric}: We incorporate three widely used general evaluation metrics: the objective evaluation metric FAD, FD and KL divergence scores. \nFAD and FD measure the distribution similarity with feature embedding extracted from pre-trained on VGGish model~\\citep{vggish}.\n\n\\begin{wrapfigure}{r}{0.35\\textwidth}\n    \\centering\n    \\includegraphics[width=0.34\\textwidth]{figs/relation_eval_vis2.pdf}\n    \\vspace{-2mm}\n    \\caption{\\small relation aware evaluation. Audio event detection model is applied to get audio events. The meta data of each event contains start time $t_1$, end time $t_2$, confidence score $s$ and class label $c$. Various relations can be discovered from these audio events.}\n    \\label{fig:relation_eval_vis}\n    \\vspace{-3mm}\n\\end{wrapfigure}\n\n\\textbf{Relation aware Evaluation Metric MSR-RiTTA}: To directly measure how accurately the text-indicated relation is reflected in the generated audio, we incorporate relation aware metrics for each specified relation.\n\nIn relation aware evaluation, we base on the individual audio event to compute the metrics, which allows us to measure the relation between audio events. Let's denote $(\\mathcal{A}_g, \\mathcal{T}, \\mathcal{R}, \\mathcal{A}_p)$ by ground truth audios, text prompts, relations and generated audios, respectively. We first extract audio events $\\mathcal{E}$ from generated audios $\\mathcal{A}_p$. For example, for the $i$-th generated audio $a_i^p$, we apply pre-trained audio event detection model~(we use finetuned PANNS~\\citep{panns}, see Sec.~\\ref{sec:app:panns_finetune} in Appendix) to extract all potential audio events involved in the audio $E_{a_i^p}=\\{(e_j, m_j)|s\\}_{i=1}^{k}$ by a given event confidence threshold $s\\in \\mathcal{S}$, where $e_j$ is the $j$-th audio event and $m_j$ is the corresponding meta data~(\\textit{e.g.}, audio event class label, confidence score, temporal start time and end time, see Fig.~\\ref{fig:relation_eval_vis}). To obtain audio events data for ground truth audios, we can either apply the same pre-trained model or directly extract from text prompts. Finally, we can get $(\\mathcal{A}_g, \\mathcal{T}, \\mathcal{R}, \\mathcal{A}_p, \\mathcal{E}_p, \\mathcal{E}_g)$, the relation aware evaluation function $f(\\cdot)$ depends on the audio events $\\mathcal{E}_p$, $\\mathcal{E}_g$ and relations $\\mathcal{R}$, $f(\\mathcal{E}_p, \\mathcal{E}_g|\\mathcal{R}, s)$. We adopt a multi-stage relation~(\\emph{MSR-RiTTA}) aware evaluation strategy.\n\n\\textbf{Stage 1:} Target Audio Events Presence~(\\textbf{Pre}). The paramount requirement for a successful audio generation is the presence of text-specified audio events in the generated audio. In this evaluation, the ground truth audio events and generated audio events are treated as \\textit{set}. For a given ground truth and generated audio events pair $(E_g, E_p)$, we iterate over each audio event $e_g$ in the ground truth $E_g$ to check if it exists in the generated audio events $E_p$, regardless of its number and temporal position. \n\n\\begin{equation}\n    f_p(E_p, E_g) = \\frac{1}{k}\\sum_{e_g \\in E_{g}}\\mathds{1} (e_{g}, E_{p});\\ \\ \\ \n    \\mathds{1}(e_g, E_p) = \\left\\{\n    \\begin{aligned}\n        1,&\\ \\text{if}\\ e_g \\in E_p  \\\\\n        0,& \\ \\ \\ \\ \\text{otherwise},\n    \\end{aligned}\n    \\right.\n\\label{eqn:eval_presence}\n\\end{equation}\n\n\\noindent where $k$ is audio event number in the ground truth. $s_l(e_g)$ is a potential event meeting the confidence threshold in the generated audio. We select the event with the highest confidence score as the target.\n\n\\textbf{Stage 2:} Relation Correctness~(\\textbf{Rel}). Once confirming the aforementioned target audio presence, we further investigate if these audio events obey text-specified relation. The relation is correctly modelled if at least a subset of generated audio events meet the relation. We give score 1 if relation is correctly modelled, otherwise score 0.\n\\begin{equation}\n    f_r(E_p|R) = \\prod_{E_t \\in E_p \\cap E_g} \\mathds{1}(E_t,R);\\ \\ \\ \n    \\mathds{1}(E_t,R) = \\left\\{\n    \\begin{aligned}\n        1,&\\ \\ \\  \\text{if}\\ \\ E_t \\ \\ \\text{satisfies relation} \\ \\ R, \\\\\n        0,& \\ \\ \\ \\ \\text{otherwise},\n    \\end{aligned}\n    \\right.\n\\label{eqn:eval_relation}\n\\end{equation}\n\n\\textbf{Stage 3:} Audio Parsimony~(\\textbf{Par}). Apart from requiring to generate all target audios, we should discourage the model from generating excessive intra-class audio events or irrelevant inter-class audio events. We call this property \\textit{Audio Parsimony}. Once it is violated, we introduce extra penalty.\n\n\\begin{equation}\n    f_s(E_p, E_g) = \\exp{(-w_s\\cdot|n(E_p) - n(E_g)|)}\n\\label{eqn:eval_parsimony}\n\\end{equation}\n\n\\noindent where $n(\\cdot)$ indicates audio event number. $w_s$ is the weight adjusting the penalty~(in our case, $w_s=0.1$). The higher audio event number difference incurs lower parsimony score, the resulting parsimony score lies within $(0,1)$. The final relation aware score based on the audio event confidence threshold $s$ equals to the multiplication of the three stage scores,\n\\begin{equation}\n    f(\\mathcal{E}_p, \\mathcal{E}_g|\\mathcal{R}, s) = \\frac{1}{N}  \\sum_{(E_p, E_g, R) \\in (\\mathcal{E}_p, \\mathcal{E}_g, \\mathcal{R})} f_p(E_p, E_g) \\cdot f_r(E_p|R) \\cdot f_s(E_p, E_g)\n\\label{eqn:MSR}\n\\end{equation}\n\\noindent where $N$ is data size number. The final average MSR~(AMSR) score $f(\\mathcal{E}_p, \\mathcal{E}_g|\\mathcal{R}, s)$ lies within $[0, 1)$~(the higher of the score, the better of the model's performance). Following prior COCO object detection evaluation strategy~\\citep{coco_dataset}, we further average across multiple discrete audio event confidence thresholds to get the mean average MSR score~(mAMSR), $f(\\mathcal{E}_p, \\mathcal{E}_g|\\mathcal{R})$,\n\\begin{equation}\n    f(\\mathcal{E}_p, \\mathcal{E}_g|\\mathcal{R}) = \\frac{1}{K}  \\sum_{s\\in \\mathcal{S}} f(\\mathcal{E}_p, \\mathcal{E}_g|\\mathcal{R})\n\\label{eqn:mMSR}\n\\end{equation}\n\\noindent where $K$ is the discrete audio event confidence thresholds number. In our case we use uniformly sample four confidence thresholds in range $[0.5, 0.8]$ with step size $0.1$.\n\n\\section{Relation Aware TTA Finetuning}\n\\label{sec:relation_finetune}\n\n\\begin{wrapfigure}{r}{0.35\\textwidth}\n    \\centering\n    \\includegraphics[width=0.34\\textwidth]{figs/ritta_finetune_pipeline.pdf}\n    \\vspace{-2mm}\n    \\caption{\\small RiTTA finetune pipeline.}\n    \\label{fig:ritta_finetune_pipeline}\n    \\vspace{-3mm}\n\\end{wrapfigure}\nExisting TTA models adopt audio-language pre-trained model to extract text and audio embeddings, including CLAP~\\citep{laionclap2023} and FLAN-T5~\\citep{FLAN-T5}. Prior work~\\citep{ma2023crepe,yuksekgonul2023when,audiotext_noNL,ghosh2024compa} show that existing audio-language pre-trained models~(\\textit{e.g.}, CLAP~\\citep{laionclap2023}) performs like bag-of-words~(BoW), which means they are far better at audio event retrieval task than audio events temporal relation task. \n\nMoreover, the dataset used to pre-train audio-language such as AudioSet~\\citep{audioset_dataset} and AudioCaps~\\citep{audiocaps} are dominated by unary audio event~($64\\%~$\\citep{ghosh2024compa}), limiting models from learning meaningful representations for audio event relations.\n\nBased on aforementioned discussion, we propose to finetune the existing latest Tango model~\\citep{ghosal2023tango} with our created relation aware dataset~(we finetuned Tango~2 as well, but found it gave inferior performance than Tango). Tango depends on prior TTA frameworks AudioLDM~\\citep{liu2023audioldm} to use a Variational Autoencoder~(VAE) for audio encoding and decoding, a latent diffusion model~(LDM)~\\citep{stable_diffusion} for audio generation and HiFiGAN~\\citep{HiFiGAN} to generate final audio waveform from VAE decoder decoded mel-spectrogram. Unlike AudioLDM~\\citep{liu2023audioldm} which depend on CLAP~\\citep{laionclap2023} for text prompt encoding, Tango adopts pre-trained Flan-T5~\\citep{FLAN-T5} model for text prompt encoding. Latest TTA models such as Tango~\\citep{ghosal2023tango}, Tango~2~\\citep{ghosal2023tango2} and AudioLDM~2~\\citep{audioldm2} show that Flan-T5 can achieve better performance than CLAP~\\citep{laionclap2023} in TTA task. Benefiting from the latest advancement, we fine-tune Tango by just tuning latent diffusion model~(LDM) and fixing VAE, HiFiGAN and Flan-T5 components. In our case, we finetune Tango with the curated 44~hrs training dataset. The finetuning workflow is shown in Fig.~\\ref{fig:ritta_finetune_pipeline} and finetuing detail in Sec.~\\ref{sec:relation_finetune}.\n\n\\section{Experiment}\n\nWe run two experiments: benchmarking existing TTA methods on our curated 22~hrs benchmark dataset~(aka testing dataset). Fine-tuning the advanced TTA model on our curated 44~hrs training dataset and further test its relation modeling capability.\n\\subsection{More Discussion on Data Creation}\n\\label{sec:data_prepare}\n\nWe follow the strategy presented in Sec.~\\ref{sec:seed_audio_corpus} to create the dataset. Specifically, for each of the 11 sub-relations in Table~\\ref{tab:relation_corpus}, we create 720~(2~hrs audio) \\texttt{<textpromt,audio>} pairs for testing~(aka benchmark dataset) and 1440 pairs~(4~hrs audio) for training~(aka finetuning dataset). The highlight of the training/testing dataset is given in Table~\\ref{tab:ritta_datacreate}.\n\nTo ensure that all relations can be effectively evaluated using our method, we applied two key constraints during the data creation process. First, to make the audio events countable without ambiguity, we selected inter-category audio events to form the \\texttt{<textprompt,audio>} pairs. This avoids the ambiguity that arises when using intra-category events, especially for those with repetitive, similar local occurrences (\\textit{e.g.}, multiple instances of dog barking). Second, for the \\emph{Spatial Distance} relation, we introduced a temporal order constraint to ensure that the two audio events do not overlap in time. Temporal overlap would require complex source separation models~\\citep{source_separation_icassp} to distinguish individual events. By enforcing this non-overlapping constraint, the evaluation of \\emph{Spatial Distance} becomes manageable using an audio event detection model~(see Sec.~\\ref{sec:panns_finetune} in Appendix). The basic information of data creation is given in Table~\\ref{tab:ritta_datacreate}.\n\n\\begin{table}[t]\n\\centering\n\\small\n\\caption{Benchmark quantitative result across all relations. mAPre, mARel and mAPar are in $10^{-2}$. mAPre and mARel can be treated as \\textit{presence}, \\textit{relation correctness} percentage ratio, they lie in range $[0, 100]$. mAPar score also lies within $[0, 100]$. mAMSR~($10^{-4}$) lies in range~$[0,1]$. The \\colorbox{topcolor}{top-}, \\colorbox{secondcolor}{second-} and \\colorbox{thirdcolor}{third-} performing methods are labelled in different colors, respectively.}\n\\begin{tabular}{l|p{1.0cm}<{\\centering}|p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}|p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}|p{0.9cm}<{\\centering}}\n\\hline\n\\multirow{2}{*}{Model} & \\multirow{2}{*}{\\#param} & \\multicolumn{3}{c|}{General Evaluation}  & \\multicolumn{4}{c}{Relation Aware Evaluation~($\\uparrow$)} \\\\\n\\cline{3-9}\n &   & FAD~$\\downarrow$ &  KL~$\\downarrow$ & FD~$\\downarrow$ & mAPre& mARel & mAPar &mAMSR \\\\\n\\hline\nAudioLDM~(S-Full)~(\\citeyear{liu2023audioldm})  & $185$~M  & \\cellcolor{secondcolor}5.65  & 38.95 & \\cellcolor{thirdcolor}37.30 & 2.76 & 0.50 & 2.52 & 0.04\\\\\nAudioLDM~(L-Full)~(\\citeyear{liu2023audioldm})  & $739$~M  & \\cellcolor{topcolor}5.47  & \\cellcolor{thirdcolor}38.42 & 37.96 & 3.09 & 0.77 & 2.56 & 0.08\\\\\nAudioLDM~2~(L-Full)~(\\citeyear{audioldm2}) & $844$~M & 6.68 & \\cellcolor{secondcolor}29.07 &  \\cellcolor{secondcolor}35.85& \\cellcolor{secondcolor}12.26& \\cellcolor{secondcolor}2.41 & \\cellcolor{secondcolor}10.01 & \\cellcolor{secondcolor}3.39 \\\\\nMakeAnAudio~(\\citeyear{makeanaudio}) & $452$~M & 9.46 & 82.72  & 45.98 & 8.14  & 1.68 & 6.47 & 1.02\\\\\nAudioGen~(\\citeyear{kreuk2022audiogen}) & $1.5$~B & \\cellcolor{thirdcolor}6.43 & \\cellcolor{topcolor}28.01& \\cellcolor{topcolor}32.04 & 9.61 & 2.12 & 8.60 & 2.27\\\\\nTango~(\\citeyear{ghosal2023tango}) & $866$~M & 10.79 & 90.26 & 39.46 & \\cellcolor{thirdcolor}11.13 & \\cellcolor{thirdcolor}2.27 & \\cellcolor{thirdcolor}9.88 & \\cellcolor{thirdcolor}3.10\\\\\nTango~2~(\\citeyear{ghosal2023tango2}) & $866$~M & 13.84  &  89.66 & 44.03 & \\cellcolor{topcolor}16.63 & \\cellcolor{topcolor}4.40 & \\cellcolor{topcolor}12.53 & \\cellcolor{topcolor}11.55 \\\\\n\\hline\n\\end{tabular}\n\\label{tab:general-results}\n\\end{table}\n\n\\subsection{More Discussion on RiTAA Evaluation}\n\nSection~\\ref{sec:relation_eval} has introduced the metrics in general. In practice, we further adjust the audio generation process for relations under \\emph{Compositionality} and \\emph{Spatial Distance} to so as to ensure these relations can be accurately evaluated under our proposed framework.\n\nFirst, we skip general evaluation for \\texttt{<Not>} as it lacks a corresponding ground truth reference audio. During fintuning, we generate silent audio for \\texttt{<Not>} for create finetuing pairs. Second, for the \\texttt{<if-then-else>} and \\texttt{<Or>} sub-relations, which correspond to two possible ground truth audios, we handle evaluation by computing the L2 distance~(in the time domain) between the generated audio and the two reference audios. For example, for the prompt \\textit{if event A then event B, else event C}, the first reference is the combination of events A and B, while the second contains only event C. We use the reference audio with smaller L2 distance to the generated audio for general evaluation.\n\nThird, precise evaluation of the three sub-relations~(\\texttt{<closefirst>}, \\texttt{<farfirst>}, and \\texttt{<equaldist>}) under \\emph{Spatial Distance} from unconstrained audio requires sound event detection and localization~(SELD~\\citep{soundsynp,seld_dcase19}) techniques to spatially localize each audio event, which is impossible with mono-channel audio. To address this, we approximate spatial distance by calculating the loudness, which can be estimated using the L2 norm of the audio waveform. The rationale behind this approach is that greater distances result in a dampening of waveform amplitude~(and consequently reduced loudness) due to energy decay along the audio propagation path. When the loudness difference exceeds a predefined threshold~(for \\texttt{<closefirst>}, \\texttt{<farfirst>}) or is within that threshold~(for \\texttt{<equaldist>}), we consider the evaluation accurate. Specifically, we use a loudness reduction ratio $\\sigma_1$ (with $\\sigma_1=0.2$ in our case). For \\texttt{<closefirst>}, if the closer event's loudness is at least $\\sigma$ times greater than the further event's loudness, the relation is considered correct. Similarly, for \\texttt{<equaldist>}, the loudness difference between the two events should be within $\\sigma_2$~(with $\\sigma_2=0.4$ in our case) of the louder event's loudness. This estimation is also reflected in the data generation process (see Sec~\\ref{sec:data_prepare}).\n\n\\begin{table}[t]\n\\centering\n\\small\n\\caption{Benchmark quantitative result w.r.t. the four main relations. We report FAD sore and mAMSR score for general evaluation and relation aware evaluation, respectively.}\n\\begin{tabular}{l|p{0.5cm}<{\\centering}p{1.1cm}<{\\centering}p{.8cm}<{\\centering}p{.8cm}<{\\centering}|p{0.5cm}<{\\centering}p{1.1cm}<{\\centering}p{0.8cm}<{\\centering}p{.8cm}<{\\centering}}\n\\hline\n\\multirow{2}{*}{Model} & \\multicolumn{4}{c|}{General Evaluation~(FAD $\\downarrow$)}  & \\multicolumn{4}{c}{Relation Aware Eval.~(mAMSR $\\uparrow$)} \\\\\n\\cline{2-9}\n &  \\emph{Count} &  \\emph{TempOrder} & \\emph{SpatDist} & \\emph{Compos} &  \\emph{Count} & \\emph{TempOrder} & \\emph{SpatDist} & \\emph{Compos}  \\\\\n\\hline\nAudioLDM~(S-Full)~(\\citeyear{liu2023audioldm})   & \\cellcolor{secondcolor}{3.85} & \\cellcolor{secondcolor}6.86 &  \\cellcolor{thirdcolor}4.56 & \\cellcolor{secondcolor}9.36 & 0.00 & 0.05 & 0.00 & 0.18 \\\\\nAudioLDM~(L-Full)~(\\citeyear{liu2023audioldm})  & \\cellcolor{topcolor}3.68  & \\cellcolor{topcolor}6.45  & \\cellcolor{secondcolor}4.10 & \\cellcolor{topcolor}8.98 & 0.00 & 0.05 & 0.06 & 0.17\\\\\nAudioLDM~2~(L-Full)~(\\citeyear{makeanaudio}) & \\cellcolor{thirdcolor}5.03 & 8.94 &  4.72 & \\cellcolor{thirdcolor}9.41 & 0.14 & 1.87 & \\cellcolor{secondcolor}1.46 & \\cellcolor{secondcolor}9.89\\\\\nMakeAnAudio~(\\citeyear{makeanaudio}) & 6.02 & 10.21 &  8.18 & 12.78 & 0.12 & 0.66 & 0.44 & 2.40 \\\\\nAudioGen~(\\citeyear{kreuk2022audiogen})& 6.14 & \\cellcolor{thirdcolor}8.39 & \\cellcolor{topcolor}3.38 & 9.98 & \\cellcolor{secondcolor}0.32 & \\cellcolor{secondcolor}3.83 &  0.48 & 4.18\\\\\nTango~(\\citeyear{ghosal2023tango}) & 8.54 &  10.25 & 10.11 & 13.97 & \\cellcolor{thirdcolor}0.16 & \\cellcolor{thirdcolor}3.44 & \\cellcolor{thirdcolor}0.82 & \\cellcolor{thirdcolor}8.10 \\\\\nTango~2~(\\citeyear{ghosal2023tango2}) & 10.01  & 13.91 & 13.23 & 17.04 & \\cellcolor{topcolor}0.96 & \\cellcolor{topcolor}20.92 & \\cellcolor{topcolor}1.92& \\cellcolor{topcolor}23.25\\\\\n\\hline\n\\end{tabular}\n\\label{tab:results-fourcates}\n\\end{table}\n\n\\subsection{Relation Aware Benchmarking Result}\n\nWe benchmark our curated test dataset on 7 most recent TTA models: AudioLDM~\\citep{liu2023audioldm}~(two versions), AudioLDM~2~\\citep{audioldm2}, MakeAnAudio~\\citep{makeanaudio}, AudioGen~\\citep{kreuk2022audiogen}, Tango~\\citep{ghosal2023tango} and Tango~2~\\citep{ghosal2023tango2}. We directly depend on their released models to generate a 10~second audio from each text prompt. We then adopt general evaluation and relation-aware evaluation metrics~(see Sec.~\\ref{sec:relation_eval}) for assessing the generated audios quality. The detailed configuration of each method is given in Table~\\ref{app:tab:model_setting} in Appendix. \n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.98\\linewidth]{figs/ritta_radar_chart.pdf}\n    \\caption{\\small Top~3 performing in audio events relation modelling TTA methods' performance w.r.t. the 11 sub-relations. We report mAPre, mARel, mAPar and mAMSR scores separately.}\n    \\label{fig:exp_radar_chart}\n\\end{figure}\n\nThe quantitative evaluation results across all relations are shown in Table~\\ref{tab:general-results}. From this table we can observe that the general evaluation results are inconsistent with our proposed relation aware evaluation metrics. The best performing methods under generational evaluations~(the two AudioLDM versions) perform the worst under relation aware evaluations, and vice versa. These discrepancies highlight the necessity of proposing evaluation metrics specifically tailored for audio events relations. Additionally, while the performance differences among the seven benchmarking methods under general evaluation are relatively minor, the corresponding differences under relation aware evaluation are significantly more pronounced~(\\textit{e.g.}, Tango~2 outperforms AudioLDM~(S-Full) by about 200 times). However, even the top-performing method, Tango~2~\\citep{ghosal2023tango2}, still struggles to model audio events relations, as both its presence accuracy and relation accuracy rate are below $1\\%$~(mAPre is just 0.02\\% and mARel 0.04\\%), and it generates an average of two redundant audio events~(mAPar=0.1253). All of these observations demonstrate the limitations of existing TTA methods in modelling audio events relation and the necessity to systematically study audio events relation in TTA, highlighting the importance of our proposed work.\n\nThe quantitative evaluation results~(mAMSR score) w.r.t the four main relation categories are presented in Table~\\ref{tab:results-fourcates}. We observe that both general and relation-aware evaluations show better performance on \\emph{Temporal Order} and \\emph{Compositionality} compared to \\emph{Count} and \\emph{Spatial Distance}. This suggests that the \\emph{Count} and \\emph{Spatial Distance} relations pose significant challenges for TTA tasks. Additionally, we visualize the detailed relation aware evaluation results for the 11 sub-relations, highlighting the top three performing methods AudioLDM~2~\\citep{audioldm2}, Tango~\\citep{ghosal2023tango}, and Tango~2~\\citep{ghosal2023tango2}, in Fig.~\\ref{fig:exp_radar_chart}. We can observe that all the three methods 1. achieve exceedingly high presence score on \\texttt{Not} relation, which is expected since a high \\textbf{Presence} score~(Subfig.~A) can be easily obtained by simply not generating the specified audio event. 2. perform well in modelling \\texttt{And} relation~(Subfig.~B)~(then \\texttt{<equaldist>} and the three relations in \\emph{Temporal Order}); 3. exhibit strength in generating concise audios particularly for \\texttt{Not} relation~(Subfig.~C). Overall, all the three methods excel in modelling \\texttt{And} relation and then the three sub-relations in \\emph{Temporal Order}, which is also reflected by the result in Table~\\ref{tab:results-fourcates}.\n\n\\begin{wraptable}{r}{5.4cm}\n\\scriptsize\n\\vspace{-2mm}\n\\begin{tabular}{l}\n\\toprule\n\\textbf{1.} generation eval. contradicts with RiTTA eval.\\\\\n\\textbf{2.} \\emph{TemOrder}/\\emph{Compos} better than \\emph{Count}/\\emph{SpatDist}\\\\\n\\textbf{3.} event presence in \\texttt{Not} is the highest;\\\\\n\\textbf{4.} relation correctness in \\texttt{And} is the highest;\\\\\n\\textbf{5.} parsimony score in \\texttt{Not} is the highest;\\\\\n\\textbf{6.} event presence accuracy rate is below $1\\%$; \\\\\n\\textbf{7.} relation correctness accuracy rate is below $1\\%$;\\\\\n\\textbf{8.} An average of 2 redundant audio events;\\\\\n\\bottomrule\n\\end{tabular}\n\\vspace{-3mm}\n\\caption{\\small Key findings from experiments of TTA models on our RiTTA benchmark.}\n\\label{tab:benchmark_findings}\n\\end{wraptable} \n\nThe key findings from the relation-aware benchmarking are summarized in the Table~\\ref{tab:benchmark_findings}.\n\nIn summary, we conclude that, 1. existing TTA models lack the ability to model audio events relation described by the text prompt in the generated audio, emphasizing the importance of our work in systematically study audio events relation in TTA. 2. Existing TTA evaluation metrics fall short in accurately measuring audio events relations from the generated audio. Our proposed multi-stage relation evaluation framework suffices to measure the relation accuracy from various aspects.\n\n\\subsection{Finetuning Experimental Result}\n\nWe finetune Tango with the Adam optimizer and follow the finetuning strategy outlined in Tango~2~\\citep{ghosal2023tango2}. The results, shown in Table~\\ref{tab:finetune_result}, clearly demonstrate that finetuning Tango with relation aware datasets significantly improves its improves its ability to model audio event relations across both general and relation aware evaluations. This underscores the importance of benchmarking \\emph{RiTTA} with both comprehensive datasets and tailored evaluation metrics. Given that we finetuned only the latent diffusion model with a relatively small dataset (1.6~k pairs), further improvements can be expected by jointly finetuning other modules~(\\textit{e.g.}, FLAN-T5) with a larger dataset. Moreover, the boosted performance indicates that audio events relation can indeed be modelled by TTA methods. We hope this benchmark and initial exploration will inspire more researchers to explore this area further.\n\nTwo qualitative examples are in Fig.~\\ref{fig:tango_finetune_vis}~A. It is evident that the finetuned Tango successfully models the \\texttt{<before>} relation~(Table~\\ref{tab:relation_test} and Fig.~\\ref{fig:teasing_fig} show all existing TTA models fail on this case), and \\texttt{<count>} relation. The mAPre score w.r.t. the 11 sub-relations is shown in Fig.~\\ref{fig:tango_finetune_vis}~B~(the mARel, mAPar, mAMSR are in Fig.~\\ref{fig:tango_finetune_vis_append} in Appendix). The results clearly indicates that finetuned Tango achieves significant improvements in target audio events presence across most relations, particularly in \\texttt{<Or>}, \\texttt{<And>}, \\texttt{<simultaneity>}, \\texttt{<after>} and \\texttt{<before>}. The performance drop in \\texttt{<Not>} relation may be attributed to the dataset preparation: as we pair \\texttt{<Not>} relation with silent audio~(all-zero waveforms), yet the text prompts might contain arbitrary audio events. Finetuning on such created data may confuse the model, leading to ambiguity in audio events generation. Further investigation is needed to address this challenge. \n\n\\begin{table}[t]\n\\centering\n\\scriptsize\n\\caption{Quantitative result across general and relation aware evaluation for Tango w/o finetuing.}\n\\vspace{-2mm}\n\\begin{tabular}{l|p{0.5cm}<{\\centering}p{0.5cm}<{\\centering}p{0.5cm}<{\\centering}|p{0.5cm}<{\\centering}p{0.5cm}<{\\centering}p{0.5cm}<{\\centering}|p{0.7cm}<{\\centering}|p{0.5cm}<{\\centering}p{0.9cm}<{\\centering}p{0.9cm}<{\\centering}p{.7cm}<{\\centering}}\n\\hline\n\\multirow{2}{*}{Model}  & \\multicolumn{3}{c|}{General Evaluation}  & \\multicolumn{4}{c|}{Relation Aware Evaluation~($\\uparrow$)} &  \\multicolumn{4}{c}{mAMSR Across Four Main Relations}\\\\\n\\cline{2-12}\n & FAD~$\\downarrow$ &  KL~$\\downarrow$ & FD~$\\downarrow$ & mAPre& mARel & mAPar &mAMSR &  \\emph{Count} & \\emph{TempOrder} & \\emph{SpatDist} & \\emph{Compos} \\\\\n\\hline\nTango~(\\citeyear{ghosal2023tango}) &  10.79 &  90.26 &  39.46 & 11.13 &  2.27 &  9.88 &  3.10 & 0.16 & 3.44 & 0.82 & 8.10\\\\\nTango~(finetuing) & \\cellcolor{topcolor}4.60  &  \\cellcolor{topcolor}23.92 & \\cellcolor{topcolor}27.03 & \\cellcolor{topcolor}21.23 & \\cellcolor{topcolor}10.78 & \\cellcolor{topcolor}20.35 & \\cellcolor{topcolor}48.67 & \\cellcolor{topcolor}8.04 & \\cellcolor{topcolor}324.10 & \\cellcolor{topcolor}1.88 & \\cellcolor{topcolor}44.42 \\\\\n\\hline\n\\end{tabular}\n\\label{tab:finetune_result}\n\\end{table}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.98\\linewidth]{figs/ritta_finetune_vis.pdf}\n    \\caption{\\small Qualitative visualization comparison of Tango w/o finetuning~(A) and mAPre w.r.t. 11 sub-relations. Listenable audios are provided in supplementary material.}\n    \\label{fig:tango_finetune_vis}\n\\end{figure}\\section{Conclusion and Future Works}\n\nComplex relationships within audio bring the world to life. While text-to-audio~(TTA) generation models have made remarkable progress in generating high-fidelity audio with fine-grained context understanding, they often fall short in capturing the relational aspect of audio events in real-world. The world around us is composed of interconnected audio events, where audio event rarely occurs in isolation. Simply generating single sound sources is insufficient for producing realistic audio that reflects the richness of the world.\n\nTo analyze the capabilities of current state-of-the-art TTA generative models, we first conduct a systematic study of these models in audio event relation modeling. We introduce a benchmark for this task by creating a comprehensive relational corpus covering all potential relations in the real-world scenarios. Further, we propose new evaluation metric framework to assess audio event relation modeling from various perspectives. Additionally, we propose a finetuning strategy to boost existing models' ability in modelling audio events relation, and we show improvement across all relation metrics. Finally, we will release both the dataset and the code for the evaluation metrics, which will be useful for future research in this domain.\n\nGoing forward, our work provides a unique research opportunity to bring the world to life by exploring ways to generate long-term audio events to acoustically understand the physical world. Further, understanding the successes and failures of these models in generating such complex audio events is another promising research direction. This analysis could lead to further improvements in TTA models and their applications in areas such as virtual reality, cinema and immersive media. \n\\newpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2301.12503v3.tex",
        "arXiv-2404.09956v4.tex",
        "arXiv-2412.15922v3.tex"
    ],
    "group_id": "group_14",
    "response": "### Summary of Recent Advances in Text-to-Audio Generation Models\n\n#### Title\nRecent Advances in Text-to-Audio Generation Models: A Comparative Analysis of AudioLDM, Tango2, and RiTTA\n\n#### Introduction\nText-to-Audio (TTA) generation models have emerged as a promising area within the broader field of multimodal AI, enabling the synthesis of audio signals directly from textual descriptions. This capability has significant implications for applications such as augmented reality, game development, and video editing, where the creation of sound effects, music, and speech according to specific requirements is essential. The history of TTA generation can be traced back to earlier work in signal processing and generative models, but recent advancements have been driven by the integration of large language models and diffusion architectures. These models have shown impressive progress in generating high-fidelity audio with fine-grained context understanding, yet they often struggle to accurately model the relationships between audio events as specified in the input text.\n\nCurrent TTA models face challenges in capturing the nuanced relationships between audio events, which are crucial for realistic audio generation. This limitation is particularly evident in the generation of complex acoustic environments or soundscapes, where the interplay of different sound sources and their temporal and spatial relationships must be accurately represented. To address these challenges, researchers have proposed various methods, including the use of contrastive learning, preference optimization, and fine-tuning strategies. However, the effectiveness of these methods in enhancing the generation of audio events with correct relationships remains an open question. This summary aims to provide a comparative analysis of three recent TTA models: AudioLDM, Tango2, and RiTTA, highlighting their contributions and limitations in the context of audio event relation modeling.\n\n#### Main Content of Each Paper\n\n**AudioLDM: Text-to-Audio Generation with Latent Diffusion Models**\nAudioLDM is a TTA system that leverages latent diffusion models (LDMs) to generate audio signals from text descriptions. The model uses a mel-spectrogram-based variational autoencoder (VAE) to compress the high-dimensional audio signals into a latent space. This latent space is then used to train an LDM conditioned on CLAP embeddings, which are obtained from a contrastive language-audio pretraining (CLAP) model. CLAP embeddings provide cross-modal information that aligns audio and text embeddings, enabling the model to generate audio without the need for explicit audio-text paired data. The authors of AudioLDM demonstrate that their model can achieve state-of-the-art (SOTA) performance on the AudioCaps dataset, outperforming other open-sourced systems in both objective and subjective evaluations. Additionally, AudioLDM enables various text-guided audio manipulations, such as style transfer, super-resolution, and inpainting, in a zero-shot manner.\n\n**Tango2: Improving Diffusion-based Text-to-Audio Generation through Direct Preference Optimization**\nTango2 is an enhanced version of the Tango model, which utilizes a latent diffusion model (LDM) and a large language model (LLM) for text-to-audio generation. The authors of Tango2 introduce a novel approach to fine-tune the model using direct preference optimization (DPO) loss on a synthesized preference dataset. This preference dataset contains diverse audio descriptions (prompts) paired with their respective preferred (winner) and undesirable (loser) audio outputs. The undesirable audio outputs are generated by perturbing the text descriptions using GPT-4 and by applying adversarial filtering to select audio samples with lower CLAP scores. The model is fine-tuned on this dataset to improve the semantic alignment between the input prompt and the generated audio. Tango2 achieves significant improvements over the original Tango and AudioLDM2 models in both objective and subjective evaluations, demonstrating the effectiveness of DPO in enhancing the quality and relevance of generated audio.\n\n**RiTTA: Modeling Event Relations in Text-to-Audio Generation**\nRiTTA is a framework designed to systematically study and enhance the ability of TTA models to model audio event relations as described in the input text. The authors construct a comprehensive corpus of audio event relations and categories, and propose a new evaluation framework called multi-stage relation-aware evaluation (MSR-RiTTA) to assess the performance of TTA models in relation modeling. RiTTA benchmarks existing TTA models on a curated dataset and demonstrates that these models struggle to accurately capture the relationships between audio events, even when they generate high-fidelity audio. The authors also introduce a finetuning strategy for Tango, which significantly improves its ability to model audio event relations. RiTTA highlights the need for more nuanced evaluation metrics and datasets that focus on the relational aspects of audio generation.\n\n#### Commonalities and Innovations\nAll three papers focus on improving the quality and relevance of audio generated from text descriptions using diffusion models. However, they differ in their approaches and contributions. AudioLDM introduces a latent diffusion model (LDM) that operates in a continuous latent space, enabling efficient and high-quality audio generation. Tango2 employs direct preference optimization (DPO) to fine-tune the model on a synthesized preference dataset, enhancing the semantic alignment between text prompts and generated audio. RiTTA, on the other hand, proposes a comprehensive benchmark for audio event relation modeling and a new evaluation framework to assess the performance of TTA models in relation modeling.\n\n**Commonalities**\n1. **Diffusion Models**: All three papers utilize diffusion models for generating audio from text prompts. Diffusion models are known for their ability to generate high-quality, diverse samples by gradually denoising a latent representation.\n2. **Latent Space**: AudioLDM and Tango2 both use a latent space to represent audio signals, which allows for more efficient and controllable generation processes.\n3. **Contrastive Learning**: AudioLDM and RiTTA rely on contrastive learning to align audio and text embeddings, enabling the model to generate audio based on text descriptions without explicit audio-text paired data.\n\n**Innovations**\n1. **AudioLDM**: AudioLDM introduces a continuous latent diffusion model (LDM) for TTA generation, which outperforms existing models in both objective and subjective evaluations. The model also enables zero-shot text-guided audio manipulations, such as style transfer, super-resolution, and inpainting.\n2. **Tango2**: Tango2 innovates by using direct preference optimization (DPO) to fine-tune the model on a synthesized preference dataset. This approach enhances the semantic alignment between text prompts and generated audio, leading to improved performance in both objective and subjective evaluations.\n3. **RiTTA**: RiTTA systematically studies audio event relation modeling and proposes a new evaluation framework, MSR-RiTTA, to assess the performance of TTA models in relation modeling. The framework evaluates the presence, correctness, and parsimony of audio events in the generated audio, providing a more nuanced evaluation compared to existing metrics.\n\n#### Comparison of Results\nThe three papers present different evaluation metrics and datasets to assess the performance of their respective models. AudioLDM achieves SOTA performance on the AudioCaps dataset, with a Fréchet Distance (FD) of 23.31, outperforming the DiffSound baseline by a large margin. Tango2 also demonstrates significant improvements over the original Tango and AudioLDM2 models, achieving the best performance in both objective and subjective evaluations. RiTTA benchmarks existing TTA models on a curated dataset and highlights their limitations in relation modeling. The model's finetuning strategy significantly improves Tango's performance in relation modeling, but the overall performance remains below 1% in terms of presence and relation correctness accuracy rates.\n\n**Objective Evaluations**\n- **AudioLDM**: Achieves an FD of 23.31, IS of 8.13, and KL of 1.59 on the AudioCaps dataset.\n- **Tango2**: Achieves an FD of 4.60, KL of 23.92, and a CLAP score of 48.67 on the AudioCaps dataset.\n- **RiTTA**: Achieves a mAMSR score of 48.67 across all relations, with significant improvements in relation modeling after finetuning Tango.\n\n**Subjective Evaluations**\n- **AudioLDM**: Achieves an OVL of 65.91 and REL of 65.97 on the AudioCaps dataset.\n- **Tango2**: Achieves an OVL of 3.99 and REL of 4.07 on the AudioCaps dataset.\n- **RiTTA**: Achieves an OVL of 3.99 and REL of 4.07 on the AudioCaps dataset, demonstrating significant improvements in relation modeling after finetuning Tango.\n\nThe results indicate that while AudioLDM and Tango2 excel in generating high-fidelity audio, RiTTA highlights the need for more nuanced evaluation metrics and datasets that focus on the relational aspects of audio generation. The performance differences among the models suggest that there is still room for improvement in relation modeling, particularly in handling complex temporal and spatial relationships.\n\n#### Conclusion\nThe three papers, AudioLDM, Tango2, and RiTTA, each contribute to advancing the field of TTA generation models. AudioLDM introduces a continuous latent diffusion model that achieves SOTA performance in both objective and subjective evaluations. Tango2 demonstrates the effectiveness of direct preference optimization in enhancing the semantic alignment between text prompts and generated audio. RiTTA provides a comprehensive benchmark for audio event relation modeling and a new evaluation framework, highlighting the limitations of existing TTA models in relation modeling.\n\nThe main findings from these studies are:\n1. **AudioLDM**: Achieves high-quality audio generation with a single training dataset and a single GPU, outperforming other models in both objective and subjective evaluations.\n2. **Tango2**: Improves the semantic alignment between text prompts and generated audio through direct preference optimization, achieving better performance than the original Tango and AudioLDM2 models.\n3. **RiTTA**: Highlights the limitations of existing TTA models in relation modeling and proposes a new evaluation framework to assess the performance of TTA models in relation modeling.\n\nFuture research directions include:\n1. **Enhancing Relation Modeling**: Further improving the ability of TTA models to model complex relationships between audio events, particularly in handling temporal and spatial relationships.\n2. **Developing More Nuanced Evaluation Metrics**: Creating more sophisticated evaluation metrics that can accurately measure the performance of TTA models in relation modeling.\n3. **Expanding Datasets**: Building larger and more diverse datasets that cover a wide range of audio events and their relationships, to better train and evaluate TTA models.\n\nThese advancements could lead to more realistic and controllable audio generation, with significant implications for applications such as augmented reality, game development, and immersive media."
}