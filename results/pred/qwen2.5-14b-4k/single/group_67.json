{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{A Unified Taxonomy and Multimodal Dataset for Events in Invasion Games}\n\n\\begin{document}\n\n\\title{A Unified Taxonomy and Multimodal Dataset for Events in Invasion Games}\n\n\\author{Henrik Biermann}\n\\authornote{Both authors contributed equally to this research.}\n\\email{h.biermann@dshs-koeln.de}\n\\orcid{0000-0001-5660-9876}\n\\affiliation{%\n  \\institution{Institute of Exercise Training and Sport Informatics, German Sport University Cologne}\n  \\streetaddress{}\n  \\city{Cologne}\n  \\state{}\n  \\country{Germany}\n  \\postcode{}\n}\n\n\\author{Jonas Theiner}\n\\authornotemark[1]\n\\orcid{0000-0002-8966-4860}\n\\email{theiner@l3s.de}\n\\affiliation{\n  \\institution{L3S Research Center, Leibniz University Hannover}\n  \\city{Hannover}\n  \\state{}\n  \\country{Germany}\n  \\postcode{}\n}\n\n\\author{Manuel Bassek}\n\\orcid{0000-0002-9394-913X}\n\\email{m.bassek@dshs-koeln.de}\n\\affiliation{%\n  \\institution{Institute of Exercise Training and Sport Informatics, German Sport University Cologne}\n  \\streetaddress{}\n  \\city{Cologne}\n  \\state{}\n  \\country{Germany}\n  \\postcode{}\n}\n\n\\author{Dominik Raabe}\n\\email{dominik.raabe@dshs-koeln.de}\n\\orcid{0000-0001-7264-4575}\n\\affiliation{%\n  \\institution{Institute of Exercise Training and Sport Informatics, German Sport University Cologne}\n  \\streetaddress{}\n  \\city{Cologne}\n  \\state{}\n  \\country{Germany}\n  \\postcode{}\n}\n\n\\author{Daniel Memmert}\n\\orcid{0000-0002-3406-9175}\n\\email{d.memmert@dshs-koeln.de}\n\\affiliation{%\n  \\institution{Institute of Exercise Training and Sport Informatics, German Sport University Cologne}\n  \\streetaddress{}\n  \\city{Cologne}\n  \\state{}\n  \\country{Germany}\n  \\postcode{}\n}\n\n\\author{Ralph Ewerth}\n\\orcid{0000-0003-0918-6297}\n\\email{ralph.ewerth@tib.eu}\n\\affiliation{\n  \\institution{L3S Research Center \\\\ TIB -- Leibniz Information Centre for Science and\nTechnology}\n  \\city{Hannover}\n  \\state{}\n  \\country{Germany}\n  \\postcode{}\n}\n\n\\renewcommand{\\shortauthors}{Biermann and Theiner, et al.}\n\n\\begin{abstract}\nThe automatic detection of events in complex sports games like soccer and handball using positional or video data is of large interest in research and industry.\nOne requirement is a fundamental understanding of underlying concepts, i.e., events that occur on the pitch.\nPrevious work often deals only with so-called low-level events based on well-defined rules such as free kicks, free throws, or goals.\nHigh-level events, such as passes, are less frequently approached due to a lack of consistent definitions. This introduces a level of ambiguity that necessities careful validation when regarding event annotations.\nYet, this validation step is usually neglected as the majority of studies adopt annotations from commercial providers on private datasets of unknown quality and focuses on soccer only.\nTo address these issues, \nwe present~(1)~a universal taxonomy that covers a wide range of \\textit{low} and \\textit{high-level} events for invasion games and is exemplarily refined to soccer and handball,\nand~(2)~release two multi-modal datasets comprising video and positional data with gold-standard annotations to foster research in fine-grained and ball-centered event spotting. \nExperiments on human performance demonstrate the robustness of the proposed taxonomy, and that disagreements and ambiguities in the annotation increase with the complexity of the event.\nAn I3D model for video classification is adopted for event spotting and reveals the potential for benchmarking.\nDatasets are available at: \\url{https://github.com/mm4spa/eigd}\n\\end{abstract}\n\n\\maketitle\n\n\\section{Introduction}\\label{sec:intro}\n\n\\begin{figure}[bt!]\n\t\\centering\n\t\\includegraphics[width=\\linewidth]{img/annotation_examples.pdf}\n\t\\caption{\\acrshort{dataset_provider}: Pass annotations from a \\textcolor{set1_red}{data provider} vs. an \\textcolor{set1_blue}{expert}. \\acrshort{dataset_soccer} and \\acrshort{dataset_handball}: Example annotations from experienced annotators~(\\textcolor{set1_red}{red}, \\textcolor{set1_blue}{blue}, \\textcolor{set1_green}{green}) using our proposed taxonomy: Despite uncertainties regarding the concrete event type, the annotated timestamp often aligns. The mapping back to shared characteristics such as the motoric skill (e.g., ball release), leads to higher levels of agreement. \n\t}\n\t\\label{fig:example_annotations}\n\\end{figure}\n\nEvents play an important role for the (automatic) interpretation of complex invasion games like soccer, handball, hockey, or basketball.\nOver the last years, three fundamental perspectives emerged with regard to the analysis of sports games, which all value different characteristics of the respective sports: \n(1)~The \\emph{sports science} domain demands semantically precise descriptions of the individual developments to analyze success factors~\\cite{lamas2014invasion}.\n(2)~The \\emph{machine learning} community aims to find automatic solutions for specific tasks~(often supervised).\n(3)~\\emph{Practitioners}~, i.e., coaches or analysts, show little interest in the description of the sport since they are rather interested in the immediate impact of specific modification of training or tactics. \nWhile the general objective to understand and exploit the underlying concepts in the sports is common to all perspectives, synergistic effects are barely observed~\\cite{rein2016big}.\nDescriptive statistics such as possession or shot frequency rely on events that occur on the pitch.\nHowever, collecting semantic and (spatio-)~temporal properties for events during matches is non-trivial, highly dependent on the underlying definitions, and is, in the case of (accurate)~manual annotations, very time-consuming and expensive~\\cite{pappalardo2019public}.\nWhile it is a common practice of data providers~\\cite{wyscout, opta, stats} for (certain)~matches in professional sport to delegate the annotation of events to human annotators, \nvarious approaches have been suggested to automate the process.\nIn this respect, the automatic detection of (spatio-)~temporal events has been addressed for (broadcast)~video data~\\cite{giancola2018soccernet, giancola2021temporally, sarkar2019generation, vats2020event, sanford2020group, sorano2020automatic, hu2020hfnet, yu2019soccer, jiang2016automatic, liu2017soccer, tomei2021rms, karimi2021soccer, mahaseni2021spotting} and positional data~\\cite{sanford2020group, xie2020passvizor, khaustov2020recognizing, chacoma2020modeling, richly2016recognizing, richly2017utilizing, morra2020slicing}.\n\nThe \\textit{temporal event} localization is the task of predicting a semantic label of an event and assigning its start and end time, commonly approached in the domain of video understanding~\\cite{caba2015activitynet}.  \nDespite a general success in other domains~\\cite{lin2019bmn, feichtenhofer2019slowfast, caba2015activitynet, nguyen2018weakly}, it has already been observed that this definition can lead to ambiguous boundaries~\\cite{sigurdsson2017actions}.\nSports events can also be characterized by a single representative time stamp~(\\emph{event spotting}~\\cite{giancola2018soccernet}) and recently there has been success in spotting \\textit{low-level} events~\\cite{giancola2021temporally, deliege2020soccernet, cioppa2020context} in soccer videos such as goals and cards. \nIn contrast, these data acquisition approaches lack more complex, ambiguous, and more frequent events like passes or dribblings that are not covered by existing publicly available~(video) datasets~\\cite{feng2020sset, deliege2020soccernet}. Indeed, some definitions of \\textit{high-level} events in soccer are provided in the literature~\\cite{kim2019attacking, fernandes2019design}, but there is no global annotation scheme or even taxonomy that covers various events that can be evaluated with few meaningful metrics.\nAlthough there are related events in other invasion games such as handball, neither a set of \\textit{low-level} and \\textit{high-level} events nor a taxonomy are defined in this domain.\n\nA shared property for both tasks~(spotting and localization with start and end), regardless of the underlying event complexity, event property (temporal, spatial, or semantic), or data modality~(video or positional data), is the need for labeled event datasets to train and especially to evaluate machine learning approaches.\nIt is common to integrate~\\cite{sanford2020group, fernandez2020soccermap} private event data from data-providers~(e.g., from \\cite{wyscout, opta, stats}) of unknown~\\cite{liu2013reliability} or moderate~(Figure~\\ref{fig:example_annotations}~\\acrshort{dataset_provider} as an example) quality.\n\nIn summary, we observe a lack of a common consensus for the majority of events in the sport.\nNeither precise definitions of individual events nor the temporal annotation or evaluation process are consistent. \nPublicly available datasets are uni-modal, focus on soccer, and often consider only a small subset of events that does not reflect the entire match.\nThese inconsistencies make it for all aforementioned three perspectives difficult to assess the performance of automatic systems and to identify state-of-the-art approaches for the real-world task of fine-grained and ball-centered event spotting from multimodal data sources.\n\nIn this paper, we target the aforementioned problems and present several contributions: 1) We propose a unified taxonomy for \\textit{low-level}, and \\textit{high-level} ball-centered events in invasion games and exemplary refine it to the specific requirements of soccer and handball. This is practicable as most invasion games involve various shared motoric tasks~(e.g., a ball catch), which are fundamental to describe semantic concepts~(involving intention and context from the game).\nHence, it incorporates various base events relating to \\textit{game status}, \\textit{ball possession}, \\textit{ball release}, and \\textit{ball reception}.\n2) We release two multimodal benchmark datasets~(video and audio data for soccer~(\\acrshort{dataset_soccer}), synchronized video, audio, and positional data for handball~(\\acrshort{dataset_handball})) with gold-standard event annotations for a total of 125 minutes of playing time per dataset.\nThese datasets contain frame-accurate manual annotations by domain experts performed on the videos based on the proposed taxonomy~(see Figure~\\ref{fig:example_annotations}).\nIn addition, appropriate metrics suitable for both benchmarking and useful interpretation of the results are reported.\nExperiments on the human performance show the strengths of the \\textit{hierarchical} structure, the successful applicability to two invasion games, and reveal the expected performance of automatic models for certain events.\nWith the increasing complexity of an event~(generally deeper in the \\textit{hierarchy}), ambiguous and differing subjective judgments in the annotation process increases.\nA case study demonstrates that the annotations from data providers should be reviewed carefully depending on the application.\n3) Lastly an \\emph{I3D}~\\cite{carreira2017quo} model for video chunk classification is adapted for the spotting task using a sliding window and non-maximum suppression and is applied.\n\nThe remainder of this paper is organized as follows. \nIn Section~\\ref{sec:rw}, existing definitions for several events and publicly available datasets are reviewed. The proposed universal taxonomy is presented in Section~\\ref{sec:taxonomy}.\nSection~\\ref{sec:datasets} contains a description of the creation of the datasets along with the definition of evaluation metrics, while Section~\\ref{sec:experiments} evaluates the proposed taxonomy, datasets, and baseline concerning annotation quality and uncertainty of specific events. \nSection~\\ref{sec:conclusion} concludes the paper and outlines areas of future work.\n\n\\section{Related Work}\\label{sec:rw}\n\nWe discuss related work on events in invasion games~(Section~\\ref{rw:event_types}) and review existing datasets~(Section~\\ref{rw:datasets}).\n\n\\subsection{Events Covered in Various Invasion Games}\\label{rw:event_types} % Event Taxonomy\n\nCommon movement patterns have been identified in the analysis of spatio-temporal data~\\cite{dodge2008towards} such as concurrence or coincidence.\nWhile these concepts are generally applicable to invasion games, % in this work, \nour taxonomy and datasets focus on single actions of individuals~(players), which do not require a complex description of~(team)~movement patterns.\nFor the sport of handball, there are rarely studies on the description of game situations.\nHowever, the influence of commonly understood concepts, such as shots and rebounds has been investigated~\\cite{burger2013analysis}.\nIn contrast, for soccer, the description of specific game situations has been approached. \\citet{kim2019attacking} focus on the attacking process in soccer.\n\\citet{fernandes2019design} introduce an observational instrument for defensive possessions. The detailed annotation scheme includes 14 criteria with 106 categories %(also considering situational variables, e.g., \\textit{game status}, opponent quality, and match location) \nand achieved sufficient agreement in expert studies. However, the obtained semantic description and subjective rating of defensive possessions largely differ from our fine-grained objective approach. \nA common practice for soccer matches in top-flight leagues is to (manually) capture \\textit{event data}~\\cite{opta, pappalardo2019public}. \nThe acquired data describe the on-ball events on the pitch in terms of soccer-specific events with individual attributes. \nWhile, in general, the inter-annotator agreement for % the modality \nthis kind of data has been validated~\\cite{liu2013reliability}, especially the \\textit{high-level} descriptions of events are prone to errors. \n\\citet{deliege2020soccernet} consider 17 well-defined categories which describe meta events, on-ball events, and semantic events during a soccer match. However, due to the focus of understanding a holistic video rather than a played soccer match, only 4 of the 17 event types describe on-ball actions, while more complex events, i.e., passes, are not considered. \n\\citet{sanford2020group} spot \\textit{passes}, \\textit{shots}, and \\textit{receptions} in soccer using both positional and video data. However, no information regarding definitions and labels is provided.\n\n\\subsection{Datasets}\\label{rw:datasets}\n\nTo the best of our knowledge, there is no publicly available real-world dataset including positional data, video, and corresponding events, not to mention shared events across several sports.\nThe majority of datasets for event detection rely on video data and an individual sport domain.\nIn this context, \\emph{SoccerNetV2}~\\cite{deliege2020soccernet, giancola2018soccernet} was released, which is a large-scale action spotting dataset for soccer videos. %~(17 classes).\nHowever, the focus is on spotting general and rarely occurring events such as \\textit{goals}, \\textit{shots}, or cards.\n\\emph{SoccerDB}~\\cite{jiang2020soccerdb} and \\emph{SSET}~\\cite{feng2020sset} cover a similar set of general events. Even though they relate the events to well-defined soccer rules, they only annotate temporal boundaries.\n\\citet{pappalardo2019public} present a large event dataset, but it lacks definitions of individual events or any other data such as associated videos.\nFor basketball, \\citet{Ramanathan_2016_CVPR} generated a dataset comprising five types of \\textit{shots}, their related outcome (successful), and the \\emph{steal event} by using Amazon Mechanical Turk. Here, the annotators were asked to identify the end-point of these events since the definition of the start-point is not clear. \nThe \\emph{SoccER} dataset~\\cite{morra2020soccer} contains synthetically generated data~(positional data, video, and events) from a game engine. \nThe volleyball dataset~\\cite{ibrahim2016hierarchical} contains short clips with eight group activity labels such as right set or right spike where the center frame of each clip is annotated with per-player actions like standing or blocking. \n\nTo summarize Section~\\ref{rw:event_types} and~\\ref{rw:datasets}, many studies consider only a subset of relevant~(\\textit{low} and \\textit{high-level}) events to describe a match.\nThe quality of both unavailable and available datasets is limited due to missing general definitions~(even spotting vs. duration) apart from well-defined~(per rule) events.\n\n\\section{General Taxonomy Design}\\label{sec:taxonomy}\n\n\\begin{figure*}[tbh]\n\t\\centering\n\t\\includegraphics[width=\\textwidth]{img/general_taxonomy_with_features.pdf}\n\t\\caption{Base taxonomy for invasion games and example refinements for soccer and handball. Starting with basic motoric \\emph{individual ball events}, the finer the hierarchy level, the semantic and necessary context information increases.} \n\t\\label{fig:taxonomy}\n\\end{figure*}\n\nIn this section, we construct a unified taxonomy for invasion games that can be refined for individual sports and requirements~(Figure~\\ref{fig:taxonomy}). \nInitially, targeted sports and background from a sports science perspective are presented in Section~\\ref{subsec:sports}. Preliminaries and requirements for our general taxonomy are listed in Section~\\ref{subsec:characteristics}.\nFinally, the proposed taxonomy, including concrete event types and design decisions, is addressed in Section~\\ref{subsec:categories}.\n\n\\subsection{Targeted Sports \\& Background}\\label{subsec:sports}\nSports games share common characteristics and can be categorized in groups~(\\emph{family resemblances})~\\cite{Wittgenstein1999}.\nBased on that idea,~\\cite{Read1997, Hughes2002} structured sports games into three families: (1)~Net and wall games, which are score dependent (e.g., tennis, squash, volleyball), (2)~striking/fielding games, which are innings dependent (e.g., cricket, baseball), and (3)~invasion games, which are time-dependent (e.g., soccer, handball, basketball, rugby). \nIn this paper, we focus on the latter. \nInvasion games all share a variation of the same objective: to send or carry an object (e.g., ball, frisbee, puck) to a specific target (e.g., in-goal or basket) and prevent the opposing team from reaching the same goal~\\cite{Read1997}. The team that reaches that goal more often in a given time wins. Hence, we argue that the structure of our taxonomy can be applied to all invasion games with a sport-specific refinement of the base events. Please note that we refer to the object in the remainder of this work as a ball for clarity. \n\nBasic motor skills required in all invasion games involve controlled receiving of, traveling with, and sending of the ball~\\cite{Roth2015}, as well as intercepting the ball and challenging the player in possession~\\cite{Read1997}. \nAlthough different invasion games use different handling techniques, they all share the ball as an underlying characteristic. \nThus, we find that ball events are central for describing invasion games. Moreover, since complex sport-science-specific events such as counterattack, possession play, tactical fouls, or any group activities like pressing are rather sports-specific, we focus on on-ball-ball events in this paper and refer to non-on-ball events as future work.  \n\n\\subsection{Characteristics \\& Unification of Perspective}\\label{subsec:characteristics}\n\nWe iteratively design the base taxonomy for invasion games to meet certain standards. To provide insights into this process, the following section details the underlying objectives.  \n\n\\paragraph{Characteristics}\nFor the design of a unified taxonomy for invasion games, we view specific characteristics as favorable.\n~(1)~A \\textit{hierarchical} architecture, in general, is a prerequisite for a clear, holistic structure. We aim to incorporate a format that represents a broad (general) description of events at the highest level and increases in degree of detail when moving downwards in the \\textit{hierarchy}. This enables, i.e., an uncomplicated integration of individual annotations with varying degrees of detail as different annotated events (e.g., \\textit{shot} and \\textit{pass}) can fall back on their common property (here \\emph{intentional ball release}) during evaluation.\nHowever, please note that there exists no cross-relation in the degree of detail between different paths~(colors in Figure~\\ref{fig:taxonomy}). Events from the same \\textit{hierarchical} level may obtain different degrees of detail when from different paths.\n~(2)~We target our taxonomy to be \\textit{minimal} and \\textit{non-redundant} since these characteristics require individual categories to be well-defined and clearly distinguishable from others. In this context, a specific event in the match should not relate to more than one annotation category to support a clear, unambiguous description of the match.\n~(3)~The taxonomy needs to enable an \\emph{exact} description of the match. While the previously discussed \\textit{minimal}, \\textit{non-redundant} design is generally important, an overly focus on these properties may disallow the description of the \\textit{exact} developments in a match. Thus, any neglecting or aggregation for individual categories is carefully considered in the design of the taxonomy.\n~(4)~Finally, we aim for a \\textit{modular expendable} taxonomy. This allows for a detailed examination of specific sports and concepts while still ensuring a globally valid annotation that is comparable (and compatible) with annotations regarding different sports and concepts. \n\n\\paragraph{Unification of Perspectives}\nThe targeted invasion games can generally be perceived from a variety of different perspectives. A mathematical view corresponds to a description of moving objects (players and the ball) with occasional stoppage and object resets (set-pieces reset the ball). On the other hand, a sport-scientist view interprets more complex concepts such as the mechanics of different actions or the semantics of specific situations of play.      \nTo unify these perceptions into a global concept, different approaches such as the \\emph{SportsML} language~\\cite{SportsML} or \\emph{SPADL}~\\cite{decroos2019actions} previously targeted a universal description of the match. However, given that the formats originate from a journalist perspective~\\cite{SportsML} or provide an integration tool~\\cite{decroos2019actions} for data from event providers~(see Section~\\ref{exp:data_prov_quality}), they do not pursue the definition of precise annotation guidelines.\nIn contrast, we aim to provide a universal and \\textit{hierarchical} base taxonomy that can be utilized by different groups and communities for the targeted invasion games.\n\n\\subsection{Annotation Categories}\\label{subsec:categories}\nThe iteratively derived base taxonomy for invasion games is illustrated in Figure~\\ref{fig:taxonomy}. Its %individual \ncategories and attributes comply with the previously discussed characteristics~(see Section~\\ref{subsec:characteristics}) and are outlined in this section~(see Appendix for more detailed definitions).\n\n\\subsubsection{Game Status Changing Event}\nWe initialize the first path in our \\textit{base taxonomy} such that it corresponds with the most elemental properties of invasion games. Thus, we avoid integrating any semantic information (tactics, mechanics) and regard the so-called, \\textit{game status} which follows fixed game rules~\\cite{IFAB, IHF}.\nThe \\textit{game status} provides a deterministic division of any point in the match into either active (running) on inactive (paused) play. %(also referred to as the activeness). \nIn the sense of a \\textit{minimal} taxonomy, we find that an \\textit{exact} description of the current \\textit{game status} is implicitly included by annotating only those events which cause changes to the current \\textit{game status}~(see yellow fields in Figure~\\ref{fig:taxonomy}).\nMoreover, in all targeted invasion games, a shift of the \\textit{game status} from active to inactive only occurs along with a rule-based \\textit{referee's decision} (foul, ball moving out-of-bounds, game end, or sport-specific stoppage of play) while a shift from inactive to active only occurs along \\textit{static-ball-action} (game start, ball in field, after foul, or sport-specific resumption of play). Thus, we discriminate between these two specifications in the path and maintain this \\textit{hierarchical} structure.\n\n\\subsubsection{Ball Possession}\nThe following paths in our taxonomy comprise additional semantic context to enable a more detailed assessment of individual actions and situations. In this regard, we consider the concept of \\textit{possession} (see purple field in Figure~\\ref{fig:taxonomy}) as defined by~\\citet{link2017individual}. Albeit generally not included in the set of rules of all targeted invasion games (exceptions, i.e., for basketball), the assignment of a team's \\textit{possession} is a common practice, and its importance is indicated, for instance, by the large focus of the sports science community~\\cite{camerino2012dynamics, casal2017possession, jones2004possession, lago2010game}. \nSimilar to the \\textit{game status}, we only consider the changes to the \\textit{possession} with respect to a \\textit{minimal} design. \n\n\\subsubsection{Individual Ball Events}\n\nRelated to the concept of individual ball \\textit{possession} are \\textit{individual ball events}, defined as events within the sphere of an individual \\textit{possession}~\\cite{link2017individual}~(see green fields in Figure~\\ref{fig:taxonomy}). \nAlong with the definition for an individual \\textit{possession}, \\citet{link2017individual} define individual \\textit{ball control} as a concept requiring a certain amount of motoric skill.\nThis also involves a specific start and end time for \\textit{ball control} which already enables a more precise examination of \\textit{individual ball event}.\n\nAt the start point of individual \\textit{possession}, the respective player gains (some degree of) \\textit{ball control}. We refer to this moment as a \\textit{ball reception}, describing the motoric skill of gaining \\textit{ball control}.\nAnalogously, at the endpoint of \\textit{possession}, the respective player loses \\textit{ball control}. We specify this situation as a \\textit{ball release}, independent of the related intention or underlying cause. Please note that for a case where a player only takes one touch during an individual \\textit{ball control}, we only consider the \\textit{ball release} as relevant for the description.\nFor time span between \\textit{ball reception} and \\textit{ball release}, in general, the semantic concept of a \\textit{dribbling} applies. However, various definitions for (different types of) \\textit{dribbling} depend on external factors such as the sport, the context, and the perspective. As this semantic ambiguity prevents an \\textit{exact} annotation, we do not list \\textit{dribbling} as a separate category in the taxonomy but refer this concept to sport-specific refinements.     \n\nAt this point, we utilized the two concepts \\textit{game status} and \\textit{possession} in invasion games to design the initial \\textit{hierarchical} levels for a total of three different paths within the taxonomy~(yellow boxes, purple boxes, and two highest \\textit{hierarchical} levels of the green boxes). Accordingly, the required amount of semantic information for the presented levels is within these two concepts. Moreover, since an assessment of these concepts requires low semantic information, we think that the current representation is well-suited for providing annotations in close proximity to the previously presented mathematical perspective on the match. \n\nHowever, we aim for a step-wise approach towards the previously presented sport-scientist perspective for the subsequent \\textit{hierarchical} levels.\nTherefore, we increase the amount of semantic interpretation by regarding additional concepts, i.e., the overall context within a situation or the intention of players.\nTo this end, we distinguish between two different subcategories for a \\textit{ball release}: \\textit{intentional} or \\textit{unintentional ball release}.\nRegarding an \\textit{unintentional ball release}, we generally decide between the categories \\textit{successful interference}~(from an opposing player) and \\textit{self-induced}~(describing a loss of \\textit{ball control} without direct influence of an opponent).\n\\newpage\nIn contrast, for an \\textit{intentional ball release}, we further assess the underlying intention or objective of a respective event. We discriminate between a \\textit{pass}, including the intention that a teammate receives the released ball, and a \\textit{shot} related with an intention (drawn) towards the target. \nIn some rare cases, the assessment of this intention may be subjective and difficult to determine. However, specific rules in invasion games require such assessment, i.e., in soccer, the goalkeeper is not allowed to pick up a released ball from a teammate when it is \"\\emph{deliberately}\" kicked towards him~\\cite{IFAB}.\n\nPlease note that we define \\textit{individual ball events} as mutually exclusive, i.e., only one event from that path can occur at a specific timestamp. However, a single point in time may generally include multiple events from different paths in the \\textit{taxonomy}. \nExamples for this, in particular, are set-pieces. Here, we annotated a \\textit{static-ball event} such as \\emph{ball in field}, indicating the previously discussed shift of the \\textit{game status}, and an \\textit{individual ball event} (e.g., a \\textit{pass}) describing the concrete execution. This is necessary as the type of \\textit{static-ball event} does not definitely determine the type of \\textit{individual ball event}~(i.e., a free-throw in basketball can theoretically be played as a pass from the rim). Nevertheless, since each set piece involves some sort of \\textit{ball release}~(per definition of rule, a set-piece permits a double contact of the executing player), an \\textit{exact} annotation of set-pieces is provided by the implicit link of simultaneous~(or neighboring) \\textit{ball release} and \\textit{static ball events}.\n\n\\subsubsection{Attributes}\nA global method to add semantic information to the annotation is provided by defining specific \\textit{attributes} for certain events. \nWhile not representing a specific path in the \\textit{base taxonomy}, an \\textit{attribute} is defined as a name or description like \\emph{pixel location} of the event in the video~(Figure~\\ref{fig:taxonomy} upper-right) and thus provides additional information to the respective event.\nWhen an \\textit{attribute} is defined for an event at a certain \\textit{hierarchical} level, it is valid for all child events in lower levels.\n\n\\section{Events in Invasion Games Dataset}\\label{sec:datasets}\n\n\\begin{table}[b]\n\\caption{Dataset distribution: Approx. 40\\,\\% of all events are reserved for testing, i.e., two of five matches, respectively.}\n\\label{tab:stats}\n\\centering\n\\small\n\\fontsize{7}{10}\\selectfont\n\\def\\arraystretch{0.7}\n\n\\begin{tabularx}{\\linewidth}{Xlrr}\n\\toprule\nShared Parent Event & Event &    \\acrshort{dataset_soccer} &  \\acrshort{dataset_handball} \\\\\n\\midrule \\midrule\n\\textbf{ball possession change} &  &            171 &              136 \\\\ \\hline\n\\textbf{ball reception} &  &           923 &            2268 \\\\ \\hline\n\\textbf{ball release} &  &           1531 &            2470 \\\\ \\hline\n\\textbf{pass} &  &            1346 &              2292 \\\\\n & intercepted &            83 &              14 \\\\\n                           & off target &           175 &               9 \\\\\n                           & successful deflected &            24 &               6 \\\\\n                           & successful untouched &          1064 &            2263 \\\\\n\\textbf{shot} &  &            31 &              175 \\\\\n                           & blocked/intercepted &   17 &              61 \\\\\n                           & goal frame &             0 &               8 \\\\\n                           & off target &             8 &              12 \\\\\n                           & successful &             6 &              94 \\\\\n\\textbf{unintentional} & other &            74 &               0 \\\\\n                           & successful interference &            80 &               3 \\\\ \\hline\n\\textbf{referee decision}         &        &            142 &               252 \\\\ \\hline\n                           & ball out of field &           101 &              21 \\\\\n                           & foul &            32 &             114 \\\\\n                           & goal &             3 &              86 \\\\\n                           & other &             5 &              10 \\\\\n                           & two min &             n.d. &               8 \\\\\n                           & yellow &             1 &              13 \\\\ \\hline\n\\textbf{static ball action}         &         &            121 &               207 \\\\ \\hline\n                           & corner &            11 &               n.d. \\\\\n                           & free-kick &            29 &              84 \\\\\n                           & game start &             1 &               2 \\\\\n                           & goal-kick &            18 &               n.d. \\\\\n                           & kick-off &             1 &              87 \\\\\n                           & other &             0 &               6 \\\\\n                           & penalty &             1 &              11 \\\\\n                           & throw-in &            60 &              17 \\\\\n\\bottomrule\n\\end{tabularx}\n\n   \n\n\\end{table}\nThe following Section~\\ref{sec:dataset_description} describes our multimodal (video, audio, and positional data) and multi-domain (handball and soccer) dataset for ball-centered event spotting~(\\acrshort{dataset}). \nIn Section~\\ref{sec:metrics}, appropriate metrics for benchmarking are introduced.\n\n\\subsection{Data Source \\& Description}\\label{sec:dataset_description}\nTo allow a large amount of data diversity as well as a complete description of a match, we regard longer sequences from different matches and stadiums. \nWe select 5 sequences Ã  5 minutes from 5 matches resulting in 125 minutes of raw data, respectively, for handball and soccer.\n\n\\paragraph{Data Source}\nFor the handball subset, referred as \\acrshort{dataset_handball}, synchronized video and positional data from the first German league from 2019 are kindly provided by the Deutsche Handball Liga and Kinexon\\footnote{\\url{https://kinexon.com/}} and contain HD~($1280 \\times 720$ pixels) videos at 30\\,fps and positional data for all players in 20\\,Hz.\nThe videos include unedited recordings of the game from the main camera~(i.e., no replays, close-ups, overlays, etc.).\nSome events are more easily identified from positional data, other events that require visual features can be extracted from video, making \\acrshort{dataset_handball} interesting for multimodal event detection.\n\nFor the soccer dataset, referred to as \\acrshort{dataset_soccer}, we collect several publicly available broadcast recordings of matches from the FIFA World Cup (2014, 2018) in 25\\,fps due to licensing limitations without positional data.\nA characteristic for annotation purpose is naturally given regarding the utilized videos. \\acrshort{dataset_soccer} includes difficulties for the annotation, such as varying camera angles, long replays, or the fact that not all players are always visible in the current angle, making it challenging to capture all events for a longer sequence.\nAll events are included that are either visible or can be directly inferred by contextual information~(e.g., timestamp of a \\emph{ball release} is not visible due to a cut from close-up to the main camera). However, with exceptions for \\emph{replays} as these do not reflect the actual time of the game.\n\n\\paragraph{Annotation Process \\& Dataset Properties}\\label{sec:dataset:annotation_process}\n\nTo obtain the dataset annotations, the general task is to spot the events in the lowest \\textit{hierarchy} level since the parent events~(from higher \\textit{hierarchy} levels) are implicitly annotated. Therefore, the taxonomy~(Section~\\ref{sec:taxonomy}) and a concrete annotation guideline including definitions for each event, examples, and general hints~(see Appendix) were used.\nAn example for \\emph{unintentional ball release - self-induced} is this situation: A player releases the ball without a directly involved opponent, e.g., slips/stumble or has no reaction time for a controlled \\textit{ball release}. For instance, after an \\textit{intercepted/blocked pass} or \\textit{shot} event. Timestamp: on \\textit{ball release}.\n\nWe hired nine annotators~(sports scientists, sports students, video analysts). \nDue to the complexity of soccer, four of them annotated each sequence of the \\acrshort{dataset_soccer} test set.\nNote, that two of the five matches are indicated as a test set~(\\acrshort{dataset_soccer}-T, \\acrshort{dataset_handball}-T), respectively.\nIn addition, one inexperienced person without a background in soccer annotated the \\acrshort{dataset_soccer}-T.\nFor \\acrshort{dataset_handball}, three experienced annotators also processed each sequence of the test set.\nAn experienced annotator has labeled the remaining data~(e.g., reserved for training).\nThe annotation time for a single video clip is about 30 minutes for both datasets.\nThe number of events given the entire dataset and one expert annotation is presented in Table~\\ref{tab:stats} and we refer to Section~\\ref{exp:aggreement} for the assessment of the annotation quality.\nFigure~\\ref{fig:example_annotations} shows two sequences with annotations from several persons as an example.\n\nFor each dataset, we assess the human performance~(see Section~\\ref{exp:aggreement}) for each individual annotator. The annotation with the highest results is chosen for release and as reference in the evaluation of the baseline~(see Section~\\ref{exp:baseline}).\n\n\\subsection{Metrics}\\label{sec:metrics}\n\nFor events with a duration, it is generally accepted~\\cite{lin2019bmn, caba2015activitynet} to measure the \\acrfull{temporal_iou}. \nThis metric computes for a pair of annotations by dividing the intersection~(overlap), by the union of the annotations. The \\acrshort{temporal_iou} for multiple annotations is given by the ratio of aggregated intersection and union. \nRegarding events with a fixed timestamp, a comparison between annotations is introduced in terms of temporal tolerance. Thereupon, when given a predicted event, count a potentially corresponding ground-truth event within the respective class as a true-positive, if and only if it falls within a tolerance area~(in seconds or frames). \nYet, the definition of corresponding events from two different annotations is non-trivial~\\cite{sanford2020group, deliege2020soccernet, giancola2018soccernet}, especially for annotations with different numbers of annotated events. \nA common method to circumvent this task is to introduce a simplification step using a \\acrfull{nnm} which considers events with the same description on the respective \\textit{hierarchy} level. \nAfter defining true positive, false positive, and false negative, this enables the computation of the \\acrfull{temporal_ap}~(given by the average over multiple temporal tolerance areas~\\cite{deliege2020soccernet, giancola2018soccernet}) or precision and recall for a fixed temporal tolerance area~\\cite{sanford2020group}.\n\nHowever, as the \\acrshort{nnm} generally allows many-to-one mappings, a positive bias is associated with it. For instance, when multiple events from a prediction are assigned to the same ground-truth event~(e.g., \\textit{shot}), they might all be counted as true positives~(if within the tolerance area), whereas the mismatch in the number of events is not further punished. This bias is particularly problematic for automatic solutions that rely on~(unbiased) objectives for training and evaluation. \nTherefore, \\citet{sanford2020group} apply a \\acrfull{nms} which only allows for a single prediction within a respective \\acrshort{nms} window. While this presents first step, a decision on the (hyper-parameter) \\acrshort{nms} window length can be problematic. When chosen too large, the \\acrshort{nms} does not allow for a correct prediction of temporally close events. In contrast, when chosen too small, the \\acrshort{nms} only partially accounts for the issue at hand. Moreover, the lack of objectivity draws a hyper-parameter tuning, e.g., a grid search, towards favoring smaller window lengths for \\acrshort{nms}. \n\nTo avoid these issues, we propose an \\emph{additional} method to establish a one-to-one mapping for corresponding events from two annotations~(with possibly different numbers of events). \nIn theory, this mapping can only be established if the number of event types between the annotations is equal. However, in practice, this requirement is rarely fulfilled for the whole match. Moreover, even when fulfilled, possibly additional and missing events might cancel each other out. \nBased on this, a division of the match into independent~(comparable) segments is a reasonable pre-processing step. Thus, we define a \\textit{sequence} as the time of an active match between two \\textit{game status-changing events}~(objectively determined by the set of rules~\\cite{IFAB, IHF}). Then, (i)~we count the number of \\textit{sequences} in two \\textit{annotations} to verify that no \\textit{game status changing events} were missed~(and adopt \\textit{game status changing events} that were missed), (ii)~count the number of annotated events of the same category within a \\textit{sequence}, and (iii)~assign the corresponding events relative to the order of occurrence within the \\textit{sequence} only if the number of annotations matches.\nIf this number does not match, we recommend to either separately consider the \\textit{sequence} or to fully discard the included \\textit{annotations}.\nIn analogy to \\acrshort{nnm}, we refer to this method as \\acrfull{scm}.\nPlease note that, relative to the degree of detail within the compared \\textit{annotations}, the contained additional information (for example player identities) can be used to increase the degree of detail in \\acrshort{scm}.\n\n\\section{Experiments}\\label{sec:experiments}\n\n\\begin{table}[b!]\n\\caption{Measuring the temporal IoU of several annotators for events with a duration.}\n\\label{tab:tiou}\n\\centering\n\\setlength{\\tabcolsep}{2pt}\n\\small\n\\fontsize{7}{10}\\selectfont\n\\begin{tabularx}{\\linewidth}{Xl|r|r}\n\\toprule\nAnnotation & Dataset &  Game Status     & Ball Possession   \\\\ \\midrule                        \n\\multicolumn{1}{l|}{\\multirow{2}{*}{Average Experienced}}               & \\acrshort{dataset_handball}-T  &     $0.68 \\pm 0.02$            &    $0.72 \\pm 0.02$          \\\\\n\\multicolumn{1}{l|}{}                                           & \\acrshort{dataset_soccer}-T  &   $ 0.92 \\pm 0.01 $            &  $0.78 \\pm 0.03$             \\\\\n\\multicolumn{1}{l|}{Inexperienced vs. Experienced}                                           &\\acrshort{dataset_soccer}-T  &     $0.92 \\pm 0.03$           & $0.73 \\pm 0.05$  \\\\\n\\bottomrule\n\\end{tabularx}\n\\end{table}\n\n\\newcolumntype{R}[2]{%\n    >{\\adjustbox{angle=#1,lap=\\width-(#2)}\\bgroup}%\n    l%\n    <{\\egroup}%\n}\n\\newcommand*\\rot{\\multicolumn{1}{R{90}{1em}}}% no optional argument here, please!\n\n\\begin{table*}[tbh!]\n\\caption{Expected human and from the baseline achieved performance: Precision and Recall (in~$\\%$) after applying Nearest Neighbour (NN) matching and the proposed Sequence Consistency (SC) matching for representative events at multiple hierarchy levels. Note, that an appropriate event-specific evaluation window length~$w_\\text{eval}$~[s] is applied. The number of consistent events for SC matching (in~$\\%$) are indicated in brackets. {\\normalfont\\footnotesize $^2$No \\emph{ball out of field} annotations provided.}}\n\\label{tab:aggreement_nn_cm}\n\\centering\n\\setlength{\\tabcolsep}{2.2pt}\n\\small\n\\fontsize{7}{10}\\selectfont\n\\def\\arraystretch{1.0} % 0.85\n\\begin{tabularx}{\\textwidth}{l|l|llllllllllllllllll}\n\\toprule\n                        & Event                    & \\multicolumn{2}{l}{\\emph{Status Change}}   & \\multicolumn{2}{l}{\\emph{Reception}}   & \\multicolumn{2}{l}{\\emph{Release}}   & \\multicolumn{2}{l}{\\emph{Int. Release}}   & \\multicolumn{2}{l}{\\emph{Unint. Release}}   & \\multicolumn{2}{l}{\\emph{Shot}}   & \\multicolumn{2}{l}{\\emph{Suc. Interf.}}   & \\multicolumn{2}{l}{\\emph{Suc. Pass}}   & \\multicolumn{2}{l}{\\emph{Interc. Pass}}   \\\\\nDataset                 & Matching                 & NN       & SC           & NN       & SC           & NN       & SC           & NN       & SC           & NN       & SC           & NN      & SC            & NN       & SC           & NN       & SC           & NN       & SC           \\\\ \\hline \n\n\\multirow{6}{*}{\\acrshort{dataset_handball}-T} & $w_\\text{eval}$            & \\multicolumn{2}{c}{6.04} & \\multicolumn{2}{c}{0.44} & \\multicolumn{2}{c}{0.44} & \\multicolumn{2}{c}{0.44} & \\multicolumn{2}{c}{2.04} & \\multicolumn{2}{c}{0.44} & \\multicolumn{2}{c}{2.04} & \\multicolumn{2}{c}{0.44} & \\multicolumn{2}{c}{0.44} \\\\ \\hline \\midrule\n& Num. Events               & \\multicolumn{2}{c}{$135.7 \\pm 23.0$}  & \\multicolumn{2}{c}{$821.0 \\pm 10.7$}  & \\multicolumn{2}{c}{$844.7 \\pm 26.8$}  & \\multicolumn{2}{c}{$841.0 \\pm 26.5$}  & \\multicolumn{2}{c}{$3.7 \\pm 0.5$}  & \\multicolumn{2}{c}{$62.7 \\pm 1.2$}  & \\multicolumn{2}{c}{$3.0 \\pm 0.8$}  & \\multicolumn{2}{c}{$765.7 \\pm 25.8$}  & \\multicolumn{2}{c}{$6.7 \\pm 0.9$}  \\\\ \\cline{2-20}\n                        & Mean Exp. (Prc.=Rec.)              \n                        & 78.9         & 90.0 (40)\n                        & 92.2         & 89.7 (45)\n                        & 92.7         & 82.3 (33)\n                        & 92.9         & 83.3 (33)\n                        & 18.2         & 100 (18)\n                        & 96.3         & 99.4 (93)\n                        & 22.2         & 100 (22)\n                        & 92.4         & 82.6 (37)    \n                        & 45.0         & 100 (45)\n                        \\\\\n                        & Baseline vs. Exp. Prc.              \n                        &\\multicolumn{2}{c}{-}\n                        & 45.6         &    0.0 (0)          \n                        &  \\multicolumn{2}{c}{-}           \n                        &  \\multicolumn{2}{c}{-}            \n                        & \\multicolumn{2}{c}{-}            \n                        &  43.2        &   0.0 (0)            \n                        &  \\multicolumn{2}{c}{-}         \n                        &  46.4        &    0.0 (0)          \n                        &    \\multicolumn{2}{c}{-} \n                        \\\\\n                        & Baseline vs. Exp. Rec.              \n                        & \\multicolumn{2}{c}{-}\n                        & 93.9         &    0.0 (0)          \n                        &  \\multicolumn{2}{c}{-}          \n                        &  \\multicolumn{2}{c}{-}            \n                        &  \\multicolumn{2}{c}{-}            \n                        & 41.0         &   0.0 (0)            \n                        & \\multicolumn{2}{c}{-}           \n                        & 91.5         &  0.0 (0)            \n                        &    \\multicolumn{2}{c}{-} \n\n                        \n \\\\ \\hline \\midrule\n \n\\multirow{4}{*}{\\acrshort{dataset_soccer}-T}\n& Num. Events               & \n                        \\multicolumn{2}{c}{$113.4 \\pm 3.4$ %(77)\n                        } & \\multicolumn{2}{c}{$362.6 \\pm 5.6$ %(61)\n                        } & \\multicolumn{2}{c}{$550.2  \\pm 13.4$ %(45)\n                        }  & \\multicolumn{2}{c}{$500.0  \\pm 7.0$ %(49)\n                        }  & \\multicolumn{2}{c}{$50.2 \\pm 14.2$ %(48)\n                        }  & \\multicolumn{2}{c}{$12.2  \\pm 0.4$ %(98)\n                        } & \\multicolumn{2}{c}{$32.0 \\pm 4.3$ %(56)\n                        } & \\multicolumn{2}{c}{$385.2 \\pm 4.5$ %(54)\n                        } & \\multicolumn{2}{c}{$58.8  \\pm 18.9$ %(38)$\n                        }  \\\\ \\cline{2-20}\n                        & Mean Exp. (Prc.=Rec.)           \n                        & 95.0 & 98.7 (78)\n                        & 94.9 & 94.5 (61)\n                        & 95.7 & 90.8 (42)\n                        & 96.2 & 93.3 (48)\n                        & 62.7 & 84.4 (48)\n                        & 100.0 & 100.0 (100)\n                        & 68.5 & 91.8 (57)\n                        & 96.0 & 94.0 (54)\n                        & 60.0 & 85.0 (33)\n                        \\\\\n                        \n                        & Inexp. vs Exp. Prc.     \n                        & 95.9 & 98.8 (77)\n                        & 90.3 & 89.7 (61)\n                        & 93.4 & 90.0 (49)\n                        & 94.8 & 87.9 (51)\n                        & 64.1 & 78.9 (49)\n                        & 92.3 & 100 (100)\n                        & 64.5 & 83.3 (55)\n                        & 92.6 & 93.2 (53)\n                        & 65.5 & 85.5 (44)\n                        \\\\\n                        & Inexp. vs Exp. Rec.     \n                        & 93.8 & 98.8 (74)\n                        & 88.9 & 89.6 (60)\n                        & 92.5 & 89.5 (49)\n                        & 93.2 & 87.0 (50)\n                        & 60.0 & 78.9 (47)\n                        & 100 & 100 (92)\n                        & 62.0 & 83.3 (55)\n                        & 94.3 & 92.5 (54)\n                        & 71.4 & 85.5 (48)\n                        \\\\ \\hline \\midrule \n                        \\multirow{3}{*}{\\acrshort{dataset_provider}}\n                         & Num. Events & \\multicolumn{2}{c}{$649.0 \\pm 8.0$\\tablefootnote{ }} \n                         & \\multicolumn{2}{c}{-}  &  \\multicolumn{2}{c}{-}  & \\multicolumn{2}{c}{-}  & \\multicolumn{2}{c}{-}  & \\multicolumn{2}{c}{$100.5 \\pm 2.5$}  & \\multicolumn{2}{c}{-}  & \\multicolumn{2}{c}{$2428.0 \\pm 24.0$}  & \\multicolumn{2}{c}{-}  \\\\\n                        \\cline{2-20}\n                        & Data Provider Prc.\n                        &  74.7 & 71.1 (96)          \n                        &  \\multicolumn{2}{c}{-} \n                        &  \\multicolumn{2}{c}{-} \n                        &  \\multicolumn{2}{c}{-}               \n                        &  \\multicolumn{2}{c}{-}               \n                        &  6.8 & 8.0 (73)                \n                        &  \\multicolumn{2}{c}{-}               \n                        & 12.9         & 12.3 (65)              \n                        &  \\multicolumn{2}{c}{-} \n                        \\\\\n                        & Data Provider Rec.\n                        &  73.5 & 71.1 (93)           \n                        &  \\multicolumn{2}{c}{-} \n                        &  \\multicolumn{2}{c}{-} \n                        &  \\multicolumn{2}{c}{-}               \n                        &  \\multicolumn{2}{c}{-}              \n                        &  7.1 & 8.1 (76)               \n                        &  \\multicolumn{2}{c}{-}               \n                        & 12.6         & 12.6 (65)              \n                        &  \\multicolumn{2}{c}{-}  \n                    \n \\\\ \\bottomrule\n\\end{tabularx}\n\\end{table*}\n\nWe assess the quality of our proposed dataset by measuring the expected human performance~(Section~\\ref{exp:aggreement}) and present a baseline classifier that only utilizes visual features~(Section~\\ref{exp:baseline}). \nThe %uncertainty \nquality of annotations from an official data provider is evaluated in Section~\\ref{exp:data_prov_quality}.\n\n\\subsection{Assessment of Human Performance}\\label{exp:aggreement}\n\nDespite we aim to provide as clear as possible definitions for the annotated events, the complex nature of invasion games might lead to uncertain decisions during the annotation process.\nAccording to common practice, we assess the annotation quality and, hence, expected performance of automatic solutions by measuring the average human performance on several evaluation metrics~(Section~\\ref{sec:metrics}). \nIn this respect, one annotator is treated as a predictor and compared to each other annotator, respectively, considered as reference. \nConsequently, the average over all reference annotators represents the individual performance of one annotator while the average across all individual performances corresponds to the average human performance. We report the average performance for experienced annotators for \\acrshort{dataset_handball}-T and \\acrshort{dataset_soccer}-T while we additionally assess the generality of our taxonomy by comparing the individual performance of domain experts and an inexperienced annotator for \\acrshort{dataset_soccer}-T.\n\nFor events with a duration (\\emph{game status}, \\emph{possession}), we report the \\acrshort{temporal_iou}.\nTo evaluate the event spotting task, a sufficient assessment of human performance requires a multitude of metrics. Similar to~\\citet{sanford2020group}, we report the precision and recall by applying the \\acrshort{nnm} for individual events at different levels of our proposed \\textit{hierarchy}. %However, in contrast to previous works, \nWe define strict but meaningful tolerance areas for each event to support the general interpretability of the results. \nAdditionally, we apply the \\acrshort{scm} where we compensate for a possible varying number of sequences by adopting the sequence borders in case of a possible mismatch. \nWe report precision and recall for events from consistent sequences along with the percentage of events from consistent sequences. The Appendix provides a detailed overview of each individual annotator performance.\n\n\\paragraph{Results \\& Findings}\n\nThe overall results for events with a duration~(Table~\\ref{tab:tiou}) and events with a timestamp~(Table~\\ref{tab:aggreement_nn_cm}) indicate a general agreement for the discussed concepts.\nMoreover, the minor discrepancies in the performance of the experienced and the inexperienced annotator for \\acrshort{dataset_soccer}-T also indicate that a sufficient annotation of our base taxonomy does generally not require expert knowledge. This observation shows the low amount of semantic interpretation included in our proposed taxonomy. Please note that due to the asymmetry in the comparison (one inexperienced annotator as prediction and four experienced annotators as reference), for this case, the precision and recall differ in Table~\\ref{tab:aggreement_nn_cm}.\n\nIn Table~\\ref{tab:tiou}, the agreement for \\textit{game status} in soccer is significantly higher than the agreement in \\textit{possession}. For handball, while the results for \\textit{possession} are comparable to soccer, the agreement for \\textit{game status} is significantly lower. This likely originates from the rather fluent transitions between active and inactive play which complicate a clear recognition of \\textit{game status change events} in handball.\nIn contrast, general similarities in the annotations for \\acrshort{dataset_soccer}-T and \\acrshort{dataset_handball}-T can be found in agreement for individual ball events~(Table~\\ref{tab:aggreement_nn_cm}). Beneath the previously discussed differences in the ambiguity of \\textit{game status}, reflected in inferior agreement of \\textit{game status change events}, similar trends are observable in both sports~(limitations, i.e., for infrequent events in handball such as \\textit{unintentional ball release} or \\textit{successful interference}).\nFor both datasets, the \\textit{hierarchical} structure positively influences the results where the highest level shows a high overall agreement which decreases when descending in the \\textit{hierarchy}. This relates to the similarly increasing level of included semantic information~(see Section~\\ref{subsec:characteristics}) complicating the annotation. However, this general observation does not translate to each particular event in the taxonomy. \n\nThe results for \\acrshort{scm} provide a valuable extension to the informative value of \\acrshort{nnm}, i.e., to detect the positive bias~(Section~\\ref{sec:metrics}). For instance, the \\textit{successful pass} for \\acrshort{dataset_handball}-T shows a general high agreement. However, a positive bias in this metric can be recognized regarding the comparatively low amount of sequence-consistent events~(in brackets). These differences are probably caused by the high frequency of \\textit{successful passes} in handball and the connected issues with assignment, detailed in Section~\\ref{sec:metrics}.  \n\nTypical misclassifications are often related to the assignment of intention. For ambiguous situations~(see Figure~\\ref{fig:example_annotations}), this assignment can be difficult and largely depending on the outcome. For instance, if a played ball lands close to a teammate the situation will rather be annotated as \\textit{intentional ball release}. However, this does not comply with the concept of intention that needs to be distinguished in the moment of the execution. Yet, due to the complex nature of invasion games, even the player who played the ball might not give a definite answer.          \nA different type of error are temporal mismatches~(such as delays). While generally excluded from the annotation, still, a common source for these temporal differences are cuts, replays, or close-ups in the video data. As we aim to include the majority of events\nif the action on the pitch can be derived from the general situation~(i.e., a replay only overlaps with a small fraction of an event), a common source of error are different event times. This is especially relevant for \\textit{game status change events} where cuts and replays commonly occur.\n\n\\subsection{Vision-based Baseline}\\label{exp:baseline}\nTo present some potential outputs of an automated classifier model, we create a baseline that only uses visual features from the raw input video to spot events.\nDue to the lack of existing end-to-end solutions for event spotting and density of events (approx. each second in \\acrshort{dataset_handball}), we follow common practice, where first a classifier is trained on short clips and then a sliding window is applied to produce frame-wise output~(e.g., feature vectors or class probabilities). \nWe follow \\cite{sanford2020group} and directly apply \\acrshort{nms} to the predicted class probabilities to suppress several positive predictions around the same event.\n\n\\subsubsection{Setup for Video Chunk Classification}\nFor the model, we choose an Inflated 3D ConvNet~\\emph{I3D}~\\cite{carreira2017quo, NonLocal2018} with a \\emph{ResNet-50} as backbone which is pre-trained on \\emph{Kinetics400}~\\cite{kay2017kinetics}. \nWe select three classes~(\\emph{reception}, \\emph{successful pass}, and \\emph{shot})~(plus a background event). We train one model for \\acrshort{dataset_handball} on the entire~(spatial) visual content with fixed input dimension of $Tx3x256x456$.\nShort clips~($T=32$ frames), centered around the annotation, are created to cover temporal context. For the background event, all remaining clips are taken with a stride of 15 frames. \nTemporal resolution is halved to during training~(i.e., 15\\,fps for \\acrshort{dataset_handball}).\nFor remaining details we refer to the Appendix.\nThe model with the lowest validation loss is selected for the event spotting task.\n\n\\subsubsection{Evaluating the Event Spotting Task}\nWe collect all predicted probabilities at each frame using a sliding window and apply \\acrshort{nms} on validated event-specific filter lengths~$w^e_\\text{nms}$. \nAs several events can occur at the same time, for each event~$e$ a confidence threshold~$\\tau_e$ is estimated.\nBoth hyper-parameters are optimized for each event on the $F_1$ score with \\acrshort{nnm} using a grid search on the training dataset. We use the same search space as \\citet{sanford2020group}.\n\nResults are reported in Table~\\ref{tab:aggreement_nn_cm} where precision and recall are calculated considering the expert annotation with the highest human performance as ground-truth.\nDespite the limited amount of training data, the baseline demonstrates that our proposed datasets are suitable for benchmarking on-ball events.\nWe qualitatively observe that an excessive number of positive predictions in spite of \\acrshort{nms} causes bad performance using \\acrshort{scm}, which is only partly visible when using \\acrshort{nnm}. \nThis confirms the need for the proposed metric and identifies the error cases of the baseline.\nThe model achieves sufficiently robust recognition performance with temporal centered ground-truth events, it predicts the actual event with high confidence when an ground-truth event in the sliding window is not centered.\nWe refer to future work~(1)~to improve the visual model for instance with hard-negative sample mining, or temporal pooling~\\cite{giancola2021temporally} and~(2)~for the usage of multimodal data~(e.g., \\cite{vanderplaetse2020improved}).\n\n\\subsection{Annotation Quality of Data Providers}\\label{exp:data_prov_quality}\n\nAs previously discussed, annotations (in soccer) are frequently obtained from data providers that are not bound to fulfill any requirements or to meet a common gold standard.\nTo this end, we explore the quality of a data provider on the exemplary \\acrfull{dataset_provider} which contains four matches of a first European soccer league from the 2014/2015 season. Here, we avoid an examination of semantically complex events like \\textit{successful interference} % where differences in the definition may constrain the obtained results.\nand perform an examination of the \\textit{successful pass}, \\textit{shot}, and \\textit{game status changing events} where we find the largest compliance with the data-provider event catalog ~\\cite{liu2013reliability}. To obtain a reference, we instruct a domain expert to acquire frame-wise accurate annotations by watching unedited recordings of the matches. \nSimilar to the previous experiments, we compute precision and recall while we account for differences in the number of total annotated events by application of \\acrshort{scm}~(with specific consideration of passing player identities for passes). The results are given in Table~\\ref{tab:aggreement_nn_cm} and a representative example is displayed in Figure~\\ref{fig:example_annotations}.\nWe observe a low agreement between the precise expert and the data-provider annotation~(compared to results for~\\acrshort{dataset_soccer}). While due to the consideration of player identities, slightly more \\textit{successful pass} events are consistent, the agreement for \\acrshort{scm} is also poor.\nThis is caused by a general imprecision in the data-provider annotation. This imprecision likely originates from the real-time manual annotation which data providers demand. The human annotators are instructed to collect temporal (and spatial) characteristics of specific events while simultaneously deciding on a corresponding event type from a rather large range of \\textit{high-level} event catalog ~\\cite{liu2013reliability}. \nThese results reveal the need for exact definitions and annotation guidelines and emphasize the value of automatic solutions.\nWe intend to show with this exploratory experiment that, the quality of the annotations provided should be taken into account depending on the targeted application. \nOf course, we cannot draw conclusions about quality of other data and seasons based on this case study.\n\n\\section{Conclusions}\\label{sec:conclusion}\nIn this paper, we have addressed the real-world task of fine-grained event detection and spotting in invasion games. \nWhile prior work already presented automatic methods for the detection of individual sport-specific events with focus on soccer, they lacked objective event definitions and complete description for invasion games.\nDespite the wide range of examined events, their complexity, and ambiguity, the data quality had not been investigated making the assessment of automatic approaches difficult. Even current evaluation metrics are inconsistent. \nTherefore, we have contributed a \\textit{hierarchical} taxonomy that enables a \\textit{minimal} and \\textit{objective} annotation and is \\textit{modular expendable} to fit the needs of various invasion games. \nIn addition, we released two multimodal datasets with gold standard event annotations~(soccer and handball).\nExtensive evaluation have validated the taxonomy as well as the quality of our two benchmark datasets while a comparison with data-provider annotations revealed advantages in annotation quality.\nThe results have shown that high agreement can be achieved even without domain knowledge. In addition, the \\textit{hierarchical} approach demonstrates that (semantically) complex events can be propagated to a shared parent event to reach an increase in agreement.\n\nWith the presented taxonomy, datasets, and baseline, we create a foundation for the design and the benchmarking of upcoming automatic approaches for the spotting of on-ball events.\nAlso, other domains that work with video, positional, and event data, could benefit from the taxonomy and the datasets introduced in this paper.\nIn the future, we plan to integrate non-on-ball events into the taxonomy and to exploit \\textit{hierarchical} information and attention to the ball position during training of a deep model.\n\n\\section*{Acknowledgement}\nThis project has received funding from the German Federal Ministry of Education and Research (BMBF -- Bundesministerium fÃ¼r Bildung und Forschung) under 01IS20021A, 01IS20021B, and 01IS20021C.\nThis research was supported by a grant from the German Research Council (DFG, Deutsche Forschungsgemeinschaft) to DM (grant ME~2678/30.1).\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{[Article Title]{Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports}\n\n\n\n\\author[1]{\\fnm{Haopeng} \\sur{Li}}\n\n\\author[2]{\\fnm{Andong} \\sur{Deng}}\n\n\\author*[3]{\\fnm{Jun} \\sur{Liu}}\n\n\\author[4]{\\fnm{Hossein} \\sur{Rahmani}}\n\n\\author[5]{\\fnm{Yulan} \\sur{Guo}}\n\n\\author[6]{\\fnm{Bernt} \\sur{Schiele}}\n\n\\author[7]{\\fnm{Mohammed} \\sur{Bennamoun}}\n \n\\author*[1,8]{\\fnm{Qiuhong} \\sur{Ke}}\\email{qiuhong.ke@monash.edu}\n\n\n\\affil[1]{\\orgdiv{School of Computing and Information Systems}, \\orgname{University of Melbourne}}\n\n\\affil[2]{\\orgdiv{Center for Research in Computer Vision}, \\orgname{University of Central Florida}}\n\n\\affil[3]{\\orgdiv{Information Systems Technology and Design (ISTD) Pillar}, \\orgname{Singapore University of Technology and Design}}\n\n\\affil[4]{\\orgdiv{School of Computing and Communications}, \\orgname{Lancaster University}}\n\n\\affil[5]{\\orgdiv{School of Electronics and Communication Engineering}, \\orgname{Sun Yat-sen University}}\n\n\\affil[6]{\\orgdiv{Department of Computer Vision and Machine Learning}, \\orgname{Max Planck Institute for Informatics, Saarland Informatics Campus}}\n\n\\affil[7]{\\orgdiv{School of Physics, Maths and Computing}, \\orgname{University of Western Australia}}\n\n\\affil[8]{\\orgdiv{Department of Data Science \\& AI}, \\orgname{Monash University}}\n\n\n\n\\abstract{Reasoning over sports videos for question answering is an important task with numerous applications, such as player training and information retrieval. However, this task has not been explored due to the lack of relevant datasets and the challenging nature it presents. Most datasets for video question answering (VideoQA) focus mainly on general and coarse-grained understanding of daily-life videos, which is not applicable to sports scenarios requiring professional action understanding and fine-grained motion analysis. In this paper, we introduce the first dataset, named Sports-QA, specifically designed for the sports VideoQA task. The Sports-QA dataset includes various types of questions, such as descriptions, chronologies, causalities, and counterfactual conditions, covering multiple sports. Furthermore, to address the characteristics of the sports VideoQA task, we propose a new Auto-Focus Transformer (AFT) capable of automatically focusing on particular scales of temporal information for question answering. We conduct extensive experiments on Sports-QA, including baseline studies and the evaluation of different methods. The results demonstrate that our AFT achieves state-of-the-art performance.}\n\n\n\n\\keywords{Video Question Answering, Sports Video, Benchmark, Auto-Focus Transformer}\n\n\n\n\\maketitle\n\n\n\\section{Introduction}\n\n\nSports video analysis has been attracting increasing attention in recent years\n\\citep{yuan2021spatio, li2021groupformer, koshkina2021contrastive, zhu2022fencenet, martin2020fine, wang2022shuttlenet,6516867}.\nWhile research progress has been made on tasks such as\nsports action recognition \\citep{li2021multisports, zhu2022fencenet, rasmussen2022compressing}, reasoning over sports videos for question answering\nhas not been explored. As humans, we can not only recognize the actions of the players in sports videos but also understand the effects of the players' actions, explain why a team loses the score, and imagine what would happen under counterfactual situations. Our impressive capabilities in reasoning allow us to answer complex questions related to sports videos, which is crucial in applications such as obtaining crucial statistics in matches for player/team performance qualification, analyzing players' actions and team strategies for performance improvement, and efficiently retrieving information for audiences and analysts.\nWhile sports video reasoning is clearly important, it is under-explored due to challenges and the lack of datasets.\n\n\n\\begin{figure}[tbp]\n\\centering\n\\includegraphics[width=0.9\\columnwidth]{pics/qadiff.pdf}\n\\caption{\nIllustrations of general VideoQA, which focus on common basic understanding, and sports VideoQA, which requires professional action understanding and action relation reasoning\\protect\\footnotemark. \n}\n\\label{qadiff}\n\\end{figure}\n\n\\footnotetext[\\thefootnote]{We treat the action ``\\textit{block}\" as a noun.}\n\n\nIn this paper, we tackle sports VideoQA, focusing on reasoning over sports videos for question answering. This is distinct from general VideoQA, which centers on general video understanding, mainly encompassing descriptive and temporal aspects \\citep{jang2017tgif, xu2017video, xiao2021next, yu2019activitynet}, as depicted in Fig. \\ref{qadiff}. % illustrates the difference between general VideoQA and sports VideoQA. %given a sports video, general VideoQA is mostly concerned with common %descriptive questions including  action recognition\\ke{questions  such as general action recognition (i think general also has temporal or other types of questions)}, while \nCompared to general VideoQA, sports VideoQA is more challenging because the questions may involve particular teams and professional actions, and it requires a fine-grained understanding of actions and intra/inter-team relation modeling. %action understanding and intra/inter-team relations including  temporal and causal reasoning. \nAs pointed out in \\citep{shao2020finegym}: 1) coarse-grained global video understanding is insufficient for sports analysis where the actions are rapid and hard to differentiate; 2) the models pre-trained on coarse-grained datasets are not always beneficial to fine-grained tasks. \nIn the meantime, although significant strides have been made in the fine-grained recognition of individual actions \\citep{shao2020finegym,liu2022fineaction,sun2017taichi}, these datasets may fall short in effectively evaluating the ability for temporal and causal reasoning over actions performed by individuals or teams in sports scenarios.\nA large-scale dataset containing professional sports scenarios with detailed annotations of sports-relevant questions is thus required for explicit and comprehensive reasoning for sports VideoQA.\n\nTo address this gap, we introduce a novel dataset called Sports-QA, the first video QA dataset specifically focusing on sports activities. Leveraging sports videos and professional action labels from MultiSports \\citep{li2021multisports} and FineGym \\citep{shao2020finegym}, we construct our dataset. Both sources provide high-quality sports videos with well-defined action labels, allowing us to annotate essential attributes for each crucial action instance. Using these annotations, we generate QA pairs based on pre-defined templates covering aspects such as description, chronology, causality, and counterfactual conditions. The Sports-QA dataset is the first of its kind, comprising approximately 94K QA pairs, encompassing multiple sports and various question types for sports VideoQA. Table \\ref{set} provides a comparison between Sports-QA and several widely-used VideoQA datasets.\n\n\\begin{table}[tbp]\n\\caption{VideoQA datasets comparison. ``Auto/Man\" represents ``automatic/manual\". ``MC/OE\" represents ``multi-choice/open-ended\".}\n\\label{set}\n\\begin{tabular}{@{}lcrrcc@{}}\n\\toprule\nDataset                           & Topic   & \\multicolumn{1}{c}{\\#Video} & \\multicolumn{1}{c}{\\#QA} & Annotation  & QA Task  \\\\ \\midrule\nMSRVTT-QA \\citep{xu2017video}      & \\multirow{4}{*}{General} & 10K                         & 244K                     & Auto        & OE       \\\\\nMSVD-QA \\citep{xu2017video}        &  & 2K                          & 51K                      & Auto        & OE       \\\\\nNExT-QA \\citep{xiao2021next}       &  & 5.4K                        & 52K                      & Man         & OE\\&MC \\\\\nTGIF-QA \\citep{jang2017tgif}       &  & 72K                         & 165K                     & Auto\\&Man & OE\\&MC \\\\ \\midrule\nMovieQA \\citep{tapaswi2016movieqa} & Movie   & 408                         & 15K                      & Man         & MC       \\\\\nSocial-IQ \\citep{zadeh2019social}  & Social  & 1K                          & 8K                       & Man         & MC       \\\\\nDramaQA \\citep{choi2021dramaqa}    & Drama   & 24K                         & 16K                      & Auto\\&Man & MC       \\\\\n\\textbf{Sports-QA (Ours)}                  & Sports  & 6K                          & 94K                      & Auto\\&Man & OE       \\\\ \\botrule\n\\end{tabular}\n\\end{table}\n\n\nCompared with existing datasets, our Sports-QA provides new insights for VideoQA: 1) It encompasses questions related to specific terms and actions in both group activities (e.g., basketball) and single-person sports (e.g., gym). This requires models to possess the capability to capture dynamic patterns and perform reasoning in scenarios with varying numbers of interactions for question answering. 2) To achieve a comprehensive understanding of complex sports videos, Sports-QA includes diverse questions that involve video information at various temporal granularities. This encompasses global long-term temporal dependencies and fine-grained short-term ones. For instance, a question asking about the number of actions requires the model to capture global dependencies, while a question about the effect of a specific action relies on short-term temporal information. Furthermore, Sports-QA has the potential to be leveraged for other tasks. The pre-defined attributes of actions can be treated as multiple labels, allowing the dataset to be used for multi-label classification tasks for comprehensive action understanding. Additionally, based on the annotated action attributes, we can modify the templates from question answering to declarative sentences, generating descriptive or explanatory narrations and enabling comprehensive sports video captioning.\n\n\nIn addition to the dataset, we present a novel method for sports VideoQA. Recognizing that sports VideoQA demands the model to capture information from multiple frames at various temporal granularities, we introduce the Auto-Focus Transformer (AFT), featuring an Auto-Focus Attention mechanism. Conceptually, the model operates akin to a temporal magnifying glass with a dynamic focal length, allowing it to inspect the video to answer questions with diverse temporal dependencies. More specifically, our designed Auto-Focus Attention dynamically selects attention focus based on the question. This mechanism empowers the model to handle questions involving video information across various time spans.\n\n\n\n\nThe contributions are summarized as follows:\n\n\\begin{itemize}\n\n    \\item We contribute a large-scale dataset, which consists of various types of questions and multiple sports for VideoQA.  %It provides  various question types and multiple sports. It also provides   \nTo our knowledge, it is the first dataset for complex reasoning over professional sports actions. \n\n    \\item We propose a new Auto-Focus Transformer (AFT), where the attention focus is adaptive based on the question,  enabling the model to deal with questions requiring temporal information of various scales.\n\n\n    \\item We conduct extensive experiments on our dataset, including baseline study, evaluation of existing methods, and visualization of predictions. The results show the superiority of the proposed AFT for sports VideoQA. %limitations of existing methods and provide directions for future improvements.\n\\end{itemize}\n\n\n\n\n\n\n\\section{Related Work}\n\n\n\\noindent\\textbf{VideoQA Datasets.}  The development of VideoQA has been greatly facilitated by the emergence of various datasets, such as TGIF-QA \\citep{jang2017tgif}, MSVD-QA, and MSRVTT-QA \\citep{xu2017video}, DramaQA \\citep{choi2021dramaqa}, NExT-QA \\citep{xiao2021next}, NExT-OOD \\citep{10107423}. TGIF-QA, for instance, offers a comprehensive set of four sub-tasks designed for temporal reasoning in general videos and is widely recognized in the VideoQA community. MSVD-QA and MSRVTT-QA are open-ended datasets constructed from existing video captions, while DramaQA focuses on understanding drama stories with hierarchical QAs and character-centered video annotation. NExT-QA, introduced for describing and explaining temporal actions, provides both multi-choice and open-ended questions through manual annotation. However, existing datasets predominantly center around general video understanding in daily scenarios or involve coarse-grained action/event reasoning. \\textbf{In contrast, our contribution lies in fine-grained and professional analysis within sports scenarios. We present Sports-QA, a dataset that addresses aspects of description, chronology, causality, and counterfactual conditions for multiple sports of diverse characteristics.}\n\n\\noindent\\textbf{VideoQA Methods.} \nVideoQA poses a significant challenge as it necessitates models to grasp both spatial and temporal information from videos to answer questions. Various deep models have been developed, approaching this task from different perspectives \\citep{fan2019heterogeneous,jiang2020reasoning,li2022invariant,antol2015vqa,gao2018motion,10172254,9770842,10214041,10146482}.\nFor instance, the deep heterogeneous graph alignment network by Jiang et al. \\citep{jiang2020reasoning} addresses VideoQA by simultaneously aligning intra/inter-modality information. Another approach involves a multimodal attention model proposed by Fan et al. \\citep{fan2019heterogeneous}, where heterogeneous memory learns global context from visual features, and question memory captures the complex semantics of questions.\nIGV, introduced by Li et al. \\citep{li2022invariant}, grounds question-critical scenes in videos by considering causal relations that remain invariant to complement contents.\nHowever, a limitation in these approaches is the lack of consideration for the fact that different questions may require temporal dependencies of specific scales. \\textbf{In response to this challenge, we propose the Auto-Focus Transformer, designed to automatically focus on a specific temporal scale based on the question for VideoQA.}\n\n\n\\noindent\\textbf{Sports Video Understanding.} Sports video understanding has drawn increasing attention in recent years \\citep{yuan2021spatio,li2021groupformer,koshkina2021contrastive,zhu2022fencenet,martin2020fine,wang2022shuttlenet}. Researchers have made great efforts in various tasks such as sports action recognition \\citep{shao2020finegym}, multi-person action detection \\citep{li2021multisports}, and action quality assessment \\citep{tang2020uncertainty}. Meanwhile, numerous sports datasets are constructed \\citep{parmar2019action,li2021multisports,giancola2018soccernet,deliege2021soccernet}.  \nFor example, AQA-7 \\citep{parmar2019action} is constructed for professional action quality assessment. Seven types of actions are included in this dataset and all the action instances are associated with quality scores. \nMultiSports \\citep{li2021multisports} is proposed for the spatial-temporal detection of professional sports actions. FineGym~\\citep{shao2020finegym} focuses on fine-grained activity localization that requires an accurate understanding of the atomic level of a gymnastic action.\n\\textbf{In this work, we construct Sports-QA, the first dataset for sports VideoQA, bridging the gap between the visual modality and the textual modality in sports analysis.}\n\n\n\n\n\n\n\\section{Sports-QA Dataset}\n\n\n\\begin{table}[tbp]\n\\caption{The statistics for the MultiSports dataset are presented.} \n\\label{mss}\n\\begin{tabular}{@{}lcrcc@{}}\n\\toprule\n Sports          & \\# Action & \\# Instance & Avg. Action/Video Duration & \\# Bounding box \\\\ \\midrule\nGym        & 21      & 8,703     & 1.5s / 30.7s        & 325K    \\\\\nVolleyball & 12      & 7,645     & 0.7s / 10.5s        & 139K   \\\\\nFootball   & 15      & 12,254    & 0.7s / 22.6s        & 225K    \\\\\nBasketball & 18      & 9,099     & 0.9s / 19.7s        & 213K   \\\\\\midrule\nTotal      & 66      & 37,701    & 1.0s / 20.9s        & 902K   \\\\ \\botrule\n\\end{tabular}\n\\end{table}\n\n\\subsection{Data Source}\n\nRegarding the collection of sports videos, we consider the following aspects: 1) The visual quality of the video data should be high enough to conduct fine-grained video understanding, such as video resolution and frame rate. 2) Instead of applying to a single type of sports, we expect a VideoQA dataset involving multiple sports.\nAfter a deep survey of the works on sports video understanding, we find that the MultiSports \\citep{li2021multisports} and FineGym \\citep{shao2020finegym} datasets are highly suitable for our purpose. The details of MultiSports and FineGym are as follows.\n\n\n\\noindent\\textbf{MultiSports} \\citep{li2021multisports} is a dataset for the temporal localization of sports actions, encompassing four sports (i.e., basketball, football, volleyball, and aerobic gymnastics) and 66 fine-grained action categories\\footnote{Project homepage: \\url{https://deeperaction.github.io/datasets/multisports.html}}. The action categories in MultiSports are professional terms (such as ``\\textit{volleyball spike}\", ``\\textit{football tackle}\", and ``\\textit{basketball defensive rebound}\") instead of common and atomic actions like ``\\textit{run}\" and ``\\textit{stand}\". For each action instance, the dataset provides the bounding boxes of a player from the starting frame of an action to the ending frame, forming the action spatial-temporal tube. The statistics of MultiSports are shown in Table \\ref{mss}.\n\n\\noindent\\textbf{FineGym} \\citep{shao2020finegym} is a dataset designed to elevate the field of action recognition by addressing the limitations observed in existing techniques\\footnote{Project homepage: \\url{https://sdolivia.github.io/FineGym}}. Developed to surpass current benchmarks, it offers a unique combination of richness, quality, and diversity in its content.\nThis dataset is constructed on gymnasium videos, providing a realistic and varied environment for action recognition studies. What sets FineGym apart from other datasets is its meticulous temporal annotation at both action and sub-action levels, featuring a three-level semantic hierarchy. This hierarchical structure allows for a more nuanced understanding of activities, enabling researchers to explore and analyze actions in finer detail. As an illustrative example, an event is annotated as a sequence of elementary sub-actions Importantly, each sub-action within these sets is further annotated with finely defined class labels, contributing to a higher level of granularity in action recognition. Fig. \\ref{ms} shows the action hierarchy of the MultiSports and FineGym datasets.\n\n\n\n\n\\begin{figure}[tbp]\n\\centering\n\\includegraphics[width=\\columnwidth]{pics/MultiSports1.pdf}\n\\caption{The action hierarchy of the MultiSports dataset is depicted at the top, while the FineGym dataset's hierarchy is shown at the bottom. It's important to note that the figure includes only four example actions for each sport.}\n\\label{ms}\n\\end{figure}\n\nThe action categories in MultiSports and FineGym are professional terms (such as ``\\textit{volleyball spike}\", ``\\textit{football tackle}\", ``\\textit{basketball defensive rebound}\", and ``\\textit{vault Salto backward tucked}\") instead of common and atomic actions (such as ``\\textit{run}\" and ``\\textit{stand}\"). Such professional labels are of great value to our dataset.\nSince the length of videos varies greatly (from a few seconds to a few minutes), we segment the videos into clips and generate QA pairs based on these clips.\n\n\\subsection{Question-Answer Pair Generation}\n\nIn most existing works, QA pairs are generated using two approaches:\n1) Automatically generating questions and answers based on video captions using a set of predefined question templates \\citep{jang2017tgif, xu2017video, yang2021just}. This method is efficient, but it may introduce obvious grammatical errors or lose crucial information during the conversion of captions into questions and answers.\n2) Manually annotating questions and answers through crowdsourcing \\citep{xiao2021next, yu2019activitynet, garcia2020knowit}. Although this approach produces QA pairs of high quality in terms of accuracy and expression, manual annotation is time-consuming and expensive.\nIn this work, we aim to ensure the quality of the textual data while considering annotation costs. We achieve this by generating QA pairs using pre-defined templates based on existing labels and newly-labeled attributes of actions. Our approach guarantees grammatically error-free QA pairs that capture crucial information in videos, thanks to the careful design of sophisticated templates. Furthermore, as ball games and gymnastics have distinct characteristics, we generate the QA pairs in different ways as described as follows.\n\n\\subsubsection{QA Pair Generation for Ball Games}\n\nSpecifically, for ball games, including basketball, football, and volleyball, we define five attributes (Team, Outcome, Cause of Outcome, Cause of Action, and Effect of Action) for actions. We then manually annotate the attributes of each action. The definitions and annotation process of these attributes are elaborated as follows.\n\n\n\\noindent\\textbf{Attribute Definition.} MultiSports consists of 45 action categories from ball games, with some being crucial for professional sports statistics or quantifying the performance of players/teams. For instance, the ``\\textit{2-Point Field Goal Percentage}\" in a basketball match necessitates a fine-grained understanding of all ``\\textit{2-point shot}\" actions, while the ``\\textit{saving}\" actions in football directly reflect the performance of goalkeepers. In this work, we focus on 28 crucial actions, listed in Table \\ref{acts}. Considering both practical applications and research purposes, we define five attributes for each of the crucial action instances as follows.\n\n\\noindent 1. \\texttt{Team}: The team that the player of the action belongs to is denoted by two options: \\texttt{left team} or \\texttt{right team}. Specifically, in basketball and football, the determination of the left or right team depends on the team attacking towards the right or left. In volleyball, the left or right team is designated based on their position relative to the left or right side of the net.\n\n\\noindent 2. \\texttt{Outcome}: This attribute assesses whether the result of the action meets the expectation and is defined as a binary label: \\texttt{successful} or \\texttt{failed}. For example, if a ``\\textit{2-point shot}\" in basketball scores a goal, it is annotated as successful; otherwise, it is labeled as failed.\n\n\\noindent 3. \\texttt{Cause of Outcome}: \nThis attribute indicates the cause of failure for offensive actions, such as a ``\\textit{2-point shot}\" in basketball, ``\\textit{shoot}\" in football, and ``\\textit{spike}\" in volleyball, or the cause of success for defensive actions, like ``\\textit{block}\" in football and ``\\textit{save}\" in volleyball. Specifically, the cause of failure for offensive actions reflects player shortcomings. For example, the failure of an offensive ``\\textit{long pass}\" in football could be attributed to \\texttt{defensive interception}, \\texttt{bad pass}, or \\texttt{bad catch}. Identifying the precise cause helps teams identify their weaknesses and improve performance. Similarly, the cause of success for defensive actions highlights player strengths. For instance, the success of a defensive ``\\textit{save}\" in volleyball might be due to \\texttt{offensive out of bounds}, \\texttt{offensive blocked by net}, or \\texttt{actually catch the ball}, with the last case directly reflecting defensive performance. We do not consider the cause of success for offensive actions or the cause of failure for defensive actions, as they cannot be attributed to different cases. For example, the failure of ``\\textit{football saving}\" can only be explained by the goalkeeper missing the ball. Therefore, these causes are not worth discussing. We define various cause options for applicable action categories and ask annotators to choose from them. Note that this attribute is applicable to certain action categories, and the causes vary for different action categories.\n\n\\noindent 4. \\texttt{Cause of Action}: This attribute indicates the actions that cause the current action. Each action instance in a video is initially labeled with its time order, based on the start time, which serves as the unique ID of the action in the video. For each crucial action, annotators are asked to provide the IDs of the actions that cause the current action; these causes are not necessarily crucial actions. Consequently, the \\texttt{Cause of Action} is a list consisting of the IDs of actions that are mostly temporally adjacent to the current action. Not all crucial action categories are required to provide this attribute because the causes of some actions are unique. For instance, the only cause of ``\\textit{volleyball first pass}\" is ``\\textit{volleyball serve}\".\n\n\\noindent 5. \\texttt{Effect of Action}: This attribute aims to identify the actions caused by the current action, analogous to \\texttt{Cause of Action}. Although the cause and effect are conjunctive, we may consider only one of them, as some actions are not crucial. For instance, if ACTION M (crucial action) causes ACTION N (non-crucial action), we only label ACTION N as an effect of ACTION M.\n\n\n\\noindent\\textbf{Attribute Annotation.}\nAfter defining the attributes mentioned above, we proceed to annotate them for each crucial action instance in MultiSports. The annotation process is divided into three stages: pre-annotation, formal annotation, and quality check.\nIn the pre-annotation stage, annotators are grouped into three categories, with each group assigned responsibility for a specific type of ball game. Each annotator within a group is then assigned several videos corresponding to their designated sport. During this stage, annotators are tasked with labeling the attributes of the first 50 action instances. Following this, an intra-group check is conducted to address any issues related to understanding bias or potential mistakes, as well as to handle unexpected or rare situations. The annotation can only progress to the next stage when annotators achieve a consistent understanding of each attribute, as outlined in the protocols.\nMoving to the formal annotation stage, annotators use shared protocols to amend their previous annotations and label the remaining action instances. Once all actions are labeled, we perform an inter-group quality check on all annotations. The purpose of this inter-group check is to ensure that the annotations align with the common understanding of average individuals.\nThe annotation process is completed within one month and involves the collaboration of 15 graduate students.\n\n\nBased on the attributes, we generate questions about the videos by designing various templates (listed in the end of the paper) to inquire about these attributes. For instance, given a video shown in Fig. \\ref{egdata}, after annotating the attributes for the action ``\\textit{spike}\", a question querying a particular attribute (e.g., Outcome) of the action can be generated, such as ``\\textit{Is the `spike' of the right team successful?}\".\nSpecifically, our dataset involves four types of questions: descriptive, temporal, causal, and counterfactual. The details of each type are described as follows.\n\n\\begin{figure*}[tbp]\n\\centering\n\\includegraphics[width=0.93\\textwidth]{pics/egs.pdf}\n\\caption{Example of Sports-QA: The actions in the green boxes (such as ``\\textit{2-point shot}\") are the query actions, while the actions in the yellow boxes (such as ``\\textit{block}\") represent the effects. For ball games, annotators provide attribute labels, and we generate QA pairs based on these attributes. In gymnastics, we generate QA pairs using annotations from MultiSports/FineGym.\n}\n\\label{egdata}\n\\end{figure*}\n\n\n\n\\noindent 1. \\textbf{Descriptive:} Descriptive questions require holistic comprehension and involve various aspects of information from the videos. These questions include simple queries such as ``\\textit{What is the video about?}\" and ``\\textit{Does SOME-TEAM perform SOME-ACTION?}\", as well as complex ones involving counting and the outcome of actions, such as ``\\textit{How many times does SOME-TEAM perform SOME-ACTION?}\" and ``\\textit{Does SOME-TEAM successfully do their i-th SOME-ACTION?}\"\n\n\\noindent 2. \\textbf{Temporal:} Temporal questions focus on the temporal relations among the actions of the same or different teams. Specifically, these questions require an understanding of intra-group temporal relations and inter-group ones. Typical templates for this type of question are ``\\textit{What does SOME-TEAM do before/after their i-th SOME-ACTION?}\" and ``\\textit{What does the left/right team do before/after the other team performs their i-th SOME-ACTION?}\"\n\n\\noindent 3. \\textbf{Causal:} Causal questions aim to uncover the causal relations among the action instances and to explain the reasons or the specific process of the actions. Different from temporal questions, these questions require causal reasoning based on visual cues. The templates of these questions include causal queries such as ``\\textit{Why does SOME-TEAM do the i-th SOME-ACTION?}\" and ``\\textit{What is the effect of the i-th SOME-ACTION of SOME-TEAM?}\", as well as explanation queries such as ``\\textit{How does SOME-TEAM succeed in doing/fail to do the i-th SOME-ACTION?}\"\n\n\\noindent 4. \\textbf{Counterfactual:} Different from the above three types of questions querying the details that actually happen, counterfactual questions set hypothetical conditions that do not occur in the video and query about the expected outcomes based on the conditions. This type of question requires reasoning about various imagined situations and expects the outcomes according to the causal relations among actions. The template for this type of question is ``\\textit{Would the left/right team succeed in do the i-th SOME-ACTION if the other team did not do SOME-ACTION?}\"\n\n\n\\noindent\\textbf{From Attributes to Question-Answer Pairs.} In particular, the questions are queries about the attributes, and their answers can be obtained directly from our annotations or through logical reasoning and statistical analysis over the annotations. For instance, for a question asking about the cause of a specific action, the answer can be easily retrieved from its attribute of cause; For a question asking whether a specific action is successful, we can check its outcome attribute. We guarantee the correctness of the answers as we have checked the annotations of attributes as mentioned previously.\nThese answers include responses like ``\\textit{yes/no}\", numerical values, sports names (such as ``\\textit{volleyball}\"), action categories in MultiSports and FineGym (such as ``\\textit{block}\"), and short phrases describing the reasons (such as ``\\textit{out of bounds}\"). See Fig. \\ref{egdata} for more concrete examples of the attributes and the corresponding generated QA pairs. \n\n\n\n\n\n\nAn interesting aspect of our dataset is that it can be utilized for other tasks due to our two-stage annotation process. Specifically, we have defined several attributes for each action instance, which can be considered as multiple labels. Consequently, our dataset can be employed for conducting multi-label classification.\nMoreover, by altering the template from question answering to declarative sentences, we can generate descriptive or explanatory narrations for sports videos. Hence, our dataset can also be exploited for sports video captioning.\n\n\\subsubsection{QA Pair Generation for Gymnastics}\n\nA major difference between aerobic gymnastics/FineGym and ball games is the absence of the concept of a team. Furthermore, there are no causal relations among the actions performed in these sports. Therefore, we only generate descriptive and temporal questions for aerobic gymnastics/FineGym based on their annotations. In addition to the templates used for ball games, we have also designed some new templates. For descriptive questions, we include queries about the number of players or actions, such as ``\\textit{How many actions does the player perform?}\" and ``\\textit{How many times does the player perform SOME-ACTION?}\" For temporal questions, we add counting questions like ``\\textit{How many times do the players do SOME-ACTION before SOME-ACTION?}\" Such a QA design leverages the fine-grained features in the datasets and constitutes challenging temporal reasoning that forces the models to focus on the salient sub-actions in the whole gymnastic event.\n\n\n\n\n\n\n\\subsection{Diversity, Debiasing and Problem Setting}\n\n\\noindent \\textbf{Diversity of Questions.} The limited diversity in generating questions using pre-designed templates may not significantly impact the significance of datasets for several reasons. First, in the context of sports, there is a high level of restriction imposed by rules and specific content, resulting in a more focused attention from the audience on particular actions and events. This inherent structure in sports content narrows down the range of potential questions, making template-based question generation a feasible approach.\nSecondly, the practice of template-based question generation is widely accepted and employed in the construction of VideoQA datasets \\citep{jang2017tgif, xu2017video, yang2021just}. \nThe adoption of this approach in reputable datasets attests to its effectiveness and acceptance. Therefore, while the questions are template-based, their relevance and significance in the context of sports are preserved, ensuring the robustness of the dataset for analysis and evaluation.\n\n\n\n\n\n\\noindent \\textbf{Debiasing.} Unlike daily scenarios, players in sports are highly restricted by rules. For instance, after the left team executes a ``\\textit{volleyball serve}\", the right team must follow with a ``\\textit{first-pass}\". This results in highly correlated actions in the videos, leading to questions and answers that are also highly correlated in our generated pairs. However, such questions are often meaningless for VideoQA, as the answers can be easily inferred through rules or common sense and should be removed.\nTo address this, we first obtain meta-questions by removing team information and the order of actions from the original questions. For example, the meta-question for ``\\textit{What does the left team do after the other team does the second spike?}\" becomes ``\\textit{What does the team do after the other team does spike?}\" We then examine the correlations between the meta-questions and their answers. If there is only one answer to the meta-question, we remove the corresponding QA pairs. If an answer to a meta-question occurs more frequently than other answers (frequency larger than 0.5), we randomly remove the corresponding QA pairs to balance the frequencies, ensuring they are all lower than 0.5.\n\n\\noindent \\textbf{Problem Setting.} Following the approach in \\citep{jang2017tgif, xu2017video}, we define an open-ended task based on the generated QA pairs. It is important to note that the open-ended setting, along with the multiple-choice setting, is widely used in the field of VideoQA. We have chosen to commit to the open-ended task instead of the multiple-choice one for two reasons: 1) The open-ended task is more challenging as it requires models to choose from a large answer set rather than selecting from several given options. 2) Limiting the answer choices to only several options would significantly decrease the diversity of the dataset. Specifically, all answers in the QA pairs form an answer pool, which is treated as 191 classes (after discarding classes with fewer than 30 samples). \n\n\\noindent \\textbf{Why not Fine-Grained Action Recognition?} It is essential to underscore that merely achieving fine-grained action recognition is insufficient for Sports-QA. The nature of the questions in this context demands not only precise detection of actions but also involves intricate temporal and causal modeling. To illustrate, questions that inquire about the effects of certain actions necessitate the accurate identification of temporally adjacent actions and a nuanced understanding of their causality. In summary, Sports-QA requires a more advanced level of comprehension and temporal reasoning beyond basic action recognition.\n\n\\subsection{Dataset Statistics}\n\n\nTable \\ref{n_qa} presents the numbers of QA pairs for various question types and different sports in Sport-QA. Our dataset comprises approximately 94K QA pairs. It's worth noting that a substantial portion of our descriptive questions involves complex action temporal localization and counting, presenting challenges similar to other question types. Additionally, we have balanced the number of questions across different sports.\n\n\\begin{table}[tbp]\n\\caption{The numbers of QA pairs for different types and different sports.}\n\\label{n_qa}\n\n\\begin{tabular}{@{}lrrrrr@{}}\n\\toprule\n Sports       & Descriptive & Temporal & Causal & Counterfactual & Total  \\\\ \\midrule\nBasketball & 5,629 & 22    & 785    & 278      & 6,714  \\\\\nFootball   & 6,659 & 1,355 & 1,949  & 523      & 10,486 \\\\\nVolleyball & 6,120 & 360   & 1,942  & 685      & 9,107  \\\\\nGym     & 6,382 & 1,997 & 0      & 0        & 8,379  \\\\\nFloor Exercise      &  6,046      &  11,012     & 0      & 0        &   19,418     \\\\\nBalance Beam       &   7,477     &   12,773    & 0      & 0        &   20,250     \\\\\nUneven Bars      &   7,294     &   12,124    & 0      & 0        &    17,058    \\\\\nVault      &   2,661     &   0    & 0      & 0       &   2,661     \\\\ \\midrule\nTotal   &   48,268     &   39,643    & 4,676  & 1,486    &    94,073    \\\\ \\botrule\n\\end{tabular}\n\\end{table}\n\n\\begin{figure*}[tbp]\n\\includegraphics[width=\\textwidth]{pics/ans_dist_type.pdf}\n\\caption{The distributions of answer classes broken down by question types.}\n\\label{ans_dist_type}\n\\end{figure*}\n\n\n\nThe total number of videos in our dataset is 5,967, distributed across different sports as follows: basketball (879), football (1,030), volleyball (586), gym (505), vault (501), uneven bars (834), floor exercise (770), and balance beam (862). Sports-QA is divided into training, validation, and testing sets (60\\%/20\\%/20\\%). Specifically, videos of the same sport are randomly assigned to each set to ensure similar distributions of sport types across subsets. Additionally, we have ensured that the distributions of answer classes are similar in different subsets.\n\nFig. \\ref{ans_dist_type} illustrates the distribution of answer classes broken down by question types in Sports-QA. Note that we only showcase the first 80 classes with more examples. As depicted in the figure, the distribution of answers is long-tailed, and the unbalanced nature adds to the challenge of our dataset. Meanwhile, classes of the same type, such as ``yes/no\", are balanced. This further increases the difficulty of our dataset, as the models are required to examine the video for answering the question instead of merely guessing the answer based on the question type. \n\n\n\n\\section{Auto-Focus Transformer for Sports Video Question Answering}\n\nA key characteristic of the sports VideoQA task is the need for reasoning over temporal dependencies of various scales for multiple objects. For instance, a question asking about the number of actions requires the model to capture global dependencies in the video, whereas a question querying the effect of an action relies on short-term temporal information. The scale of temporal dependency required varies based on the question. However, current Transformer-based or GNN-based VideoQA methods tend to focus on global dependencies, irrespective of the required scale of temporal information. To address this limitation, we propose the Auto-Focus Transformer (AFT), a new Transformer encoder featuring a novel multi-head Auto-Focus Attention (AFA) mechanism, designed specifically for sports VideoQA.\n\n\n\\begin{figure}[tbp]\n\\centering\n\\includegraphics[width=\\columnwidth]{pics/aft.pdf}\n\\caption{The structure of the sports  VideoQA model based on the proposed Auto-Focus Transformer.}\n\\label{aft}\n\\end{figure}\n\nSpecifically, we start by extracting the appearance feature and motion feature of the frame using pretrained ResNet \\citep{he2016deep} and I3D \\citep{carreira2017quo} (refer to Section \\ref{feaext} for more details). Subsequently, we concatenate these features and input them into a linear layer for dimension reduction and feature fusion. Consequently, for each video, we obtain a sequence of frame representations ($\\bm{R}$). Regarding the question, we utilize word embedding and a bi-directional RNN to obtain the text representations, encompassing both the global one ($\\bm{w}$) and the local ones ($\\bm{W}$).\n\nWe then  capture the dependencies among frames using AFT where the AFA mechanism\nis computed as follows (the multi-head AFA is defined likewise): given the key set $\\left\\{\\bm{k}_i\\right\\}$, the value set $\\left\\{\\bm{v}_i\\right\\}$, and a query $\\bm{q}_j$ ($\\bm{q}_j,\\bm{k}_i,\\bm{v}_i \\in \\mathbb{R}^d$ are mapped from $\\bm{R}$ in AFA), \n\\begin{equation}\n{\\rm AFA}(\\bm{q}_j)=\\sum_{f\\in \\mathcal{F}}\\alpha_f\\sum_{i\\in \\mathcal{D}_j^f}{\\rm softmax}_i\\left(\\frac{\\bm{q}_j^\\mathrm{T}\\bm{k}_i}{\\sqrt{d}}\\right)\\bm{v}_i,\n\\end{equation}\nwhere $\\mathcal{F}$ is a set of pre-defined focal lengths. $\\mathcal{D}_j^f=\\{i||i-j|\\leq f\\}$ is the index set of the neighbours of $\\bm{q}_j$ within focal length $f$. $\\alpha_f\\in(0,1)$ is the dynamic weight of focus $f$, satisfying $\\sum_f\\alpha_f=1$ and depended on the question. As shown in Fig. \\ref{aft},   $\\bm{\\alpha}=\\{\\alpha_f\\}_{f=1}^\\mathcal{|F|}$ are obtained by applying linear projection and softmax to the global representation of question.  \nCompared to traditional attention that models global dependencies in any case, our AFA captures only the temporal dependencies of the scale required by the question. This design allows the model to focus solely on crucial information and discard irrelevant parts for better prediction. \nWe denote the output of AFT as $\\bm{M}$.\n\nFor the video-text integration, we draw inspiration from \\citep{li2022invariant} and \\citep{jiang2020reasoning}, employing a graph convolutional network (GCN) and block fusion \\citep{ben2019block}. Refer to \\citep{li2022invariant} for detailed information. Ultimately, we obtain a fused feature $\\bm{s}$ and predict the answer using a linear projection and softmax.\n\n\n\n\n\\section{Experiments}\n \n\n\n \n\\subsection{Experimental Settings}\n\\label{feaext}\n\nWe start by explaining the experimental settings, covering data pre-processing and feature extraction, implementation details, and the evaluation metrics.\n\n\\noindent\\textbf{Pre-processing and Representation.} The videos in our dataset are down-sampled to 5 FPS to reduce temporal redundancy. We extract two types of visual features for experiments: appearance feature and motion feature. For each feature type, we obtain both global and local features.\nSpecifically, we detect 10 objects in every two frames of each video using Faster-RCNN \\citep{ren2015faster} (with ResNet-101 \\citep{he2016deep} as the backbone and a score threshold set to 0.6), pretrained on COCO \\citep{lin2014microsoft}. These detected objects serve as local appearance features in our work. Additionally, we use the global average pooling of Res4 activation in Faster-RCNN as the global appearance feature.\nTo extract motion features, we employ I3D \\citep{carreira2017quo} pretrained on Kinetics \\citep{carreira2017quo}. Specifically, for each frame, we combine it with its adjacent 7 frames, creating a clip that we then feed to I3D to extract a 3D feature map. The global motion feature of a frame is obtained by applying global average pooling to the 3D feature. The local motion feature of an object in a frame is obtained by applying RoIAlign \\citep{he2017mask} (output size $1\\times 1$) to the 3D feature with its corresponding bounding box. Note that our method uses only the global feature.\nFor language, we explore two types of word embeddings: GloVe \\citep{pennington2014glove} and pre-trained BERT \\citep{devlin2018bert}.\n \n\\noindent\\textbf{Implementation Details.} We set the attention focus to $\\mathcal{F}=\\{3,9,80\\}$ to cover short-term, mid-term, and long-term dependencies, respectively. The hidden dimension $d$ is set to 512. The loss function is cross-entropy. The models are trained for 50 epochs with a learning rate of $1\\times 10^{-4}$ and a batch size of 16, using the Adam optimizer \\citep{kingma2014adam}.\n\n\\noindent\\textbf{Evaluation Metrics.} For evaluation metrics, we employ accuracy to showcase the models' performance on our dataset. Additionally, recognizing the long-tail distribution of our dataset, we evaluate methods using F1-score. F1-score is particularly important for sports VideoQA as it explicitly reflects performance averaged over imbalanced classes.\n\n\n\n\\begin{table}[tbp]\n\\caption{Baseline results on Sports-QA, including random choosing, semantic-aware (S-A) random choosing, LSTM-based BlindQA, and Transformer-based BlindQA. The best and the second-best results (\\%) are \\textbf{bold} and \\underline{underlined}.} \n\\label{bl}\n\\begin{tabular}{@{}ccccccc@{}}\n\\toprule\n\\multirow{2}{*}{Metric} & \\multicolumn{2}{c}{Baseline} & \\multicolumn{2}{c}{GloVe}            & \\multicolumn{2}{c}{BERT}                    \\\\\\cmidrule(l){2-3} \\cmidrule(l){4-5} \\cmidrule(l){6-7} \n                        & Random   & S-A Random   & LSTM & Transformer                   & LSTM                          & Transformer \\\\ \\midrule\nAccuracy                & 0.5      & 24.6         & 44   & 43.7                          & \\underline{43.9} & 43.9        \\\\\nF1-score                & 0.3      & 5.4          & 13.5 & \\underline{14.9} & 13.8                          & 15.8        \\\\ \\bottomrule\n\\end{tabular}\n\\end{table}\n\n\n\n\n\n\\begin{table*}[tbp]\n\\caption{The results of different methods on Sports-QA. The sport-wise accuracy, question-type-wise accuracy, overall accuracy and F1-score are presented (\\%).\nThe best and second best results are \\textbf{bold} and \\underline{underlined}.} \n\\label{res1}\n\n\\resizebox{0.95\\columnwidth}{!}{\\begin{tabular}{clcccccccccll}\n\\toprule\n \\multirow{2}{*}{Word Emb.}& \\multirow{2}{*}{Model} & \\multicolumn{5}{c}{Sports} & \\multicolumn{4}{c}{Question Type} & \\multirow{2}{*}{Acc.} & \\multirow{2}{*}{F1-score} \\\\ \\cmidrule(lr){3-7}\\cmidrule(lr){8-11}\n &  & Basket. & Foot. & Volley. & Gym  &FineG.& Desc. & Temp. & Causal & Counter. &  &  \\\\ \\hline\\hline\n & \\cellcolor[HTML]{C0C0C0}BlindQA & \\cellcolor[HTML]{C0C0C0}33.4& \\cellcolor[HTML]{C0C0C0}62.9& \\cellcolor[HTML]{C0C0C0}44.9& \\cellcolor[HTML]{C0C0C0}36.5&\\cellcolor[HTML]{C0C0C0}42.0& \\cellcolor[HTML]{C0C0C0}51.7& \\cellcolor[HTML]{C0C0C0}32.7& \\cellcolor[HTML]{C0C0C0}48.7& \\cellcolor[HTML]{C0C0C0}51.3& \\cellcolor[HTML]{C0C0C0}43.7& \\cellcolor[HTML]{C0C0C0}14.9\\\\\n & CoMem \\citep{gao2018motion} & 72.8& 63.1& 69.8& 70.0&50.3& 77.0& 32.9& 50.0& 60.9& 57.1& 23.1\\\\\n & HME \\citep{fan2019heterogeneous} & 64.8& 63.3& 68.5& 71.6&49.3& 76.5& 31.2& 50.4& 54.3& 56.0& 23.6\\\\\n & HGA \\citep{jiang2020reasoning} & 71.3& 65.4& 72.2& 72.2&51.3& 78.1& 34.4& 52.0& 59.2& 58.3& 24.5\\\\\n & HQGA \\citep{xiao2022video} & 72.8& 64.6& 70.5& 71.6&51.0& 77.6& 33.8& 54.0& 58.2& 57.9& \\underline{25.0}\\\\\n & IGV \\citep{li2022invariant} & 68.8& 66.0& 70.6& 72.4&52.1& 78.2& 34.7& 52.1& 59.2& \\underline{58.5}& 24.8\\\\\n & MASN \\citep{seo2021attend} & 71.2& 64.2& 70.1& 66.9&50.5& 76.4& 33.6& 50.2& 58.9& 57.0& 23.4\\\\ \\cmidrule(l){2-13} \n & Baseline (Ours) & 69.5& 65.4& 68.9& 71.8&52.3& 77.8& 35.0& 52.4& 59.9& 58.0& 23.7\\\\\n\\multirow{-9}{*}{GloVe} & AFT (Ours) & 73.9& 67.8& 69.5& 71.8&52.5& 78.9& 35.3& 55.1& 56.3& \\textbf{59.2} {\\color[HTML]{036400}(+1.2)}& \\textbf{25.6} {\\color[HTML]{036400}(+1.9)}\\\\ \\hline\\hline\n & \\cellcolor[HTML]{C0C0C0}BlindQA & \\cellcolor[HTML]{C0C0C0}34.7& \\cellcolor[HTML]{C0C0C0}52.1& \\cellcolor[HTML]{C0C0C0}45.9& \\cellcolor[HTML]{C0C0C0}38.3&\\cellcolor[HTML]{C0C0C0}43.9& \\cellcolor[HTML]{C0C0C0}51.8& \\cellcolor[HTML]{C0C0C0}33.3& \\cellcolor[HTML]{C0C0C0}48.0& \\cellcolor[HTML]{C0C0C0}53.9& \\cellcolor[HTML]{C0C0C0}43.9& \\cellcolor[HTML]{C0C0C0}15.8\\\\\n & CoMem \\citep{gao2018motion} & 74.1& 64.0& 68.7& 70.7&49.3& 77.2& 32.1& 48.4& 55.6& 56.6& 26.4\\\\\n & HME \\citep{fan2019heterogeneous} & 64.7& 63.4& 66.2& 70.6&49.3& 76.6& 30.7& 46.6& 53.6& 55.6& 22.9\\\\\n & HGA \\citep{jiang2020reasoning} & 71.6& 66.5& 69.6& 70.7&51.3& 78.0& 33.8& 54.3& 56.6& 58.1& \\underline{25.1}\\\\\n & HQGA \\citep{xiao2022video} & 74.1& 63.5& 67.2& 70.8&50.3& 76.6& 33.6& 49.1& 57.9& 57.0& 25.0\\\\\n & IGV \\citep{li2022invariant} & 72.0& 63.9& 70.4&72.4 &51.7& 78.0&34.2 & 51.9& 63.2& \\underline{58.2}&23.8 \\\\\n & MASN \\citep{seo2021attend} & 73.0& 63.0& 69.7& 66.6&50.1& 76.4& 32.5& 50.6& 61.2& 56.6& 24.1\\\\ \\cmidrule(l){2-13} \n & Baseline (Ours) & 70.7& 66.6& 69.2& 71.5&51.8& 78.0& 34.8& 50.5& 60.2& 57.9& 23.9\\\\\n\\multirow{-9}{*}{BERT} & AFT (Ours) & 72.3& 67.9& 70.4& 71.6&52.4& 78.3& 35.5& 56.8& 58.2& \\textbf{59.1} {\\color[HTML]{036400}(+1.2)}& \\textbf{25.4} {\\color[HTML]{036400}(+1.5)}\\\\ \\botrule\n\\end{tabular}}\n\\end{table*}\n\n\n\\begin{table}[tbp]\n\\caption{The visual features used by different models. A./M./G./L. represents Appearance/Motion/Global/Local. Mem, IG, HL, and GNN stand for Memory, Invariant Grounding, Hierarchical Learning, and Graph Neural Network, respectively.} \n\\label{fea}\n\\begin{tabular}{@{}lcccccc@{}}\n\\toprule\nModel                           & Venue   & Insight            & G.A        & G.M.       & L.A.       & L.M.       \\\\ \\midrule\nCoMem \\citep{gao2018motion}      & CVPR'18 & Mem             & \\checkmark & \\checkmark &            &            \\\\\nHME \\citep{fan2019heterogeneous} & CVPR'19 & Mem             & \\checkmark & \\checkmark &            &            \\\\\nHGA \\citep{jiang2020reasoning}   & AAAI'20 & GNN                & \\checkmark & \\checkmark &            &            \\\\\nIGV \\citep{li2022invariant}      & CVPR'22 & IG & \\checkmark & \\checkmark &            &            \\\\\nHQGA \\citep{xiao2022video}       & AAAI'22 & HL            & \\checkmark & \\checkmark & \\checkmark &            \\\\\nMASN \\citep{seo2021attend}       & ACL'21  & GNN                & \\checkmark & \\checkmark & \\checkmark & \\checkmark \\\\\\midrule\nAFT (Ours)      &   &   AFA              & \\checkmark  & \\checkmark  &  & \\\\\\botrule\n\\end{tabular}\n\\end{table}\n\n\n\n\\subsection{Baseline Study}\n\n\n\nSeveral baselines are constructed to evaluate their performance on our Sports-QA dataset as follows.\n\n\\noindent\\textbf{Random Choosing:} Answers are randomly selected from the 191 answer classes, resulting in an accuracy of approximately $\\frac{1}{191} \\approx 0.5\\%$. The F1-score is obtained by running the random test for 200 times and taking the average.\n\n\\noindent\\textbf{Semantic-Aware Random Choosing:} The randomly selected answer is constrained by the question type. For example, binary questions (beginning with ``\\textit{Do}\" or ``\\textit{Is}\") can only choose from ``\\textit{yes}\" or ``\\textit{no}\"; questions asking about the number of something (beginning with ``\\textit{How many}\") can only choose a number.\n\n\\noindent\\textbf{BlindQA:} Models are constructed with a question encoder and an MLP-based answer decoder, without exploiting visual information. Two types of word embeddings (Glove and pre-trained BERT) and two question encoders (LSTM and Transformer \\citep{vaswani2017attention}) are studied.\n\nThe results of the baselines are shown in Table \\ref{bl}. Random choosing exhibits poor accuracy and F1-score, but considering constraints from the questions significantly improves performance. BlindQA achieves considerable results in both accuracy and F1-score, indicating noticeable semantic correlations between questions and answers. By maximizing the likelihood of the answer conditioned on a specific question type, the model fits the answer distribution given different questions in the training set.\n\nRegarding word embedding, pre-trained BERT achieves slightly better accuracy than GloVe, with noticeable improvements in F1-score. Concerning different question encoders, Transformer and LSTM achieve similar accuracy, but Transformer outperforms LSTM in F1-score. In summary, word embedding has a greater impact on performance than the question encoder.\n\n\\subsection{Benchmark on Sports-QA}\n\nWe benchmark several VideoQA methods on our Sports-QA dataset, comparing them to CoMem \\citep{gao2018motion}, HME \\citep{fan2019heterogeneous}, HGA \\citep{jiang2020reasoning}, MASN \\citep{seo2021attend}, HQGA \\citep{xiao2022video}, and IGV \\citep{li2022invariant}, with HQGA \\citep{xiao2022video} and IGV \\citep{li2022invariant} representing the state of the art. It's worth noting that we only evaluate methods without large-scale video-text pretraining, excluding pretrained models like MERLOT \\citep{zellers2021merlot}, VIOLET \\citep{fu2021violet}, and All-in-one \\citep{wang2022all}. The exclusion is based on the following reasons: 1) pretrained models require large-scale additional data, making comparisons unfair for methods without pretraining; 2) we aim to highlight the characteristics of our dataset, and the universal knowledge gained by pretrained models could introduce bias in understanding sports scenarios. Table \\ref{fea} shows the types of visual features used by each model, along with a summary of each method's insights. For CoMem, HME, and HGA, we use re-implementations provided by \\citep{xiao2021next}, and for MASN, HQGA, and IGV, we utilize the official implementations by their respective authors. \\textbf{The baseline model has the same architecture as shown in Fig. \\ref{aft}, with AFT replaced with a standard Transformer Encoder.}\n\n\n\n\nIn Table \\ref{res1}, the VideoQA methods demonstrate substantial improvements compared to BlindQA. Notably, the most significant improvements are observed for basketball (approximately 40\\% increase in accuracy), indicating the critical role of visual information in basketball for question answering. Conversely, the improvements for football are less pronounced (about 4\\%), suggesting that BlindQA already achieves considerable accuracy in this sport. The results suggest high correlations between questions and answers in football, posing challenges for VideoQA models to fully exploit visual information, potentially due to the complexity introduced by a larger number of players.\n\nExamining different question types, the greatest improvements are observed for descriptive questions (around 27\\% increase in accuracy). Additionally, VideoQA performance on causality and counterfactual questions shows significant improvements compared to BlindQA. However, results for temporal questions are similar between VideoQA methods and BlindQA (except for our AFT), suggesting that VideoQA models may struggle with modeling fine-grained temporal dependencies among actions in sports videos.\n\nAmong the different VideoQA models, our AFT achieves the best performance in both accuracy and F1-score. The significant improvements compared to the baseline indicate the effectiveness of the proposed attention mechanism in distinguishing semantically similar classes.\n\n\n\n\n\n\n\n\n\\subsection{Further Analysis}\n\n\\noindent\\textbf{Generalization Ability across Sports}. In this study, our primary objective is to assess the generalization capabilities across different sports domains. Our approach involves an initial pretraining phase on a set of four sports, followed by fine-tuning on a fifth, distinct sport. We meticulously evaluate the performance of this pretraining-fine-tuning methodology in comparison to training a model entirely from scratch.\nThe outcomes, detailed in Table \\ref{resf}, showcase a notable enhancement in accuracy when employing the pretraining strategy on diverse sports. These findings suggest that the model acquires a nuanced understanding of common semantic features during the initial pretraining. Moreover, the model demonstrates its ability to effectively generalize this acquired knowledge to previously unseen sports during the subsequent fine-tuning phase.\n\n\\begin{table}[tbp]\n\\caption{The table presents comparisons (accuracy \\%) between models trained from scratch for individual sports and those fine-tuned using data from other sports, along with the corresponding improvements (Imp.).} \n\\label{resf}\n\n\\begin{tabular}{cccccc}\n\\toprule\nPretraining& Basketball                       & Football                         & Volleyball                       & Gym                              & FineG. \\\\ \\midrule\n\\xmark    & 57.1& 55.0& 56.4& 70.2&        48.5\\\\\n\\cmark    & 58.3& 56.5& 57.7& 71.7&        50.3\\\\ \\midrule\n Imp.& {\\color[HTML]{036400}+1.2}& {\\color[HTML]{036400}+1.5}& {\\color[HTML]{036400}+1.3}& {\\color[HTML]{036400}+1.5}&{\\color[HTML]{036400}+1.8}\\\\\\botrule\n\\end{tabular}\n\\end{table}\n\n\n\n\n\\begin{table}[t]\n\\caption{The accuracy (\\%) of various focus lengths and their combinations. Note that the combination of the same focal length implies the use of only one focus length.} \n\\label{fs}\n\n\\begin{tabular}{c|ccc|ccc|c}\n\\toprule\n   & 1 & 3 & 5 & 7 & 9 & 11 & 80   \\\\ \\midrule\n1  & \\cellcolor[HTML]{C0C0C0}58.2& / & / & 58.9& 59.0& 58.7& 58.5\\\\\n3  & / & \\cellcolor[HTML]{C0C0C0}58.7& / & 59.0& \\textbf{59.1}& 58.6& 58.9\\\\\n5  & / & / & \\cellcolor[HTML]{C0C0C0}58.6& 58.8& 58.9& 58.7& 58.8\\\\ \\midrule\n7  & / & / & / & \\cellcolor[HTML]{C0C0C0}58.7& / & /  & 58.9\\\\\n9  & / & / & / & / & \\cellcolor[HTML]{C0C0C0}58.8& /  & 59.0\\\\\n11 & / & / & / & / & / & \\cellcolor[HTML]{C0C0C0}58.4& 58.6\\\\ \\midrule\n80 & / & / & / & / & / & /  & \\cellcolor[HTML]{C0C0C0}58.0 \\\\ \\botrule\n\\end{tabular}\n\\end{table}\n\n\\noindent\\textbf{Impact of Focus Length.} In this experiment, we investigate the impact of different focus lengths and their combinations. For short-term dependency, we consider lengths of 1, 3, and 5. For the mid-term, we explore lengths of 7, 9, and 11. Regarding long-term (global) dependency, we set the focus length to the video length, which is 80 seconds. The results are presented in Table \\ref{fs}.\nFrom the results, it is evident that the model with global attention performs less effectively compared to those with local attention. However, upon applying our auto-focus attention mechanism, there is a significant improvement in performance. Notably, the combination of focus lengths 3 and 9 achieves the highest accuracy. In our final model, we incorporate the global focus, resulting in the best accuracy at 59.2\\%.\n\n\n\n\\subsection{Visualization}\n\n\n\n\n\n\n\n\n\n\n\\noindent \\textbf{Qualitative Results.} Fig. \\ref{pred} illustrates predictions from various methods. In some instances, BlindQA successfully guesses the correct answer by leveraging the correlation between the question and the answer in the training set. However, this approach is not foolproof, as the blinded model consistently selects the most correlated answer to the question.\nIn comparison, the VideoQA models struggle to discern subtle differences between semantically or visually similar answer classes. For example, actions like ``\\textit{long pass}\" and ``\\textit{cross}\" both involve passing the ball to other players but differ in passing direction and distance, presenting challenges for accurate distinction. Additionally, the compared VideoQA models exhibit limitations in fine-grained motion analysis, such as action counting, as evident in the last example. With its ability to capture various types of temporal dependencies, our AFT outperforms the compared models on these challenging samples.\n\n\n\\begin{figure}[tbp]\n\\centering\n\\includegraphics[width=\\columnwidth]{pics/vis2.pdf}\n\\caption{\nVisualization of the predictions. The correct/wrong predictions are highlighted in \\textcolor[RGB]{84,130,53}{green}/\\textcolor[RGB]{192,0,0}{red}. The predicted probability of each answer is also reported. \n}\n\\label{pred}\n\\end{figure}\n\n\n\\begin{figure}[tbp]\n\\centering\n\\includegraphics[width=\\columnwidth]}\n\n\\begin{document}\n\n\\title[Article Title]{Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports}\n\n\\author[1]{\\fnm{Haopeng} \\sur{Li}}\n\n\\author[2]{\\fnm{Andong} \\sur{Deng}}\n\n\\author*[3]{\\fnm{Jun} \\sur{Liu}}\n\n\\author[4]{\\fnm{Hossein} \\sur{Rahmani}}\n\n\\author[5]{\\fnm{Yulan} \\sur{Guo}}\n\n\\author[6]{\\fnm{Bernt} \\sur{Schiele}}\n\n\\author[7]{\\fnm{Mohammed} \\sur{Bennamoun}}\n \n\\author*[1,8]{\\fnm{Qiuhong} \\sur{Ke}}\\email{qiuhong.ke@monash.edu}\n\n\\affil[1]{\\orgdiv{School of Computing and Information Systems}, \\orgname{University of Melbourne}}\n\n\\affil[2]{\\orgdiv{Center for Research in Computer Vision}, \\orgname{University of Central Florida}}\n\n\\affil[3]{\\orgdiv{Information Systems Technology and Design (ISTD) Pillar}, \\orgname{Singapore University of Technology and Design}}\n\n\\affil[4]{\\orgdiv{School of Computing and Communications}, \\orgname{Lancaster University}}\n\n\\affil[5]{\\orgdiv{School of Electronics and Communication Engineering}, \\orgname{Sun Yat-sen University}}\n\n\\affil[6]{\\orgdiv{Department of Computer Vision and Machine Learning}, \\orgname{Max Planck Institute for Informatics, Saarland Informatics Campus}}\n\n\\affil[7]{\\orgdiv{School of Physics, Maths and Computing}, \\orgname{University of Western Australia}}\n\n\\affil[8]{\\orgdiv{Department of Data Science \\& AI}, \\orgname{Monash University}}\n\n\\abstract{Reasoning over sports videos for question answering is an important task with numerous applications, such as player training and information retrieval. However, this task has not been explored due to the lack of relevant datasets and the challenging nature it presents. Most datasets for video question answering (VideoQA) focus mainly on general and coarse-grained understanding of daily-life videos, which is not applicable to sports scenarios requiring professional action understanding and fine-grained motion analysis. In this paper, we introduce the first dataset, named Sports-QA, specifically designed for the sports VideoQA task. The Sports-QA dataset includes various types of questions, such as descriptions, chronologies, causalities, and counterfactual conditions, covering multiple sports. Furthermore, to address the characteristics of the sports VideoQA task, we propose a new Auto-Focus Transformer (AFT) capable of automatically focusing on particular scales of temporal information for question answering. We conduct extensive experiments on Sports-QA, including baseline studies and the evaluation of different methods. The results demonstrate that our AFT achieves state-of-the-art performance.}\n\n\\keywords{Video Question Answering, Sports Video, Benchmark, Auto-Focus Transformer}\n\n\\maketitle\n\n\\section{Introduction}\n\nSports video analysis has been attracting increasing attention in recent years\n\\citep{yuan2021spatio, li2021groupformer, koshkina2021contrastive, zhu2022fencenet, martin2020fine, wang2022shuttlenet,6516867}.\nWhile research progress has been made on tasks such as\nsports action recognition \\citep{li2021multisports, zhu2022fencenet, rasmussen2022compressing}, reasoning over sports videos for question answering\nhas not been explored. As humans, we can not only recognize the actions of the players in sports videos but also understand the effects of the players' actions, explain why a team loses the score, and imagine what would happen under counterfactual situations. Our impressive capabilities in reasoning allow us to answer complex questions related to sports videos, which is crucial in applications such as obtaining crucial statistics in matches for player/team performance qualification, analyzing players' actions and team strategies for performance improvement, and efficiently retrieving information for audiences and analysts.\nWhile sports video reasoning is clearly important, it is under-explored due to challenges and the lack of datasets.\n\n\\begin{figure}[tbp]\n\\centering\n\\includegraphics[width=0.9\\columnwidth]{pics/qadiff.pdf}\n\\caption{\nIllustrations of general VideoQA, which focus on common basic understanding, and sports VideoQA, which requires professional action understanding and action relation reasoning\\protect\\footnotemark. \n}\n\\label{qadiff}\n\\end{figure}\n\n\\footnotetext[\\thefootnote]{We treat the action ``\\textit{block}\" as a noun.}\n\nIn this paper, we tackle sports VideoQA, focusing on reasoning over sports videos for question answering. This is distinct from general VideoQA, which centers on general video understanding, mainly encompassing descriptive and temporal aspects \\citep{jang2017tgif, xu2017video, xiao2021next, yu2019activitynet}, as depicted in Fig. \\ref{qadiff}. % illustrates the difference between general VideoQA and sports VideoQA. %given a sports video, general VideoQA is mostly concerned with common %descriptive questions including  action recognition\\ke{questions  such as general action recognition (i think general also has temporal or other types of questions)}, while \nCompared to general VideoQA, sports VideoQA is more challenging because the questions may involve particular teams and professional actions, and it requires a fine-grained understanding of actions and intra/inter-team relation modeling. %action understanding and intra/inter-team relations including  temporal and causal reasoning. \nAs pointed out in \\citep{shao2020finegym}: 1) coarse-grained global video understanding is insufficient for sports analysis where the actions are rapid and hard to differentiate; 2) the models pre-trained on coarse-grained datasets are not always beneficial to fine-grained tasks. \nIn the meantime, although significant strides have been made in the fine-grained recognition of individual actions \\citep{shao2020finegym,liu2022fineaction,sun2017taichi}, these datasets may fall short in effectively evaluating the ability for temporal and causal reasoning over actions performed by individuals or teams in sports scenarios.\nA large-scale dataset containing professional sports scenarios with detailed annotations of sports-relevant questions is thus required for explicit and comprehensive reasoning for sports VideoQA.\n\nTo address this gap, we introduce a novel dataset called Sports-QA, the first video QA dataset specifically focusing on sports activities. Leveraging sports videos and professional action labels from MultiSports \\citep{li2021multisports} and FineGym \\citep{shao2020finegym}, we construct our dataset. Both sources provide high-quality sports videos with well-defined action labels, allowing us to annotate essential attributes for each crucial action instance. Using these annotations, we generate QA pairs based on pre-defined templates covering aspects such as description, chronology, causality, and counterfactual conditions. The Sports-QA dataset is the first of its kind, comprising approximately 94K QA pairs, encompassing multiple sports and various question types for sports VideoQA. Table \\ref{set} provides a comparison between Sports-QA and several widely-used VideoQA datasets.\n\n\\begin{table}[tbp]\n\\caption{VideoQA datasets comparison. ``Auto/Man\" represents ``automatic/manual\". ``MC/OE\" represents ``multi-choice/open-ended\".}\n\\label{set}\n\\begin{tabular}{@{}lcrrcc@{}}\n\\toprule\nDataset                           & Topic   & \\multicolumn{1}{c}{\\#Video} & \\multicolumn{1}{c}{\\#QA} & Annotation  & QA Task  \\\\ \\midrule\nMSRVTT-QA \\citep{xu2017video}      & \\multirow{4}{*}{General} & 10K                         & 244K                     & Auto        & OE       \\\\\nMSVD-QA \\citep{xu2017video}        &  & 2K                          & 51K                      & Auto        & OE       \\\\\nNExT-QA \\citep{xiao2021next}       &  & 5.4K                        & 52K                      & Man         & OE\\&MC \\\\\nTGIF-QA \\citep{jang2017tgif}       &  & 72K                         & 165K                     & Auto\\&Man & OE\\&MC \\\\ \\midrule\nMovieQA \\citep{tapaswi2016movieqa} & Movie   & 408                         & 15K                      & Man         & MC       \\\\\nSocial-IQ \\citep{zadeh2019social}  & Social  & 1K                          & 8K                       & Man         & MC       \\\\\nDramaQA \\citep{choi2021dramaqa}    & Drama   & 24K                         & 16K                      & Auto\\&Man & MC       \\\\\n\\textbf{Sports-QA (Ours)}                  & Sports  & 6K                          & 94K                      & Auto\\&Man & OE       \\\\ \\botrule\n\\end{tabular}\n\\end{table}\n\nCompared with existing datasets, our Sports-QA provides new insights for VideoQA: 1) It encompasses questions related to specific terms and actions in both group activities (e.g., basketball) and single-person sports (e.g., gym). This requires models to possess the capability to capture dynamic patterns and perform reasoning in scenarios with varying numbers of interactions for question answering. 2) To achieve a comprehensive understanding of complex sports videos, Sports-QA includes diverse questions that involve video information at various temporal granularities. This encompasses global long-term temporal dependencies and fine-grained short-term ones. For instance, a question asking about the number of actions requires the model to capture global dependencies, while a question about the effect of a specific action relies on short-term temporal information. Furthermore, Sports-QA has the potential to be leveraged for other tasks. The pre-defined attributes of actions can be treated as multiple labels, allowing the dataset to be used for multi-label classification tasks for comprehensive action understanding. Additionally, based on the annotated action attributes, we can modify the templates from question answering to declarative sentences, generating descriptive or explanatory narrations and enabling comprehensive sports video captioning.\n\nIn addition to the dataset, we present a novel method for sports VideoQA. Recognizing that sports VideoQA demands the model to capture information from multiple frames at various temporal granularities, we introduce the Auto-Focus Transformer (AFT), featuring an Auto-Focus Attention mechanism. Conceptually, the model operates akin to a temporal magnifying glass with a dynamic focal length, allowing it to inspect the video to answer questions with diverse temporal dependencies. More specifically, our designed Auto-Focus Attention dynamically selects attention focus based on the question. This mechanism empowers the model to handle questions involving video information across various time spans.\n\nThe contributions are summarized as follows:\n\n\\begin{itemize}\n\n    \\item We contribute a large-scale dataset, which consists of various types of questions and multiple sports for VideoQA.  %It provides  various question types and multiple sports. It also provides   \nTo our knowledge, it is the first dataset for complex reasoning over professional sports actions. \n\n    \\item We propose a new Auto-Focus Transformer (AFT), where the attention focus is adaptive based on the question,  enabling the model to deal with questions requiring temporal information of various scales.\n\n    \\item We conduct extensive experiments on our dataset, including baseline study, evaluation of existing methods, and visualization of predictions. The results show the superiority of the proposed AFT for sports VideoQA. %limitations of existing methods and provide directions for future improvements.\n\\end{itemize}\n\n\\section{Related Work}\n\n\\noindent\\textbf{VideoQA Datasets.}  The development of VideoQA has been greatly facilitated by the emergence of various datasets, such as TGIF-QA \\citep{jang2017tgif}, MSVD-QA, and MSRVTT-QA \\citep{xu2017video}, DramaQA \\citep{choi2021dramaqa}, NExT-QA \\citep{xiao2021next}, NExT-OOD \\citep{10107423}. TGIF-QA, for instance, offers a comprehensive set of four sub-tasks designed for temporal reasoning in general videos and is widely recognized in the VideoQA community. MSVD-QA and MSRVTT-QA are open-ended datasets constructed from existing video captions, while DramaQA focuses on understanding drama stories with hierarchical QAs and character-centered video annotation. NExT-QA, introduced for describing and explaining temporal actions, provides both multi-choice and open-ended questions through manual annotation. However, existing datasets predominantly center around general video understanding in daily scenarios or involve coarse-grained action/event reasoning. \\textbf{In contrast, our contribution lies in fine-grained and professional analysis within sports scenarios. We present Sports-QA, a dataset that addresses aspects of description, chronology, causality, and counterfactual conditions for multiple sports of diverse characteristics.}\n\n\\noindent\\textbf{VideoQA Methods.} \nVideoQA poses a significant challenge as it necessitates models to grasp both spatial and temporal information from videos to answer questions. Various deep models have been developed, approaching this task from different perspectives \\citep{fan2019heterogeneous,jiang2020reasoning,li2022invariant,antol2015vqa,gao2018motion,10172254,9770842,10214041,10146482}.\nFor instance, the deep heterogeneous graph alignment network by Jiang et al. \\citep{jiang2020reasoning} addresses VideoQA by simultaneously aligning intra/inter-modality information. Another approach involves a multimodal attention model proposed by Fan et al. \\citep{fan2019heterogeneous}, where heterogeneous memory learns global context from visual features, and question memory captures the complex semantics of questions.\nIGV, introduced by Li et al. \\citep{li2022invariant}, grounds question-critical scenes in videos by considering causal relations that remain invariant to complement contents.\nHowever, a limitation in these approaches is the lack of consideration for the fact that different questions may require temporal dependencies of specific scales. \\textbf{In response to this challenge, we propose the Auto-Focus Transformer, designed to automatically focus on a specific temporal scale based on the question for VideoQA.}\n\n\\noindent\\textbf{Sports Video Understanding.} Sports video understanding has drawn increasing attention in recent years \\citep{yuan2021spatio,li2021groupformer,koshkina2021contrastive,zhu2022fencenet,martin2020fine,wang2022shuttlenet}. Researchers have made great efforts in various tasks such as sports action recognition \\citep{shao2020finegym}, multi-person action detection \\citep{li2021multisports}, and action quality assessment \\citep{tang2020uncertainty}. Meanwhile, numerous sports datasets are constructed \\citep{parmar2019action,li2021multisports,giancola2018soccernet,deliege2021soccernet}.  \nFor example, AQA-7 \\citep{parmar2019action} is constructed for professional action quality assessment. Seven types of actions are included in this dataset and all the action instances are associated with quality scores. \nMultiSports \\citep{li2021multisports} is proposed for the spatial-temporal detection of professional sports actions. FineGym~\\citep{shao2020finegym} focuses on fine-grained activity localization that requires an accurate understanding of the atomic level of a gymnastic action.\n\\textbf{In this work, we construct Sports-QA, the first dataset for sports VideoQA, bridging the gap between the visual modality and the textual modality in sports analysis.}\n\n\\section{Sports-QA Dataset}\n\n\\begin{table}[tbp]\n\\caption{The statistics for the MultiSports dataset are presented.} \n\\label{mss}\n\\begin{tabular}{@{}lcrcc@{}}\n\\toprule\n Sports          & \\# Action & \\# Instance & Avg. Action/Video Duration & \\# Bounding box \\\\ \\midrule\nGym        & 21      & 8,703     & 1.5s / 30.7s        & 325K    \\\\\nVolleyball & 12      & 7,645     & 0.7s / 10.5s        & 139K   \\\\\nFootball   & 15      & 12,254    & 0.7s / 22.6s        & 225K    \\\\\nBasketball & 18      & 9,099     & 0.9s / 19.7s        & 213K   \\\\\\midrule\nTotal      & 66      & 37,701    & 1.0s / 20.9s        & 902K   \\\\ \\botrule\n\\end{tabular}\n\\end{table}\n\n\\subsection{Data Source}\n\nRegarding the collection of sports videos, we consider the following aspects: 1) The visual quality of the video data should be high enough to conduct fine-grained video understanding, such as video resolution and frame rate. 2) Instead of applying to a single type of sports, we expect a VideoQA dataset involving multiple sports.\nAfter a deep survey of the works on sports video understanding, we find that the MultiSports \\citep{li2021multisports} and FineGym \\citep{shao2020finegym} datasets are highly suitable for our purpose. The details of MultiSports and FineGym are as follows.\n\n\\noindent\\textbf{MultiSports} \\citep{li2021multisports} is a dataset for the temporal localization of sports actions, encompassing four sports (i.e., basketball, football, volleyball, and aerobic gymnastics) and 66 fine-grained action categories\\footnote{Project homepage: \\url{https://deeperaction.github.io/datasets/multisports.html}}. The action categories in MultiSports are professional terms (such as ``\\textit{volleyball spike}\", ``\\textit{football tackle}\", and ``\\textit{basketball defensive rebound}\") instead of common and atomic actions like ``\\textit{run}\" and ``\\textit{stand}\". For each action instance, the dataset provides the bounding boxes of a player from the starting frame of an action to the ending frame, forming the action spatial-temporal tube. The statistics of MultiSports are shown in Table \\ref{mss}.\n\n\\noindent\\textbf{FineGym} \\citep{shao2020finegym} is a dataset designed to elevate the field of action recognition by addressing the limitations observed in existing techniques\\footnote{Project homepage: \\url{https://sdolivia.github.io/FineGym}}. Developed to surpass current benchmarks, it offers a unique combination of richness, quality, and diversity in its content.\nThis dataset is constructed on gymnasium videos, providing a realistic and varied environment for action recognition studies. What sets FineGym apart from other datasets is its meticulous temporal annotation at both action and sub-action levels, featuring a three-level semantic hierarchy. This hierarchical structure allows for a more nuanced understanding of activities, enabling researchers to explore and analyze actions in finer detail. As an illustrative example, an event is annotated as a sequence of elementary sub-actions Importantly, each sub-action within these sets is further annotated with finely defined class labels, contributing to a higher level of granularity in action recognition. Fig. \\ref{ms} shows the action hierarchy of the MultiSports and FineGym datasets.\n\n\\begin{figure}[tbp]\n\\centering\n\\includegraphics[width=\\columnwidth]{pics/MultiSports1.pdf}\n\\caption{The action hierarchy of the MultiSports dataset is depicted at the top, while the FineGym dataset's hierarchy is shown at the bottom. It's important to note that the figure includes only four example actions for each sport.}\n\\label{ms}\n\\end{figure}\n\nThe action categories in MultiSports and FineGym are professional terms (such as ``\\textit{volleyball spike}\", ``\\textit{football tackle}\", ``\\textit{basketball defensive rebound}\", and ``\\textit{vault Salto backward tucked}\") instead of common and atomic actions (such as ``\\textit{run}\" and ``\\textit{stand}\"). Such professional labels are of great value to our dataset.\nSince the length of videos varies greatly (from a few seconds to a few minutes), we segment the videos into clips and generate QA pairs based on these clips.\n\n\\subsection{Question-Answer Pair Generation}\n\nIn most existing works, QA pairs are generated using two approaches:\n1) Automatically generating questions and answers based on video captions using a set of predefined question templates \\citep{jang2017tgif, xu2017video, yang2021just}. This method is efficient, but it may introduce obvious grammatical errors or lose crucial information during the conversion of captions into questions and answers.\n2) Manually annotating questions and answers through crowdsourcing \\citep{xiao2021next, yu2019activitynet, garcia2020knowit}. Although this approach produces QA pairs of high quality in terms of accuracy and expression, manual annotation is time-consuming and expensive.\nIn this work, we aim to ensure the quality of the textual data while considering annotation costs. We achieve this by generating QA pairs using pre-defined templates based on existing labels and newly-labeled attributes of actions. Our approach guarantees grammatically error-free QA pairs that capture crucial information in videos, thanks to the careful design of sophisticated templates. Furthermore, as ball games and gymnastics have distinct characteristics, we generate the QA pairs in different ways as described as follows.\n\n\\subsubsection{QA Pair Generation for Ball Games}\n\nSpecifically, for ball games, including basketball, football, and volleyball, we define five attributes (Team, Outcome, Cause of Outcome, Cause of Action, and Effect of Action) for actions. We then manually annotate the attributes of each action. The definitions and annotation process of these attributes are elaborated as follows.\n\n\\noindent\\textbf{Attribute Definition.} MultiSports consists of 45 action categories from ball games, with some being crucial for professional sports statistics or quantifying the performance of players/teams. For instance, the ``\\textit{2-Point Field Goal Percentage}\" in a basketball match necessitates a fine-grained understanding of all ``\\textit{2-point shot}\" actions, while the ``\\textit{saving}\" actions in football directly reflect the performance of goalkeepers. In this work, we focus on 28 crucial actions, listed in Table \\ref{acts}. Considering both practical applications and research purposes, we define five attributes for each of the crucial action instances as follows.\n\n\\noindent 1. \\texttt{Team}: The team that the player of the action belongs to is denoted by two options: \\texttt{left team} or \\texttt{right team}. Specifically, in basketball and football, the determination of the left or right team depends on the team attacking towards the right or left. In volleyball, the left or right team is designated based on their position relative to the left or right side of the net.\n\n\\noindent 2. \\texttt{Outcome}: This attribute assesses whether the result of the action meets the expectation and is defined as a binary label: \\texttt{successful} or \\texttt{failed}. For example, if a ``\\textit{2-point shot}\" in basketball scores a goal, it is annotated as successful; otherwise, it is labeled as failed.\n\n\\noindent 3. \\texttt{Cause of Outcome}: \nThis attribute indicates the cause of failure for offensive actions, such as a ``\\textit{2-point shot}\" in basketball, ``\\textit{shoot}\" in football, and ``\\textit{spike}\" in volleyball, or the cause of success for defensive actions, like ``\\textit{block}\" in football and ``\\textit{save}\" in volleyball. Specifically, the cause of failure for offensive actions reflects player shortcomings. For example, the failure of an offensive ``\\textit{long pass}\" in football could be attributed to \\texttt{defensive interception}, \\texttt{bad pass}, or \\texttt{bad catch}. Identifying the precise cause helps teams identify their weaknesses and improve performance. Similarly, the cause of success for defensive actions highlights player strengths. For instance, the success of a defensive ``\\textit{save}\" in volleyball might be due to \\texttt{offensive out of bounds}, \\texttt{offensive blocked by net}, or \\texttt{actually catch the ball}, with the last case directly reflecting defensive performance. We do not consider the cause of success for offensive actions or the cause of failure for defensive actions, as they cannot be attributed to different cases. For example, the failure of ``\\textit{football saving}\" can only be explained by the goalkeeper missing the ball. Therefore, these causes are not worth discussing. We define various cause options for applicable action categories and ask annotators to choose from them. Note that this attribute is applicable to certain action categories, and the causes vary for different action categories.\n\n\\noindent 4. \\texttt{Cause of Action}: This attribute indicates the actions that cause the current action. Each action instance in a video is initially labeled with its time order, based on the start time, which serves as the unique ID of the action in the video. For each crucial action, annotators are asked to provide the IDs of the actions that cause the current action; these causes are not necessarily crucial actions. Consequently, the \\texttt{Cause of Action} is a list consisting of the IDs of actions that are mostly temporally adjacent to the current action. Not all crucial action categories are required to provide this attribute because the causes of some actions are unique. For instance, the only cause of ``\\textit{volleyball first pass}\" is ``\\textit{volleyball serve}\".\n\n\\noindent 5. \\texttt{Effect of Action}: This attribute aims to identify the actions caused by the current action, analogous to \\texttt{Cause of Action}. Although the cause and effect are conjunctive, we may consider only one of them, as some actions are not crucial. For instance, if ACTION M (crucial action) causes ACTION N (non-crucial action), we only label ACTION N as an effect of ACTION M.\n\n\\noindent\\textbf{Attribute Annotation.}\nAfter defining the attributes mentioned above, we proceed to annotate them for each crucial action instance in MultiSports. The annotation process is divided into three stages: pre-annotation, formal annotation, and quality check.\nIn the pre-annotation stage, annotators are grouped into three categories, with each group assigned responsibility for a specific type of ball game. Each annotator within a group is then assigned several videos corresponding to their designated sport. During this stage, annotators are tasked with labeling the attributes of the first 50 action instances. Following this, an intra-group check is conducted to address any issues related to understanding bias or potential mistakes, as well as to handle unexpected or rare situations. The annotation can only progress to the next stage when annotators achieve a consistent understanding of each attribute, as outlined in the protocols.\nMoving to the formal annotation stage, annotators use shared protocols to amend their previous annotations and label the remaining action instances. Once all actions are labeled, we perform an inter-group quality check on all annotations. The purpose of this inter-group check is to ensure that the annotations align with the common understanding of average individuals.\nThe annotation process is completed within one month and involves the collaboration of 15 graduate students.\n\nBased on the attributes, we generate questions about the videos by designing various templates (listed in the end of the paper) to inquire about these attributes. For instance, given a video shown in Fig. \\ref{egdata}, after annotating the attributes for the action ``\\textit{spike}\", a question querying a particular attribute (e.g., Outcome) of the action can be generated, such as ``\\textit{Is the `spike' of the right team successful?}\".\nSpecifically, our dataset involves four types of questions: descriptive, temporal, causal, and counterfactual. The details of each type are described as follows.\n\n\\begin{figure*}[tbp]\n\\centering\n\\includegraphics[width=0.93\\textwidth]{pics/egs.pdf}\n\\caption{Example of Sports-QA: The actions in the green boxes (such as ``\\textit{2-point shot}\") are the query actions, while the actions in the yellow boxes (such as ``\\textit{block}\") represent the effects. For ball games, annotators provide attribute labels, and we generate QA pairs based on these attributes. In gymnastics, we generate QA pairs using annotations from MultiSports/FineGym.\n}\n\\label{egdata}\n\\end{figure*}\n\n\\noindent 1. \\textbf{Descriptive:} Descriptive questions require holistic comprehension and involve various aspects of information from the videos. These questions include simple queries such as ``\\textit{What is the video about?}\" and ``\\textit{Does SOME-TEAM perform SOME-ACTION?}\", as well as complex ones involving counting and the outcome of actions, such as ``\\textit{How many times does SOME-TEAM perform SOME-ACTION?}\" and ``\\textit{Does SOME-TEAM successfully do their i-th SOME-ACTION?}\"\n\n\\noindent 2. \\textbf{Temporal:} Temporal questions focus on the temporal relations among the actions of the same or different teams. Specifically, these questions require an understanding of intra-group temporal relations and inter-group ones. Typical templates for this type of question are ``\\textit{What does SOME-TEAM do before/after their i-th SOME-ACTION?}\" and ``\\textit{What does the left/right team do before/after the other team performs their i-th SOME-ACTION?}\"\n\n\\noindent 3. \\textbf{Causal:} Causal questions aim to uncover the causal relations among the action instances and to explain the reasons or the specific process of the actions. Different from temporal questions, these questions require causal reasoning based on visual cues. The templates of these questions include causal queries such as ``\\textit{Why does SOME-TEAM do the i-th SOME-ACTION?}\" and ``\\textit{What is the effect of the i-th SOME-ACTION of SOME-TEAM?}\", as well as explanation queries such as ``\\textit{How does SOME-TEAM succeed in doing/fail to do the i-th SOME-ACTION?}\"\n\n\\noindent 4. \\textbf{Counterfactual:} Different from the above three types of questions querying the details that actually happen, counterfactual questions set hypothetical conditions that do not occur in the video and query about the expected outcomes based on the conditions. This type of question requires reasoning about various imagined situations and expects the outcomes according to the causal relations among actions. The template for this type of question is ``\\textit{Would the left/right team succeed in do the i-th SOME-ACTION if the other team did not do SOME-ACTION?}\"\n\n\\noindent\\textbf{From Attributes to Question-Answer Pairs.} In particular, the questions are queries about the attributes, and their answers can be obtained directly from our annotations or through logical reasoning and statistical analysis over the annotations. For instance, for a question asking about the cause of a specific action, the answer can be easily retrieved from its attribute of cause; For a question asking whether a specific action is successful, we can check its outcome attribute. We guarantee the correctness of the answers as we have checked the annotations of attributes as mentioned previously.\nThese answers include responses like ``\\textit{yes/no}\", numerical values, sports names (such as ``\\textit{volleyball}\"), action categories in MultiSports and FineGym (such as ``\\textit{block}\"), and short phrases describing the reasons (such as ``\\textit{out of bounds}\"). See Fig. \\ref{egdata} for more concrete examples of the attributes and the corresponding generated QA pairs. \n\nAn interesting aspect of our dataset is that it can be utilized for other tasks due to our two-stage annotation process. Specifically, we have defined several attributes for each action instance, which can be considered as multiple labels. Consequently, our dataset can be employed for conducting multi-label classification.\nMoreover, by altering the template from question answering to declarative sentences, we can generate descriptive or explanatory narrations for sports videos. Hence, our dataset can also be exploited for sports video captioning.\n\n\\subsubsection{QA Pair Generation for Gymnastics}\n\nA major difference between aerobic gymnastics/FineGym and ball games is the absence of the concept of a team. Furthermore, there are no causal relations among the actions performed in these sports. Therefore, we only generate descriptive and temporal questions for aerobic gymnastics/FineGym based on their annotations. In addition to the templates used for ball games, we have also designed some new templates. For descriptive questions, we include queries about the number of players or actions, such as ``\\textit{How many actions does the player perform?}\" and ``\\textit{How many times does the player perform SOME-ACTION?}\" For temporal questions, we add counting questions like ``\\textit{How many times do the players do SOME-ACTION before SOME-ACTION?}\" Such a QA design leverages the fine-grained features in the datasets and constitutes challenging temporal reasoning that forces the models to focus on the salient sub-actions in the whole gymnastic event.\n\n\\subsection{Diversity, Debiasing and Problem Setting}\n\n\\noindent \\textbf{Diversity of Questions.} The limited diversity in generating questions using pre-designed templates may not significantly impact the significance of datasets for several reasons. First, in the context of sports, there is a high level of restriction imposed by rules and specific content, resulting in a more focused attention from the audience on particular actions and events. This inherent structure in sports content narrows down the range of potential questions, making template-based question generation a feasible approach.\nSecondly, the practice of template-based question generation is widely accepted and employed in the construction of VideoQA datasets \\citep{jang2017tgif, xu2017video, yang2021just}. \nThe adoption of this approach in reputable datasets attests to its effectiveness and acceptance. Therefore, while the questions are template-based, their relevance and significance in the context of sports are preserved, ensuring the robustness of the dataset for analysis and evaluation.\n\n\\noindent \\textbf{Debiasing.} Unlike daily scenarios, players in sports are highly restricted by rules. For instance, after the left team executes a ``\\textit{volleyball serve}\", the right team must follow with a ``\\textit{first-pass}\". This results in highly correlated actions in the videos, leading to questions and answers that are also highly correlated in our generated pairs. However, such questions are often meaningless for VideoQA, as the answers can be easily inferred through rules or common sense and should be removed.\nTo address this, we first obtain meta-questions by removing team information and the order of actions from the original questions. For example, the meta-question for ``\\textit{What does the left team do after the other team does the second spike?}\" becomes ``\\textit{What does the team do after the other team does spike?}\" We then examine the correlations between the meta-questions and their answers. If there is only one answer to the meta-question, we remove the corresponding QA pairs. If an answer to a meta-question occurs more frequently than other answers (frequency larger than 0.5), we randomly remove the corresponding QA pairs to balance the frequencies, ensuring they are all lower than 0.5.\n\n\\noindent \\textbf{Problem Setting.} Following the approach in \\citep{jang2017tgif, xu2017video}, we define an open-ended task based on the generated QA pairs. It is important to note that the open-ended setting, along with the multiple-choice setting, is widely used in the field of VideoQA. We have chosen to commit to the open-ended task instead of the multiple-choice one for two reasons: 1) The open-ended task is more challenging as it requires models to choose from a large answer set rather than selecting from several given options. 2) Limiting the answer choices to only several options would significantly decrease the diversity of the dataset. Specifically, all answers in the QA pairs form an answer pool, which is treated as 191 classes (after discarding classes with fewer than 30 samples). \n\n\\noindent \\textbf{Why not Fine-Grained Action Recognition?} It is essential to underscore that merely achieving fine-grained action recognition is insufficient for Sports-QA. The nature of the questions in this context demands not only precise detection of actions but also involves intricate temporal and causal modeling. To illustrate, questions that inquire about the effects of certain actions necessitate the accurate identification of temporally adjacent actions and a nuanced understanding of their causality. In summary, Sports-QA requires a more advanced level of comprehension and temporal reasoning beyond basic action recognition.\n\n\\subsection{Dataset Statistics}\n\nTable \\ref{n_qa} presents the numbers of QA pairs for various question types and different sports in Sport-QA. Our dataset comprises approximately 94K QA pairs. It's worth noting that a substantial portion of our descriptive questions involves complex action temporal localization and counting, presenting challenges similar to other question types. Additionally, we have balanced the number of questions across different sports.\n\n\\begin{table}[tbp]\n\\caption{The numbers of QA pairs for different types and different sports.}\n\\label{n_qa}\n\n\\begin{tabular}{@{}lrrrrr@{}}\n\\toprule\n Sports       & Descriptive & Temporal & Causal & Counterfactual & Total  \\\\ \\midrule\nBasketball & 5,629 & 22    & 785    & 278      & 6,714  \\\\\nFootball   & 6,659 & 1,355 & 1,949  & 523      & 10,486 \\\\\nVolleyball & 6,120 & 360   & 1,942  & 685      & 9,107  \\\\\nGym     & 6,382 & 1,997 & 0      & 0        & 8,379  \\\\\nFloor Exercise      &  6,046      &  11,012     & 0      & 0        &   19,418     \\\\\nBalance Beam       &   7,477     &   12,773    & 0      & 0        &   20,250     \\\\\nUneven Bars      &   7,294     &   12,124    & 0      & 0        &    17,058    \\\\\nVault      &   2,661     &   0    & 0      & 0       &   2,661     \\\\ \\midrule\nTotal   &   48,268     &   39,643    & 4,676  & 1,486    &    94,073    \\\\ \\botrule\n\\end{tabular}\n\\end{table}\n\n\\begin{figure*}[tbp]\n\\includegraphics[width=\\textwidth]{pics/ans_dist_type.pdf}\n\\caption{The distributions of answer classes broken down by question types.}\n\\label{ans_dist_type}\n\\end{figure*}\n\nThe total number of videos in our dataset is 5,967, distributed across different sports as follows: basketball (879), football (1,030), volleyball (586), gym (505), vault (501), uneven bars (834), floor exercise (770), and balance beam (862). Sports-QA is divided into training, validation, and testing sets (60\\%/20\\%/20\\%). Specifically, videos of the same sport are randomly assigned to each set to ensure similar distributions of sport types across subsets. Additionally, we have ensured that the distributions of answer classes are similar in different subsets.\n\nFig. \\ref{ans_dist_type} illustrates the distribution of answer classes broken down by question types in Sports-QA. Note that we only showcase the first 80 classes with more examples. As depicted in the figure, the distribution of answers is long-tailed, and the unbalanced nature adds to the challenge of our dataset. Meanwhile, classes of the same type, such as ``yes/no\", are balanced. This further increases the difficulty of our dataset, as the models are required to examine the video for answering the question instead of merely guessing the answer based on the question type. \n\n\\section{Auto-Focus Transformer for Sports Video Question Answering}\n\nA key characteristic of the sports VideoQA task is the need for reasoning over temporal dependencies of various scales for multiple objects. For instance, a question asking about the number of actions requires the model to capture global dependencies in the video, whereas a question querying the effect of an action relies on short-term temporal information. The scale of temporal dependency required varies based on the question. However, current Transformer-based or GNN-based VideoQA methods tend to focus on global dependencies, irrespective of the required scale of temporal information. To address this limitation, we propose the Auto-Focus Transformer (AFT), a new Transformer encoder featuring a novel multi-head Auto-Focus Attention (AFA) mechanism, designed specifically for sports VideoQA.\n\n\\begin{figure}[tbp]\n\\centering\n\\includegraphics[width=\\columnwidth]{pics/aft.pdf}\n\\caption{The structure of the sports  VideoQA model based on the proposed Auto-Focus Transformer.}\n\\label{aft}\n\\end{figure}\n\nSpecifically, we start by extracting the appearance feature and motion feature of the frame using pretrained ResNet \\citep{he2016deep} and I3D \\citep{carreira2017quo} (refer to Section \\ref{feaext} for more details). Subsequently, we concatenate these features and input them into a linear layer for dimension reduction and feature fusion. Consequently, for each video, we obtain a sequence of frame representations ($\\bm{R}$). Regarding the question, we utilize word embedding and a bi-directional RNN to obtain the text representations, encompassing both the global one ($\\bm{w}$) and the local ones ($\\bm{W}$).\n\nWe then  capture the dependencies among frames using AFT where the AFA mechanism\nis computed as follows (the multi-head AFA is defined likewise): given the key set $\\left\\{\\bm{k}_i\\right\\}$, the value set $\\left\\{\\bm{v}_i\\right\\}$, and a query $\\bm{q}_j$ ($\\bm{q}_j,\\bm{k}_i,\\bm{v}_i \\in \\mathbb{R}^d$ are mapped from $\\bm{R}$ in AFA), \n\\begin{equation}\n{\\rm AFA}(\\bm{q}_j)=\\sum_{f\\in \\mathcal{F}}\\alpha_f\\sum_{i\\in \\mathcal{D}_j^f}{\\rm softmax}_i\\left(\\frac{\\bm{q}_j^\\mathrm{T}\\bm{k}_i}{\\sqrt{d}}\\right)\\bm{v}_i,\n\\end{equation}\nwhere $\\mathcal{F}$ is a set of pre-defined focal lengths. $\\mathcal{D}_j^f=\\{i||i-j|\\leq f\\}$ is the index set of the neighbours of $\\bm{q}_j$ within focal length $f$. $\\alpha_f\\in(0,1)$ is the dynamic weight of focus $f$, satisfying $\\sum_f\\alpha_f=1$ and depended on the question. As shown in Fig. \\ref{aft},   $\\bm{\\alpha}=\\{\\alpha_f\\}_{f=1}^\\mathcal{|F|}$ are obtained by applying linear projection and softmax to the global representation of question.  \nCompared to traditional attention that models global dependencies in any case, our AFA captures only the temporal dependencies of the scale required by the question. This design allows the model to focus solely on crucial information and discard irrelevant parts for better prediction. \nWe denote the output of AFT as $\\bm{M}$.\n\nFor the video-text integration, we draw inspiration from \\citep{li2022invariant} and \\citep{jiang2020reasoning}, employing a graph convolutional network (GCN) and block fusion \\citep{ben2019block}. Refer to \\citep{li2022invariant} for detailed information. Ultimately, we obtain a fused feature $\\bm{s}$ and predict the answer using a linear projection and softmax.\n\n\\section{Experiments}\n \n\n \n\\subsection{Experimental Settings}\n\\label{feaext}\n\nWe start by explaining the experimental settings, covering data pre-processing and feature extraction, implementation details, and the evaluation metrics.\n\n\\noindent\\textbf{Pre-processing and Representation.} The videos in our dataset are down-sampled to 5 FPS to reduce temporal redundancy. We extract two types of visual features for experiments: appearance feature and motion feature. For each feature type, we obtain both global and local features.\nSpecifically, we detect 10 objects in every two frames of each video using Faster-RCNN \\citep{ren2015faster} (with ResNet-101 \\citep{he2016deep} as the backbone and a score threshold set to 0.6), pretrained on COCO \\citep{lin2014microsoft}. These detected objects serve as local appearance features in our work. Additionally, we use the global average pooling of Res4 activation in Faster-RCNN as the global appearance feature.\nTo extract motion features, we employ I3D \\citep{carreira2017quo} pretrained on Kinetics \\citep{carreira2017quo}. Specifically, for each frame, we combine it with its adjacent 7 frames, creating a clip that we then feed to I3D to extract a 3D feature map. The global motion feature of a frame is obtained by applying global average pooling to the 3D feature. The local motion feature of an object in a frame is obtained by applying RoIAlign \\citep{he2017mask} (output size $1\\times 1$) to the 3D feature with its corresponding bounding box. Note that our method uses only the global feature.\nFor language, we explore two types of word embeddings: GloVe \\citep{pennington2014glove} and pre-trained BERT \\citep{devlin2018bert}.\n \n\\noindent\\textbf{Implementation Details.} We set the attention focus to $\\mathcal{F}=\\{3,9,80\\}$ to cover short-term, mid-term, and long-term dependencies, respectively. The hidden dimension $d$ is set to 512. The loss function is cross-entropy. The models are trained for 50 epochs with a learning rate of $1\\times 10^{-4}$ and a batch size of 16, using the Adam optimizer \\citep{kingma2014adam}.\n\n\\noindent\\textbf{Evaluation Metrics.} For evaluation metrics, we employ accuracy to showcase the models' performance on our dataset. Additionally, recognizing the long-tail distribution of our dataset, we evaluate methods using F1-score. F1-score is particularly important for sports VideoQA as it explicitly reflects performance averaged over imbalanced classes.\n\n\\begin{table}[tbp]\n\\caption{Baseline results on Sports-QA, including random choosing, semantic-aware (S-A) random choosing, LSTM-based BlindQA, and Transformer-based BlindQA. The best and the second-best results (\\%) are \\textbf{bold} and \\underline{underlined}.} \n\\label{bl}\n\\begin{tabular}{@{}ccccccc@{}}\n\\toprule\n\\multirow{2}{*}{Metric} & \\multicolumn{2}{c}{Baseline} & \\multicolumn{2}{c}{GloVe}            & \\multicolumn{2}{c}{BERT}                    \\\\\\cmidrule(l){2-3} \\cmidrule(l){4-5} \\cmidrule(l){6-7} \n                        & Random   & S-A Random   & LSTM & Transformer                   & LSTM                          & Transformer \\\\ \\midrule\nAccuracy                & 0.5      & 24.6         & 44   & 43.7                          & \\underline{43.9} & 43.9        \\\\\nF1-score                & 0.3      & 5.4          & 13.5 & \\underline{14.9} & 13.8                          & 15.8        \\\\ \\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\begin{table*}[tbp]\n\\caption{The results of different methods on Sports-QA. The sport-wise accuracy, question-type-wise accuracy, overall accuracy and F1-score are presented (\\%).\nThe best and second best results are \\textbf{bold} and \\underline{underlined}.} \n\\label{res1}\n\n\\resizebox{0.95\\columnwidth}{!}{\\begin{tabular}{clcccccccccll}\n\\toprule\n \\multirow{2}{*}{Word Emb.}& \\multirow{2}{*}{Model} & \\multicolumn{5}{c}{Sports} & \\multicolumn{4}{c}{Question Type} & \\multirow{2}{*}{Acc.} & \\multirow{2}{*}{F1-score} \\\\ \\cmidrule(lr){3-7}\\cmidrule(lr){8-11}\n &  & Basket. & Foot. & Volley. & Gym  &FineG.& Desc. & Temp. & Causal & Counter. &  &  \\\\ \\hline\\hline\n & \\cellcolor[HTML]{C0C0C0}BlindQA & \\cellcolor[HTML]{C0C0C0}33.4& \\cellcolor[HTML]{C0C0C0}62.9& \\cellcolor[HTML]{C0C0C0}44.9& \\cellcolor[HTML]{C0C0C0}36.5&\\cellcolor[HTML]{C0C0C0}42.0& \\cellcolor[HTML]{C0C0C0}51.7& \\cellcolor[HTML]{C0C0C0}32.7& \\cellcolor[HTML]{C0C0C0}48.7& \\cellcolor[HTML]{C0C0C0}51.3& \\cellcolor[HTML]{C0C0C0}43.7& \\cellcolor[HTML]{C0C0C0}14.9\\\\\n & CoMem \\citep{gao2018motion} & 72.8& 63.1& 69.8& 70.0&50.3& 77.0& 32.9& 50.0& 60.9& 57.1& 23.1\\\\\n & HME \\citep{fan2019heterogeneous} & 64.8& 63.3& 68.5& 71.6&49.3& 76.5& 31.2& 50.4& 54.3& 56.0& 23.6\\\\\n & HGA \\citep{jiang2020reasoning} & 71.3& 65.4& 72.2& 72.2&51.3& 78.1& 34.4& 52.0& 59.2& 58.3& 24.5\\\\\n & HQGA \\citep{xiao2022video} & 72.8& 64.6& 70.5& 71.6&51.0& 77.6& 33.8& 54.0& 58.2& 57.9& \\underline{25.0}\\\\\n & IGV \\citep{li2022invariant} & 68.8& 66.0& 70.6& 72.4&52.1& 78.2& 34.7& 52.1& 59.2& \\underline{58.5}& 24.8\\\\\n & MASN \\citep{seo2021attend} & 71.2& 64.2& 70.1& 66.9&50.5& 76.4& 33.6& 50.2& 58.9& 57.0& 23.4\\\\ \\cmidrule(l){2-13} \n & Baseline (Ours) & 69.5& 65.4& 68.9& 71.8&52.3& 77.8& 35.0& 52.4& 59.9& 58.0& 23.7\\\\\n\\multirow{-9}{*}{GloVe} & AFT (Ours) & 73.9& 67.8& 69.5& 71.8&52.5& 78.9& 35.3& 55.1& 56.3& \\textbf{59.2} {\\color[HTML]{036400}(+1.2)}& \\textbf{25.6} {\\color[HTML]{036400}(+1.9)}\\\\ \\hline\\hline\n & \\cellcolor[HTML]{C0C0C0}BlindQA & \\cellcolor[HTML]{C0C0C0}34.7& \\cellcolor[HTML]{C0C0C0}52.1& \\cellcolor[HTML]{C0C0C0}45.9& \\cellcolor[HTML]{C0C0C0}38.3&\\cellcolor[HTML]{C0C0C0}43.9& \\cellcolor[HTML]{C0C0C0}51.8& \\cellcolor[HTML]{C0C0C0}33.3& \\cellcolor[HTML]{C0C0C0}48.0& \\cellcolor[HTML]{C0C0C0}53.9& \\cellcolor[HTML]{C0C0C0}43.9& \\cellcolor[HTML]{C0C0C0}15.8\\\\\n & CoMem \\citep{gao2018motion} & 74.1& 64.0& 68.7& 70.7&49.3& 77.2& 32.1& 48.4& 55.6& 56.6& 26.4\\\\\n & HME \\citep{fan2019heterogeneous} & 64.7& 63.4& 66.2& 70.6&49.3& 76.6& 30.7& 46.6& 53.6& 55.6& 22.9\\\\\n & HGA \\citep{jiang2020reasoning} & 71.6& 66.5& 69.6& 70.7&51.3& 78.0& 33.8& 54.3& 56.6& 58.1& \\underline{25.1}\\\\\n & HQGA \\citep{xiao2022video} & 74.1& 63.5& 67.2& 70.8&50.3& 76.6& 33.6& 49.1& 57.9& 57.0& 25.0\\\\\n & IGV \\citep{li2022invariant} & 72.0& 63.9& 70.4&72.4 &51.7& 78.0&34.2 & 51.9& 63.2& \\underline{58.2}&23.8 \\\\\n & MASN \\citep{seo2021attend} & 73.0& 63.0& 69.7& 66.6&50.1& 76.4& 32.5& 50.6& 61.2& 56.6& 24.1\\\\ \\cmidrule(l){2-13} \n & Baseline (Ours) & 70.7& 66.6& 69.2& 71.5&51.8& 78.0& 34.8& 50.5& 60.2& 57.9& 23.9\\\\\n\\multirow{-9}{*}{BERT} & AFT (Ours) & 72.3& 67.9& 70.4& 71.6&52.4& 78.3& 35.5& 56.8& 58.2& \\textbf{59.1} {\\color[HTML]{036400}(+1.2)}& \\textbf{25.4} {\\color[HTML]{036400}(+1.5)}\\\\ \\botrule\n\\end{tabular}}\n\\end{table*}\n\n\\begin{table}[tbp]\n\\caption{The visual features used by different models. A./M./G./L. represents Appearance/Motion/Global/Local. Mem, IG, HL, and GNN stand for Memory, Invariant Grounding, Hierarchical Learning, and Graph Neural Network, respectively.} \n\\label{fea}\n\\begin{tabular}{@{}lcccccc@{}}\n\\toprule\nModel                           & Venue   & Insight            & G.A        & G.M.       & L.A.       & L.M.       \\\\ \\midrule\nCoMem \\citep{gao2018motion}      & CVPR'18 & Mem             & \\checkmark & \\checkmark &            &            \\\\\nHME \\citep{fan2019heterogeneous} & CVPR'19 & Mem             & \\checkmark & \\checkmark &            &            \\\\\nHGA \\citep{jiang2020reasoning}   & AAAI'20 & GNN                & \\checkmark & \\checkmark &            &            \\\\\nIGV \\citep{li2022invariant}      & CVPR'22 & IG & \\checkmark & \\checkmark &            &            \\\\\nHQGA \\citep{xiao2022video}       & AAAI'22 & HL            & \\checkmark & \\checkmark & \\checkmark &            \\\\\nMASN \\citep{seo2021attend}       & ACL'21  & GNN                & \\checkmark & \\checkmark & \\checkmark & \\checkmark \\\\\\midrule\nAFT (Ours)      &   &   AFA              & \\checkmark  & \\checkmark  &  & \\\\\\botrule\n\\end{tabular}\n\\end{table}\n\n\\subsection{Baseline Study}\n\nSeveral baselines are constructed to evaluate their performance on our Sports-QA dataset as follows.\n\n\\noindent\\textbf{Random Choosing:} Answers are randomly selected from the 191 answer classes, resulting in an accuracy of approximately $\\frac{1}{191} \\approx 0.5\\%$. The F1-score is obtained by running the random test for 200 times and taking the average.\n\n\\noindent\\textbf{Semantic-Aware Random Choosing:} The randomly selected answer is constrained by the question type. For example, binary questions (beginning with ``\\textit{Do}\" or ``\\textit{Is}\") can only choose from ``\\textit{yes}\" or ``\\textit{no}\"; questions asking about the number of something (beginning with ``\\textit{How many}\") can only choose a number.\n\n\\noindent\\textbf{BlindQA:} Models are constructed with a question encoder and an MLP-based answer decoder, without exploiting visual information. Two types of word embeddings (Glove and pre-trained BERT) and two question encoders (LSTM and Transformer \\citep{vaswani2017attention}) are studied.\n\nThe results of the baselines are shown in Table \\ref{bl}. Random choosing exhibits poor accuracy and F1-score, but considering constraints from the questions significantly improves performance. BlindQA achieves considerable results in both accuracy and F1-score, indicating noticeable semantic correlations between questions and answers. By maximizing the likelihood of the answer conditioned on a specific question type, the model fits the answer distribution given different questions in the training set.\n\nRegarding word embedding, pre-trained BERT achieves slightly better accuracy than GloVe, with noticeable improvements in F1-score. Concerning different question encoders, Transformer and LSTM achieve similar accuracy, but Transformer outperforms LSTM in F1-score. In summary, word embedding has a greater impact on performance than the question encoder.\n\n\\subsection{Benchmark on Sports-QA}\n\nWe benchmark several VideoQA methods on our Sports-QA dataset, comparing them to CoMem \\citep{gao2018motion}, HME \\citep{fan2019heterogeneous}, HGA \\citep{jiang2020reasoning}, MASN \\citep{seo2021attend}, HQGA \\citep{xiao2022video}, and IGV \\citep{li2022invariant}, with HQGA \\citep{xiao2022video} and IGV \\citep{li2022invariant} representing the state of the art. It's worth noting that we only evaluate methods without large-scale video-text pretraining, excluding pretrained models like MERLOT \\citep{zellers2021merlot}, VIOLET \\citep{fu2021violet}, and All-in-one \\citep{wang2022all}. The exclusion is based on the following reasons: 1) pretrained models require large-scale additional data, making comparisons unfair for methods without pretraining; 2) we aim to highlight the characteristics of our dataset, and the universal knowledge gained by pretrained models could introduce bias in understanding sports scenarios. Table \\ref{fea} shows the types of visual features used by each model, along with a summary of each method's insights. For CoMem, HME, and HGA, we use re-implementations provided by \\citep{xiao2021next}, and for MASN, HQGA, and IGV, we utilize the official implementations by their respective authors. \\textbf{The baseline model has the same architecture as shown in Fig. \\ref{aft}, with AFT replaced with a standard Transformer Encoder.}\n\nIn Table \\ref{res1}, the VideoQA methods demonstrate substantial improvements compared to BlindQA. Notably, the most significant improvements are observed for basketball (approximately 40\\% increase in accuracy), indicating the critical role of visual information in basketball for question answering. Conversely, the improvements for football are less pronounced (about 4\\%), suggesting that BlindQA already achieves considerable accuracy in this sport. The results suggest high correlations between questions and answers in football, posing challenges for VideoQA models to fully exploit visual information, potentially due to the complexity introduced by a larger number of players.\n\nExamining different question types, the greatest improvements are observed for descriptive questions (around 27\\% increase in accuracy). Additionally, VideoQA performance on causality and counterfactual questions shows significant improvements compared to BlindQA. However, results for temporal questions are similar between VideoQA methods and BlindQA (except for our AFT), suggesting that VideoQA models may struggle with modeling fine-grained temporal dependencies among actions in sports videos.\n\nAmong the different VideoQA models, our AFT achieves the best performance in both accuracy and F1-score. The significant improvements compared to the baseline indicate the effectiveness of the proposed attention mechanism in distinguishing semantically similar classes.\n\n\\subsection{Further Analysis}\n\n\\noindent\\textbf{Generalization Ability across Sports}. In this study, our primary objective is to assess the generalization capabilities across different sports domains. Our approach involves an initial pretraining phase on a set of four sports, followed by fine-tuning on a fifth, distinct sport. We meticulously evaluate the performance of this pretraining-fine-tuning methodology in comparison to training a model entirely from scratch.\nThe outcomes, detailed in Table \\ref{resf}, showcase a notable enhancement in accuracy when employing the pretraining strategy on diverse sports. These findings suggest that the model acquires a nuanced understanding of common semantic features during the initial pretraining. Moreover, the model demonstrates its ability to effectively generalize this acquired knowledge to previously unseen sports during the subsequent fine-tuning phase.\n\n\\begin{table}[tbp]\n\\caption{The table presents comparisons (accuracy \\%) between models trained from scratch for individual sports and those fine-tuned using data from other sports, along with the corresponding improvements (Imp.).} \n\\label{resf}\n\n\\begin{tabular}{cccccc}\n\\toprule\nPretraining& Basketball                       & Football                         & Volleyball                       & Gym                              & FineG. \\\\ \\midrule\n\\xmark    & 57.1& 55.0& 56.4& 70.2&        48.5\\\\\n\\cmark    & 58.3& 56.5& 57.7& 71.7&        50.3\\\\ \\midrule\n Imp.& {\\color[HTML]{036400}+1.2}& {\\color[HTML]{036400}+1.5}& {\\color[HTML]{036400}+1.3}& {\\color[HTML]{036400}+1.5}&{\\color[HTML]{036400}+1.8}\\\\\\botrule\n\\end{tabular}\n\\end{table}\n\n\\begin{table}[t]\n\\caption{The accuracy (\\%) of various focus lengths and their combinations. Note that the combination of the same focal length implies the use of only one focus length.} \n\\label{fs}\n\n\\begin{tabular}{c|ccc|ccc|c}\n\\toprule\n   & 1 & 3 & 5 & 7 & 9 & 11 & 80   \\\\ \\midrule\n1  & \\cellcolor[HTML]{C0C0C0}58.2& / & / & 58.9& 59.0& 58.7& 58.5\\\\\n3  & / & \\cellcolor[HTML]{C0C0C0}58.7& / & 59.0& \\textbf{59.1}& 58.6& 58.9\\\\\n5  & / & / & \\cellcolor[HTML]{C0C0C0}58.6& 58.8& 58.9& 58.7& 58.8\\\\ \\midrule\n7  & / & / & / & \\cellcolor[HTML]{C0C0C0}58.7& / & /  & 58.9\\\\\n9  & / & / & / & / & \\cellcolor[HTML]{C0C0C0}58.8& /  & 59.0\\\\\n11 & / & / & / & / & / & \\cellcolor[HTML]{C0C0C0}58.4& 58.6\\\\ \\midrule\n80 & / & / & / & / & / & /  & \\cellcolor[HTML]{C0C0C0}58.0 \\\\ \\botrule\n\\end{tabular}\n\\end{table}\n\n\\noindent\\textbf{Impact of Focus Length.} In this experiment, we investigate the impact of different focus lengths and their combinations. For short-term dependency, we consider lengths of 1, 3, and 5. For the mid-term, we explore lengths of 7, 9, and 11. Regarding long-term (global) dependency, we set the focus length to the video length, which is 80 seconds. The results are presented in Table \\ref{fs}.\nFrom the results, it is evident that the model with global attention performs less effectively compared to those with local attention. However, upon applying our auto-focus attention mechanism, there is a significant improvement in performance. Notably, the combination of focus lengths 3 and 9 achieves the highest accuracy. In our final model, we incorporate the global focus, resulting in the best accuracy at 59.2\\%.\n\n\\subsection{Visualization}\n\n\\noindent \\textbf{Qualitative Results.} Fig. \\ref{pred} illustrates predictions from various methods. In some instances, BlindQA successfully guesses the correct answer by leveraging the correlation between the question and the answer in the training set. However, this approach is not foolproof, as the blinded model consistently selects the most correlated answer to the question.\nIn comparison, the VideoQA models struggle to discern subtle differences between semantically or visually similar answer classes. For example, actions like ``\\textit{long pass}\" and ``\\textit{cross}\" both involve passing the ball to other players but differ in passing direction and distance, presenting challenges for accurate distinction. Additionally, the compared VideoQA models exhibit limitations in fine-grained motion analysis, such as action counting, as evident in the last example. With its ability to capture various types of temporal dependencies, our AFT outperforms the compared models on these challenging samples.\n\n\\begin{figure}[tbp]\n\\centering\n\\includegraphics[width=\\columnwidth]{pics/vis2.pdf}\n\\caption{\nVisualization of the predictions. The correct/wrong predictions are highlighted in \\textcolor[RGB]{84,130,53}{green}/\\textcolor[RGB]{192,0,0}{red}. The predicted probability of each answer is also reported. \n}\n\\label{pred}\n\\end{figure}\n\n\\begin{figure}[tbp]\n\\centering\n\\includegraphics[width=\\columnwidth]{pics/attn_vis.pdf}\n\\caption{Visualization of focus weight. We show four examples of the highest weight for each focus.}\n\\label{attn_vis}\n\\end{figure}\n\n\\noindent\\textbf{Focus Weight.}\nFig. \\ref{attn_vis} depicts the focus weights of selected questions. Notably, questions seeking information about specific actions (e.g., ``\\textit{win a point}\") heavily rely on short-term information (focus length 3). In contrast, questions like ``\\textit{How many times does the right team do `block'?}\" necessitate a stronger emphasis on long-term information. Questions pertaining to the relationships between two actions show a preference for mid-term information, offering insights into the cause and effect of actions. Intuitively, the mid-term focus length can provide more nuanced information on the interactions between actions. This visualization affirms the effectiveness of our model, which dynamically integrates different focus scales based on the diverse temporal dependencies required by various questions.\n\n\\section{Conclusion}\n\nWe introduce Sports-QA, a novel dataset emphasizing professional action understanding, interactions among multiple players, and fine-grained motion analysis for sports VideoQA. Featuring multiple sports and diverse question types, the dataset offers a comprehensive evaluation platform for VideoQA methods. Additionally, we propose Auto-Focus Transformer (AFT), designed to capture various temporal dependencies. Our extensive experiments on the dataset demonstrate the effectiveness of AFT.\n\n\\section{Limitation and Future Work}\n\\label{limit}\n\nIn our study, we introduce the innovative Auto-Focus Transformer (AFT) designed to capture intricate dependencies across frames within diverse temporal windows, catering to a range of question types. Unlike the conventional Transformer model, the AFT demands increased computational resources, scaling linearly with the number of focuses. Although we have optimized the implementation for high parallelization, it may pose challenges on devices with constrained memory capacities. Looking ahead, our research will focus on devising a novel architectural framework that retains the distinctive features of AFT while ensuring computational efficiency, addressing the limitations associated with memory constraints.\n\n\\section{Ethical Consideration}\n\\label{ethical}\n\nIn developing our Sports-QA dataset, sourced from MultiSports and FineGym, we prioritized ethical considerations, adhering to data privacy and usage standards while ensuring compliance with the terms of the original datasets. To address potential biases, we actively promoted diversity in sports categories and specific sports actions, aiming for a balanced representation.\nMindful of the environmental impact, we optimized our data processing to minimize resource consumption. Acknowledging the potential for misuse, we emphasize the dataset's intended use in positive sports analytics applications, such as performance analysis. Our commitment to transparency and reproducibility is evident in our methodology and dataset accessibility, ensuring that our contribution to sports analytics and computer vision is both responsible and beneficial.\n\n\\section*{List of Crucial Actions}\n\nThe selected crucial actions are listed in Table \\ref{acts_}.\n\n\\begin{table}[tbp]\n\\caption{Crucial actions selected for Sports-QA, which are classified into offensive actions and defensive actions.} \n\\label{acts_}\n\n\\begin{tabular}{@{}lll@{}}\n\\toprule\n\\multirow{2}{*}{Sports}     & \\multicolumn{2}{c}{Action}                                \\\\ \\cmidrule(l){2-3} \n                            & Offensive                    & Defensive                  \\\\ \\midrule\n\\multirow{5}{*}{Volleyball} & \\textit{serve}               & \\textit{first pass}        \\\\\n                            & \\textit{dink}                & \\textit{defend}            \\\\\n                            & \\textit{spike}               & \\textit{protect}           \\\\\n                            & \\textit{second attack}       & \\textit{adjust}            \\\\\n                            & \\textit{no offensive attack} & \\textit{save}              \\\\ \\midrule\n\\multirow{5}{*}{Football}   & \\textit{shoot}               & \\textit{diving}            \\\\\n                            & \\textit{cross}               & \\textit{tackle}            \\\\\n                            & \\textit{through pass}        & \\textit{steal}             \\\\\n                            & \\textit{long pass}           & \\textit{block}             \\\\\n                            & \\textit{aerial duels}        & \\textit{}                  \\\\ \\midrule\n\\multirow{5}{*}{Basketball} & \\textit{jump ball}           & \\textit{save}              \\\\\n                            & \\textit{free throw}          & \\textit{pass steal}        \\\\\n                            & \\textit{2-point shot}        & \\textit{dribble steal}     \\\\\n                            & \\textit{3-point shot}        & \\textit{defensive rebound} \\\\\n                            & \\textit{offensive rebound}   & \\textit{}                  \\\\ \\botrule\n\\end{tabular}\n\\end{table}\n\n\\section*{Question Templates for Sports-QA}\n\n\\subsection*{\\centering Gymnastics}\n\n\\subsubsection*{Descriptive Question}\n\\begin{itemize}\n    \\item \\textit{What is the video about?}\n \\item \\textit{How many SOME-ACTION do the players perform?}\n \\item \\textit{How many types of SOME-ACTION do the players perform?}\n \\item \\textit{How many times do the players perform SOME-ACTION?}\n \\item \\textit{How many players perform the i-th SOME-ACTION?}\n \\item \\textit{Do the players perform SOME-ACTION?}\n\\end{itemize}\n\n\\subsubsection*{Temporal Question}\n\\begin{itemize}\n \\item \\textit{Do the players perform SOME-ACTION before/after the i-th SOME-ACTION?}\n \\item \\textit{What do the players perform before/while/after performing the i-th SOME-ACTION?}\n \\item \\textit{How many times do the player do SOME-ACTION before/after doing the i-th SOME-ACTION?}\n\\end{itemize}\n\n\\subsection*{\\centering Ball Games}\n\\subsubsection*{Descriptive Question}\n\n\\begin{itemize}\n    \\item \\textit{What is the video about?}\n\\item \\textit{Does one team score in the video?}\n\\item \\textit{Which team scores in the video?}\n\\item \\textit{How many times does SOME-TEAM do SOME-ACTION?}\n\\item \\textit{How many times does SOME-TEAM do SOME-ACTION successfully?}\n\\item \\textit{Does SOME-TEAM do their i-th SOME-ACTION successfully?}\n\\end{itemize}\n\n\\subsubsection*{Temporal Question}\n\n\\begin{itemize}\n    \\item \\textit{What does SOME-TEAM do before/after their i-th SOME-ACTION?}\n    \\item \\textit{What does SOME-TEAM do before/after the other team does their i-th SOME-ACTION?}\n\\end{itemize}\n\n\\subsubsection*{Causal Question}\n\n\\begin{itemize}\n    \\item \\textit{Why does SOME-TEAM do the i-th SOME-ACTION?}\n    \\item \\textit{What is the effect of the i-th SOME-ACTION of SOME-TEAM?}\n    \\item \\textit{How does SOME-TEAM succeed in doing the i-th something?}\n\\end{itemize}\n\n\\subsubsection*{Counterfactual Question}\n\n\\begin{itemize}\n    \\item \\textit{Would SOME-TEAM succeed in doing the i-th SOME-ACTION if the other team did not do SOME-ACTION?}\n\\end{itemize}\n\n\\section*{Declarations}\n\n\\subsection*{Data and Code Availability}\nThe proposed dataset, Sports-QA, and the code of the proposed method are available at \\url{https://github.com/HopLee6/Sports-QA}.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{X-VARS: Introducing Explainability in Football Refereeing\\\\with Multi-Modal Large Language Models}\n\n\\begin{document}\n\n\\title{X-VARS: Introducing Explainability in Football Refereeing\\\\with Multi-Modal Large Language Models}\n\n\\author{Jan Held$^{1}$%\\\\ {\\small UniversitÃ© Libre de Bruxelles} \\and\n\\quad\nHani Itani$^{2}$  %\\\\ {\\small KAUST}  \\and \n\\quad\nAnthony Cioppa$^{1}$  %\\\\ {\\small University of LiÃ¨ge, KAUST}  \\and\n\\quad\nSilvio Giancola$^{2}$  %\\\\ {\\small KAUST}  \\and \n\\\\\nBernard Ghanem$^{2}$  %\\\\ {\\small KAUST}  \\and\n\\quad\nMarc Van Droogenbroeck$^{1}$  %\\\\ {\\small University of LiÃ¨ge}\n\\\\ $^1$ {\\small University of LiÃ¨ge}\n\\quad $^2$ {\\small KAUST}\n}\n\\maketitle\n\n\\begin{abstract}\nThe rapid advancement of artificial intelligence has led to significant improvements in automated decision-making. However, the increased performance of models often comes at the cost of explainability and transparency of their decision-making processes. \nIn this paper, we investigate the capabilities of large language models to explain decisions, using football refereeing as a testing ground, given its decision complexity and subjectivity.\nWe introduce the E\\textbf{\\underline{X}}plainable \\textbf{\\underline{V}}ideo \\textbf{\\underline{A}}ssistant \\textbf{\\underline{R}}eferee \\textbf{\\underline{S}}ystem, X-VARS, a multi-modal large language model designed for understanding football videos from the point of view of a referee. \nX-VARS can perform a multitude of tasks, including video description, question answering, action recognition, and conducting meaningful conversations based on video content and in accordance with the Laws of the Game for football referees.\nWe validate X-VARS on our novel dataset, \\emph{SoccerNet-XFoul}, which consists of more than $22$k video-question-answer triplets annotated by over $70$ experienced football referees. \nOur experiments and human study illustrate the impressive capabilities of X-VARS in interpreting complex football clips. \nFurthermore, we highlight the potential of X-VARS to reach human performance and support football referees in the future.\nWe will provide code, model, dataset, and demo upon publication.\n\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{pictures/dataset_example.pdf}\n    \\caption{\\textbf{SoccerNet-XFoul dataset.} \nExamples of annotations from two different referees for the same foul.\n   The second example illustrates the complexity and subjectivity of refereeing decisions. %Two referees provide different interpretations of the same foul.\n    }\n    \\label{fig:dataset_example}\n\\end{figure}\n\nRecently, the field of Artificial Intelligence (AI) has witnessed remarkable progress in the development of large language models~\\cite{Touvron2023LLaMA-arxiv, Brown2020Language-arxiv, Devlin2018BERT-arxiv, Hoffmann2022Training-arxiv, Chowdhery2022PaLM-arxiv}.\nThese models have acquired a strong language understanding, enabling them to tackle a broad range of linguistic tasks, ranging from text generation and conversation to zero-shot question answering.\nThe development has further progressed with multi-modal language models, going beyond the constraints of text-based inputs, but also including images, videos, and audio~\\cite{Zhu2023MiniGPT4-arxiv, Maaz2023VideoChatGPT-arxiv, Ye2023mPLUGOwl2-arxiv, Alayrac2022Flamingo-arxiv}.\nHowever, as the capabilities of these models continue to advance, the increase in model performance often comes at the cost of reduced explainability and transparency. \nThis trend poses several challenges for users and developers who seek to understand the \\textit{why} and \\textit{how} behind a model's decision-making process. \nExplaining the reasoning process of AI models is particularly crucial in domains requiring high levels of trust, such as healthcare, autonomous driving, or sports.\nIn the context of football, referee decisions can significantly impact the financial future and existence of clubs, making it essential for AI models to transparently explain their decision-making process. Such transparency is key to building trust and facilitating the acceptance and integration of AI in sports.\nFor instance, during the Qatar World Cup 2022, FIFA employed an AI system for semi-automated offside detection~\\cite{FIFA2023SemiAutomatedOffsideTechnology}.\nTo enhance the system's transparency and explainability, it generates a 3D representation of the game to allow referees and spectators to visually verify offside positions with undeniable clarity, bridging the gap between AI decision-making and human understanding.\n\nIn this paper, we explore the use of large language models to enhance transparency and explainability of automated referee decision-making. % through textual explanations. \nFootball refereeing offers an ideal environment, as many of the decisions that referees make are subjective and reliant on each individual's interpretation of the rules of the game. \nParticularly, we introduce X-VARS, the first multi-modal large language model designed to explain football refereeing decisions.\nX-VARS is based on a Vision Language Model (VLM)~\\cite{Maaz2023VideoChatGPT-arxiv} that adapts the design of the LLaVa~\\cite{Liu2023Visual-arxiv} model for spatio-temporal video modeling. \nWe train X-VARS using a new training paradigm where we input the visual features and the multi-task predictions of our fine-tuned visual encoder CLIP ViT-L/14~\\cite{Radford2021Learning} to the language model. \nWe validate X-VARS on our novel \\textit{SoccerNet-XFoul} dataset containing more than $22$k video-question-answer triplets about the most fundamental refereeing questions. More than $70$ professional referees annotated our dataset and provided, for each question, detailed explanations about their decisions.\nX-VARS achieves state-of-the-art performance on the \\textit{SoccerNet-MVFoul} dataset, and our human study demonstrates that X-VARS generates explanations for its decisions at a level comparable to human referees. \nFinally, X-VARS can analyze and understand complex football duels and provide accurate decision explanations, opening doors for future applications to support referees in their decision-making processes.\n\n\\mysection{Contributions.} We summarize our contributions as follows:\n\\textbf{(i)} We publicly release \\textit{SoccerNet-XFoul}, a new multi-modal dataset containing more than $22$k video-question-answer triplets about refereeing questions. \n\\textbf{(ii)} We introduce \\textit{X-VARS}, a new vision language model that can perform multiple multi-modal tasks such as visual captioning, question-answering, video action recognition, and can generate explanations of its decisions on-par with human level.\n\\textbf{(iii)} We perform a thorough evaluation of our model, including analyses of our new training paradigm, the influence of the CLIP text predictions, and a human study that compares X-VARS to human referees.\n\n\\section{Related work}\n\\label{sec:related_work}\n\n\\mysection{Sports video understanding.}\nThe field of sports video understanding has gained a lot of interest lately~\\cite{Thomas2017Computer}. It encompasses a wide range of tasks such as player segmentation, detection, and tracking~\\cite{Cioppa2019ARTHuS, Maglo2022Efficient, Vandeghen2022SemiSupervised, Mansourian2023Multitask, Seweryn2024Improving-arxiv}, keypoint detection~\\cite{Ludwig2023AllKeypoints}, summarizing~\\cite{Gautam_2022, Midoglu_2024}, player re-identification~\\cite{Somers2023Body, Mansourian2023Multitask}, action spotting in untrimmed videos~\\cite{Cioppa2020AContextaware, Cioppa2021Camera,Hong2022Spotting,Soares2022Temporally, Soares2022Action-arxiv, Giancola2023Towards, Seweryn2023Survey-arxiv, Denize2023COMEDIAN-arxiv}, pass prediction and feasibility~\\cite{ArbuesSanguesa2020Using, Honda2022Pass}, foul recognition~\\cite{Held2023VARS, Fang2024Foul-arxiv} or dense video captioning for football broadcasts commentaries~\\cite{Mkhallati2023SoccerNetCaption}. \nSuch tasks can be formulated and solved by leveraging recent advances in deep learning for general video understanding. However, progress in sports video understanding heavily relies on the availability of sports-centric large-scale annotated datasets~\\cite{Pappalardo2019Apublic,Yu2018Comprehensive,Scott2022SoccerTrack,Jiang2020SoccerDB,VanZandycke2022DeepSportradarv1, Istasse2023DeepSportradarv2, Midoglu2022MMSys}.\nSoccerNet~\\cite{Giancola2018SoccerNet,\nDeliege2021SoccerNetv2,\nCioppa2022Scaling, \nCioppa2022SoccerNetTracking, \nGiancola2022SoccerNet, \nCioppa2023SoccerNetChallenge-arxiv} \nstands among the largest and most comprehensive dataset for video understanding in soccer. \\emph{SoccerNet-MVFoul}~\\cite{Held2023VARS} further extended \\textit{SoccerNet} by proposing a novel multi-view football dataset designed for foul classification annotated by professional referees.\nIn this work, we further extend \\emph{SoccerNet-MVFoul} into a visual-question-answering dataset focused on football refereeing questions, named \\emph{SoccerNet-XFoul}.\n\n\\mysection{Vision language models.}\nNatural Language Processing (NLP) has witnessed remarkable advancements with the emergence of open-source Large Language Models (LLMs)~\\cite{Touvron2023LLaMA-arxiv, Brown2020Language-arxiv, Devlin2018BERT-arxiv, Hoffmann2022Training-arxiv, Chowdhery2022PaLM-arxiv, Ouyang2022Training-arxiv, Workshop2022BLOOM-arxiv}. These models have demonstrated exceptional capabilities in language understanding and generation tasks. LLMs have also served as the basis for the success of many vision-language models that are based on projecting the visual features of an image~\\cite{Zhu2023MiniGPT4-arxiv, Chen2023ShareGPT4V-arxiv, Huang2023Language-arxiv, Xu2022BridgeTower-arxiv, Liu2023Visual-arxiv} or a video~\\cite{Ye2023mPLUGOwl2-arxiv, Alayrac2022Flamingo-arxiv} encoder onto the input embedding space of an LLM. This idea and its variants allowed leveraging the power of LLMs for multi-modal understanding. \nIn this work, we propose X-VARS, a vision language model for visual captioning, question-answering, action video recognition, and conducting meaningful conversations based on video content.\n\n\\mysection{Explainability.} Recently, explainability in machine learning has received lots of attention, leading to the development of various techniques to demystify complex models. LIME (Local Interpretable Model-agnostic Explanations)\\cite{Ribeiro2016Why-arxiv} explains the predictions of any machine learning classifier by approximating any classifier locally with an interpretable model. \nMeanwhile, SHAP (SHapley Additive exPlanations)~\\cite{Lundberg2017AUnified-arxiv} offers a unified perspective on feature importance by averaging all feature combinations, ensuring consistent attributions.\nGrad-CAM (Gradient-weighted Class Activation Mapping)~\\cite{Selvaraju2016GradCAM-arxiv} employs gradient information from the final convolutional layer to generate a heat map highlighting crucial input image regions.\nCounterfactual Explanations~\\cite{Wachter2017Counterfactual-arxiv} identify the least number of changes required in the input data to alter the model's prediction, offering insights into decision boundaries and feature importance. \nLastly, Explanation via Language~\\cite{Lakkaraju2022Rethinking-arxiv} emphasizes natural language dialogues for enhanced interaction between experts and models, underscoring the importance of interactive systems tailored to user requirements. \nIn this paper, we investigate how large language models can explain decisions, using football refereeing as a testing environment given its decision complexity and subjectivity.\n\n\\begin{table*}[t]\n    \\centering\n    \\resizebox{\\linewidth}{!}{% <------ Don't forget this %\n    \\begin{tabular}{l|cccc|ccc}\n    \\toprule\n\\textbf{Dataset} & \\textbf{Type} & \\textbf{\\#Instances} & \\textbf{\\#Questions}\n& \\textbf{\\#Context} &\\textbf{VQA} &\\textbf{Captioning} &\\textbf{AR} \\\\\\midrule \\rowcolor[HTML]{EFEFEF}\n\nConceptual 12M~\\cite{Changpinyo2021Conceptual-arxiv} & Images & $12M$ & $12M$ & Various & - & \\checkmark  & - \\\\\n\nLAION-5B~\\cite{Schuhmann2022LAION5B-arxiv} & Images & $3B$ & $3B$ & Various  & - & \\checkmark  & - \\\\\\rowcolor[HTML]{EFEFEF}\n\nLLava dataset \\cite{Liu2023Visual-arxiv}  & Images & $158k$ &$158k$  & Various   & \\checkmark & \\checkmark   & - \\\\\n\nMovieQA \\cite{Tapaswi2015MovieQA-arxiv} & Videos & $408$ & $15k$ & Movies  & \\checkmark & - & -  \\\\\\rowcolor[HTML]{EFEFEF}\n\nTVQA \\cite{Lei2018TVQA-arxiv} & Videos & $21k$ &  $150k$  & TV shows & \\checkmark & -  & - \\\\\n\nVideo Instruction Dataset~\\cite{Maaz2023VideoChatGPT-arxiv} &Videos & $100k$ & $100k$ & Various    & \\checkmark & \\checkmark & - \\\\\\rowcolor[HTML]{EFEFEF}\n\nHowTo100M \\cite{Miech2019HowTo100M-arxiv}  & Videos & $136M$ &$136M$ & Youtube  & -  & \\checkmark   & -\\\\\\midrule\n\nGOAL \\cite{Suglia2022Going-arxiv}  &Videos &$1k$  &$53k$ & Football  & - & \\checkmark  & - \\\\\\rowcolor[HTML]{EFEFEF}\n\nSports-QA \\cite{Li2024SportsQA-arxiv}  &Videos &$6k$ &$94k$ &Football  & \\checkmark  & -  & \\checkmark \\\\\n\nSoccerNet-caption~\\cite{Mkhallati2023SoccerNetCaption}  &  Videos & $942$ &  - & Football games  & - & \\checkmark & - \\\\  \\midrule\n\n\\textbf{SoccerNet-XFoul (Ours)} & Football &$10k$ & $22k$ & Football fouls    & \\checkmark & \\checkmark & \\checkmark \\\\ \n\\bottomrule\n    \\end{tabular}\n    }\n    \\caption{ \\textbf{Comparative overview of relevant datasets.} \\textit{SoccerNet-XFoul} contains high-quality answers annotated by more than 70 experienced referees. Our dataset is the largest dataset in sports with complex questions and the only one focusing on refereeing questions. \n    VQA stands for Visual Question Answering, AR stands for Action Recognition.\n    }\n    \\label{tab:MOT_Comparison}\n\\end{table*}\n\n\\section{SoccerNet-XFoul dataset}\n\\label{sec:dataset}\n\nThe performance of supervised models mostly relies on the quality and quantity of available annotated datasets. Multi-modal datasets are generally harder to curate and annotate, which explains their usually smaller size compared to uncurated datasets.\nTable~\\ref{tab:MOT_Comparison} shows a comparative overview of multi-modal datasets in the literature, specifically highlighting those that contain combinations of text and image, as well as text and video pairs.\nWe introduce \\emph{SoccerNet-XFoul}, a dataset specifically designed for foul video recognition and explanation. \nIt consists of high-quality video-text pairs with more than $10$k video clips and $22$k questions, annotated by more than $70$ experienced referees. \nCompared to the other sports datasets, \\textit{SoccerNet-XFoul} has the most video clips and much more complex questions. \nIn the following, we present our approach to creating this high-quality human-annotated dataset.\n\n\\mysection{Questions.}\nWe identify $4$ key questions on the most foundational, complex, and game-impacting decisions a referee must confront during a game.\nTo answer the two first questions,\n\\textit{``Is it a foul or not? Why?''} and \\textit{``What card would you give? Why?''}, the model requires an in-depth understanding of the rules of the game~\\cite{IFAB2022Laws} as well as an understanding of the context in which an action occurred. Factors such as the intent, the foul location, the game dynamic and the intensity of the contact must all be considered.\nThe two last questions, if \\textit{``the defender stops a promising attack or a goal-scoring opportunity?''} and if \\textit{``the referee could have given an advantage?''} add a new layer of difficulty and prediction analysis. The answers to the questions are not only visual, since the model has to make predictions about potential future outcomes. \nFor instance, in assessing whether the referee should have given an advantage, the model needs to evaluate whether the attacking team would benefit more from continuing play rather than being granted a free-kick.\n\n\\mysection{Annotators.} \nAs no public dataset is available that provides detailed answers and explanations to these refereeing questions, we conducted an annotation campaign with over $70$ referees over a three-month period.\nTo ensure high-quality answers, only experienced referees were selected for the annotations. \nThe referees have officiated between $140$ and $2{,}279$ official games, with an average of $655$ games.\nThey were allowed to assess as many video clips as they wished, with the flexibility to pause at any time to avoid fatigue.\nEach annotator had the option to provide answers in German, French, English, or Spanish to prevent any linguistic barriers.\nThe answers were translated from the original language to English by ChatGPT-3.5~\\cite{Brown2020Language-arxiv} and then reviewed by another human referee to ensure accurate translation.\n\n\\mysection{Subjectivity.}\nFigure~\\ref{fig:dataset_example} displays an example of the subjectivity of the annotations. While one referee annotator would not give a card because he thought the foul was made with low intensity, the other annotator would give a yellow card because he believed the tackling was made with high speed and was reckless.\nDue to this inherent subjectivity in refereeing, our objective was to gather multiple answers for the same action, rather than collecting a single decision and explanation for each question.\nTherefore, the multiple decisions and explanations actually help the model to learn a range of valid interpretations and reasoning strategies employed by human referees. All in all, this can improve the robustness of the AI model, enabling it to make informed decisions even in ambiguous or subjective situations.\nPractically, the annotators were randomly assigned different video clips, ensuring that the same action might be evaluated multiple times. In the end, for each action, we have, on average, $1.5$ answers for the same question.\n\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=1\\linewidth]{pictures/Distribution.pdf}\n\\caption{\n\\textbf{Distribution of the most common words.} \nThe most frequent words are ``foul\" and ``defender,\" followed by semantically related words related to football and referee actions and terms. There is thus a significant imbalance in the distribution.\n}\n\\label{fig:distribution}\n\\end{figure}\n\\mysection{Statistics.} \nOur dataset contains $10$k video clips with over $22$k referee-generated questions and answers.\nFigure~\\ref{fig:distribution}, shows the distribution of the most common words in the explanations of the referee annotators. \nThe most frequently used words are specific terminologies for describing a duel between two players, ranging from descriptive terms such as \\textit{defender} or \\textit{card} to key terms to consider for evaluating fouls such as \\textit{intention} or \\textit{intensity}.\nThe number of words per answer ranges from $1$ to $66$, with a total of more than $540$k words and an average of almost $25$ words per answer, with a significant imbalance in the distribution of words.\n\n\\mysection{Novelty.} \nCompared to traditional visual-question answering datasets, the \\textit{SoccerNet-XFoul} dataset is the first to answer refereeing questions, with detailed explanations of why a particular decision is correct. \nThis explainability is a unique approach that enhances the datasetâs complexity and ensures a deeper understanding and representation of real-world scenarios where AI models must make and explain their decisions.\nFurthermore, the interpretation of situations in our dataset is context-dependent. \nThe interpretation of a foul might differ depending on whether it occurs in the middle of the field or in the penalty area. To correctly answer questions, the model must have an in-depth understanding of the game.\nFinally, the model needs a level of predictive analysis. For instance, to determine if a defender stopped a promising attack, the model must understand what happened at the moment of the foul and what could have happened in the immediate future. This involves making complex predictions about potential future outcomes, a task that is far more advanced compared to traditional VQA datasets.\nHence, our \\textit{SoccerNet-XFoul} dataset is the first and largest visual question-answering dataset for referees in football, offering many new challenges to be explored.\n\n\\begin{figure*}[t]\n  \\centering\n  \\includegraphics[width=0.8\\linewidth]{pictures/architecture.pdf}\n\\caption{\n\\textbf{Architecture of X-VARS.}  \nX-VARS is a visual language model based on a fine-tuned CLIP visual encoder to extract spatio-temporal video features and to obtain multi-task predictions regarding the type and severity of fouls. The linear layer connects the vision encoder to the language model by projection the video features in the text embedding dimension. We input the projected spatio-temporal features alongside the text predictions obtained by the two classification heads $\\mathbf{C_{foul}}$ and $\\mathbf{C_{\\severity}}$ (for the task of determining the type of foul and the task of determining if it is a foul and the corresponding severity) into the Vicuna-v1.1 model, initialized with weights from LLaVA.\n}\n\\label{fig:architecture}\n\\end{figure*}\n\\section{Methodology}\n\\label{sec:method}\nIn this section, we provide a comprehensive description of our novel E\\textbf{\\underline{X}}plainable \\textbf{\\underline{V}}ideo \\textbf{\\underline{A}}ssistant \\textbf{\\underline{R}}eferee \\textbf{\\underline{S}}ystem, ``X-VARS'', for foul and severity recognition, providing explanations on its decision-making process. We begin by presenting the architecture with a detailed description of each individual component of X-VARS. Then, we provide an in-depth explanation of its training process.\n\n\\subsection{Architecture}\n\nFigure~\\ref{fig:architecture} illustrates the key architectural components of X-VARS. We use Video-ChatGPT~\\cite{Maaz2023VideoChatGPT-arxiv}, a multi-modal model capable of understanding and generating detailed conversations about videos, as our foundation model. We make several changes to the architecture to adapt it to our needs.\nFormally, we input a video clip $\\mathbf{v} \\in \\mathbb{R}^{T\\times H \\times W \\times C}$, with $T$, $H$, $W$ and $C$ being respectively the number of frames, height, width, and channel dimension of the video, to CLIP ViT-L/14~\\cite{Radford2021Learning},\n\\begin{equation}\n    \\boldsymbol{f_i} , \\boldsymbol{h_i} = CLIP(v) \\comma\n\\end{equation}\nand obtain the corresponding frame feature vector $\\boldsymbol{f_i}$ $\\in \\mathbb{R}^{T\\times D_1}$ and the hidden states $\\boldsymbol{h_i}$ $\\in \\mathbb{R}^{T \\times S \\times D_2}$, with $S$ being the number of tokens obtained by multiplying $w = W/p$ and $h = H/p$, where $p$ is the patch size of CLIP, $D_1$ the dimension of the output layer and $D_2$ the dimension of the hidden states.\nWe then average-pool the hidden states across the temporal dimension to obtain temporal features $\\textbf{t}\\in \\mathbb{R}^{S\\times D_2}$ and along the spatial dimension to get the video-level spatial representation $\\textbf{s}\\in \\mathbb{R}^{T\\times D_2}$.\nFinally, we concatenate both to obtain spatio-temporal features.\n\\begin{equation}\n    \\boldsymbol{z} = [\\boldsymbol{t} \\quad \\boldsymbol{s}] \\in \\mathbb{R}^{(S + T) \\times D_2} \\dotmath\n\\end{equation}\nBefore feeding the video features $\\boldsymbol{z}$ into the LLM, we project them into the same feature space as the text embeddings by applying a linear projection layer: \n\\begin{equation}\n    \\boldsymbol{w} = Linear(\\boldsymbol{z}) \\in \\mathbb{R}^{(S + T) \\times D_2}  \\dotmath\n\\end{equation}\nwith $\\boldsymbol{w}$ being a sequence of visual tokens.\nThe feature vectors $\\boldsymbol{f_i}$ are also average-pooled along the temporal dimension to obtain a single video-level representation $\\boldsymbol{f}$ $\\in \\mathbb{R}^{D_1}$.\nThe video-level feature representation $\\boldsymbol{f}$ is passed through two classification heads  $\\mathbf{C_{foul}}$ and  $\\mathbf{C_{\\severity}}$ to obtain the type of foul (\\ie Tackling, Holding, Pushing, Standing tackling, Elbowing, Dive, Challenge, or High leg) and to determine whether it is a foul or not, and the corresponding severity (\\ie No offence, Offence + No card, Offence + Yellow card or Offence + Red card), with the predictions being:\n\\begin{equation}\n    \\mathbf{P_{foul}} = \\arg\\max \\mathbf{C_{foul}}\\comma\n\\end{equation}\n\\begin{equation}\n    \\mathbf{P_{\\severity}} = \\arg\\max \\mathbf{C_{\\severity}}\\dotmath\n\\end{equation}\nThese predicted labels with the highest confidence are injected as a textual prompt to the LLM.\nHence, this multi-task classification enables the model to utilize acquired information to enhance the performance of the explanation. \n\nTo obtain high performance with LLMs, a crucial part consists in determining a prompt which is understandable by the LLM. \nAs we use the Video-ChatGPT backbone, we design our prompt with the following query:\n\\begin{flalign}\n&USER: <Question> <P_{foul}><P_{\\severity}><w>\\\\\\nonumber\n&Assistant:&&\n\\end{flalign}\nwhere $<Question>$ represents one of our questions randomly sampled from the training set of video-question-answer triplets, \n$<P_{foul}>$ and $ <P_{\\severity}>$ are the two predictions on the foul type and severity recognition task obtained from the fine-tuned CLIP, and \n$<w>$ are the projected spatio-temporal features.\nX-VARS is trained to predict the answers of the assistant as an auto-regressive model.\n\n\\subsection{Training Paradigm}\n\n\\begin{table}\n    \\centering\n    \\resizebox{\\linewidth}{!}{% <------ Don't forget this %\n    \\begin{tabular}{l|lccc}\n          &  CLIP &$\\mathbf{C_{foul}}$ \\&  $\\mathbf{C_{\\severity}}$ & Linear Layer & LLM  \\\\ \\midrule\nStage 1 & \\cincludegraphics[height=.2in]{pictures/fire_real.png} &\\cincludegraphics[height=.2in]{pictures/fire_real.png} & - & -\\\\\nStage 2 & \\cincludegraphics[height=.2in]{pictures/ice_real.png} & \\cincludegraphics[height=.2in]{pictures/ice_real.png} & \\cincludegraphics[height=.21in]{pictures/fire_real.png} & \\cincludegraphics[height=.21in]{pictures/fier_ice.png}\\\\\n    \\end{tabular}\n    }\n\\caption{ \\textbf{Overview of the training stages.} In stage 1, we fine-tune CLIP and the classification heads $\\mathbf{C_{foul}}$ and $\\mathbf{C_{\\severity}}$ to give X-VARS some prior knowledge about refereeing. \nIn stage 2, we keep them frozen and fine-tune the linear layer and partially the LLM.\n}\n    \\label{tab:fine_tune_stages}\n\\end{table}\nWe propose a two-stage training approach. The first stage fine-tunes CLIP on a multi-task classification to learn prior knowledge about football and refereeing. The second step consists in fine-tuning the projection layer and several layers of the LLM to enhance the model's generation abilities in the sport-specific domain.\n\n\\mysection{Stage 1: Fine-tuning to inject football knowledge.}\nWhile CLIP is excellent at generalizing across various image tasks, it lacks the ability to recognize fine-grained actions or events. These actions are mostly recognizable by considering the temporal dimension rather than images alone. For instance, assessing the severity of a foul requires considering factors such as the intensity and the speed, which cannot be determined by simply examining images. \nSince CLIP was not trained specifically on football data, the feature representation between two football clips would be very similar, despite the videos depicting different scenarios.\nHence, since all our videos are related to football, the output features will share similarities. This proximity between features actually poses a challenge for the LLM, making it difficult to effectively distinguish between different actions.\nTo avoid these issues, we fine-tune CLIP on the \\textit{SoccerNet-MVFoul} dataset~\\cite{Held2023VARS} to learn prior knowledge about football. \nThe training consists of minimizing the summed cross-entropy loss of both tasks. \nGiven the similar magnitudes of both losses, we sum them without scaling or weighting.\n\n\\mysection{Stage 2: Feature alignment and end-to-end training.}\nWe freeze the fine-tuned CLIPs weights and continue training the linear projection layer and the LLM.\nTraining a projection layer from scratch requires many high-quality video-text pairs and computational resources.\nTo alleviate this, we use the pre-trained weights of the projection layer from Video-ChatGPT~\\cite{Maaz2023VideoChatGPT-arxiv}, which was trained on a dataset of $100$k video-text pairs.\nWe further fine-tune this projection layer to map the spatio-temporal features of our football clips into the same dimensional space as the word embeddings.\nAs demonstrated in~\\cite{Maaz2023VideoChatGPT-arxiv, Liu2023Visual-arxiv, Zhu2023MiniGPT4-arxiv, Lyu2023MacawLLM-arxiv}, a simple linear layer is sufficient for connecting video features with the word embedding.\nDuring training, we replace the predictions of CLIP $<P_{foul}>$ and $<P_{\\severity}>$ by the ground truth $<G_{foul}>$ and $ <G_{\\severity}>$ as CLIPs predictions might be noisy, which could lead to confusions.\nSince determining foul type and severity is subjective, alignment between the ground truth of the \\textit{SoccerNet-MVFoul} dataset and the referee responses from our \\textit{SoccerNet-XFoul} dataset may vary. Consequently, even when giving $<G_{foul}>$ and $<G_{\\severity}>$ during training, the model may not only use this information without using the video tokens to produce the text. % because the ground truth is not always in alignment with the referees answer.\n\n\\begin{figure*}\n    \\centering\n    \\begin{subfigure}[t]{0.49\\textwidth}\n        \\centering\n        \\includegraphics[width=\\linewidth]{pictures/holding_no_card_main.pdf}\n        \\caption{\\textbf{Q:} What card would you give? Why? \\textbf{GT:} No card because the defender briefly held onto the attacker's arm during the fight for the ball, without it being unsportsmanlike.\n        }\n        \\label{fig:qualitative_res_a}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[t]{0.49\\textwidth}\n        \\centering\n        \\includegraphics[width=\\linewidth]{pictures/tackling_yellow_main.pdf}\n        \\caption{\\textbf{Q:} What card would you give? Why? \\textbf{GT:}  No card. Even though the defender had no chance to play the ball, he touched the attacker with low intensity on the foot.}\n        \\label{fig:qualitative_res_b}\n    \\end{subfigure}\n    \\caption{\n\\textbf{Qualitative results.}\nAlthough X-VARS has never been specifically fine-tuned for conversation, it has inherited its conversational capabilities from the pre-trained model. X-VARS demonstrates impressive discussion skills while being aligned with the video content and the Laws of the Game.\n(a) X-VARS is close to the ground truth and is able to accurately answer the user's question.\n(b) This example shows the subjectivity of foul situations. X-VARS interprets the foul as medium intensity, while the human referee interprets it as low intensity with no chance to play the ball.\n}\n    \\label{fig:qualitative}\n\\end{figure*}\n\n\\section{Experiments} \\label{sec:benchmarks}\n\nIn this section, we analyze the performance of X-VARS on the two most important refereeing questions: \\textit{``Is it a foul or not? Why?''} and \\textit{``What card would you give? Why?''}. Given the importance of these questions, we conduct a comprehensive and detailed analysis, providing insights into the improvements in the video recognition performance, a human study to assess the model's explanations, some qualitative results, and a thorough ablation study. \n\n\\subsection{Implementation details}\nWe fine-tune CLIP-L/14 on the \\textit{SoccerNet-XFoul} dataset for $14$ epochs with a learning rate of $5\\times10^{-6}$ on a single Nvidia V100 GPU with a batch size of $64$, using gradient accumulation to \novercome memory limitation.\nThe fine-tuning of the model takes about $9$ hours. We use $16$ frames in $224$p resolution per clip, with $8$ frames before and $8$ frames after the foul. %The quality of the frames is 224p.\nFor the second stage, we employ QLORA~\\cite{Hu2021LoRA-arxiv, Dettmers2023QLoRA-arxiv} to enhance memory efficiency and enable training on a single GPU.\nWe only fine-tune $1$\\% of the layers for $3$ epochs using a learning rate of $2\\times10^{-4}$ and an\noverall batch size of $32$. The training on $2$ A100 $40$GB GPUs takes about $2$ hours.\nTable~\\ref{tab:fine_tune_stages} provides an overview of the state of the various key components during training.\n\n\\begin{table}\n    \\centering\n    \\resizebox{\\linewidth}{!}{% <------ Don't forget this %\n    \\begin{tabular}{l|c|lcccc}\n        \\multicolumn{1}{c|}{} &  \\multicolumn{1}{c|}{} & \n        \\multicolumn{5}{c}{\\bf Distribution} \\\\ \\midrule\n        & \\bf Mean  & \\bf 1 & \\bf 2 & \\bf 3 & \\bf 4 & \\bf 5\\\\ \\midrule\n       Referees  & 4.0     &3\\% &10\\% &8\\% &46\\% &33\\% \\\\ \n       X-VARS  & 3.8  &3\\% &17\\% &4\\% &46\\% &30\\%  \\\\ \n    \\end{tabular}\n    }\n\\caption{ \\textbf{Score and distributions obtained during our human study comparing the quality of referees and X-VARS generated explanations.}\nThe mean scores of X-VARS closely match those of human referees. In 46\\% of the video clips,\nX-VARS achieved higher scores for its explanations than the human referees. \nThe distribution of the results is very similar for human referees and X-VARS. \nA score of 5 is the highest and represents \\emph{strongly agree} while 1 is \\emph{strongly disagree}.\n}\n    \\label{tab:quantitative_distribution}\n\\end{table}\\subsection{Human study on explanation performance}\nEvaluating generative tasks, such as text, image, or video generation, remains a significant challenge due to their subjective nature and the absence of proper evaluation metrics. \nTraditional language metrics are not very informative for our purpose, as two sentences can be linguistically very similar, yet have entirely different meanings. \nTo achieve quantitative results, we conducted a human study with $20$ football referees who evaluated the quality of responses without knowing if they were generated by a human referee or by X-VARS. \nThe referee officiated between $85$ and $850$ official games, with an average of $490$ games.\nEach participant assessed $20$ random video clips, each lasting $5$ seconds, with no time restrictions. \nThey evaluated the quality of the explanation, considering whether the evaluation was consistent with the video and if the decision and explanation aligned with the Laws of the Game~\\cite{IFAB2022Laws}.\nThey rated each explanation on a scale of $1$ to $5$, with $5$ indicating \\textit{strongly agree} and $1$ indicating \\textit{strongly disagree}.\nTable~\\ref{tab:quantitative_distribution} shows the results, with X-VARS performing similarly to the human referees, with only minimal score differences. \nX-VARS's explanations were more convincing in $46\\%$ of the cases than the referee's. \nWhile both show similar results for \\textit{strongly agree} and \\textit{agree}, X-VARS obtains more \\textit{disagree} responses than human referees. \nThe majority of videos where participants disagreed with X-VARS involve types of fouls that are rare in our dataset, \\ie when the defender uses his arms illegally by pushing his opponent or hitting him with the elbow in the face.\nOverall, the human study highlights X-VARS's impressive ability to understand football videos and explain its decisions at a level comparable to human referees.\n\n\\subsection{Qualitative results}\n\nFigure~\\ref{fig:qualitative} showcases two examples of conversations generated by our proposed X-VARS. Particularly, we illustrate its remarkable ability to understand and generate decisions with explanations related to visual content and the Laws of the Game~\\cite{IFAB2022Laws}. \nAlthough X-VARS was only fine-tuned on two questions, Figure~\\ref{fig:qualitative} illustrates that X-VARS can generalize and accurately answer or describe video content without any specific fine-tuning. \nFurthermore, X-VARS was not fine-tuned for conversation, but we inherited these capabilities from the pre-trained conversational model Video-ChatGPT~\\cite{Maaz2023VideoChatGPT-arxiv}, which serves as the foundation for X-VARS. \nHence, throughout our two fine-tuning stages, we retained the conversation capabilities of the foundation model and can generate meaningful conversations with X-VARS about football and refereeing.\nAnother interesting fact is the typical characteristic of LLMs to consistently agree with human users. \nSurprisingly, X-VARS mostly maintains its decision and offers comprehensive explanations for it, even when asked questions such as \\textit{``Should the defender receive a red card?''}, when the specific foul would not require any.\nHowever, similarly to other LLMs, X-VARS has also inherited typical issues such as hallucinations, in which it recognizes actions in the video that are not present. \nFuture work could investigate if more high-quality data or more advanced LLMs would limit this hallucination effect.\n\n\\subsection{Ablation study}\n\n\\begin{table}[t]\n    \\centering\n    \\resizebox{\\linewidth}{!}{% <------ Don't forget this %\n    \\begin{tabular}{lc|cc|lc}\n        \\multicolumn{2}{c|}{} &  \\multicolumn{2}{c|}{\\bf Type of Foul} & \n        \\multicolumn{2}{c}{\\bf Offence Severity} \\\\ \\midrule\n       \\bf Feat. extr. & \\bf Pooling  & \\bf Acc. & \\bf BA. & \\bf  Acc. & \\bf BA.\\\\ \\midrule\n       ResNet~\\cite{Held2023VARS} & Mean     &0.30     & 0.27  &0.34     &0.25 \\\\ \n       ResNet~\\cite{Held2023VARS}  & Max      &0.32  &0.27   & 0.32  &0.24 \\\\\n       R(2+1)D~\\cite{Held2023VARS} & Mean     &0.32     & 0.34  &0.34     &0.30 \\\\ \n       R(2+1)D~\\cite{Held2023VARS}  & Max      &0.34  &0.33   & 0.39  &0.31 \\\\\n       VARS-MViT~\\cite{Held2023VARS}  & Mean     &0.44     & \\textbf{0.40}   &0.38     &0.31 \\\\ \n       VARS-MViT~\\cite{Held2023VARS}  & Max      &0.45  &0.39   & 0.43  &0.34 \\\\\\midrule\n       CLIP-L/14 & Single-view &\\textbf{0.51}  & 0.39   &0.52 &\\textbf{0.35} \\\\\n       X-VARS & Single-view  & /  &/  &\\textbf{0.62} &\\textbf{0.35}\n    \\end{tabular}\n    }\n    \\caption{\\textbf{Multi-task classification.} We compare the multi-task classification accuracy of the fine-tuned CLIP-L/14 and X-VARS (fine-tuned CLIP + LLM) to the performance obtained by Held \\etal ~\\cite{Held2023VARS}. We obtain state-of-the-art performances for three of the four metrics while using a single view instead of multi-views. X-VARS enhances the classification accuracy of offence and severity by 19\\% compared to the previous SOTA.\n    \\textbf{Acc.} stands for Accuracy and \\textbf{BA.} stands for balanced accuracy.\n    }\n    \\label{tab:classification}\n\\end{table}\n\\mysection{Video action recognition performance.}\nTable~\\ref{tab:classification} shows the performance of CLIP-L/14 after the fine-tuning process in foul classification tasks. We compare it to the previous state-of-the-art (SOTA) achieved by Held~\\etal~\\cite{Held2023VARS} by using the same number of frames and video quality to have a fair comparison.\nHeld~\\etal used the MViT~\\cite{li2022mvitv2, fan2021multiscale} video encoder to extract spatio-temporal features. As \\textit{SoccerNet-MVFoul} contains multiple views for each action, they aggregate the features of the different views by mean or max pooling.\nIn this work, we fine-tune CLIP on a single view and evaluate it on the same actions, using only a single view instead of the multi-views. \nDespite fewer views, CLIP outperforms the previous SOTA in three of the four metrics, especially enhancing foul and severity classification, with an increase of 9\\% in accuracy.\nTo determine the recognition performance of X-VARS, we asked X-VARS for each video clip, if it was a foul or not, and its corresponding severity.\nWe then asked ChatGPT-3.5 to extract the classification predictions from the generated explanations. \nFinally, comparing these predictions of X-VARS to the ground truth, we observe a significant performance increase to $62\\%$ accuracy in determining whether a foul occurred and its severity compared to CLIPs predictions.\nHence, X-VARS outperforms the previous state-of-the-art VARS (MViT+Max Pool) system~\\cite{Held2023VARS} by $19\\%$. \nHowever, since most of the explanations of X-VARS do not explicitly indicate the type of foul, it is not possible to accurately extract it from the explanations.\nFor this reason, we were not able to evaluate the accuracy of X-VARS in determining the type of foul.\n\n\\mysection{Influence of the video tokens.}\nWe investigated if the LLM simply generates its answers based on the multi-task predictions that we give as input to the LLM or if it also considers the video tokens.\nTo test this hypothesis, we use the X-VARS prediction obtained in the previous section, and we compare it to the CLIPs prediction provided as input. Interestingly, X-VARS did not simply replicate the CLIP prediction as it only agreed on $76\\%$ of the cases.\nThis result shows that X-VARS, throughout its training, developed the ability to re-evaluate the multi-task predictions and understand that they are not always reliable.\nConsequently, X-VARS does not only rely on the text predictions for its answers but also incorporates information from the video tokens.\n\n\\mysection{CLIPs classification predictions \\vs no text predictions.} \nTo validate our new training paradigm, we compared the quality of our X-VARS trained with classification prediction as additional text input against training it only with video features. To compare the two models, we randomly selected a set of $40$ video clips with a uniform distribution of various types of actions and severities. % (where possible. For instance, only 2 red cards are available in the test set).\nBy qualitatively analyzing the results on the selected set, both models generate similar outcomes for most of the video clips. The main difference occurs for less frequent types of actions and severities. For instance, X-VARS without predictions fails to predict a single ``No foul'' instance and achieves a balanced accuracy of only $29\\%$, while X-VARS obtains a $6\\%$ higher balanced accuracy. \nFigure~\\ref{fig:qualitative_res_a} shows a clip of a defender holding his opponent, an underrepresented action in our dataset. X-VARS without predictions incorrectly predicts: ``\\textit{No card, as the defender pushed the attacker in the back with low intensity during the fight for the ball, without any risk of injury}''.\nOn the other hand, X-VARS with predictions provides an accurate explanation: ``\\textit{No card, as the defender is holding onto the attackerâs jersey without it being unsportsmanlike and without any risk of injury.}''. \nThroughout our testing, we encountered several instances where X-VARS with prediction tokens aligned more closely with the ground truth, especially for underrepresented actions.\nThese results show the effectiveness of our new training paradigm in achieving higher accuracy and more accurate explanations.\n\n\\section{Conclusion}\n\\label{sec:conclusion}\nIn this work, we investigated the potential of using LLMs to enhance transparency and explainability within decision-making processes.\nWe proposed X-VARS, a multi-modal language model, which can perform a multitude of tasks, including video description, question answering, video action recognition, and conducting meaningful conversations based on video content. \nX-VARS achieves state-of-the-art performance in determining whether a duel between players constitutes a foul and in assessing the corresponding severity. \nThe qualitative results and human study underscore the exceptional capabilities of X-VARS in explaining its decision, indicating its potential to enhance football refereeing by providing accurate decisions and explanations.\n{\n\n\\mysection{Acknowledgement.}\nThis work was partly supported by the King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research through the Visual Computing Center (VCC) funding and the SDAIA-KAUST Center of Excellence in Data Science and Artificial Intelligence (SDAIA-KAUST AI). \nJ. Held and A. Cioppa are funded by the F.R.S.-FNRS. The present research benefited from computational resources made available on Lucia, the Tier-1 supercomputer of the Walloon Region, infrastructure funded by the Walloon Region under the grant agreement nÂ°1910247.\n\n}\n\n\\cleardoublepage\n\n{\\small\nbib/activity,\nbib/action,\nbib/learning,\nbib/dataset,\nbib/labo,\nbib/llm,\nbib/soccer,\nbib/soccernet-challenge,\nbib/sports,\nbib/explainability}\n}\n\n\\cleardoublepage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2108.11149v2.tex",
        "arXiv-2401.01505v4.tex",
        "arXiv-2404.06332v1.tex"
    ],
    "group_id": "group_67",
    "response": "### Summary: Advances in Sports Video Analysis and Refereeing with AI\n\n#### Introduction\nThe field of sports video analysis and refereeing has seen significant advancements in recent years, driven by the rapid progress in artificial intelligence (AI) and machine learning (ML). Historically, sports analysis has relied heavily on manual annotations and expert judgment, which are labor-intensive and subject to human error. With the advent of deep learning and multimodal datasets, there has been an increasing interest in automating the process of analyzing sports videos to extract meaningful information, such as events, actions, and referee decisions. However, the complexity and subjectivity of sports events, particularly in invasion games like soccer and handball, pose unique challenges for automatic systems. These challenges include the need for precise definitions of events, the integration of spatio-temporal data, and the ability to provide explanations for decisions. This summary examines three recent research papers that address these challenges by introducing new datasets, methodologies, and models for sports video analysis and refereeing.\n\nThe first paper, \"A Unified Taxonomy and Multimodal Dataset for Events in Invasion Games,\" focuses on creating a unified taxonomy and multimodal dataset for invasion games, specifically soccer and handball. It addresses the lack of consistent definitions for events and the need for high-quality, publicly available datasets to train and evaluate machine learning models. The second paper, \"Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports,\" introduces a dataset and model for sports video question answering (Sports-QA), which is the first of its kind to cover multiple sports and diverse question types. The third paper, \"X-VARS: Introducing Explainability in Football Refereeing with Multi-Modal Large Language Models,\" proposes X-VARS, a multi-modal large language model designed to explain football refereeing decisions, using a novel dataset called SoccerNet-XFoul. These papers collectively highlight the importance of precise definitions, multimodal data, and explainability in sports video analysis and refereeing.\n\n#### Main Content of Each Paper\n\n**Paper 1: A Unified Taxonomy and Multimodal Dataset for Events in Invasion Games**\nThis paper addresses the need for a unified taxonomy and high-quality datasets for invasion games, particularly soccer and handball. The authors propose a hierarchical taxonomy that covers both low-level and high-level events, such as game status changes, ball possession, ball reception, and ball release, in addition to more complex events like passes and shots. The taxonomy is designed to be minimal, non-redundant, and modular, allowing for refinement to fit specific sports requirements. To validate the taxonomy, the authors release two multimodal datasets: one for soccer and one for handball, each containing gold-standard annotations for 125 minutes of playing time. These datasets include video and positional data, providing a comprehensive resource for evaluating automatic models. The authors also introduce a new metric called Sequence Consistency Matching (SCM) to address the limitations of existing metrics, such as Nearest Neighbor Matching (NNM), which can introduce biases in evaluation. Experiments show that the taxonomy and datasets are robust, with high agreement among experienced annotators. The paper concludes with a vision-based baseline model that achieves state-of-the-art performance on the proposed datasets, demonstrating the potential for future research.\n\n**Paper 2: Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports**\nThis paper presents Sports-QA, a large-scale dataset specifically designed for sports video question answering (Sports VideoQA). The dataset includes over 94K QA pairs across multiple sports, such as basketball, football, volleyball, and gymnastics. The authors define five attributes for actions in ball games and generate QA pairs based on these attributes, ensuring high-quality and relevant textual data. Sports-QA is balanced across different sports and question types, addressing the challenges of fine-grained and professional action understanding. The authors introduce the Auto-Focus Transformer (AFT), a new model designed to automatically focus on specific temporal scales based on the question. AFT uses a multi-head Auto-Focus Attention (AFA) mechanism to capture temporal dependencies of various scales, which is crucial for sports VideoQA. The authors conduct extensive experiments, showing that AFT outperforms existing methods on Sports-QA, achieving state-of-the-art performance. The results demonstrate the model's ability to handle diverse question types and its potential for other tasks, such as multi-label classification and sports video captioning.\n\n**Paper 3: X-VARS: Introducing Explainability in Football Refereeing with Multi-Modal Large Language Models**\nThis paper introduces X-VARS, a multi-modal large language model designed to explain football refereeing decisions. X-VARS is based on a fine-tuned CLIP visual encoder and a Vicuna-v1.1 language model, which is initialized with weights from LLaVA. The authors create a new dataset called SoccerNet-XFoul, which contains over 22K video-question-answer triplets annotated by experienced referees. X-VARS is trained in two stages: first, CLIP is fine-tuned on the SoccerNet-MVFoul dataset to learn prior knowledge about football and refereeing, and second, the projection layer and several layers of the LLM are fine-tuned to enhance the model's generation abilities. The authors conduct a human study to evaluate the quality of X-VARS's explanations, showing that X-VARS performs comparably to human referees. X-VARS also demonstrates impressive capabilities in understanding and generating decisions with explanations, even for underrepresented actions. The authors conclude that X-VARS has the potential to support football referees in the future, providing accurate decisions and explanations.\n\n#### Commonalities and Innovations\nAll three papers focus on advancing the field of sports video analysis and refereeing through the development of new datasets and models. Paper 1 introduces a hierarchical taxonomy for invasion games and releases two multimodal datasets, SoccerNet and HandballNet, to facilitate research in fine-grained event spotting. Paper 2 presents Sports-QA, a large-scale dataset for sports VideoQA, and proposes the Auto-Focus Transformer (AFT) to capture temporal dependencies of various scales. Paper 3 introduces X-VARS, a multi-modal large language model for explaining football refereeing decisions, using a novel dataset called SoccerNet-XFoul.\n\nThe innovations in these papers include:\n- **Paper 1**: A hierarchical taxonomy for invasion games and the introduction of a new metric, Sequence Consistency Matching (SCM), to address the limitations of existing metrics.\n- **Paper 2**: The first sports VideoQA dataset, Sports-QA, and the Auto-Focus Transformer (AFT) model designed to handle diverse question types.\n- **Paper 3**: A multi-modal large language model, X-VARS, for explaining football refereeing decisions and a new dataset, SoccerNet-XFoul, annotated by experienced referees.\n\n#### Comparison of Results\nThe results of the three papers demonstrate the effectiveness of their proposed methodologies and datasets. Paper 1 reports high agreement among experienced annotators for events with a duration and timestamp, indicating the robustness of the proposed taxonomy and datasets. The vision-based baseline model achieves state-of-the-art performance, with a precision and recall of 95.0 and 98.7 for successful passes, respectively. Paper 2 shows that AFT outperforms existing methods on Sports-QA, achieving a best accuracy of 72.3\\% and a best F1-score of 25.4\\%. The human study in Paper 3 highlights that X-VARS performs comparably to human referees, with a mean score of 3.8 and a balanced accuracy of 62\\% in determining whether a foul occurred and its severity.\n\n#### Conclusion\nThe three papers collectively contribute to the field of sports video analysis and refereeing by introducing new datasets, methodologies, and models. Paper 1's taxonomy and datasets provide a foundation for fine-grained event spotting in invasion games, while Paper 2's Sports-QA dataset and AFT model enable comprehensive understanding and reasoning over sports videos. Paper 3's X-VARS model demonstrates the potential of large language models to explain football refereeing decisions, supporting the integration of AI in sports. Future research could focus on integrating non-on-ball events into the taxonomy, improving the visual model for event spotting, and addressing the limitations of X-VARS, such as hallucinations and memory constraints, to enhance its performance and robustness."
}