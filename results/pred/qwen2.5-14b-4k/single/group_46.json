{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{3D-MoLM: Towards 3D Molecule-Text Interpretation in Language Models}\n\n\\begin{document}\n\n\\maketitle\n\n\\vspace{-10pt}\n\n\\vspace{-2mm}\n\\begin{abstract}\n    \\vspace{-2mm}\n    Language Models (LMs) have greatly influenced diverse domains. However, their inherent limitation in comprehending 3D molecular structures has considerably constrained their potential in the biomolecular domain. To bridge this gap, we focus on 3D molecule-text interpretation, and propose \\textbf{3D-MoLM}: \\underline{3D}-\\underline{Mo}lecular \\underline{L}anguage \\underline{M}odeling. Specifically, 3D-MoLM enables an LM to interpret and analyze 3D molecules by equipping the LM with a 3D molecular encoder. This integration is achieved by a 3D molecule-text projector, bridging the 3D molecular encoder's representation space and the LM's input space. Moreover, to enhance 3D-MoLM's ability of cross-modal molecular understanding and instruction following, we meticulously curated a 3D molecule-centric instruction tuning dataset -- \\textbf{3D-MoIT}. Through 3D molecule-text alignment and 3D molecule-centric instruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder and LM. It significantly surpasses existing baselines on downstream tasks, including molecule-text retrieval, molecule captioning, and more challenging open-text molecular QA tasks, especially focusing on 3D-dependent properties. \n    We release our codes and datasets at \\url{https://github.com/lsh0520/3D-MoLM}.\n\\end{abstract}\n\\vspace{-3mm}\n\n\\vspace{-1mm}\n\\section{Introduction}\n\\vspace{-3mm}\nThe advancement of Language Models (LMs) \\citep{bert,gpt4,LLaMA} has triggered a series of remarkable innovations across multiple disciplines \\citep{zhao2023survey}. \nNotably, LMs excel at text-based molecule understanding tasks, such as question-answering (QA) in the chemical and medical domains \\citep{galactica}, by pretraining on extensive biochemical literature.\nRecognizing the potential of LMs in harnessing extensive biochemical knowledge for molecule-relevant tasks,\nmolecule-text modeling emerges as a new research direction \\citep{text2mol,molt5}. \nPrevious works have been dedicated to harmonizing texts with 1D molecular sequences \\citep{kvplm, galactica} and 2D molecular graphs \\citep{liu2023molca, momu, stm}, aiding in tasks like molecule-text retrieval and molecule captioning.\nHowever, they mostly leave 3D molecular structures untouched, which are crucial to understanding molecular dynamics, protein-ligand interactions, enzymatic functions, and a range of other biomolecular phenomena \\citep{3D-molecule-1, 3D-molecule-2}.\n\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[width=\\textwidth]{figures/framework}\n\\vspace{-16pt}\n\\caption{Demonstration of 3D-MoLM. 3D-MoLM is a general-purpose molecular LM that can be applied for molecule-text retrieval, molecule captioning, and molecular QA tasks. \nFlame \\includegraphics[width=0.02\\textwidth]{figures/fire} denotes tunable modules, while snowflake \\includegraphics[width=0.02\\textwidth]{figures/snow_flake} indicates frozen modules.\n}\n\\vspace{-16pt}\n\\label{fig:overview}\n\\end{figure}\n\nTo bridge this gap, we focus on 3D molecule-text interpretation, with the goal of enabling an LM to interpret and analyze 3D molecular structures through text generation.\nGiven the recent successes of 3D molecular encoders in tasks like molecule property prediction, docking, and conformation prediction \\citep{uni-mol,uni-mol-plus,GEM}, it is promising to incorporate one as an LM's perception module for 3D molecules. \nUpon examination of existing literature \\citep{instructblip,3D-LLM,FlanModel}, \nwe identify two key challenges to seamlessly integrate a 3D molecular encoder into an LM for 3D molecule-text interpretation: \n\\begin{itemize}[leftmargin=*]\n\\item \\textbf{3D Molecule-Text Alignment} maps 3D molecular representations into the input textual space where the LM can understand. \n\\item \\textbf{3D Molecule-centric Instruction Tuning} fine-tunes the model to follow human instructions on 3D molecule relevant tasks.\n\\end{itemize}\n\nTo address these challenges, we propose \\textbf{3D-MoLM}: \\underline{3D}-\\underline{Mo}lecular \\underline{L}anguage \\underline{M}odeling, as depicted in Figure~\\ref{fig:overview}. Specifically, it consists of two key components: 1) a 3D molecule-text projector for 3D molecule-text alignment, which aligns the latent representation spaces between the 3D molecular encoder and the LM, and 2) a dataset for \\underline{3D} \\underline{Mo}lecule-centric \\underline{I}nstruction \\underline{T}uning, \\textbf{3D-MoIT}, as shown in Figure~\\ref{fig:dataset}. 3D-MoIT enhances the model's ability to follow human instructions and discern 3D-dependent properties of molecules.\n\nFor 3D molecule-text alignment, we employ Q-Former \\citep{blip2} as the 3D molecule-text projector, drawing inspiration from leading vision-language modeling methods \\citep{minigpt4,instructblip}. \nGiven a molecule's 3D structure, Q-Former converts it into tokens, which serve as 1D soft prompts~\\citep{PrefixTuning}, harmonizing seamlessly with the language space of the LM. \nThis translation facilitates the LM's interpretation of 3D molecular structures.   \nTo cultivate the Q-Former's alignment capability, two training stages are conducted  -- \n\\lshr{the first stage focuses on 3D molecule-text representation learning, while the second stage optimizes for 3D molecule-text alignment. }\nAs depicted in Figure~\\ref{fig:dataset}, these two training stages are facilitated by our collected 316K molecule-text pairs from PubChem \\citep{pubchem}. \nTo promote the 3D molecule-text alignment process, we manipulate the dataset by generating the 3D conformations based on SMILES using RDKit~\\citep{rdkit} and enriching the molecular descriptions with GPT-3.5~\\citep{ChatGPT}.\nWe will detail the collection and enrichment of PubChem Dataset in Section~\\ref{sec:alignment} and Appendix~\\ref{app:pubchem-dataset}.\n\nUpon aligning 3D molecules with texts, we conduct instruction tuning using our curated dataset 3D-MoIT. \nIt is designed to cultivate 3D-MoLM's ability to follow instructions, and to enhance its perception of 3D-dependent molecule properties. \nSpecifically, 3D-MoIT is sourced from two databases: 1) PubChem, which offers a wide range of molecular properties, origins, and applications, and 2) PubChemQC \\citep{pubchemqc}, which specializes in 3D-dependent molecular properties. \nAs shown in Figure~\\ref{fig:dataset}, for the PubChem portion, we leverage GPT-3.5 to generate QA pairs based on their descriptions.\nYet, molecular properties collected from PubChem (\\eg molecular weight and LogP) can be largely inferred from 1D or 2D molecular data. \nTo enhance 3D-MoIT's perception of 3D molecular structures, we further incorporate data from PubChemQC, which includes 3D-dependent molecule properties (\\eg HOMO and LUMO; \\cite{mcquarrie1997physical}). \nWe fill these properties into a set of text templates, transforming them into instruction tuning formats, as Figure~\\ref{fig:overview} illustrates.\n \nOur contributions can be summarized as follows:\n\\begin{itemize}[leftmargin=*]\n\\item We propose 3D-MoLM, a new framework for 3D molecule-text interpretation. 3D-MoLM employs a 3D molecule-text projector to bridge the modality gap between a 3D molecular encoder and an LM, enabling the LM to perceive 3D molecular structures.\n\n\\item We curate 3D-MoIT, a 3D molecule-centric instruction tuning dataset. We extract and transform data from PubChem and PubChemQC to an instruction following format, to cultivate 3D-MoLM's ability in instruction following and 3D molecule-text interpretation.\n\n\\item 3D-MoLM achieves state-of-the-art performances in extensive downstream tasks. Notably, on the PubChem Dataset, for molecule-text retrieval and molecule captioning, it outperforms baselines by 20\\% accuracy and 6.47 ROUGE-L, respectively. Further, it surpasses the baselines with 1D or 2D molecular perceptions on open-text QA tasks, especially on 3D-dependent properties, verifying the capability of 3D molecule-text interpretation.\n\\end{itemize}% \\input{chapters/2_related}\n\\vspace{-3mm}\n\\section{3D-MoLM: 3D Molecular Language Modeling}\n\\vspace{-2mm}\n\\lshr{3D-MoLM incorporates a 3D molecular encoder into an LM, aiming to align 3D molecular geometries with textual concepts and facilitate a comprehensive cross-modal understanding of molecules.\nConsequently, 3D-MoLM is able to read 3D molecular structures, amplifying its molecular understanding and facilitating 3D-text interpretation.\nOur idea draws from related works in molecule-text modeling, multi-modal instruction tuning, and multi-modal LMs. See Appendix~\\ref{sec:related-work} for a comprehensive literature review.\nHere we delve into 3D-MoLM's architecture and its training pipeline. }\n\n\\begin{figure}[t]\n\\centering\n\\small\n\\begin{subfigure}[b]{\\textwidth}\n\\centering\n\\small\n\\includegraphics[width=0.8\\textwidth]{figures/qformer}\n\\vspace{-4pt}\n\\caption{\\lshr{Stage 1. The 3D molecule-text projector (\\ie Q-Former) with the attached frozen 3D molecule encoder is optimized for 3D molecule-text representation learning. Stage 1 involves three training objectives: molecule-text matching, molecule-text contrasting, and molecule captioning.}}\n\\label{fig:qformer}\n\\end{subfigure}\n\\vspace{1cm}\n\\begin{subfigure}[b]{\\textwidth}\n\\centering\n\\small\n\\includegraphics[width=0.8\\textwidth]{figures/stage23}\n\\caption{\\lshr{Stage 2 \\& 3. 3D-MoLM is trained to perform 3D molecule-to-text generations given 3D molecular tokens (extracted by the Q-former) and 1D textual prompt tokens.}}\n\\vspace{-32pt}\n\\label{fig:stage23}\n\\end{subfigure}\n\\caption{\\lshr{Illustration of 3D-MoLM's architectures at different stages.}}\n\\label{fig:qformer_and_stage23}\n\\vspace{-12pt}\n\\end{figure}\n\n\\vspace{-1mm}\n\\subsection{Model Architecture}\n\\vspace{-1mm}\n3D-MoLM's architecture consists of three key components: 1) a 3D molecular encoder, focusing on encoding 3D molecular structures; 2) a 3D molecule-text projector, aiming to map the 3D molecular encoder's representations to the input space of the LM; and 3) an LM, which specializes in text generation and is later adapted for understanding 3D molecular structures.\n\n\\textbf{3D Molecular Encoder.}\nWe adopt Uni-Mol~\\citep{uni-mol} as our 3D molecular encoder $f_{\\text{mol}}$.\nSpecifically, Uni-Mol is pretrained on a large molecule dataset comprising 209M 3D molecular conformations.\nFormally, let $m=(\\Set{V},\\Mat{h}, \\Mat{C})$ denote a molecule, where $\\Set{V}$ and $\\Mat{h}$ separately represent atomic nodes and their features, and $\\Mat{C} \\in \\space{R}^{|\\Set{V}| \\times 3}$ collects the 3D coordinates of nodes.\nIn Uni-Mol, the representation for each pair of atoms is initialized using invariant spatial positional encoding derived from 3D coordinates $\\Mat{C}$. \nThis encoding, grounded in the pair-wise Euclidean distances between atoms, ensures that the representation remains consistent regardless of global rotations or translations.\nSubsequently, representations of atoms and atom pairs engage in a self-attention mechanism, generating the molecular representation with 3D spatial information.\nOverall, the 3D molecular encoder $f_{\\text{mol}}$ performs molecule encoding procedure to obtain the atomic representations:\n\\begin{equation}\\label{molecule-encoding}\n\\Mat{X} = [\\Vtr{x}_1, \\Vtr{x}_2,...,\\Vtr{x}_{|\\Set{V}|}] = f_{\\text{mol}}(m),\n\\end{equation}\nwhere $\\Vtr{x}_i$ corresponds to the representation of the $i$-th atom.\n\n\\textbf{3D Molecule-Text Projector.}\nTaking inspiration from the leading vision-language models \\citep{blip2, instructblip}, we architect the 3D molecule-text projector $f_{\\text{pro}}$ as a Querying Transformer (\\ie Q-Former) and initialize it from the Sci-BERT's checkpoint \\citep{scibert}. \nAs illustrated in Figure~\\ref{fig:qformer}, Q-Former has two transformers \\lshr{with shared self-attention layers}: one molecule transformer for processing 3D molecule features, and one text transformer for processing texts. The text transformer follows the same architecture of BERT~\\citep{bert}, while the molecule transformer adds cross-attention modules between the modules of self-attention and feed-forward to extract molecule features.  \nSpecifically, the molecule transformer maintains $K$ learnable query tokens. Given 3D molecule input, the query tokens can interact with the 3D molecular encoder's representations through the cross-attention modules. Therefore, the $K$ query tokens' output representations contain molecule information, represented as $\\Mat{M} = [\\Vtr{m}_1, \\Vtr{m}_2,...,\\Vtr{m}_K]$.\nThe 3D molecule-text projector's forward function can be written as:\n\\vspace{-1mm}\n\\begin{equation}\\label{cross-modal-projector}\n\\Mat{M} = [\\Vtr{m}_1, \\Vtr{m}_2,...,\\Vtr{m}_K]=f_{\\text{pro}}(\\Mat{X}).\n\\end{equation}\n\n\\textbf{Language Model (LM).}\nWe employ Llama2 \\citep{llama2} as our base LM $f_{\\text{lm}}$ to leverage its powerful text generation capability and internal chemistry knowledge. Although pretrained for general-purpose usage, the extensive biomedical literature in Llama 2's pretraining corpus enables it to efficiently interpret 1D molecular sequences (\\eg SMILES) and proficiently address essential QA tasks that are relevant to molecular understanding. \\lshr{In this work, we let Llama2 process mixed token sequences that includes both textual tokens and 3D molecular tokens, which is detailed in Section~\\ref{sec:alignment}.} Formally, we denote a mixed token sequence that include $l$ textual and molecular tokens as $\\Mat{Z} = [\\Vtr{z}_1, \\Vtr{z}_2,...,\\Vtr{z}_l]$. \nFurther, the LM adopts a causal mask to generate textual response $\\hat{\\Mat{Z}}$ with length $n$, where the prediction for the $i$-th token, $\\hat{\\Vtr{z}}_i$, is dependent on its previous tokens:\n\\begin{equation}\\label{language-model}\n\\hat{\\Mat{Z}} = [\\hat{\\Vtr{z}}_{l+1}, \\hat{\\Vtr{z}}_{l+2},...,\\hat{\\Vtr{z}}_{l+n}], \\qquad \\hat{\\Vtr{z}}_i = f_{\\text{lm}}(\\Mat{Z}_{<i}), \\qquad \\Mat{Z}_{<i} = [\\Vtr{z}_1, \\Vtr{z}_2,...,\\Vtr{z}_{l},\\hat{\\Vtr{z}}_{l+1},...,\\hat{\\Vtr{z}}_{i-1}],\n\\end{equation}\nwhere each $\\hat{\\Vtr{z}}_i$ is later transformed by a linear layer $f_{\\text{vocab}}$ accompanied by a softmax function, converting into a probabilistic distribution over the vocabulary.\nThe final prediction $\\tilde{\\Vtr{z}}_i$ for the $i$-th token is the word in the vocabulary with the highest probability, defined as:\n\\begin{equation}\\label{language-model-word}\n\\tilde{\\Vtr{z}}_i=\\arg \\max_{w \\in \\text{vocab}} f_{\\text{vocab}}(\\hat{\\Vtr{z}}_i)[w].\n\\end{equation}\n\\vspace{-2mm}\n\n\\subsection{Model Training}\n\\label{sec:model_training}\n\\vspace{-2mm}\nTo tackle the identified two challenges of 3D molecule-text alignment and 3D molecule-centric instruction tuning, we delineate a three-stage training pipeline (\\cf Figure~\\ref{fig:dataset}) for 3D-MoLM, including 1) 3D molecule-text representation learning, 2) 3D molecule-text alignment via gerative learning, and 3) 3D molecule-centric instruction tuning.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.8\\textwidth]{figures/datasetv2.pdf}\n\\vspace{-4pt}\n\\caption{Illustration of the model architectures (upper part) and the dataset usage (bottom part) for the three training stages. \\lshr{PubChem is used for the stage 1 (\\ie 3D molecule-text representation learning)  and stage 2 (\\ie 3D molecule-text alignment via generative learning)}. 3D-MoIT is used for 3D molecule-centric instruction tuning. Texts in the same color indicate the same information source.\n}\n\\label{fig:dataset}\n\\vspace{-13pt}\n\\end{figure}\n\n\\subsubsection{3D Molecule-Text Alignment}\\label{sec:alignment}\n\\textbf{Data Preparation -- PubChem.}\nA substantial collection of 3D molecule-text pairs is essential to bridge the gap between the 3D molecular representation space and the LM's input space. \nWe begin by collecting molecular SMILES-text pairs from PubChem and further employ GPT-3.5 to enrich the less annotated molecular descriptions. Molecular 3D conformations are obtained by running the MMFF algorithm in RDKit~\\citep{rdkit}. \nAs a result, we obtain 316K 3D molecule-text pairs for the alignment pretraining and downstream tasks of molecule-text retrieval and molecule captioning.\nThe details of processing, subset split, and quality evaluation are in Appendix~\\ref{app:pubchem-dataset}.\n\n\\textbf{Stage 1: 3D Molecule-Text Representation Learning.} \nIn the first stage, we jointly pretrain the Q-Former together with the frozen 3D molecular encoder on the collected 3D molecule-text pairs. Following BLIP-2 \\citep{blip2}, we perform \\lshr{multi-objective} training, including \\lshr{molecule-text matching, molecule-text contrasting and molecule captioning} (\\cf Figure \\ref{fig:qformer}).\nThese training objectives are designed to cultivate Q-Former's ability to extract molecular features that resonate profoundly with the corresponding text. Specifically, the molecule-text matching task mandates the model to differentiate between matched and unmatched pairs, enhancing the fine-grained alignment of cross-modal representations.\nIn molecule-text contrasting, the similarity between a molecule and its corresponding text (\\ie positive pair) is contrasted against those of negative pairs, aiming to maximize the mutual information between the molecular and textual representations.\n\\lshr{In molecule captioning}, the Q-Former is trained to generate text descriptions, based on given molecular inputs.\n\n\\lshr{\\textbf{Stage 2: 3D Molecule-Text Alignment via Generative Learning.}\nIn this stage, we connect the 3D molecular encoder and the Q-Former with the LM for 3D molecule-to-text generation, as illustrated in Figure~\\ref{fig:stage23}. The training objective is conditional language modeling: 3D-MoLM is trained to generate textual responses given 3D molecular tokens and 1D textual prompt tokens. Specifically, the 3D molecular tokens are extracted by the Q-Former, while the 1D textual prompt tokens include the molecule's SMILES sequence and a textual task description. See Appendix~\\ref{app:ablation} for ablation studies on prompt templates. This training objective encourages the LM to discern the contextual interplay between textual and molecular tokens, thus aiding in 3D molecule-text interpretation. For efficiency consideration, we freeze the 3D molecular encoder and employ LoRA tuning for the LM~\\citep{lora}. \nOverall, this stage aligns 3D molecular representations with the LM's textual input space, aiding in effective molecule-to-text generation.}\n\n\\begin{table}[t]\n\\centering\n\\scriptsize\n\\caption{Statistics of 3D-MoIT. Comp.$\\rightarrow$computed property; Desc.$\\rightarrow$descriptive property.}\n\\vspace{-8pt}\n\\label{tab:3d-moit-dataset}\n\\begin{tabular}{lccccc}\n\\toprule\n\\multirow{2}{*}{Subset}            & \\multicolumn{2}{c}{PubChemQC}              & \\multicolumn{3}{c}{PubChem}                                                  \\\\ \\cmidrule(lr){2-3} \\cmidrule(lr){4-6}\n            & \\#Mol & \\#Comp. QA  & \\#Mol & \\#Comp. QA  & \\#Desc. QA  \\\\\\midrule\nPretrain &      3,119,717       & 12,478,868                   &      301,658       & 1,199,066                    & 1,508,290                       \\\\\nTrain    &      623,944       & 2,495,776                    &    12,000         & 46,680                       & 60,000                          \\\\\nValid    &       77,993      & 311,972                      &     1,000        & 3,898                        & 5,000                           \\\\\nTest     &        77,993     & 311,972                      &       2,000      & 7,785                       & 10,000                          \\\\\\bottomrule\n\\end{tabular}\n\\vspace{-16pt}\n\\end{table}\n\n\\subsubsection{3D Molecule-centric Instruction Tuning}\\label{sec:instruct-tuning}\n\\lshr{\\textbf{Stage 3: 3D Molecule-centric Instruction Tuning.}}\nIn the final stage, we freeze the 3D molecular encoder and conduct instruction fine-tuning to jointly optimize the 3D molecule-text projector and the LM. \nThis fine-tuning has two purposes: 1) to enhance our model's ability to follow various instructions; and 2) to improve our model's understanding of 3D molecular structures, especially in recognizing 3D-dependent properties. \nIt is framed as conditional text generation optimization based on the input prompt and the standard language modeling loss is adopted.\nNow, we elaborate on the details of our instruction tuning dataset 3D-MoIT.\n\n\\textbf{Data Preparation -- 3D-MoIT.} 3D-MoIT sources data from the PubChem~\\citep{pubchem} and PubChemQC~\\citep{pubchemqc} databases. \nSpecifically, the instruction tuning data from PubChem can be divided into two types: computed molecular properties and descriptive texts, where computed molecular properties are numerical values and descriptive texts characterize molecule properties in free texts. To effectively activate the 3D perception, we further include computed molecular properties from PubChemQC. PubChemQC contains 3D-dependent molecule attributes, which mostly cannot be inferred from 1D or 2D molecular representations. PubChemQC also includes DFT-determined 3D conformations, commonly regarded as the ground truth in 3D molecular modeling. The statistics of 3D-MoIT are shown in Table~\\ref{tab:3d-moit-dataset}. We detail the dataset construction as follows:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{PubChem: Instruction Format.} We select the following computed properties: molecular weight, LogP, TPSA, and complexity. They are transformed into instruction tuning format using a pre-defined set of text templates. For descriptive text, we adopt GPT-3.5 to read molecular descriptions and generate five QA pairs for each molecule, as illustrated in Figure~\\ref{fig:dataset}. PubChem includes a diverse range of molecule properties, enhancing the comprehensiveness of 3D-MoIT.\n    \\item \\textbf{PubChemQC: Instruction Format.} We select the following computed molecular properties: HOMO, LUMO, HOMO-LUMO Gap, and SCF-energy. These properties are transformed into instruction tuning format by filling the values into pre-defined text templates, as illustrated in Figure~\\ref{fig:dataset}. We use the processed dataset released by~\\citep{molecule3d} and follow the scaffold split.\n\\end{itemize}\n\n\\vspace{-0.2cm}\n\\section{Experiment} \\label{sec:experiment}\n\\vspace{-0.2cm}\nIn this section, we conduct extensive experiments, including molecule-text retrieval, molecule captioning, and open-text molecular QA tasks, to demonstrate the effectiveness of 3D-MoLM for 3D molecule-text interpretation.\nSee Appendix~\\ref{app:exp-setup} for experimental details of each downstream task.\n\n\\vspace{-0.1cm}\n\\subsection{Molecule-Text Retrieval}\\label{sec:retrieval}\n\\vspace{-0.1cm}\nWe assess the Stage-1 checkpoint of 3D-MoLM on the downstream subsets of PubChem Dataset for molecule-text retrieval. \nThese subsets contain real-world molecules paired with textual descriptions longer than 20 words. \nWe opt not to evaluate existing molecule-text datasets of PCDes~\\citep{kvplm}, because of data leakage. PCDes and our curated PubChem dataset stem from the same source (\\ie some PCDes test data present in our pretraining set).\nWe employ Sci-BERT~\\citep{scibert}, KV-PLM~\\citep{kvplm}, and MoMu~\\citep{momu} as baselines and evaluate the performance by Accuracy and Recall@20, both within a batch of 64 samples and across the entire test set. \nBaselines are initiated from their official checkpoints and finetuned using the downstream partition of the PubChem Dataset, except $\\text{MoMu}^\\dag$, which is our re-implementation with the original PubChem texts without GPT-3.5 enrichment.\nFrom Table~\\ref{tab:cross-modal-retrieval}, we have the following observations:\n\n\\begin{table}[t]\n\\centering\n\\scriptsize\n\\caption{Molecule-Text retrieval results on the PubChem Dataset. \n$\\dag$ denotes pretraining on the original PubChem texts without GPT-3.5 enrichment. \nWe report performances of both using molecule to retrieve text (M2T) and using text to retrieve molecule (T2M). \n}\n\\vspace{-3mm}\n\\label{tab:cross-modal-retrieval}\n \\begin{tabular}{lcccccccc}\n    \\toprule\n    & \\multicolumn{4}{c}{Retrieval in batch} & \\multicolumn{4}{c}{Retrieval in test set} \\\\ \n    & \\multicolumn{2}{c}{M2T (\\%)} & \\multicolumn{2}{c}{T2M (\\%)} & \\multicolumn{2}{c}{M2T (\\%)} & \\multicolumn{2}{c}{T2M (\\%)} \\\\\\cmidrule(lr){2-3}\\cmidrule(lr){4-5} \\cmidrule(lr){6-7} \\cmidrule(lr){8-9}\n    Model & Acc & R@20 & Acc & R@20 & Acc & R@20 & Acc & R@20 \\\\\\midrule\n    \\multicolumn{2}{l}{\\textbf{1D SMILES}} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} \\\\\n    Sci-BERT & 85.32 & 98.74 & 84.20 & 98.43 & 41.67 & 87.31 & 40.18 & 86.77 \\\\\n    KV-PLM & 86.05 & 98.63 & 85.21 & 98.47 & 42.80 & 88.46 & 41.67 & 87.80 \\\\\\midrule\n    \\multicolumn{2}{l}{\\textbf{2D Graph}} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} \\\\\n    MoMu-S & 87.58 & 99.24 & 86.44 & 99.38 & 47.29 & 90.77 & 48.13 & 89.92 \\\\\n    MoMu-K & 88.23 & 99.41 & 87.29 & 99.42 & 48.47 & 91.64 & 49.46 & 90.73 \\\\\n    \\lshr{$\\text{MoMu-S}^\\dag$} & \\lshr{90.43} & \\lshr{99.53} & \\lshr{89.38} & \\lshr{99.60} & \\lshr{60.51} & \\lshr{93.24} & \\lshr{58.36} & \\lshr{91.35} \\\\\n    \\lshr{$\\text{MoMu-K}^\\dag$} & \\lshr{90.89} & \\lshr{99.67} & \\lshr{90.16} & \\lshr{\\underline{99.44}} & \\lshr{62.07} & \\lshr{93.06} & \\lshr{59.17} & \\lshr{92.01} \\\\\n    \\midrule\n    \\multicolumn{2}{l}{\\textbf{3D Conformation}} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} \\\\\n    $\\text{3D-MoLM}^\\dag$ & \\textbf{94.48} & \\underline{99.74} & \\textbf{94.78} & 99.34 & \\textbf{72.06} & \\textbf{96.42} & \\textbf{71.30} & \\textbf{95.96} \\\\\n    3D-MoLM & \\underline{93.50} & \\textbf{100.00} & \\underline{92.89} & \\textbf{99.59} & \\underline{69.05} & \\underline{95.91} & \\underline{70.13} & \\underline{94.88}\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\vspace{-12pt}\n\\end{table}\n\n1) 3D-MoLM surpasses existing baselines, including both molecular 1D-language models (\\ie Sci-BERT, KV-PLM) and 2D-language models (\\ie MoMu-S/K), by a large margin.\nThe improvement can be attributed to two pivotal factors. \n\\lshr{\nFirstly, the performance gain $\\text{3D-MoLM}^\\dag$ over $\\text{MoMu}^\\dag$, which are both pretrained on the same PubChem molecule-text pairs, demonstrates that Q-Former benefits from multi-objective pretraining across diverse granularities, distinguishing it from other models that predominantly rely on the molecule-text contrastive learning objective. \nSecondly, the scale of the PubChem Dataset, which we curated, offers a considerable advantage, which is verified by the performance gain $\\text{MoMu}^\\dag$ over the original MoMu.\nWith the collection of 301K molecule-text pairs for pretraining, it markedly surpasses the 15K pairs in the MoMu \\wrt scale.\nThis increased scale provides our model with a richer and more diverse set of training examples, allowing it to better capture the molecular-text interactions.\n}\nSuch superior performance underscores the effectiveness of 3D molecular representation learning in Stage 1, demonstrating the capability of Q-Former to extract molecular features that are strongly related to the textual descriptions.\n\n2) The retrieval performance on the PubChem test set appears to be negatively impacted by GPT-3.5 enrichment. \nWe infer that this decline is caused by the enrichment process enlarging the distribution gap between the pretraining and downstream subsets. \nWhile the original texts might be succinct and hence less informative, they exhibit word patterns more congruent with those in the test set, in stark contrast to the distinctive GPT-style exhibited by the enriched texts.\nNonetheless, we argue that this enrichment process benefits forging a more comprehensive connection between molecular structures and their inherent properties.\nThis assertion is further substantiated by subsequent experiments of textual generation tasks, where GPT-3.5 enrichment boosts the performance of 3D-MoLM.\n\n\\newcolumntype{M}[1]{>{\\scriptsize\\arraybackslash}m{#1}}\n\\begin{table*}[t]\n\\centering\n\\caption{\\lshr{Molecule captioning results on PubChem Dataset.\n$\\dag$ denotes pretraining on the original PubChem texts without GPT-3.5 enrichment.\nLlama2-7B, without a molecule-text projector, goes through Stage 2 training with the prompt of 1D SMILES.\n2D-MoLM replaces the 3D molecular encoder with a 2D molecular encoder and goes through the same training process as 3D-MoLM.\n}\n}\n\\vspace{-6pt}\n\\scriptsize\n\\begin{subtable}[t]{\\textwidth}\n\\centering\n\\caption{Molecule captioning results.}\n\\vspace{-4pt}\n\\begin{tabular}{llcccccc}\\toprule\n    Type & Model                  & BLEU-2               & BLEU-4               & ROUGE-1              & ROUGE-2              & ROUGE-L              & METEOR               \\\\\\midrule\n    \\multirow{11}{*}{Specialist} &\n    \\multicolumn{2}{l}{\\textbf{1D SMILES}}            & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} \\\\\n    &MolT5-Small                                & 22.53                 & 15.23                 & 30.44                 & 13.45                 & 20.30                 & 23.98                 \\\\\n    &MolT5-Base                                 & 24.51                 & 16.61                 & 32.19                 & 14.04                 & 21.35                 & 26.10                 \\\\\n    &MolT5-Large                                 & 25.87                 & 17.28                 & 34.07                 & 16.42                 & 23.41                 & 28.04                 \\\\\n    \\cmidrule{2-8}\n    &\\multicolumn{3}{l}{\\textbf{1D SMILES + 2D Graph}} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} \\\\\n    &MoMu-Small                                  & 22.86                 & 16.01                 & 30.98                    & 13.65                    & 20.75                 & 24.35                 \\\\\n    &MoMu-Base                                  & 24.74                 & 16.77                 & 32.45                    &14.62                    & 22.09                 & 27.16                 \\\\\n    &MoMu-Large                                 & 26.34                 & 18.01                 & 34.75                    & 16.86                    & 24.76                 & 28.73                 \\\\\n    \\cmidrule{2-8}\n    &\\multicolumn{3}{l}{\\textbf{1D SMILES + 3D Conformation}}  & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} \\\\\n    &3D-MoLM$\\dag$    & \\underline{29.82}        & \\underline{22.39}        & \\textbf{37.23}        & \\underline{22.49}        & \\underline{31.07}        & \\underline{32.69}       \\\\\n    &3D-MoLM    & \\textbf{30.32}        & \\textbf{22.52}       & \\underline{36.84}        & \\textbf{22.32}       & \\textbf{31.23}        & \\textbf{33.06}       \\\\      \n    \\midrule\n    \\multirow{7}{*}{\\lshr{Generalist}} \n    &\\multicolumn{3}{l}{\\textbf{\\lshr{1D SMILES}}}  & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} \\\\\n    &\\lshr{Llama2-7B}    & \\lshr{27.01}        & \\lshr{20.94}        & \\lshr{35.76}        & \\lshr{20.68}        & \\lshr{28.88}        & \\lshr{32.11}       \\\\\n    \\cmidrule{2-8}\n    &\\multicolumn{3}{l}{\\textbf{\\lshr{1D SMILES + 2D Graph}}}  & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} \\\\\n    &\\lshr{2D-MoLM}    & \\lshr{27.15}        & \\lshr{21.19}       & \\lshr{36.02}        & \\lshr{20.76}       & \\lshr{29.12}        & \\lshr{32.28}       \\\\\n    \\cmidrule{2-8}\n    &\\multicolumn{3}{l}{\\textbf{\\lshr{1D SMILES + 3D Conformation}}}  & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} & \\multicolumn{1}{l}{} \\\\\n    &\\lshr{3D-MoLM$\\dag$}    & \\lshr{\\textbf{29.25}}        & \\lshr{\\textbf{22.07}}       & \\lshr{\\underline{36.48}}        & \\lshr{\\textbf{21.80}}       & \\lshr{\\textbf{30.95}}        & \\lshr{\\underline{33.12}}       \\\\\n    &\\lshr{3D-MoLM}    &     \\lshr{\\underline{28.95}}        & \\lshr{\\underline{21.63}}        & \\lshr{\\textbf{36.51}}        & \\lshr{\\underline{21.26}}        & \\lshr{\\underline{30.02}}        & \\lshr{\\textbf{33.55}}       \\\\ \n    \\bottomrule\n    \\addlinespace[0.1cm]\n    \\end{tabular}\n\\end{subtable}\n\n\\begin{subtable}[t]{\\textwidth}\n\\centering\n\\caption{A sample of molecule captioning.}\n    \\begin{tabular}{M{2.25cm} M{5cm} M{5cm}}\n        \\toprule\n        \\scriptsize{Molecule} & Ground Truth & 3D-MoLM  \\\\\n        \\midrule\n        \\includegraphics[width=0.17\\textwidth]{figures/cap-molecule-1-v2.png} & \n        \\lsh{Globostellatic acid} C is a \\lsh{tricyclic triterpenoid} of the isomalabaricane group. \\lsh{It has a role as} an antineoplastic agent and \\lsh{a metabolite}. \\lsh{It is a tricyclic triterpenoid}, \\lsh{an acetate ester}, an enone, an ether and an oxo \\lsh{monocarboxylic acid}. It is a conjugate acid of a globostellatate C(1-). & \n        \\lsh{Globostellatic acid} B is a \\lsh{tricyclic triterpenoid} with formula C34H48O6, originally isolated from Globostellata. \\lsh{It has a role as a metabolite}. \\lsh{It is a tricyclic triterpenoid}, \\lsh{a monocarboxylic acid} and \\lsh{an acetate ester}. Globostellatic acid B is a natural product found in Globostellata with data available. \\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{subtable}\n\\label{tab:molecule-captioning}\n\\vspace{-20pt}\n\\end{table*}\n\n\\vspace{-2mm}\n\\subsection{Molecule Captioning}\\label{sec:captioning}\n\\vspace{-1mm}\n\\lshr{\\textbf{Generalist and Specialist Models.} For generation tasks of molecule captioning and open-text molecule QA, we present 3D-MoLM's performances of two different variants: the specialist, which is fine-tuned for each generation task separately, and the generalist, which is a unified model trained on all the generation tasks. Both models are obtained by fine-tuning the Stage-2 checkpoint.\n}\n\nHere we evaluate 3D-MoLM for molecule captioning on the PubChem Dataset. \nWe opt not to use CheBI-20~\\citep{molt5} for two reasons: firstly, its origin from PubChem poses potential data leakage issues, akin to the concerns with PCDes; secondly, CheBI-20 is curated in a way that molecular names are replaced with ``the molecule'', driving the model to emphasize properties instead of names. \nHowever, the molecular nomenclature system is intrinsically related to the identification of distinct molecular structures, encompassing features such as hydrocarbon chains and benzene rings. \nConsequently, a model's ability to accurately predict these names serves as a testament to its adeptness in comprehending the underlying molecular structures.\nThus, we elevate the task's complexity by retaining molecular names in the texts, positioning this task as a combination of molecule captioning without molecular names~\\citep{molt5}, and name prediction~\\citep{iupac-name}.\n\\lshr{To demonstrate the effectiveness of 3D molecular perception, we include 3D-MoLM's variants of 1D (\\ie Llama2-7B) and 2D (\\ie 2D-MoLM) perception as baselines.\nSpecifically, Llama2-7B, without a molecule-text projector, goes through Stage 2 \\& 3 training using 1D SMILES as molecule representations.\n2D-MoLM replaces the 3D molecular encoder with a 2D molecular encoder~\\citep{graphmvp}, and undergoes the same training process as 3D-MoLM.\nSpecialist models are fine-tuned using the training set from the PubChem Dataset. }\nTable~\\ref{tab:molecule-captioning} presents the performances with metrics of BLEU, ROUGE, and METEOR, accompanied by a concrete sample.\nWe observe that:\n\n1) 3D-MoLM demonstrates superior performance across the board, with the highest scores on all evaluation metrics. \nWhile slightly erring in identifying the molecule as ``Globostellatic acid B'' rather than ``Globostellatic acid C'', it pinpoints roles and primary structures such as the tricyclic triterpenoid, acetate ester, and monocarboxylic acid.\nThis demonstrates the effectiveness of 3D molecule-text alignment training to bridge the gap between 3D molecular representations and LM's input space.\n\\lshr{We also provide detailed analysis and discussion on failure cases in Appendix \\ref{app:failure-case}.}\n \n2) The enrichment process via GPT bolsters the text generation capability based on 3D molecular structures. \nThis underscores the hypothesis that the enrichment strengthens the connection between molecular structures and their associated properties, enhancing cross-modal molecular understanding.\nSimilar gain can be observed in the following open-text QA tasks as well.\n\n3) 3D-MoLM's heightened performance, when juxtaposed with finetuned Llama2-7B and 2D-MoLM subjected to a similar training process but modeling molecules as 1D SMILES and 2D graphs, highlights the pivotal role of 3D structure perception in bolstering molecular understanding.\n\n\\subsection{Open-text Molecular Question-Answering (QA)}\\label{sec:open-text-qa}\n\nWe evaluate 3D-MoLM for open-text molecular QA on the 3D-MoIT dataset. \nConsidering that open-text molecular QA is mostly unexplored in existing works, we mainly compare 3D-MoLM with its variants of 1D or 2D molecular perceptions. \\lshr{Notably, we report performances of specialists, which are trained for each task separately, and generalists, which are unified models trained on all the generation tasks.} Table~\\ref{tab:open-text-qa} presents the quantitative evaluations and QA samples. We observe that:\n\n\\textbf{Observations for Descriptive Property QA:} 1) Superiority of 3D-MoLM over baselines. \nIt exhibits a commendable performance in 3D molecule-text interpretation, clearly surpassing the baselines. \nSpecifically, It correctly identifies beryllium acetate as a solid that is soluble in water and goes beyond the ground truth by providing supplementary details, such as pinpointing the elevated melting and boiling points and attributing them to the potent intermolecular forces between beryllium and acetate, which underlines its depth of molecular understanding.\n\n2) Benefit of descriptive instructions. \nIncorporating them amplifies the model's molecular comprehension. \nThis can be verified by that through instruction tuning, Llama2-7B (generalist) advances over its initialization point, manifesting improvements of 2.46 in BLEU-2 and 3.22 in METEOR.\n\n\\textbf{Observations for Computed Property QA:} 1) 3D-MoLM achieves superior performances on computed property QA task, consistently achieving the lowest MAE, especially on those properties intrinsically determined by 3D conformations (\\ie highlighted properties in Table \\ref{tab:computed-qa}).\nA remarkable performance lead of 0.77 eV among generalists on HOMO-LUMO Gap accentuates 3D-MoLM's adeptness at 3D molecular understanding, which we attribute to its 3D perception. \nHowever, for properties that mainly stem from atomic compositions and interatomic connections (\\ie molecule weight, LogP, TPSA, and complexity), the advantage, while persistent, is more subdued. \nThis aligns with Uni-Mol, which displays larger advantages over 2D molecular models for predicting quantum mechanical properties grounded in 3D structures.\n\n2) Efficacy of instruction tuning. \nInstruction tuning amplifies the model's capability to adhere to and act upon instructions. \nThis is illuminated by Llama2-7B's official checkpoint's occasional ineptitude in generating valid numerical responses, particularly when tasked with approximating 3D-dependent computed properties that aren't straightforwardly derivable from SMILES.\n\n\\lshr{\n3) Comparison with Uni-Mol. 3D-MoLM can enhance the accuracy of molecular property prediction by leveraging both the rich contextual knowledge found in chemistry literature and 3D molecular conformations. \nFor instance, the pretraining corpus of 3D-MoLM contains descriptions of hydrophobicity (LogP) and solubility (TPSA). \nWhile Uni-Mol excels at predicting molecular properties by interpreting 3D conformations, it cannot utilize textual descriptions of chemical properties. \nThis dual-source knowledge utilization can potentially enhance the prediction of molecular properties.\n}\n\n\\lshr{\\textbf{Observations for generalist and specialist:} While the generalist model slightly underperforms in comparison to the specialist models, it still exhibits a performance gain over other baselines. This underscores 3D-MoLM's versatility and capability to effectively handle multiple tasks.}\n\n\\begin{table*}[t]\n\\centering\n\\caption{Open-text QA results on 3D-MoIT.\n* denotes the official checkpoint without any finetuning.\n$\\dag$ denotes molecule-text alignment on the original PubChem texts without GPT-3.5 enrichment.\nLlama2-7B, without a molecule-text projector, goes through Stage 3 instruction tuning by modeling molecules as 1D SMILES.\n2D-MoLM replaces the 3D molecular encoder with a 2D molecular encoder and goes through three-stage training as 3D-MoLM.\n}\n\\scriptsize\n\\vspace{-4pt}\n\\begin{subtable}[t]{\\textwidth}\n\\centering\n\\caption{Descriptive property QA results.}\n\\vspace{-4pt}\n\\begin{tabular}{llcccccc}\n\\toprule\nType& Model &  BLEU-2 & BLEU-4 & ROUGE-1 & ROUGE-2 & ROUGE-L & METEOR \\\\\n\\midrule\n\\multirow{4}{*}{Specialist} \n&Llama2-7B        &  28.15 & 23.24 & 35.14 & 22.08  &  30.41 & 46.87 \\\\\n&2D-MoLM        & \\underline{30.84} & \\underline{25.09} &  38.46 & \\underline{24.22}  & \\underline{33.04}  & 50.92 \\\\\n&3D-MoLM$\\dag$        &  30.33 & 24.47 & \\underline{38.48}  & 23.93  & 32.98  & \\underline{51.33} \\\\\n&3D-MoLM        &  \\textbf{32.00} & \\textbf{26.13} & \\textbf{40.13}  & \\textbf{25.55}  & \\textbf{34.64}  & \\textbf{52.15} \\\\\n\\midrule\n\\multirow{4}{*}{\\lshr{Generalist}} \n&\\lshr{Llama2-7B*}        & \\lshr{25.22}  & \\lshr{21.16} & \\lshr{31.48} & \\lshr{19.21}  & \\lshr{25.22}  & \\lshr{43.17} \\\\\n&\\lshr{Llama2-7B}        &  \\lshr{27.68} & \\lshr{22.81} & \\lshr{34.73} & \\lshr{21.55}  &  \\lshr{29.91} & \\lshr{46.39} \\\\\n&\\lshr{2D-MoLM}        & \\lshr{\\underline{30.23}} & \\lshr{\\underline{24.57}} &  \\lshr{37.85} & \\lshr{22.95}  & \\lshr{32.28}  & \\lshr{50.08} \\\\\n&\\lshr{3D-MoLM$\\dag$}        &  \\lshr{29.92} & \\lshr{24.44} & \\lshr{\\underline{38.62}}  & \\lshr{\\underline{22.83}}  & \\lshr{\\underline{32.30}}  & \\lshr{\\underline{50.81}} \\\\\n&\\lshr{3D-MoLM}        &  \\lshr{\\textbf{31.81}} & \\lshr{\\textbf{26.08}} & \\lshr{\\textbf{40.13}}  & \\lshr{\\textbf{25.87}}  & \\lshr{\\textbf{34.99}}  & \\lshr{\\textbf{51.93}} \\\\\n\\bottomrule\n\\addlinespace[0.1cm]\n\\end{tabular}\n\\end{subtable}\n\\begin{subtable}[t]{\\textwidth}\n\\centering\n\\caption{A concrete sample of descriptive property QA.}\n\\vspace{-4pt}\n    \\begin{tabular}{M{1.25cm} M{2.2cm} M{3cm} M{5.75cm}}\n        \\toprule\n        \\scriptsize{Molecule} & Question & Ground truth & 3D-MoLM   \\\\\n        \\midrule\n        \\includegraphics[width=0.075\\textwidth]{figures/desc-molecule-1-v2.png} & \n        What are the physical properties of beryllium acetate? & Beryllium acetate is typically a \\lsh{white} or colorless \\lsh{solid} and is \\lsh{soluble in water}. &\n        On the physical properties of beryllium acetate, it is a \\lsh{white} crystalline \\lsh{solid} that is \\lsh{soluble in water}. It has a relatively high melting point and boiling point due to the strong intermolecular forces between the beryllium and acetate. \\\\\n        \\bottomrule\n        \\addlinespace[0.1cm]\n    \\end{tabular}\n\\end{subtable}\n\\begin{subtable}[t]{\\textwidth}\n\\centering\n\\caption{Computed property QA results. We report the MAE with a valid answer rate (\\%) -- LMs sometimes fail to follow instructions to generate numerical responses.\nWe \\textcolor{mygreen}{highlight} properties largely dependent on 3D structures.}\n{\\setlength{\\tabcolsep}{1.5pt}\n\\begin{scriptsize}\n\\begin{tabular}{llcccccccc}\n\\toprule\nType& Model &Weight (g/mol)&LogP&TPSA (\\text{\\AA}$^2$)&Complexity&\\textcolor{mygreen}{HOMO (eV)}&\\textcolor{mygreen}{LUMO (eV)}&\\textcolor{mygreen}{H-L Gap (eV)}&\\textcolor{mygreen}{SCF ($10^4$eV)}\\\\\n\\midrule\n\\lshr{Non-LM} & \\lshr{Uni-Mol} & \\lshr{20.35} &\\lshr{0.59}&\\lshr{13.48}&\\lshr{57.24}&\\lshr{0.32}&\\lshr{0.35}&\\lshr{0.21}&\\lshr{0.45}\\\\\n\\midrule\n\\multirow{4}{*}{Specialist}\n&Llama2-7B        &  22.10 (96\\%) & 1.45 (95\\%) & 15.87 (92\\%) &  69.74 (93\\%) & 1.24 (96\\%)  & 1.04 (95\\%) & 0.88 (92\\%) & 0.70 (99\\%)\\\\\n&2D-MoLM        & 21.48 (94\\%) & \\underline{0.88} (96\\%) &  13.52 (92\\%) & 55.74 (94\\%)  & 0.92 (98\\%)  & 0.80 (96\\%) & 0.67 (93\\%) & 0.71 (99\\%)\\\\\n&3D-MoLM$\\dag$        & \\underline{16.18} (96\\%) & 0.95 (96\\%) &  \\underline{10.26} (94\\%) & \\underline{49.15} (95\\%)  & \\underline{0.45} (98\\%)  & \\underline{0.36} (96\\%) & \\underline{0.41} (94\\%)&\\underline{0.39} (99\\%)\\\\\n&3D-MoLM        & \\textbf{14.79} (95\\%) & \\textbf{0.66} (97\\%) & \\textbf{9.71} (93\\%) & \\textbf{44.85} (94\\%) & \\textbf{0.26} (97\\%) &  \\textbf{0.25} (94\\%) & \\textbf{0.28} (94\\%) & \\textbf{0.35} (99\\%)\\\\\n\\midrule\n\\multirow{5}{*}{\\lshr{Generalist}}\n&\\lshr{Llama2-7B*}        & \\lshr{42.18 (82\\%)} & \\lshr{2.10 (85\\%)} & \\lshr{27.11 (84\\%)} & \\lshr{121.87 (76\\%)} & \\lshr{2.87 (70\\%)}  & \\lshr{1.89 (71\\%)} & \\lshr{1.86 (70\\%)} & \\lshr{3.84 (23\\%)}\\\\\n&\\lshr{Llama2-7B}        &  \\lshr{27.42 (92\\%)} & \\lshr{1.78 (93\\%)} & \\lshr{17.07 (90\\%)} &  \\lshr{78.16 (92\\%)} & \\lshr{1.89 (90\\%)}  & \\lshr{1.26 (90\\%)} & \\lshr{1.25 (91\\%)} & \\lshr{0.87 (99\\%)}\\\\\n&\\lshr{2D-MoLM}        & \\lshr{20.80 (92\\%)} & \\lshr{1.36 (94\\%)} &  \\lshr{12.47 (89\\%)} & \\lshr{\\underline{52.70} (91\\%)}  & \\lshr{1.52 (93\\%)}  & \\lshr{1.13 (92\\%)} & \\lshr{1.09 (88\\%)} & \\lshr{0.96 (99\\%)}\\\\\n&\\lshr{3D-MoLM$\\dag$}        & \\lshr{\\underline{19.54} (93\\%)} & \\lshr{\\underline{0.92} (92\\%)} &  \\lshr{\\underline{11.14} (92\\%)} & \\lshr{54.68 (90\\%)}  & \\lshr{\\underline{0.65} (94\\%)}  & \\lshr{\\underline{0.41} (92\\%)} & \\lshr{\\underline{0.55} (89\\%)} &\\lshr{\\underline{0.49} (99\\%)}\\\\\n&\\lshr{3D-MoLM}        & \\lshr{\\textbf{16.58} (92\\%)} & \\lshr{\\textbf{0.78} (95\\%)} & \\lshr{\\textbf{10.90} (90\\%)} & \\lshr{\\textbf{45.49} (89\\%)} & \\lshr{\\textbf{0.35} (95\\%)} &  \\lshr{\\textbf{0.36} (93\\%)} & \\lshr{\\textbf{0.32} (90\\%)} & \\lshr{\\textbf{0.38} (98\\%)}\\\\\n\\bottomrule\n\\addlinespace[0.1cm]\n\\end{tabular}\n\\end{scriptsize}\n}\n\\label{tab:computed-qa}\n\\end{subtable}\n\\begin{subtable}[t]{\\textwidth}\n\\centering\n\\caption{Concrete samples of computed property QA.}\n    \\begin{tabular}{M{1.8cm} M{5cm} M{2.65cm} M{2.5cm}}\n        \\toprule\n        \\scriptsize{Molecule} & Question & Ground truth & 3D-MoLM   \\\\\n        \\midrule\n        \\includegraphics[width=0.12\\textwidth]{figures/comp-molecule-1-v2.png} & \n        Please provide the Molecular Weight value for this molecule. If uncertain, provide an estimate. Respond with the numerical value only. & The Molecular Weight for the input molecule is \\lsh{286.28 g/mol}. &\n        Input molecule has a Molecular Weight of \\lsh{288.30 g/mol}. \\\\\n        \\midrule\n        \\includegraphics[width=0.12\\textwidth]{figures/comp-molecule-2-v2.png} & \n        Could you give me the HOMO-LUMO Gap value of this molecule? If uncertain, provide an estimate. Respond with the numerical value only. & The HOMO-LUMO Gap for the input molecule is \\lsh{5.325 eV}. &\n        The HOMO-LUMO Gap for the input molecule is \\lsh{5.762 eV}. \\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{subtable}\n\\label{tab:open-text-qa}\n\\vspace{-16pt}\n\\end{table*}\\vspace{-0.1cm}\n\\section{Conclusion}\n\\vspace{-0.4cm}\nIn this work, we introduce 3D-MoLM, a new approach tailored for 3D-molecular language modeling. \n3D-MoLM equips an LM with a 3D molecular encoder for 3D molecule-text interpretation.\nThis is achieved by a 3D molecule-text projector that aims to map 3D molecular representations into the LM's textual space. \nFurthermore, 3D-MoLM incorporates 3D molecule-centric instruction tuning, enhancing both its adherence to human instructions and 3D molecular understanding. \nExtensive experiments demonstrate that 3D-MoLM excels in various tasks, including molecule-text retrieval, molecule captioning, and open-text molecular QA.\n\nDespite the promising results, our work has a few limitations. Compared with vision-language modeling methods~\\citep{instructblip, 3D-LLM}, the scale of our 3D molecule-text dataset is notably constrained, inherently limiting the performance.\nThis motivates our search for high-quality texts closely related to 3D molecular structures.\nFurthermore, this study does not explore other intriguing capabilities of large LMs, such as in-context learning and chain-of-thought reasoning. \nMoreover, integrating principles of invariant learning \\citep{InvCF,AdvDrop} can help enhance generalization and mitigate potential hallucinations.\\section{Acknowledgements}\nThis research is supported by the National Natural Science Foundation of China (92270114) and partially supported by the National Research Foundation Singapore under the AI Singapore Programme (AISG Award No: AISG2-TC-2023-010-SGIL), the Singapore Ministry of Education Academic Research Fund Tier 1 (Award No: T1 251RES2207) and the Google Cloud Research Credits program (Award No: 6NW8-CF7K-3AG4-1WH1).  \nThis research is also supported by the CCCD Key Lab of Ministry of Culture and Tourism and NExT Research Center.\n\n\\newpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Large Language Models are In-Context Molecule Learners}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nLarge Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts.\nHowever, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs.\nTo resolve the challenges, we propose \\textbf{I}n-\\textbf{C}ontext \\textbf{M}olecule \\textbf{A}daptation (\\textbf{ICMA}), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning. \nSpecifically, ICMA incorporates the following three stages: Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning.\nInitially, Hybrid Context Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve informative context examples. \nAdditionally, we also propose Post-retrieval Re-ranking with Sequence Reversal and Random Walk to further improve the quality of retrieval results. \nFinally, In-Context Molecule Tuning unlocks the in-context molecule learning capability of LLMs with retrieved examples and adapts the parameters of LLMs for the molecule-caption translation task.\nExperimental results demonstrate that ICMA can empower LLMs to achieve state-of-the-art or comparable performance without extra training corpora and intricate structures, showing that LLMs are inherently in-context molecule learners.\n\\end{abstract}\n \n\n\\section{Introduction}\nMolecules play a crucial role across various fields, such as medicine ~\\cite{ding2019selective}, agriculture ~\\cite{twyman2003molecular}, and material science ~\\cite{higuchi2023material}, as they are widely used in the development of drugs, fertilizers, and advanced materials. Recently, LLMs have demonstrated remarkable success in the molecular domain, as molecules can be represented as Simplified Molecular-Input Line-Entry System (SMILES) strings ~\\cite{weininger1988smiles}, which can be comprehended and generated by LLMs in a similar manner to natural languages. \nTo further bridge the gap between the molecules and natural languages, ~\\citet{edwards-etal-2022-translation} proposes the molecule-caption translation task, which comprises two sub-tasks: molecule captioning (Mol2Cap) and text-based de novo molecule generation (Cap2Mol). Specifically, Mol2Cap involves generating a textual description that elucidates the features of the given molecule, while Cap2Mol focuses on predicting the exact molecule based on the textual caption. The study of the molecule-caption translation task offers an accessible and chemist-friendly venue for molecule discovery, which has raised wide research focus.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\columnwidth]{figures/introduction.pdf}\n    \\caption{An illustration of three similar molecules alongside their molecule captions. The molecules are represented as both SMILES strings and graphs, while the molecule captions elucidate their structures and functions. Here, the three molecules are similar, considering their 2D graph embeddings, and the overlaps in their captions are highlighted in blue and pink.}\n    \\label{fig:intro}\n    \\vskip -0.2in\n\\end{figure}\n\nGenerally, there are two main paradigms for adapting LLMs to the molecule-caption retrieval task. \nThe first paradigm is the domain-specific pre-training \\& fine-tuning. For instance, MolT5 ~\\cite{edwards-etal-2022-translation} first proposes and handles the molecule-caption translation task as the language translation task, pre-training the MolT5 model with chemical corpora like PubChem ~\\cite{kim2019pubchem} and then fine-tuning the model on the ChEBI-20 dataset ~\\cite{edwards2021text2mol}. Additionally, MoMu ~\\cite{su2022molecular} and MolCA ~\\cite{liu2023molca} introduce an extra modality alignment stage before fine-tuning on downstream tasks, which aligns the output of 2D molecule graph encoder with the input space of LLMs.\nIn contrast, the other paradigm involves prompting and utilizing the in-context learning capability of LLMs. For example, MolReGPT ~\\cite{li2023empowering} introduces In-Context Few-Shot Molecule Learning, prompting general LLMs like GPT-3.5 and GPT-4 to achieve competitive performance without extra parameter adaptations.\n\nHowever, the current paradigms still face challenges. On one hand, the domain-specific pre-training \\& fine-tuning paradigm requires extra pre-training stages (i.e., domain-specific pre-training and modality alignment), which is challenging due to the scarcity of high-quality chemical datasets, especially molecule-caption pairs, making this paradigm difficult to scale up to the most advanced LLMs with billion parameters.\nBesides, the domain-specific pre-training \\& fine-tuning paradigm also suffers from weak alignment between molecules and texts, as phrases in molecule captions often indicate specific sub-structures of molecules rather than the entire molecule.\nDespite attempts to introduce extra modalities for better alignment \\cite{su2022molecular, liu2023molca}, the integration of the additional modalities (e.g., 2D molecule graph) is still focused on the entire graph level and can only be applied to the molecule captioning task, while ignoring the text-based generation of molecules, which is much more valuable for drug discovery.\nOn the other hand, the in-context learning \\& prompting paradigm puts a harsh requirement on LLMs' emergent capabilities, such as reasoning and in-context learning abilities. However, LLMs with these emergent capabilities usually have billions of parameters, making them computationally expensive.\nConsequently, there is a demand for a unified and efficient approach that effectively enhances the performance of the most advanced LLMs in both two sub-tasks of molecule-caption translation.\n\nIn this case, we propose \\textbf{I}n-\\textbf{C}ontext \\textbf{M}olecule \\textbf{A}daptation (\\textbf{ICMA}) as a new paradigm for adapting LLMs in molecule-caption translation. \nDifferent from previous paradigms, ICMA aims to instruct LLMs to derive knowledge from informative context examples, especially the alignment between molecule SMILES representations and captions, via In-Context Molecule Tuning. As shown in Figure \\ref{fig:intro}, similar molecules often share similar properties, as indicated by the overlaps among molecule captions. Conversely, similar captions tend to describe molecules with similar SMILES representations. In this case, with ICMA, general LLMs could fulfill their reasoning and in-context learning capability to better grasp the alignment between molecules and textual captions from context examples, thereby achieving better performance.\n\nSpecifically, ICMA incorporates three stages: Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning.\nIn the initial stage, Hybrid Context Retrieval, we employ Caption Retrieval and Molecule Graph Retrieval to fetch similar captions and molecules, respectively.\nSubsequently, we introduce the Post-retrieval Re-ranking stage to enhance the quality of the retrieval algorithms. This stage incorporates two innovative strategies: Sequence Reversal and Random Walk, which aim to refine and reprioritize the retrieved examples.\nFinally, we apply In-context Molecule Tuning to adapt the parameters of LLMs, enabling them to learn from the contextual mappings and effectively utilize the learned knowledge for the current generation task.\nExperiments are conducted across two real-world molecule-caption translation datasets, ChEBI-20 and PubChem324k. Results show that ICMA could enable LLMs to achieve state-of-the-art or comparable performance in both the two sub-tasks (i.e., Mol2Cap and Cap2Mol). Meanwhile, we also study the factors related to the model performance, including retrieval algorithms, context settings (i.e., context example number and maximum input length), and model scales. Lastly, ablation studies are conducted to justify the effectiveness of Post-retrieval Re-ranking components.\n\nOur contribution mainly lies in:\n\\begin{itemize}\n    \\item We propose In-context Molecule Adaptation (ICMA) to improve the performance of LLMs in the molecule-caption translation task. ICMA could empower the reasoning and in-context learning capabilities of LLMs for better alignment between molecules and texts.\n    \\item We implement ICMA through three stages, including Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-Context Molecule Tuning, significantly enhancing the informativeness of context examples.\n    \\item We conduct synthetic experiments, and the results show that our method enables LLMs to outperform previous paradigms, enabling better alignment between molecules and texts. Notably, our approach elevates the Mistral-7b model to establish state-of-the-art or comparable performance across both the two sub-tasks of molecule-caption translation, achieving 0.581 BLEU-4 score in Mol2Cap and 0.460 exact-matched score in Cap2Mol. Additionally, we comprehensively study the mechanism and influential factors of ICMA, showing that LLMs are inherently in-context molecule learners.\n\\end{itemize}\n\n\\section{Related Work}\n\\label{sec:relatedwork}\nIn this section, we discuss the related work of molecule-caption translation and the development of in-context learning.\n\n\\subsection{Molecule-Caption Translation}\nInspired by the image captioning task, \\citet{edwards2021text2mol} introduces a new dataset, ChEBI-20, with pairs of molecules and manually labeled captions that describe the molecular properties. The molecule-caption translation task was initially proposed in MolT5~\\cite{edwards-etal-2022-translation}. Meanwhile, MolT5 proposes a T5 model that is jointly pre-trained on molecule SMILES and general text corpus.\nMolXPT ~\\cite{liu2023molxpt} pre-trains a GPT model by introducing extra-wrapped texts as the pre-training corpus, demonstrating better molecule-text alignment.\nHowever, the generation of SMILES strings suffers from the problem of invalid SMILES due to the mismatches of brackets.\nTo overcome the generation issue of SMILES strings, BioT5 ~\\cite{pei2023biot5} introduces Self-referencing Embedded Strings (SELFIES) ~\\cite{krenn2020self} instead of SMILES strings to represent molecules in LLMs and proposes the BioT5 model that is jointly trained on single-modal data, wrapped text data, and molecule/protein-description pairs.\nMeanwhile, as molecules can also be represented as graphs, some methods focus on molecule understanding by introducing molecule graph information.\nFor example, MoMu ~\\cite{su2022molecular} first proposes a graph encoder to encode molecule graph information and utilizes contrastive learning to bridge the semantic gap between the graph encoder and the LLM.\nTo better fuse the molecule graph information, MolCA ~\\cite{liu2023molca} follows the BLIP-2 and utilizes a Q-Former to project the output of the graph encoder into the LLMs, showing better molecule understanding performance.\nHowever, most of these methods still adhere to the pre-training \\& fine-tuning paradigm, which necessitates the joint pre-training of LLMs on both general text and extra chemical domain corpora. As the size of the training corpora and model weights of LLMs continue to increase, this approach has become extremely inefficient.\nTo address this issue, MolReGPT ~\\cite{li2023empowering} proposes the In-Context Few-Shot Molecule Learning to enable LLMs to learn the molecule-caption translation task from the context examples without modifying the model weights while still achieving comparable performance to these fine-tuned methods.\n\n\\subsection{In-Context Learning}\nWith the scaling of model size and corpus size ~\\cite{brown2020language, chowdhery2023palm}, LLMs emerge the in-context learning capability ~\\cite{wang2022self}, which enables LLMs to learn from contexts augmented with several examples ~\\cite{dong2022survey}. \nBy utilizing the capability of ICL, LLMs can solve complex tasks without the necessity of being fine-tuned. \nFor instance, with a few examples, GPT-3 could demonstrate similar performance to fine-tuned models in unseen tasks ~\\cite{brown2020language}.\nWhat's more, based on context examples, LLMs could achieve better mathematical reasoning ability with the assistance of chain-of-thought (CoT) ~\\cite{wei2022chain}.\nWith the powerful in-context learning capabilities, ~\\citet{edwards2023synergpt} proposes in-context drug synergy learning to apply LLMs for personalized drug synergy prediction and drug design, while ~\\citet{jablonka2024leveraging} fine-tunes LLMs such as GPT-3 with chemical questions and answers to solve predictive tasks\nIn the scenario of molecule-caption translation, MolReGPT ~\\cite{li2023empowering} proves the retrieval quality is closely related to the model performance, while it still imposes strict requirements on the model scale, as the in-context learning capability only becomes apparent when the model weights reach a certain size.\\section{In-Context Molecule Adaptation}\n\\label{sec:methodlogy}\n\\begin{figure*}[htb]\n    \\centering\n    \\vskip -0.2in\n    \\includegraphics[width=\\textwidth]{figures/model_structure.pdf}\n    \\vskip -0.1in\n    \\caption{Framework of In-Context Molecule Adaptation (ICMA). Generally, ICMA consists of three stages, Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-Context Molecule Tuning. \n    }\n    \\label{fig:model}\n    \\vskip -0.2in\n\\end{figure*}\n\nIn this section, we introduce In-Context Molecule Adaptation (ICMA) as a novel paradigm to adapt LLMs to molecule-caption translation. As shown in Figure~\\ref{fig:model}, ICMA incorporates three stages, including Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Specifically, Hybrid Context Retrieval first retrieves $N$ rough examples from the training set $\\mathcal{D}$ by calculating the similarity between the current query and molecule-caption pairs. After that, Post-retrieval Re-ranking with Random Walk and Sequence Reversal is adopted to obtain $n$ refined examples from the $N$ rough examples. Finally, the In-context Molecule Tuning can be performed to update the parameters of LLMs to learn the molecule-text alignment from refined context examples. \n\n\\subsection{Hybrid Context Retrieval}\nThe retrieval quality is closely related to the informativeness of context examples. For example, if the retrieved molecules are more similar to the current query molecule, they are likely to exhibit more overlaps in their respective caption, which could enable better alignments between molecules and texts. Therefore, the development of retrieval algorithms plays a crucial role in ICMA. In this work, we introduce Hybrid Context Retrieval, which adopts hybrid modalities (i.e., 2D molecule graph and text), as well as hybrid retrieval algorithms designed for the specific tasks (i.e., Molecule Graph Retrieval for Mol2Cap and BM25 Caption Retrieval for Cap2Mol). \n\nFor the \\textbf{Mol2Cap} task, ICMA adopts Molecule Graph Retrieval to better refine the retrieval quality.\nPreviously, MolReGPT ~\\cite{li2023empowering} utilizes the Morgan Fingerprints (Morgan FTS) and Dice similarity to evaluate the similarity between molecules, which encodes the pre-defined handcraft structures as the embedding of the molecule. However, Morgan FTS are typically based on handcrafted chemical feature extraction, which may have limited capability to capture the comprehensive information of complex molecule structures and properties. On the other side, Graph Neural Networks (GNNs), pre-trained on millions of molecules, could effectively capture topological structures of molecule graphs, providing complete chemical semantics~\\cite{ijcai2023p760}. This makes GNNs a better option for molecule similarity calculation.\nIn ICMA, we adopt a pre-trained GNN encoder to obtain the molecule graph embeddings:\n\\begin{align}\n    \\textbf{e}_m = G\\!N\\!N(g_m),\n\\end{align}\nwhere $\\textbf{e}_m$ denotes the embedding of the given 2D graph $g_m$ of the molecule $m$. Specifically, we adopt Mole-BERT ~\\cite{xia2022mole} as the GNN encoder. \n\nSubsequently, cosine similarity is leveraged to evaluate the similarity between the current query molecule graph $m^q$ and the other molecule graphs $m^i$ in the training set of molecules ($m^i \\in \\mathcal{D}_m$). Thus, the molecule similarity ranking function $\\mathcal{R}^m$ can be represented as:\n\\begin{align}\n    \\mathcal{R}^m(m^q, m^i) = \\cos{(\\textbf{e}_{m^q}, \\textbf{e}_{m^i})}.\n\\end{align}\nIn the \\textbf{Cap2Mol} task, we inherit the BM25 Caption Retrieval ~\\cite{robertson2009probabilistic} from MolReGPT ~\\cite{li2023empowering} as it focuses on the detail matching of molecule captions, showing competitive performance and is much faster than LLM-based methods like Sentencebert ~\\cite{reimers2019sentence}. Specifically, given captions in the test set as query captions $Q_c$ and the training set of captions $\\mathcal{D}_c$, the caption similarity ranking function $\\mathcal{R}^c$ can be denoted as:\n\n{\\tiny\n\\begin{align}\n    \\mathcal{R}^c\\!(\\!Q_c\\!,\\!\\mathcal{D}_c\\!)\\!=\\!\\sum_{i=1}^T\\!I\\!D\\!F(\\!c^q_i\\!)\\!*\\!\\frac{t\\!f(c^q_i,\\mathcal{D}_c)*(k_1+1)}{t\\!f(\\!c^q_i,\\!\\mathcal{D}_c\\!)\\!+\\!k_1\\!*\\!(\\!1\\!-\\!b\\!+\\!b\\!*\\!\\frac{|\\mathcal{D}_c|}{avgdl}\\!)},\n\\end{align}\n}\n\n\\noindent where $T$ is the number of query terms in the query caption, $c^q_i$ is the $i$-th query term, $I\\!D\\!F(c^q_i)$ is the inverse document frequency of $c^q_i$, $t\\!f(c^q_i, \\mathcal{D}_c)$ is the term frequency of $c^q_i$ in $\\mathcal{D}_c$, $k_1$ and $b$ are hyperparameters, $|\\mathcal{D}_c|$ is the length of $\\mathcal{D}_c$, and $avgdl$ is the average caption length in the corpus.\n\n\\subsection{Post-retrieval Re-ranking}\nAlthough refined retrieval algorithms could bring better retrieval quality, there are still some problems considering the arrangement of context examples. Thus, we propose Post-retrieval Re-ranking with Random Walk and Sequence Reversal to re-rank the priorities and positions of context examples, thus enhancing In-Context Molecule Tuning quality.\n\n\\subsubsection{Random Walk}\nThe molecules ranked top by retrieval algorithms can sometimes share too many overlaps, impairing the informativeness of context examples. \nIn this case, for the context diversity and generalization performance of ICMA, it is necessary to give these less similar examples a chance to be visited.\nInspired by the random walk mechanism in graph theory, we propose random walk as a post-retrieval method to select examples from the top-$N$ retrieved results so that examples with lower rank still have a chance to be selected, which provides more useful information and complements the context diversity.\nMathematically, we adopt a dynamic chance for examples with different ranks. Specifically, for the $j$-th example in the $N$ rough results, where $1\\leq j \\leq $N, the possibility of skipping is represented as $(9*\\frac{N-j}{N-1}) \\%$, which reduces as the rank moves down to ensure that at least one example is selected for the context. \nIf the $j$-th example is skipped, then it will perform a Random Walk in the remaining examples until an example is selected. Practically, if $N$ is too large (i.e., $N \\gg n$), the retrieval quality will be impaired, while if $N$ is too small (i.e., $N \\approx n$), it might hurt the diversity. Therefore, considering $n$ is not quite large and usually less than 5 due to the context length limitation, $N$ is set to 10 in this work.\n\n\\subsubsection{Sequence Reversal}\nDue to the training strategy and the inherent characteristic of natural languages, LLMs have difficulty capturing long-range dependencies or relationships between words that are far apart in the input text, namely the distance dependency. The positions of examples in the context might influence the generation results due to the distance dependency of LLMs ~\\cite{frermann-etal-2023-conflicts}. In this case, it is of significance to put the most informative example exactly near the current input query.\n\nFormally, given the context examples, previous works like MolReGPT tend to organize the input text by directly fitting them into the context. Therefore, the context could be represented as:\n\n{\\small\n\\vskip -0.2in\n\\begin{align}\n    \\mathcal{P}(x_1,y_1)\\oplus\\mathcal{P}(x_2,y_2)\\oplus ... \\oplus \\mathcal{P}(x_n,y_n),\n\\end{align}\n\\vskip -0.1in}\nwhere $\\mathcal{P}$ denotes the prompt template and $(x_i, y_i)$ is the $i$-th refined similar molecule-caption pair, while $\\oplus$ represents the concatenation.\nObviously, $(x_n,y_n)$ is generally the least informative molecule-caption pair among $n$ refined examples, while it is the closest example to the current input query. \nHere, we propose Sequence Reversal to resolve this question by simply reversing the sequence of examples. Specifically, the context can be represented as:\n\n{\\small\n\\vskip -0.2in\n\\begin{align}\n    \\mathcal{P}(x_n,y_n)\\!\\oplus\\!\\mathcal{P}(x_{n-1},y_{n-1})\\!\\oplus...\\oplus\\! \\mathcal{P}(x_1,y_1).\n\\end{align}\n}\n\n\\subsection{In-Context Molecule Tuning}\nAs shown in Figure \\ref{fig:intro}, similar molecules typically share similar structures and chemical properties. Building on this principle, MolReGPT ~\\cite{li2023empowering} has demonstrated the effectiveness of in-context learning, which aims to prompt Large Language Models (LLMs) to learn from similar examples without the need for domain-specific pre-training and fine-tuning. However, MolReGPT heavily relies on the reasoning and in-context learning capabilities of LLMs, resulting in poor performance with relatively small language models.\nTo address the deficits \n, we propose In-Context Molecule Tuning to fine-tune the parameters of LLMs, enabling them to learn from context examples and reason on the current input.\nNotably, In-Context Molecule Tuning can be easily adapted to any LLM, allowing even smaller language models to unlock their in-context molecule learning capability by learning the differences and similarities between molecule-caption pairs in the context. \n\nFormally, given the training dataset $\\mathcal{D}$ and the parameters of the LLM $\\theta$, let the current input of the LLM be $x$ and the target output be $y$, where $(x,y) \\in D$ denotes the molecule-caption pair. Traditional supervised fine-tuning methods directly learn the mapping from the input to the output $f: x\\rightarrow y$ and the loss function $\\mathcal{L}^{ft}(\\theta)$ could be denoted as:\n\n{\\small\n\\vskip -0.2in\n\\begin{align}\n    \\mathcal{L}^{ft}(\\theta) =  \\underset{(x,y)\\in \\mathcal{D}}{\\sum}[-\\log p_\\theta(y|x)].\n\\end{align}\n\\vskip -0.1in\n}\n\nIn contrast, ICMA does not simply conduct the next token prediction on the output part but learns the entire input in an auto-regressive manner. ICMA first employs the Hybrid Context Retrieval and Post-retrieval Re-ranking to obtain a subset $D_{(x,y)} \\subset D$ containing $n$ similar examples $\\{(x_i, y_i) | 1\\leq i \\leq n\\}$, from the training set. Different from the previous ICL objective, ICMA is also motivated to learn the mapping $f_i: x_i\\rightarrow y_i$ inside the context examples in an obvious manner. Notably, molecule-caption mapping is the most informative part in the context examples because similar molecule sub-structures inherit similar characteristics. For example, RCOOH (carboxyl group) usually indicates that the molecule is an acid. In this way, learning the alignment between functional groups and molecule captions in the context examples could benefit the final prediction. For simplicity, we could assume that context examples are independent of each other as the molecule-caption mapping plays the most important role in the prediction.\nIn this case, the aggregation of mappings $\\mathcal{F}_{(x,y)}=\\{f_1, f_2, ..., f_{n}\\}$ could be learned from context and will altogether contribute to the final prediction with the corresponding context $C_{(x,y)}$, which wraps the context examples into the input text.  \nTherefore, the objective of ICMA can be represented as:\n\n\\begin{small} \n\\begin{equation} \n\\mathcal{L}(\\mathcal{F}_{(x,y)}) = \\underset{(x_i,y_i)\\in D_{(x,y)}}{\\sum}-\\log p_\\theta(y_i|x_i) ,\n\\end{equation} \n\\begin{equation}\n    \\mathcal{L}^{I\\!C\\!M\\!A}(\\theta)\\!=\\!\\underset{(x,y)\\in D}{\\sum}\\!\\left(\\!-\\log p_\\theta(y|x, C_{(x,y)})\\!+\\! \\mathcal{L}(\\mathcal{F}_{(x,y)})\\right)\\!,\n\\end{equation}\n\\end{small}\n\n\\noindent where $\\mathcal{L}(\\mathcal{F}_{(x,y)})$ represents the aggregated mapping loss for molecule-caption pair $(x,y)$, while $\\mathcal{L}^{I\\!C\\!M\\!A}(\\theta)$ denotes the overall loss function.\n\nBy learning the context examples as well as the corresponding mappings, ICMA enables LLMs to learn the alignment between molecular and textual spaces in a more explainable manner. Moreover, ICMA could effectively harness the reasoning capabilities of LLMs and seamlessly adapt general LLMs to the task of molecule-caption translation.\n\\section{Experiments}\n\\label{sec:Experiments}\nIn this section, we aim to evaluate the effectiveness of ICMA. Firstly, we introduce the experimental settings.\nThen, we compare ICMA with selected baseline models on the ChEBI-20 dataset and further test ICMA on a smaller dataset, PubChem324k.\nFurthermore, we also study the factors that will affect the performance of IMCA, including different foundation models, retrieval algorithms, context settings, and model scales.\nLastly, an ablation study is also conducted to justify the design of Post-retrieval Re-ranking components.\n\n\\subsection{Experimental Settings}\nWe will first detail our experiment settings. \nAll the hyper-parameters are illustrated in Appendix A Table \\ref{tab:hyper}. If not specifically stated, the cutoff length is set to 1024 and n\\_shot is set to 2 to control variable. Notably, for LLMs with over 7 billion parameters, we apply LoRA ~\\cite{hu2021lora} to save the GPU memory and accelerate computation. Otherwise, we fine-tune the full model of LLMs. For the dataset, we apply two different molecule-caption translation datasets,\nChEBI-20 ~\\cite{edwards2021text2mol} and PubChem324k ~\\cite{liu2023molca}. The details of the datasets are shown in Table \\ref{tab:dataset}. \n\\begin{table}[htb]\n    \\centering\n    \\vskip -0.05in\n    \\resizebox{1.0\\columnwidth}{!}{\n    \\begin{tabular}{c|c|c|c}\n    \\toprule\n    Dataset & Train & Validation & Test \\\\\n    \\midrule\n    ChEBI-20 &  26,407     & 3,001           & 3,000  \\\\\n    PubChem324k & 12,000 & 1,000  & 2,000 \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\vskip -0.1in\n    \\caption{Details of the datasets, ChEBI-20 and PubChem324k. For PubChem324k, we follow the split in ~\\citet{liu2023molca}, while ignoring the \\emph{Pretrain} fold.}\n    \\label{tab:dataset}\n    \\vskip -0.1in\n\\end{table}\n\nFor comparison, we select two different foundation LLMs as the backbones of ICMA, namely Galactica-125M ~\\cite{taylor2022galactica} and Mistral-7B-instruct-v0.2 ~\\cite{jiang2023mistral}. The former is a representative and smaller LLM that has been pre-trained on unstructured scientific corpora, which have been aware of the molecular knowledge from the pre-training stage, while the latter is a general LLM with 7 billion parameters, whose capabilities are comparable to GPT-3.5-turbo.\nNotably, in the factor analysis and ablation study parts, we select Galactica-125M for experiments to reduce the computational costs.\nAdditionally, considering baseline models, we mainly select MolT5-base, MolT5-large from ~\\citet{edwards-etal-2022-translation} and MolReGPT (GPT-3.5-turbo and GPT-4-0314) from ~\\citet{li2023empowering} for comparison on the ChEBI-20 dataset.\n\n\\subsection{Performance Comparison}\n\\begin{table*}[htb]\n    \\centering\n    \\vskip -0.2in\n    \\resizebox{1.8\\columnwidth}{!}{\n    \\begin{tabular}{c|c|c|c|c|c|c}\n    \\toprule\n    Methods & BLEU-2$\\uparrow$ & BLEU-4$\\uparrow$ & ROUGE-1$\\uparrow$ & ROUGE-2$\\uparrow$ & ROUGE-L$\\uparrow$ & METEOR$\\uparrow$ \\\\\n    \\midrule\n    MolT5-base \\cite{edwards-etal-2022-translation} & 0.540 & 0.457 & 0.634 & 0.485 & 0.578 & 0.569 \\\\ \n    MolReGPT (GPT-3.5-turbo) & 0.565 & 0.482 & 0.623 & 0.450 & 0.543 & 0.585 \\\\\n    MolT5-large \\cite{edwards-etal-2022-translation} & 0.594 & 0.508 & 0.654 & 0.510 & 0.594 & 0.614 \\\\\n    MolReGPT (GPT-4-0314) & 0.607 & 0.525 & 0.634 & 0.476 & 0.562 & 0.610 \\\\\n    \\midrule\n    Galactica-125M & 0.585 &\t0.501 &\t0.630 &\t0.474 &\t0.568 & \t0.591 \\\\\n    ICMA(Galactica-125M)$_{2,2048}$ & \\underline{0.636} &\t\\underline{0.565} \t&\\underline{0.674} &\t\\underline{0.536} &\t\\underline{0.615} &\t\\underline{0.648} \\\\\n    Mistral-7B &0.566 &\t0.478 &\t0.614 &\t0.449 &\t0.547 \t&0.572  \\\\\n    ICMA(Mistral-7B)$_{2,2048}$& \\textbf{0.651} &\t\\textbf{0.581} & \\textbf{0.686} & \\textbf{0.550} &\t\\textbf{0.625} \t& \\textbf{0.661} \\\\\n    \\midrule\n    \\bottomrule\n    \\end{tabular}\n            \n    }\n    \\vskip -0.1in\n    \\caption{Mol2Cap results on ChEBI-20 dataset (\\textbf{Best}, \\underline{Second Best}). Here, the results of MolT5-base and MolT5-large are domain-specific pre-training \\& fine-tuning results retrieved from \\citet{edwards-etal-2022-translation}, while MolReGPT(GPT-3.5-turbo) and MolReGPT(GPT-4-0314) are prompting \\& in-context learning results retrieved from \\citet{li2023empowering}. More importantly, Galactica-125M and Mistral-7B demonstrate vanilla supervised fine-tuned results, while ICMA(Galactica-125M) and ICMA(Mistral-7B) illustrate the ICMA results.}\n\n    \\label{tab:m2c}\n    \\vskip -0.1in\n\\end{table*}\n\n\\begin{table*}[htb]\n    \\centering      \n    \\resizebox{1.8\\columnwidth}{!}{\n    \\begin{tabular}{c|c|c|c|c|c|c|c}\n    \\toprule\n    Method & BLEU$\\uparrow$ & EM$\\uparrow$ & Levenshtein$\\downarrow$ & MACCS FTS$\\uparrow$ & RDK FTS$\\uparrow$ & Morgan FTS$\\uparrow$ & Validity$\\uparrow$ \\\\\n    \\midrule\n    MolT5-base \\cite{edwards-etal-2022-translation} & 0.769 & 0.081 & 24.458 & 0.721 & 0.588 & 0.529 & 0.772 \\\\\n    MolReGPT(GPT-3.5-turbo) & 0.790 & 0.139 & 24.91 & 0.847 & 0.708 & 0.624 & 0.887 \\\\ \n    MolT5-large \\cite{edwards-etal-2022-translation} & 0.854 & 0.311 & \\textbf{16.071} & 0.834 & 0.746 & 0.684 & 0.905 \\\\\n    MolReGPT(GPT-4-0314) & \\textbf{0.857} & 0.280 & \\underline{17.14} & \\underline{0.903} & 0.805 & 0.739 & 0.899 \\\\\n    \\midrule\n    Galactica-125M & 0.781 & \t0.173 &\t26.34 &\t0.836 &\t0.708 &\t0.631 &\t0.916  \\\\\n    ICMA(Galactica-125M)$_{4,2048}$ & 0.836 &\t\\underline{0.342} &\t21.48 &\t0.893 &\t\\underline{0.809}& \t\\underline{0.743} &\t\\underline{0.934}   \\\\\n    Mistral-7B & 0.767 &\t0.234 &\t27.39 &\t0.852 &\t0.718 &\t0.649 &\t0.918   \\\\\n    ICMA(Mistral-7B)$_{4,2048}$& \\underline{0.855} & \t\\textbf{0.460} &\t18.73 &\t\\textbf{0.916} &\t\\textbf{0.837} &\t\\textbf{0.789} &\t\\textbf{0.958}  \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\vskip -0.1in\n    \\caption{Cap2Mol results on ChEBI-20 dataset (\\textbf{Best}, \\underline{Second Best}). Here, the results of MolT5-base and MolT5-large are domain-specific pre-training \\& fine-tuning results retrieved from \\citet{edwards-etal-2022-translation}, while MolReGPT(GPT-3.5-turbo) and MolReGPT(GPT-4-0314) are prompting \\& in-context learning results retrieved from \\citet{li2023empowering}. More importantly, Galactica-125M and Mistral-7B demonstrate vanilla supervised fine-tuned results, while ICMA(Galactica-125M) and ICMA(Mistral-7B) illustrate the ICMA results.}\n\n    \\label{tab:c2m}\n    \\vskip -0.2in\n\\end{table*}\nWe compare and analyse the performance of ICMA with previous baselines and their original foundation models with naive supervised fine-tuning (SFT) from the two subtasks of molecule caption translation, namely Mol2Cap and Cap2Mol.\n\n\\noindent\\textbf{Mol2Cap}. As illustrated in Table \\ref{tab:m2c}, Galactica-125M with SFT has already shown competitive performance to previous baselines due to its pre-training on scientific corpora. However, ICMA could still improve the performance of Galactica-125M by 12.8\\% and 8.3\\% considering the BLEU-4 and ROUGE-L scores on ChEBI-20 dataset. With only 125 million parameters, ICMA(Galactica-125M) can beat MolT5-large, which owns 780 million parameters.\nMeanwhile, the general LLM, Mistral-7B with naive SFT, only achieves a performance that is slightly better than MolT5-base, despite the fact that Mistral-7B is 70 times larger than MolT5-base. \nThis outcome is not surprising because Mistral-7B is not specifically designed or pre-trained for biomolecular purposes.\nIt also reveals that although general LLMs have illustrated powerful capabilities with billions of parameters, few works have adapted them to the biomolecular domain due to their unsatisfactory fine-tuning performance. \nHowever, with ICMA, Mistral-7B could easily demonstrate its superior in-context learning and reasoning capabilities and perform its advantage of parameters.\nAs a result, ICMA(Mistral-7B) achieves the best performance across all the models, obtaining 0.581 BLEU-4 and 0.661 METEOR scores on the ChEBI-20 dataset, which is even better than the domain-specific pre-trained Galactica-125M.\n\n\\noindent\\textbf{Cap2Mol}. Similarly, as shown in Table \\ref{tab:c2m}, compared to their original foundation models with SFT, ICMA significantly boosts the molecule generation performance. Notably, ICMA (Mistral-7B) achieves state-of-the-art molecule generation performance, generating 46.0\\% exactly matched molecules, which nearly doubles the results of naive supervised fine-tuned Mistral-7B. Meanwhile, ICMA(Mistral-7B) also achieves superior performance on the three molecule fingerprints scores and obtains the best validity score, illustrating better molecule generation quality with ICMA.\n\nAdditionally, experiments are also conducted on a smaller dataset, PubChem324k. As shown in Table \\ref{tab:m2c_pub}, ICMA could still boost the Mol2Cap performance of LLMs like Galactica-125M and Mistral-7B under the low-data scenario. However, due to the lack of similar examples, Table \\ref{tab:c2m_pub} demonstrates that the molecule generation capability can be influenced when context examples are not sufficient to support LLMs in making a correct decision.\n\n\\begin{table*}[htbp]\n    \\centering\n\n    \\resizebox{1.8\\columnwidth}{!}{\n    \\begin{tabular}{c|c|c|c|c|c|c}\n    \\toprule\n    Method & BLEU-2$\\uparrow$ & BLEU-4$\\uparrow$ & ROUGE-1$\\uparrow$ & ROUGE-2$\\uparrow$ & ROUGE-L$\\uparrow$ & METEOR$\\uparrow$ \\\\\n    \\midrule\n    Galactica-125M & 0.333\t& 0.265&\t0.465&\t0.322&\t0.417\t&0.406\\\\\n    ICMA(Galactica-125M)$_{2,1024}$& \\underline{0.383}\t&\\underline{0.311}&\t\\underline{0.479} &\\underline{0.341}\t&\\underline{0.428}&\t\\underline{0.438}\\\\\n    Mistral-7B & 0.361&\t0.288\t&0.471&\t0.325&\t0.419&\t0.421   \\\\\n    ICMA(Mistral-7B)$_{2,2048ß}$& \\textbf{0.386}\t& \\textbf{0.314}\t&\\textbf{0.483} &\t\\textbf{0.345} &\t\\textbf{0.431} &\t\\textbf{0.443} \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\vskip -0.1in\n    \\caption{Mol2Cap results on PubChem324k dataset (\\textbf{Best}, \\underline{Second Best}). Here, Galactica-125M and Mistral-7B demonstrate vanilla supervised fine-tuned results, while ICMA(Galactica-125M) and ICMA(Mistral-7B) illustrate the ICMA results.}\n    \n    \\label{tab:m2c_pub}\n    \\vskip -0.1in\n\\end{table*}\n\n\\begin{table*}[htbp]\n    \\centering\n    \\resizebox{1.8\\columnwidth}{!}{\n    \\begin{tabular}{c|c|c|c|c|c|c|c}\n    \\toprule\n    Method & BLEU$\\uparrow$ & EM$\\uparrow$ & Levenshtein$\\downarrow$ & MACCS FTS$\\uparrow$ & RDK FTS$\\uparrow$ & Morgan FTS$\\uparrow$ & Validity$\\uparrow$ \\\\\n    \\midrule\n    Galactica-125M &\\underline{0.485}&\t0.031\t&\\underline{62.08}&\t0.681&\t0.510\t&0.403\t& 0.835\\\\\n    ICMA(Galactica-125M)$_{1,1024}$& \\textbf{0.569}\t& 0.035\t&\\textbf{52.75}\t&0.719\t&\\underline{0.579}\t&\\textbf{0.652}\t&0.825\\\\\n    Mistral-7B & 0.438&\t\\textbf{0.082}\t&74.16&\t\\underline{0.731}&\t0.577&\t0.472\t&\\underline{0.866}   \\\\\n    ICMA(Mistral-7B)$_{1,2048}$& 0.450 &\t\\underline{0.065}\t& 77.01 &\t\\textbf{0.764}\t& \\textbf{0.624}\t& \\underline{0.504}\t& \\textbf{0.891}  \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\vskip -0.1in\n    \\caption{Cap2Mol results on PubChem324k dataset (\\textbf{Best}, \\underline{Second Best}). Here, Galactica-125M and Mistral-7B demonstrate vanilla supervised fine-tuned results, while ICMA(Galactica-125M) and ICMA(Mistral-7B) illustrate the ICMA results.}\n    \\label{tab:c2m_pub}\n    \\vskip -0.2in\n\\end{table*}\n\n\\subsection{Study of Retrieval Algorithms}\n\\begin{table}[htbp]\n    \\centering\n\n    \\resizebox{1.0\\columnwidth}{!}{\n    \\begin{tabular}{c|c|c|c|c|c|c}\n    \\toprule\n    Method & BLEU-2$\\uparrow$ & BLEU-4$\\uparrow$ & ROUGE-1$\\uparrow$ & ROUGE-2$\\uparrow$ & ROUGE-L$\\uparrow$ & METEOR$\\uparrow$ \\\\\n    \\midrule\n    Random & 0.573 &\t0.496 &\t0.634 &\t0.484 &\t0.572 &\t0.596 \\\\ \n    Morgan FTS & \\underline{0.599} &\t\\underline{0.526} &\t\\underline{0.650} &\t\\underline{0.508} &\t\\underline{0.590} &\t\\underline{0.626}\\\\\n    Mole-BERT & \\textbf{0.618} &\t\\textbf{0.545} &\t\\textbf{0.662} &\t\\textbf{0.521} &\t\\textbf{0.602} &\t\\textbf{0.637}\\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\vskip -0.1in\n    \\caption{Performance comparison of different retrieval algorithms for Mol2Cap task (\\textbf{Best}, \\underline{Second Best}). The backbone is ICMA(Galactica-125M)$_{2,1024}$.}\n    \\vskip -0.1in\n    \\label{tab:m2c_algo}\n\\end{table}\n\n\\begin{table}[htbp]\n    \\centering\n    \\resizebox{1.0\\columnwidth}{!}{\n    \\begin{tabular}{c|c|c|c|c|c|c|c}\n    \\toprule\n    Method & BLEU$\\uparrow$ & EM$\\uparrow$ & Levenshtein$\\downarrow$ & MACCS FTS$\\uparrow$ & RDK FTS$\\uparrow$ & Morgan FTS$\\uparrow$ & Validity$\\uparrow$ \\\\\n    \\midrule\n    Random &  \\underline{0.802} &\t\\underline{0.286} &\t\\underline{25.29} &\t0.867 &\t0.763 &\t\\underline{0.699} &\t0.923 \\\\\n    SBERT & 0.795 &\t0.245 &\t26.05 &\t\\underline{0.868} &\t\\underline{0.765} &\t0.691 &\t\\underline{0.926}\\\\\n    BM25 & \\textbf{0.818} &\t\\textbf{0.293} &\t\\textbf{24.20} &\t\\textbf{0.881} &\t\\textbf{0.784} &\t\\textbf{0.717} &\t\\textbf{0.927}\\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\vskip -0.1in\n    \\caption{Performance comparison of different retrieval algorithms for Cap2Mol task (\\textbf{Best}, \\underline{Second Best}). The backbone is ICMA(Galactica-125M)$_{2,1024}$.}\n    \\label{tab:c2m_algo}\n    \\vskip -0.2in\n\\end{table}\nWe study the influence of retrieval algorithms to illustrate the importance of retrieval qualities. For molecule retrieval, we compare the new proposed Molecule Graph Retrieval using Mole-BERT with random retrieval and the Morgan FTS retrieval. As shown in Table \\ref{tab:m2c_algo},  Molecule Graph Retrieval using Mole-BERT achieves the best results on all of the metrics, proving its superiority in molecule retrieval.\nFor the caption retrieval, we compare BM25 Caption Retrieval with random retrieval and SBERT retrieval under the framework of ICMA.\nAs depicted in Table \\ref{tab:c2m_algo}, BM25 Caption Retrieval illustrates its excellent performance among the three caption retrieval methods. Notably, SBERT retrieval achieves a worse score than random retrieval because it loses detailed information during the compression, further proving the importance of details in caption retrieval.\n\n\\subsection{Study of Context Settings}\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1.0\\columnwidth]{figures/context_settings.png}\n    \\vskip -0.1in\n    \\caption{The model performance with the change of context settings, including the number of examples (i.e., n\\_shot) and cutoff length. Mol2Cap Results (Left) and Cap2Mol Results (Right).}\n    \\label{fig:model_length}\n    \\vskip -0.2in\n\\end{figure}\nIn ICMA, the context settings, including the example number and cutoff length, are also important to its performance. The increase of context example number will also drastically add to the context length. However, if the context length is larger than the cutoff length, then the training will be insufficient because most of the inputs are cropped, and the information is lost. \nIn this case, we want to make sure that most of the context examples fit in the cutoff length. Considering that the input length limitation of Galactica-125M is 2048, we test the cutoff length within the range of \\{512, 1024, 1536, 2048\\} and the example number from 1 to 4 for analysis.\nAs illustrated in Figure \\ref{fig:model_length}, when the example number increases, the performance becomes worse if the cutoff length is too short. For example, when the cutoff length is set to 512, the context is not complete for LLMs to consistently learn valuable knowledge from it, leading to worse results. However, when the cutoff length is long enough, the performance mainly becomes better as the increased example number provides more information to help LLMs make accurate predictions.\n\n\\subsection{Study of Scaling Law}\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\columnwidth]{figures/scaling_law.png}\n    \\caption{The scaling law of ICMA. Three models with different levels of parameters are selected, including Galactica-125M, Galactica-1.3B, and Mistral-7B. Mol2Cap Results (Left) and Cap2Mol Results (Right).}\n    \\vskip -0.1in\n    \\label{fig:model_scale}\n\\end{figure}\nAs ICMA creates a new paradigm for adapting powerful LLMs with billion parameters to the molecule-caption translation task, it is interesting to study the relationship between the performance and model scales. In this case, we select LLMs with different scales of parameters, ranging from 125 million to 7 billion, to study the scaling law of ICMA. Specifically, we select Galactica-125M, Galactica-1.3B, and Mistral-7B for demonstration.\n\nAs illustrated in Figure \\ref{fig:model_scale}, with the increase of model scale, the performance of ICMA also improves. Especially in Mol2Cap task, the METEOR score has been drastically improved when the model parameters scale from 1.3 billion to 7 billion. The improvement of molecule understanding significantly benefits from the language generation capability as the model scale reaches a certain level. Meanwhile, for Cap2Mol task, the accuracy for predicting exactly matched molecules demonstrates a slightly different pattern from the Mol2Cap task. The model's performance in generating exact-matched molecules seems to have a log-linear relationship with the model scales, showing that larger language models could explore the molecular space better owing to their more powerful capabilities. \nOverall, ICMA allows general LLMs with billion parameters to achieve similar or better performance to these domain-specific pre-trained LLMs in the molecule-caption translation task, showing that LLMs are inherently in-context molecule learners. \n\n\\subsection{Ablation Study}\n\\begin{table}[htbp]\n    \\centering\n    \\vskip -0.2in\n    \\resizebox{1.0\\columnwidth}{!}{\n    \\begin{tabular}{c|c|c|c|c|c|c}\n    \\toprule\n    Method & BLEU-2$\\uparrow$ & BLEU-4$\\uparrow$ & ROUGE-1$\\uparrow$ & ROUGE-2$\\uparrow$ & ROUGE-L$\\uparrow$ & METEOR$\\uparrow$ \\\\\n    \\midrule\n    ICMA & \\textbf{0.618} &\t\\textbf{0.545} &\t\\textbf{0.662(4)} &\t0.521(1) &\t\\textbf{0.602(3)} &\t0.637(3)\\\\\n    w/o Random Walk &\\underline{0.603}& \t\\underline{0.534} &\t\\underline{0.661(9)} &\t\\textbf{0.523(3)} &\t\\underline{0.602(1)} &\t\\textbf{0.638(1)}\\\\\n    w/o Sequence Reverse& 0.597 &\t0.528 \t&0.661(8) &\t\\underline{0.522(5)} \t&0.601(8) &\t\\underline{0.638(0)}\\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\vskip -0.1in\n    \\caption{Ablating components of Post-retrieval Re-ranking for Mol2Cap task (\\textbf{Best}, \\underline{Second Best}). The backbone is ICMA(Galactica-125M)$_{2,1024}$.}\n    \\vskip -0.2in\n    \\label{tab:m2c_ablation}\n\\end{table}\n\n\\begin{table}[htbp]\n    \\centering\n    \\resizebox{1.0\\columnwidth}{!}{\n    \\begin{tabular}{c|c|c|c|c|c|c|c}\n    \\toprule\n    Method & BLEU$\\uparrow$ & EM$\\uparrow$ & Levenshtein$\\downarrow$ & MACCS FTS$\\uparrow$ & RDK FTS$\\uparrow$ & Morgan FTS$\\uparrow$ & Validity$\\uparrow$ \\\\\n    \\midrule\n    ICMA & \\textbf{0.818} &\t\\textbf{0.293} &\t\\textbf{24.20} \t&\\textbf{0.881} \t&\\textbf{0.784}\t&\\textbf{0.717} \t& \\underline{0.927} \\\\\n    w/o Random Walk & 0.771 &\t0.103 &\t28.99 &\t0.838 \t&0.749 &\t0.654 &\t0.925 \\\\\n    w/o Sequence Reverse &\\underline{0.807} &\t\\underline{0.279} &\t\\underline{24.65} &\t\\underline{0.876} \t&\\underline{0.782} &\t\\underline{0.713} &\t\\textbf{0.929} \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\vskip -0.1in\n    \\caption{Ablating components of Post-retrieval Re-ranking for Cap2Mol task (\\textbf{Best}, \\underline{Second Best}). The backbone is ICMA(Galactica-125M)$_{2,1024}$.}\n    \\label{tab:c2m_ablation}\n    \\vskip -0.1in\n\\end{table}\nLastly, the ablation study is also conducted to illustrate how the Post-retrieval Re-ranking components affect the predictions. \nWe conduct ablation experiments for the Mol2Cap and Cap2Mol sub-tasks by deactivating the Random Walk and Sequence Reversal.\nAs shown in Table \\ref{tab:m2c_ablation} and \\ref{tab:c2m_ablation}, without Random Walk or Sequence Reversal, the performance of ICMA drops in both tasks, proving the effectiveness of Post-retrieval Re-ranking.\n\n\\section{Conclusion}\n\\label{sec:conclusion}\nIn this work, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm for adapting LLMs to the molecule-caption translation task. \nInstead of domain-specific pre-training and fine-tuning, ICMA enables LLMs to utilize their in-context learning capability to learn molecule-text alignment via In-Context Molecule Tuning, which significantly improves the performance of LLMs in the molecule-caption translation task, demonstrating that LLMs are inherently in-context molecule learners.\nMore importantly, our study provides a viable framework for deploying advanced LLMs with billion-level parameters in the scientific field.\n\n\\section{Limitations}\nThe limitation of this work primarily lies in the following aspects:\n\\begin{itemize}\n    \\item Firstly, we limited the retrieval range to the training split of the dataset, which might be insufficient, given the vastness of the molecular space. \n    \\item Secondly, this work is limited to the molecule-caption translation task. However, it is worth noting that there are various molecule-related downstream tasks, such as molecule property prediction, which is also important to verify the generalization capability of ICMA.\n    \\item Lastly, due to the limitation of hardware, we were unable to investigate the potential of language models with more than 7 billion parameters with ICMA. It would be interesting to explore the capabilities of larger, more sophisticated language models under the framework of ICMA.\n\\end{itemize}\n\n\\section{Acknowledgement}\nWe thank any anonymous reviewers for their valuable suggestions. We also thank ChatGLM-4\\footnote{\\url{https://chatglm.cn}} and Github Copilot\\footnote{\\url{https://github.com/features/copilot}} for assistance in the paper revision and code writing.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{MolX: Enhancing Large Language Models for Molecular Learning\\\\with A Multi-Modal Extension}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nLarge Language Models (LLMs) with their strong task-handling capabilities have shown remarkable advancements across a spectrum of fields, moving beyond natural language understanding. However, their proficiency within the chemistry domain remains restricted, especially in solving professional molecule-related tasks. This challenge is attributed to their inherent limitations in comprehending molecules using only common textual representations, i.e., SMILES strings. In this study, we seek to enhance the ability of LLMs to comprehend molecules by equipping them with a multi-modal external module, namely MolX. In particular, instead of directly using a SMILES string to represent a molecule, we utilize specific encoders to extract fine-grained features from both SMILES string and 2D molecular graph representations for feeding into an LLM. Moreover, a handcrafted molecular fingerprint is incorporated to leverage its embedded domain knowledge. Then, to establish an alignment between MolX and the LLM’s textual input space, the whole model in which the LLM is frozen, is pre-trained with a versatile strategy including a diverse set of tasks. Experimental evaluations show that our proposed method outperforms baselines across 4 downstream molecule-related tasks ranging from molecule-to-text translation to retrosynthesis, with and without fine-tuning the LLM, while only introducing a small number of trainable parameters—0.53\\% and 0.82\\%, respectively. \n\\end{abstract}\n\n\\section{1. Introduction}\nIn the last few years, Large Language Models (LLMs) have demonstrated impressive performances across a wide array of fields. Extending beyond the boundaries of natural language understanding, LLMs have facilitated various scientific disciplines \\cite{taylor2022galactica, telenti2024large}. Without exception, with a high-level understanding of chemical concepts obtained from the wealth of chemical literature in pre-training data, LLMs have recently been investigated for augmenting research in chemistry as an alternative approach to the traditional supervised learning approach \\cite{castro2023large, achiam2023gpt}. \n\nDespite their strong task-handling capabilities, LLMs still struggle with the chemistry domain, as evidenced by their limited performances on various professional molecule-related tasks \\cite{zhao2023scientific, guo2023can}. For instance, the capable LLM, Llama-2 \\cite{touvron2023llama}, performs unsatisfactorily on the molecule-to-text translation tasks such as molecule description generation and IUPAC name generation, being more than twice as inferior compared to the supervised learning models. Additionally, such LLM fails to predict molecule activity for high-level properties even using expert-designed prompts. One potential cause of this challenge has been figured out that most existing LLMs represent molecules only by their common textual representations, i.e., SMILES strings \\cite{weininger1988smiles}, and process them in a paradigm similar to texts \\cite{guo2023can, li2023towards}, as illustrated in Figure \\ref{Fig1}a. While convenient, several issues make it challenging for LLMs to comprehend molecules by solely interpreting SMILES strings. Firstly, LLMs lack an inherent understanding of SMILES strings and blindly treat them as sequences of separate characters relying on their byte-pair encoding tokenizers \\cite{sennrich2016neural}, which break SMILES strings into smaller pieces in ways that do not represent chemical laws behind these strings. Furthermore, without an understanding of chemical laws, it is difficult for LLMs to capture molecules’ topological structures from SMILES strings due to potential inaccuracies such as incorrect transcription of complex aromatic systems or the absence of hydrogens and other atoms \\cite{voinarovska2023yield}, as shown in Figure \\ref{Fig1}b and Figure \\ref{Fig1}c. \n\n\\begin{figure*}[!ht]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{AnonymousSubmission//LaTeX//figs/Fig1.pdf}\n    \\caption{Current paradigm of using an LLM for molecule-related tasks and its issues.}\n    \\label{Fig1}\n\\end{figure*}\n\nIn light of these issues, there have been some early attempts to enhance LLMs for solving molecule-related tasks. For instance, \\citet{su2022molecular} strive to employ a GNN-based graph encoder to extract features from the molecule’s 2D molecular graph and directly input such features into the LLM to perform molecule-to-text translation tasks. Developed from that idea, \\citet{li2023towards} input features extracted from the 2D or 3D molecular graph into the LLM through an intermediate projector, which is previously aligned with the LLM’s textual input space by a pre-training stage. Although bridging the gap between the 2D or 3D molecular graph and the LLMs, previous works still ineffectively make use of another essential representation, i.e., SMILES string, as well as handcrafted molecular descriptors, which have their own advantages over 2D or 3D molecular graph \\cite{david2020molecular, jo2020message}, hence, might lead to suboptimal performances. Furthermore, existing methods are only optimized for a limited number of molecule-related tasks, omitting other crucial tasks such as molecule property prediction, molecule optimization, or retrosynthesis. \n\nIn this study, we introduce a novel framework for enhancing LLMs to extensively comprehend molecules from multiple representations, thus, improving their performances on various molecule-related tasks. Our proposed framework consists of two main components which are a multi-modal external module, namely MolX, equipped with the LLMs, and a versatile pre-training strategy for aligning MolX into the LLMs’ textual input space. To be more precise, we first utilize a pre-trained BERT-like \\cite{devlin2019bert} SMILES encoder to extract features from the SMILES string instead of directly using it to represent a molecule. Because of its initial pre-training stage, the SMILES encoder has been enabled to work with its tokenizer to capture long-range dependencies identified in the SMILES string. Besides that, we simultaneously utilize a pre-trained GNN-based graph encoder to extract features from the molecule’s 2D molecular graph, capturing its topological structures. To accomplish MolX, in addition to features extracted from raw representations, i.e., SMILES string and 2D molecular graph, a handcrafted molecular fingerprint \\cite{morgan1965generation} containing abundant domain knowledge is further incorporated in a weighted scheme. Eventually, the whole model, in which the LLM is frozen, undergoes a versatile pre-training strategy with a diverse set of tasks, providing the model with comprehensive information about the molecules. This process encourages an alignment between MolX and the LLM’s textual input space. Figure \\ref{Fig2} provides an overview of our proposed method. \n\n\\begin{figure*}[!ht]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{AnonymousSubmission//LaTeX//figs/Fig2.pdf}\n    \\caption{An overview of our proposed method with the main pre-training task.}\n    \\label{Fig2}\n\\end{figure*}\n\nOur experimental results demonstrate that the proposed method outperforms baselines by a substantial margin on various downstream molecule-related tasks in two different model configurations, with and without fine-tuning the LLM. It is worth noting that MolX can flexibly act as a plug-in module to the LLMs for enhancing the performances on molecule-related tasks while fully preserving its general-purpose usage in other domains. \n\nTo summarize, our contributions are outlined as follows: \n\\begin{itemize}\n    \\item We introduce a novel framework enhancing LLMs to comprehend molecules, thus, improving their performances on various molecule-related tasks. The LLMs are equipped with a multi-modal external module, MolX, to extract features from both SMILES string and 2D molecular graph representations, as well as leverage a handcrafted molecular fingerprint. \n    \\item A versatile pre-training strategy including a diverse set of tasks, is applied to establish an alignment between MolX and the LLMs’ textual input space. This process parallelly advances the models’ ability of molecular understanding, as well as instruction following. \n    \\item Extensive experimental evaluations demonstrate that our proposed method outperforms baselines by a substantial margin on a diverse range of downstream molecule-related tasks in two different model configurations, with and without fine-tuning the LLM. \n\\end{itemize}\n\n\\section{2. Related Work}\nIn this section, we provide a review of the literature related to molecular learning via language modeling and leveraging LLMs for solving molecule-related tasks. \n\n\\subsection{Molecular Learning}\nMolecules form the basis of chemistry and molecular learning has been a long-standing problem in cheminformatics \\cite{baum2021artificial, hustrategies, xiamole, pei2023biot5, pei-etal-2024-biot5}. Traditionally, molecular fingerprints such as Morgan fingerprint \\cite{morgan1965generation} or ECFP \\cite{rogers2010extended} serve as one of the most important descriptors for molecules, encoding a molecule into a fixed bit string, where each bit indicates the presence of a certain substructure. In the last decade, with the rapid development of language modeling, another representation has become more widely used due to its textual nature, i.e., SMILES strings \\cite{weininger1988smiles}. Studying the molecule property prediction task, \\citet{wang2019smiles} introduce SMILES-BERT, a BERT-like model \\cite{devlin2019bert} and is pre-trained with the masked language modeling mechanism (MLM) on a large-scale set of unlabeled molecules. Following that, while \\citet{wang2021chemical} propose using chemical reactions to assist the pre-training, \\citet{ahmad2022chemberta} on the other hand propose using auxiliary tasks with more domain relevance for chemistry such as predicting computed properties of molecules, supporting MLM. \\citet{irwin2022chemformer} investigate the challenging sequence-to-sequence tasks such as retrosynthesis and then introduce Chemformer. \\citet{zhong2022root} propose the root-aligned SMILES (R-SMILES), adopting a tighter representation for those tasks. Parallelly, \\citet{edwards2022translation} are interested in molecule-to-text translation tasks and vice versa and proposed MolT5, which is pre-trained with the multi-lingual MLM, considering SMILES strings as a conventional language. Moreover, \\citet{lu2022unified} and \\citet{christofidellis2023unifying} present ChemT5 and Text+ChemT5, unifying all sequence-to-sequence tasks. Additionally, several works \\cite{guo2020graseq, liu2023prediction} have demonstrated that fusing the molecule’s 2D molecular graph with language modeling provides complementary benefits to molecular learning, improving performances on tasks such as molecule property prediction. Recently, with their rising advancements across a wide array of fields, including chemistry \\cite{castro2023large, achiam2023gpt}, LLMs have emerged as an evolution of the traditional language modeling approach for molecular learning. \n\n\\subsection{LLMs for Molecule-Related Tasks}\nSeveral studies have attempted to evaluate LLMs regarding their knowledge of chemistry. \\citet{castro2023large} early explored how well ChatGPT understands chemistry by posing 5 student-level tasks in different subareas of chemistry and noticed moderate performances. \\citet{zhao2023scientific} investigate the molecule property prediction task and discover that LLMs tend to rely on memorized information for making predictions, which may limit their applications in practice. After that, \\citet{guo2023can} conduct a more comprehensive evaluation by benchmarking various existing LLMs on 8 practical molecule-related tasks. Empirical results reveal that capable LLMs such as Llama-2 \\cite{touvron2023llama} typically fail to perform challenging tasks of molecule-to-text translation or predict molecule activity for high-level properties even using expert-designed prompts. A potential reason behind this challenge has been identified that most existing LLMs represent molecules only by their common textual representations, i.e., SMILES strings, which LLMs have a limited understanding of. In response to such findings, \\citet{su2022molecular} propose MoMu to enhance LLMs by applying a GNN-based graph encoder to extract features from the molecule’s 2D molecular graph and input such features into the LLM for performing molecule-to-text translation tasks. Following that, \\citet{li2023towards} proposed 2D and 3D MoLM to leverage an intermediate projector for feeding features extracted from the 2D or 3D molecular graph into the LLM, which is previously aligned with the LLM’s textual input space by a pre-training stage. Despite improvements by bridging the gap between the 2D or 3D molecular graph and the LLMs, the importance of another essential representation, i.e., SMILES string, as well as handcrafted molecular descriptors are neglected in previous works. Also, existing methods are only optimized for a limited set of molecule-related tasks, how well the enhanced LLMs perform on other crucial tasks such as molecule property prediction, molecule optimization, or retrosynthesis is under-explored. \n\n\\section{3. Methodology}\nWe propose a framework enhancing LLMs to comprehend molecules from multiple representations, consisting of two main components, a multi-modal external module and a novel pre-training strategy. Here we delve into the details of these components. \n\n\\subsection{Model Architecture}\nThe proposed MolX, which is equipped with a base LLM, consists of two key designs: 1) Trainable encoders, focusing on encoding raw representations of a molecule, i.e., SMILES string and 2D molecular graph; 2) A weighted scheme to incorporate a handcrafted molecular fingerprint. \n\n\\noindent\n\\textbf{Trainable Encoders.} First of all, we formulate a molecule as $m$ and consider $m_S$ and $m_G$ to depict its SMILES string and 2D molecular graph, respectively. While $m_S$ is simply a sequence of ASCII characters, $m_G$ is considered as $m_G = \\{\\mathcal{V}, \\mathcal{E}\\}$, where each node in $\\mathcal{V}$ indicates an atom and each edge in $\\mathcal{E}$ indicates a chemical bond. Also, $\\boldsymbol{X} \\in \\mathbb{R}^{|\\mathcal{V}| \\times N}$ is the attribute matrix of $m_G$ where $x_n = \\boldsymbol{X}[n, :]^T$ is the $N$-dimensional attribute vector of the node $v_n \\in \\mathcal{V}$. \n\n\\noindent\nTo encode the SMILES string $m_S$, we adopt a pre-trained BERT-like \\cite{devlin2019bert} SMILES encoder, ChemBERTa \\cite{ahmad2022chemberta}, which is constructed by stacking multiple Transformer layers. Notably, ChemBERTa, denoted as $E_S$ is pre-trained on a large-scale set of unlabeled molecules with MLM, enabling it to capture long-range dependencies identified in the SMILES string. In detail, an average is taken over outputs of $E_S$ to obtain an embedding vector for $m_S$, which is then projected to the hidden dimension $d$ of the base LLM by a multi-layer perceptron $f_S$: \n\\begin{equation}\ne_S = f_S(\\text{Average}(\\{t_i, t_i \\in E_S(m_S)\\})) \\in \\mathbb{R}^d. \n\\end{equation}\n\n\\noindent\nTo encode the 2D molecular graph $m_G$, we adopt a pre-trained GNN-based graph encoder, ChemGraphCL \\cite{you2020graph}, which is constructed based on an emerging message-passing GNN, GIN \\cite{yifan2020measuring}. Notably, ChemGraphCL, denoted as $E_G$ is pre-trained on a large-scale set of unlabeled molecules with a contrastive learning strategy \\cite{radford2021learning}, thus, being able to capture the topological structures of the molecule from its 2D molecular graph. In detail, starting from initial $x_n$, after multiple layers of message propagation, $E_G$ produces an updated attribute vector $h_n$ for the node $v_n \\in \\mathcal{V}$. Then an average is taken over all node-level attribute vectors to obtain an embedding vector for $m_G$, which is then projected to the hidden dimension $d$ of the base LLM by a multi-layer perceptron $f_G$: \n\\begin{equation}\ne_G = f_G(\\text{Average}(\\{h_n, h_n \\in E_G(m_G)\\})) \\in \\mathbb{R}^d. \n\\end{equation}\n\n\\noindent\nAfter that, $e_S$ and $e_G$ are averaged to establish a unified embedding vector $e \\in \\mathbb{R}^d$. \n\n\\begin{figure}[!ht]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{AnonymousSubmission//LaTeX//figs/Fig3.pdf}\n    \\caption{Examples of auxiliary tasks in our instruction-based pre-training strategy.}\n    \\label{Fig3}\n\\end{figure}\n\n\\noindent\n\\textbf{Molecular Fingerprint Incorporation.} The handcrafted molecular fingerprints are one of the most important descriptors of molecules due to their abundant domain knowledge. While SMILES string and 2D molecular graph capture global information about the molecule, molecular fingerprints capture detailed information about the local atomic environments and neighborhoods, explicitly encoding the presence of specific substructures \\cite{doi2022screening}. Nevertheless, molecular fingerprints are typically disregarded by using deep learning models even though they have been shown to be valuable for specific tasks such as molecule property prediction \\cite{xia2024understanding}. Therefore, here we seek to bring their benefits by incorporating the popular Morgan fingerprint \\cite{morgan1965generation} into the unified embedding vector $e$ from trainable encoders described above. Specifically, a computational tool RDKit \\cite{landrum2013rdkit} is utilized to compute the Morgan fingerprint with a radius of 2 from the molecule $m$, which is then also projected to the hidden dimension $d$ of the base LLM by a multi-layer perceptron $f_F$. The incorporation scheme works as follows: \n\\begin{equation}\n\\begin{aligned}\ne = w_e \\cdot e + w_{e_F} \\cdot \\text{ }& e_F, \\\\\n\\text{where }& e_F = f_F(\\texttt{MorganFP}(m)), \n\\end{aligned}\n\\end{equation}\nwhere $w_e$ and $w_{e_F}$ are trainable parameters introduced for providing the model sufficient flexibility to incorporate the Morgan fingerprint into $e$. \n\n\\subsection{Pre-training Strategy}\nThere is a noticeable misalignment in the latent spaces of MolX and the base LLM where the former encodes molecules while the latter has a textual input space, hence, a cross-space alignment stage is needed. To this end, after feeding the embedding vector from MolX into the LLM as a soft token, we propose to pre-train the MolX-enhanced LLM with a diverse set of tasks including a molecule-to-text translation task, i.e., molecule description generation, accompanied by a couple of auxiliary tasks. It is worth noting that while MolX is trainable, the base LLM is kept frozen during pre-training. This setting maintains the LLM’s inherent generalizability, forcing MolX to produce embedding vectors that are well-suited in the LLM’s textual input space, as well as can be effectively understood by the LLM to generate accurate answers. Besides, this allows the LLM to function normally on general domains by flexibly using MolX as a plug-in module for handling molecule-related tasks. \n\n\\begin{table*}[!ht]\n\\centering\n\\caption{Experimental results for molecule-to-text translation.}\n\\label{molecule-to-text-translation}\n\\setlength{\\tabcolsep}{2.99pt}\n\\renewcommand{\\arraystretch}{0.90}\n\\scriptsize\n\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}ll|cccccc|cccccc}\n\\toprule\n\n\\multirow[t]{2}{*}{} &\\multirow[t]{2}{*}{Model} &\\multicolumn{6}{c}{Description Generation} &\\multicolumn{6}{c}{IUPAC Name Generation} \\\\\n& &BLE-2↑ &BLE-4↑ &ROG-1↑ &ROG-2↑ &ROG-L↑ &MET↑ &BLE-2↑ &BLE-4↑ &ROG-1↑ &ROG-2↑ &ROG-L↑ &MET↑ \\\\\n\\midrule\n\\multirow[t]{2}{*}{Infer-only}\n&Llama-2-7B                 &03.64 &02.98 &18.28 &04.26 &12.87 &16.21 &05.55 &01.81 &05.40 &00.23 &04.39 &10.30 \\\\\n&Llama-2-7B + \\textbf{MolX} &\\textbf{08.22} &\\textbf{06.40} &\\textbf{30.82} &\\textbf{21.69} &\\textbf{28.94} &\\textbf{21.77} &\\textbf{10.67} &\\textbf{04.76} &\\textbf{14.61} &\\textbf{01.24} &\\textbf{11.47} &\\textbf{18.54} \\\\\n\\midrule\n\\multirow[t]{5}{*}{LoRA FT}\n&Llama-2-7B                 &27.54 &21.24 &36.50 &21.33 &28.99 &31.69 &51.43 &36.94 &48.54 &20.57 &40.53 &53.38 \\\\\n&Llama-2-7B + MoMu          &27.68 &21.50 &36.76 &21.42 &29.23 &31.86 &51.70 &37.38 &48.89 &20.65 &40.87 &53.66 \\\\\n&Llama-2-7B + MoLM-2D       &27.95 &21.77 &38.66 &22.99 &30.92 &33.69 &52.32 &37.65 &51.77 &21.83 &43.62 &57.10 \\\\\n&Llama-2-7B + MoLM-3D       &29.82 &22.39 &39.12 &23.62 &32.64 &34.34 &55.70 &38.93 &52.03 &22.78 &45.63 &57.84 \\\\\n&Llama-2-7B + \\textbf{MolX} &\\textbf{31.40} &\\textbf{24.25} &\\textbf{44.20} &\\textbf{28.96} &\\textbf{38.76} &\\textbf{39.55} &\\textbf{56.88} &\\textbf{45.01} &\\textbf{55.45} &\\textbf{30.14} &\\textbf{48.19} &\\textbf{59.35} \\\\\n\\rowcolor{customgray}\n&LlaSMol-7B                 &26.71 &18.06 &38.75 &22.77 &33.32 &32.63 &49.48 &36.33 &52.38 &28.53 &45.20 &58.48 \\\\\n\\rowcolor{customgray}\n&ChemDFM-13B                &13.02 &08.30 &20.42 &11.31 &17.93 &18.44 &39.33 &22.83 &37.61 &09.49 &28.68 &45.99 \\\\\n\\midrule\n\\multirow[t]{2}{*}{Full FT}\n&MolT5-Large        \t    &25.87 &17.28 &34.07 &16.42 &23.41 &28.04 &50.88 &38.69 &45.89 &21.11 &33.03 &44.82 \\\\\n&MolT5-Large + MoMu \t    &26.34 &18.01 &34.75 &16.86 &24.76 &28.73 &51.81 &40.32 &46.81 &21.68 &34.93 &45.92 \\\\\n\n\\bottomrule\n\\end{tabular*}\n\\end{table*}\n\n\\noindent\n\\textbf{Multi-Task Dataset.} To conduct the pre-training stage, we utilize the pre-train subset of the PubChem dataset \\cite{li2023towards}, a dataset that contains roughly 300k molecule-description pairs collected from PubChem database \\footnote{https://pubchem.ncbi.nlm.nih.gov} for the molecule description generation task. By using this task as an objective, MolX is encouraged to produce meaningful embedding vectors, so that the LLM can caption molecules with their substructures and properties accurately, as illustrated in Figure \\ref{Fig2}. Although a valuable dataset collected from a reliable source, descriptions in the dataset retain several limitations that might hinder the model’s ability of molecular understanding. For instance, the average number of words in the dataset’s descriptions is roughly 20, not capacity sufficient to describe a molecule. Additionally, a certain amount of the dataset’s descriptions is discovered to be noisy and uninformative \\cite{li2023towards}. Therefore, to assist the molecule description generation objective, we design a set of auxiliary tasks including predicting the basic chemical and physical properties of molecules such as the number of heavy atoms or molecular weight. We select a set of 10 low-level properties that are available for easy collection from PubChem and present comprehensive information about the molecules. Further, leveraging the fact that a molecule can be represented by multiple valid SMILES strings \\cite{bjerrum2018improving}, we utilize one more special auxiliary task which is canonicalizing the molecule’s SMILES string. This objective enhances the model’s understanding of chemical laws behind SMILES strings. To keep the pre-training stage controllable, 10\\% of the dataset is used for each auxiliary task. Examples of proposed auxiliary tasks are shown in Figure \\ref{Fig3} and details are in Appendix C. \n\n\\noindent\n\\textbf{Instruction-based Pre-training.} Despite demonstrated strong capabilities, LLMs tend to exhibit hallucinations in the domain of chemistry \\cite{guo2023can}, generating unexpected answers regarding a molecule. Hence, we enrich our pre-training dataset by designing an informative instruction for each task, then employ instruction-based pre-training \\cite{victor2022multitask, ouyang2022training}, enhancing the model’s ability of instruction following. Formally, we first define $p(.)$ as the textual distribution parameterized by the base LLM. The base LLM is decomposed into two subparts, the text embedder $F_{emb}$ and self-attention layers $F_{att}$, in which the text embedder $F_{emb}$ converts an instruction of a task into a list of $T$ tokens $Z = [z_1, z_2, .., z_T]$. Given a molecule $m$ and its label $y$ for the given task, after the embedding vector $e$ is extracted from MolX, the auto-regressive loss for pre-training is defined as: \n\\begin{equation}\n\\begin{aligned}\n\\mathcal{L}_{reg} &= -\\texttt{log} \\hspace{0.2em} p(y|F_{att}(z_1, z_2, .., z_T, e)) \\\\\n&= -\\sum_{l=1}^{L} \\texttt{log} \\hspace{0.2em} p(y_l|F_{att}(z_1, z_2, .., z_T, e), y_1, ..., y_{l-1}), \n\\end{aligned}\n\\end{equation}\nwhere $L$ is the length of the label $y$ for the given task. \n\n\\section{4. Experiments}\nIn this section, we conduct extensive experiments on various downstream molecule-related tasks including molecule-to-text translation, molecule property prediction, molecule optimization, and retrosynthesis, to demonstrate the effectiveness of our proposed method. Throughout experiments, we utilize a capable LLM, Llama-2 \\cite{touvron2023llama} with 7B parameters as our base LLM to leverage its powerful text generation capability and internal chemistry knowledge. We consider two different model configurations for the evaluation: I) Inference-only: The model is frozen after pre-training for direct question answering on downstream tasks, evaluating the model’s generalizability without fine-tuning; II) LoRA fine-tuning: The model is fine-tuned on downstream tasks using a parameter-efficient technique, LoRA \\cite{hu2021lora}, verifying the model’s adaptability in scenarios where downstream data are available. In addition to direct comparing with previous related works including MoMu \\cite{su2022molecular}, as well as 2D and 3D MoLM \\cite{li2023towards}, we also compare with competitive supervised learning models in each task. For further reference, we evaluate 2 recently introduced generalist chemical LLMs derived from Llama-2 \\cite{touvron2023llama} that are tailored for molecule-related tasks, i.e., LlaSMol-7B \\cite{yu2024llasmol} and ChemDFM-13B \\cite{zhao2024chemdfm}. The experimental settings and hyper-parameters are provided in Appendix C. \n\n\\begin{table*}[!ht]\n\\centering\n\\caption{Experimental results for molecule property prediction.}\n\\label{molecule-property-prediction}\n\\setlength{\\tabcolsep}{2.99pt}\n\\renewcommand{\\arraystretch}{0.90}\n\\scriptsize\n\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}ll|cccccccc}\n\\toprule\n\n\\multirow[t]{2}{*}{} &\\multirow[t]{2}{*}{Model} &ESOL &FreeSolv &Lipophilicity &MUV &HIV &BACE &BBBP &Tox21 \\\\\n& &RMSE↓ &RMSE↓ &RMSE↓ &ACC↑ $|$ F1↑ &ACC↑ $|$ F1↑ &ACC↑ $|$ F1↑ &ACC↑ $|$ F1↑ &ACC↑ $|$ F1↑ \\\\\n\\midrule\n\\multirow[t]{2}{*}{Infer-only}\n&Llama-2-7B                 &58.719 &357.371 &222.426 &0.110 $|$ 0.100 &0.135 $|$ 0.129 &0.522 $|$ 0.362 &0.485 $|$ 0.351 &0.090 $|$ 0.084 \\\\\n&Llama-2-7B + \\textbf{MolX} &\\phantom{5}\\textbf{4.929} &\\phantom{35}\\textbf{9.692} &\\phantom{22}\\textbf{1.605} &\\textbf{0.827 $|$ 0.454} &\\textbf{0.807 $|$ 0.484} &\\textbf{0.530 $|$ 0.524} &\\textbf{0.588 $|$ 0.516} &\\textbf{0.622 $|$ 0.459} \\\\\n\\midrule\n\\multirow[t]{5}{*}{LoRA FT}\n&Llama-2-7B                 &\\phantom{5}2.061 &\\phantom{35}4.203 &\\phantom{22}0.956 &0.984 $|$ 0.572 &0.960 $|$ 0.610 &0.612 $|$ 0.584 &0.603 $|$ 0.564 &0.740 $|$ 0.578 \\\\\n&Llama-2-7B + MoMu          &\\phantom{5}2.112 &\\phantom{35}4.214 &\\phantom{22}0.998 &0.992 $|$ 0.576 &0.968 $|$ 0.614 &0.618 $|$ 0.587 &0.612 $|$ 0.574 &0.746 $|$ 0.582 \\\\\n&Llama-2-7B + MoLM-2D       &\\phantom{5}1.521 &\\phantom{35}3.161 &\\phantom{22}0.898 &0.992 $|$ 0.588 &0.968 $|$ 0.627 &0.631 $|$ 0.599 &0.624 $|$ 0.586 &0.746 $|$ 0.594 \\\\\n&Llama-2-7B + MoLM-3D       &\\phantom{5}1.095 &\\phantom{35}2.119 &\\phantom{22}0.780 &0.992 $|$ 0.600 &0.968 $|$ 0.640 &0.644 $|$ 0.587 &0.637 $|$ 0.574 &0.746 $|$ 0.606 \\\\\n&Llama-2-7B + \\textbf{MolX} &\\phantom{5}\\textbf{0.967} &\\phantom{35}2.371 &\\phantom{22}0.808 &\\textbf{0.994 $|$ 0.609} &\\textbf{0.972 $|$ 0.649} &\\textbf{0.704 $|$ 0.697} &\\textbf{0.666 $|$ 0.650} &\\textbf{0.748 $|$ 0.616} \\\\\n\\rowcolor{customgray}\n&LlaSMol-7B                 &\\phantom{5}1.871 &\\phantom{35}6.047 &\\phantom{22}1.361 &0.829 $|$ 0.434 &0.968 $|$ 0.492 &0.467 $|$ 0.318 &0.529 $|$ 0.346 &0.608 $|$ 0.475 \\\\\n\\rowcolor{customgray}\n&ChemDFM-13B                &\\phantom{5}8.476 &\\phantom{35}9.686 &\\phantom{22}2.180 &0.923 $|$ 0.483 &0.952 $|$ 0.534 &0.564 $|$ 0.518 &0.522 $|$ 0.505 &0.677 $|$ 0.529 \\\\\n\\midrule\nFull FT\n&ChemGraphCL  \t   \t    &\\phantom{5}1.231 &\\phantom{35}2.951 &\\phantom{22}0.822 &0.992 $|$ 0.589 &0.968 $|$ 0.628 &0.659 $|$ 0.657 &0.638 $|$ 0.629 &0.746 $|$ 0.596 \\\\\n&ChemGraphMVP \t\t    &\\phantom{5}1.091 &\\phantom{35}\\textbf{2.106} &\\phantom{22}\\textbf{0.718} &0.993 $|$ 0.590 &0.971 $|$ 0.630 &0.691 $|$ 0.689 &0.647 $|$ 0.638 &0.747 $|$ 0.597 \\\\\n\n\\bottomrule\n\\end{tabular*}\n\\end{table*}\n\n\\subsection{Molecule-to-Text Translation}\nWe first consider the molecule-to-text translation tasks, i.e., molecule description generation and IUPAC name generation. These kinds of tasks reflect the general molecular understanding of the model and have crucial applications in practice, enabling humans to gain an overview of a molecule. We conduct experiments on the downstream subset of the PubChem dataset \\cite{li2023towards}, which has 15k high-quality molecule-description pairs and is separate from the pre-train one. We opt not to use the CheBI-20 dataset \\cite{edwards2022translation} because it is also sourced from PubChem and can be viewed as an older version of the used dataset. Following \\cite{edwards2022translation, li2023towards}, we adopt BLEU-2, BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L, and METEOR as evaluation metrics. \n\n\\noindent\nTable \\ref{molecule-to-text-translation} presents experimental results for these tasks across 6 different metrics. Firstly, based on the Inference-only results, we observe the proposed framework significantly enhances the base LLM for direct question answering on both tasks without fine-tuning. In the scenario of LoRA fine-tuning, the MolX-enhanced LLM demonstrates superior performances compared to baselines with the highest scores on all metrics, especially for ROUGE-based and METEOR metrics which might be attributed to the proposed versatile pre-training strategy that provides the model with comprehensive information about the molecules. We make another observation that the approach of fine-tuning the LLM to establish multi-modal models shows better performances than generalist chemical LLMs, i.e., LlaSMol-7B \\cite{yu2024llasmol} and ChemDFM-13B \\cite{zhao2024chemdfm}, as well as competitive supervised learning models such as MolT5 \\cite{edwards2022translation} and its MoMu-enhanced one \\cite{su2022molecular}. \n\n\\begin{figure}[!t]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{AnonymousSubmission//LaTeX//figs/Fig4.pdf}\n    \\caption{An example of molecule property prediction.}\n    \\label{Fig4}\n\\end{figure}\n\n\\subsection{Molecule Property Prediction}\nBesides the overall understanding, we assess the model’s perception of molecular properties by conducting experiments on the molecule property prediction task. This task involves approximating quantitative attributes such as solubility or determining the activity for high-level assays of a molecule. We employ the MoleculeNet dataset \\cite{wu2018moleculenet} with 8 different subsets including ESOL, FreeSolv, Lipophilicity, MUV, HIV, BACE, BBBP, and Tox21. As evaluation metrics, RMSE is used for regression subsets, and Accuracy and F1 are used for classification, following \\cite{yu2024llasmol}. Figure \\ref{Fig4} shows an example of this task. \n\n\\noindent\nExperimental results in Table \\ref{molecule-property-prediction} show that MolX improves performances of the base LLM in both model configurations, especially for Inference-only results, MolX remarkably narrows approximation errors. Additionally, MolX enhances the model’s ability of instruction following, generating expected answers without LLMs’s favorite phrases. In addition to LoRA fine-tuned models, we consider ChemGraphCL \\cite{you2020graph} which serves as the GNN-based graph encoder in MolX, ensuring an adequate comparison. We observe that the MolX-enhanced LLM achieves the best scores in 6 out of 8 subsets of the MoleculeNet dataset and is the second-best in the other 2. Notably, properties in the MoleculeNet dataset are high-level properties and unseen from the pre-training stage, showing the strong adaptability of our proposed method on unseen downstream tasks. \n\n\\subsection{Molecule Optimization}\nMolecule optimization \\cite{he2021molecular} is a more challenging task to assess the model’s perception of molecular properties and the understanding of chemical laws behind SMILES strings. This task aims to modify a molecule toward a target property profile and the model is expected to generate the SMILES string of the modified molecule. The used dataset, ChEMBL-02 \\cite{he2021molecular}, contains 200k molecule pairs from ChEMBL database \\cite{gaulton2012chembl} with changes in properties, i.e., solubility, clearance, and LogD. Following \\cite{edwards2022translation}, we adopt BLEU-2, Levenshtein, Morgan fingerprint-based Similarity, and Validity as evaluation metrics. Figure \\ref{Fig5} shows an example of this task. \n\n\\noindent\nExperiential results for this task are shown in Table \\ref{molecule-optimization}. For inference-only results, MolX not only boosts the performances of the base LLM to an acceptable level but also reduces hallucinations with chemically unreasonable SMILES strings, which are typically found when LLMs generate SMILES strings \\cite{guo2023can}. As an example in Figure \\ref{Fig5}, although still imperfect, the MolX-enhanced LLM recognized that the Fluorine atom is the key modification. In the LoRA fine-tuning scenario, the MolX-enhanced LLM outperforms baselines including robust supervised learning models, Chemformer \\cite{irwin2022chemformer} and ReactionT5 \\cite{sagawa2023reactiont5} in most metrics. \n\n\\begin{table}[!ht]\n\\centering\n\\caption{Experimental results for molecule optimization.}\n\\label{molecule-optimization}\n\\setlength{\\tabcolsep}{2.99pt}\n\\renewcommand{\\arraystretch}{0.90}\n\\scriptsize\n\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}ll|cccc}\n\\toprule\n\n\\multirow[t]{2}{*}{} &\\multirow[t]{2}{*}{Model} &BLE-2↑ &Leven↓ &FTS↑ &Valid↑ \\\\\n\\midrule\n\\multirow[t]{2}{*}{Infer-only}\n&Llama-2-7B                 &08.49 &666.70 &-      &00.00 \\\\\n&Llama-2-7B + \\textbf{MolX} &\\textbf{30.87} &\\phantom{6}\\textbf{88.66} &\\textbf{0.3732} &\\textbf{07.27} \\\\\n\\midrule\n\\multirow[t]{5}{*}{LoRA FT}\n&Llama-2-7B                 &72.32 &\\phantom{6}17.34 &0.5715 &91.31 \\\\\n&Llama-2-7B + MoMu          &63.78 &\\phantom{6}22.20 &0.4659 &92.59 \\\\\n&Llama-2-7B + MoLM-2D       &73.16 &\\phantom{6}17.32 &0.6010 &93.20 \\\\\n&Llama-2-7B + MoLM-3D       &73.83 &\\phantom{6}16.99 &0.5834 &94.05 \\\\\n&Llama-2-7B + \\textbf{MolX} &\\textbf{74.32} &\\phantom{6}\\textbf{16.82} &\\textbf{0.6113} &94.29 \\\\\n\\rowcolor{customgray}\n&LlaSMol-7B                 &34.95 &\\phantom{6}39.50 &0.5431 &\\textbf{99.85} \\\\\n\\rowcolor{customgray}\n&ChemDFM-13B                &32.94 &\\phantom{6}50.65 &0.5302 &43.08 \\\\\n\\midrule\nFull FT\n&Chemformer                 &66.60 &\\phantom{6}20.86 &0.5690 &99.36 \\\\\n&ReactionT5-Large           &73.45 &\\phantom{6}18.91 &0.6058 &99.81 \\\\\n\n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n\\begin{figure}[!ht]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{AnonymousSubmission//LaTeX//figs/Fig5.pdf}\n    \\caption{An example of molecule optimization.}\n    \\label{Fig5}\n\\end{figure}\n\n\\begin{table}[!ht]\n\\centering\n\\caption{Experimental results for retrosynthesis.}\n\\label{retro-synthesis}\n\\setlength{\\tabcolsep}{2.99pt}\n\\renewcommand{\\arraystretch}{0.90}\n\\scriptsize\n\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}ll|cccc}\n\\toprule\n\n\\multirow[t]{2}{*}{} &\\multirow[t]{2}{*}{Model} &BLE-2↑ &Leven↓ &FTS↑ &Valid↑ \\\\\n\\midrule\n\\multirow[t]{2}{*}{Infer-only}\n&Llama-2-7B                 &10.10 &468.74 &-      &00.00 \\\\\n&Llama-2-7B + \\textbf{MolX} &\\textbf{36.73} &\\phantom{4}\\textbf{62.33} &\\textbf{0.4041} &\\textbf{13.71} \\\\\n\\midrule\n\\multirow[t]{5}{*}{LoRA FT}\n&Llama-2-7B                 &80.37 &\\phantom{4}16.22 &0.6981 &89.27 \\\\\n&Llama-2-7B + MoMu          &70.88 &\\phantom{4}20.77 &0.5691 &90.53 \\\\\n&Llama-2-7B + MoLM-2D       &82.05 &\\phantom{4}15.90 &0.7126 &91.13 \\\\\n&Llama-2-7B + MoLM-3D       &81.31 &\\phantom{4}16.21 &0.7341 &90.31 \\\\\n&Llama-2-7B + \\textbf{MolX} &\\textbf{82.59} &\\phantom{4}\\textbf{15.74} &\\textbf{0.7466} &92.19 \\\\\n\\rowcolor{customgray}\n&LlaSMol-7B                 &50.09 &\\phantom{4}31.28 &0.7351 &\\textbf{99.65} \\\\\n\\rowcolor{customgray}\n&ChemDFM-13B                &39.93 &\\phantom{4}57.48 &0.5380 &14.04 \\\\\n\\midrule\nFull FT\n&Chemformer                 &74.01 &\\phantom{4}19.51 &0.6951 &97.14 \\\\\n&ReactionT5-Large           &81.63 &\\phantom{4}17.69 &0.7400 &97.58 \\\\\n\n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n\\begin{figure}[!ht]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{AnonymousSubmission//LaTeX//figs/Fig6.pdf}\n    \\caption{An example of retrosynthesis.}\n    \\label{Fig6}\n\\end{figure}\n\n\\subsection{Retrosynthesis}\nRetrosynthesis is a crucial task in chemistry and is well-known as a bottleneck in drug design \\cite{oliveira2022machine}. This task involves a reverse extrapolation from a molecule to possible reactants used in its synthesis. The model is expected to generate SMILES strings of reactants separated by a ‘.’. We use the USPTO-50k dataset \\cite{schneider2016s}, containing 50k reactions for conducting experiments. Following \\cite{edwards2022translation}, we adopt evaluation metrics similar to those used for the molecule optimization task. Figure \\ref{Fig6} shows an example of this task. \n\n\\begin{table}[!ht]\n\\centering\n\\caption{Numbers of trainable parameters in experiments.}\n\\label{efficiency}\n\\setlength{\\tabcolsep}{2.99pt}\n\\renewcommand{\\arraystretch}{0.90}\n\\scriptsize\n\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}ll|rr}\n\\toprule\n\n\\multirow[t]{2}{*}{} &\\multirow[t]{2}{*}{Model} &\\multicolumn{2}{c}{\\# Trainable Params} \\\\\n& &Pre-training↓ &Downstream↓ \\\\\n\\midrule\n\\multirow[t]{5}{*}{LoRA FT}\n&Llama-2-7B                 &   0.0M (0.00\\%) &  20.5M (0.30\\%) \\\\\n&Llama-2-7B + MoMu          &   2.0M (0.00\\%) &  22.5M (0.30\\%) \\\\\n&Llama-2-7B + MoLM-2D       & 120.0M (1.74\\%) & 120.0M (1.74\\%) \\\\\n&Llama-2-7B + MoLM-3D       & 120.0M (1.74\\%) & 120.0M (1.74\\%) \\\\\n&Llama-2-7B + \\textbf{MolX} &  36.1M (0.53\\%) &  56.6M (0.82\\%) \\\\\n\\rowcolor{customgray}\n&LlaSMol-7B                 &   0.0M (0.00\\%) & 113.2M (1.64\\%) \\\\\n\\rowcolor{customgray}\n&ChemDFM-13B                &    13B (100.\\%) &   13B (100.\\%) \\\\\n\\midrule\nFull FT\n&MolT5-Large                &   0.0M (0.00\\%) & 780.1M (100.\\%) \\\\\n&MolT5-Large + MoMu         &   2.0M (0.00\\%) & 782.1M (100.\\%) \\\\\n\n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n\\noindent\nFrom experiential results presented in Table \\ref{retro-synthesis}, we can observe that MolX improves the Inference-only results of the base LLM and alleviates the hallucinations with a similar effect as the molecule optimization task. As an example in Figure \\ref{Fig6}, the MolX-enhanced LLM recognized the first reactant and slightly erred the second one with the lack of an isocyanate group O=C=N. In the LoRA fine-tuning scenario, the MolX-enhanced LLM surpasses baselines and robust supervised learning models, Chemformer \\cite{irwin2022chemformer} and ReactionT5 \\cite{sagawa2023reactiont5} in most metrics. \n\n\\section{5. Conclusion}\nIn this paper, we propose a novel framework enhancing LLMs to comprehend molecules, thus, improving their performances on molecule-related tasks. The LLMs are equipped with a multi-modal external module, MolX, which is aligned with their textual input space using a versatile pre-training strategy. Experimental evaluations show that our proposed method consistently outperforms baselines across 4 downstream molecule-related tasks ranging from molecule-to-text translation to retrosynthesis, with and without fine-tuning the LLM, while only introducing a small number of trainable parameters—0.53\\% and 0.82\\%, respectively. As shown in Table \\ref{efficiency}, our proposed method is designed to be more efficient than most baselines while giving superior performances. We conduct ablation studies on the influence of building components in our proposed framework and verify the agnosticism to the base LLM of MolX where we use another LLM, Mistral \\cite{jiang2023mistral}. Due to the page limit, we present the results of ablation studies in \\textcolor{red}{Appendix A} and an additional analysis in \\textcolor{red}{Appendix B}. \n\n\\noindent\nHere we discuss the limitations of our work and future directions. Firstly, we are aligning MolX into the LLM via a soft token, though simple but effective. Although we are aware of advanced cross-space alignment techniques such as Q-Former \\cite{li2023blip}, we opt not to employ them since they require a large number of high-quality molecule-description pairs and an extra pre-training stage, leading to high computational costs. A better alignment technique tailored for molecule-related tasks needs to be explored. Moreover, throughout experiments, we show the limitations of current generalist chemical LLMs, therefore, a novel generalist chemical LLM enhanced with MolX should be developed. LLMs also have been demonstrated to have intriguing abilities like In-context Learning \\cite{brown2020language} or Chain-of-Thought \\cite{wei2022chain}. Leveraging these abilities for molecule-related tasks is a potential direction.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2401.13923v2.tex",
        "arXiv-2403.04197v2.tex",
        "arXiv-2406.06777v4.tex"
    ],
    "group_id": "group_46",
    "response": "### Summary: Enhancing Large Language Models for 3D Molecular Understanding and Textual Generation\n\n#### Introduction\nLarge Language Models (LLMs) have revolutionized various domains, including the field of chemistry, by demonstrating remarkable capabilities in understanding and generating text. However, their proficiency in comprehending and analyzing 3D molecular structures remains limited, which poses significant constraints on their potential applications in biomolecular research. The ability to interpret 3D molecular structures is essential for understanding molecular dynamics, protein-ligand interactions, and enzymatic functions, among other critical biomolecular phenomena. This summary focuses on three recent research papers that explore methods to enhance LLMs for 3D molecular understanding and textual generation tasks. The papers introduce novel frameworks and datasets to bridge the gap between LLMs and 3D molecular structures, aiming to improve the models' performance in tasks such as molecule-text retrieval, molecule captioning, and open-text molecular question-answering (QA).\n\nThe field of molecular language modeling has seen significant advancements with the introduction of models like MolT5 and MoMu, which integrate molecular graph information into LLMs. However, these models predominantly rely on 1D SMILES sequences and 2D molecular graphs, leaving 3D molecular structures largely unexplored. The challenge lies in aligning the complex 3D molecular representation space with the textual input space of LLMs, necessitating the development of new methodologies and datasets that can effectively capture and interpret 3D molecular structures. Additionally, there is a need to enhance LLMs' ability to follow human instructions and discern 3D-dependent properties of molecules, which are crucial for advanced biomolecular applications.\n\n#### Main Content of Each Paper\n\n**Paper 1: 3D-MoLM - 3D Molecule-Text Interpretation in Language Models**\n\nThis paper introduces 3D-MoLM, a novel framework designed to enable LLMs to interpret and analyze 3D molecular structures through text generation. The framework consists of two key components: a 3D molecular encoder and a 3D molecule-text projector. The 3D molecular encoder, based on Uni-Mol, is pre-trained on a large dataset of 3D molecular conformations, providing a rich representation of molecular structures. The 3D molecule-text projector, inspired by vision-language models like BLIP-2, translates 3D molecular representations into tokens that can be seamlessly integrated into the LLM's input space. This translation process facilitates the LLM's interpretation of 3D molecular structures, enhancing its cross-modal understanding of molecules.\n\nTo address the challenge of 3D molecule-text alignment, the authors propose a three-stage training pipeline. The first stage focuses on representation learning, where the Q-Former (the 3D molecule-text projector) is pre-trained on a large dataset of 316K molecule-text pairs from PubChem. The second stage optimizes for 3D molecule-text alignment via generative learning, connecting the 3D molecular encoder with the LLM for molecule-to-text generation. The final stage involves instruction tuning using a curated dataset, 3D-MoIT, which enhances the model's ability to follow human instructions and discern 3D-dependent properties of molecules. The 3D-MoIT dataset is enriched with descriptions generated by GPT-3.5 and includes computed properties from PubChemQC, ensuring a comprehensive understanding of molecular structures and properties.\n\n**Paper 2: Large Language Models are In-Context Molecule Learners (ICMA)**\n\nIn this paper, the authors propose ICMA, a new paradigm for adapting LLMs to molecule-caption translation tasks, which includes two sub-tasks: molecule captioning (Mol2Cap) and text-based de novo molecule generation (Cap2Mol). ICMA leverages the in-context learning (ICL) capabilities of LLMs to learn molecule-text alignment from context examples, without the need for extra domain-specific pre-training stages. The framework consists of three stages: Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning.\n\nHybrid Context Retrieval utilizes both BM25 Caption Retrieval and Molecule Graph Retrieval to fetch informative context examples. Post-retrieval Re-ranking employs strategies like Random Walk and Sequence Reversal to enhance the quality of retrieval results. In-context Molecule Tuning adapts the parameters of LLMs to learn from the refined context examples and generate accurate molecule captions. The authors demonstrate that ICMA significantly improves the performance of LLMs on molecule-caption translation tasks, achieving state-of-the-art or comparable results without requiring additional training corpora or complex structures.\n\n**Paper 3: MolX - Enhancing Large Language Models for Molecular Learning with a Multi-Modal Extension**\n\nThis paper presents MolX, a framework that enhances LLMs to comprehend molecules from multiple representations, including SMILES strings and 2D molecular graphs, along with handcrafted molecular fingerprints. MolX consists of two main components: trainable encoders for SMILES strings and 2D molecular graphs, and a weighted scheme to incorporate molecular fingerprints. The framework is pre-trained with a diverse set of tasks, including molecule-to-text translation and auxiliary tasks like predicting basic chemical and physical properties.\n\nThe authors conduct extensive experiments on various downstream molecule-related tasks, such as molecule-to-text translation, molecule property prediction, molecule optimization, and retrosynthesis. They demonstrate that MolX significantly improves the performance of LLMs in these tasks, achieving superior results compared to existing baselines and generalist chemical LLMs. The framework is designed to be efficient, introducing only a small number of trainable parameters, and is agnostic to the base LLM, allowing it to be applied to different models.\n\n#### Commonalities and Innovations\n\nAll three papers aim to enhance LLMs' ability to understand and generate text related to 3D molecular structures, addressing the inherent limitations of LLMs in comprehending complex molecular geometries. They introduce novel frameworks and datasets to bridge the gap between the 3D molecular representation space and the textual input space of LLMs, thereby improving the models' performance in molecule-related tasks.\n\n**Commonalities:**\n- **Enhancement of LLMs**: Each paper enhances LLMs to better understand molecules by incorporating additional components or datasets.\n- **3D Molecular Representation**: All papers emphasize the importance of 3D molecular representations in capturing molecular dynamics and properties.\n- **Versatile Tasks**: They all evaluate the enhanced models on a variety of molecule-related tasks, including molecule-to-text translation, property prediction, and retrosynthesis.\n\n**Innovations:**\n- **3D-MoLM**: Introduces a 3D molecule-text projector and a 3D molecule-centric instruction tuning dataset (3D-MoIT) to align 3D molecular structures with textual inputs.\n- **ICMA**: Proposes a novel paradigm that leverages in-context learning to adapt LLMs to molecule-caption translation tasks, without the need for extra pre-training stages.\n- **MolX**: Enhances LLMs by integrating SMILES strings, 2D molecular graphs, and handcrafted molecular fingerprints, and employs a versatile pre-training strategy to align these representations with the LLM's input space.\n\n#### Comparison of Results\n\nThe three papers report significant improvements in various molecule-related tasks, with each framework demonstrating unique strengths and advantages.\n\n**Molecule-Text Retrieval and Captioning:**\n- **3D-MoLM**: Achieves state-of-the-art performance in molecule-text retrieval and captioning tasks, surpassing existing baselines by 20% accuracy and 6.47 ROUGE-L, respectively.\n- **ICMA**: Shows substantial improvements in both Mol2Cap and Cap2Mol tasks, achieving 0.581 BLEU-4 and 0.460 exact-matched scores on ChEBI-20 dataset.\n- **MolX**: Outperforms baselines in molecule-to-text translation tasks, achieving the highest scores on all evaluation metrics, especially for ROUGE-based and METEOR metrics.\n\n**Molecule Property Prediction:**\n- **3D-MoLM**: Demonstrates superior performance on computed property QA tasks, especially on properties intrinsically determined by 3D conformations, such as HOMO-LUMO Gap.\n- **ICMA**: Achieves state-of-the-art performance in molecule property prediction, with significant improvements in accuracy and F1 scores across various subsets of the MoleculeNet dataset.\n- **MolX**: Shows remarkable improvements in property prediction tasks, achieving the best scores in 6 out of 8 subsets of the MoleculeNet dataset.\n\n**Molecule Optimization:**\n- **3D-MoLM**: Enhances the model's ability to follow instructions and generate valid SMILES strings, reducing hallucinations and improving the validity of generated molecules.\n- **ICMA**: Improves the performance of LLMs in molecule optimization tasks, achieving superior results in most metrics compared to baselines and generalist chemical LLMs.\n- **MolX**: Boosts the base LLM's performance in molecule optimization, reducing hallucinations and improving the validity of generated SMILES strings.\n\n**Retrosynthesis:**\n- **3D-MoLM**: Achieves better performance in retrosynthesis tasks, generating more accurate SMILES strings for reactants.\n- **ICMA**: Surpasses baselines and robust supervised learning models in most metrics, demonstrating the effectiveness of in-context learning for molecule-related tasks.\n- **MolX**: Improves the base LLM's performance in retrosynthesis, achieving superior results in most metrics compared to existing models.\n\n#### Conclusion\n\nThe three papers introduce innovative frameworks and datasets to enhance LLMs' understanding of 3D molecular structures and improve their performance in molecule-related tasks. 3D-MoLM, ICMA, and MolX each contribute unique methodologies and components to address the limitations of LLMs in comprehending complex molecular geometries. 3D-MoLM employs a 3D molecule-text projector and a 3D molecule-centric instruction tuning dataset, ICMA leverages in-context learning to adapt LLMs to molecule-caption translation tasks, and MolX integrates SMILES strings, 2D molecular graphs, and handcrafted molecular fingerprints into LLMs.\n\nExperimental evaluations across various tasks, including molecule-to-text translation, property prediction, optimization, and retrosynthesis, demonstrate that these frameworks consistently outperform existing baselines. The results highlight the effectiveness of incorporating 3D molecular information and multi-modal representations into LLMs, significantly enhancing their performance in biomolecular applications.\n\nFuture research directions include exploring advanced cross-space alignment techniques for better integration of molecular and textual spaces, developing novel generalist chemical LLMs enhanced with multi-modal extensions, and leveraging emerging capabilities of LLMs like in-context learning and chain-of-thought reasoning for molecule-related tasks. Additionally, there is a need to investigate the potential of larger, more sophisticated LLMs in handling complex molecular tasks and to further enrich datasets with high-quality texts closely related to 3D molecular structures."
}