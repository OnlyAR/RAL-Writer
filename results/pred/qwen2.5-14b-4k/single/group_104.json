{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Long Document Summarization with Top-down and Bottom-up Inference}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nText summarization aims to condense long documents and retain key information. Critical to the success of a summarization model is the faithful inference of latent representations of words or tokens in the source documents. Most recent models infer the latent representations with a transformer encoder, which is purely bottom-up. Also, self-attention-based inference models face the challenge of quadratic complexity with respect to sequence length. We propose a principled inference framework to improve summarization models on these two aspects. Our framework assumes a hierarchical latent structure of a document where the top-level captures the long range dependency at a coarser time scale and the bottom token level preserves the details. Critically, this hierarchical structure enables token representations to be updated in both a bottom-up and top-down manner. In the bottom-up pass, token representations are inferred with local self-attention to leverage its efficiency. Top-down correction is then applied to allow tokens to capture long-range dependency. We demonstrate the effectiveness of the proposed framework on a diverse set of summarization datasets, including narrative, conversational, scientific documents and news. Our model achieves (1) competitive or better performance on short documents with higher memory and compute efficiency, compared to full attention transformers, and (2) state-of--the-art performance on a wide range of long document summarization benchmarks, compared to recent efficient transformers. We also show that our model can summarize an entire book and achieve competitive performance using $0.27\\%$ parameters (464M vs. 175B) and much less training data, compared to a recent GPT-3-based model. These results indicate the general applicability and benefits of the proposed framework. \n\\end{abstract}\n\n\\section{Introduction}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.96\\textwidth]{figures/top-down-transformer-v2.png}\n    \\caption{\\footnotesize An overview of the top-down transformer. Suppose a document with 7 tokens is the inputs to the model, as shown on the bottom left. The bottom-up inference is achieved with local self-attention ($N_1$ layers) as shown in the left panel. To initialize the top-level representations, we pool bottom-up-inferred token representations with either equal weights or adaptive weights (see Section~\\ref{sec:pooling} for details). Top-level representations are then updated with full self-attention ($N_2$ layers) to capture global context. They are then used to update bottom-up-inferred token representations, accounting for the top-down update for token representations, as shown in the middle panel. The final token representations are attended by the decoder to generate a summary. Note that inference is used in the sense of statistical inference for latent variables and does not imply no training. }\n    \\label{fig:top-down-transformer}\n\\end{figure}\n\nText summarization involves compressing a document and preserving key content and meaning. It can be done in either an extractive or abstractive manner. While an extractive summarization model extracts salient fragments (e.g., words, sentences) from the source document to form a summary, an abstractive summarization system aims to generate a semantically coherent and linguistically fluent summary by conditioning on the document. The abstractive approach aligns better with how a human does summarization and generally performs better than extractive models in recent works~\\citep{pilault-etal-2020-extractive, zhang2020pegasus}. We thus focuses on abstractive summarization.\n\nThe dominant approach for abstractive summarization is to use a Seq2Seq model~\\citep{sutskever2014sequence} with an encoder-decoder architecture instantiated with either RNNs~\\citep{hochreiter1997long} or, more recently, transformers~\\citep{vaswani2017attention}. In such a model, an encoder infers the latent representations of observed tokens (words or subwords) in the document, conditioning on which a decoder generates a summary. This paper studies the problem of how to infer good latent representations, which in turn would improve summarization. We propose a framework which (1) assumes a multi-scale latent structure of a document and (2) synergizes bottom-up inference with top-down inference. In a multi-scale structure, high-level variables (like those representing sentences, segments) model the document at a coarser time-scale and abstract away details, and are suitable for capturing long range dependency of the document; in contrast, low-level variables (like those representing tokens) preserves details, and prevent the summary from losing key details. In our framework, the summary is generated by conditioning on token representations (low-level variables), similar to recent abstractive summarization models \\citep{zhang2020pegasus, zaheer2020big, beltagy2020longformer}. There is however a critical difference. In our framework, token representations are first bottom-up inferred and then top-down updated with high level representations, hence rendering low-level representations aware of long range information. We hypothesize that the proposed inference approach would improve summarization.\n\nMulti-level models have been widely studied in modeling for images~\\citep{sonderby2016ladder}, speech~\\citep{mehri2016samplernn}, and language~\\citep{chung2016hierarchical}. Prior summarization works~\\citep{cheng-lapata-2016-neural, nallapati2016abstractive, zhang-etal-2019-hibert, xu-etal-2020-unsupervised} have also explored hierarchical models. But they mostly focus on extractive summarization and follow a bottom-up inference approach. They pool information in words or sub-words to form sentence representations, based on which a classification is done to make an extraction decision. \n\nIn comparison, our framework combines bottom-up and top-down inference. This draws direct inspiration from a line of work which examines variational inference for hierarchical top-down generative models~\\citep{sonderby2016ladder, maaloe2019biva, child2020very}. In these models, in the bottom-up path distribution parameters of higher level stochastic variables are computed as a function of lower level stochastic variables, while in the top-down path distribution parameters of lower level variables are corrected for as a function of higher level variables. Although we do not assume stochasticity of the document latent representations, our encoder or inference model follows the same idea to infer token representations. \n\nThe proposed framework is agnostic to model architecture. Due to the dominance of transformer models in NLP \\citep{chen-etal-2018-best, zhang2020pegasus, sun-etal-2019-utilizing, martin-etal-2020-camembert} and to leverage pre-trained language models \\citep{Liu2019RoBERTaAR, lewis-etal-2020-bart}, we instantiate our framework with a transformer-based model. There is a bottleneck of applying transformers to long documents, because its computational and memory cost has a quadratic dependency on the sequence length. This issue is especially critical for summarization since we are more interested in summarizing long documents since short ones can be quickly read through by humans. To address this issue, a large amount of prior works have been devoted to developing efficient transformers with sub-quadratic complexity. They approach this problem with kernel-based methods \\citep{katharopoulos_et_al_2020, choromanski2020rethinking}, by low-rank approximation to the attention matrix~\\citep{wang2020linformer}, by synthesizing the attention weights \\citep{tay2021synthesizer}, or by designing content-independent \\citep{Child2019GeneratingLS, beltagy2020longformer, ainslie-etal-2020-etc, zaheer2020big} or content-dependent sparse attention mechanisms \\citep{kitaev2020reformer, roy-etal-2021-efficient, wang-etal-2021-cluster}.  \n\nOur framework provides a natural way to diminish this quadratic complexity issue. In the bottom-up inference, we use local self-attention where each token only attends tokens within a local fixed-length window, and thus the complexity does not grow as a function of the input sequence length. The top-down correction for the token representations enable them to capture long-range context, reducing the limitation of local attention. Furthermore, in contrast to most prior efficient transformers that are incompatible with pre-training language models, our framework is flexible for leveraging any pre-trained encoder-decoder models such as BART~\\citep{lewis-etal-2020-bart}, T5~\\citep{raffel2020exploring}. \n\nWe call the transformer-based model following the proposed framework as top-down transformer, to emphasize the importance of the top-down inference.  We evaluate the top-down transformer on a set of distinct summarization benchmarks. These benchmarks cover documents from a variety of domains, including news articles and scientific, conversational, and narrative documents, and of various lengths ranging from hundreds of words (e.g., a news article), several thousands to over ten thousands of words (e.g., a scientific paper, a book chapter), to even over hundred thousands of words (e.g., an entire book). On short documents, models following our framework achieves on-par or better summarization performance than models with full self-attention, and are more compute-and memory-efficient. Across all long document datasets, our models achieve state-of-the-art performance. In the end, we show that our model is able to summarize a whole book. Compared to a concurrent work~\\citep{wu2021recursively} using GPT-3 and requiring humans to extensively label data, our model achieves competitive performance with 380 times less parameters and a small amount of publicly available data. The diverse and strong empirical results support the effectiveness and wide applicability of the proposed model.\n\n\\section{Methods}\nFigure~\\ref{fig:top-down-transformer} gives a graphical overview of the top-down transformer, instantiating the proposed framework. We introduce its details in this section. Suppose a document has $N$ tokens, $\\bm{t} = \\{t_i\\}_{i=1}^N$. In our method, token representations are inferred by combining top-down and bottom-up inference. This leads to effective and efficient inference for token representations. They are then attended by a decoder to generate a summary, as in a regular encoder-decoder transformer.\n\n\\subsection{Bottom-Up Inference}\nIn the bottom-up inference, contextual embeddings of the tokens, $\\{e_i \\mid e_i \\in \\mathbb{R}^d\\}_{i=1}^N$, are computed with $N_1$ layers of local self-attention. In particular, each token $t_i$ only attends to nearby tokens within a window of size of $w$. The complexity is hence $O(Nw)$, in contrast to $O(N^2)$ for a full self-attention models. \\textcolor{black}{Please see Supplementary~\\ref{sec:supp-local-self-attn} for more details.} \n\n\\subsection{Top-Down Inference}\n\\label{sec:top-down}\nThe efficiency with local self-attention in the bottom-up inference nevertheless comes with a limitation, that is, each $e_i$ only captures the context within a local window instead of that of the whole document. To mitigate this issue, we propose a top-down inference for token representations.\n\nConsider a two-level multi-scale latent structure for a document. The low level consists of token representations, $\\{e_i\\}_{i=1}^N$, computed by the bottom-up inference. The top level consists of units at a coarser level. It is affordable to apply full self-attention at the top level due to its coarser granularity, allowing these top-level units to capture global document context. In our work, the self-attention mechanism for the top-level representations is simply the original multi-head self-attention proposed in \\cite{vaswani2017attention}. Readers are referred to \\cite{vaswani2017attention} for details.\n\nDenote the top level representations after self-attention update as $\\{s_j \\mid s_j \\in \\mathbb{R}^d\\}_{j=1}^M$ (see Section~\\ref{sec:pooling} for details on top-level representation initialization methods). We can then update the bottom-up-inferred token representations with the top-level representations. This is achieved with $N_3$ top-down inference layers, as illustrated by the middle panel in Figure \\ref{fig:top-down-transformer}. Each layer contains three transformations on $\\{e_i\\}$: (1) token self-attention, (2) token-segment cross-attention, (3) feed-forward. (1) and (3) are the same as those in the bottom-up inference layers or regular self-attention layer with local attention. (2) implementing the cross-attention between the top and bottom levels is the critical operation. In particular, each $e_i$ is updated with cross-attention, \n\\begin{equation}\n    \\label{eq:cross-attention}\n    \\tilde{e}_i = e_i + {\\rm LayerNorm}(\\sum_{j=1}^M \\alpha_{ij} f_v(s_j)), \\quad \\alpha_{ij} = \\frac{\\exp{(f_q(e_i)^T f_k(s_j)})}{\\sqrt{d}\\sum_{l=1}^M \\exp{(f_q(e_i)^T f_k(s_l))}}\n\\end{equation}\nwhere $f_q, f_k,$ and $f_v$ indicate query, key, and value linear mappings, respectively. For notional clarity, Equation~\\ref{eq:cross-attention} only illustrates the case with a single attention head. In practice, we use multi-heads. The cross-attention operation injects global contextual information into bottom-up-inferred token representations, $e_i$, and yields global-context-aware token representations, $\\tilde{e}_i$, conditioning on which a summary can be generated by a decoder. \n\nTo instantiate the top-down inference, we need to make two choices: (1) the number of top-levels above the token level and (2) the unit representation for each top-level. We choose to use one top level since it is sufficiently coarser to apply full self-attention for a wide range of long document benchmarks we experimented on. A natural choice for top level units is sentence, paragraph, and chapter, depending on the number top level considered. Such a choice however might lead to complicated implementations and non-scalability due to the varying length of these units. We hence choose a simpler approach, where the top level consists of fixed-length segments of the documents. While we use a single top level, multiple top levels can be simply achieved with segments with increasingly coarser granularity. \n\nIn the top-down inference, segment-level self-attention has a complexity of $O(M^2)$, and token-segment cross-attention has a complexity of $O(NM)$. Thus, together with bottom-up inference, the complexity is $O(Nw + M^2 + NM)$. In practice, we use relatively small $w$ (window size) and $M$ (number of segments).\n\n\\subsection{Pooling Methods}\n\\label{sec:pooling}\nAs aforementioned, we use a single top level, consisting of fixed-length segments, in the current work. The segment representations are initialized by pooling token representations. Following the notation above, suppose a document is divided into $M$ segments, and the embedding of the $j$th segment is initialized as,\n\\begin{equation}\n    s_j^{(0)} = \\sum_{n=1}^k p_n e_{j \\times d + n}\n\\end{equation}\nwhere $k$ is the kernel size and $d$ is the stride. $p_n$ is the weight for the $n$th token. We introduce two approaches to compute the weights. The first method is average pooling \\textcolor{black}{(AvgPool)} and hence $p_n = \\frac{1}{k}$, which is simple and convenient. In the second approach, we leverage the reference summary to define the importance of each token to assign adaptive weights \\textcolor{black}{(AdaPool)}. Particularly, we learn an importance tagger with labels constructed with the reference summaries, which involves three steps:\n\\begin{enumerate}\n    \\item construct training labels for the importance tagger: (1) word lemmatization for document and reference words; (2) label a document word as important if it appears in the reference word list and is a non-stopword\n    \\item train a top-down transformer encoder with constructed labels as the importance tagger\n    \\item train the summarization model with oracle weights (i.e., constructed labels from Step 1.) and test it with the adaptive importance weight assigned by the learned tagger\n\\end{enumerate}\n\n\\textcolor{black}{In our experiments, we also used OracleAdaPool where the weights are obtained from Step 1 with the reference summaries.}\nNote that if $\\{p_n\\}_{n=1}^k$ does not form a valid probability distribution, $s_j$ can be computed with a normalized weight distribution within each pooling window as follows,\n\\begin{equation}\n    s_j^{(0)} = \\frac{\\sum_{n=1}^k \\exp(p_n) e_{j \\times d + n}}{\\sum_{n=1}^k \\exp(p_n)}.\n\\end{equation}\n$\\{s_j^{(0)}\\}_{j=1}^M$ are updated with self-attention, yielding $\\{s_j\\}_{j=1}^M$, which are then used in top-down inference for token representations, as discussed in Section~\\ref{sec:top-down}. \n\n\\section{Experiments}\n\nWe thoroughly evaluate the proposed framework on distinct summarization datasets. See Table~\\ref{table:datasets} for a summary of datasets used in the current work. Our model is first evaluated on two standard long document summarization benchmarks, PubMed and arXiv~\\citep{cohan-etal-2018-discourse}. It outperforms various efficient transformers and other approaches and achieves state-of-the-art performance. Although we focus on long document summarization, models under our framework is also applicable to shorter documents. We test our model on CNN-Dailymail~\\citep{see-etal-2017-get}, the most widely used short summarization dataset. Compared to a full self-attention model, our model achieves competitive or better performance but is more memory- and compute-efficient. Recently, a more challenging benchmark, SummScreen \\citep{chen2021summscreen}, is proposed, where summarization systems need to summarize TV show scripts. These documents convey plot events often indirectly and implicitly in dialogues, in contrast to news and scientific articles where statements follow a logical order and facts are offered explicitly. Moreover, a typical episode contains multiple subplots that proceed in parallel. Solving this benchmark thus requires a system to draw information from utterances spreading out through the entirety of the input and integrate them to a concise description. Our model outperforms strong baselines on this challenging benchmark by a significant margin. Another challenging dataset, BookSum~\\citep{kryscinski2021booksum}, is also recently released. It covers books from the literature domain, including stories, plays, and novels. Similar to ScreenSum, it requires integrating plot events from indirectly expressed descriptions. A further challenge is to process long-form texts up to hundreds of pages or over 100,000 words. A model under our framework does well on this challenge, achieving competitive or superior performance compared to a concurrent work~\\citep{wu2021recursively} using GPT-3. While the GPT-3-based model has 175 billion parameters and requires human labelers to extensively write summaries and provide reward information, our model with 464 million parameters is 380 times smaller and merely requires training on relatively minimal data. These results suggest our framework is a generally effectively for documents of various lengths, domains. \n\n\\begin{table*}[h!]\n\\small\n\\centering\n\\begin{tabular}{l c c c c} \n\\toprule\nDataset & \\# Docs. & \\# Input Words & \\# Summary Words & Domain\\\\\n\\hline\nPubMed &133K & 3,224 & 214 & Scientific \\\\ \narXiv &215K & 6,913 & 292 & Scientific \\\\ \nTVMegaSite & 22.5K & 6,420 & 380 & Conversational \\\\\nForeverDreaming & 4.3K & 7,605 & 113  & Conversational \\\\\nBookSum-Chapter-Level & 12K & 5,102 & 505 & Narrative \\\\\nBookSum-Book-Level & 436 & 112,885 & 1,167 & Narrative \\\\\nCNN-DM & 311K & 906 & 63 & News \\\\ \n\\hline\n\\end{tabular}\n\\caption{\\scriptsize Summarization Datasets. It shows the total number of documents, the average number of input words, the average number of summary words, and the domain for each dataset.}\n\\label{table:datasets}\n\\end{table*}\n\nWe use the same encoder-decoder architecture for all datasets. The encoder has 8 bottom-up inference layers and 4 top-down inference layers for tokens, and 2 self-attention layers for segments. The decoder has 12 layers. The encoder layers for tokens (12 layers) and the decoder layers are all initialized from BART~\\citep{lewis-etal-2020-bart} except the parameters for token-segment cross-attention in the top-down inference layers, which are randomly initialized. The self-attention parameters for segments are also randomly initialized. The window size is $1024$ unless otherwise specified. Our settings closely follow Longformer~\\citep{beltagy2020longformer} which has 12 layers for the encoder and decoder, is initialized from BART, and uses a local window size of $1024$. Thus, comparison with Longformer is a test of the effect of top-down correction for token representations. PubMed, arXiv, and CNN-DailyMail are obtained from Huggingface Datasets \\footnote{https://huggingface.co/datasets}. SummScreen and BookSum are provided by the authors. Standard train/validation/test splits, provided by either Huggingface or the dataset authors, are used for all datasets. Model performance is evaluated with ROUGE scores~\\citep{lin-2004-rouge}. Reported performance is based on the checkpoint with the best validation R-2 score. Summary samples for each dataset generated by our models are provided in the appendix. \n\n\\subsection{Scientific Documents}\nWe first test the effectiveness of our framework on two widely used datasets based on scientific documents, PubMed and arXiv. They consists of long documents of length ranging from several thousands of words to over ten thousands words. Each document in PubMed is a scientific article, collected from PubMed.com, and the reference summary is the associated abstract. Documents in arXiv are collected from arxiv.org. Three variants of our model with various pooling weights are presented. AvgPool, AdaPool, and OracleAdaPool in Table~\\ref{table:pubmed} indicate average pooling, pooling with adaptive weights, pooling with adaptive weights determined by references, respectively. \n\n\\begin{table}[h!]\n\\small\n\\centering\n\\begin{tabular}{c l c c c c c c} \n\\toprule\n& & \\multicolumn{3}{c}{\\textbf{PubMed}} & \\multicolumn{3}{c}{\\textbf{arXiv}}\\\\\n& & R-1 & R-2 & R-L & R-1 & R-2 & R-L\\\\\n\\hline\n&Pegasus \\textcolor{black}{(568M)} & 44.21 & 16.95 & 38.83 & 44.21 & 16.95 & 38.83 \\\\ \n&Dancer & 46.34 & 19.97 & 42.42 & 45.01 & 17.60 & 40.56\\\\ \\cline{2-8} \n&TLM-I+E & 42.13 & 16.27 & 39.21 & 41.62 & 14.69 & 38.03\\\\ \n&SSN-DM & 46.73 & 21.00 & 34.10 & 44.90 & 19.06 & 32.77\\\\ \\cline{2-8}\n&BigBird \\textcolor{black}{(577M)} & 46.32 & 20.65 & 42.33 & 46.63 & 19.02 & 41.77\\\\ \n&Longformer \\textcolor{black}{(460M)} & 46.97 & 20.23 & 42.88 & 46.63 & 19.62 & 41.83\\\\ \n&LSH & 48.12 & 21.06 & 42.72 & - & - & -\\\\ \n\\hline\n\\multirow{3}{*}{\\bf Ours} & \\todof (AvgPool) \\textcolor{black}{(464M)} & \\underline{48.34} & \\underline{21.40} & \\underline{44.22} & \\underline{48.67} & \\underline{20.70} & \\underline{43.91}\\\\ \n    & \\todof (AdaPool) \\textcolor{black}{(464M)} & \\textbf{51.05} & \\textbf{23.26} & \\textbf{46.47} & \\textbf{50.95} & \\textbf{21.93} & \\textbf{45.61}\\\\ \\cline{2-8}\n    & \\todof (OracleAdaPool) & 55.15 & 26.55 & 50.25 & 64.16 & 33.39 & 56.88\\\\ \n\\hline\n\\end{tabular}\n\\caption{\\scriptsize Results on Scientific Articles. Best performance (not relying on oracle) is in bold, and the second best is underlined.}\n\\label{table:pubmed}\n\\end{table}\n\nThe experiment results are displayed in Table~\\ref{table:pubmed}. Pegasus~\\citep{zhang2020pegasus} is pretrained on a large-scale of dataset with a pretraining objective specifically designed for summarization. It uses a full self-attention encoder and thus has to truncate the source document due to the quadratic memory complexity. The summarization-oriented large-scale pre-training makes it a strong baseline. Dancer~\\citep{gidiotis2020divide} takes a divide-and-conquer approach in which the summary is divided into sections and each section is paired to the appropriate section of the document and the model is trained on short sequences and has a low memory requirement. This is a straightforward approach achieving strong performance.\n\nTLM-I+E~\\citep{pilault-etal-2020-extractive} first extracts salient sentences and then uses a GPT-style model to generate a summary by conditioning on the introduction section and extracted sentences (instead of the whole document), thus reducing memory requirement. SSN-DM~\\citep{cui-hu-2021-sliding} is an extractive model and uses a sliding encoder to process segments of a document and a memory module to capture autoregressive dependency between segments. These two models bear similarities to our model in that they use a multi-scale structure. The extracted only salient sentences in TLM-I+E can be considered a representation of the document at a coarser granularity since salient information is retained. Instead of keeping the coarser representations in the latent space, TLM-I+E reads out them to the observed word space. In SSN-DM, the fixed-size memory module pooling information from each segments can also be considered a high level representation of the document. Despite these similarities, our model, following a principled framework to synergize bottom-up and top-down inference, clearly outperforms these prior models.  \n\nBigBird~\\citep{zaheer2020big}, Longformer~\\citep{beltagy2020longformer}, and LSH~\\citep{kitaev2020reformer, huang-etal-2021-efficient} are efficient transformers. BigBird based on Pegasus pre-training combines local attention, random attention tokens, and global attention tokens. LSH uses content-dependent sparse attention based on local sensitivity hashing. Longformer is closely related to our models. It uses the same local attention as in our bottom-up inference except it has an extra [CLS] token which is a global attention token. Longformer is also initialized from BART, same as ours. The only difference is that our models infer token representations with both top-down and bottom-up inference, in contrary to pure bottom-up inference in Longformer. The clear performance improvement over Longformer and other efficient transformers indicates the effectiveness of the synergy of bottom-up and top-down inference.   \n\n\\subsection{Short Documents}\n\n\\begin{table}[h!]\n\\small\n\\centering\n\\begin{tabular}{c l c c c} \n\\toprule\n& \\multicolumn{4}{c}{\\textbf{CNN-DailyMail}}\\\\\n& & R-1 & R-2 & R-L \\\\\n\\hline\n& BART (Reported) & 44.15 & \\underline{21.28} & 40.90 \\\\ \n& BART (Re-eval) & 43.93 & 20.81 & 40.79 \\\\ \n\\hline\n\\multirow{3}{*}{\\bf Ours} & \\todof (AvgPool) & \\underline{44.32} & 21.03 & \\textbf{41.40} \\\\ \n    & \\todof (AdaPool) & \\textbf{44.85} & \\textbf{21.31} & \\underline{41.15} \\\\ \\cline{2-5}\n    & \\todof (OracleAdaPool) & 63.87 & 38.42 & 59.10 \\\\ \n\\hline\n\\end{tabular}\n\\caption{\\scriptsize Results on CNN-DailyMail. Best performance (not relying on oracle) is in bold, and the second best is underlined.}\n\\label{table:cnn}\n\\end{table}\n\nTo demonstrate the general applicability of the proposed framework, we show its efficiency and effectiveness on short document summarization and compare it to full self-attention inference model. We hypothesize that although the bottom-up inference uses local self-attention (for efficiency), the top-down correction would enable the effectiveness of our inference and hence lead to competitive or better summarization performance. \n\nOur model parameters are initialized from BART. Hence, BART with full self-attention forms a natural baseline, allowing for direct comparison. In the bottom-up inference, the local attention window size is $256$. As shown in Table~\\ref{table:cnn}, models under our framework achieve slightly better performance, especially in terms of R-1 and R-L, than BART. It confirms our hypothesis that a synergy of bottom-up inference with local attention and top-down inference with global attention is effective and achieves on-par or better performance as full self-attention.\n\n\\subsection{SummScreen}\n\n\\begin{table}[h!]\n\\small\n\\centering\n\\begin{tabular}{c l c c c c c c} \n\\toprule\n& & \\multicolumn{3}{c}{\\textbf{TVMegaSite}} & \\multicolumn{3}{c}{\\textbf{ForeverDreaming}}\\\\\n& & R-1 & R-2 & R-L & R-1 & R-2 & R-L \\\\\n\\hline\n& Extractive Oracle & 49.0 & 11.6 & 46.9 & 38.8 & 11.5 & 33.9 \\\\ \\cline{2-8} \n& Longformer & 42.9 & 11.9 & 41.6 & 25.9 & 4.2 & 23.8 \\\\ \n& Hybrid (BART + Content Selection) & 38.8 &10.2 & 36.9 & 25.3 & 3.9 & 23.1 \\\\\n& Hybrid (BART + Oracle Content Selection) & 42.1 & 11.9 & 40.9 & 26.4 & 5.0 & 23.3 \\\\\n\\hline\n\\multirow{3}{*}{\\bf Ours} & \\todof (AvgPool) & \\underline{49.30} & \\underline{14.35} & \\underline{47.45} & \\underline{35.84} & \\underline{8.86} & \\underline{30.62} \\\\ \n    & \\todof (AdaPool) & \\textbf{51.02} & \\textbf{14.66} & \\textbf{49.01} & \\textbf{36.84} & \\textbf{9.19} & \\textbf{31.12} \\\\ \\cline{2-8}\n    & \\todof (OracleAdaPool) & 53.55 & 15.63 & 51.29 & 39.54 & 10.08 & 33.59 \\\\ \n\\hline\n\\end{tabular}\n\\caption{\\scriptsize Results on SummScreen. Best performance (not relying on oracle) is in bold, and the second best is underlined.}\n\\label{table:summ-screen}\n\\end{table}\n\nScientific and news articles often require that facts are offered explicitly and statements follow a logical order, which might allow summarization models to exploit layout and stylistic biases. We next test the proposed framework on a more challenging dataset, SummScreen, which requires a model to draw and integrate information from indirect expressions across a wide range of the document. SummScreen~\\citep{chen2021summscreen} provides two datasets, TVMegaSite and ForeverDreaming, collecting from two different TV show transcript websites. Each document is the transcript of a TV show episode and the summary is an associated recap.\n\nTable~\\ref{table:summ-screen} summarizes the results. Extractive oracle is an extractive method by extracting nearest neighbors based on Rouge scores. Longformer is an abstractive method and takes the whole document as input. Hybrid models first select salient sentences and then input them to BART. Our models outperform these strong baselines and even achieves comparable or superior performance than those having access to oracle information.\n\n\\subsection{BookSum}\nBookSum~\\citep{kryscinski2021booksum} is another challenging dataset, consisting of books from the literature domain including stories, plays and novels. It includes examples on three levels of granularity with increasing difficulty: (1) paragraph-level with inputs with hundreds of words, (2) chapter-level, with inputs with several thousands or over ten thousands of words, (3) book-level, with inputs spanning up to hundreds of pages and over hundred thousands of words. The chapter-level examples have comparable lengths to other popular long-form summarization datasets such as PubMed, arXiv. We first test our models on the chapter level. The book-level summarization is extremely challenging. First, the number of examples (313 books) is limited. Second, a book is too long to fit in current models. We train our model in a curriculum and recursive way to address the two issues. \n\n\\subsubsection{Chapter Level}\n\n\\begin{table}[h!]\n\\small\n\\centering\n\\begin{tabular}{c l c c c} \n\\toprule\n& \\multicolumn{4}{c}{\\textbf{BookSum Chapter Level}}\\\\\n& & R-1 & R-2 & R-L \\\\\n\\hline\n&Extractive Oracle  & 42.68 & 9.66 & 21.33 \\\\ \\cline{2-5} \n&BART \\textcolor{black}{(406M)}  & 37.09 & 8.23 & 15.37 \\\\ \n&T5 \\textcolor{black}{(738M)} & 37.38 & 8.42 & 16.77 \\\\ \n&Pegasus \\textcolor{black}{(568M)} & 36.17 & 7.79 & 16.09 \\\\ \\cline{2-5}\n&Longformer  \\textcolor{black}{(460M)} & 32.84 & 7.45 & 14.59 \\\\ \n&BigBird \\textcolor{black}{(577M)} & 31.78 & 6.50 & 14.17 \\\\ \n\\hline\n\\multirow{3}{*}{\\bf Ours} & \\todof (AvgPool) \\textcolor{black}{(464M)} & \\underline{37.99} & \\underline{9.10} & \\underline{18.02} \\\\ \n    & \\todof (AdaPool) \\textcolor{black}{(464M)} & \\textbf{38.34} & \\textbf{9.19} & \\textbf{18.08} \\\\ \\cline{2-5}\n    & \\todof (OracleAdaPool) & 41.10 & 9.49 & 19.19 \\\\ \n\\hline\n\\end{tabular}\n\\caption{\\scriptsize Results on BookSum Chapter Level. Best performance (not relying on oracle) is in bold, and the second best is underlined.}\n\\label{table:booksum-chapter}\n\\end{table}\n\nTable~\\ref{table:booksum-chapter} displays the results. \\cite{kryscinski2021booksum} takes a divide-and-conquer approach to summarize chapters. They finetune BART, T5, and Pegasus on the paragraph level data and the chapter summary is obtained by concatenating the paragraph summary. This might miss the intra-paragraph context. Our models directly summarize the whole chapters and outperform these divide-and-conquer models. Efficient transformers, Longformer and BigBird, are also able to take in the whole chapters as inputs. But these bottom-up approaches clearly underperform our models.  \n\n\\subsubsection{Book Level}\nWe first train a top-down transformer on the chapter-level data and then fine-tune it on the book-level data. The inputs to the book-level model are (1) the concatenated chapter reference summaries in training or (2) the concatenated chapter summaries generated by the chapter-level model in testing. The chapter-to-book curriculum training is to mitigate the scarcity of book-level data. The recursive summarization of chapters and then books can be considered abstractive content selection applied to book data, and is used to address the extremely long length of books.\n\n\\begin{table}[h!]\n\\small\n\\centering\n\\begin{tabular}{l c c c} \n\\toprule\n\\multicolumn{4}{c}{\\textbf{BookSum Book Level}}\\\\\n& R-1 & R-2 & R-L \\\\\n\\hline\nExtractive Oracle  & 46.62 & 9.17 & 18.31 \\\\ \n\\hline\nBART  & 29.97 & 6.02 & 10.97 \\\\ \nT5  &  39.46 & 7.69 & 13.77 \\\\ \nPegasus  & 35.29 & 6.79 & 12.71 \\\\  \n\\hline\n175B full tree RL  & 41.51 & 10.46 & \\underline{16.88} \\\\ \n175B first subtree RL  & \\underline{43.19} & \\underline{10.63} & \\textbf{17.10} \\\\ \n6B full tree RL  & 36.79 & 7.22 & 14.84 \\\\ \n\\hline\n\\todof(464M)  & \\textbf{44.19} & \\textbf{10.89} & 16.13 \\\\ \n\\hline\n\\end{tabular}\n\\caption{\\scriptsize Results on BookSum Book Level. Best performance (not relying on oracle) is in bold, and the second best is underlined.}\n\\label{table:booksum-book}\n\\end{table}\n\nTable~\\ref{table:booksum-book} summarizes the book-level results. The middle section shows the performance for the models with the divide-and-conquer approach~\\citep{kryscinski2021booksum}, same as those for the chapter-level data. A concurrent work~\\citep{wu2021recursively} based on GPT-3 \\textcolor{black}{with reinforcement learning (RL)} also attempts to summarize books. Their method shares similarity with ours in that they decompose books into shorter sequences and train the model and summarize the text segments recursively. There are four major differences between our approach and theirs. First, our model has only 464 million parameters and is 380 times smaller than GPT-3 with 175 billion parameters. Second, we train our model with the limited and publicly available data from BookSum, while \\cite{wu2021recursively} requires human labelers to write summaries and give preference, which is highly costly. Third, our model has lower complexity, allowing it to takes in longer input. Thus, we only need to decompose the book one time (into chapters), in contrast to multiple recursive decomposition steps. Multiple recursive summarization steps is prone to accumulating errors. Forth, GPT-3 uses bottom-up inference to infer token representations, in contrast to the synergy of bottom-up and top-down inference in our approach, which we believe leads to better representation inference. The last two differences might account for our competitive performance using a much smaller model and less data. \n\n\\section{Related Work}\n\n\\paragraph{Summarization Models}\nPrior works have proposed extractive models~\\citep{nallapati2017summarunner, cui-hu-2021-sliding}, abstractive models~\\citep{nallapati2016abstractive, zhang2020pegasus}, and hybrid models combining extractive and abstractive methods~\\citep{gehrmann-etal-2018-bottom, pilault-etal-2020-extractive}, for text summarization. Although our model mostly follows the abstractive approach, it also has connections to the hybrid models. These models usually first extract salient sentences from the source document and then summarize the extracted sentences with an abstractive model. Extracted sentences can be viewed a high level representation of the document, although it is the observed space but not in the latent space as in our framework. A continuous representations in the latent space facilities end-to-end learning. Moreover, assigning importance weight with the importance tagger in our method resembles an extractive step in a hybrid model, and thus top down transformer with learned importance tagger can be considered a hybrid model. \n\n\\paragraph{Inference for Hierarchical Latent Variable Model}\nOur work draws inspiration from the latent variable inference for hierarchical top-down generative models. To faithfully infer multi-layer latent variables needs to account for the dependency between them. MCMC approaches \\cite{nijkamp2020learning} naturally accounts for such dependency. Amortized inference \\cite{sonderby2016ladder, maaloe2019biva, child2020very} makes a special design to capture the multi-layer dependency. In particular, in a bottom-up path, the parameters of the distribution of higher level variables are computed as a function of the lower level variables; in a top-down path, the parameters of the distribution of lower level variables are corrected as a function of the higher level variables.  \n\n\\paragraph{Efficient Transformers}\nDespite the effectiveness of transformers on a variety of tasks, its quadratic complexity with respect to the sequence length has limited its application to problems with long sequences. A large amount of works have attempted to address this limitation. A major line of work focuses on designing various sparse attention mechanisms. These works can be roughly categorized into two groups, depending on whether the sparsity pattern is content-dependent~\\citep{kitaev2020reformer, roy-etal-2021-efficient, wang-etal-2021-cluster} or content-independent~\\citep{Child2019GeneratingLS, beltagy2020longformer, ainslie-etal-2020-etc, zaheer2020big}. Our work is mostly related to content-independent sparse attetntion. A main assumption of content-independent sparse attention is that the context temporally and/or spatially proximate to the query token is more important, which is intuitively sensible and supported by empirical attention analysis \\citep{Child2019GeneratingLS}. Thus, a common and basic sparse attention pattern is local attention, where each query token only attends to a neighborhood within a fixed temporal and/or spatial window. While this reduces the complexity to be linear, a model with only local attention cannot model long-range dependency. Prior works combine local attention with other attention patterns with wider or global receptive field such as dilated attention, random attention tokens, and global attention tokens~\\citep{beltagy2020longformer, zaheer2020big}. Our models also use local attention for its efficiency and leverage top-down inference to enable global-context awareness. \n\n\\section{Conclusion}\nIn this work, we propose a principled inference framework to improve latent representation inference for summarization models. It assumes a hierarchical latent structure of a document where the top-level captures the long range dependency at a coarser granularity and the bottom token level preserves the details. We leverage this hierarchical structure and synergize bottom-up inference with top-down inference to improve token representation inference. In the bottom-up pass, token representations are inferred with local self-attention to exploit its efficiency. Top-down correction is then applied to allow tokens to capture long-range dependency. We demonstrate the effectiveness of the proposed framework on a wide range of summarization datasets, including narrative, conversational, scientific documents and news. Our model achieves (1) comparable or superior performance on short documents with higher memory and compute efficiency, compared to full attention transformers, (2) state-of--the-art performance on a wide range of long document summarization benchmarks, compared to recent efficient transformers, and (3) competitive performance on summarizing whole books using $0.27\\%$ parameters and much less training data, compared to a recent GPT-3-based model. These results indicate the general applicability and benefits of the proposed framework. \n\n\\clearpage\n\\appendix\n\\section{Qualitative Examples}\n\n\\begin{table}[!htbp]\n    \\centering\n    \\tiny\n    \\begin{tabular}{p{\\linewidth}} \n    \\toprule\n    \\textbf{PubMed Example \\#1: Reference} \\\\\n    \\midrule\na new class of water - soluble c60 transfecting agents has been prepared using hirschbingel chemistry and assessed for their ability to act as gene - delivery vectors in vitro. in an effort to elucidate the relationship between the hydrophobicity of the fullerene core, the hydrophilicity of the water - solubilizing groups, and the overall charge state of the c60 vectors in gene delivery and expression, several different c60 derivatives were synthesized to yield either positively charged, negatively charged, or neutral chemical functionalities under physiological conditions. these fullerene derivatives were then tested for their ability to transfect cells grown in culture with dna carrying the green fluorescent protein ( gfp ) reporter gene. statistically significant expression of gfp was observed for all forms of the c60 derivatives when used as dna vectors and compared to the ability of naked dna alone to transfect cells. however, efficient in vitro transfection was only achieved with the two positively charged c60 derivatives, namely, an octa - amino derivatized c60 and a dodeca - amino derivatized c60 vector. all c60 vectors showed an increase in toxicity in a dose - dependent manner. increased levels of cellular toxicity were observed for positively charged c60 vectors relative to the negatively charged and neutral vectors. structural analyses using dynamic light scattering and optical microscopy offered further insights into possible correlations between the various derivatized c60 compounds, the c60 vector / dna complexes, their physical attributes ( aggregation, charge ) and their transfection efficiencies. recently, similar gd@c60 - based compounds have demonstrated potential as advanced contrast agents for magnetic resonance imaging ( mri ). thus, the successful demonstration of intracellular dna uptake, intracellular transport, and gene expression from dna using c60 vectors suggests the possibility of developing analogous gd@c60 - based vectors to serve simultaneously as both therapeutic and diagnostic agents.\n    \\\\\n    \\midrule\n    \\textbf{PubMed Example \\#1: Model Output} \\\\\n    \\midrule\na new class of water - soluble c60 transfecting agents with positively charged, negatively charged, or neutral chemical functionalities under physiological conditions was prepared using hirschbingel chemistry. transfection, cytotoxicity, and biophysical structure / activity studies were performed in an effort to elucidate the relationship between the hydrophobicity of the fullerene core, hydrophilicity, and the overall charge state of the c60 vectors in gene delivery / expression. only two positively charged c60 derivatives, namely, an octa - amino derivatized c60 and a dodeca - amino derived c60 vector, showed efficient in vitro dna delivery into cells. increased levels of cellular toxicity were observed for positively charged compounds relative to the negatively charged and neutral vectors, and aggregation was the major factor that negatively affected the cytotoxic profiles of these c60 - vector / dna complexes. the successful demonstration of intracellular dna uptake and gene expression from dna dna conjugates formed from c60 compounds suggests the possibility of developing analogous gd@c60 vectors to serve as both therapeutic and diagnostic agents.\n    \\\\\n    \\end{tabular}\n    \\begin{tabular}{p{\\linewidth}} \n    \\midrule\n    \\textbf{PubMed Example \\#2: Reference} \\\\\n    \\midrule\ncontext : isolated involvement of the appendix in crohn's disease is reported to be 0.2 \\% to 1.8 \\%, and is usually associated with ileocaecal crohn's disease in 25 \\% of ileal and 50 \\% of caecal disease. while appendicitis in a patient who was previously diagnosed to have ileocaecal crohn's may be managed with appendicectomy and ileocaecal resection, appendicectomy alone when performed for appendicitis in a patient with unsuspected ileocaecal crohn's disease could lead to postoperative complications including enterocutaneous fistula.case report : a young female patient who underwent appendicectomy elsewhere for acute appendicitis presented to us with a persistent enterocutaneous fistula of 6 weeks duration. she had complained of general ill health and occasional altered bowel habits for 6 months prior to the acute appendicitis presentation. our investigations, including a ct scan, suggested the possibility of ileocaecal crohn's disease. she underwent excision of the enterocutaneous fistula and ileocaecal resection, and histopathology of the resected specimen confirmed crohn's disease. in the postoperative period she received mesasalazine. when last seen 2 years later during her regular follow - up, she was found to be in good health.conclusion : the possibility of ileocaecal crohn's disease  should be considered in patients presenting with unexplained postoperative enterocutaneous fistula following appendicectomy. a high index of clinical suspicion is required to make a prompt diagnosis and institute appropriate further treatment in form of ileocaecal resection.\n    \\\\\n    \\midrule\n    \\textbf{PubMed Example \\#2: Model Output} \\\\\n    \\midrule\nintroduction : isolated involvement of the appendix in crohn's disease is reported to be 0.2 \\% to 1.8 \\%, and is usually associated with ileocaecal crohn`s disease in 25 \\% of ileal and 50 \\% of caecal disease. while appendicitis in a patient who was previously diagnosed to have ileocecal ileocecal crohn's may be managed with appendicectomies, appendicectomy alone when performed for acute appendiculectomy could lead to postoperative complications including enterocutaneous fistula.case report : a young female patient who underwent appendiclectomy elsewhere presented to us with a persistent enterococcal fistula of 6 weeks duration. on further questioning, she informed us that she had presented to this hospital with a 2 - week history of abdominal pain, confined to the right lower abdomen. she had complained of general ill health and occasional altered bowel habits for 6 months prior to the acute appendectomy presentation. our investigations, including a computerized tomography scan, suggested the possibility of crohns disease. she underwent excision of the enterococcus fistula and resection of the caecum, and histopathology of the resected specimen confirmed the diagnosis. in the postoperative period she received mesasalazine. when last seen 2 years later during her regular follow - up, she was found to be in good health.conclusion : appendicection alone is a safe procedure in patients with suspected crohn ileocoecal involvement. while appendiceal resection will suffice in those who present subacutely and are diagnosed preoperatively by clinical signs, a high index of clinical suspicion is required to make a prompt diagnosis.    \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\n    Summary Samples for PubMed\n    }\n\\end{table}\n\n\\begin{table}[!htbp]\n    \\centering\n    \\tiny\n    \\begin{tabular}{p{\\linewidth}} \n    \\toprule\n    \\textbf{arXiv Example \\#1: Reference} \\\\\n    \\midrule\nwe study experimentally the thermal fluctuations of energy input and dissipation in a harmonic oscillator driven out of equilibrium, and search for fluctuation relations. we study transient evolution from the equilibrium state, together with non equilibrium steady states. fluctuations relations are obtained experimentally for both the work and the heat, for the stationary and transient evolutions. a stationary state fluctuation theorem is verified for the two time prescriptions of the torque. but a transient fluctuation theorem is satisfied for the work given to the system but not for the heat dissipated by the system in the case of linear forcing. experimental observations on the statistical and dynamical properties of the fluctuation of the angle, we derive analytical expressions for the probability density function of the work and the heat. we obtain for the first time an analytic expression of the probability density function of the heat. agreement between experiments and our modeling is excellent.\n    \\\\\n    \\midrule\n    \\textbf{arXiv Example \\#1: Model Output} \\\\\n    \\midrule\n\nwe study the fluctuations of energy and dissipation in a thermostated harmonic oscillator driven out of equilibrium by an external deterministic forcing. this oscillator is very well described by a second order langevin equation. using the fluctuation dissipation theorem, we obtain fluctuation relations for the work given to the system by the external torque and for the heat dissipated by the system, by writing the first principle of thermodynamics between the two steady states. from the experimental results, we derive analytical expressions for the probability density functions of the work and the dissipated heat, and for their finite time corrections, both for a forcing linear in time and a forcing sinusoidal in time. from these expressions, we present some analytical derivations of fluctuation theorems based on experimental observations.\n    \\\\\n    \\end{tabular}\n    \\begin{tabular}{p{\\linewidth}} \n    \\midrule\n    \\textbf{arXiv Example \\#2: Reference} \\\\\n    \\midrule\nthe pairing hamiltonian constitutes an important approximation in many- body systems, it is exactly soluble and quantum integrable. on the other hand, the continuum single particle level density ( cspld ) contains information about the continuum energy spectrum. the question whether one can use the hamiltonian with constant pairing strength for correlations in the continuum is still unanswered. in this paper we generalize the richardson exact solution for the pairing hamiltonian including correlations in the continuum. the resonant and non - resonant continuum are included through the cspld. the resonant correlations are made explicit by using the cauchy theorem. low lying states with seniority zero and two are calculated for the even carbon isotopes. we conclude that energy levels can indeed be calculated with constant pairing in the continuum using the cspld. it is found that the nucleus @xmath0c is unbound. the real and complex energy representation of the continuum is developed and their differences are shown. the trajectory of the pair energies in the continuum for the nucleus @xmath1c is shown.\n    \\\\\n    \\midrule\n    \\textbf{arXiv Example \\#2: Model Output} \\\\\n    \\midrule\nthe exact solution of the richardson pairing hamiltonian is obtained by including the resonant and non resonant continuum through the continuum single particle level density ( cspld ). the gamow states, which appear in the complex energy representation, provide the main contribution from the continuum. the inclusion of the continuum has been used to study the unbound isotope @xmath0c and beyond. it was found that the continuum pairs ( pair energies with positive real components ) converge to the real part of the uncorrelated pair energy and they do not appear in complex conjugate partners. as a result the total energy of the system may be complex. from the exact solutions of the pairing and pairing - like hamiltonians the csmld can be used to investigate many - body correlations.\n\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\n    Summary Samples for arXiv\n    }\n\\end{table}\n\n\\begin{table}[!htbp]\n    \\centering\n    \\tiny\n    \\begin{tabular}{p{\\linewidth}} \n    \\toprule\n    \\textbf{CNN-DailyMail Example \\#1: Reference} \\\\\n    \\midrule\nMichelle MacLaren is no longer set to direct the first \" Wonder Woman \" theatrical movie. MacLaren left the project over \" creative differences \" Movie is currently set for 2017.\n    \\\\\n    \\midrule\n    \\textbf{CNN-DailyMail Example \\#1: Model Output} \\\\\n    \\midrule\nCNN confirms that director Michelle MacLaren is leaving the \" Wonder Woman \" movie. The movie, starring Gal Gadot, is still set for release on June 23, 2017.\n    \\\\\n    \\end{tabular}\n    \\begin{tabular}{p{\\linewidth}} \n    \\midrule\n    \\textbf{CNN-DailyMail Example \\#2: Reference} \\\\\n    \\midrule\nAndrew Mogni, 20, from Glen Ellyn, Illinois, had only just arrived for a semester program when the incident happened in January. He was flown back to Chicago via air on March 20 but he died on Sunday. Initial police reports indicated the fall was an accident but authorities are investigating the possibility that Mogni was robbed. His cousin claims he was attacked and thrown 40 ft from a bridge.\n    \\\\\n    \\midrule\n    \\textbf{CNN-DailyMail Example \\#2: Model Output} \\\\\n    \\midrule\nAndrew Mogni, 20, from Glen Ellyn, Illinois, had only just arrived for a semester program in Italy when the incident happened in January. He was flown back to Chicago via air ambulance on March 20, but he died on Sunday after falling off a 40 ft bridge in Rome in a suspected robbery attack in Rome, police reports indicated the fall was an accident but authorities are investigating the possibility he was robbed.\n    \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\n    Summary Samples for CNN-DailyMail\n    }\n\\end{table}\n\n\\begin{table}[!htbp]\n    \\centering\n    \\tiny\n    \\begin{tabular}{p{\\linewidth}} \n    \\toprule\n    \\textbf{TVMegaSite Example \\#1: Reference} \\\\\n    \\midrule\nJake meets Tad at ConFusion where Tad is enjoying a salad. Tad doesn't believe David's story as to why he is in Gloucester. Liza joins them and serves Tad with a restraining order to stay away from the bar in Gloucester. Amanda takes Trevor for an exam at the hospital and joins Angie. David also joins them. Erica sits alone in her hotel room when Opal comes to ask if she's seen the documentary on Pine Valley. Ryan stares at a blank television when Emma comes downstairs with Corinna. Emma asks Ryan if he is going to watch the documentary. In Gloucester, Gayle comes in to check on Greenlee. Greenlee tells Gayle that she has to get home. Greenlee clinches her fist as she imagines how it would be if she were home. Erica panics as to what Ryan might have said on the documentary. Erica receives another copy of the documentary that Hayley did of Pine Valley. David asks Angie if read the gift that he gave her. Amanda tells David that she knew that he had lied to her about having patients to see at the hospital and instead had gone to Gloucester. Tad reprimands Jake for wanting to go to Gloucester without telling Amanda. Jake gets up to go back to work and sees Amanda and Trevor. Jake asks her how long she had been standing there. Amanda answers, \" Long enough.\" Madison and Angie discuss Madison's mom. Erica finally gives in and agrees to watch the DVD. Opal is thrilled, but Erica insists that she wants to watch it alone. Ryan visits with DVD in hand and suggests that they watch it together. David meets with Dr. Clayton and tells him about Greenlee. Greenlee dreams of her family and friends back in Pine Valley. Erica and Ryan watch the DVD. Liza lets Madison know that her father had gotten jail time, but would be out of jail within a year. Amanda and Jake discuss David and how Jake doesn't believe that he is really sick. Amanda tells Jake that if her persists in accusing David then she doesn't know how much longer they can go on. Greenlee meets with Dr. Clayton about her surgery. Ryan and Erica kiss. Liza and Tad kiss in his apartment.\n    \\\\\n    \\midrule\n    \\textbf{TVMegaSite Example \\#1: Model Output} \\\\\n    \\midrule\nTad and Jake are at Krystal's. Jake lets Tad know that David is going to Gloucester. Liza walks in and gives Tad a restraining order against him. At the hospital, Angie tells Amanda that she had seen her on Hayley's documentary. David walks up and listens to their conversation. At home, Opal questions Erica if she had watched the documentary on television about Pine Valley. Erica tells Opal that she doesn't want to see the documentary. Ryan gets ready for Emma and Corinna's sleepover. Ryan lets Emma know that he hadn't been able to watch the documentary that Hayley had shown on television. Greenlee dreams that she is back in Pine Valley with her family. Erica awakens from her coma and finds out that Jackson is alive. Jackson tells Greenlee how much he had missed her. Erica lets Opal know that she would like to fall in love again. Opal asks her if she is insecure about Ryan and what is going on with him. Erica gets a visit from a man, who gives her a DVD of the documentary from Hayley. David and Angie argue over the fact that he isn't as sick as he claims to be. Amanda accuses David of lying to her about where he had been the night that she was stabbed. Tad tries to talk Liza out of breaking into David's bar in Gloucester, but she refuses to listen to him. Madison comes into the hospital and tells Angie that she can not go to her father's sentencing. Madison lets Angie know that her father is being sentenced today and she was going to go, but Angie encourages her to go. Jake and Tad try to talk her out of going to the hearing, but Liza insists on going. Jake tries to get Tad to promise that he will not go into Gloucester without Liza's permission. Erica and Opal watch the video of Ryan's confession. Ryan comes to visit Erica and asks her to watch a little TV. David meets with Dr. Clayton about Greenlee's condition. David introduces Greenlee to Clayton. Tad visits Liza at the bar and apologizes to her for putting her in this position. Tad and Liza begin to argue over his interference in her life. David calls Greenlee and tells her that they are going to take her on a tour of the medical facilities in Gloucestershire.\n    \\\\\n    \\end{tabular}\n    \\begin{tabular}{p{\\linewidth}} \n    \\midrule\n    \\textbf{TVMegaSite Example \\#2: Reference} \\\\\n    \\midrule\nErica and Ryan are in her office at Fusion kissing when Greenlee walks up to the door and starts to turn the doorknob. David clasps his hand over her mouth to keep her from screaming. At ConFusion, Liza and Tad kiss before they start to dance. Krystal watches then asks Rob to take her back to his place. Jake and Amanda spend a quiet evening at home when there is an incessant knocking on the door. Jake opens the door and Opal lets them know that she read her tea leaves and knows that someone is headed back into their lives. She fears that it is David. Jake and Amanda don't seem to be too concerned by Opal's anxiety attack over her tea leaves. David reminds Greenlee that Dr. Coleman said she could face another surgery. In talking to David, Greenlee realizes that Ryan is with another woman and comes to the conclusion that it is Kendall. At ConFusion, Tad sees David rushing out to his car. Ryan and Erica come clean to the press that they are involved. Liza visits Amanda and tries to soothe her fears that David is back in town. Tad and Jake visit David at Wildwind. Jake promises David that he will be watching him. Ryan and Erica come home to his penthouse and finds things completely out of order. Erica and Ryan make love in front of the fireplace. Greenlee lets herself into Ryan's place and sees him and Erica making love.\n    \\\\\n    \\midrule\n    \\textbf{TVMegaSite Example \\#2: Model Output} \\\\\n    \\midrule\nAt the hospital, Liza kisses Tad. Krystal walks in and sees them kissing. Liza asks Tad if she can steal him. At Wildwind, Jake and Amanda are in bed with the baby when there is a knock on the door. Jake answers it and it is Opal. Opal tells Jake that she knows that David is coming back to town. Jake assures her that he doesn't know where David is. At Fusion, Greenlee questions David as to what he is doing here. Greenlee demands to see Ryan, but David refuses to let her see Ryan. David tries to get Greenlee to calm down and let him examine her, but Greenlee insists on going up on the roof to talk to Ryan. At Ryan's home, Erica tells Ryan that she is not used to things going so smoothly in their relationship. Jake tells Opal that Wildwind is being sold and a real estate agent is showing it to the public. Jake lets Opal know that he hadn't heard from David in a while. Amanda comes downstairs and tells Jake about Opal's tea leaves giving her a strong feeling that something from their past is coming to town and that David could be there already. Jake asks Amanda if she is all right. Amanda lets Jake know that David had disappeared. At the Confusion bar, Tad tells Liza that he thinks that there is something going on between him and Krystal, but Liza denies it. David examines Greenlee and lets her know that Ryan is with another woman. David offers to take Greenlee back to his place, but she insists on knowing who the woman is before she changes her mind. Erica asks Ryan to take her home for a romantic dinner and a fire in the fireplace. Ryan and Erica arrive home to find a picture of the moon on the table. David tells Greenlee that he is taking her back to Gloucester for another surgery. David lets Greenlee know that Zach and Kendall had left town with Spike and had taken Spike with them. Jake and Tad burst into David's hotel room and find him hanging upside down in the attic. Jake orders David to leave town. David insists that he will leave town, but Jake tells him that they will be watching for him. David leaves the hotel room, but when he opens the door to leave, he finds that Greenlee is gone. Jake calls Amanda and tells her that they have to file a lawsuit against David because he is back in town.\n    \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\n    Summary Samples for TVMegaSite\n    }\n\\end{table}\n\n\\begin{table}[!htbp]\n    \\centering\n    \\tiny\n    \\begin{tabular}{p{\\linewidth}} \n    \\toprule\n    \\textbf{ForeverDreaming Example \\#1: Reference} \\\\\n    \\midrule\n    Sydney races to find a cure for Vaughn, but in order to find the antidote, Sydney must make a deal with Sark that could endanger Sloane's life. Meanwhile, Will continues his research for Vaughn and discovers some disturbing inconsistencies involving 20-year - old standardized IQ tests. Sydney finds out that Vaughn has a girlfriend.\n    \\\\\n    \\midrule\n    \\textbf{ForeverDreaming Example \\#1: Model Output} \\\\\n    \\midrule\n    Sydney must find a way to save Vaughn's life while trying to track down Sark's base of operations in order to obtain the antidote to a deadly virus that Sloane has been infected with. Meanwhile, Vaughn is in critical condition after being exposed to Rambaldi's toxin, and Sydney is forced to make a dangerous deal with Sark in exchange for his aid in getting rid of him. Will meets with a professor to find out if any of the CIA's standardized tests were ever used to recruit American children in the 1980's, and discovers that one in ten thousand first graders could pass the test.\n    \\\\\n    \\end{tabular}\n    \\begin{tabular}{p{\\linewidth}} \n    \\midrule\n    \\textbf{ForeverDreaming Example \\#2: Reference} \\\\\n    \\midrule\n    Half a universe from Earth, the Doctor and Martha answer a distress call from the cargo ship SS Pentallian, which will impact a star in 42 minutes. The ship scooped out part of the star, which is alive, for cheap fuel. The star uses crew members Korwin and Ashton as host bodies, and begins taking out everyone on board. Martha and crew member Riley, while trying to reach the front of the ship, are jettisoned into space in an escape pod by Ashton. The Doctor remagnetises the pod to bring Martha and Riley back. The Doctor begins being taken over by the star, and tells Martha to vent the engines, getting rid of the \" sun particles \" in the fuel. This causes the engines to start working again, and frees the ship from the star's pull.\n    \\\\\n    \\midrule\n    \\textbf{ForeverDreaming Example \\#2: Model Output} \\\\\n    \\midrule\n    The Doctor and Martha are trapped on board a space station in the Torajji system, where the crew are trying to prevent the ship from colliding with the sun. The Doctor uses the sonic screwdriver on Martha's mobile phone to activate Universal Roaming Activation, which allows him to travel anywhere in space and time without interference from the ship's control centre. However, the device malfunctions and the ship begins to fall towards the sun, and the Doctor is forced to use the emergency escape pod to escape. The pod, which contains the Doctor, Martha and two other crewmembers, is destroyed by the impact, but the Doctor manages to return to the control centre to try and stop the ship hitting the sun before it does so.\n    \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\n    Summary Samples for ForeverDreaming\n    }\n\\end{table}\n\n\\begin{table}[!htbp]\n    \\centering\n    \\tiny\n    \\begin{tabular}{p{\\linewidth}} \n    \\toprule\n    \\textbf{BookSum Book-Level Example \\#1: Reference} \\\\\n    \\midrule\nAt the opening of Act I, it is a cloudy autumn day on a Russian country estate. In the garden, the old nurse Marina stands at the samovar and offers Doctor Astrov something to eat, but he refuses. He complains about the difficulty of his job. Telegin, an impoverished local landowner, sits with them. Voynitsky, known as Vanya, comes out of the house and joins them. He is almost fifty and is weary and irritable. He complains about his brother-in-law, Serebryakov, Serebryakov's young second wife, Helen, and about how their visit has turned the place upside down. Serebryakov, Helen, and Serebryakov's daughter, Sonya, join them for a moment. After they depart, Vanya sighs about Helen's beauty and then complains about how he has toiled his whole life on this estate for the professor and it has come to naught. After Vanya's sister's death, he and Sonya worked here so the professor could continue his studies and his writings, but Vanya has come to see that work as foolish and irrelevant. When Astrov suggests that Vanya is jealous, Vanya laughs that he obviously is, especially as the old, gout-and-rheumatism-ridden man seems to attract beautiful women. Helen ventures outside and tells Astrov his services are not needed for her husband. Mrs. Voynitsky, Vanya's mother and Sonya's grandmother, tells them about a new pamphlet written by a friend in Kharkov. When Vanya sneers that all they do is read pamphlets, she becomes distressed and claims he hates her. Vanya merely says he is old, tired, and frustrated. A laborer arrives and tells Astrov he is wanted at the factory; the doctor bitterly departs, but not before they all discuss how he is very interested in forestry work. Sonya speaks up cheerfully about how Astrov is trying to save the old forest from destruction because forests make people happier. Astrov speaks of how Russians have torn down the forests and destroyed the wildlife: they no longer create, but rather destroy. After Sonya walks Astrov out, Vanya tries to seduce Helen, but she pushes him away. She muses about how Sonya clearly seems to love the doctor but he does not love her back. Helen sighs that she is simply bored and life is too much for her. In Act II, Serebryakov complains to Helen of how he is old and no one respects him. His querulous behavior only annoys Helen, who begs him to stop it. Serebryakov ignores her and bemoans how his life of scholarship seems to be nothing now. Sonya joins them and tells them Serebryakov must see Astrov now; she wants her father to stop behaving like a child. The elderly nurse Marina comforts Serebryakov and leads him out. Helen tells Vanya, who entered the room, that her husband wearies her. Vanya can only lament that everything is over for him and his life was wasted on trivial things. Helen is annoyed and moves to leave, but he bars her way. She accuses him of being drunk, and he admits to it. After Helen sweeps out of the room, Vanya ruminates on what a fool he was not to fall in love with her when she was younger; he once admired the professor, but now he does not. When Astrov returns, he mocks Vanya for having feelings for Helen, but Vanya will not admit it. Astrov leaves to get a drink; Sonya pulls him aside and makes him promise to stop drinking and stop getting her uncle drunk. He agrees. They continue to talk for a moment. He comments that Helen is beautiful but idle and useless. This country life makes people like that, and he despises it; he has been beaten down and sees no light at the end for himself. The peasants are all the same, and educated people are ridiculous. He only likes forests. Sonya compliments him and tries to cheer him up. As he prepares to leave, she asks how he might feel if he were to out that a friend of hers has feelings for him, and he drolly says he cannot love anyone. After he leaves, Sonya feels a surge of happiness though she is not sure why. In Act III, Sonya confesses to Helen that she loves Astrov, and Helen suggests that she say something to see if the doctor loves Sonya too. Sonya gives her permission for Helen to do this. Astrov and Helen meet to ostensibly look at his forestry maps. He discourses volubly on the patterns of deforestation until he sees that Helen is uninterested. Helen insists she is interested but says they should talk about something else. She point-blank asks if he likes Sonya, and he says no. He then moves in to seduce Helen, but she wants none of it. As he tries to kiss her, Vanya enters the room with flowers. Helen is horrified by the situation and begs Vanya to tell her husband that they must leave today. A moment later, Serebryakov and the others enter and Serebryakov announces that he has an idea to sell the estate because he and Helen need to afford a place in the city. This announcement angers Vanya tremendously, and he begins to complain violently about how Serebryakov is a fraud, is uninspired, is thankless, and how he, Vanya, has labored for Serebryakov his whole life and for no reason. He insists this is Sonya's estate. He runs out of the room. Serebryakov is startled by Vanya's outburst. He insists he cannot stay here anymore. Sonya implores him to talk to her uncle and he agrees. He departs, and those in the room hear a gunshot, then another. Helen and Vanya struggle over a revolver as Vanya screeches that he missed the professor. Cursing, he sinks into a chair. In Act IV, Telegin and Marina discuss Serebryakov and Helen's planned departure for that day, then exit the room. Vanya and Astrov come in. Astrov mocks Vanya for his behavior and asks him to return what he stole. Vanya maintains that he is innocent of theft. Astrov laments how this parochial existence crushes people: it is stultifying and useless. When Sonya enters Astrov tells her to tell her uncle to return the bottle of morphia he stole. Sonya turns to her uncle and tearfully asks him for the bottle. He complies. She takes him to make up with her father. Helen enters to tell Astrov goodbye. He tries to seduce her again in a rather lackluster fashion; she kisses him and bids him farewell. Everyone bids goodbye to Serebryakov and Helen, who leave the estate. Sonya and Vanya return to work. Marina suggests Astrov stay for dinner; he refuses and says he must be off. Astrov leaves, and even though Sonya knows he did not love her, she is sad. Vanya, though, claims he is extremely depressed. With both in tears, Sonya comforts her uncle. She tells him that life may be difficult for them now, but the afterlife will be full of peace, love, and rest.\n    \\\\\n    \\midrule\n    \\textbf{BookSum Book-Level Example \\#1: Model Output} \\\\\n    \\midrule\nThe scene opens in a country house in the Russian countryside. Ivan, a young man, is sitting with his mother, who is knitting him a Christmas stocking. He tells her that he does not want to have any tea, but she insists that he drink it anyway. She asks him how long they have known each other, and he says that it has been ten years since he first met her. He has not had a day of rest since he has known her, and has toiled without repose or a day's freedom ever since. He had a typhoid epidemic in the third week of Lent, and when he returned home, he had to take care of a man who had died of typhoid fever. He then sat down and closed his eyes for a while and wondered if he had killed the man. He wonders if the future generations will forget him, but he is sure that they will not. Ivan tells them that the house is a mess, because the Professor has been up all night writing and doesn't have time to eat or drink. He asks them to wake him up at two in the morning, but when they do not wake him, he thinks that the Professor wants some tea. They all come in from their walk, and as they come in, they talk about how happy they are to be together, and how they are all living in peace and contentment. Ivan is struck by the sight of a beautiful woman, and says that as he looks at her, his heart swells with unbridled happiness. The work of preserving the forest is thrilling, he says, and Dr. Astroff watches over the old woods and sets out new plantations every year. He wants to show them to Helena and Sonia, and Helena asks him if he finds it interesting. Helena is sitting next to him, and tells him to look for Batushka's works in the library the next day. He is worried that he is getting angina from his old age. Helena tries to tell him to make peace with himself, and to stop being so angry with everyone else. But he says he will not listen to her, because he has spent too much time thinking about the past, and now he cannot bear to watch the success of others and to fear death. Helena gets up and walks away from him, sitting down at a distance. She tries to get him to tell her why he doesn't love anyone. He says he is not an idiot, that he can understand her, but that he cannot understand her because she is young and healthy and beautiful and longing for life and he is an old dotard, almost a dead man. She tells him that she can understand him and that he should be able to understand her. Now, she says, she is too old and sick to continue to live in the country, and she has a young daughter and a young wife to care for. She doesn't want to live on the income generated by the estate, so they have to find some way to guarantee that they can live in town on the money generated each year. They decide to sell the woods, because they don't have enough money to do so every year, and they can't afford to buy the woods every year either. They need to find a way to make sure they can afford a certain amount of money to buy a cottage in Finland. They also have to figure out what to do with the rest of the property. The estate is worth ninety-five thousand roubles, and the estate was bought from his uncle, who had a debt of twenty-five. Semyon Lakedemonoff's brother bought the estate from the Lakedemons' uncle. The family ties are what got the estate to be worth so much money. Everyone agrees that the estate is for Sonia's good, and that she should get to keep it. But now that he has gotten old and ill, the time has come for him to dispose of his property in regard to the interests of his VOITSKI, ASTROFF, SEREBRAKOFF, Vanya, and SONIA arrive at the house to say goodbye to Tommo and Marina. They are to move to Kharkov to live with the professor and his wife. They have been frightened by what they have just witnessed, and decide to go to the city to see if they can find a place to live there. They will not be staying in the village any longer. Except for Vanya and Alexander, who stay to say good-bye to his wife and son-in-law. The Professor kisses them all three times, and then goes out to see them off. He gives them one last kiss to each of them before he leaves. They say they will always remember each other with pleasure, that they are interesting and original, and original. They shall rest\n    \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\n    Summary Samples for BookSum Book-Level\n    }\n\\end{table}\n\n\\begin{table}[!htbp]\n    \\centering\n    \\tiny    \n    \\begin{tabular}{p{\\linewidth}} \n    \\midrule\n    \\textbf{BookSum Book-Level Example \\#2: Reference} \\\\\n    \\midrule\nIn his London studio, artist Basil Hallward puts the finishing touches on his latest portrait, that of a young man. Although Lord Henry, who is visiting with Basil, asks about the young man's identity, Basil declines to answer, noting his preference for secrecy. Basil never intends to exhibit the painting, because if he did, it would bare the deepest feelings in his soul. However, Basil lets slip that the subject of the portrait is Dorian Gray, who shortly thereafter pays the two men a house call. Lord Henry immediately begins to influence Dorian, suggesting that he should treasure and guard his youth and beauty while he has them, because they will soon fade. Terrified of aging, Dorian wishes he could trade his soul to stay as young as he looks in the portrait; a short while later, he again wishes that he could stay young while the image in the painting aged. The portrait thus begins to take on a life-like existence; in fact, Basil's threat to burn the portrait is likened to \"murder\" and Basil prefers the company of the portrait to the real Dorian. Dorian falls in love with a young actress, Sibyl Vane, a woman he barely knows. She plays a different woman at each night's performance, earning the label of \"genius\" from Dorian, who is as smitten with her acting more than with her personality. They become engaged, much to the surprise of Lord Henry and Basil. The sweet, wholesome Sibyl discusses her engagement with her family. Because her mother is indebted to the theatre manager, Mr. Isaacs, for fifty pounds, she is against the marriage unless Dorian is wealthy; they do not know that he is. Sibyl's angry brother, James, is leaving for Australia, but he vows to kill Dorian if he wrongs his sister in any way. James also confronts his mother about gossip he has heard -- that his mother and deceased father never married, which Mrs. Vane admits is true. Dorian attends a performance of Sibyl's with Lord Henry and Basil, but the performance is terrible. Sibyl tells Dorian she can no longer act, because he has shown her a beautiful reality. Dorian is disgusted by her poor acting, because her performances were what drew him to her; he dismisses her and returns home. To his surprise, the portrait shows marks of cruelty around the mouth, lines that do not show on Dorian's face. He begins to suspect that his wish is coming true, so he vows to be good so that both he and the portrait can remain young. He, therefore, intends to apologize to Sibyl the next day and makes to marry her after all. However, he is too late: Sibyl commits suicide at the theatre that night. Dorian first feels responsibility for her death, but then views it both as wonderful entertainment and a selfish act on her part. Lord Henry tries to keep Dorian's name out of the scandal. Dorian and Lord Henry spend the evening at the opera. The next morning, Basil arrives and expresses concern for Dorian, given the events of the previous day. Dorian, however, is completely unconcerned about Sibyl or her family; he wants to talk only of happy subjects. The next day, he covers his portrait and moves it to the attic, to which Dorian has the only key. He then settles in to read a yellow book sent by Lord Henry; the book becomes Dorian's blackprint for life. Several years pass, and Dorian lives a hedonistic life according to the guidelines established by Lord Henry and the yellow book. While the face in the portrait has turned ugly, Dorian remains young, beautiful, and innocent. People talk about Dorian's \"madness of pleasure\" and his dreadful influence on the people around him, but that is of no consequence to him. Finally, when he is thirty-eight years old, Dorian shows the portrait to Basil, who begs Dorian to repent of his sin and ask that the wish be revoked. Instead, Dorian kills Basil and hides his body. Blackmailing his old friend Alan Campbell, Dorian is able to dispose of Basil's body. An hour later, Dorian attends a party, but is bored and distracted. He then heads for an opium den and, out on the street, meets Sibyl's younger brother, who has been waiting for an opportunity to harm Dorian for nearly twenty years. Dorian makes a case for mistaken identity when he claims to have the face of a twenty-year-old and cannot be the man James is looking for. A woman in the street reveals that Dorian \"sold himself to the devil for a pretty face,\" so James again pursues Dorian. At his country estate one week later, Dorian entertains guests but believes James in hunting him. Dorian soon learns, however, that a man accidentally killed in a hunting accident is James, and so he feels safe. The novel concludes six months later. Dorian and Lord Henry dine, and talk turns serious -- Dorian talks of Basil, and Lord Henry reflects on a sermon he heard the previous Sunday while walking in the park. Lord Henry also inquires about the secret of Dorian's youth, which Dorian dismisses. Dorian then asks Lord Henry never to give the yellow book to anyone else. That evening, while Dorian examines the portrait, he decides to destroy it with the knife used to murder Basil. Soon after, Dorian's servants and a police officer find an old, ugly man lying dead on the ground in front of a portrait of a young and innocent Dorian.\n    \\\\\n    \\midrule\n    \\textbf{BookSum Book-Level Example \\#2: Model Output} \\\\\n    \\midrule\nDorian Gray is sitting in the garden of his uncle's house in London, playing the piano. In the center of the room is a portrait of a young man of extraordinary beauty, and in front of it is Basil Hallward's portrait of Dorian Gray. The portrait belongs to Hallward, and he refuses to show it at the Grosvenor Academy, because it contains too much of himself in it. He tells Dorian's uncle, Lord Henry Wotton, that he is in love with Dorian and wants to marry him. Dorian is sitting at the piano and finishes a painting that he has been working on. He looks at the portrait and realizes that it is of him. He realizes that he will never be able to look at it again, but he cannot stop looking at it because he wants to remember the beauty of his own face. He asks his uncle for some information about Dorian, and his uncle tells him that Dorian has a beautiful mother who was married to a poor man who was killed in a duel. She left him a son, who is very good-looking and who has inherited all of her property. Lord Henry tells him to write to him and ask for some advice, and Dorian agrees. One day, Dorian meets Sibyl Vane, a beautiful young woman who works as a governess for a rich family in the East End of London. She is in the employ of Lord Henry's friend, Mr. Erskine of Treadley, and Lord Henry wants to see her. He also wants to get her out of the hands of the Jew who has her bound to him for three years and eight months. He proposes to her, but she refuses him. She says that she does not think he is good enough for her, and she will never love anyone of his rank. He is disappointed, but does not say anything to his mother about it. The next day, he meets the Duchess of Monmouth, who tells him he should find a wife and marry her. She wants him to have a future and not to spend his money frivolously. He agrees, but when he tells her that he does not love her, she laughs at him and refuses to call him by his new name, Prince Charming. He goes to see the play, and is horrified to see that the face on the canvas is that of the portrait of Romeo and Juliet. He cannot believe that he could have done such a terrible thing to Juliet and that she could still be his wife. He leaves the theater and wanders the streets of London until he finds himself in Covent Garden. He finds some women waiting for him, and one of them laughs when he calls her by his nickname, \"Prince Charming.\" She curses him and runs away. He runs into a dark alley and is suddenly grabbed by a man with a gun pointed at his head. It is James Vane. Vane threatens to kill Dorian if he doesn't make peace with God. He gives Dorian one minute to make his peace before he kills him. When Dorian gets to the street, he finds that the man he was trying to kill is not the same man he thought he was. It turns out that Vane is twenty-eight years younger than Dorian. The woman who took his money tells him not to talk to her again. She runs off, and when Dorian looks back, the woman has disappeared. When he wakes up the next morning, he has not had a nightmare. He writes two letters to his assistant, Alan Campbell, telling him that there is a dead man sitting on a table in his house, and that he must destroy the body so that no one will ever know who he is. He then goes to his bedroom and finds a small box of lacquer, which he takes out and puts inside. He puts the box back, gets into a horse-drawn carriage, and gives the driver an address. The driver takes him to the address, and as he is leaving the house, he sees the dead body of a man on the table. When Campbell returns, he tells Alan not to disturb the body, but to come back at seven o'clock in the evening. When the man arrives, he throws the picture over the table, but Dorian does not believe that it has been disturbed. He returns home and finds that Campbell has brought back the chemicals and the irons, and the other things that he needs to do the job. He opens the cabinet where he had hidden Basil's coat and bag, and finds the green paste. At midnight, he gets a hansom and leaves the house with the instructions to meet him at 7 o' clock the next day. He sits in the back of the carriage as the driver drives him through the streets. He wonders if it is possible to cure the soul by means of the senses and the body by way of the soul. He wakes up in the middle of the night to find that the portrait has not changed.\n    \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\n    Summary Samples for BookSum Book-Level\n    }\n\\end{table}\n\n\\clearpage\n\\section{\\textcolor{black}{Additional Experiment Details}}\n\\textcolor{black}{\nDue to the space limit, additional experiment details are reported here. As we discussed in the main text, all of our models are initialized from BART Large with 12 layers~\\citep{lewis-etal-2020-bart}. In the bottom-up inference module (the left panel in Figure~\\ref{fig:top-down-transformer}), the local self-attention in our models has 8 layers and all parameters are initialized from the first 8 layers of BART (including parameters for layer normalization). We use 4 layers for top-down inference (the middle panel in Figure~\\ref{fig:top-down-transformer}). Each layer consists of (1) token local self-attention, (2) token-segment cross-attention, and (3) feedforward. (1) and (3) are initialized from the last 4 layers of BART (including parameters for layer normalization). All other parameters are randomly initialized. The segment-pooling has a kernel size of 32 and a stride size of 24. The maximum document lengths for PubMed, arXiv, CNN-DM, TVMegaSite, ForeverDreaming, BookSum are 8192, 16384, 1024, 12288, 12288, 12288, respectively.\n}\n\n\\subsection{\\textcolor{black}{Local Self-Attention}}\n\\label{sec:supp-local-self-attn}\n\\textcolor{black}{The local self-attention used in our work is widely used in prior works on sparse attention~\\citep{beltagy2020longformer, zaheer2020big}. It is illustrated in Figure~\\ref{fig:local-self-attention}. It shows local self-attention of 9 tokens with window size 4. Each token attends 2 tokens on the left and 2 tokens on the right, as long as there are sufficient right and left neighbors. The attended nearby tokens are in light green. Each token also attends itself, as indicated by dark green. White color in Figure~\\ref{fig:local-self-attention} indicates absence of attention.}\n\n\\textcolor{black}{It is also called sliding window attention~\\citep{beltagy2020longformer}. We call it local self-attention to make a direct contrast with the full self-attention used for the segment-level representations. Despite its efficiency, it misses information outside of the local attention window. Thus, it is often used together with other attention mechanisms. In Longformer, sliding window attention is combined with dilated sliding window attention and global token attention. In BigBird, it is combined with random attention and global token attention. In our work, we use segment-level tokens to collect long range information which is then used to enrich token-level representations through token-segment cross attention (see top-down inference in Figure~\\ref{fig:top-down-transformer}). }\n\n\\begin{figure}[!htbp]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{figures/local-self-attention.png}\n    \\caption{\\footnotesize \\textcolor{black}{An illustration of local self-attention. It illustrates local self-attention of 9 tokens with window size 4. Each token attends 2 tokens on the left and 2 tokens on the right, as long as there are sufficient right and left neighbors. The attended nearby tokens are in light green. Each token also attends itself, as indicated by dark green. White color indicates absence of attention.} }\n    \\label{fig:local-self-attention}\n\\end{figure}\n\n\\clearpage\n\\section{\\textcolor{black}{Ablation Studies}}\n\n\\textcolor{black}{\nWe present results for a series of ablation studies in this section. The experiments are performed with PubMed. The results are summarized in Table~\\ref{table:ablation-all}. The first row shows the performance of the top-down transformer with top-down update via cross-attention and window size 1024, which is our final model (Please see Figure~\\ref{fig:top-down-transformer} for an illustration). \n}\n\n\\textcolor{black}{\nThe second row shows the performance for a variant of top-down update. In this variant, to update the bottom-up inferred token representations, we concatenate the token representations with the corresponding top-level segment representations, in contrast to the cross-attention approach used in the final model. We can see a clear performance degradation, indicating the importance of the cross-attention-based top-down update. \n}\n\n\\textcolor{black}{\nThe third row displays the results without top-down update, and the decoder attends the bottom-up-inferred token representations to generate summaries. Compared to our final model, the performance is also degraded, suggesting the effectiveness of the top-down update. \n}\n\n\\textcolor{black}{\nThe bottom panel of Table~\\ref{table:ablation-all} presents ablation results on the window size of local self-attention (see Figure~\\ref{fig:local-self-attention} for an illustration). These results are also plotted in Figure~\\ref{fig:ablation-window}. They show an effect of window size. That is, as the window size increases, the performance on all metrics enhances. The effect is quite large when the window size is increased from 32 to 256. The effect becomes smaller after 256, but the model performance can still benefit from larger window size. \n}\n\n\\begin{table}[!htbp]\n\\small\n\\centering\n\\begin{tabular}{l l l c c c} \n\\toprule\n& & & R-1 & R-2 & R-L \\\\\n\\hline\nTop-Down Transformer & with top-down update via cross-attention & window size - 1024 & 48.34 & 21.40 & 44.22 \\\\ \n& with top-down update via concat  & window size - 1024 & 47.04 & 20.36 & 43.03 \\\\ \n& without top-down update & window size - 1024 & 46.97 & 20.23 & 42.88 \\\\ \n\\hline\n& with top-down update via cross-attention & window size - 32 & 46.30 & 19.55 & 42.21 \\\\ \n& with top-down update via cross-attention & window size - 64  & 47.25 & 20.37 & 43.12 \\\\ \n& with top-down update via cross-attention & window size - 128  & 47.44 & 20.56 & 43.35 \\\\ \n& with top-down update via cross-attention & window size - 256  & 47.89 & 21.06 & 43.77 \\\\ \n& with top-down update via cross-attention & window size - 512  & 48.08 & 21.16 & 44.05 \\\\ \n\\hline\n\\end{tabular}\n\\caption{\\textcolor{black}{Ablation studies of Top-Down Transformer with PubMed.}}\n\\label{table:ablation-all}\n\\end{table}\n\n\\begin{figure}[!htbp]\n    \\centering\n    \\includegraphics[width=0.6\\textwidth]{figures/window-size-ablation.png}\n    \\caption{\\footnotesize \\textcolor{black}{Ablation on attention window size with PubMed. The window sizes tested are 32, 64, 128, 256, 512, 1024.} }\n    \\label{fig:ablation-window}\n\\end{figure}\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{GoSum: Extractive Summarization of Long Documents by Reinforcement \\\\ Learning and Graph Organized discourse state}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nExtracting summaries from long documents can be regarded as sentence classification  using the structural information of the documents.\nHow to use such structural information to summarize a document is challenging.\nIn this paper, we propose GoSum, a novel graph and reinforcement learning based extractive model for long-paper summarization.\nIn particular, GoSum encodes sentence states in reinforcement learning by building a heterogeneous graph for each input document at different discourse levels. An edge in the graph reflects the discourse hierarchy of a document for restraining the semantic drifts across section boundaries.\nWe evaluate GoSum on two datasets of scientific articles summarization: PubMed and arXiv. \nThe experimental results have demonstrated that GoSum achieve state-of-the-art results compared with strong baselines of both extractive and abstractive models.\nThe ablation studies further validate that the performance of our GoSum benefits from the use of discourse information.\n\n\\end{abstract}\n\\section{Introduction}\n\\label{sec:intro}\n\nDocument summarization refers to generating short and conductive summaries over given texts, which can help readers rapidly acquire essential knowledge from documents.\nThere are two main categories of approaches to summarization: the extractive approach and the abstractive approach.\nThe extractive approaches score and filter out the sentences of a given document % come from the input document,and it is usually obtained by scoring and filtering sentences of the document.%\nto ensure the semantic and grammatical correctness of the selected sentences in the summary.\nAbstractive approaches mostly read an input text, comprehend it, and output its summary within the seq2seq framework.\nThis procedure is similar to humans'  summarising articles.\nThe theoretical upper bound on the performance of the seq2seq model is higher than what extractive approaches can achieve.\nHowever, abstractive approaches  have the drawback of producing some meaningless and unfaithful summaries~\\cite{fact_2020}. The generated summaries read smoothly with a high ROUGE score, but there is a significant gap in semantic information between them and the gold summaries.\n\nIn this paper, we focus on the use of extractive models for summarizing scientific literature.\nExtractive summarization~\\cite{matchsum_2020,zhou2018neural} has been extensively studied in short summarization datasets such as CNN/DailyMail \\cite{cnndm}.\nHowever, studies on long texts have lagged relatively behind because long document summarization is more challenging due to the following two reasons:\n1) An increase in the input length expands the memory cost of the model; and\n2) The complex discourse structural information about long-form documents should be taken into account.\nReading a long text, especially scientific literature, one usually glances at the discourse structure of the whole text. Once reading a section title, one roughly should know on which this section focuses. Using this structural information  of a text, one can better understand the meanings of its sentences. From the perspective of extractive summarization, it would be better to use this information for encoding sentences.\nThe previous method encodes sentences and sections separately, making it difficult to capture the hierarchical structure of the document.\nIn this paper, we thereby propose to use a graph neural network (GNN) to well represent the structure information of documents. The  additional benefit is that the computational complexity of GNNs is linear for long inputs.\n\nUnlike abstractive approaches that are trained by  using available gold summaries directly, the training labels of an extractive model need to be obtained by using a search algorithm (typically greedy search) based on the gold summary provided.\nThis kind of label is not optimal and deterministic, i.e., the algorithm yields a single extracted label for each pair of document-abstract.\nIn fact, there may be many valid labels that are very similar to these suboptimal labels.\nInsufficient such positive pairs may cause under-fitting~\\cite{rl_2018}. \nThese problems can be alleviated by increasing the number of samples and giving each training sample a reward from reinforcement learning (RL).\n\nTo address the above problems, we propose a novel model called GoSum that is trained by using reinforcement learning.\nBased on a given input and previously extracted sentences, GoSum generates the sentences of a summary sequentially.  \nThe process of scoring and selecting a sentence is regarded as an action in reinforcement learning.\nThis action is taken after the agent (the GoSum model) takes the  sentence state as input.\nFor encoding sentence states, we  leverage the  structure of a document.\nSpecifically,  we  use  a graph neural network to encode  the hierarchical structure of a document.\nIn more detail, we treat each sentence and section as a node of a heterogeneous graph.\n A state contains 1) a local representation of a sentence with discourse awareness, 2) the global context of a sentence within the document, and 3) information about  the extraction history.\nAs such, we seamlessly integrate RL with GNN in GoSum. To summarize, our main contributions of this paper are:\n1) We propose an  approach called GoSum \\footnote{Source code is available  on Supplementary Files} as a novel graph-based discourse-aware extractive summarization model. GoSum can  generate a concise and informative summary operating on a subsentential discourse unit level. 2) We effectively integrate reinforcement learning with GNN under GoSum. With obtaining sufficient samples in reinforcement learning,  GoSum relies on GNN to capture discourse information about documents, particularly for the discourse hierarchy, to extract  compact summaries. 3) We have conducted comprehensive experiments to validate the performance of GoSum. GoSum has achieved state-of-the-art performance compared with strong baselines on two benchmark datasets: PubMed and arXiv.\n\n\\section{Related work}\n\\label{sec:relate}\n\n\\subsection{Long Document Summarization}\nUnlike the short-input summarization that BERT-based models~\\cite{2019_bertsum} have been successfully used, studies on long document summarization struggle with long-input sequences. \nResearch on abstractive models~\\cite{bigbrid_2020,hepo_2021} mainly exploring different architectures of Transformer to cope with excessively long inputs.\nHowever, the study of extractive models focus on other perspectives.\nFor example, long documents follow a standard discourse structure, i.e. scientific papers are written section by section to describe the background, methodology, experiment etc.\nSeveral methods~\\cite {localglobal_2019,collins2017supervised,discourse_2021} leverage such section information to guide the generation of summaries.\nReinforcement learning has also successfully been applied to long document extractive summarization.\nLG+RdLoss~\\cite{rdloss_2020} is an improved version of LG~\\cite{localglobal_2019} that constrains sentence redundancy with reinforcement learning.\nDiffering from LG-RdLoss, MemSum ~\\cite{memsum_2022} uses extraction history~\\cite{neusumm_history_2018}, and treat extractive summarization as a multi-step episodic Markov decision process.\n\n\\subsection{Graph-based Extractive Summarization}\nEarly summarization solutions are graph-based unsupervised methods~\\cite{graph0_2004}, relying on explicit surface features. They construct a similarity graph between sentences and formulate extractive summarization as a task of ranking nodes.\nRecently, researchers use graph neural network on supervised summarization.\nHSG~\\cite{hsg_2020} was the first  to construct a heterogeneous graph neural network for extractive document summarization.\nHahSum~\\cite{hahsum_2020} considers inter-sentence redundancy in graph construction.\nHEROS~\\cite{discourse_2021} applies graph-based to the long text field and uses the information about input article discourse.\nAll these methods treat sentences and words as nodes in a graph.\nBased on the RST tree, DiscoSum\\cite{disco_2020} uses a graph  to capture the long-range dependencies among discourse units,  with  Elementary Discourse Units as the nodes in a graph.\n To some extent, the graph-based approach solves the quadratic computational and memory complexities encoded using Transformer and works well with the structural information of the input. Therefore, we choose to use GNNs for GoSum.\n\\begin{figure*}[htp]\n    \\centering\n    \\includegraphics[width=2.0\\columnwidth]{./miscs/framework3.pdf}\n    \\caption{The overall framework of GoSum. MHP: multi-head pooling, and MLP: multi-layer perceptrons }\n    \\label{fig_framework}\n\\end{figure*}\n\n\\section{GoSum }\n\\label{sec:method}\n\nFigure 1 shows the architecture of GoSum. With the input of a structural text,  GoSum starts with constructing a  graph of the text\nand then generates  an embedding for the current state by using three sub-encoders: 1) The Graph-based Discourse Awareness Encoder, 2) The Global Context Encoder, and 3) The Extraction History Encoder. After this, the extractor decides whether to stop or continue the extraction based on the current embedding. \n\n\\subsection{Task Definition}\nExtractive summarization is regarded as a sequence labeling task.\nDenote $D=\\{s_1,s_2,...,s_n\\}$ as a document that consists of $n$ sentences. Extractive summarizer produces a sequence of indexes $\\hat{Y} = \\{\\hat{y}_1,\\hat{y}_2,...,\\hat{y}_T\\}$ to determine which sentences should be included in the summaries.\n$\\hat{y}_i$ denotes the index of the sentence.\nSince the datasets only contain document-abstract pairs, we use beam search and automatic metric ROUGE to sample a set of oracle labels $\\{Y^1,Y^2,...\\}$.\nThen, we keep the ROUGE score of each oracle label's corresponding summary against the abstract as a reward for reinforcement learning.\n\n\\subsection{GoSum via Policy Gradient}\nFrom the perspective of RL for extractive summarization, we can view our GoSum model  as an agent, parameters of the network as a policy $\\pi_{\\theta}$, and  extracting at each step  as an action.\nGiven an oracle label $Y = \\{y_1,y_2,...,y_T\\}$, $R=(r_1,r_2,...,r_T)$ is a reward list, $r_t$ is the reward of an action to select sentence $y_t$ after the set of $\\{y_1,y_2,...,y_{t-1}\\}$ are already selected.\nThe goal of policy gradient in GoSum is to maximize objective function $\\mathcal{L}(\\theta)=E_{\\pi_{\\theta}} (R)$.\nThe reward value $r_t$ is the same as the ROUGE \\cite{rouge_2004} score $r$ between the oracle summary and gold abstract.\n\\begin{align}\n    r = \\frac{1}{3} \\left(  \\text{ROUGE-1}_f + \\text{ROUGE-2}_f + \\text{ROUGE-L}_f  \\right)\n\\end{align}\n\nIn reinforcement learning~\\cite{rl_1992}, the policy gradient is defined as:\n\n\\begin{align}\n    \\nabla \\mathcal{L} (\\theta) = -E_{\\pi_\\theta} \\left[  r \\sum\\limits_{t=1}\\limits^T \\nabla_{\\theta} \\log \\pi_{\\theta} (A_t|S_t,\\theta)  \\right]\n\\end{align}\n\nwhere $\\pi_{\\theta} (A_t|S_t,\\theta)$ represents the likelihood of action $A_t$ from policy net $\\pi_{\\theta}$ when a state is $S_t$ and the time step is $t$.\nUsually, the extractive method extracts a fixed number of sentences. However, GoSum uses a stop mechanism, which  determines the point at which to stop extracting itself. So the policy likelihood can be written in the following form:\n\\begin{align}\n    \\pi(A_t|S_t,\\theta) = p(\\text{stop} | S_t, \\theta) p(A_t|\\text{stop}, S_t,\\theta)\n\\end{align}\n\nIn each step, the policy net first outputs a probability $p_{stop}$. If $p_{stop}$ is greater than a pre-defined threshold, then the model will stop extracting, otherwise, the model continues to find the next sentence.\n\n\\subsection{State Encoder}\n\n\\subsubsection{Graph-based Discourse Awareness Encoder}\n\n\\noindent \\textbf{Graph Construction:}\nGoSum constructs a heterogeneous graph that represents sections and sentences of a document at the discourse level.\nThere are only two kinds of nodes in the graph: sentence nodes and section nodes.\nThe way we build the graph is slightly different from the previous graph-based approach ~\\cite{discourse_2021,hsg_2020,hahsum_2020} in that we discard the word nodes.\nAs reinforcement learning is  time-consuming, removing word nodes can significantly improve the running time  of GoSum, \nIn addition, the information transferred from word nodes to sentence nodes is essentially about the representation of the sentence's local content.  Therefore, the use of a simple encoder is sufficient, such as LSTM.\nWe connect edges between each sentence and the section containing the sentence.\nAlso, a fully-connected subgraph is built among each section.\n\n\\noindent \\textbf{Graph Initialization:}\nAfter the graph is constructed, we give each node an initial representation.\nSuppose that  a sentence in a document consists of $s$ words: $(sw_1,sw_2,...,sw_s)$, and the text of a section (e.g. \"Related work\") is composed of $c$ words: $(cw_1,cw_2,...,cw_c)$.\nWe   first employ  Glove\\cite{glove_2014} word embeddings to embed these words, then use BiLSTM \\cite{lstm_1997} with Multi-head pooling (MHP) to produce sentence representation $h_s^0$ and section representation $h_c^0$:\n\\begin{align}\n    h_c^0 = \\text{MHP} (\\text{LSTM} ( \\text{Glove} (cw_1,cw_2,...,cw_c) ) ) \\\\\n    h_s^0 = \\text{MHP} (\\text{LSTM} ( \\text{Glove} (sw_1,sw_2,...,sw_s) ) )\n\\end{align}\n\n\\noindent \\textbf{Graph Attention Networks:}\nWith the available graph $G$ and its node features, we use a graph attention layer (GAT) \\cite{gat_2017} to update our semantic nodes. The expressions of GAT are as follows:\n\n\\begin{align}\n    e_{ij} &= \\text{LeakyRELU} (W_a[W_q h_i; W_k h_j]) \\\\\n    \\alpha_{ij} &= \\frac{\\exp (e_{ij})}{\\sum_{k\\in \\mathcal{N}_i} \\exp (e_{ik})} \\\\\n    h_i^{\\prime} &= \\sigma (\\sum_{j\\in \\mathcal{N}_i} \\alpha_{ij} W_v h_i) + h_i\n\\end{align}\n\nwhere $W_a, W_q, W_k,$ and $ W_v$ are trainable weights, and $h_i$ is the node representation of the $i-th$ node in the graph. $\\mathcal{N}_i$ is the neighbor nodes of node $i$.\n\n\\noindent \\textbf{Message Passing:} We first update section nodes with their neighbor sentence nodes via the GAT and Feed Forward Net (FFN) layers:\n\\begin{align}\n    U_{s\\rightarrow c} &= \\text{GAT} ({H}_c^0, {H}_s^0, {H}_s^0) \\\\\n    {H}_c^1 &= \\text{FFN}(U_{s\\rightarrow c} + {H}_c^0)\n\\end{align}\nwhere ${H}_s^0$ is the initialized representation of sentence nodes, and ${H}_c^0$ is for section nodes.\n $\\text{GAT} ({H}_c^0, {H}_s^0, {H}_s^0)$ denotes ${H}_c^0$ as an attention query, and ${H}_s^0$ as a key and value.\nWe continue to update section nodes by section to section edges:\n\n\\begin{align}\n    U_{c\\rightarrow c} &= \\text{GAT} ({H}_c^1, {H}_c^1, {H}_c^1) \\\\\n    {H}_c^2 &= \\text{FFN}(U_{c\\rightarrow c} + {H}_c^1)\n\\end{align} \\label{eq:mp3}\nAfter a section node is updated, it already has section-level discourse information. We then pass this discourse information to each corresponding sentence node:\n\\begin{align}\n    U_{c\\rightarrow s} &= \\text{GAT} ({H}_s^0, {H}_c^2, {H}_c^2) \\\\\n    {H}_s^1 &= \\text{FFN}(U_{c\\rightarrow s} + {H}_s^0)\n\\end{align}\nSince GoSum uses only one-layer GAT, the output is ${H}_s^1$.\n\n\\subsubsection{Global Context Encoder}\nAfter that, a Bi-LSTM takes ${H}_s^1$ as input to produce sentence embeddings ${H}_s^g$ that encodes global contextual information. \nThis module encodes global contextual information such as the sentences position in the document and information on neighboring sentences.\n\n\\subsubsection{Extraction History Encoder}\nIn extractive summarization, extracting sentences by an extraction history encoder is first used in NeuSum \\cite{neusumm_history_2018}, in order to avoid redundancy.\nComparing the extracted sentences and the remaining unextracted sentences, an extraction history encoder(EHE) generates the embedding for each of the remaining sentences. The result is used to guide the scoring of those unextracted sentences.\nOur design of the extraction history encoder(EHE) in GoSum follows \\cite{memsum_2022}.\nIt consists of a series of $N_h$ identical layers.\nEach layer first performs a multi-head self-attention between the remaining sentences, followed by another multi-head self-attention performed on the sentences that have been extracted.\nTwo attention sublayers capture the information of both extracted and remaining sentences.\nFor those sentences that have not been extracted yet in time step $t$, an extraction history embedding ${H}_e^t$ is obtained.\n\n\\subsection{Extractor}\nAs shown in Eq(3), the extractor decides whether to stop extraction or generate the score of each remaining sentence according to the state.\nThe state $S_t$ is described by concatenating three types of vectors:\nsentence representation from discourse graph $H_s^1$,\nsentence global content representation ${H}_s^g$,\nand extraction history embedding ${H}_e^t$ as:\n\\begin{align}\n    S_t = [H_s^1;H_s^g;H_e^t]\n\\end{align}\nA multi-head pooling followed by a multi-layer perceptrons (MLP) is used to compute stop signial of extraction. Another MLP decides to extract which sentence.\n\n\\begin{algorithm}[tb]\n    \\caption{Training procedure in one iteration}\n    \\label{alg:algorithm}\n    \\textbf{Input}: Document-Summary pair $<D, S>$\\\\\n    \\textbf{Parameters}: Learning rate: $l$, and model parameters: $\\theta$ \\\\\n    \\begin{algorithmic}[1]\n        \\STATE A label sequence ${Y}=\\{y_1, y_2,...,y_T\\}$ is sampled using beam search, with corresponding summaries having ROUGE scores $r$ against $S$.\n        \\STATE Obtain discourse-aware sentence embedding $H_s^1$:\n        \\STATE \\quad Initialize Graph $H_s^0, H_c^0$\n        \\STATE \\quad Message passing $(H_s^0, H_c^0) \\rightarrow H_c^1$\n        \\STATE \\quad Message passing $(H_c^1, H_c^1) \\rightarrow H_c^2$\n        \\STATE \\quad Message passing $(H_c^2, H_s^0) \\rightarrow H_s^1$\n        \\STATE Obtain global-content sentence embedding $H_s^g$\n        \\STATE Let $t=1$.\n        \\WHILE{$t$ is no larger than $T$}\n            \\STATE Produce extraction history embedding $H_e^t$ for the remaining sentences.\n            \\STATE Output the probability of the sentence from the Extractor to select $y_t$ and $p_{stop}$ by using state $S_t= [ H_s^1, H_s^g, H_e^t ]$.\n            \\STATE Update policy gradient: $\\theta \\leftarrow \\theta + l \\cdot r \\nabla \\pi(A_t|S_t,\\theta) $\n            \\STATE $t \\leftarrow t + 1$\n        \\ENDWHILE\n    \\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Training}\nUsually, the training samples of reinforcement learning algorithms are obtained by sampling the policy net that is currently being trained. Since the golden standard is already known at the time of training for extractive summarization, we can obtain high-quality training samples by performing beam search sampling in advance.\nThis saves the time spent on sampling and allows the model to converge more quickly.\nThe flow of the training process in GoSum is shown in Algorithm 1.\n\\section{Experiments}\n\\label{sec:exp}\n\n\\subsection{Summarization Datasets}\nWe evaluate our model on the two scientific paper datasets: PubMed and arXiv  \\cite{pubmedarxiv_2018}.\nBoth datasets provide information about the structures of the papers.\nThe inputs of these datasets are the full text of scientific papers except for the abstract, and the gold summaries are the corresponding abstracts.\nAs can be seen from Table ~\\ref{tab:dataset}, both datasets are relatively large in size, especially the arXiv dataset.\n\n\\begin{table}[h]\n  \\centering\n  \\begin{tabular}{l|p{12mm}p{12mm}}\\toprule\n                                & PubMed    &   arXiv       \\\\ \\midrule\n     avg. \\# sents of doc        & 89        &   207         \\\\\n     avg. \\# sents of summ       & 6.8         &   9.8          \\\\\n     avg. \\# tokens of doc       & 2730      &   5206        \\\\\n     avg. \\# tokens of summ      & 181       &   238         \\\\\n     avg. \\# sections of doc     & 6.0      &   5.5         \\\\\\midrule\n     \\# Train                   & 116 937   &   202 880      \\\\\n     \\# Valid                   & 6633      &   6436        \\\\\n     \\# Test                    & 6658      &   6440        \\\\\n     \\bottomrule\n\\end{tabular}\n\\caption{The datasets we used in the expertiments.}\\label{tab:dataset}\n\\end{table}\n\n\\subsection{Experimental Setup}\n\n\\subsubsection{Evaluation Metrics}\nROUGE score \\cite{rouge_2004} is used to evaluate the model performance.\nWe report the F1 score of unigram, bigram overlap (ROUGE-1, ROUGE-2), and the longest common subsequence (ROUGE-L).\n\n\\subsubsection{Training data Sampling}\nThe original PubMed and arXiv datasets do not provide extractive training labels.\nWe use beam search to obtain extractive oracle summaries. For each document-abstract pair, the algorithm generates at most 15 different summaries with the largest ROUGE score. For the PubMed and arXiv datasets, we set the maximum sequence length of extracted summaries to 7 and 8, respectively.\n\n\\subsubsection{Implementation Details}\nOur model is trained using adam\\cite{adam_2015} optimizer with the learning rate $1e-4$, $\\beta_1 = 0.9$, and $\\beta_2=0.999$.\nGoSum and its variants are trained from 20 epochs on the both pubmed and arxiv dataset.\nIn each iteration, for each input document, we randomly sample one pre-prepare label for training.\nModel checkpoints are saved and evaluated every 10,000 steps.\nDuring the testing phase, the threshold of $p_{stop}$ for PubMed and arXiv is set to $0.6,$ and $ 0.45$, respectively.\nGoSum and its variants are all trained on four TITAN XP GPUs.\n\n\\begin{table}[t]\n  \\centering\n  \\begin{tabular}{p{36mm}|p{8mm}p{8mm}p{8mm}}\\toprule\n        Models          &  R-1  &  R-2  &  R-L   \\\\ \\midrule\n        Oracle                          & 60.00 & 30.60 & 53.03  \\\\  \\midrule\n        \\multicolumn{4}{c}{Extractive models}  \\\\ \\midrule\n          Lead-10                       & 30.52 & 10.33 & 31.44  \\\\\n          Local-Global                  & 43.77 & 17.50 & 38.71  \\\\\n          \\quad + RdLoss                & 44.01 & 17.79 & 39.09  \\\\\n          Sent-CLF                      & 34.01 &  8.71 & 30.41  \\\\\n          Sent-PTR                      & 42.32 & 15.63 & 38.06  \\\\\n          HEROS                         & 47.74 & 20.46 & 42.39  \\\\\n          \\quad w/o content ranking     & 45.90 & 18.33 & 40.78  \\\\\n          Topic-GraphSum                & 46.05 & 19.97 & 33.61  \\\\\n          MemSum                        & 48.42 & 20.30 & 42.54  \\\\\n          \\textbf{GoSum (ours)}         & \\textbf{48.61} & \\textbf{20.53} & \\textbf{42.80}  \\\\ \\midrule\n          \\multicolumn{4}{c}{Abstractive models}  \\\\ \\midrule\n          PEGASUS                       & 44.21 & 16.95 & 38.83  \\\\\n          BigBird-base                  & 41.22 & 16.43 & 36.96  \\\\\n          BigBird-large                 & 46.63 & 19.02 & 41.17  \\\\\n          Dancer                        & 45.01 & 17.60 & 40.56  \\\\\n          HAT                           & 46.68 & 19.07 & 42.17  \\\\\n          Hepos-Sinkhorn                & 47.87 & 20.00 & 41.50  \\\\\n          Hepos-LSH                     & 48.24 & 20.26 & 41.78  \\\\\n          \\bottomrule\n  \\end{tabular}\n  \\caption{Results on arXiv Dataset.} \\label{tab:sota_arxiv}\n\\end{table}\n\n\\subsubsection{Baselines}\nWe compare GoSum with state-of-the-art extractive methods and abstractive methods on the two datasets mentioned above.\nIn particular, the extractive baselines are \\textit{Local-Global}~\\shortcite{localglobal_2019}  that incorporates local and global contexts to extract summaries, and \\textit{Local-Global+RdLoss}~\\shortcite{rdloss_2020}. \nthat further adds a redundancy reinforcement learning loss. \n\\textit{HEROS}~\\cite{discourse_2021} use heterogeneous graph-based with nodes from different discourse levels.\nTo ensures that the input is consistent with other baseline, we also record its results without content ranking module. \n\\textit{NeuSum}~\\shortcite{neusumm_history_2018} is a model that considers the extraction history. \n\\textit{MemSum}~\\shortcite{memsum_2022} is a reinforcement-learning-based extractive summarizer.\n\\textit{Sent-CLF} and \\textit{Sent-PTR} \\cite{clfptr_2020} are a LSTM based sentence classifier and a hierarchical seq2seq sentence pointer.\n\nFor the abstractive methods, we compare GoSum with the following methods:\n\\textit{PEGASUS}~\\cite{pegasus_2020} is a pre-trained language model for summarization.\n\\textit{Dancer}~\\cite{dancer_2020} is a divide-and-conquer method.\n\\textit{BigBird}~\\shortcite{bigbrid_2020} uses sparse and windowed attentions to handle long input sequences.\n\\textit{Hepos}~\\cite{hepo_2021} uses the efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source.\n\\textit{HAT}~\\cite{hat_2021} adds hierarchical attention layers to an encoder-decoder model to summarize long documents.\n\n\\begin{table}[t]\n  \\centering\n  \\begin{tabular}{p{36mm}|p{8mm}p{8mm}p{8mm}}\\toprule\n        Models          &  R-1  &  R-2  &  R-L   \\\\ \\midrule\n        Oracle                          & 61.99 & 34.95 & 56.76  \\\\  \\midrule\n        \\multicolumn{4}{c}{Extractive models}  \\\\ \\midrule\n          Lead-10                       & 37.45 & 14.19 & 34.07  \\\\\n          Local-Global                  & 45.18 & 20.20 & 40.72  \\\\\n          \\quad + RdLoss                & 45.30 & 20.42 & 40.95  \\\\\n          Sent-CLF                      & 45.01 & 19.91 & 41.16  \\\\\n          Sent-PTR                      & 43.30 & 17.92 & 39.47  \\\\\n          HEROS                         & 48.14 & 21.82 & 43.33  \\\\\n          \\quad w/o content ranking     & 46.63 & 20.63 & 42.01  \\\\\n          Topic-GraphSum                & 48.85 & 21.76 & 35.19  \\\\\n          NeuSum                        & 47.46 & 21.92 & 42.87  \\\\\n          MemSum                        & 49.25 & 22.94 & 44.42  \\\\\n          \\textbf{GoSum (ours)}         & \\textbf{49.83} & \\textbf{23.56} & \\textbf{45.10}  \\\\ \\midrule\n          \\multicolumn{4}{c}{Abstractive models}  \\\\ \\midrule\n          PEGASUS                       & 45.97 & 20.15 & 41.34  \\\\\n          BigBird-base                  & 43.70 & 19.32 & 39.99  \\\\\n          BigBird-large                 & 46.32 & 20.65 & 42.33  \\\\\n          Dancer                        & 46.34 & 19.97 & 42.42  \\\\\n          HAT                           & 48.36 & 21.43 & 37.00  \\\\\n          Hepos-Sinkhorn                & 47.93 & 20.74 & 42.58  \\\\\n          Hepos-LSH                     & 48.12 & 21.06 & 42.72  \\\\ \\bottomrule\n  \\end{tabular}\n  \\caption{Results on PubMed Dataset.} \\label{tab:sota_pubmed}\n\\end{table}\n\n\\section{Results}\n\n\\subsection{Performance Comparisons}\nTables~\\ref{tab:sota_arxiv} and ~\\ref{tab:sota_pubmed} report the results of our model on arXiv and PubMed datasets, respectively.\nOn both datasets, GoSum outperforms state-of-the-art extractive and abstractive baselines. \nRL-based methods like GoSum, MemSum and LG-RdLoss show substantial performance gain, demonstrating the effectiveness of the reinforcement learning.\nCompared with MemSum, GoSum has better performance. The results depend on two factors: 1) the use of the structural information from the input articles; and 2) the use of graphs to model sentences and sections. In this way, sentences can obtain more abundant information from sections, and sections can share and propagate their topical information.\nGoSum has more performance improvement on PubMed dataset compared to arXiv dataset. One reason for this may be that the section information provided by the pubmed dataset is more accurate, as explained in more detail in section 5.3.\n\n\\subsection{Ablation Studies}\n\nIn table~\\ref{tab:ablation_pubmed}, we conduct ablation studies by comparing GoSum with its variants.\n\nTo validate the performance of the graph structure, we set the following GoSum variants:\n\\textbf{GoSum w/o sec2sec edges} remove section-to-section edges in graph construction, and take $H_c^1$ as a key input in Eq(13).\n\\textbf{GoSum w/o graph} has no graph modeling. In particular, the global contextual embedding $H_s^g$ is obtained directly using $H_s^0$. State representation $S_t$ in Eq(15) includes one more embedding $H_c^0$ to capture section information.\n\\textbf{GoSum w/o sec \\& graph} does not use document structural information and graph modeling.\n\nImprovements from \\textbf{GoSum w/o graph} to \\textbf{GoSum w/o sec2sec edges} demonstrate that the addition of paper structure information can slightly improve GoSum. The performance of GoSum has a greater improvement if using graphs to model the relationships between sentences and sections.\n\nNext, we examine the effects of different embeddings on the performance of GoSum.\nFor \\textbf{GoSum w SecE}, the extractor takes additional section representation $H_c^2$ in Eq(12).\n\\textbf{GoSum w/o GCE}, \\textbf{GoSum w/o DLE}, and \\textbf{GoSum w/o EHE}\nremove Global Context Embedding $H_s^g$, Discourse aware Local sentence Embedding $H_s^1$, and Extraction History Embedding $H_s^e$ in Eq(12), respectively.\n\nAlthough \\textbf{GoSum w SecE} adds an extra embedding, the resulting scores instead slightly decrease. This indicates that the information about section nodes has been incorporated into the local content embedding during the graph update process so that adding section embedding will be redundant with possible over-fitting.\nIf the other three embeddings are removed, the performance drops. \\textbf{GoSum w/o DLE} with removing $H_s^1$ results in the most decrease. This  also indicates that the discourse-aware local sentence embedding contains more useful information.\n\n\\begin{table}[t]\n  \\centering\n  \\begin{tabular}{l|p{13mm}p{13mm}p{13mm}}\\toprule\n                              & \\hfil R-1  & \\hfil R-2  & \\hfil R-L    \\\\ \\midrule\n        GoSum                 & \\hfil 49.83 & \\hfil 23.56 & \\hfil 45.10   \\\\ \\midrule\n        \\quad w/o sec2sec edges     & \\hfil 49.72 & \\hfil 23.46 & \\hfil 44.99   \\\\\n        \\quad w/o graph             & \\hfil 49.44 & \\hfil 23.24 & \\hfil 44.74   \\\\\n        \\quad w/o sec \\& graph      & \\hfil 49.22 & \\hfil 23.02 & \\hfil 44.49   \\\\ \\midrule\n        \\quad w   SecE              & \\hfil 49.80 & \\hfil 23.56 & \\hfil 45.08   \\\\\n        \\quad w/o GCE               & \\hfil 48.80 & \\hfil 22.34 & \\hfil 44.16   \\\\\n        \\quad w/o DLE               & \\hfil 48.01 & \\hfil 21.84 & \\hfil 43.57   \\\\\n        \\quad w/o EHE               & \\hfil 49.24 & \\hfil 23.09 & \\hfil 44.28   \\\\ \\bottomrule\n  \\end{tabular}\n  \\caption{Abaltion studies on PubMed dataset.} \\label{tab:ablation_pubmed}\n\\end{table}\n\\subsection{What exactly enhances GoSum?}\n\\noindent \\textbf{Aspects of graph-organized discourse states:}\nWith the use of reinforcement learning and graph neural networks, GoSum has achieved a significant performance improvement over other extractive approaches.\nA nature question of what exact reasons why GNN can enhance GoSum is raised. There may be two reasons:\nOne is that the use of a graph captures the input discourse information; another could be that the hierarchy of this graph makes  section nodes to be a sink of information diffusion of their sentences.\nTo validate these answers, we examine the  performance of GoSum by gradually scrambling the section attribution information of the input sentences without changing the other parameters of the model. The disruption of section attribution information can blur the discourse information, but the graph still keeps the hierarchical structure of documents.\n\nFor the above reasons, we disrupt the section attribution of the input sentences proportionally, with an increment of 10\\% in each experiment.\nSince experimenting with the full data set is too time-consuming, we select 10,000 samples from each PubMed and arXiv datasets for training.\nAs seen from  Fig~\\ref{fig:exp1}, the performance of GoSum decreases rapidly with the declining amount of discourse information.\nBecause of the small number of training samples and the instability of reinforcement learning, the performance of GoSum fluctuates slightly from dataset to dataset but shows a slow decreasing trend overall.\nThe performance of GoSum decreases significantly at the beginning, which indicates that GoSum is sensitive to the accuracy of section information. It also confirms that accurate discourse information is required to improve the performance of GoSum.\n\n\\begin{table}[t]\n  \\centering\n  \\begin{tabular}{l|p{13mm}p{13mm}p{13mm}}\\toprule\n                    & \\hfil R-1  & \\hfil R-2  & \\hfil R-L    \\\\ \\midrule\n                    \\multicolumn{4}{c}{PubMed}     \\\\ \\midrule\n        GoSum                       & \\hfil 49.83 & \\hfil 23.56 & \\hfil 45.10   \\\\\n        \\quad w/o SecTitle           & \\hfil $\\downarrow$ 0.20 & \\hfil $\\downarrow$ 0.11 & \\hfil $\\downarrow$ 0.12   \\\\ \\midrule\n                    \\multicolumn{4}{c}{arXiv}     \\\\ \\midrule\n        GoSum                       & \\hfil 48.61 & \\hfil 20.53 & \\hfil 42.80   \\\\\n        \\quad w/o SecTitle           & \\hfil $\\downarrow$ 0.08 & \\hfil $\\downarrow$ 0.03 & \\hfil $\\downarrow$ 0.06   \\\\ \\bottomrule\n  \\end{tabular}\n   \\caption{Comparisions between GoSum trained with the complete datasets and those without the section titles.} \\label{tab:exp2}\n\\end{table}\nIn addition to the discourse information of the literature, which divides the sentences into different sections, there are also section-specific names, such as ``introduction\", ``methodology\" etc.\nThese specific text title contexts contain semantic information, which helps to improve the performance of GoSum.\nFirst, we set up a control model of \\textbf{GoSum w/o SecTitle}, which has the same architecture as GoSum, but the section title in the training data is replaced with a meaningless text \"section \\#id\".\nThe experimental results in Table~\\ref{tab:exp2} show that the performance of  \\textbf{GoSum w/o SecTitle} is slightly worse than that of GoSum. This indicates that the semantic information about section title text is useful but not essential. The key to performance improvement is the discourse hierarchies of documents.\nMoreover, \\textbf{GoSum w/o SecTitle} drops more significantly on the PubMed dataset. The difference in the performance between \\textbf{GoSum w/o SecTitle}  and GoSum on the arXiv dataset is not significant, probably because the title quality of the documents in the arXiv dataset is not satisfied.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1.1\\columnwidth]{./miscs/exp1.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width. This setup will avoid overfull boxes.\n    \\caption{GoSum performance varies as section information is corrupted at a rate (x-axis). Y-axis is the average ROUGE score. The green dots show the scores of GoSum on the PubMed dataset, while the blue dots show the results of GoSum on the arXiv dataset.}\n    \\label{fig:exp1}\n\\end{figure}\n\n\\noindent \\textbf{Aspects of reinforcement learning:}\nThere are two factors that can improve the performance of GoSum by using reinforcement learning: First, more sampling is performed, which is equivalent to data augmentation; and second, the model gives a feedback reward to different samples during training which helps to distinguish between good and bad samples. The experimental results on investigating the impact of these two factors on the GoSum performance are reported in Table 6. \n\n\\textbf{w/o reward} sets rewards of all samples to 1, and the experimental results are slightly lower than those of the complete RL model.\n\\textbf{Complete RL} samples an average of 6.52 label sequences per document-abstract pair.\nThe \\textbf{sample top-k} indicates that GoSum is trained with only the $k$ highest sampled label sequences of an input document.\nAs the number of samples increases, the  performance of GoSum improves significantly. In conclusion, the experimental results on RL verified our conjecture.\n\n\\section{Conclusion}\n\\label{sec:conclude}\n\nIn this paper, we have presented a novel approach called GoSum for extracting summaries from long documents. It effectively integrates reinforcement learning with a graph neural network. In particular, we have shown how graph-organized discourse information can  be applied in reinforcement learning-based extractive summarization.\nExperimental results on the arXiv and PubMed datasets have demonstrated that GoSum achieves state-of-the-art performance. The ablation experiments examine the effect of discourse information on GoSum. The results show that the performance of GoSum comes from the use of the hierarchical attribution of sentences and the semantic information about section titles of documents.\nWith achieving satisfactory results in scientific literature, GoSum requires  hierarchical discourse information about long texts as its inputs. In the future, we will attempt to automatically generate discourse information from documents.\n\n\\begin{table}[t]\n  \\centering\n  \\begin{tabular}{l|p{13mm}p{13mm}p{13mm}}\\toprule\n                    & \\hfil R-1  & \\hfil R-2  & \\hfil R-L    \\\\ \\midrule\n        Complete RL                  & \\hfil 49.83 & \\hfil 23.56 & \\hfil 45.10   \\\\\n        \\quad w/o reward             & \\hfil 49.64 & \\hfil 23.37 & \\hfil 44.96   \\\\\n        \\quad sample top-1          & \\hfil 49.10 & \\hfil 23.00 & \\hfil 44.42   \\\\\n        \\quad sample top-2          & \\hfil 49.27 & \\hfil 23.07 & \\hfil 44.61   \\\\\n        \\quad sample top-4          & \\hfil 49.64 & \\hfil 23.33 & \\hfil 44.96   \\\\ \\bottomrule\n  \\end{tabular}\n   \\caption{ GoSum performance by reinforcement learning with different settings on PubMed dataset.} \\label{tab:exp3}\n\\end{table}\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nIn an era where digital text is proliferating at an unprecedented rate, efficient summarization tools are becoming indispensable. While Large Language Models (LLMs) have been successfully applied in various NLP tasks, their role in extractive text summarization remains underexplored. This paper introduces EYEGLAXS (Easy Yet Efficient larGe LAnguage model for eXtractive Summarization), a framework that leverages LLMs, specifically LLAMA2-7B and ChatGLM2-6B, for extractive summarization of lengthy text documents. Instead of abstractive methods, which often suffer from issues like factual inaccuracies and hallucinations, EYEGLAXS focuses on extractive summarization to ensure factual and grammatical integrity. Utilizing state-of-the-art techniques such as Flash Attention and Parameter-Efficient Fine-Tuning (PEFT), EYEGLAXS addresses the computational and resource challenges typically associated with LLMs. The system sets new performance benchmarks on well-known datasets like PubMed and ArXiv. Furthermore, we extend our research through additional analyses that explore the adaptability of LLMs in handling different sequence lengths and their efficiency in training on smaller datasets. These contributions not only set a new standard in the field but also open up promising avenues for future research in extractive text summarization.\n\\end{abstract}\n\n\\section{Introduction}\n\n \n\nIn the era of information overload, text summarization has emerged as a critical tool for distilling essential information from expansive text documents. This paper focuses on automatic text summarization, which can be broadly categorized into two paradigms: abstractive and extractive methods. Abstractive methods, despite their ability to generate flexible and creative summaries, often grapple with issues of grammatical inaccuracy and factual inconsistencies, commonly referred to as \"hallucinations\" \\cite{bishop2022gencomparesum,Ji_2023hallucination,zhang2023sirens}. These challenges are exacerbated when summarizing long texts and can be particularly detrimental in critical applications such as healthcare, scientific research, and legislation. In contrast, extractive summarization offers a more reliable approach by selecting pertinent sentences directly from the source text, thereby ensuring grammatical and factual integrity. Traditionally, this task has been framed as a sentence classification problem and has predominantly employed encoder-only pre-trained models \\cite{liu-lapata-2019-text,cho-etal-2022-toward,bian2023gosum}.\nDespite the promising capabilities of Large Language Models (LLMs) in various NLP tasks, their potential in extractive summarization remains largely untapped. This oversight is partly due to the computational challenges and fine-tuning limitations associated with these sizable models. However, recent advancements in long sequence processing for decoder-only models offer a glimmer of hope for harnessing LLMs in this context.\nTo bridge this gap, our paper introduces EYEGLAXS (Easy Yet Efficient larGe LAnguage model for eXtractive Summarization), a system that leverages the power of LLMsspecifically LLAMA2-7B\\cite{touvron2023llama} and ChatGLM2-6B \\cite{zeng2022glm}. We employ Flash Attention 2 \\cite{dao2023flashattention2} and Parameter-Efficient Fine-Tuning (PEFT)\\cite{lialin2023scaling} techniques to mitigate some of the challenges associated with using LLMs. Our contributions are manifold: we not only propose a novel method for employing LLMs in long extractive summarization tasks but also demonstrate their competitive performance against state-of-the-art methods. We further explore the adaptability of LLMs in handling varying sequence lengths and investigate their training efficiency on smaller datasets. Lastly, we delve into the issue of position bias inherent in LLMs.\n\n\\section{Litterature Review}\n\n\\subsection{Long Extractive Text Summarization}\n\nIn the literature, the task of extractive text summarization is predominantly approached as a sentence classification problem. In this framework, models are trained to predict a label for each sentence in the input document to determine whether or not the sentence should be included in the generated summary. Most state-of-the-art methods leverage pre-trained transformer models that are adapted for natural language understanding tasks. One of the pioneering works in this area slightly modified BERT's architecture by incorporating a priori information on sentence splitting and adding layers of inter-sentence transformers before feeding them into the classifier for prediction \\cite{liu-lapata-2019-text}. To address the issue of limited context size, various transformer architectures have been proposed to mitigate the quadratic complexity problem associated with self-attention computation. For instance, Longformer \\cite{beltagy2020longformer} and Bigbird \\cite{Zaheer2020bigbird} employ attention sparse methods such as sliding windows to handle longer sequences. Building on these architectures, many systems introduce additional mechanisms that exploit the unique characteristics of documents to improve performance on long sequences. For example, the work by \\cite{xiao-carenini-2019-extractive} generates different representations that consider both local and global contexts. The system described in \\cite{ruan-etal-2022-histruct} explicitly incorporates hierarchical information by using section titles and the hierarchical position of sentences to enrich representations. Similarly, \\cite{bian2023gosum} represents the hierarchical structure of the text through a heterogeneous graph of sentences and sections, while integrating reinforcement learning with a graph neural network. The approach by \\cite{cho-etal-2022-toward} aims to discover the latent structure of the document by jointly optimizing a secondary task of section segmentation alongside sentence extraction. Moreover, \\cite{xie2022pre} incorporates domain-specific knowledge into the model by using adapters to infuse medical expertise. The abstractive-extractive approach has also been explored. For instance, \\cite{bishop2022gencomparesum} generates an abstractive summary that later guides the extraction of salient sentences, irrespective of document length. Most of these approaches employ pre-trained RoBERTa \\cite{DBLProberta} as the backbone model. However, this comes with limitations, such as the complexity of learning new positional encoding tokens and the relatively small number of parameters and tokens encountered during the pre-training stage, especially when compared to larger language models.\n\n\\subsection{Large Language Models}\n\nOver the past few years, pre-trained Large Language Models (LLMs) have transitioned from being virtually unknown to becoming pervasive in the realm of machine learning. Their widespread adoption is largely due to their proven effectiveness in addressing zero-shot and few-shot learning challenges. They have been successfully applied to tasks such as abstractive summarization and translation of short documents. While these models excel in generative tasks, their application to extractive tasks poses greater challenges. A common workaround is to transform an extractive task into a text generation task by utilizing cloze prompting templates \\cite{pretrainprompt}. While this technique is well-suited for simpler tasks like sentiment detection, its complexity escalates for tasks with more intricate scoring systems, such as named entity recognition, and becomes nearly unfeasible for tasks like extractive summarization \\cite{2023arXiv230410428WGPTNER}. Additionally, these models are prone to hallucination issues, limiting their applicability in critical fields like healthcare \\cite{kaddour2023challenges}. Although zero-shot approaches to extractive summarization have been explored \\cite{zhang2023extractivegpt}, to the best of our knowledge, no attempts have been made to evaluate the fine-tuning of these models specifically for extractive summarization. Furthermore, it is plausible that the representations learned by these LLMs are richer than those learned by encoder models, owing to their larger number of parameters and more extensive pretraining data \\cite{ni-etal-2022-sentence}. Finally, efforts have been made to significantly increase the context size of these LLMs, notably through the use of scalable positional encodings \\cite{Rotaryrope}, extending the initial pretraining context length \\cite{chen2023extendingPI}, or optimizing attention computation at the GPU memory level \\cite{dao2022flashattention,dao2023flashattention2}.\n\n\\subsection{Parameter-Efficient Fine-Tuning methods}\n\nLarge Language Models (LLMs) are increasingly being utilized to achieve state-of-the-art performance in various NLP tasks, capturing the attention of both researchers and industry professionals \\cite{bubeck2023sparks}. However, these models come with significant computational and memory requirements for training from scratch. Recent iterations of these models often boast more than 70 billion parameters \\cite{touvron2023llama,zeng2022glm}, making the fine-tuning process highly resource-intensive.\nTo address this challenge, a new family of techniques known as Parameter-Efficient Fine-Tuning (PEFT)\\cite{lialin2023scaling} has been introduced. These methods advocate for the training of a relatively small number of additional parameters, which, in comparison to the overall size of the models, represent only a fraction. This approach substantially reduces both storage and computational costs. Among the PEFT techniques, prompt tuning, prefix tuning \\cite{liu-etal-2022-ptuning}, and LoRA \\cite{hu2022LoRA} have garnered significant attention. Models trained using PEFT methods have demonstrated performance levels comparable to those achieved through full fine-tuning \\cite{lialin2023scaling}. \nIn this article, we propose to fine-tune pre-trained LLMs using LoRA for the specific task of extractive summarization.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\linewidth]{ROPE_EYEGLAXS_4.drawio.png}\n    \\caption{The overall framework of EYEGLAXS. Residual connections and normalizations do not appear for better readability. Snowflake logo means that weights are frozen, while Fire logo means that weights are trainable.}\n    \\label{fig:fig1}\n\\end{figure}\n\n\\section{Method}\n\n\\subsection{Task Definition}\n\nWe define extractive summarization as a sentence classification problem. Let note $D = \\{s_1,s_2, \\dots,s_n\\}$ the document D consisting of $n$ sentences. The extractive summarization task aims to predict labels $\\hat{y}_i \\in (0,1)$ for each \\( i \\in [0, n] \\) where $\\hat{y}_i=1$ and $\\hat{y}_i=0$ means the sentence should be included , or not, in the summary respectively. As datasets only contain document-abstract pairs, in order to train these models in a supervised manner, reference extractive abstracts are generated using methods such as greedy algorithms with the objective to maximize a chosen metric, mostly ROUGE scores \\cite{kedzie-etal-2018-content,lin-2004-rouge}. This allows associating a label to each sentence of the original document based on their inclusion in these oracle summaries.\nThese oracle summaries represent an upper bound for the expected performances of extractive models. Thus a label $y_i\\in (0,1)$, is associated to each sentence of the text depending on its inclusion in the oracle. \n\nWe propose a system called EYEGLAXS (Easy Yet Efficient larGe LAnguage model for eXtractive Summarization), a system based on Large Language Models for the long text extractive summarization task described in Figure \\ref{fig:fig1}. \n\n\\subsection{Choice of Large Language Models}\n\nLarge Language Models (LLMs) are notably resource-intensive to train from scratch, making it common practice to leverage pre-existing architectures for new applications. To improve both reproducibility and evaluation, we employ fine-tuning techniques on models that have publicly available checkpoints. A crucial aspect of our selection process is the choice of models that feature extendable positional encoding mechanisms \\cite{Rotaryrope, press2021trainalibi}. We also ensure that the selected positional encoding can be efficiently parallelized to benefit from Flash Attention 2, which offers efficient GPU memory consumption \\cite{dao2022flashattention, dao2023flashattention2}. As a result, models relying on relative positional encodingsuch as those based on the T5 architectureare not suitable for our use case. Likewise, models with learned positional encoding, like XLM \\cite{goyal-etal-2021-largerxlm}, pose scalability challenges unless new positional encodings are retrained.\nGuided by these considerations, we have narrowed our evaluation to two distinct LLMs: first, LLAMA2 \\cite{touvron2023llama}, a decoder-only model that has gained widespread adoption; and second, ChatGLM2 \\cite{zeng2022glm}, a prefix decoder model that shows promise for superior information integration through the use of bidirectional attention mechanisms.\nMore specifically, we assess the performance of the pre-trained long-sequence instruct-based models LLAMA2-7B-32K-Instruct\\footnote{https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct} and ChatGLM2-6B-32K\\footnote{https://huggingface.co/THUDM/chatglm2-6b-32k}, both available on HuggingFace. The former has been fine-tuned for long-context summarization using the BookSum dataset, while the latter excels in conversational contexts. During our experiments, the focus was primarily on evaluating the feasibility of fine-tuning these models using LoRA. We did not place particular emphasis on the instructions used during training. Instead, we adhered to minimalist prompts, which were formed by concatenating the sentences of the input documents. This approach respected the format used by each model and did not include any additional instructions.\n\n\\subsection{Transformer}\n\nLarge Language Models (LLMs) are commonly built using transformer architectures. Architectures such as LLAMA2 \\cite{touvron2023llama} and GLM \\cite{zeng2022glm} consist of an initial embedding input layer followed by multiple decoder layers. Each of these decoder layers comprises Query-Key-Value (QKV) Projection Layers, Rotary Position Encoding, a self-attention module, an output projection, a multi-layer perceptron, residual connections, and normalization. For the sake of readability, we focus solely on our modifications to the original architecture in the areas of QKV Projection Layers, Rotary Position Encoding, self-attention module, and output projection.\n\n\\subsection{Query Key Value Projection Layers with LoRA} \n\\label{sec:qkvLoRA}\nThis module transforms the input token $x_m$ with position $m$ into a trio of queries, keys, and valuesrepresented as $\\{q_m, k_m, v_m\\}$ through linear projection layers with corresponding weight matrices $\\{Wq , Wk , Wv\\}$. These matrices are later used to compute attention values. We apply LoRA on these specific matrices. More precisely, for a pre-trained matrix $W \\in \\mathbb{R}^{d\\cdot p}$ , we add low rank adapter $\\delta W$ such as :\n\n\\begin{equation}\n\\begin{split}\nq_m = (W_q+\\delta W_q)x_m = (W + B_q A_q)x_m \\\\\nk_m = (W_k+\\delta W_k)x_m = (W + B_k A_k)x_m \\\\\nv_m = (W_v+\\delta W_v)x_m = (W + B_v A_v)x_m \\\\\n\\end{split}\n\\end{equation}\nwhere $B_{\\{q,k,v\\}} \\in \\mathbb{R}^{d\\times r}$ , $A_{\\{q,k,v\\}} \\in \\mathbb{R}^{r\\times p}$, $d$ is the dimension of $x_m$ and with $r \\ll min(d,p)$ . The idea is that the weight updates in pre-trained models have a low intrinsic rank during adaptation. Thus, during training, $W_{\\{q,k,v\\}}$ weights are frozen and the number of trainable parameters (i.e. $B_{\\{q,k,v\\}}$ and $A_{\\{q,k,v\\}}$ ) are drastically reduced compared to full fine-tuning setting.\n\n\\subsection{Rotary Positional Encoding }\n\nWe employ architectures using a novel positional encoding scheme called Rotary Positional Embeddings (RoPE)\\cite{Rotaryrope}. RoPE is a distinctive form of positional embedding used in Transformer models to encode the absolute and relative positional information of tokens within a sequence. Moreover, RoPE is valued for its flexibility to expand to any sequence lengths \\cite{Rotaryrope}. \n\nThe mathematical intuition behind RoPE aims to devise a positional encoding function $f(x,m)$ for a token $x$ at position $m$ such that for a query vector $q_m$ and a key vector $k_l$ at positions $m$ and $l$ respectively, the inner product between $f(q_m,m)$ and $f(k_l,l)$ is sensitive only to the values of $q_m$, $k_n$, and their relative position $(m-l)$.\n\nIn practice, new vectors $\\tilde{q_m}$ and $\\tilde{k_l}$ are computed following this specific equation\n\\begin{equation}\n\\begin{split}\n\\tilde{q_m}=R^m_{\\theta,d}(q_m) \\\\\n\\tilde{k_l}=R^l_{\\theta,d}(k_l)\n\\end{split}\n\\end{equation}\nwith \n\\begin{equation}\n\\begin{multlined}\n R^d_{\\Theta, m}(x) = \n\t\\begin{pmatrix}\n\t\tx_1\\\\\n\t\tx_2\\\\\n\t\tx_3\\\\\n\t\tx_4\\\\\n\t\t\\vdots\\\\\n\t\tx_{d-1}\\\\\n\t\tx_d\n\t\\end{pmatrix}\n\t\\otimes\n\t\\begin{pmatrix}\n\t\t\\cos{m\\theta_1} \\\\\n\t\t\\cos{m\\theta_1} \\\\\n\t\t\\cos{m\\theta_2} \\\\\n\t\t\\cos{m\\theta_2} \\\\\n\t\t\\vdots \\\\\n\t\t\\cos{m\\theta_{d/2}} \\\\\n\t\t\\cos{m\\theta_{d/2}} \n\t\\end{pmatrix}\n\t+ \\\\\n\t\\begin{pmatrix}\n\t\t-x_2\\\\\n\t\tx_1\\\\\n\t\t-x_4\\\\\n\t\tx_3\\\\\n\t\t\\vdots\\\\\n\t\t-x_{d-1}\\\\\n\t\tx_d\n\t\\end{pmatrix}\n\t\\otimes\n\t\\begin{pmatrix}\n\t\t\\sin{m\\theta_1}\\\\\n\t\t\\sin{m\\theta_1}\\\\\n\t\t\\sin{m\\theta_2}\\\\\n\t\t\\sin{m\\theta_2}\\\\\n\t\t\\vdots\\\\\n\t\t\\sin{m\\theta_{d/2}}\\\\\n\t\t\\sin{m\\theta_{d/2}}\n\t\\end{pmatrix}\n \\end{multlined}\n \\end{equation} \n\nwhere $m$ is the indice position and $d$ is the dimension of the $x$.\n\nWhile RoPE is in theory expandable to any sequence lengths, we found an exploding perplexity when directly extending a pre-trained model beyond the context length \\(L\\) of the pretraining process\\cite{chen2023extendingPI}. In order to overcome this problem, we interpolate position indices from longer context length \\(L'\\) (i.e., \\([0, L')\\)) to original pre-trained context length \\(L\\) (i.e., \\([0, L)\\)) in order to match the original range of indices.\n\nFormally, we replace $R^d_{\\Theta, m}(x)$  by ${R^{'d}_{\\Theta, m}}(x)$ function where \n\\begin{equation}\n     {R^{'d}_{\\Theta, m}}(x)=R^d_{\\Theta, m \\cdot \\alpha}(x)\n\\end{equation}\nwith a parameterized scaling factor $\\alpha$ defined as below: \\[\\alpha=\\dfrac{L}{L'}\\]\n\n\\subsection{Self-Attention Module and Output Projection}\n\nOnce $\\{\\tilde{q},\\tilde{k},v\\}$ are computed, we compute the outputs $o$ via a self-attention module as\n\\begin{equation}\n    o=softmax(\\tilde{q}\\tilde{k}^T)v\n\\end{equation}\nA significant challenge associated with self-attention is its computational burden when handling long sequences. Specifically, both computational and memory requirements increase quadratically with the length of the sequence. To address this limitation, we replace the original attention computation with Flash Attention 2\\cite{dao2023flashattention2}, an Input-Output attention algorithm that scales memory consumption linearly and accelerates the training process. It's important to note that the attention computed using Flash Attention 2 is identical to that of the original operation. In practical terms, this allows us to process sequences of up to 12,000 tokens on a single A10 GPU card.\n\nSubsequently, these outputs are projected by a linear layer with a weight matrix $W_o$. Similarly to the projection matrices $\\{W_q , W_k , W_v\\}$ We apply LoRA to the output projection matrix $W_o$.\n\n\\subsection{Mean Pooling and Classification Layer}\n\nUnlike BERT models, LLMs such as LLAMA2 or ChatGLM2 do not use a 'CLS' token like in other models at the beginning of each sentence to get its representation. Nonetheless, we think that some knowledge is still well encoded within the token representations and a mean pooling across all input sentence tokens should provide a natural sentence representation\\cite{ni-etal-2022-sentence}. Therefore, we apply a mean pooling at the sentence level. More precisely, for each sentence $s_i$ comprised of a list of $M_i$ contextualized words processed by previous decoder layers of the LLM $\\{w_{i,1},w_{i,2} \\dots w_{i,M_i}\\}$, we compute $\\bar{s_i}$\n\\begin{equation}\n\\bar{s_i} = \\frac{1}{M_i} \\sum_{j=1}^{M_i} w_{i,j}    \n\\end{equation}\n\nOnce, $\\bar{s_i}$ obtained, we pass it through a linear classification layer :\n\n\\begin{equation}\n    \\hat{y_i} = \\sigma(W_c \\bar{s_i} + b)\n\\end{equation}\n\nwhere $W_c$ is a weight matrix trainable, $b$ is a biais term trainable and $\\sigma$ is the sigmoid function. The loss function used is the binary cross entropy between $\\hat{y_i}$ and the oracle $y_i$.\n\n\\section{Experiments}\n\nIn this section, we present the results of our various experiments demonstrating the performances of our models in different settings compared to strong baselines of the state-of-the-art. %We specify the detailed experimental settings of each experiment and share the models and used code on our repository \\footnote{anonymous}.\n\n\\subsection{Datasets}\n\n\\begin{table*}[t]\n    \\small\n    \\centering\n    \\begin{tabular}{|c|c|c|c|c|c|c|} % 7 columns in total\n        \\hline\n        Set& \\multicolumn{3}{|c|}{PubMed} & \\multicolumn{3}{|c|}{Arxiv} \\\\ % Main headers\n        \\hline\n        & Length-4K& Length-12K& Length-16K& Length-4K& Length-12K& Length-32K\\\\ % Sub-columns\n        \\hline\n         Train&  70893&  127192&  131233&  38532&  153802&  202648\\\\ % Data row\n        \\hline\n         Validation&  3630&  6442&  6630&  1124&  5089&  6435\\\\\n        \\hline\n         Test&  3682&  6472&  6657&  1076&  5085&  6439\\\\ \\hline\n    \\end{tabular}\n    \\caption{The datasets we used in the experiments. cell values correspond to the number of documents for each dataset and split. Pubmed-16k and Arxiv-32k have been truncated from original datasets, contrary to the other datasets where longer documents have been filtered out.}\n    \\label{table:datasets}\n\\end{table*}\n\nFor our experiments, we evaluate our approach with two sources wildly used in summarization tasks, namely the arXiv and PubMed datasets\\cite{cohan-etal-2018-discourse}. They consist of rather long scientific papers, with PubMed focusing on the biomedical domain while arXiv includes articles from various scientific fields.\n\nTo train the model in a supervised manner for the extractive summarization task, sentence labels are needed. We use the already-computed labels from \\cite{cho-etal-2022-toward}\\footnote{https://github.com/tencent-ailab/Lodoss/tree/main}. They followed the methodology of \\cite{kedzie-etal-2018-content} with the objective of maximizing the average of the R1 and R2 scores. Moreover, we derive from the original dataset two filtered datasets containing only documents shorter that a given sequence length, in order to evaluate EYEGLAXS trained on shorter documents when tested on longer documents.\nCharacteristics of the resulting datasets are shown in the Table \\ref{table:datasets}.\n\n\\subsection{Experimental settings}\n\\label{section:experimental_setting}\n\nWe used and modified the implementation released on the TransformerSum\\footnote{https://github.com/HHousen/TransformerSum/tree/master}. Experiments have been carried out on 8 NVIDIA A10G GPUs. Aside from the experiments on low-volume data, models have been trained for 5 epochs, with a validation step occurring every fifth of an epoch. Models were saved based on the smallest validation loss achieved. We use a batch size of 1 with gradient accumulation every 32 steps and the adam8bit optimizer with a 3e-5 learning rate. Gradient-checkpointing and bf16-mixed precision are used. Deepspeed stage 1 is employed. No advanced hyperparameter search was performed. We use sequence lengths of 4k and 12k to train our models before testing them on the full length dataset. Results are obtained without trigram blocking by selecting the 7 and 5 sentences with the highest probability scores for the PubMed and arXiv datasets respectively as it is done in \\cite{cho-etal-2022-toward}. The scaling factor $\\alpha$ for RoPE is set to 8 to handle up to 32K length context. Rank $r$ of LoRA is set to 8. \n\n\\subsection{Evaluation Metrics}\n\nWe use ROUGE scores to evaluate the model performance\\cite{lin-2004-rouge}. More precisely, we report the F1 score of unigram, bigram overlap (ROUGE-1, ROUGE-2) and the longest common subsequence (ROUGE-L). We use the python implementation\\footnote{https://github.com/google-research/google-research/tree/master/rouge}.\n\n\\subsection{Baseline systems}\n\n\\begin{table}\n    \\centering\n    \\small\n    \\begin{tabular}{|l|c|c|c|} \\hline \n         Models&  R-1&  R-2& R-L\\\\ \\hline \n         \\hline\n         \\multicolumn{4}{|c|}{Abstractive Models}\\\\ \\hline \n         Bigbird-large&46.32&20.65&42.33\\\\ \\hline \n         Long-T5&50.23  & 24.76 & 46.67 \\\\ \\hline \n         \\hline\n         \\multicolumn{4}{|c|}{Extractive Models}\\\\ \\hline \n         ORACLE& 61.49 & 34.70 & 55.92 \\\\ \\hline \n LEAD-10& 37.45&14.19 &34.07\\\\ \\hline \n SumBasic& 37.15&11.36 &33.43\\\\ \\hline \n LexRank&39.19&13.89 &34.59 \\\\ \\hline \n Sent-PTR& 45.01&19.91 &41.16\\\\ \\hline \n GenCompareSum&42.10 &16.51 &38.25\\\\ \\hline \n Histruct+&46.59 & 20.39&42.11\\\\ \\hline \n Lodoss-base (Longformer)& 48.10&22.53 &43.51\\\\ \\hline \n Lodoss-full-LG& 49.38&23.89 &44.84\\\\ \\hline \n GoSum&49.83 &23.56 &45.10\\\\\\hline \\hline \n \\hline\n \\multicolumn{4}{|c|}{Our system EYEGLAXS (Extractive)}\\\\ \\hline \n CHATGLM2-6B (4K)&49.96 &24.04 &45.50\\\\ \\hline \n CHATGLM2-6B (12K)&50.17 &24.41 &45.66\\\\ \\hline \n LLAMA2-7B (4K)&49.48 &23.64 &45.08\\\\ \\hline \n LLAMA2-7B (12K)&\\textbf{50.34 }&\\textbf{24.57} &\\textbf{45.96}\\\\ \\hline\n    \\end{tabular}\n    \\caption{ROUGE results on the PubMed dataset}\n    \\label{tab:result_pubmed}\n\\end{table}\n\nUsing the arXiv and PubMed datasets, which are two popular datasets in the domain of extractive summarization, allows us to easily assess the relevance of our approach. We can directly compare the results obtained with our models against previous systems of the state-of-the-art on the same ROUGE metrics. \nAmong the baselines, we compare our approach with standard lexical methods like Sumbasic \\cite{vanderwende2007beyond} that is based on word frequencies or LexRank \\cite{erkan2004lexrank} that uses a graph-based approach and centrality scoring of sentences. Taking advantage of what language models have to offer and building upon the BERTSUM framework \\cite{liu-lapata-2019-text}, we have strong extractive baselines: HiStruct+ \\cite{ruan-etal-2022-histruct} explicitly exploits the hierarchical structure of the text, taking advantage of the position of sentences within sections. Among more recent models, Lodoss \\cite{cho-etal-2022-toward} represents a strong baseline that achieved great performances by being jointly trained for text summarization and segmentation in addition to using a novel regularizer to boost diversity among the selected summary sentences. GoSum \\cite{bian2023gosum} is another state-of-the-art model that showed some of the best results by exploiting graph neural networks and reinforcement learning. Alongside these extractive models that we directly compare ourselves to, we also show the performances of some popular abstractive baseline. Rather than measuring up against them, we simply add them as a reference to put the results into perspective. \n\n\\section{Results and Analyses}\n\n\\begin{table}\n    \\centering\n    \\small\n    \\begin{tabular}{|c|c|c|c|} \\hline \n         Models&  R-1&  R-2& R-L\\\\ \\hline \n         \\multicolumn{4}{|c|}{Abstractive Models}\\\\ \\hline \n         Bigbird-large&46.63&19.02&41.77\\\\ \\hline \n         Long-T5& 48.35 & 21.92 & 44.27 \\\\ \\hline \n         \\multicolumn{4}{|c|}{Extractive Models}\\\\ \\hline \n         ORACLE& 59.41 & 30.05 & 52.34 \\\\ \\hline \n LEAD-10&35.52 &10.33 &31.44\\\\ \\hline \n SumBasic&29.47 &6.95&26.30\\\\ \\hline \n LexRank&33.85 &10.73 &28.99\\\\ \\hline \n Sent-PTR& 42.32& 15.63&38.06\\\\ \\hline \n GenCompareSum& 39.66& 12.30&35.38\\\\ \\hline \n Histruct+& 45.22& 17.67&40.16\\\\ \\hline \n Lodoss-base (Longformer)& 47.64& 19.73&41.71\\\\ \\hline \n Lodoss-full-LG& 48.45& 20.72&42.55\\\\ \\hline \n GoSum& 48.61& 20.53&42.80\\\\\\hline \\hline \n \\multicolumn{4}{|c|}{Our system EYEGLAXS (Extractive)}\\\\ \\hline \n CHATGLM2-6B (4K)& 46.87&18.96 &41.37 \\\\ \\hline \n CHATGLM2-6B (12K)&\\textbf{49.02}& 21.01&\\textbf{43.33} \\\\ \\hline  \n LLAMA2-7B (4K)& 48.68& 20.72&42.97\\\\ \\hline \n LLAMA2-7B (12K)&48.96 &\\textbf{21.07} &43.30\\\\ \\hline\n    \\end{tabular}\n    \\caption{ROUGE results on the arXiv dataset}\n    \\label{tab:result_arxiv}\n\\end{table}\n\nResults on PubMed and arXiv are shown respectively in table \\ref{tab:result_pubmed} and table \\ref{tab:result_arxiv}. For both datasets, the EYEGLAXS variants, specifically ChatGLM2-6B (4K) and ChatGLM2-6B (12K), alongside the LLAMA variants, LLAMA2-7B (4K) and LLAMA2-7B (12K), showcase competitive performance compared to the state-of-the-art even if they are trained on smaller and shorter dataset. Models trained on a longer context (12K variant) exhibit superior performance compared to their counterpart trained on a shorter context. LLAMA2-7B (12K) and ChatGLM2-6B (12K) seem to have similar performances on both datasets. However, ChatGLM2-6B (4K) seems to underperform compared to LLAMA2-7B (4K) on arXiv, we hypothesize that since the arXiv dataset contains longer documents, LLAMA2 benefited more from his pretraining stage on long document compared to ChatGLM2 and could potentially need less amount of training data to converge (see section \\ref{sec:small_datasets} ). Finally, we obtain new state-of-the-art results compared to other extractive methods on both datasets. We provide also in appendix the complete table results on filtered datasets (e.g. training LLAMA2-7B on the 4K dataset and testing it on the 12K dataset).\n\n\\subsection{Evaluating LoRA's Impact on Fine-Tuning EYEGLAXS}\n\\begin{table}\n    \\centering\n    \\small\n    \\begin{tabular}{|l|c|c|c|} \\hline  \n Model& \\multicolumn{3}{|c|}{4K PubMed Dataset}\\\\ \\hline  \n & R1& R2& RL\\\\ \\hline  \nCHATGLM2-6B (4K) - Frozen& 42.79& 17.20& 38.68\\\\ \\hline \nCHATGLM2-6B (4K) - LoRA& 49.96& 24.04& 45.50\\\\ \\hline  \nLLAMA2-7B (4K) - Frozen& 42.38& 17.12& 38.42\\\\ \\hline \nLLAMA2-7B (4K) - LoRA& 49.48& 23.64& 45.08\\\\ \\hline  \n\n    \\end{tabular}\n    \\caption{Comparison of ROUGE Scores between Frozen Weights and EYEGLAXS Models on 4K PubMed Dataset}\n    \\label{tab:LoRA_frozen}\n\\end{table}\n\nTo assess the contribution of LoRA and determine the relevance of the hidden representations provided by LLMs, we contrasted the performance of EYEGLAXS models trained on the 4K PubMed filtered dataset with a variant of same models with frozen weights, albeit with a trainable classifier head. The comparative outcomes are presented in Table \\ref{tab:LoRA_frozen}. The notable enhancement in ROUGE scores underscores the necessity of employing Parameter-Efficient Fine-Tuning (PEFT) methodologies like LoRA to fine-tune LLMs, suggesting that the standalone hidden representations from LLMs may fall short of ensuring optimal performance.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\linewidth]{error_1.png}\n    \\caption{Number of sentences selected at each relative position by the EYEGLAXS models and baselines compared to the oracles}\n    \\label{fig:fig_err_1}\n\\end{figure}\n\n\\subsection{Position Error Analysis}\n\nTo get a better understanding of the strengths and weaknesses of our system, we further analyze the outputs of our models.\nThis section is used to check the general behavior of the model, and can help verify some reported problems with bias when using LLMs for long sequences, including a natural bias at the beginning and end of each document \\cite{liu2023lost}. We choose to examine the extracted sentences and compare them to the ones forming the oracles.\nTo achieve this, we first trace out the distribution of the selected sentences for both versions of our model as well as a Longformer baseline (Lodoss-base) \\cite{cho-etal-2022-toward} and the oracles. %For each document, we predict a number of sentences equal to the oracle's. \nThe lengths of the documents forming the datasets having a wide amplitude, we choose to use the relative positions of sentences to ensure an overall homogeneous comparison. To do so, we compare the absolute index of each extracted sentences against the documents total number of sentences, then plot the resulting histogram. This gives us a histogram showing the relative position of the sentences selected by each model. The results are showed for the PubMed dataset on the test split. %, the same analysis having been performed on the 4k dataset with similar results.\nFrom the Figure \\ref{fig:fig_err_1}, we can see that the sentences chosen for the oracles or predicted by the models tend to be near the beginning and the end of the document. It is not surprising since both the introduction and the conclusion usually contain sentences that are representative of the document's subject. We can observe that the three models tend to choose sentences near both ends of the text in excess. On the subject of accessing relevant information located in the middle of inputs, even though all the models are lagging behind compared to the oracles, we notice that the EYEGLAXS models follow the oracle trend a little better.% we remark that EYEGLAXS models follow the trend \\mdquest{dessine} by the oracles slightly better.\n\n\\subsection{Training on smaller datasets}\n\\label{sec:small_datasets}\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{small_dataset.png}\n    \\caption{ROUGE-2 F1 Measure scores for Longformer (Lodoss-base), ChatGLM2-LoRA (4K) and LLAMA2-LoRA across varying training data sizes. The exact number of training instances is indicated in parentheses.}\n    \\label{fig:datasetsize}\n\\end{figure}\n\nIn the medical field, obtaining a large database like PubMed is often challenging. Assessing the performance of Large Language Models (LLMs) on smaller databases compared to traditional methods becomes crucial. To this end, we conducted an experiment comparing the performance of ChatGLM2-6B (4K), Longformer (Lodoss-base) \\cite{cho-etal-2022-toward}, and LLAMA2-7B LoRA (4k), on the filtered PubMed 4K dataset using varying portions of the training data. Specifically, we examined the performance when utilizing 1\\%, 5\\%, 10\\%, and 100\\% of the PubMed 4K dataset. The findings are illustrated in Figure \\ref{fig:datasetsize}.\nThe results confirm a positive correlation between the size of the training dataset and the performance metrics for all three models, aligning with the intuitive expectation that larger training sets generally lead to improved model performance. Interestingly, the performance gap among the three models exhibits varying dynamics as the training data size increases. \nChatGLM2-6B (4K) and LLAMA2-7B(4K) consistently outperform Longformer across all sizes of training data, validating the efficacy of LLMs even when limited data is available. %Interestingly, ChatGLM2-6B (4K) continues to show improvements as the dataset size increases, suggesting a potentially higher ceiling for performance gains with additional training data. \nLLAMA2-7B LoRA (4K) starts off with a strong performance at just 1\\% of the training data and maintains the lead as the dataset grows. However, its performance appears to converge, showing marginal gains as the dataset size increases compared to CHATGLM2-6B (4K) which seems to show improvements as the dataset size increases. This divergence in behavior may be attributed to the architectural differences between the two models. Specifically, ChatGLM2-6B (4K) employs bidirectional attention mechanisms, which could require a larger dataset to optimize but also offer a more favorable inductive bias for information extraction tasks.\n\n\\section{Conclusion}\n\nThis paper introduces EYEGLAXS, a novel system that leverages Large Language Models (LLMs) for long text extractive summarization. Our work challenges the traditional reliance on encoder-only models, showcases the adaptability of LLMs in managing different sequence lengths, and sets new performance standards on the PubMed and arXiv datasets. Despite these advancements, the use of LLMs comes with its own set of challenges, notably in computational resource requirements and the limitations of fine-tuning. Looking ahead, we aim to integrate sliding attention mechanisms in LLMs to further refine our system. Additionally, we plan to enrich the LLM backbone with existing techniques such as graph-based methods or Reinforcement Learning. Overall, our work paves the way for new research avenues in extractive text summarization and substantiates the utility of LLMs in this field.\n\n\\section{Limitations}\nWhile EYEGLAXS demonstrates promising advancements in extractive text summarization, it is not without its challenges. The use of Large Language Models (LLMs) requires significant computational resources, making it less accessible for those with limited capabilities. Moreover, we report only a single run for each of our experiment due to the expensive training time, as can be seen in the Table \\ref{tab:training_time} in the appendix showing the duration of the training epochs. Additionally, the sheer size of these LLMs restricts the possibility of full fine-tuning, thereby limiting further optimization and reporting upper limit of the full fine-tuning. The model's performance is also closely tied to the size of the training dataset, especially for CHATGLM2, which could be a constraint in fields where large, labeled datasets are not readily available. Lastly, the system's generalizability remains untested outside of scientific contexts like PubMed and arXiv. These limitations offer valuable avenues for future research to improve the system's robustness and applicability. Finally, we wanted to highlight again the security risk if such tool is used in sensitive applications (e.g., legal, medical), poor performance or errors could lead to serious consequences.\n\n\\appendix\n\n\\section{Results on the different datasets}\n\n\\begin{table*}\n    \\centering\n    \\small\n    \\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|} \\hline  \n          \\multicolumn{1}{|c|}{Model}&Training Context Length&  \\multicolumn{9}{|c|}{Evaluation Context Length}\\\\ \\hline  \n & & \\multicolumn{9}{|c|}{ARXIV}\\\\ \\hline  \n  && \\multicolumn{3}{|c|}{4K}& \\multicolumn{3}{|c|}{12K}& \\multicolumn{3}{|c|}{32K}\\\\ \\hline  \n & & R1& R2& RL& R1& R2& RL& R1& R2&RL\\\\ \\hline  \n  CHATGLM2-6b&4K& 45.18& 18.07& 39.73& 47.55& 19.64& 41.97& 46.87& 18.96&41.37\\\\ \\hline  \n  &12k& 46.11& 19.10& 40.59& 48.96& 21.11& 43.26& 49.02& 21.01&43.33\\\\ \\hline  \n LLAMA2-7b& 4K& 45.84& 18.84& 40.37& 48.58& 20.81& 42.90& 48.68& 20.72&42.98\\\\ \\hline  \n & 12K& 45.97& 19.08& 40.54& 48.80& 21.09& 43.16& 48.96& 21.07&43.30\\\\ \\hline  \n & & \\multicolumn{9}{|c|}{PUBMED}\\\\ \\hline  \n  && \\multicolumn{3}{|c|}{4K}& \\multicolumn{3}{|c|}{12K}& \\multicolumn{3}{|c|}{16K}\\\\ \\hline  \n & & R1& R2& RL& R1& R2& RL& R1& R2&RL\\\\ \\hline  \n  CHATGLM2-6b&4K& 49.43& 25.17& 45.35& 50.08& 24.24& 45.64& 49.96& 24.04&45.50\\\\ \\hline  \n  &12k& 49.40& 25.21& 45.26& 50.21& 24.52& 45.72& 50.17& 24.41&45.66\\\\ \\hline  \n LLAMA2-7b& 4K& 48.59& 24.55& 44.58& 49.54& 23.80& 45.15& 49.48& 23.64&45.08\\\\ \\hline  \n & 12K& 50.38& 24.70& 46.02& 50.39& 24.70& 46.03& 50.34 &24.57 &45.96\\\\ \\hline\n    \\end{tabular}\n    \\caption{ROUGE Metrics of EYEGLAXS Variants on ARXIV and PUBMED Datasets at Different Training and Evaluation Context Lengths.}\n    \\label{tab:context_length_complete}\n\\end{table*}\n\nWe provide in Table \\ref{tab:context_length_complete} the different results of all variants from EYEGLAXS tested on the different versions of datasets we have built. This table provides us a better idea about how model trained on short document perfom on a dataset containing longer documents.\n\n\\section{Model Training Time}\n\nWe show in Table \\ref{tab:training_time} the training time for one epoch for each model on both arXiv and PubMed. Hardware specifics and training parameters are specified in the Experimental Settings section.\n\n\\begin{table}\n    \\centering\n    \\begin{tabular}{|c|c|c|c|} \\hline\n        \\multirow{2}{*}{Model} &Training Context&  \\multicolumn{2}{|c|} {Training Time} \\\\ \\cline{3-4}\n         &Length& ARXIV & PUBMED\\\\ \\hline\n         CHATGLM2-6b & 4K & 8h 08m & 8h 06m \\\\ \\hline\n         CHATGLM2-6b & 12K & 52h 54m & 31h 14m \\\\ \\hline\n         LLAMA2-7b & 4K & 8h 36mn & 8h 33m \\\\ \\hline\n         LLAMA2-7b & 12K & 51h 35m & 32h 29m \\\\ \\hline\n    \\end{tabular}\n    \\caption{Training Time for One Epoch of CHATGLM2-6b and LLAMA2-7b Models on ARXIV and PUBMED Datasets at Different Context Lengths.}\n    \\label{tab:training_time}\n\\end{table}\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2203.07586v1.tex",
        "arXiv-2211.10247v2.tex",
        "arXiv-2408.15801v1.tex"
    ],
    "group_id": "group_104",
    "response": "### Title: Advances in Long Document Summarization: A Comparative Analysis of Hierarchical Inference, Graph Neural Networks, and Large Language Models\n\n### Introduction\n\nText summarization is a critical task in natural language processing (NLP) that aims to generate concise summaries from extensive documents, preserving key information and meaning. Historically, summarization models have been categorized into two main approaches: extractive and abstractive. Extractive models select salient sentences or fragments from the source document, while abstractive models generate summaries by conditioning on the document's content, often producing summaries that are semantically coherent and linguistically fluent. However, abstractive models face challenges such as factual inaccuracies and grammatical inconsistencies, commonly referred to as \"hallucinations,\" especially when dealing with long documents. Extractive models, on the other hand, are more reliable in terms of factual and grammatical integrity but have traditionally struggled with long documents due to computational and memory constraints. Recent advancements in NLP have introduced hierarchical inference frameworks, graph neural networks (GNNs), and large language models (LLMs) to address these challenges, aiming to provide more efficient and effective summarization solutions.\n\nThis summary focuses on three recent research papers that explore different methodologies for long document summarization. The first paper introduces a hierarchical inference framework called the Top-Down Transformer, which combines bottom-up and top-down inference to improve latent representation inference for summarization models. The second paper, GoSum, employs a graph neural network and reinforcement learning (RL) to enhance extractive summarization by leveraging the hierarchical structure of documents. The third paper, EYEGLAXS, proposes a system that leverages LLMs for extractive summarization, using techniques like LoRA (Low-Rank Adaptation) and Flash Attention to mitigate computational challenges. These papers collectively address the issues of memory and compute efficiency, long-range dependency capture, and the use of structural information in summarization tasks.\n\n### Main Content of Each Paper\n\n#### Paper 1: Top-Down Transformer\n\nThe Top-Down Transformer paper proposes a hierarchical inference framework to improve summarization models on long documents. This framework assumes a hierarchical latent structure of a document where the top-level captures long-range dependencies at a coarser time scale, while the bottom token level preserves detailed information. The model uses a combination of local self-attention for bottom-up inference and full self-attention for top-down correction. Local self-attention reduces complexity to $O(Nw)$, where $N$ is the number of tokens and $w$ is the window size, making it more efficient than full self-attention models which have a complexity of $O(N^2)$. The top-down correction allows token representations to be updated with global context, mitigating the limitations of local attention. The model is evaluated on diverse summarization datasets, including scientific articles, news, and books, demonstrating competitive or superior performance compared to existing models.\n\nThe paper introduces two pooling methods for initializing segment representations: average pooling and adaptive pooling. The adaptive pooling method uses a learned importance tagger to assign weights to tokens based on their relevance to the summary. The model's performance is evaluated using ROUGE scores, a standard metric for summarization tasks. The authors also conduct ablation studies to validate the importance of the top-down correction and the window size of local self-attention. The results indicate that the top-down correction significantly enhances the model's performance, especially on long documents.\n\n#### Paper 2: GoSum\n\nGoSum focuses on extractive summarization of long documents using reinforcement learning (RL) and graph neural networks (GNNs). The model encodes sentence states in RL by constructing a heterogeneous graph that reflects the discourse hierarchy of the document. Each sentence and section is treated as a node in the graph, and edges are used to restrain semantic drifts across section boundaries. The graph is initialized using GloVe embeddings and BiLSTM with multi-head pooling (MHP) to produce sentence and section representations. The graph attention network (GAT) updates these representations by passing messages between sentence and section nodes. The model then uses a Bi-LSTM to encode global context and an extraction history encoder (EHE) to avoid redundancy in the summary.\n\nGoSum generates summaries sequentially by scoring and selecting sentences based on their state representation. The state representation includes local discourse-aware sentence embeddings, global context embeddings, and extraction history embeddings. The model is trained using RL, where the reward is based on ROUGE scores. The authors conduct comprehensive experiments on the PubMed and arXiv datasets, achieving state-of-the-art results compared to both extractive and abstractive baselines. They also perform ablation studies to validate the importance of discourse information and the hierarchical structure of the document.\n\n#### Paper 3: EYEGLAXS\n\nEYEGLAXS leverages large language models (LLMs) for extractive summarization, specifically using models like LLAMA2-7B and ChatGLM2-6B. The system uses LoRA (Low-Rank Adaptation) to fine-tune the models efficiently, significantly reducing the number of trainable parameters. It also employs Flash Attention to handle long sequences effectively. The model does not use a 'CLS' token for sentence representations but instead applies mean pooling across all tokens in a sentence. The authors evaluate EYEGLAXS on the PubMed and arXiv datasets, demonstrating competitive performance compared to state-of-the-art extractive and abstractive models. They also conduct experiments to assess the impact of LoRA and the effect of training on smaller datasets, showing that EYEGLAXS can achieve new state-of-the-art results even with limited data.\n\n### Commonalities and Innovations\n\nAll three papers address the challenges of summarizing long documents, focusing on improving the efficiency and effectiveness of summarization models. They share a common goal of capturing long-range dependencies and leveraging structural information in documents. However, each paper introduces unique methodologies to achieve this goal.\n\n- **Top-Down Transformer** uses a hierarchical inference framework with both bottom-up and top-down inference. The bottom-up inference uses local self-attention for efficiency, while the top-down correction allows tokens to capture long-range dependencies through cross-attention between token and segment representations.\n- **GoSum** integrates RL with GNNs to encode sentence states in a hierarchical manner, capturing discourse information and extraction history. The model uses a graph to represent the structure of the document and RL to generate summaries that maximize ROUGE scores.\n- **EYEGLAXS** leverages LLMs for extractive summarization, using LoRA and Flash Attention to fine-tune the models efficiently. The system does not rely on 'CLS' tokens but uses mean pooling to generate sentence representations, and it evaluates the impact of LoRA on fine-tuning and the effect of training on smaller datasets.\n\n### Comparison of Results\n\nThe performance of the models is evaluated using ROUGE scores, which measure the overlap between the generated summary and the reference summary in terms of unigram, bigram, and longest common subsequence (LCS). The results are summarized in the following tables:\n\n#### Table 1: ROUGE Scores on PubMed Dataset\n\n| Models                  | R-1   | R-2   | R-L   |\n|-------------------------|-------|-------|-------|\n| Top-Down Transformer (AvgPool) | 48.34 | 21.40 | 44.22 |\n| Top-Down Transformer (AdaPool) | 51.05 | 23.26 | 46.47 |\n| GoSum                    | 49.83 | 23.56 | 45.10 |\n| EYEGLAXS (LLAMA2-7B 4K)  | 49.48 | 23.64 | 45.08 |\n| EYEGLAXS (LLAMA2-7B 12K) | 50.34 | 24.57 | 45.96 |\n| EYEGLAXS (CHATGLM2-6B 4K) | 49.96 | 24.04 | 45.50 |\n| EYEGLAXS (CHATGLM2-6B 12K) | 49.02 | 21.01 | 43.33 |\n\n#### Table 2: ROUGE Scores on arXiv Dataset\n\n| Models                  | R-1   | R-2   | R-L   |\n|-------------------------|-------|-------|-------|\n| Top-Down Transformer (AvgPool) | 48.67 | 20.70 | 43.91 |\n| Top-Down Transformer (AdaPool) | 50.95 | 21.93 | 45.61 |\n| GoSum                    | 48.61 | 20.53 | 42.80 |\n| EYEGLAXS (LLAMA2-7B 4K)  | 48.59 | 24.55 | 44.58 |\n| EYEGLAXS (LLAMA2-7B 12K) | 48.96 | 21.09 | 43.30 |\n| EYEGLAXS (CHATGLM2-6B 4K) | 46.87 | 18.96 | 41.37 |\n| EYEGLAXS (CHATGLM2-6B 12K) | 49.02 | 21.01 | 43.33 |\n\nFrom the tables, it is evident that the Top-Down Transformer and GoSum achieve the highest ROUGE scores on both datasets. EYEGLAXS, although trained on smaller datasets, also demonstrates strong performance, especially when fine-tuned on longer contexts. The AdaPool variant of the Top-Down Transformer outperforms all other models on the PubMed dataset, while GoSum achieves the best performance on the arXiv dataset. EYEGLAXS trained on 12K contexts performs better than its 4K counterpart on both datasets, suggesting that longer training contexts can improve performance.\n\n### Conclusion\n\nThe three papers introduce novel approaches to long document summarization, each addressing unique challenges and contributing to the field in different ways. The Top-Down Transformer improves latent representation inference by combining bottom-up and top-down inference, achieving state-of-the-art performance on a wide range of summarization benchmarks. GoSum integrates RL with GNNs to capture discourse information and extraction history, demonstrating strong performance on scientific literature datasets. EYEGLAXS leverages LLMs for extractive summarization, using LoRA and Flash Attention to fine-tune the models efficiently and achieve competitive performance even on smaller datasets.\n\nThe main findings from these papers suggest that hierarchical inference, graph neural networks, and large language models can significantly enhance summarization performance on long documents. However, each approach has its own limitations and challenges. The Top-Down Transformer requires careful initialization of segment representations and efficient handling of cross-attention operations. GoSum relies on accurate discourse information and section titles, which may not always be available or consistent. EYEGLAXS faces computational resource requirements and the need for substantial training data, although it demonstrates adaptability to different sequence lengths.\n\nFuture research directions could include integrating sliding attention mechanisms in LLMs, enriching the LLM backbone with graph-based methods or RL, and testing the generalizability of these models outside of scientific contexts. Additionally, addressing the issue of position bias in LLMs and exploring methods to automatically generate discourse information from documents could further improve summarization performance. The use of these advanced techniques in summarization models can pave the way for more efficient and effective summarization solutions, particularly in fields where large, labeled datasets are not readily available."
}