{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Kernelized Memory Network for Video Object Segmentation}\n\n\\begin{document}\n\n\\pagestyle{headings}\n\\mainmatter\n\\def\\ECCVSubNumber{4152}  % Insert your submission number here\n\n\\title{Kernelized Memory Network for Video Object Segmentation} % Replace with your title\n\n\\begin{comment}\n\\titlerunning{ECCV-20 submission ID \\ECCVSubNumber} \n\\authorrunning{ECCV-20 submission ID \\ECCVSubNumber} \n\\author{Anonymous ECCV submission}\n\\institute{Paper ID \\ECCVSubNumber}\n\\end{comment}\n\n\\titlerunning{Kernelized Memory Network for Video Object Segmentation}\n\\author{Hongje Seong \\and %\\orcidID{0000-0001-7221-409X}\nJunhyuk Hyun \\and\nEuntai Kim\\thanks{Corresponding author.}} %\\orcidID{0000-0002-0975-8390}\n\\authorrunning{H. Seong et al.}\n\\institute{School of Electrical and Electronic Engineering, Yonsei University, Seoul, Korea \\\\\n\\email{\\{hjseong,jhhyun,etkim\\}@yonsei.ac.kr}\n}\n\\maketitle\n\n\\begin{abstract}\nSemi-supervised video object segmentation (VOS) is a task that involves predicting a target object in a video when the ground truth segmentation mask of the target object is given in the first frame. Recently, space-time memory networks (STM) have received significant attention as a promising solution for semi-supervised VOS. However, an important point is overlooked when applying STM to VOS. The solution (STM) is non-local, but the problem (VOS) is predominantly local. To solve the mismatch between STM and VOS, we propose a kernelized memory network (KMN). Before being trained on real videos, our KMN is pre-trained on static images, as in previous works. Unlike in previous works, we use the Hide-and-Seek strategy in pre-training to obtain the best possible results in handling occlusions and segment boundary extraction.\nThe proposed KMN surpasses the state-of-the-art on standard benchmarks by a significant margin (+5\\% on DAVIS 2017 test-dev set).\nIn addition, the runtime of KMN is 0.12 seconds per frame on the DAVIS 2016 validation set, and the KMN rarely requires extra computation, when compared with STM.\n\\keywords{Video object segmentation, Memory network, Gaussian kernel, Hide-and-Seek}\n\\end{abstract}\n\n\\section{Introduction}\n\\label{s1}\nVideo object segmentation (VOS) is a task that involves tracking target objects at the pixel level in a video. It is one of the most challenging problems in computer vision. VOS can be divided into two categories: semi-supervised VOS and unsupervised VOS. In semi-supervised VOS, the ground truth (GT) segmentation mask is provided in the first frame, and the segmentation mask must be predicted for the subsequent frames. In unsupervised VOS, however, no GT segmentation mask is provided, and the task is to find and segment the salient object in the video. In this paper, we consider semi-supervised VOS.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.7\\linewidth]{figures/visualize_GaussianKernel.pdf}\n\\caption{\nIllustration of KMN. In STM \\cite{Oh_2019_ICCV}, two cars in the query frame are matched with a car in the memory frame owing to the non-local matching between the query and memory. The car in the middle is the correct match, while the car on the left is an incorrect match. In KMN, however, non-local matching between the query and memory is controlled by the Gaussian kernel. Only the car in the middle of the query frame is matched with the car in the memory.\n}\n\\label{fig:visualize_GaussianKernel}\n\\end{figure}\n\nSpace-time memory networks (STM) \\cite{Oh_2019_ICCV} have recently received significant attention as a promising solution for semi-supervised VOS. The basic idea behind the application of STM to VOS is to use the intermediate frames between the first frame and the current frame. In STM, the current frame is considered to be the query frame for which the target is to be predicted, whereas the past (already predicted) frames are used as memory frames. This approach, however, overlooks an important point. The solution (STM) is non-local, but the problem (VOS) is predominantly local, as illustrated in Fig. \\ref{fig:visualize_GaussianKernel}. Specifically, STM is based on non-local matching between the query frame and memory frames. However, in VOS, the target object in the query frame usually appears in the local neighborhood of the targetâ€™s appearance in the memory frames. To solve the problem arising from the use of STM for VOS, we propose a kernelized memory network (KMN). In KMN, the Gaussian kernel is employed to reduce the degree of non-localization of the STM and improve the effectiveness of the memory network for VOS.\n\nBefore being trained on real videos, our KMN is pre-trained on static images, as in some previous works. In particular, multiple frames based on a random affine transform were used in \\cite{wug2018fast,Oh_2019_ICCV}. Unlike the training process in the previous works, however, we employ a Hide-and-Seek strategy during pre-training to obtain the best possible results in handling occlusions and segment boundary extraction. The Hide-and-Seek strategy \\cite{singh2017hide} was initially developed for weakly supervised object localization, but we used it to pre-train the KMN. This provides two key benefits. First, Hide-and-Seek achieves segmentation results that are considerably robust to occlusion. To the best of our knowledge, this is the first time that Hide-and-Seek has been applied to VOS in order to make the predictions robust to occlusion. Second, Hide-and-Seek is used to refine the boundary of the object segment. Because most of the ground truths in segmentation datasets contain unclear and incorrect boundaries, it is fairly challenging to predict accurate boundaries in VOS. The boundaries created by Hide-and-Seek, however, are clear and accurate. Hide-and-seek appears to provide instructive supervision for clear and precise cuts for objects, as shown in Fig. \\ref{fig:hide-and-seek}. We conduct experiments on DAVIS 2016, DAVIS 2017, and Youtube-VOS 2018 and significantly outperform all previous methods, even compared with online-learning approaches.\n\nThe contributions of this paper can be summarized as follows. First, KMN is developed to reduce the non-locality of the STM and make the memory network more effective for VOS. Second, Hide-and-Seek is used to pre-train the KMN on static images.\n\n\\section{Related Work}\n\\label{s2}\n\n\\subsubsection{Semi-supervised video object segmentation}\\cite{perazzi2016benchmark,pont20172017,xu2018youtube} is a task involving prediction of the target objects in all frames of a video sequence where information of the target objects is provided in the first frame. Because the object mask for the first frame of the video is given at the test time, many previous studies \\cite{shin2017pixel,cheng2017segflow,perazzi2017learning,caelles2017one,hu2017maskrnn,ci2018video,bao2018cnn,luiten2018premvos,maninis2018video,voigtlaender2017online,li2018video,Wang_2019_ICCV} fine-tuned their networks on the given mask. This is known as the online-learning strategy. Online-learning methods can provide accurate prediction results, but require considerable time for inference and finding the best hyper-parameters of the model for each sequence. Offline-learning methods \\cite{marki2016bilateral,jampani2017video,yang2018efficient,chen2018blazingly,voigtlaender2019feelvos,wug2018fast,cheng2018fast,Zhang_2019_ICCV,Oh_2019_ICCV} use a fixed parameter set trained on the whole training sequence. Therefore, they can have a fast run time, while achieving comparable accuracy. Our proposed method follows the offline approach.\n\n\\subsubsection{Memory networks}\\cite{sukhbaatar2015end} use the query, \\textbf{key}, and \\textbf{value} (QKV) concept. The QKV concept is often used when the target information of the current input exists at the other inputs. In this case, memory networks set the current input and the other inputs as the query and memory, respectively. The \\textbf{key} and \\textbf{value} are extracted from memory, and the correlation map of the query and memory is generated through a non-local matching operation of the query and \\textbf{key} feature. Then, the weighted average \\textbf{value} based on the correlation map is retrieved. The QKV concept is widely used in a variety of tasks, including natural language processing \\cite{vaswani2017attention,miller2016key,kumar2016ask}, image processing \\cite{parmar2018image,Zhu_2019_ICCV}, and video recognition \\cite{wang2018non,girdhar2019video,seong2019video}. In VOS, STM \\cite{Oh_2019_ICCV} has achieved significant success by repurposing the concept of the QKV. However, applications in STM tend to overlook an important feature of VOS, leading to a limitation that will be addressed in this paper.\n\n\\subsubsection{Kernel soft argmax}\\cite{lee2019sfnet} uses Gaussian kernels on the correlation map to create a gradient propagable argmax function for semantic correspondence. The semantic correspondence task requires only a single matching flow from a source image to a target image for each given source point. However, applying a discrete argmax function on the correlation map makes the network untrainable. To solve this problem, kernel soft argmax applies Gaussian kernels on the correlation map and then averages the correlation scores. Our work is inspired by the kernel soft argmax, but its application and objective are completely different. The kernel soft argmax applies Gaussian kernels to the results of the searching flow (\\textit{i.e.}, memory frame) to serve as a gradient propagable argmax function, whereas we applied Gaussian kernels on the opposite side (\\textit{i.e.}, query frame) to solve the case as shown in Fig. \\ref{fig:visualize_GaussianKernel}.\n\n\\subsubsection{Hide-and-Seek}\\cite{singh2017hide} is a weakly supervised framework that has been proposed to improve object localization. Training object localization in a weakly supervised manner using intact images leads to poor localization by finding only the most salient parts of the objects. Hiding some random patches of the object during training helps to improve object localization by forcing the system to find relatively less salient parts. We have found that Hide-and-Seek can improve VOS which is a fully supervised learning task. As a result, we achieved comparable performance to the other offline-learning approaches, even when we trained only on the static images.\n\n\\subsubsection{Difficulties in segmentation near object boundaries.} \nAlthough there has been significant progress in image segmentation, accurate segmentation of the object boundary is still challenging. A low-level layer has been trained in EGNet \\cite{zhao2019egnet} using object boundaries to accurately predict object boundaries. The imbalance between boundary pixels and non-boundary pixels has been addressed in LDF \\cite{wei2020label} by separating them and training them separately. In this paper, we deal with the problem of GTs that are inaccurate near the object boundary. Hide-and-Seek addresses the problem by generating clean boundaries.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.99\\linewidth]{figures/overall_architecture.pdf}\n\\caption{\nOverall architecture of our kernelized memory network (KMN). We follow the frameworks of \\cite{Oh_2019_ICCV} and propose a new operation of kernelized memory read. The numbers next to the block indicate the spatial size and channel dimension, respectively.\n}\n\\label{fig:overall_architecture}\n\\end{figure}\n\n\\section{Kernelized Memory Network}\n\\label{s3}\n\n\\subsection{Architecture}\n\\label{s31}\nIn this section, we present a kernelized memory network (KMN). The overall architecture of KMN is fairly similar to that of STM \\cite{Oh_2019_ICCV}, as illustrated in Fig. \\ref{fig:overall_architecture}. As in STM \\cite{Oh_2019_ICCV}, the current frame is used as the query, while the past frames with the predicted masks are used as the memory. Two ResNet50 \\cite{b23} are employed to extract the \\textbf{key} and \\textbf{value} from the memory and query frames. In memory, the predicted (or given) mask input is concatenated with the RGB channels. Then, the \\textbf{key} and \\textbf{value} features of the memory and the query are embedded via a convolutional layer from the \\texttt{res4} feature \\cite{b23}, which has a $1/16$ resolution resolution with respect to the input image. The structures of the \\textbf{key} and \\textbf{value} embedding layers for the query and memory are the same, but the weights are not shared. The memory may take several frames, and all frames in the memory are independently embedded and then concatenated along the temporal dimension. In the query, because it takes a single frame, the embedded \\textbf{key} and \\textbf{value} are directly used for memory reading.\n\nThe correlation map between the query and memory is generated by applying the inner product to all possible combinations of \\textbf{key} features in the query and memory. From the correlation map, highly matched pixels are retrieved through a \\textit{kernelized memory read} operation, and the corresponding \\textbf{values} of the matched pixels in the memory are concatenated with the \\textbf{value} of the query. Subsequently, the concatenated value tensor is fed to a decoder consisting of a residual block \\cite{he2016identity} and two stacks of refinement modules. The refinement module is the same as that used in \\cite{Oh_2019_ICCV,wug2018fast}. We recommend that the readers refer to \\cite{Oh_2019_ICCV} for more details about the decoder.\n\nThe main innovation in KMN, distinct from STM \\cite{Oh_2019_ICCV}, lies in the memory read operation. In the memory read of STM \\cite{Oh_2019_ICCV}, only \\texttt{Query-to-Memory} matching is conducted. In the kernelized memory read of KMN. however, both \\texttt{Query-to-Memory} matching and \\texttt{Memory-to-Query} matching are conducted. A detailed explanation of the kernelized memory read is provided in the next subsection.\n\n\\subsection{Kernelized Memory Read}\n\\label{s32}\nIn the memory read operation of STM \\cite{Oh_2019_ICCV}, the non-local correlation map $c ( {\\bf{p}} , {\\bf{q}} )$ is generated using the embedded \\textbf{key} of the memory ${{\\bf{k}}^{M}} = \\left\\{ {{{k}}^{M}} ( {\\bf{p}} ) \\right\\} \\in {\\mathbb{R}^{T \\times H \\times W \\times C/8}}$ and query ${{\\bf{k}}^{Q}} = \\left\\{ {{{k}}^{Q}} ( {\\bf{q}} ) \\right\\} \\in {\\mathbb{R}^{H \\times W \\times C/8}}$ as follows:\n\\begin{equation}\nc \\left( {\\bf{p}} , {\\bf{q}} \\right) = {{{{k}}^{M}}({\\bf{p}})} {k^Q}({\\bf{q}})^\\top\n\\label{eq1}\n\\end{equation}\nwhere $H$, $W$, and $C$ are the height, width, and channel size of \\texttt{res4} \\cite{b23}, respectively. ${\\bf{p}} = \\left[ p_t, p_y, p_x \\right]$ and ${\\bf{q}} = \\left[ q_y , q_x \\right]$ indicate the grid cell positions of the \\textbf{key} features. Then, the query at position ${\\bf{q}}$ retrieves the corresponding \\textbf{value} from the memory using the correlation map by\n\\begin{equation}\nr\\left( {\\bf{q}} \\right) = \\sum\\limits_{\\bf{p}} {\\frac{{\\exp \\left( {c\\left( {{\\bf{p}},{\\bf{q}}} \\right)} \\right)}}{{\\sum\\limits_{\\bf{p}} {\\exp } \\left( {c\\left( {{\\bf{p}},{\\bf{q}}} \\right)} \\right)}}} {v^M}\\left( {\\bf{p}} \\right)\n\\label{eq2}\n\\end{equation}\nwhere ${{\\bf{v}}^{M}} = \\left\\{ {{{v}}^{M}} ( {\\bf{p}} ) \\right\\} \\in {\\mathbb{R}^{T \\times H \\times W \\times C/2}}$ is the embedded \\textbf{value} of the memory. Then the retrieved \\textbf{value} ${r}({\\bf{q}})$, which is of size ${H \\times W \\times C/2}$, is concatenated with the query \\textbf{value} ${{\\bf{v}}^{Q}}\\in {\\mathbb{R}^{H \\times W \\times C/2}}$, and the concatenation result is fed to the decoder.\n\nThe memory read operation of STM \\cite{Oh_2019_ICCV} has two inherent problems. First, every grid in the query frame searches the memory frames for a target object, but not vice versa. That is, there is only \\texttt{Query-to-Memory} matching in the STM. Thus, when multiple objects in the query frame look like a target object, all of them can be matched with the same target object in the memory frames. Second, the non-local matching in the STM can be ineffective in VOS, because it overlooks the fact that the target object in the query should appear where it previously was in the memory frames.\n\nTo solve these problems, we propose a kernelized memory read operation using 2D Gaussian kernels. First, the non-local correlation map $c \\left( {\\bf{p}} , {\\bf{q}} \\right) = {{{{k}}^{M}}({\\bf{p}})} {k^Q}({\\bf{q}})^\\top$ between the query and memory is computed as in STM. Second, for each grid $\\bf{p}$ in the memory frames, the best-matched query position ${\\widehat{\\bf{q}}}\\left({\\bf{p}}\\right) = \\left[ {{{\\widehat q}_y}}\\left({\\bf{p}}\\right), {{{\\widehat q}_x}}\\left({\\bf{p}}\\right) \\right]$ is searched by\n\\begin{equation}\n\\widehat {\\bf{q}}\\left( {\\bf{p}} \\right) = \\mathop {\\arg\\max }\\limits_{\\bf{q}} c\\left( {{\\bf{p}},{\\bf{q}}} \\right) .\n\\label{eq3}\n\\end{equation}\nThis is a \\texttt{Memory-to-Query} matching. Third, a 2D Gaussian kernel ${\\bf{g}} = \\left\\{g\\left({\\bf{p}} , {\\bf{q}}\\right)\\right\\} \\in {\\mathbb{R}^{T \\times H \\times W \\times H \\times W}}$ centered on $\\widehat {\\bf{q}}\\left( {\\bf{p}} \\right) $ is computed by\n\\begin{equation}\ng\\left( {{\\bf{p}},{\\bf{q}}} \\right) = \\exp \\left( { - \\frac{{{{\\left( {{q_y} - {{\\widehat q}_y}\\left( {\\bf{p}} \\right)} \\right)}^2} + {{\\left( {{q_x} - {{\\widehat q}_x}\\left( {\\bf{p}} \\right)} \\right)}^2}}}{{2{\\sigma ^2}}}} \\right)\n\\label{eq4}\n\\end{equation}\nwhere $\\sigma$ is the standard deviation. Using Gaussian kernels, the \\textbf{value} in the memory is retrieved in a local manner as follows:\n\\begin{equation}\n{r^k}\\left( {\\bf{q}} \\right) = \\sum\\limits_{\\bf{p}} {\\frac{{\\exp \\left( {c\\left( {{\\bf{p}},{\\bf{q}}} \\right)/\\sqrt d } \\right)g\\left( {{\\bf{p}},{\\bf{q}}} \\right)}}{{\\sum\\limits_{\\bf{p}} {\\exp \\left( {c\\left( {{\\bf{p}},{\\bf{q}}} \\right)/\\sqrt d } \\right)g\\left( {{\\bf{p}},{\\bf{q}}} \\right)} }}} {v^M}\\left( {\\bf{p}} \\right)\n\\label{eq5}\n\\end{equation}\nwhere $d$ is the channel size of the \\textbf{key}. This is a \\texttt{Query-to-Memory} matching. Here, $\\frac{1}{\\sqrt{d}}$ is a scaling factor adopted from \\cite{vaswani2017attention}, to prevent the argument in the softmax from becoming large in magnitude, or equivalently, to prevent the softmax from becoming saturated. The kernelized memory read operation is summarized in Fig. \\ref{fig:kernelized_memory_read}.\n\n\\begin{figure}[t]\n\\centering\n\\begin{minipage}[]{0.5\\linewidth}\n\\includegraphics[width=\\linewidth]{figures/kernelized_memory_read.pdf}\n\\caption{Kernelized memory read operation.}\n\\label{fig:kernelized_memory_read}\n\\end{minipage}%\n    \\hfill%\n\\begin{minipage}[]{0.45\\linewidth}\n\\includegraphics[width=\\linewidth]{figures/hide-and-seek.pdf}\n\\caption{A pair of images generated during pre-training using Hide-and-Seek. The mask indicated in red denotes the ground truth of the target object.}\n\\label{fig:hide-and-seek}\n\\end{minipage} \n\\end{figure}\n\n\\section{Pre-training by Hide-and-Seek}\n\\label{s33}\nAs in previous studies \\cite{perazzi2017learning,wug2018fast,Oh_2019_ICCV}, our KMN is pre-trained using static image datasets that include foreground object masks \\cite{b42,b43,hariharan2011semantic,shi2015hierarchical,cheng2014global,wang2017salient}. The basic idea of pre-training a VOS network is to synthetically generate a video with foreground object masks from a single static image. Applying random affine transforms to a static image and the corresponding object mask can yield a synthetic video, and the video can be used to pre-train a VOS network. The problem with synthetic generation of a video from a static image, however, is that the occlusion of the target object does not occur in a generated video. Thus, the simulated video cannot train the pre-trained KMN to cope with the common problem of occlusion in VOS. To solve this problem, the Hide-and-Seek strategy is used to synthetically generate a video with occlusions. Some patches are randomly hidden or blocked, and the occlusions are synthetically generated in the training samples. Here, we only consider squared occluders, but any shape can be taken. Hide-and-Seek can pre-train KMN to be robust to occlusion in the VOS. This idea is illustrated in Fig. \\ref{fig:hide-and-seek}.\n\nFurther, it should be noted that most segmentation datasets contain inaccurate masks (GTs) near the object boundaries. Pre-training KMN with accurate masks is of great importance for high-performance VOS, because inaccurate masks can lead to performance degradation. Manual correction of incorrect masks would be helpful, but it would require a tremendous amount of labor. Another benefit obtained by the use of Hide-and-Seek in pre-training KMN is that the boundaries of the object segment become cleaner and more accurate than before. An example is illustrated in Fig. \\ref{fig:hide-and-seek}. In this figure, the ground truth mask contains incorrect boundaries on the head of the running person. However, Hide-and-Seek creates a clear object boundary, as represented by the pink line in Fig. \\ref{fig:hide-and-seek}. A detailed experimental analysis is given in Section \\ref{s46}.\n\nThe use of Hide-and-Seek in the pre-training on simulated videos significantly improves the VOS pre-training performance; the results are given in Table \\ref{tab_davis16}. The pre-training performance obtained by Hide-and-Seek is much higher than that of the previous methods \\cite{wug2018fast,Oh_2019_ICCV}, and the performance is even as high as the full-training performance of some previous methods.\n\n\\section{Experiments}\n\\label{s4}\nIn this section, we describe the implementation details of the method, our experimental results on DAVIS 2016, DAVIS 2017, and Youtube-VOS 2018, and the analysis of our proposed methods.\n\n\\subsection{Training Details}\n\\label{s41}\nWe divide the training stage into two phases: one for pre-training on the static images and another for the main training on VOS datasets composed of video sequences.\n\nDuring the pre-training, we generated three frames using a single static image by randomly applying rotation, flip, color jittering, and cropping, similar to \\cite{wug2018fast,Oh_2019_ICCV}. We then used the Hide-and-Seek framework, as described in Section \\ref{s33}. We first divided the image into a $24 \\times 24$ grid, which has the same spatial size as the \\textbf{key} feature. Each cell in the grid had a uniform probability to be hidden. We gradually increased the probability from 0 to 0.5.\n\nDuring the main training, we followed the STM training strategy \\cite{Oh_2019_ICCV}. We sampled the three frames from a single video. They were sampled in time-order with intervals randomly selected in the range of the maximum interval. In the training process, the maximum interval is gradually increased from 0 to 25.\n\nFor both training phases, we used the dynamic memory strategy \\cite{Oh_2019_ICCV}. To deal with multi-object segmentation, a soft aggregation operation \\cite{Oh_2019_ICCV} was used. Note that the Gaussian kernel was not applied during training. Because the argmax function, which determines the center point of the Gaussian kernel, is a discrete function, the error of the argmax cannot be propagated backward during training. Thus, if the Gaussian kernel is used during training, it attempts to optimize networks based on the incorrectly selected \\textbf{key} feature by argmax, which leads to performance degradation.\n\nOther training details are as follows: randomly resize and crop the images to the size of $384 \\times 384$, use the mini-batch size of 4, minimize the cross-entropy loss for every pixel-level prediction, and opt for Adam optimizer \\cite{KingmaB14} with a fixed learning rate of 1e-5.\n\n\\begin{table}\n\\caption{Comparisons on the DAVIS 2016 and DAVIS 2017 validation set where ground truths are available. `OL' indicates the use of online-learning strategy. The best results are \\textbf{bold-faced}, and the second best results are \\underline{underlined}.\n}\n\\label{tab_davis16}\n\\centering\n\\begin{tabular}{clc|cccc|ccc}\n\\toprule\n                               &                                        &            & \\multicolumn{4}{c|}{DAVIS 2016 val}                                & \\multicolumn{3}{c}{DAVIS 2017 val}                     \\\\\n\\multicolumn{1}{c}{Training Data} & \\multicolumn{1}{c}{Methods}            & \\; OL \\;        & Time     & $\\mathcal{G_M}$ & $\\mathcal{J_M}$ & $\\mathcal{F_M}$ & $\\mathcal{G_M}$ & $\\mathcal{J_M}$ & $\\mathcal{F_M}$ \\\\\n\\midrule\n\n\\multirow{3}{*}{Static Images} & RGMP  \\cite{wug2018fast}               &            & 0.13$s$  & 57.1            & 55.0            & 59.1            & -               & -               & -               \\\\\n                               & STM \\cite{Oh_2019_ICCV}                &            & 0.16$s$  & -               & -               & -               & 60.0            & 57.9            & 62.1            \\\\\n                               & KMN (ours)                             &            & 0.12$s$  & \\textbf{74.8}   & \\textbf{74.7}   & \\textbf{74.8}   & \\textbf{68.9} & \\textbf{67.1} & \\textbf{70.8}               \\\\\n\\midrule\n\\multirow{30}{*}{DAVIS}       & BVS \\cite{marki2016bilateral}          &            & 0.37$s$  & 59.4            & 60.0            & 58.8            & -               & -               & -               \\\\\n                               & OSMN \\cite{yang2018efficient}          &            & -        & -               & -               & -               & 54.8            & 52.5            & 57.1            \\\\\n                               & OFL \\cite{tsai2016video}               &            & 120$s$   & 65.7            & 68.0            & 63.4            & -               & -               & -               \\\\\n                               & PLM \\cite{shin2017pixel}               & \\checkmark & 0.3$s$   & 66.0            & 70.0            & 62.0            & -               & -               & -               \\\\\n                               & VPN \\cite{jampani2017video}            &            & 0.63$s$  & 67.9            & 70.2            & 65.5            & -               & -               & -               \\\\\n                               & OSMN \\cite{yang2018efficient}          &            & 0.14$s$  & 73.5            & 74.0            & 72.9            & -               & -               & -               \\\\\n                               & SFL \\cite{cheng2017segflow}            & \\checkmark & 7.9$s$   & 74.7            & 74.8            & 74.5            & -               & -               & -               \\\\\n                               & PML \\cite{chen2018blazingly}           &            & 0.27$s$  & 77.4            & 75.5            & 79.3            & -               & -               & -               \\\\\n                               & MSK \\cite{perazzi2017learning}         & \\checkmark & 12$s$    & 77.6            & 79.7            & 75.4            & -               & -               & -               \\\\\n                               & OSVOS \\cite{caelles2017one}            & \\checkmark & 9$s$     & 80.2            & 79.8            & 80.6            & 60.3            & 56.6            & 63.9            \\\\\n                               & MaskRNN \\cite{hu2017maskrnn}           & \\checkmark & -        & 80.8            & 80.7            & 80.9            & -               & 60.5            & -               \\\\\n                               & VidMatch \\cite{hu2018videomatch}       &            & 0.32$s$  & -               & 81.0            & -               & 62.4            & 56.5            & 68.2            \\\\\n                               & FAVOS \\cite{cheng2018fast}             &            & 1.8$s$   & 81.0            & 82.4            & 79.5            & 58.2            & 54.6            & 61.8            \\\\\n                               & LSE \\cite{ci2018video}                 & \\checkmark & -        & 81.6            & 82.9            & 80.3            & -               & -               & -               \\\\\n                               & FEELVOS \\cite{voigtlaender2019feelvos} &            & 0.45$s$  & 81.7            & 80.3            & 83.1            & 69.1            & 65.9            & 72.3            \\\\\n                               & RGMP \\cite{wug2018fast}                &            & 0.13$s$  & 81.8            & 81.5            & 82.0            & 66.7            & 64.8            & 68.6            \\\\\n                               & DTN \\cite{Zhang_2019_ICCV}             &            & 0.07$s$  & 83.6            & 83.7            & 83.5            & -               & -               & -               \\\\\n                               & CINN \\cite{bao2018cnn}                 & \\checkmark & $>$30$s$ & 84.2            & 83.4            & 85.0            & 70.7            & 67.2            & 74.2            \\\\\n                               & DyeNet \\cite{li2018video}              &            & 0.42$s$  & -               & 84.7            & -               & 69.1            & 67.3            & 71.0            \\\\\n                               & RaNet \\cite{Wang_2019_ICCV}            &            & 0.03$s$  & 85.5            & 85.5            & 85.4            & 65.7            & 63.2            & 68.2            \\\\\n                               & AGSS-VOS \\cite{Lin_2019_ICCV}          &            & -        & -               & -               & -               & 66.6            & 63.4            & 69.8            \\\\\n                               & DTN \\cite{Zhang_2019_ICCV}             &            & -        & -               & -               & -               & 67.4            & 64.2            & 70.6            \\\\\n                               & OnAVOS \\cite{voigtlaender2017online}   & \\checkmark & 13$s$    & 85.5            & 86.1            & 84.9            & 67.9            & 64.5            & 71.2            \\\\\n                               & OSVOS$^S$ \\cite{maninis2018video}      & \\checkmark & 4.5$s$   & 86.0            & 85.6            & 86.4            & 68.0            & 64.7            & 71.3            \\\\\n                               & DMM-Net \\cite{Zeng_2019_ICCV}          &            & -        & -               & -               & -               & 70.7            & 68.1            & 73.3            \\\\\n                               & STM \\cite{Oh_2019_ICCV}                &            & 0.16$s$  & 86.5            & 84.8            & \\underline{88.1}            & 71.6            & 69.2            & 74.0            \\\\\n                               & PReMVOS \\cite{luiten2018premvos}       & \\checkmark & 32.8$s$  & 86.8            & 84.9            & \\textbf{88.6}   & \\textbf{77.8}   & \\underline{73.9}            & \\textbf{81.7}   \\\\\n                               & DyeNet \\cite{li2018video}              & \\checkmark & 2.32$s$  & -               & 86.2            & -               & -               & -               & -               \\\\\n                               & RaNet \\cite{Wang_2019_ICCV}            & \\checkmark & 4$s$     & \\underline{87.1}            & \\underline{86.6}            & 87.6            & -               & -               & -               \\\\\n                               & KMN (ours)                             &            & 0.12$s$  & \\textbf{87.6}   & \\textbf{87.1}   & \\underline{88.1}            & \\underline{76.0}            & \\textbf{74.2}   & \\underline{77.8}            \\\\\n\\midrule\n\\multirow{6}{*}{+Youtube-VOS}  & S2S \\cite{xu2018youtube}               & \\checkmark & 9$s$     & -               & 79.1            & -               & -               & -               & -               \\\\\n                               & AGSS-VOS \\cite{Lin_2019_ICCV}          &            & -        & -               & -               & -               & 67.4            & 64.9            & 69.9            \\\\\n                               & A-GAME \\cite{johnander2019generative}  &            & 0.07$s$  & -               & 82.0            & -               & 70.0            & 67.2            & 72.7            \\\\\n                               & FEELVOS \\cite{voigtlaender2019feelvos} &            & 0.45$s$  & 81.7            & 81.1            & 82.2            & 72.0            & 69.1            & 74.0            \\\\\n                               & STM \\cite{Oh_2019_ICCV}                &            & 0.16$s$  & \\underline{89.3}            & \\underline{88.7}            & \\underline{89.9}            & \\underline{81.8}            & \\underline{79.2}            & \\underline{84.3}            \\\\\n                               & KMN (ours)                             &            & 0.12$s$  & \\textbf{90.5}   & \\textbf{89.5}   & \\textbf{91.5}   & \\textbf{82.8}   & \\textbf{80.0}   & \\textbf{85.6}  \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\subsection{Inference Details}\n\\label{s42}\nOur network utilizes intermediate frames to obtain rich information about the target objects. For the inputs of the memory, intermediate frames use the softmax output of the network directly, while the first frame uses the given ground truth mask. Even though we predict all the frames in a sequence, using all the past frames as memory is not only computationally inefficient but also requires considerable GPU memory. Therefore, we follow the memory management strategy described in \\cite{Oh_2019_ICCV}. Both the first and previous frames are always used. The other intermediate frames are selected at five-frame intervals. Remainders are dropped.\n\nWe empirically set the fixed standard deviation $\\sigma$ of the Gaussian kernel in (\\ref{eq4}) to 7. We did not utilize any test time augmentation (\\textit{e.g.}, multi-crop testing) or post-processing (\\textit{e.g.}, CRF) and used the original image without any pre-processing (\\textit{e.g.}, optical flow).\n\n\\begin{table}[t]\n\\caption{Comparisons on the DAVIS 2017 test-dev and Youtube-VOS 2018 validation sets where ground truths are unavailable. `OL' indicates the use of online-learning strategy. The best results are \\textbf{bold-faced}, and the second best results are \\underline{underlined}.\n}\n\\label{tab_yv2018}\n\\centering\n\\begin{tabular}{lc|ccc|ccccc}\n\\toprule\n\\multicolumn{1}{c}{}                   &            & \\multicolumn{3}{c|}{DAVIS17 test-dev}                   & \\multicolumn{5}{c}{Youtube-VOS 2018 val}                                                        \\\\\n\\multicolumn{1}{c}{Methods}            & \\; OL \\;   & $\\mathcal{G_M}$ & $\\mathcal{J_M}$ & $\\mathcal{F_M}$  & Overall          & $\\mathcal{J_S}$  & $\\mathcal{J_U}$  & $\\mathcal{F_S}$  & $\\mathcal{F_U}$  \\\\\n\\midrule\nOSMN \\cite{yang2018efficient}          &            & 39.3             & 33.7             & 44.9             & 51.2             & 60.0             & 40.6             & 60.1             & 44.0             \\\\\nFAVOS \\cite{cheng2018fast}             &            & 43.6             & 42.9             & 44.2             & -                & -                & -                & -                & -                \\\\\nDMM-Net+ \\cite{Zeng_2019_ICCV}         &            & -                & -                & -                & 51.7             & 58.3             & 41.6             & 60.7             & 46.3             \\\\\nMSK \\cite{perazzi2017learning}         & \\checkmark & -                & -                & -                & 53.1             & 59.9             & 45.0             & 59.5             & 47.9             \\\\\nOSVOS \\cite{caelles2017one}            & \\checkmark & 50.9             & 47.0             & 54.8             & 58.8             & 59.8             & 54.2             & 60.5             & 60.7             \\\\\nCapsuleVOS \\cite{Duarte_2019_ICCV}     &            & 51.3             & 47.4             & 55.2             & 62.3             & 67.3             & 53.7             & 68.1             & 59.9             \\\\\nOnAVOS \\cite{voigtlaender2017online}   & \\checkmark & 52.8             & 49.9             & 55.7             & 55.2             & 60.1             & 46.6             & 62.7             & 51.4             \\\\\nRGMP \\cite{wug2018fast}                &            & 52.9             & 51.3             & 54.4             & 53.8             & 59.5             & 45.2             & -                & -                \\\\\nRaNet \\cite{Wang_2019_ICCV}            &            & 53.4             & 55.3             & 57.2             & -                & -                & -                & -                & -                \\\\\nOSVOS$^S$ \\cite{maninis2018video}      & \\checkmark & 57.5             & 52.9             & 62.1             & -                & -                & -                & -                & -                \\\\\nFEELVOS \\cite{voigtlaender2019feelvos} &            & 57.8             & 55.1             & 60.4             & -                & -                & -                & -                & -                \\\\\nRVOS \\cite{ventura2019rvos}            &            & -                & -                & -                & 56.8             & 63.6             & 45.5             & 67.2             & 51.0             \\\\\nDMM-Net+ \\cite{Zeng_2019_ICCV}         & \\checkmark & -                & -                & -                & 58.0             & 60.3             & 50.6             & 53.5             & 57.4             \\\\\nS2S \\cite{xu2018youtube}               & \\checkmark & -                & -                & -                & 64.4             & 71.0             & 55.5             & 70.0             & 61.2             \\\\\nA-GAME \\cite{johnander2019generative}  &            & -                & -                & -                & 66.1             & 67.8             & 60.8             & -                & -                \\\\\nAGSS-VOS \\cite{Lin_2019_ICCV}          &            & -                & -                & -                & 71.3             & 71.3             & 65.5             & 75.2             & 73.1             \\\\\nLucid \\cite{khoreva2019lucid}          & \\checkmark & 66.7             & 63.4             & 69.9             & -                & -                & -                & -                & -                \\\\\nCINN \\cite{bao2018cnn}                 & \\checkmark & 67.5             & 64.5             & 70.5             & -                & -                & -                & -                & -                \\\\\nDyeNet \\cite{li2018video}              & \\checkmark & 68.2             & 65.8             & 70.5             & -                & -                & -                & -                & -                \\\\\nPReMVOS \\cite{luiten2018premvos}       & \\checkmark & 71.6             & 67.5             & \\underline{75.7} & -                & -                & -                & -                & -                \\\\\nSTM \\cite{Oh_2019_ICCV}                &            & \\underline{72.2} & \\underline{69.3} & 75.2             & \\underline{79.4} & \\underline{79.7} & \\underline{72.8} & \\underline{84.2} & \\underline{80.9} \\\\\nKMN (ours)                             &            & \\textbf{77.2}    & \\textbf{74.1}    & \\textbf{80.3}    & \\textbf{81.4}    & \\textbf{81.4}    & \\textbf{75.3}    & \\textbf{85.6}    & \\textbf{83.3}   \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\subsection{DAVIS 2016 and 2017}\n\\label{s43}\nDAVIS 2016 \\cite{perazzi2016benchmark} is an object-level annotated dataset that contains 20 video sequences with a single target per video for validation. DAVIS 2017 \\cite{pont20172017} is an instance-level annotated dataset that contains 30 video sequences with multiple targets per video for validation. Both DAVIS validation sets are most commonly used in VOS to validate proposed methods. We measure the official metrics: the mean of the region similarity $\\mathcal{J_M}$, the contour accuracy $\\mathcal{F_M}$, and their average value $\\mathcal{G_M}$. We used a single parameter set that was trained on the DAVIS 2017 training dataset, which contains 60 video sequences, to evaluate our model on DAVIS 2016 and DAVIS 2017 for a fair comparison with previous works \\cite{wug2018fast,yang2018efficient,Oh_2019_ICCV}. The experimental results on the DAVIS 2016 and 2017 validation sets are given in Table \\ref{tab_davis16}. We report three different results for each training data.\n\nThe results of the training with only static images show a significant margin of improvement from previous studies. In addition, the performances of our proposed network trained on the static images show results comparable to those of the other approaches trained on DAVIS. This indicates that our Hide-and-Seek pre-training approach uses the static images effectively for VOS in training. STM \\cite{Oh_2019_ICCV} trained on DAVIS showed weak performance compared with the online-learning methods. However, our approach achieves almost similar or even higher performance than the online-learning methods, along with a fast runtime. Finally, the results trained on an additional training dataset, Youtube-VOS, showed the best performance among all existing VOS approaches. Because the ground truths of the DAVIS validation set are accessible to every user, tuning on the dataset is relatively easy. Therefore, to show that a method actually works well in general, we evaluate our approaches on the DAVIS 2017 test-dev benchmark, where ground truths are unavailable, with results shown in Table \\ref{tab_yv2018}. In DAVIS 2017 test-dev experiments, for a fair comparison, we resize the input frame to be 600p as in STM \\cite{Oh_2019_ICCV}. We find that our approach surpasses the state-of-the-art method by a significant margin (+5\\% $\\mathcal{G_M}$ score).\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.99\\textwidth]{figures/qualitative_results.pdf}\n\\caption{\nQualitative results and comparisons on the DAVIS 2017 validation set. Our results also do not utilize additional training set, Youtube-VOS.\n}\n\\label{fig:qualitative_results}\n\\end{figure}\n\n\\subsection{Youtube-VOS 2018}\n\\label{s44}\nYoutube-VOS 2018 \\cite{xu2018youtube} is the largest video object segmentation dataset. It contains 4,453 video sequences with multiple targets per video. To validate on Youtube-VOS 2018, both metrics $\\mathcal{J}$ and $\\mathcal{F}$ were calculated separately, depending on whether the object categories are seen or not during training: seen sequences with the number of 65 for $\\mathcal{J_S}$, $\\mathcal{F_S}$, and unseen sequences with the number of 26 for $\\mathcal{J_U}$, $\\mathcal{F_U}$. The ground truths of the Youtube-VOS 2018 validation set are unavailable as the DAVIS 2017 test-dev benchmark. As shown in Table \\ref{tab_yv2018}, our approach achieved state-of-the-art performance. This indicates that our approach works well in all cases.\n\n\\begin{table}\n\\begin{threeparttable}\n\\caption{\nAblation study of our proposed methods. `HaS' and `KM' indicate the use of Hide-and-Seek pre-training and kernelized memory read operation, respectively. Note that we did not use additional VOS training data for the ablation study. Only either DAVIS or Youtube-VOS is used, depending on the target evaluation benchmark.\n}\n\\label{tab_ablation}\n\\centering\n\\begin{tabular}{l|cc|cccc|ccc|ccccc}\n\\toprule\n                      &            &            & \\multicolumn{4}{c|}{DAVIS16}                                  & \\multicolumn{3}{c|}{DAVIS17}                        & \\multicolumn{5}{c}{Youtube-VOS 2018}                                                  \\\\\n                       & HaS        & KM         & Time\\tnote{$\\star$}    & $\\mathcal{G_M}$ & $\\mathcal{J_M}$ & $\\mathcal{F_M}$ & $\\mathcal{G_M}$ & $\\mathcal{J_M}$ & $\\mathcal{F_M}$ & Overall       & $\\mathcal{J_S}$ & $\\mathcal{J_U}$ & $\\mathcal{F_S}$ & $\\mathcal{F_U}$ \\\\\n\\midrule\nSTM\\cite{Oh_2019_ICCV} &            &            & 0.11$s$ & 86.5            & 84.8            & \\textbf{88.1}   & 71.6            & 69.2            & 74.0            & 79.4          & 79.7            & 72.8            & 84.2            & 80.9            \\\\\n\\midrule\n\\multirow{4}{*}{Ours}  &            &            & 0.11$s$ & 81.3            & 80.0            & 82.6            & 72.6            & 70.1            & 75.0            & 79.0 & 79.2 & 73.5 & 83.1 & 80.3            \\\\\n                       & \\checkmark &            & 0.11$s$ & 87.1            & 86.3            & 88.0            & 75.9            & 73.7            & \\textbf{78.1}   & 79.5          & 80.0            & 73.1            & 83.9            & 81.0            \\\\\n                       &            & \\checkmark & 0.12$s$ & 87.2            & 86.6            & 87.7            & 73.5            & 71.2            & 75.7            & 81.0          & 81.0            & \\textbf{75.4}   & 85.0            & 82.5            \\\\\n                       & \\checkmark & \\checkmark & 0.12$s$ & \\textbf{87.6}   & \\textbf{87.1}   & \\textbf{88.1}   & \\textbf{76.0}   & \\textbf{74.2}   & 77.8            & \\textbf{81.4} & \\textbf{81.4}   & 75.3            & \\textbf{85.6}   & \\textbf{83.3}  \\\\\n\\bottomrule\n\\end{tabular}\n{\\small\n\\begin{tablenotes}\n\\item[$\\star$] measured on our 1080Ti GPU system\n\\end{tablenotes}\n}\n\\end{threeparttable}\n\\end{table}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.99\\textwidth]{figures/qualitative_results_STM.pdf}\n\\caption{\nQualitative results and comparisons with STM \\cite{Oh_2019_ICCV}. The noticeable improvements are marked with yellow boxes. For DAVIS results, Youtube-VOS is additionally used for training. Note that the ground truths of the Youtube-VOS validation set are not available.\n}\n\\label{fig:qualitative_results_STM}\n\\end{figure}\n\n\\subsection{Qualitative Results}\n\\label{s45}\nA qualitative comparison is shown in Fig. \\ref{fig:qualitative_results}. We compare our method with the state-of-the-art methods officially released on DAVIS\\footnote{\\url{https://davischallenge.org/davis2017/soa\\_compare.html}}. The other methods in the figure do not utilize any additional VOS training data. Therefore, we show the KMN results which trained only on DAVIS in the main training stage for a fair comparison. Our results show consistently accurate predictions compared to other methods, even in cases of fast deformation (dance-twirl), the appearance of other objects, which are regarded as a background similar to the target object (car-roundabout), and the severe occlusion of the target objects (India).\n\n\\subsection{Analysis}\n\\label{s46}\n\\subsubsection{Ablation study.}\nWe conducted an ablation study to demonstrate the effectiveness of our approaches, and the experimental results are presented in Table \\ref{tab_ablation}. As shown in the table, our approaches lead to performance improvements. The runtimes were measured on our 1080Ti GPU system, which is the same as that used in \\cite{Oh_2019_ICCV}.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.99\\textwidth]{figures/loss_visualization.pdf}\n\\caption{\nPixel-level cross-entropy loss visualization during the pre-training on static images. `$S$' indicates the gird size of the Hide-and-Seek. Even if the network finds the object accurately, pixel-level losses occur near the mask boundary, because the ground truth masks near the boundary are not accurate. This makes it difficult for the network to learn the boundary correctly. Since  Hide-and-Seek can cut the object cleanly, it gives a more accurate ground truth mask near the boundary. Therefore, we can observe that the losses are not activated on the boundaries made by Hide-and-Seek.\n}\n\\label{fig:loss_visualization}\n\\end{figure}\n\n\\subsubsection{Qualitative comparison with STM.}\nWe conducted a qualitative comparison with STM \\cite{Oh_2019_ICCV}, and the results are shown in Fig. \\ref{fig:qualitative_results_STM}. To show the actual improvements from STM, we obtained STM results using the author's officially released source code\\footnote{\\url{https://github.com/seoungwugoh/STM}}. However, since the parameters for Youtube-VOS validation are not available, our parameters shown in Table \\ref{tab_ablation} were used for Youtube-VOS. For DAVIS, additional data, the Youtube-VOS set, was used for training. As shown in Fig. \\ref{fig:qualitative_results_STM}, our results are robust and accurate even in difficult cases where \\textit{multiple similar objects appear in the query} and \\textit{occlusion occurs}.\n\n\\subsubsection{Boundary quality made by Hide-and-Seek.}\nTo verify that Hide-and-Seek modified the ground truth boundary accurately, we visualized the prediction loss for each pixel in Fig. \\ref{fig:loss_visualization}. For a fair comparison, a single model trained on static images was used. As shown in the figure, \\textit{most of the losses occur near the boundary}, even when the network predicts quite accurately. This indicates that the networks struggle to learn the mask boundary because the ground truth mask has an irregular and noisy boundary. However, \\textit{the boundary of the hidden patch is not activated} in the figure. This means that the network can learn the mask boundary modified by Hide-and-Seek. Thus, Hide-and-Seek can provide more precise boundaries, and we expect that our new perspective would provide an opportunity to improve not only the quality of the segmentation masks, but also system performance for various segmentation tasks in the computer vision field.\n\n\\section{Conclusion}\n\\label{s5}\nIn this work, we present a new memory read operation and a method for handling occlusion and obtaining an accurate boundary using a static image. Our proposed methods were evaluated on the DAVIS 2016, DAVIS 2017, and Youtube-VOS benchmarks. We achieved state-of-the-art performance, even including online-learning methods. The ablation study shows the efficacy of our kernel approach, which addresses the main problem of memory networks in VOS. New approaches using the Hide-and-Seek strategy also show its effectiveness for VOS. Since our approaches can be easily reproduced and lead to significant improvements, we believe that our ideas have the potential to improve not only VOS, but also other segmentation-related fields.\n\n\\subsection*{Acknowledgement.}\nThis research was supported by Next-Generation Information Computing Development Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Science, ICT (NRF-2017M3C4A7069370).\n\n\\clearpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Supplementary Material:\\\\\nHierarchical Memory Matching Network for Video Object Segmentation}\n\n\\begin{document}\n\n\\title{Supplementary Material:\\\\\nHierarchical Memory Matching Network for Video Object Segmentation}\n\n\\author{Hongje Seong\\textsuperscript{1} \\quad\\quad Seoung Wug Oh\\textsuperscript{2} \\quad\\quad Joon-Young Lee\\textsuperscript{2} \\\\ Seongwon Lee\\textsuperscript{1} \\quad\\quad Suhyeon Lee\\textsuperscript{1} \\quad\\quad Euntai Kim\\textsuperscript{1,}\\thanks{Corresponding author.}\\vspace*{0.2cm}\\\\\n{\\textsuperscript{1}Yonsei University \\quad\\quad\\quad \\textsuperscript{2}Adobe Research}}\n\n\\maketitle\n\\ificcvfinal\\thispagestyle{empty}\\fi\n\n\\vspace{-5mm}\n\\section{Network Structure Details}\n\\label{sec:1.Network_Structure_Details}\n\\paragraph{Top-$k$ guided memory matching module at \\texttt{res2} stage.}~Fig.~\\ref{fig:topk_memory_read_detail_supp} shows a detailed implementation of the top-$k$ guided memory matching module at the \\texttt{res2} stage.\nCompare to the top-$k$ guided memory matching module at the \\texttt{res3} stage, we reduce the number of $k$ to $k/4$.\nWe also take the reduced channel dimensions of \\textbf{key} and \\textbf{value}, except for the query \\textbf{value}.\n\n\\paragraph{Detailed implementation of decoder.}~We follow the decoder architecture of STM \\cite{Oh_2019_ICCV}, and a detailed implementation is provided in Fig.~\\ref{fig:decoder}.\nNote that, in the refinement modules of STM \\cite{Oh_2019_ICCV}, the skip-connected features ($\\textbf{Z}_3$, $\\textbf{Z}_2$) are encoded via convolutional layers before fed to residual block.\nWe replace the convolutional layers with \\textbf{value} embedding layers in top-$k$ guided memory matching modules.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1.\\linewidth]{figures/topk_memory_read_detail_supp.pdf}\n\\caption{\nA detailed implementation of the top-$k$ guided memory matching module at the \\texttt{res2} stage.\nMemory and query dimensions are indicated using \\textcolor{blue}{blue} and \\textcolor{red}{red}.\n}\n\\label{fig:topk_memory_read_detail_supp}\n\\end{figure}\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=0.8\\linewidth]{figures/decoder.pdf}\n\\caption{\nA detailed implementation of decoder.\nWe notated the output scale and channel dimension next to each block in the decoder.\n}\n\\label{fig:decoder}\n\\end{figure*}\n\n\\section{More Quantitative Results}\n\\label{sec:2.More_Quantitative_Results}\nTables~\\ref{tab:davis2016_val_supp}, \\ref{tab:davis2017_val_supp}, and \\ref{tab:davis2017_test_dev_supp} provide full comparisons on DAVIS 2016 val, 2017 val, and 2017 test-dev sets, respectively.\nAs shown in the tables, recent offline-learning methods such as KMN~\\cite{seong2020kernelized}, CFBI~\\cite{yang2020collaborative}, LWL~\\cite{bhat2020learning}, and STM~\\cite{Oh_2019_ICCV} surpassed online-learning methods such as PReMVOS~\\cite{luiten2018premvos}, RaNet~\\cite{Wang_2019_ICCV}, e-OSVOS~\\cite{meinhardt2020make}, and DyeNet~\\cite{li2018video} by additionally using YouTube-VOS~\\cite{xu2018youtube} training data.\nHowever, we surpass all online-learning methods, which need additional run-time for fine-tuning during inference, even if we do not use additional YouTube-VOS training data.\nTherefore, the superiority of our HMMN has not relied on additional YouTube-VOS training data.\n\n\\begin{table}[t]\n\\begin{center}\n\\footnotesize\n\\begin{tabular}{lccccc}\n\\toprule\nMethod                                                  & OL         & $\\mathcal{J\\&F}$      & $\\mathcal{J}$         & $\\mathcal{F}$         & Time     \\\\\n\\midrule\nOSVOS \\cite{caelles2017one}                             & \\checkmark & 80.2                  & 79.8                  & 80.6                  & 9$s$     \\\\\nMaskRNN \\cite{hu2017maskrnn}                            & \\checkmark & 80.8                  & 80.7                  & 80.9                  & -        \\\\\nVidMatch   \\cite{hu2018videomatch}                      &            & -                     & 81.0                  & -                     & 0.32$s$  \\\\\nFAVOS \\cite{cheng2018fast}                              &            & 81.0                  & 82.4                  & 79.5                  & 1.8$s$   \\\\\nLSE \\cite{ci2018video}                                  & \\checkmark & 81.6                  & 82.9                  & 80.3                  & -        \\\\\nFEELVOS   \\cite{voigtlaender2019feelvos}                &            & 81.7                  & 80.3                  & 83.1                  & 0.45$s$  \\\\\nFEELVOS \\textbf{(+YV)}   \\cite{voigtlaender2019feelvos} &            & 81.7                  & 81.1                  & 82.2                  & 0.45$s$  \\\\\nFRTM   \\cite{robinson2020learning}                      & \\checkmark           & 81.7                  & -                     & -                     & 0.05$s$  \\\\\nRGMP \\cite{wug2018fast}                                 &            & 81.8                  & 81.5                  & 82.0                  & 0.13$s$  \\\\\nA-GAME \\textbf{(+YV)}   \\cite{johnander2019generative}  &            & -                     & 82.0                  & -                     & 0.07$s$  \\\\\nSAT \\cite{chen2020state}                                &            & 83.1                  & 82.6                  & 83.6                  & 0.03$s$  \\\\\nFRTM \\textbf{(+YV)}   \\cite{robinson2020learning}       & \\checkmark           & 83.5                  & -                     & -                     & 0.05$s$  \\\\\nDTN \\cite{Zhang_2019_ICCV}                              &            & 83.6                  & 83.7                  & 83.5                  & 0.07$s$  \\\\\nCINN \\cite{bao2018cnn}                                  & \\checkmark & 84.2                  & 83.4                  & 85.0                  & $>$30$s$ \\\\\nDyeNet \\cite{li2018video}                               &            & -                     & 84.7                  & -                     & 0.42$s$  \\\\\nRaNet \\cite{Wang_2019_ICCV}                             &            & 85.5                  & 85.5                  & 85.4                  & 0.03$s$  \\\\\nOnAVOS   \\cite{voigtlaender2017online}                  & \\checkmark & 85.5                  & 86.1                  & 84.9                  & 13$s$    \\\\\nSTG-Net   \\cite{liu2020spatiotemporal}                  &            & 85.7                  & 85.4                  & 86.0                  & 0.16$s$  \\\\\nOSVOS$^S$   \\cite{maninis2018video}                     & \\checkmark & 86.0                  & 85.6                  & 86.4                  & 4.5$s$   \\\\\nDIPNet \\cite{hu2020dipnet}                              & \\checkmark & 86.1                  & 85.8                  & 86.4                  & 1.09$s$  \\\\\nCFBI   \\cite{yang2020collaborative}                     &            & 86.1                  & 85.3                  & 86.9                  & 0.18$s$  \\\\\nSTM \\cite{Oh_2019_ICCV}                                 &            & 86.5                  & 84.8                  & 88.1                  & 0.16$s$  \\\\\nPReMVOS   \\cite{luiten2018premvos}                      & \\checkmark & 86.8                  & 84.9                  & 88.6                  & 32.8$s$  \\\\\ne-OSVOS   \\cite{meinhardt2020make}                      & \\checkmark & 86.8                  & 86.6                  & 87.0                  & 3.4$s$   \\\\\nDyeNet \\cite{li2018video}                               & \\checkmark & -                     & 86.2                  & -                     & 2.32$s$  \\\\\nRaNet \\cite{Wang_2019_ICCV}                             & \\checkmark & 87.1                  & 86.6                  & 87.6                  & 4$s$     \\\\\nKMN \\cite{seong2020kernelized}                          &            & 87.6                  & 87.1                  & 88.1                  & 0.12$s$  \\\\\nSTM \\textbf{(+YV)}   \\cite{Oh_2019_ICCV}                &            & 89.3                  & 88.7                  & 89.9                  & 0.16$s$  \\\\\nCFBI \\textbf{(+YV)}   \\cite{yang2020collaborative}      &            & 89.4                  & 88.3                  & 90.5                  & 0.18$s$  \\\\\nKMN \\textbf{(+YV)}   \\cite{seong2020kernelized}         &            & 90.5                  & 89.5                  & 91.5                  & 0.12$s$  \\\\\n\\midrule\nHMMN                                                    &            & 89.4 & 88.2 & 90.6  & 0.10$s$  \\\\\nHMMN \\textbf{(+YV)}                                     &            & \\textbf{90.8}         & \\textbf{89.6}         & \\textbf{92.0}         & 0.10$s$ \\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\caption{Full comparison on DAVIS 2016 validation set.\n(\\textbf{+YV}) indicates YouTube-VOS is additionally used for training, and OL denotes the use of online-learning strategies during test-time. Time measurements reported in this table are directly from the corresponding papers. \n}\n\\label{tab:davis2016_val_supp}\n\\end{table}\n\n\\begin{table}[t]\n\\begin{center}\n\\footnotesize\n\\begin{tabular}{lcccc}\n\\toprule\nMethod                                                  & OL         & $\\mathcal{J\\&F}$ & $\\mathcal{J}$ & $\\mathcal{F}$ \\\\\n\\midrule\nOSVOS \\cite{caelles2017one}                             & \\checkmark & 60.3             & 56.6          & 63.9          \\\\\nVidMatch   \\cite{hu2018videomatch}                      &            & 62.4             & 56.5          & 68.2          \\\\\nMaskRNN \\cite{hu2017maskrnn}                            & \\checkmark & -                & 60.5          & -             \\\\\nRaNet \\cite{Wang_2019_ICCV}                             &            & 65.7             & 63.2          & 68.2          \\\\\nAGSS-VOS \\cite{Lin_2019_ICCV}                           &            & 66.6             & 63.4          & 69.8          \\\\\nRGMP \\cite{wug2018fast}                                 &            & 66.7             & 64.8          & 68.6          \\\\\nDTN \\cite{Zhang_2019_ICCV}                              &            & 67.4             & 64.2          & 70.6          \\\\\nAGSS-VOS \\textbf{(+YV)}   \\cite{Lin_2019_ICCV}          &            & 67.4             & 64.9          & 69.9          \\\\\nOnAVOS   \\cite{voigtlaender2017online}                  & \\checkmark & 67.9             & 64.5          & 71.2          \\\\\nOSVOS$^S$   \\cite{maninis2018video}                     & \\checkmark & 68.0             & 64.7          & 71.3          \\\\\nDIPNet \\cite{hu2020dipnet}                              & \\checkmark & 68.5             & 65.3          & 71.6          \\\\\nFRTM   \\cite{robinson2020learning}                      & \\checkmark           & 68.8             & -             & -             \\\\\nFEELVOS   \\cite{voigtlaender2019feelvos}                &            & 69.1             & 65.9          & 72.3          \\\\\nDyeNet \\cite{li2018video}                               &            & 69.1             & 67.3          & 71.0          \\\\\nA-GAME \\textbf{(+YV)}   \\cite{johnander2019generative}  &            & 70.0             & 67.2          & 72.7          \\\\\nCINN \\cite{bao2018cnn}                                  & \\checkmark & 70.7             & 67.2          & 74.2          \\\\\nDMM-Net \\cite{Zeng_2019_ICCV}                           &            & 70.7             & 68.1          & 73.3          \\\\\nGC \\cite{li2020fast}                                    &            & 71.4             & 69.3          & 73.5          \\\\\nSTM \\cite{Oh_2019_ICCV}                                 &            & 71.6             & 69.2          & 74.0          \\\\\nFEELVOS \\textbf{(+YV)}   \\cite{voigtlaender2019feelvos} &            & 72.0             & 69.1          & 74.0          \\\\\nSAT \\cite{chen2020state}                                &            & 72.3             & 68.6          & 76.0          \\\\\nTVOS   \\cite{zhang2020transductive}                     &            & 72.3             & 69.9          & 74.7          \\\\\nLWL \\cite{bhat2020learning}                             &            & 74.3             & 72.2          & 76.3          \\\\\nAFB+URR \\cite{liang2020video}                           &            & 74.6             & 73.0          & 76.1          \\\\\nSTG-Net   \\cite{liu2020spatiotemporal}                  &            & 74.7             & 71.5          & 77.9          \\\\\nCFBI   \\cite{yang2020collaborative}                     &            & 74.9             & 72.1          & 77.7          \\\\\nDTTM-TAN \\cite{huang2020fast}                           &            & 75.9             & 72.3          & 79.4          \\\\\nKMN \\cite{seong2020kernelized}                          &            & 76.0             & 74.2          & 77.8          \\\\\nFRTM \\textbf{(+YV)}   \\cite{robinson2020learning}       & \\checkmark           & 76.7             & -             & -             \\\\\ne-OSVOS   \\cite{meinhardt2020make}                      & \\checkmark & 77.2             & 74.4          & 80.0          \\\\\nPReMVOS   \\cite{luiten2018premvos}                      & \\checkmark & 77.8             & 73.9          & 81.7          \\\\\nLWL \\textbf{(+YV)}   \\cite{bhat2020learning}            &            & 81.6             & 79.1          & 84.1          \\\\\nSTM \\textbf{(+YV)}   \\cite{Oh_2019_ICCV}                &            & 81.8             & 79.2          & 84.3          \\\\\nCFBI \\textbf{(+YV)}   \\cite{yang2020collaborative}      &            & 81.9             & 79.1          & 84.6          \\\\\nEGMN \\textbf{(+YV)}   \\cite{lu2020video}                &            & 82.8             & 80.2          & 85.2          \\\\\nKMN \\textbf{(+YV)}   \\cite{seong2020kernelized}         &            & 82.8             & 80.0          & 85.6          \\\\\n\\midrule\nHMMN                                                    &            & 80.4             & 77.7          & 83.1          \\\\\nHMMN \\textbf{(+YV)}                                     &            & \\textbf{84.7}    & \\textbf{81.9} & \\textbf{87.5}\\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\caption{Full comparison on DAVIS 2017 validation set.\n}\n\\label{tab:davis2017_val_supp}\n\\end{table}\n\n\\begin{table}[t]\n\\begin{center}\n\\footnotesize\n\\centering\n\\begin{tabular}{lcccc}\n\\toprule\nMethod                                                  & OL         & $\\mathcal{J\\&F}$ & $\\mathcal{J}$ & $\\mathcal{F}$ \\\\\n\\midrule\nOSMN \\cite{yang2018efficient}                           &            & 39.3             & 33.7          & 44.9          \\\\\nFAVOS \\cite{cheng2018fast}                              &            & 43.6             & 42.9          & 44.2          \\\\\nOSVOS \\cite{caelles2017one}                             & \\checkmark & 50.9             & 47.0          & 54.8          \\\\\nCapsuleVOS   \\cite{Duarte_2019_ICCV}                    &            & 51.3             & 47.4          & 55.2          \\\\\nOnAVOS   \\cite{voigtlaender2017online}                  & \\checkmark & 52.8             & 49.9          & 55.7          \\\\\nRGMP \\cite{wug2018fast}                                 &            & 52.9             & 51.3          & 54.4          \\\\\nRaNet \\cite{Wang_2019_ICCV}                             &            & 53.4             & 55.3          & 57.2          \\\\\nOSVOS$^S$   \\cite{maninis2018video}                     & \\checkmark & 57.5             & 52.9          & 62.1          \\\\\nFEELVOS \\textbf{(+YV)}   \\cite{voigtlaender2019feelvos} &            & 57.8             & 55.1          & 60.4          \\\\\nTVOS   \\cite{zhang2020transductive}                     &            & 63.1             & 58.8          & 67.4          \\\\\nSTG-Net   \\cite{liu2020spatiotemporal}                  &            & 63.1             & 59.7          & 66.5          \\\\\ne-OSVOS   \\cite{meinhardt2020make}                      & \\checkmark & 64.8             & 60.9          & 68.6          \\\\\nDTTM-TAN \\cite{huang2020fast}                           &            & 65.4             & 61.3          & 70.3          \\\\\nLucid \\cite{khoreva2019lucid}                           & \\checkmark & 66.7             & 63.4          & 69.9          \\\\\nCINN \\cite{bao2018cnn}                                  & \\checkmark & 67.5             & 64.5          & 70.5          \\\\\nDyeNet \\cite{li2018video}                               & \\checkmark & 68.2             & 65.8          & 70.5          \\\\\nPReMVOS   \\cite{luiten2018premvos}                      & \\checkmark & 71.6             & 67.5          & 75.7          \\\\\nSTM \\textbf{(+YV)}   \\cite{Oh_2019_ICCV}                &            & 72.2             & 69.3          & 75.2          \\\\\nCFBI \\textbf{(+YV)}   \\cite{yang2020collaborative}      &            & 74.8             & 71.1          & 78.5          \\\\\nKMN \\textbf{(+YV)}   \\cite{seong2020kernelized}         &            & 77.2             & 74.1          & 80.3          \\\\\n\\midrule\nHMMN \\textbf{(+YV)}                                     &            & \\textbf{78.6}    & \\textbf{74.7} & \\textbf{82.5}\\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\caption{Full comparison on DAVIS 2017 test-dev set.\n}\n\\label{tab:davis2017_test_dev_supp}\n\\end{table}\n\n\\section{More Qualitative Results}\n\\label{sec:3.More_Qualitative_Results}\nWe show more qualitative results on DAVIS~\\cite{pont20172017} in Fig.~\\ref{fig:qualitative_results_1} and results on YouTube-VOS~\\cite{xu2018youtube} in Figs.~\\ref{fig:qualitative_results_2} and~\\ref{fig:qualitative_results_3}.\nIn the figures, we additionally show the results of STM\\footnote{results are taken from \\url{https://github.com/seoungwugoh/STM}.}~\\cite{Oh_2019_ICCV}, KMN\\footnote{results are extracted from our reproduced model.}~\\cite{seong2020kernelized}, and CFBI\\footnote{results are taken from \\url{https://github.com/z-x-yang/CFBI}.}~\\cite{yang2020collaborative}.\nSine some frames are omitted in the figures, we further provide a comparison video:  \\url{https://youtu.be/zSofRzPImQY}.\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=1\\linewidth]{figures/qualitative_results_supp_1.pdf}\n\\caption{More qualitative results on DAVIS 2017 validation and test-dev sets.\nWe marked significant improvements from STM \\cite{Oh_2019_ICCV}, KMN \\cite{seong2020kernelized}, and CFBI \\cite{yang2020collaborative} using red boxes.\n\\vspace{-1cm}\n}\n\\label{fig:qualitative_results_1}\n\\end{figure*}\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=1\\linewidth]{figures/qualitative_results_supp_2.pdf}\n\\caption{More qualitative results on YouTube-VOS 2019 validation set.\n}\n\\label{fig:qualitative_results_2}\n\\end{figure*}\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=1\\linewidth]{figures/qualitative_results_supp_3.pdf}\n\\caption{More qualitative results on YouTube-VOS 2019 validation set.\n}\n\\label{fig:qualitative_results_3}\n\\end{figure*}\n\n{\\small\n}\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{SWEM: Towards Real-Time Video Object Segmentation with Sequential Weighted Expectation-Maximization}\n\n\\begin{document}\n\n\\title{SWEM: Towards Real-Time Video Object Segmentation with Sequential Weighted Expectation-Maximization}\n\n\\author{Zhihui Lin${^{1*}}$, Tianyu Yang${^{2}}$, Maomao Li${^2}$, Ziyu Wang${^3}$, Chun Yuan${^{4\\dag}}$, Wenhao Jiang${^3}$, and Wei Liu${^3}$ \\\\\n${^1}$Department of Computer Science and Technologies, Tsinghua University, Beijing, China \\\\\n${^2}$Tencent AI Lab, Shenzhen, China\\quad ${^3}$Tencent Data Platform, Shenzhen, China \\\\\n${^4}$Tsinghua Shenzhen International Graduate School, Peng Cheng Lab, Shenzhen, China \\\\\n{\\tt\\small \\{lin-zh14@mails,\\ yuanc@sz\\}.tsinghua.edu.cn} \\quad {\\tt\\small tianyu-yang@outlook.com} \\\\\n{\\tt\\small \\{limaomao07,\\ cswhjiang\\}@gmail.com \\quad wangziyukobe@163.com \\quad wl2223@columbia.edu}\n}\n\n\\maketitle\n\n\\begin{abstract}\n   Matching-based methods, especially those based on space-time memory, are significantly ahead of other solutions in semi-supervised video object segmentation (VOS). However, continuously growing and redundant template features lead to an inefficient inference. To alleviate this, we propose a novel \\textbf{S}equential \\textbf{W}eighted \\textbf{E}xpectation-\\textbf{M}aximization (SWEM) network to greatly reduce the redundancy of memory features. Different from the previous methods which only detect feature redundancy between frames, SWEM merges both intra-frame and inter-frame similar features by leveraging the sequential weighted EM algorithm. Further, adaptive weights for frame features endow SWEM with the flexibility to represent hard samples, improving the discrimination of templates. \n   Besides, the proposed method maintains a fixed number of template features in memory, which ensures the stable inference complexity of the VOS system. Extensive experiments on commonly used DAVIS and YouTube-VOS datasets verify the high efficiency (36 FPS) and high performance (84.3\\% $\\mathcal{J}\\&\\mathcal{F}$ on DAVIS 2017 validation dataset) of SWEM. Code is available at: \\url{https://github.com/lmm077/SWEM}.\n\\end{abstract}\n\n\\let\\thefootnote\\relax\\footnotetext{*Work done during an internship at Tencent AI Lab}\n\\let\\thefootnote\\relax\\footnotetext{$\\dag$Corresponding Author} \n\n\\section{Introduction}\nSemi-supervised video object segmentation (VOS) has seized great interest recent years\\cite{caelles2017one,perazzi2017learning,Johnander_2019_CVPR,Voigtlaender_2019_CVPR,Li_2019_ICCV,Oh_2019_ICCV,Wang_2019_ICCV,seong_2020_ECCV,lu_2020_ECCV,liang2020video,hu2021learning,xie2021efficient,wang2021swiftnet,duke2021sstvos,yang2021associating, seong2021hierarchical,cheng2021stcn} in the computer vision community.\nIt aims to segment the objects of interest from the background in a video, where only the mask annotation of the first frame is provided during testing.  A group of early methods concentrate on on-line fine-tuning\\cite{caelles2017one,bao2018cnn,maninis2018video,luiten2018premvos,khoreva2017lucid} with the first annotated frame. However, these method tends to suffer from model degradation caused by target appearance changes as video goes on. Besides, propagation-based methods use masks computed in previous frames to estimate masks in the current frame\\cite{perazzi2017learning,cheng2018fast,wug2018fast,wang2018semi}, which is, however, vulnerable to occlusions and rapid motion.\n\n\\begin{figure}\n\\centering\n  \\begin{subfigure}{0.48\\linewidth}\n  \\centering\n      \\includegraphics[width=0.95\\textwidth]{figures/stm_mem.pdf}\n    \\caption{STM}\\label{subfig:stm_mem}\n  \\end{subfigure}\n  \\hfill\n  \\begin{subfigure}{0.48\\linewidth}\n  \\centering\n      \\includegraphics[width=0.95\\textwidth]{figures/swem_mem.pdf}\n    \\caption{SWEM}\\label{fig:subfig:swem_mem}\n  \\end{subfigure}\n\\vspace{-0.3 cm}\n\\caption{Instead of storing all past frames features as memory, just like STM~\\cite{Oh_2019_ICCV} and following methods~\\cite{seong_2020_ECCV,hu2021learning,seong2021hierarchical,cheng2021stcn} do, our SWEM sequentially updates a compact set of bases with a fixed size, greatly reducing the inter-frame and intra-frame redundancy.}\n\\vspace{-0.5 cm}\n\\label{fig:stm_swem_mem} %% label for entire figure\n\\end{figure}\n\nRecently, matching-based VOS methods~\\cite{chen2018blazingly,hu2018videomatch,Voigtlaender_2019_CVPR,lu_2020_ECCV,yang_2020_ECCV,seong_2020_ECCV,liang2020video, hu2021learning,xie2021efficient,wang2021swiftnet,seong2021hierarchical,cheng2021stcn} have achieved striking performance. Such matching-based methods first exploit previous frames to construct target templates, and then calculate the pixel-level correlations between the new coming frame embeddings and the target templates to perform the segmentation.\nAs seen in Figure~\\ref{fig:stm_swem_mem}, the Space-Time Memory Network (STM)~\\cite{Oh_2019_ICCV} and the following STM-like methods ~\\cite{seong_2020_ECCV,hu2021learning,xie2021efficient,seong2021hierarchical,cheng2021stcn} leverage memory networks to store template features every T frames endlessly, which is prone to missing key-frame information and running out of memory for long-term videos. Besides, given that the inter-frame redundancy of video features would harm the efficiency of matching, another group of methods AFB\\_URR~\\cite{liang2020video} and Swift~\\cite{wang2021swiftnet} take advantage of the similarity of inter-frame features to selectively update partial features. Nonetheless, they all fail to balance the performance and efficiency through a hand-crafted similarity threshold. \n\nAlthough past efforts have achieved promising results, we argue that both inter-frame redundancy and intra-frame one pose the main obstacles that prevent efficient template matching.\nHere comes to a question that can we achieve a real-time VOS system by considering both the inter-frame and intra-frame redundancy simultaneously? In this paper, we will explore its feasibility. \n\nInspired by the Expectation-Maximization Attention (EMA) ~\\cite{Li_2019_ICCV}, we intend to construct a set of low-rank bases for memory features through Expectation-Maximization (EM)~\\cite{em} iterations. Here, the number of bases is far less than that of image pixels. Thus, bases can be regarded as a more compact representation, which can greatly reduce the intra-frame redundancy. Instead of applying EM directly, we adopt Weighted Expectation-Maximization (WEM) with the predicted mask as the \\textbf{fixed} weights to explicitly construct foreground and background bases in each frame. What's more, we also propose a weighted EM with \\textbf{adaptive} weights, which give larger weights for hard samples during generating bases. Here, the hard samples refer to those pixels that are not well expressed by bases, but are important for object segmentation.\n\nWEM can deal with the intra-frame redundancy effectively; however, inter-frame one remains unsolved. Applying WEM on a single frame is efficient, but the computation complexity will be dramatically increased if directly applying it to all growing memory features. To further reduce the inter-frame redundancy, we propose the Sequential Weighted Expectation-Maximization (SWEM), where features of only one frame participate in the EM iterations during the memory updating stage. The memory bases will be updated with the new frame features through similarities rather than a simple linear combination. Formally, this updating process is equivalent to a weighted average of all past frame features. \nAs shown in Figure~\\ref{fig:stm_swem_mem}, \ncompared with STM~\\cite{Oh_2019_ICCV} which saves all historical frame information as the memory template of objects, our SWEM only updates a more compact set of bases sequentially, thus greatly reducing the inter-frame and intra-frame redundancy.\n\nOur contributions can be summarized as follows:\n\\begin{itemize}\n\\item \n\\vspace{-0.2cm}\nWe propose a fast and robust matching-based method for VOS, dubbed Sequential Weighted Expectation-Maximization (SWEM) network,\nwhere a set of compact bases are constructed and updated sequentially, reducing both the inter- and intra-frame redundancy.\n\\item\n\\vspace{-0.2cm}\nWe introduce an adaptive weights calculation approach for weighted EM, which makes the base features pay more attention to hard samples.\n\\item\n\\vspace{-0.2cm}\n Without bells and whistles, SWEM reaches a level close to state-of-the-art performance, while maintaining an inference speed of 36 FPS.\n\\end{itemize}\n\n\\section{Related Work}\n\\noindent{\\textbf{Matching-based Methods for VOS.}} \nRecent years have seen a surge of interest in video object segmentation under the semi-supervised setting.  A number of matching-based methods~\\cite{wang2019fast,hu2018videomatch,chen2018blazingly,Wang_2019_ICCV,Oh_2019_ICCV,seong_2020_ECCV,lu_2020_ECCV,liang2020video, hu2021learning,xie2021efficient,wang2021swiftnet,seong2021hierarchical,cheng2021stcn} regard the first or intermediate frames as a target template, which is then used to match the pixel-level feature embedding in the new frame. To obtain both long-term and short-term object appearance information, FEELVOS~\\cite{Voigtlaender_2019_CVPR} and CFBI~\\cite{yang_2020_ECCV} match the current frame with both the first frame and the previous frame to obtain both global and local temporal dependencies. Besides, STM~\\cite{Oh_2019_ICCV} and the following methods~\\cite{seong_2020_ECCV, hu2021learning,xie2021efficient,seong2021hierarchical,cheng2021stcn} store multiple memory templates from all previous frames as templates, which is redundant and time-consuming during matching. In contrast, we propose a novel method named SWEM, which only stores a set of low-rank and updated basis features for each target, making the target representation more compact and efficient. \n \n\\noindent{\\textbf{Learning Fast and Robust VOS.}} \nLeaning a fast and robust model is a common goal since both accuracy and speed are important in practical applications~\\cite{Wang_2019_ICCV,wang2019fast,Robinson_2020_CVPR,Chen_2020_CVPR,Zhang_2020_CVPR,li_2020_ECCV_GCM}. RANet~\\cite{Wang_2019_ICCV} only uses the first frame as the target template for an acceptable speed. As tracker-based methods, SiamMask~\\cite{wang2019fast} and SAT~\\cite{Chen_2020_CVPR} only process the region of interest. TVOS~\\cite{Zhang_2020_CVPR} directly propagates target masks according to feature similarity in the embedding space. In general, to achieve fast VOS, the previous methods sacrificed the integrity of the target representation, which substantially degrades segmentation performance. Swift~\\cite{wang2021swiftnet} uses a variation-aware trigger module to compute the inter-frame difference to update frames with diverse dynamics. Further, only partial features that are significantly different from memory features will be updated. \n\nIn this work, we consider reducing inter- and intra-frame redundancy simultaneously. The proposed weighted EM greatly reduces the intra-frame redundancy by iteratively constructing compact base features for the whole frame. To diminish the inter-frame redundancy, we further extend weighted EM in a sequential manner,\nwhich can adaptively update the model without increasing the number of matching templates, thus making our model fast and robust.        \n\n\\section{Preliminaries}\n\n\\subsection{Expectation-Maximization Algorithm}\nThe expectation-maximization (EM)~\\cite{em} is an iteration-based algorithm, which can be used to estimate parameters of latent variable modes by maximizing the likelihood. The task is defined as estimating model parameters $\\theta$ based on observation data set $\\mathbf{X}$ %=\\{\\mathbf{x}_i\\}_{i=1}^{N}$ \nand corresponding latent variables $\\mathbf{Z}$. %=\\{\\mathbf{\\ytyn{z}}_i\\}_{i=1}^{N}$. \nEach EM iteration involves two steps, the Expectation step (E step) and the Maximization step (M step). At the $r$-th iteration, E step finds the posterior $P(\\mathbf{Z}|\\mathbf{X}, \\theta^{r-1})$ and computes the expectation:\n\\vspace{-0.2cm}\n\t\\begin{equation}\n\t\\begin{split}\n\t \\mathcal{Q}(\\theta, \\theta^{r-1})=\\sum P(\\mathbf{Z}|\\mathbf{X}, \\theta^{r-1})\\ln P(\\mathbf{X}, \\mathbf{Z}|\\theta).\n\t\\end{split}\n\t\\label{eq:EM_E}\n\t\\end{equation}\nM step estimates parameters by maximizing the above data likelihood:\n\\vspace{-0.2cm}\n\t\\begin{equation}\n\t\\begin{split}\n\t \\theta^{r} = {\\arg\\max}_{\\theta}\\mathcal{Q}(\\theta, \\theta^{r-1}).\n\t\\end{split}\n\t\\label{eq:EM_M}\n\t\\end{equation}\nThe E step and the M step are alternately executed $R$ times to achieve the convergence criterion.\n\n\\subsection{Expectation-Maximization Attention}\nExpectation-Maximization Attention (EMA)~\\cite{Li_2019_ICCV} is proposed to formulate the attention mechanism~\\cite{nonlocal} into an expectation-maximization manner. Specifically, instead of regarding \\emph{all pixels} as reconstruction bases, EMA iteratively estimates a much more compact set of bases for each image. EMA consists of three steps, namely \\textit{Responsibility Estimation} (RE), \\textit{Likelihood Maximization} (LM), and \\textit{Data Re-estimation} (DR). Denote $\\ytyn{\\mathbf{X}=\\{\\mathbf{x}_n\\}_{n=1}^{N}}\\in \\mathbb{R}^{N\\times C}$ as image feature, $\\ytyn{\\mathcal{M}=\\{\\bm{\\mu}_k\\}_{k=1}^K}\\in \\mathbb{R}^{K\\times C}$ as the randomly initialized base features, where $N$, $C$, and $K$ indicate the numbers of pixels, channels, and bases. RE estimates the hidden variable $\\ytyn{\\mathbf{Z} = \\{z_{nk}\\}_{n=1,k=1}^{N,K}}\\in \\mathbb{R}^{N\\times K}$, where the responsibility $z_{nk}$ represents the probability of the $n$-th pixel belonging to the $k$-th base:\n\\vspace{-0.2cm}\n\t\\begin{equation}\n\t\\begin{split}\n\t \\ytyn{z_{nk}}=\\frac{\\exp(\\mathbf{x}_{n}\\bm{\\mu}^{\\top}_{k}/\\tau)}{\\sum_{j=1}^{K}\\exp(\\mathbf{x}_{n}\\bm{\\mu}^{\\top}_{j}/\\tau)}.\n\t\\end{split}\n\t\\label{eq:EMA_AE}\n\t\\end{equation}\nHere, $\\tau$ is a hyper-parameter which controls the shape of distribution $\\mathbf{Z}$. Then, LM updates base features $\\ytyn{\\mathcal{M}}$ by applying the weighted average on feature $\\mathbf{X}$. That is, the $k$-th base is updated by:\n\\vspace{-0.2cm}\n\\begin{equation}\n \\bm{\\mu}_{k} = \\frac{\\sum^{N}_{n=1}\\ytyn{z_{nk}}\\mathbf{x}_{n}} {\\sum^{N}_{n=1}\\ytyn{z_{nk}}}.\n \\label{eq:EMA_AM}\n\\end{equation}\nNote that RE and LM are iteratively executed $R$ times until convergence. Finally, {DR} reconstructs a low-rank version of $\\mathbf{X}$ using $\\tilde{\\mathbf{X}}=\n\\mathbf{Z}\\ytyn{\\mathcal{M}}$. Since $K$ is much less than $N$, basis set $\\ytyn{\\mathcal{M}}$ can be treated as a compact representation for image feature $\\mathbf{X}$. Inspired by EMA, {we} consider replacing redundant memory features with more compact base features. \n\n\\begin{figure}\n\\begin{center}\n\\includegraphics[width=1.0\\linewidth]{figures/stm_inter_intra.pdf}\n\\end{center}\n\\vspace{-0.5cm}\n   \\caption{The illustration of inter-frame and intra-frame redundancy of video features.}\n\\label{fig:inter_intra}\n\\vspace{-0.5cm}\n\\end{figure}\n\n\\subsection{Redundancy of the Space-time Memory}\nTo get a more intuitive understanding of feature redundancy in videos, we evaluate the inter-frame and intra-frame cosine similarities of videos features on the DAVIS 2017~\\cite{pont20172017} validation set using the image encoder of STM~\\cite{Oh_2019_ICCV} as the feature extractor. For each pixel in the current frame, we first calculate its maximum similarity with all pixels in the previous frame. In this way, $N$ such maximum similarities can be obtained. In Figure~\\ref{fig:inter_intra}, we list the histogram of the maximum similarities, where the horizontal coordinate is the similarity range. Most of the similarities are larger than $0.6$, and nearly 87\\% of similarities are larger than $0.9$, indicating high inter-frame redundancy in video sequences. In contrast, computing the maximum similarity for intra-frame redundancy measurement is not appropriate since spatial continuity would make most maximum similarities exceed $0.9$. Thus, we calculate all pair-wise similarities and count the average pair number of each frame under different similarities. The third line in Figure~\\ref{fig:inter_intra} shows the statistics.  Most of the similarities between the two pixels in an image are positive, and more than 70\\% of them are larger than $0.3$, which demonstrates the ubiquity of intra-frame redundancy.\n\nTo verify that the EM algorithm can find a more compact representation for image features and thus restrain the frame redundancy, we calculate the inter-frame and intra-frame similarity with a basis set rather than the entire image feature, where the basis set consists of 256 bases evaluated via EM iterations. Specifically, instead of calculating similarities between inter-frame bases, we calculate the maximum similarity between each frame feature and the base feature of the previous frame. As seen in Figure~\\ref{fig:inter_intra}, more than 99\\% of inter-frame similarities are larger than $0.7$. That is, although each frame has only 256 base features, which is far less than pixel number, it still meets the need of inter-frame matching. As for intra-frame similarities, although the similarity distribution is basically the same as that of the whole image feature, the number of large similarities has decreased significantly, which demonstrates the intra-frame redundancy is greatly reduced via EM iterations.\n\n\\begin{figure*}\n\\begin{center}\n\\includegraphics[width=0.7\\linewidth]{figures/swem_pipeline2.pdf}\n\\end{center}\n\\vspace{-0.5cm}\n   \\caption{\nThe matching-based pipeline of SWEM. The backbone network receives the $t$-th frame to capture general image features as \\textbf{Key} $\\mathbf{K}^{(t)}$. The features are used to match with target-specific memories. Through the matching process, the re-aggregated value and similarity map are obtained to be the target features for the final segmentation. Multi-level skip connections help refine the segmentation results for low to high resolution. The predicted mask is then employed with the intermediate image features to update bases via our SWEM.}\n\\label{fig:SWEM}\n\\vspace{-0.5cm}\n\\end{figure*}\n\n\\section{Proposed Approach}\n\\label{sec:apporach}\nWe first introduce the weighted EM, which leverages the predicted mask as weights to explicitly construct foreground and background bases separately in each frame. \nFurthermore, the \\textbf{adaptive} weights make the model pay more attention to hard samples to improve the segmentation performance. Then, the core part of this work, the SWEM algorithm is detailed, which shows how to convert \\textbf{growing} frame features into \\textbf{fixed-size} bases. Finally, we describe the matching-based pipeline of the proposed SWEM. \n\n\\subsection{Weighted Expectation-Maximization}\n\\label{subsec:WEM}\nAlthough we have proved that using EM to find a more compact representation can reduce both inter and intra-frame redundancy, we argue that naively employing EM to learn a set of bases for memory features is not a reasonable solution in the VOS system. The reason here is that the memory bases would mix with both the foreground and background, which is unfavorable for object segmentation.\nInstead, it is desirable to build low-rank foreground and background bases separately. To this end, we leverage Weighted Expectation-Maximization (WEM)~\\cite{long2006spectral,tseng2007penalized,ackerman2012weighted,feldman2012data,gebru2016algorithms}, which is widely used for weighted data clustering. When using WEM to produce bases for images, Eq.~\\eqref{eq:EMA_AM} would be modified as:\n\\begin{equation}\n \\bm{\\mu}_{k} = \\frac{\\sum^{N}_{n=1}\\ytyn{z_{nk}}w_n\\mathbf{x}_{n}} {\\sum^{N}_{n=1}\\ytyn{z_{nk}}w_n},\n \\label{eq:WEM_M}\n\\end{equation}\nwhere $w_n$ is the weight for $\\mathbf{x}_{n}$. It is equivalent to ``seeing the $n$-th feature $w_n$ times\"~\\cite{gebru2016algorithms}.\nNote that WEM makes it possible to construct separate foreground and background bases for template matching, where\nthe object foreground mask and the background mask of each frame can be used as the corresponding \\textit{fixed} weights to substitute $w_n$. \nIn this way, any irregular target region can be represented by a set of bases with a fixed size, which greatly reduces intra-frame redundancy.\n\nThe essence of using WEM for compact representation learning is to perform clustering on all pixels in the image, and make different bases to represent each pixel. Because the number of bases is far less than that of pixels, the constructed segmentation target template would be incomplete to some extent and even be faced with the target lost situation. The expression degree of each pixel is different during basis set construction. Some pixels have little contribution for bases, but are very important for object segmentation, which are so-called hard samples.\nTo ensure that the hard samples could be assigned larger weights during basis set construction, we propose to \\textit{adaptively} adjust the weights of pixels rather than directly employing the \\textit{fixed} weights calculated via foreground and background masks.\n\nWe estimate a confidence score for each pixel by a foreground-background binary classification. Specifically, after the E-step of WEM iteration, each pixel is classified by the foreground or background bases, the classification probability of the $n$-th pixel can be calculated as:\n\\begin{equation}\n\\begin{split}\nP^{fg}(\\mathbf{x}_n) &= \\frac{\\sum_{k=1}^{K} \\mathcal{K}(\\mathbf{x}_n, \\bm{\\mu}^{fg}_k)}{\\sum_{k=1}^{K} [\\mathcal{K}(\\mathbf{x}_n, \\bm{\\mu}^{fg}_k)+ \\mathcal{K}(\\mathbf{x}_n, \\bm{\\mu}^{bg}_k)]}, \\\\\nP^{bg}(\\mathbf{x}_n) &= 1 - P^{fg}(\\mathbf{x}_n),\n\\end{split}\n\\label{eq:adapt_p}\n\\end{equation}\nwhere $\\ytyn{\\bm{\\mu}^{fg}_k}$ and $\\ytyn{\\bm{\\mu}^{bg}_k}$ are foreground and background bases separately. $\\mathcal{K}(\\cdot)$ is a kernel function for calculating the similarity of two input features. Specifically, $\\mathcal{K}(\\mathbf{a}, \\mathbf{b})=\\exp(\\frac{\\mathbf{a}\\mathbf{b}^{\\top}/\\tau}{\\|\\mathbf{a}\\|\\cdot \\|\\mathbf{b}\\|})$. Eq.~(\\ref{eq:adapt_p}) can be treated as a coarse segmentation result, since it provides the result of whether each pixel corresponds to the foreground or the background. Besides, the final segmentation (i.e., the output of the network decoder) can be obtained, which is considered more accurate than the coarse one since it is additionally constrained by ground-truth annotations. If coarse segmentation of a pixel is consistent with the final one, this pixel can be regarded as an easy sample. Otherwise, it would be regarded as a hard sample. We believe that the inconsistency of hard samples is because these pixels are neglected during the base construction process, which makes them struggle to achieve the same result as the final segmentation.\nSupposing that $m^{fg}$ and $m^{bg}$ are soft masks of final segmentation, the \\textit{adaptive weights} are estimated by:\n\\begin{equation}\n\\begin{split}\nw^{fg}_n &= m^{fg}_n P^{bg}(\\mathbf{x}_n),\\quad w^{bg}_n = m^{bg}_n P^{fg}(\\mathbf{x}_n).\n\\end{split}\n\\label{eq:adapt_w}\n\\end{equation}\nThe more inconsistent the coarse and final segmentation are, the higher weights would be given for base construction. \n\n\\subsection{Sequential Weighted EM}\nTo reduce inter-frame redundancy, previous methods  ~\\cite{liang2020video,wang2021swiftnet} set a similarity threshold to ignore or merge similar features between frames. However, the segmentation performance and computational complexity are sensitive to this hand-crafted threshold. What's worse, it is hard to find an appropriate threshold to make a trade-off between performance and complexity. In this paper, we extend WEM in a sequential manner, yielding a Sequential Weighted EM (SWEM) algorithm to reduce both intra- and inter-frame redundancy without any threshold hyperparameter. \n\nAt the time step $t$, the ideal solution is to apply WEM to all previous $t-1$ frames for basis set construction.  \nHowever, the growing scale of computing is unacceptable, which is not feasible for long-term segmentation. Therefore, we introduce a sequential weighted average of frame features when computing base assignment,  where the weights are estimated with time-dependent responsibility $\\mathbf{Z}^{(t)}$.   \n\nConcretely, we extend the WEM sequentially and reformulate Eq. (\\ref{eq:WEM_M}) as:\n\\begin{equation}\n    \\bm{\\mu}^{(t)}_k = \\frac{\\sum_{i=1}^{t}\\sum_{n=1}^{N}\\ytyn{{z}^{(t)}_{nk}}w^{(t)}_n\\mathbf{x}^{(t)}_n}{\\sum_{i=1}^{t}\\sum_{n=1}^{N}\\ytyn{{z}^{(t)}_{nk}}w^{(t)}_n}.\n\\label{swem}\n\\end{equation}\nNote that we implement Eq. (\\ref{swem}) in a recursive manner, \\ie, the numerator $\\alpha$ and denominator $\\beta$ are computed by $\\bm{\\alpha}^{(t)}_k = \\bm{\\alpha}^{(t-1)}_k + \\sum_{n=1}^{N}\\ytyn{{z}^{(t)}_{nk}}w^{(t)}_n\\mathbf{x}^{(t)}_n$ and $\\bm{\\beta}^{(t)}_k = \\bm{\\beta}^{(t-1)}_k + \\sum_{n=1}^{N}\\ytyn{{z}^{(t)}_{nk}}w^{(t)}_n$, then $\\bm{\\mu}^{(t)}_k = \\bm{\\alpha}^{(t)}_k/\\bm{\\beta}^{(t)}_k$.\n\nThis sequential way of computing base assignment achieves more smooth and adaptable model updating. Instead of storing all frame bases, maintaining only one set of adaptable bases is undoubtedly more friendly to hardware and can also help realize a real-time VOS system. Algorithm\\ref{alg:SWEM} shows the detailed pipeline of our SWEM at time step $t$.%\n\nIt is also worth noting that the updating of bases is lazy in SWEM.\nSince $\\ytyn{{z}^{(t)}_{nk}}$ represents the similarity degree between $\\mathbf{x}^{(t)}_n$ and $\\bm{\\mu}^{(t)}_k$, and if a base feature has more similar features with the current frame, it will be updated more quickly. This lazy updating strategy can help SWEM be more robust to noises and prevent drifts. In another way, $w^{(t)}_n$ also enables the hard samples to be updated faster. \n\n\\begin{algorithm}\n\\caption{The SWEM at the time step $t$}\\label{alg:SWEM}\n\\LinesNumbered\n\\KwIn{\\\\\n\\quad features of frame $t$: \\\\\n\\quad \\quad $\\mathbf{X}^{(t)}\\in \\mathbb{R}^{N\\times C}$, \\\\\n\\quad mask of frame $t$: \\\\\n\\quad \\quad $m^{fg, (t)}\\in [0, 1]^{N}$ and $m^{bg, (t)}\\in [0, 1]^{N}$ \\\\\n\\quad bases at time step $t-1$: \\\\\n\\quad \\quad $\\ytyn{\\mathcal{M}}^{fg, (t-1)} \\in \\mathbb{R}^{K\\times C}$ and $\\ytyn{\\mathcal{M}}^{bg, (t-1)} \\in \\mathbb{R}^{K\\times C}$,  \\\\\n\\quad accumulated numerators and denominators: \\\\\n\\quad \\quad $\\bm{\\alpha}^{fg, (t-1)},\\ \\bm{\\alpha}^{bg, (t-1)} $ and $\\bm{\\beta}^{fg, (t-1)}, \\bm{\\beta}^{bg, (t-1)}$}\n\\KwOut{\\\\\n\\quad bases $\\ytyn{\\mathcal{M}}^{fg, (t)}$ and $\\ytyn{\\mathcal{M}}^{bg, (t)}$}\n{\\small \\tcc{\\CUT{The following procedures are performed for both $fg$ and $bg$,} Superscript symbols $fg$ and $bg$ are omitted for simplicity.}}\n$\\ytyn{\\mathcal{M}}^{(t)}\\gets \\ytyn{\\mathcal{M}}^{(t-1)}$ \\\\ \n$w^{(t)}\\gets m^{(t)}$\n\n\\For{$r=1\\textrm{ to } R$}{\n\t{\\small \\tcp{SW-\\textbf{E} step, estimate responsibilities:}}\n     $\\ytyn{{z}^{(t)}_{nk}}\\gets \\frac{\\mathcal{K}(\\mathbf{x}^{(t)}_{n}, \\bm{\\mu}^{(t)}_{k})}{\\sum_{j=1}^{K}\\mathcal{K}(\\mathbf{x}^{(t)}_{n},\\bm{\\mu}^{(t)}_{j})}$ \n\n\t{\\small \\tcp{SW-\\textbf{M} step, update bases:}}\n     $\\bm{\\alpha}^{(t)}_k \\gets \\bm{\\alpha}^{(t-1)}_k + \\sum_{n=1}^{N}\\ytyn{{z}^{(t)}_{nk}}w^{(t)}_n\\mathbf{x}^{(t)}_n$ \\\\\n     $\\bm{\\beta}^{(t)}_k \\gets \\bm{\\beta}^{(t-1)}_k + \\sum_{n=1}^{N}\\ytyn{{z}^{(t)}_{nk}}w^{(t)}_n$ \\\\\n     $\\bm{\\mu}^{(t)}_k \\gets \\bm{\\alpha}^{(t)}_k/\\bm{\\beta}^{(t)}_k$\n\n\t{\\small \\tcp{SW-\\textbf{W} step, calculate weights:}}\n\t$P^{fg}(\\mathbf{x}^{(t)}_n)\\gets \\frac{\\sum_{k=1}^{K} \\mathcal{K}(\\mathbf{x}^{(t)}_n, \\bm{\\mu}^{fg, (t)}_k)}{\\sum_{k=1}^{K} [\\mathcal{K}(\\mathbf{x}^{(t)}_n, \\bm{\\mu}^{fg, (t)}_k)+ \\mathcal{K}(\\mathbf{x}^{(t)}_n, \\bm{\\mu}^{bg, (t)}_k)]}$ \\\\\n\t$P^{bg}(\\mathbf{x}^{(t)}_n) \\gets 1-P^{fg}(\\mathbf{x}^{(t)}_n)$ \\\\\n\t$w^{fg, (t)}_n \\gets m^{fg, (t)}_n P^{bg}(\\mathbf{x}^{(t)}_n)$ \\\\\n\t$w^{bg, (t)}_n \\gets m^{bg, (t)}_n P^{fg}(\\mathbf{x}^{(t)}_n)$\n}\n$n=1,2,...,N; \\ k=1,2..,K$\n\\end{algorithm}\n\n\\subsection{Matching-based Pipeline}\n\\label{subsec:pipeline}\nThe overview of the proposed SWEM network is illustrated in Figure \\ref{fig:SWEM}. The whole pipeline mainly consists of three stages, including 1) feature encoding, 2) segmentation, and 3) memorization. \n\n\\noindent{\\textbf{Encoding.}}\nSimilar to previous matching-based methods with Space-Time Memory~\\cite{Oh_2019_ICCV,seong_2020_ECCV,li_2020_ECCV_GCM,liang2020video, hu2021learning,xie2021efficient,wang2021swiftnet,seong2021hierarchical,cheng2021stcn}, frames are encoded into \\textbf{Key}-\\textbf{Value} pairs ($\\mathbf{K}\\in \\mathbb{R}^{N\\times C}$ and $\\mathbf{V} \\in \\mathbb{R}^{N\\times C'}$) for memory query and read. We adopt the encoder structure of STCN~\\cite{cheng2021stcn} to extract image features. The key features are also reused for memorization. Specific network details are described in Section \\ref{subsec:network}.\n\n\\noindent{\\textbf{Segmentation.}}\nThe segmentation stage includes feature matching and mask decoding. At time step $t$, the $\\textbf{Key}$ features $\\mathbf{K}^{(t)}$ are used as query to read memory features from $\\bm{\\kappa}^{(t-1)}$ and $\\bm{\\nu}^{(t-1)}$, where $\\bm{\\kappa}$ and $\\bm{\\nu}$ are base features corresponding to the key and value features and they are concatenations of foreground and background bases ($[\\bm{\\kappa}^{fg}; \\bm{\\kappa}^{bg}]\\in \\mathbb{R}^{2K\\times C}$ and $[\\bm{\\nu}^{fg}; \\bm{\\nu}^{bg}]\\in \\mathbb{R}^{2K\\times C'}$).\nA non-local~\\cite{nonlocal} formed matching process is applied as follows:\n\\begin{equation}\n \\hat{{\\mathbf{V}}}^{(t)}_n = \\sum_{k=1}^{2K}\\frac{\\mathcal{K}({\\mathbf{K}}^{(t)}_n, \\bm{\\kappa}^{(t-1)}_k)}{\\sum_{j=1}^{2K}\\mathcal{K}({\\mathbf{K}}^{(t)}_n, \\bm{\\kappa}^{(t-1)}_j)} \\bm{\\nu}^{(t-1)}_k.\n \\label{eq:matching_v}\n\\end{equation}\n$\\hat{\\mathbf{V}}^{(t)}$ is a low-rank reconstruction using memory bases, which is helpful to segmentation tasks. Different from the previous STM-like methods, our memory bases are explicitly separated into foreground and background. Therefore, the correlations $\\mathcal{K}(\\mathbf{K}^{(t)}_n, \\bm{\\kappa}^{fg, (t-1)}_k)$ and $\\mathcal{K}(\\mathbf{K}^{(t)}_n, \\bm{\\kappa}^{bg, (t-1)}_k)$ can also be used as important segmentation clues. However, the rank of base features is not fixed for different videos since unsorted correlations are not suitable as the inputs for CNNs. To tackle this problem, we design a permutation-invariant operation, which can produce segmentation clues from correlations. Define $\\mathcal{K}^{fg, (t)}_n \\in \\mathbb{R}^{K}$ and $\\mathcal{K}^{bg, (t)}_n \\in \\mathbb{R}^{K}$ as the correlations of $\\mathbf{K}^{(t)}_n$ with all foreground and background bases, respectively. The permutation-invariant feature $\\mathbf{S}^{(t)}$ can be calculated by:\n\\begin{equation}\n\\mathbf{S}^{(t)}_{nl} = \\frac{\\sum_{j\\in \\text{topl}(\\mathcal{K}^{fg, (t)}_n)} \\mathcal{K}^{fg, (t)}_{nj}}{\\sum_{j\\in \\text{topl}(\\mathcal{K}^{fg, (t)}_n)} \\mathcal{K}^{fg, (t)}_{nj} + \\sum_{j\\in \\text{topl}(\\mathcal{K}^{bg, (t)}_n)} \\mathcal{K}^{bg, (t)}_{nj}},\n\\label{eq:matching_s}\n\\end{equation}\nwhere $l=1,2,...,L$. Note that $L \\leq K$ is a hyperparameter to control the number of segmentation clues channel and the computation complexity. Besides, $\\text{topl}(\\cdot)$ means the top-$l$ correlation values.\n\nThe decoder takes segmentation clues $[\\hat{\\mathbf{V}}^{(t)}; \\mathbf{S}^{(t)}]$ as input to produce the final mask $\\tilde{m}^{(t)}$. Additional skip-connections are also adopted to make use of low-level appearance features.  \n\n\\noindent{\\textbf{Memorization.}}\nAfter the segmentation, key features $\\mathbf{K}^{(t)}$ are reused for the memorization stage. We adopt another ResNet-18 to re-encode the image-mask pair to obtain value features $\\mathbf{V}^{(t)}$. The key bases are updated by $\\bm{\\kappa}^{(t)}=\\rm{SWEM}(\\mathbf{K}^{(t)}, \\tilde{m}^{(t)}, \\bm{\\kappa}^{(t-1)})$ which is described in Algorithm \\ref{alg:SWEM}. To maintain the alignment between key and value, the updated value bases is calcudated by $\\bm{\\nu}^{(t)}_k = (\\bm{\\beta}^{(t-1)}_k \\bm{\\nu}^{(t-1)}_k + \\sum_{n=1}^{N}\\ytyn{{z}^{(t)}_{nk}}w^{(t)}_n\\mathbf{v}^{(t)}_n)/\\bm{\\beta}^{(t)}_k$, where $\\bm{\\beta}$, $\\mathbf{Z}$ and $w$ are all produced during the construction of key bases $\\bm{\\kappa}$.\n\n\\section{Implementation Details}\n\n\\subsection{Network Structure}\n\\label{subsec:network}\nWe adopt ResNet-50~\\cite{he2016deep} as the backbone to extract frame features and ResNet-18 for value feature extraction. All batch normalization layers are frozen. The stage 4 (res4) features are used for memorization and matching processes. These feature maps have a stride of 16 compared with the row image. The temperature hyper-parameter $\\tau$ is set to 0.05. The number of base features in a group is set as $K=128$, and the number of iterations $R$ is set as 4 in the SWEM algorithm. We select top-64 ($L=64$) correlation scores calculated by Eq.~\\eqref{eq:matching_s}. For simplicity and fair comparison with STM~\\cite{Oh_2019_ICCV}, we use the same two-level decoder, which consists of two refining layers, and each layer contains two residual blocks. \n\n\\subsection{Two-stage Training}\n\\noindent{\\textbf{Pre-training on static image datasets.}}\nFollowing the previous methods~\\cite{Wang_2019_ICCV,Oh_2019_ICCV,seong_2020_ECCV,lu_2020_ECCV,liang2020video}, we first perform the pre-training procedure on static image datasets~\\cite{cocolin2014microsoft,msra10kChengPAMI,ecssdshi2015hierarchical,pascalsli2014secrets,pascolvoceveringham2010pascal}. The input frames are cropped into $384\\times 384$ for training. Three frames are generated based on a single image at each step, where the random affine transformations of shearing, rotation, scaling, and cropping are applied. The Adam optimizer~\\cite{kingma2014adam} with the learning rate $1e$-$5$ is adopted for all training processes. Besides, we use the cross-entropy loss for the final segmentation.\n\n\\noindent{\\textbf{Training on video datasets.}}\nAfter pre-training on images, we fine-tune the proposed SWEM on video datasets DAVIS 2017~\\cite{pont20172017} and the YouTube-VOS 2018~\\cite{xu2018youtube}. The training process is similar to image pre-training, where the main difference is that we sample the three frames from a video clip randomly instead of one single image. For multi-object frames, we randomly select less than 3 objects. We perform all experiments on a single NVIDIA Tesla V100 GPU with a batch size of 4. \\CUT{and we also evaluate our approach on an NVIDIA 1080ti GPU.}\n\n\\section{Experiments}\n\n\\begin{table}[t]\n\\begin{center}\n\\begin{tabular}{ccccccc}\n\\toprule \n \\multirow{2}*{$K$} & \\multirow{2}*{FPS}& \\multicolumn{2}{c}{DAVIS 2016 val} & \\multicolumn{2}{c}{DAVIS 2017 val} \\\\\n &    & $\\mathcal{J}$ \\& $\\mathcal{F}$ $\\uparrow$ & $\\mathcal{J}_M$ $\\uparrow$ & $\\mathcal{J}$ \\& $\\mathcal{F}$ $\\uparrow$ & $\\mathcal{J}_M$ $\\uparrow$ \\\\\n\\midrule \n   32   & 37.3 & 88.4 & 87.6 & 80.2 & 77.7 \\\\\n   64   & 36.8 & 88.9 & 88.0 & 80.9 & 78.4 \\\\\n   128  & 36.4 & 89.5 & 88.6 & 81.9 & 79.3 \\\\\n   256  & 35.5 & 89.5 & 88.5 & 82.0 & 79.4 \\\\\n\\bottomrule\n\\end{tabular}\n\\vspace{-0.4cm}\n\\end{center}\n\\caption{Ablation study on the number of bases $K$ (with $R$=4).}\n\\label{tab:ablation_k}\n\\vspace{-0.4cm}\n\\end{table}\n\n\\begin{table*}[!t]\n\\begin{center}\n\\begin{tabular}{cccccccccc}\n\\toprule \n\\multirow{2}*{Method} & \\multirow{2}*{Pub.} & \\multirow{2}*{I} & \\multirow{2}*{FPS} & \\multicolumn{3}{c}{DAVIS 2016 val} & \\multicolumn{3}{c}{DAVIS 2017 val} \\\\\n &  &  &  & $\\mathcal{J}$ \\& $\\mathcal{F}$ $\\uparrow$ & $\\mathcal{J}_M$ $\\uparrow$ & $\\mathcal{F}_M$ $\\uparrow$ & $\\mathcal{J}$ \\& $\\mathcal{F}$ $\\uparrow$ & $\\mathcal{J}_M$ $\\uparrow$ & $\\mathcal{F}_M$ $\\uparrow$ \\\\\n\\midrule\nSTM~\\cite{Oh_2019_ICCV}                             & ICCV 2019          & \\checkmark & 6         & 86.5 & 84.8 & 88.1 & 71.6 & 69.2 & 74.0 \\\\\nAFB-URR~\\cite{liang2020video}                       & NeuralPS 2020      & \\checkmark & 4         & -    & -    & -    & 74.6 & 73.0 & 76.1 \\\\\nCFBI~\\cite{yang_2020_ECCV}                          & ECCV 2020          &            & 5         & 86.1 & 85.3 & 86.9 & 74.9 & 72.1 & 77.7 \\\\\n\\midrule\nRANet~\\cite{Wang_2019_ICCV}                         & ICCV 2019          & \\checkmark & 30        & 85.5 & 85.5 & 85.4 & 65.7 & 63.2 & 68.2 \\\\\nGC~\\cite{li_2020_ECCV_GCM}                          & ECCV 2020          & \\checkmark & 25        & 86.6 & \\textbf{87.6} & 85.7 & 71.4 & 69.3 & 73.5 \\\\\nTVOS~\\cite{Zhang_2020_CVPR}                         & CVPR 2020          &            & 37        & -    & -    & -    & 72.3 & 69.9 & 74.7 \\\\\nSAT~\\cite{Chen_2020_CVPR}                           & CVPR 2020          &            & 39        & 83.1 & 82.6 & 83.6 & 72.3 & 68.6 & 76.0 \\\\\n\\midrule\n\\textbf{SWEM}     &  CVPR2022    &            & 36        & \\textbf{88.1} & 87.3 & \\textbf{89.0} & \\textbf{77.2} & \\textbf{74.5} & \\textbf{79.8} \\\\\n\\midrule\n\\midrule\nSTM~\\cite{Oh_2019_ICCV}(+\\textbf{YV})               & ICCV 2019          & \\checkmark & 11*        & 89.3 & 88.7 & 89.9 & 81.7 & 79.2 & 84.3 \\\\\nCFBI~\\cite{yang_2020_ECCV}(+\\textbf{YV})            & ECCV 2020          & \\checkmark & 5         & 89.4 & 88.3 & 90.5 & 81.9 & 79.1 & 84.6 \\\\\nEGMN~\\cite{lu_2020_ECCV}(+\\textbf{YV})              & ECCV 2020          & \\checkmark & 5         & -    & -    & -    & 82.8 & 80.2 & 85.2 \\\\\nKMN~\\cite{seong_2020_ECCV}(+\\textbf{YV})            & ECCV 2020          & \\checkmark & 8         & 90.5 & 89.5 & 91.5 & 82.8 & 80.0 & 85.6 \\\\\nSSTVOS~\\cite{duke2021sstvos} (+\\textbf{YV})         & CVPR 2021          &            & $\\sim$ 7  & - & - & - & 82.5 & 79.9 & 85.1 \\\\\nRMNet~\\cite{xie2021efficient} (+\\textbf{YV})          & CVPR 2021          & \\checkmark & 12        & 88.8 & 88.9 & 88.7 & 83.5 & 81.0 & 86.0 \\\\\nLCM~\\cite{hu2021learning}   (+\\textbf{YV})          & CVPR 2021          & \\checkmark & 9         & 90.7 & 89.9 & 91.4 & 83.5 & 80.5 & 86.5 \\\\\n\nJOINT~\\cite{mao2021joint}   (+\\textbf{YV})          & ICCV 2021          &            & 4         & - & - & - & 83.5 & 80.8 & 86.2 \\\\\nDMN~\\cite{liang2021video}   (+\\textbf{YV})          & ICCV 2021          & \\checkmark & 7         & - & - & - & 84.0 & 81.0 & 87.0 \\\\\nHMMN~\\cite{seong2021hierarchical} (+\\textbf{YV})    & ICCV 2021          & \\checkmark & 10        & 90.8 & 89.6 & 92.0 & 84.7 & 81.9 & 87.5 \\\\\nAOT~\\cite{yang2021associating} (+\\textbf{YV})       & NeuralPS 2021      & \\checkmark & 19        & 91.0 & 89.7 & 92.3 & 83.0 & 80.3 & 85.7 \\\\\n\\midrule\nSwift~\\cite{wang2021swiftnet} (+\\textbf{YV})        & CVPR 2021          & \\checkmark & 25        & 90.4 & 90.5 & 90.3 & 81.1 & 78.3 & 83.9 \\\\\nSTCN~\\cite{cheng2021stcn} (+\\textbf{YV})            & NeuralPS 2021      & \\checkmark & 26*       & \\textbf{91.6} & \\textbf{90.8} & 92.5 & \\textbf{85.4} & \\textbf{82.2} & \\textbf{88.6} \\\\\n\\midrule\n\\textbf{SWEM}(+\\textbf{YV})  &  CVPR2022 &  \\checkmark & 36        & 91.3 & 89.9 & \\textbf{92.6} & 84.3 & 81.2 & 87.4 \\\\\n\\bottomrule\n\n\\end{tabular}\n\\end{center}\n\\vspace{-0.4cm}\n\\caption{Comparisons with previous approaches on DAVIS 2016 and DAVIS 2017 validation sets. `\\textbf{+YV}' denotes training with additional videos from YouTube-VOS. `I' indicates the pre-training on image datasets. Note that our SWEM achieves results close to state-of-the-art performance at a speed of 36 FPS on a V100 GPU without IO time. Here, '*' represents the re-evaluation on our hardware for reference.}\n\\label{tab:DAVIS_quantity}\n\\vspace{-0.4cm}\n\\end{table*}\n\n\\subsection{Ablation Study}\nWe first analyze the impact of the number of bases $K$ and that of SWEM iterations $R$, which are key factors affecting the efficiency of the model. Then we investigate the effect of adaptive weights in SWEM on model performance. We directly train all models on video datasets without pre-training on images. Models are evaluated on DAVIS 2016~\\cite{perazzi2016benchmark} and DAVIS 2017 validation datasets.   \n\n\\noindent{\\textbf{The number of bases $K$.}} %The number of bases $K$ is set as 32, 64, 128, and 256 separately. \nTable~\\ref{tab:ablation_k} shows the quantitative results and inference speed under different $K$ values. The performance saturates at $K=128$. When decreasing the number of bases, the performance degraded a lot, while this does not save too much computation, as seen in the inference speed. Therefore, we choose a relatively large $K=128$ as the default setting.\n\n\\noindent{\\textbf{The number of SWEM iterations $R$.}} The number of SWEM iterations affects the efficiency and convergence of bases construction. Table~\\ref{tab:ablation_r} shows results with $R=1\\sim 7$. The inference speed is sensitive to the number of iterations. Every increase in $R$ decreases the inference speed by $1\\sim 2$ FPS. $R=4$ achieves the best trade-off between performance and efficiency. \n\n\\begin{table}[b]\n\\begin{center}\n\\begin{tabular}{ccccccc}\n\\toprule \n \\multirow{2}*{$R$} & \\multirow{2}*{FPS}& \\multicolumn{2}{c}{DAVIS 2016 val} & \\multicolumn{2}{c}{DAVIS 2017 val} \\\\\n &    & $\\mathcal{J}$ \\& $\\mathcal{F}$ $\\uparrow$ & $\\mathcal{J}_M$ $\\uparrow$ & $\\mathcal{J}$ \\& $\\mathcal{F}$ $\\uparrow$ & $\\mathcal{J}_M$ $\\uparrow$ \\\\\n\\midrule \n   1   & 41.5 & 87.7 & 87.3 & 77.9 & 75.1 \\\\\n   2   & 39.4 & 88.7 & 88.0 & 79.5 & 76.9 \\\\\n   3   & 38.3 & 88.8 & 87.9 & 80.8 & 78.1 \\\\\n   4   & 36.4 & 89.5 & 88.6 & 81.9 & 79.3 \\\\\n   5   & 34.5 & 89.1 & 88.2 & 81.2 & 78.4 \\\\\n   6   & 33.0 & 89.0 & 88.3 & 79.8 & 77.0 \\\\\n   7   & 31.8 & 88.6 & 87.8 & 79.8 & 77.1 \\\\\n\\bottomrule\n\\end{tabular}\n\\vspace{-0.4 cm}\n\\end{center}\n\\caption{Ablation study on the number of SWEM iterations $R$ (with $K$=128).}\n\\label{tab:ablation_r}\n\\vspace{-0.5 cm}\n\\end{table}\n\n\\noindent{\\textbf{Adaptive weights in SWEM.}} \nWithout using adaptive weights (Eq.~\\ref{eq:adapt_w}), our performance drops greatly (81.9\\% $\\rightarrow$ 77.6\\%) while the improvement of inference speed is subtle (36.4 FPS $\\rightarrow$ 38.4 FPS). \nFigure~\\ref{fig:ablation_w} shows the distribution of maximum matching similarities between features of the current frame and previous bases. Although SWEM with adaptive weights has fewer high similarities, it has more similarities above 0.6 than the one with fixed weights (93.4\\% v.s. 90.5\\%), guaranteeing fewer missing matches during inference. \nWe also show the qualitative comparison between two kinds of weights in Figure~\\ref{fig:ablation_wf}. Compared with SWEM with adaptive weights, that with fixed weights is more prone to missing matches, resulting in drift issues.\n\n\\begin{figure}[b]\n\\begin{center}\n\\includegraphics[width=1.0\\linewidth]{figures/swem_adaptive_fixed.pdf}\n\\end{center}\n\\vspace{-0.4 cm}\n   \\caption{The distribution of maximum matching similarities between current frame features and previous bases.}\n\\label{fig:ablation_w}\n\\vspace{-0.5 cm}\n\\end{figure}\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=1\\linewidth]{figures/adaptive_fixed2.pdf}\n\\end{center}\n\\vspace{-0.4 cm}\n   \\caption{The qualitative comparison between adaptive-weights and fixed-weights. SWEM with fixed weights (first line for each sample) is struggled to distinguish similar objects while {the one with} adaptive weights (second line) {is} competent for this problem. Weights are visualized at the third line for each sample, where the brighter the pixel, the harder it is. As for the person in the middle, the corresponding background bases pay more attention to objects (the remaining person) which are similar to the target.}\n\\label{fig:ablation_wf}\n\\vspace{-0.5 cm}\n\\end{figure}\n\n\\subsection{Comparison with SOTA}\n\n\\begin{table}[t]\n\\begin{center}\n\\begin{tabular}{cccccc}\n\\toprule \n  \\multirow{2}*{Method} &\\multirow{2}*{$\\mathcal{G}$} & \\multicolumn{2}{c}{seen} & \\multicolumn{2}{c}{unseen} \\\\\n & & $\\mathcal{J}_M$ $\\uparrow$& $\\mathcal{F}_M$ $\\uparrow$ & $\\mathcal{J}_M$ $\\uparrow$ & $\\mathcal{F}_M$ $\\uparrow$\\\\\n\\midrule\n\nSTM~\\cite{Oh_2019_ICCV}                & 79.4 & 79.7 & 84.2 & 72.8 & 80.9 \\\\\nAFB-URR~\\cite{liang2020video}          & 79.6 & 78.8 & 83.1 & 74.1 & 82.6 \\\\\nEGMN~\\cite{lu_2020_ECCV}               & 80.2 & 80.7 & 85.1 & 74.0 & 80.9 \\\\\nKMN~\\cite{seong_2020_ECCV}             & 81.4 & 81.4 & 85.6 & 75.3 & 83.3 \\\\\nCFBI~\\cite{yang_2020_ECCV}             & 81.4 & 81.1 & 85.8 & 75.3 & 83.4 \\\\\nRMNet~\\cite{xie2021efficient}            & 81.5 & 82.1 & 85.7 & 75.7 & 82.4 \\\\\nSSTVOS~\\cite{duke2021sstvos}           & 81.7 & 81.2 & 85.9 & 76.0 & 83.9 \\\\\nLCM~\\cite{hu2021learning}              & 82.0 & 82.2 & 86.7 & 75.7 & 83.4 \\\\\nDMN~\\cite{liang2021video}              & 82.5 & 82.5 & 86.9 & 76.2 & 84.2 \\\\\nHMMN~\\cite{seong2021hierarchical}      & 82.6 & 82.1 & 87.0 & 76.8 & 84.6 \\\\\nJOINT~\\cite{mao2021joint}              & 83.1 & 81.5 & 85.9 & \\textbf{78.7} & 86.5 \\\\\nAOT~\\cite{yang2021associating}         & \\textbf{83.7} & \\textbf{82.5} & \\textbf{87.5} & 77.9 & \\textbf{86.7} \\\\\n\\midrule\nSAT*~\\cite{Chen_2020_CVPR}             & 63.6 & 67.1 & 70.2 & 55.3 & 61.7 \\\\\nTVOS*~\\cite{Zhang_2020_CVPR}           & 67.8 & 67.1 & 69.4 & 63.0 & 71.6 \\\\\nFRTM*~\\cite{Robinson_2020_CVPR}        & 72.1 & 72.3 & 76.2 & 65.9 & 74.1 \\\\\nGC*~\\cite{li_2020_ECCV_GCM}            & 73.2 & 72.6 & 75.6 & 68.9 & 75.7 \\\\\nSwift*~\\cite{Bhat_2020_ECCV_GCM}       & 77.8 & 77.8 & 81.8 & 72.3 & 79.5 \\\\\nSTCN*~\\cite{cheng2021stcn}             & \\textbf{83.0} & 81.9 & 86.5 & \\textbf{77.9} & \\textbf{85.7} \\\\\n\\midrule\n\\textbf{SWEM*}                         & 82.8 & \\textbf{82.4} & \\textbf{86.9} & 77.1 & 85.0 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\vspace{-0.4 cm}\n\\caption{Comparison with state-of-the-art methods on the YouTube-VOS 2018 validation dataset. We report all of the mean Jaccard ($\\mathcal{J}$), the boundary ($\\mathcal{F}$) scores for seen and unseen categories, and the overall scores $\\mathcal{G}$. Besides, we use `*' to indicate those methods with an inference speed $>20$ FPS. Note SSTVOS, JOINT and AOT are transformer-based methods.}\n\\label{tab:YTVOS}\n\\vspace{-0.5 cm}\n\\end{table}\n\n\\noindent{\\textbf{Datasets and evaluation metrics.}} We report the results on the DAVIS 2016, DAVIS 2017, and YouTube-VOS 2018 datasets using region similarity $\\mathcal{J}$, contour accuracy $\\mathcal{F}$, and their mean as metrics. \n\n\\noindent{\\textbf{DAVIS 2016 and DAVIS 2017.}} Table~\\ref{tab:DAVIS_quantity} presents the quantitative comparisons with recent state-of-the-art video segmentation methods on the DAVIS 2016 and 2017 validation sets. Our method achieves the best $\\mathcal{J}$ \\& $\\mathcal{F}$ on both datasets among methods without pre-training on image datasets or additional video datasets. In detail, our method outperforms SAT~\\cite{Chen_2020_CVPR}, which runs at a similar speed (39 FPS) with ours, with a large margin on DAVIS2017 (+4.9\\% $\\mathcal{J}$ \\& $\\mathcal{F}$ score). Under the setting of using additional training data from YouTube-VOS (\\textbf{+YV}), our SWEM surpasses all other top-performing methods. Note that STCN~\\cite{cheng2021stcn} uses a growing memory bank during inference, which harms the long-term segmentation while SWEM keeps the fixed number of bases and has stable computation complexity. SSTVOS~\\cite{duke2021sstvos}, AOT~\\cite{yang2021associating} and JOINT~\\cite{mao2021joint} adopt transformer~\\cite{vaswani2017attention} backbones which are more powerful than ResNet-50. Hierarchical matching used in HMMN~\\cite{seong2021hierarchical} also affects the segmentation efficiency deeply. We re-evaluated STM and STCN on our hardware and software environment for reference. Our SWEM is capable of achieving an inference speed of 36 FPS on the V100 GPU and 27 FPS on the 1080ti GPU. \n\n\\noindent{\\textbf{YouTube-VOS 2018.}} We make a comparison between our SWEM with previous methods on the YouTube-VOS 2018 via the official evaluation server in Table~\\ref{tab:YTVOS}. Note that although SWEM leverages the original ResNet-50 backbone and the identical decoder as STM~\\cite{Oh_2019_ICCV}, it achieves the 82.8\\% overall score which is very close to the state-of-the-art results. Besides, we provide more qualitative and quantitative comparisons in the Supplementary Material.\n\n \n\n\\section{Conclusion}\nIn this paper, we proposed a fast yet robust model for semi-supervised video object segmentation dubbed Sequential Weighted Expectation-Maximum (SWEM) network, which is capable of constructing compact target templates with low redundancy for pixel-wise matching. The weighted EM algorithm is used to construct bases for foreground and background features separately and reduce the intra-frame redundancy. We also proposed to compute adaptive weights instead of fixed weights when generating bases, which forces bases to pay more attention to hard samples, so as to reduce the missing match. We extended the weighted EM to sequential weighted EM to process sequential data and completely reduce the inter-frame redundancy. Overall, our method achieves a performance close to the state-of-the-art on VOS at 36 FPS.\n\n\\section*{Acknowledgement}\n\\vspace{-0.0 cm}\nThis work was supported by NSFC project Grant No.U1833101, SZSTI Grant No.JCYJ20190809172201639 and WDZC20200820200655001, the Joint Research Center of Tencent and Tsinghua.\n\n{\\small\n}\n\n\\clearpage\n\\appendix\n\\renewcommand{\\appendixname}{Appendix~\\Alph{section}}\n\\section{Overview}\nWe provide the supplementary results for our main paper, which are organized as follows. Section~\\ref{sec:TraingDetails} introduces more training details not mentioned in the original paper. In Section~\\ref{sec:ablation}, we provide more ablation analysis on the number of permutation-invariant features $L$ (i.e.,  Eq.(12) in the main paper) and the temperature hyperparameter $\\tau$ in the similarity calculation (i.e. Eq.~\\ref{eq:matching_tau} in this supplementary material ). In Section~\\ref{sec:eff}, we provide the comparison between the proposed SWEM and STCN~\\cite{cheng2021stcn} on both efficiency and performance with a commonly used GPU, NVIDIA GTX 1080ti. We also report SWEM performance on YouTube-VOS 2019 validation set in Section~\\ref{sec:YTVOS19}. In Section~\\ref{sec:TrainingData}, we analyze the effect of training with different datasets. Finally, we provide qualitative results in Section~\\ref{sec:qualitative}.\n\n\\section{More Training Details}\n\\label{sec:TraingDetails}\nWe set channel dimension as $C=128$ for the \\textbf{Key}  and $C'=512$ for the \\textbf{Value} feature, which are the same as those in STM~\\cite{Oh_2019_ICCV}. \nWe first train SWEM on image datasets for 500k iterations, including COCO~\\cite{cocolin2014microsoft}, MSRA10K~\\cite{msra10kChengPAMI}, ECSSD~\\cite{ecssdshi2015hierarchical}, PASCOL-S~\\cite{pascalsli2014secrets} and PASCOL-VOC2012~\\cite{pascolvoceveringham2010pascal}, and then on video datasets for 200k iterations, including DAVIS~\\cite{pont20172017}, and the YouTube-VOS~\\cite{xu2018youtube}. When using a single NVIDIA V100 GPU, the image training process takes about 2 days while the video training process needs about 1 day.\nFollowing STM and STCN~\\cite{cheng2021stcn}, \nwe sample video clips using an 11-times (3471 v.s 300) higher probability for YouTube-VOS than DAVIS 2017 training set. Our SWEM takes about 8GB GPU memory with batch size 4, which reveals its memory-efficient during training and testing.\n\n\\begin{algorithm}\n\\caption{Calculation of Permutation-Invariant Affinity Features}\\label{alg:S}\n\\LinesNumbered\n\\KwIn{$\\mathcal{K}^{fg}$:$K \\times HW$, $\\mathcal{K}^{bg}$:$K \\times HW$ }\n\\KwOut{$\\mathbf{S}$:$L \\times HW$}\n\n\\textcolor{blue}{function} GetPermutInvariantFeatures($\\mathcal{K}^{fg}$, $\\mathcal{K}^{bg}$): \\\n\\quad \\textcolor{cyan}{\\tcp{Get top-L affinities for each pixel.}}\n\\quad $\\tilde{\\mathcal{K}}^{fg/bg}$=torch.topk($\\mathcal{K}^{fg/bg}$, k=$L$, dim=$1$) \\\\\n\\quad \\textcolor{cyan}{\\tcp{Initialize $\\mathbf{S}^{fg}$ and $\\mathbf{S}^{bg}$  as zeros.} }\n\\quad $\\mathbf{S}^{fg/bg}$=torch.zeros\\_like($\\tilde{\\mathcal{K}}^{fg/bg}$) \\\\\n\\quad $\\mathbf{S}^{fg/bg}_{0}$=$\\tilde{\\mathcal{K}}^{fg/bg}_{0}$ \\\\\n\\quad for $l$ in range($1$, $L$): \\\\\n\\quad \\quad $\\mathbf{S}^{fg/bg}_l$=$\\mathbf{S}^{fg/bg}_{l-1}$+$\\tilde{\\mathcal{K}}^{fg/bg}_{l}$ \\\\\n\\quad $\\mathbf{S}$=$\\mathbf{S}^{fg}$/($\\mathbf{S}^{fg}$+$\\mathbf{S}^{bg}$) \\\\\n\\textcolor{blue}{return} $\\mathbf{S}$ \n\\end{algorithm}\n\n\\section{More ablation analysis}\n\\label{sec:ablation}\n\n\\noindent{\\textbf{The number of permutation-invariant features $L$.}} Recall that we introduced a permutation-invariant operation to take advantage of affinities between pixels in the current frame and memory features in Section 4.3 of the main paper. The calculation of affinity features is detailed as Eq.(12) in the main paper, which is repeated as:\n\\begin{equation}\n\\mathbf{S}^{(t)}_{nl} = \\frac{\\sum_{j\\in \\text{topl}(\\mathcal{K}^{fg, (t)}_n)} \\mathcal{K}^{fg, (t)}_{nj}}{\\sum_{j\\in \\text{topl}(\\mathcal{K}^{fg, (t)}_n)} \\mathcal{K}^{fg, (t)}_{nj} + \\sum_{j\\in \\text{topl}(\\mathcal{K}^{bg, (t)}_n)} \\mathcal{K}^{bg, (t)}_{nj}},\n\\label{eq:matching_s}\n\\end{equation}\nwhere $l=1,2,...,L$, $L \\leq K$, and $K$ is the number of foreground or background base features.\n\nDefine $\\mathcal{K}^{fg}$ and $\\mathcal{K}^{bg}$ are affinity matrices between frame features and foreground-background base features. Then the permutation-invariant features with $L$ channels can be calculated with Algorithm~\\ref{alg:S} in PyTorch~\\cite{paszke2019pytorch} style. $L$ is a hyperparameter to control the number of segmentation clues channels and the computation complexity. Table~\\ref{tab:ablation_l} shows the results when setting $L$ as 8, 16, 32, 64 and 128 separately. The segmentation performance is robust to various $L$ except $L=8$, which brings a  significant performance reduction. When increasing $L$ from 64 to 128, the inference speed is dropped by 3.4 FPS while the performance is not improved. \n\\begin{table}[t]\n\\begin{center}\n\\begin{tabular}{ccccccc}\n\\toprule \n \\multirow{2}*{$L$} & \\multirow{2}*{FPS}& \\multicolumn{2}{c}{DAVIS 2016 val} & \\multicolumn{2}{c}{DAVIS 2017 val} \\\\\n &    & $\\mathcal{J}$ \\& $\\mathcal{F}$ $\\uparrow$ & $\\mathcal{J}_M$ $\\uparrow$ & $\\mathcal{J}$ \\& $\\mathcal{F}$ $\\uparrow$ & $\\mathcal{J}_M$ $\\uparrow$ \\\\\n\\midrule \n   8   & 40.8 & 88.5 & 87.5 & 81.0 & 78.6 \\\\\n   16   & 39.1 & 89.5 & 88.6 & 81.4 & 78.8 \\\\\n   32  & 37.5 & \\textbf{89.6} & \\textbf{88.8} & 81.5 & 78.8 \\\\\n   64  & 36.4 & 89.5 & 88.6 & \\textbf{81.9} & \\textbf{79.3} \\\\\n   128  & 33.0 & 89.6 & 88.6 & 81.6 & 79.0 \\\\\n\\bottomrule\n\\end{tabular}\n\\vspace{-0.0 cm}\n\\end{center}\n\\caption{Ablation study on the number of permutation-invariant features $L$.}\n\\label{tab:ablation_l}\n\\vspace{-0.3 cm}\n\\end{table}\n\n\\begin{table}[b]\n\\begin{center}\n\\begin{tabular}{cccccc}\n\\toprule \n \\multirow{2}*{$\\tau$} & \\multicolumn{2}{c}{DAVIS 2016 val} & \\multicolumn{2}{c}{DAVIS 2017 val} \\\\\n & $\\mathcal{J}$ \\& $\\mathcal{F}$ $\\uparrow$ & $\\mathcal{J}_M$ $\\uparrow$ & $\\mathcal{J}$ \\& $\\mathcal{F}$ $\\uparrow$ & $\\mathcal{J}_M$ $\\uparrow$ \\\\\n\\midrule\n   0.01   & 88.5 & 87.6 & 79.6 & 77.1 \\\\\n   0.02  & 89.4 & 88.5 & 80.5 & 78.1 \\\\\n   0.05  & \\textbf{89.5} & \\textbf{88.6} & \\textbf{81.9} & \\textbf{79.3} \\\\\n   0.1  & 89.5 & 88.5 & 81.7 & 79.0 \\\\\n   0.2 & 87.5 & 87.2 & 72.8 & 69.9 \\\\\n   1 & 82.1 & 82.6 & 66.7 & 63.9 \\\\\n\\bottomrule\n\\end{tabular}\n\\vspace{-0.0 cm}\n\\end{center}\n\\caption{Ablation study on the temperature hyperparameter $\\tau$.}\n\\label{tab:ablation_t}\n\\vspace{-0.3 cm}\n\\end{table}\n\n\\noindent{\\textbf{The temperature hyperparameter $\\tau$.}}  The similarity in this work is calculated by:\n\\begin{equation}\n \\mathcal{K}(\\mathbf{a}, \\mathbf{b})=\\exp(\\frac{\\mathbf{a}\\mathbf{b}^{\\top}/\\tau}{\\|\\mathbf{a}\\|\\cdot \\|\\mathbf{b}\\|}),\n\\label{eq:matching_tau}\n\\end{equation}\nwhere the temperature hyperparameter $\\tau$ controls the range of similarity measures. Table~\\ref{tab:ablation_t} reports the segmentation performance on the DAIVS 2016 and DAVIS 2017 validation sets with different $\\tau$. The segmentation performance is sensitive to $\\tau$. Too large or too small $\\tau$ are both not appropriate, especially the large one. The overall $\\mathcal{J}\\&\\mathcal{F}$ on the DAVIS 2017 validation set is dropped from 81.9 to 66.7 when changing $\\tau$ from 0.05 to 1. As contrast, $\\tau=0.1$ and $\\tau=0.05$ 0.05 is able to achieve satisfying results.\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=1.\\linewidth]{figures/efficiency_swem_vs_stcn_davis16.pdf}\n\\end{center}\n   \\caption{The comparison between STCN and proposed SWEM on per-frame inference time and number of stored features in a single-object segmentation scenario. SWEM has a fixed size of memory features and stable inference time. Methods are evaluated with an NVIDIA GTX 1080ti GPU.}\n\\label{fig:Efficiency_davis16}\n\\end{figure}\n\n\\begin{figure}[b]\n\\begin{center}\n\\includegraphics[width=1.\\linewidth]{figures/efficiency_swem_vs_stcn_davis17.pdf}\n\\end{center}\n   \\caption{The comparison between STCN and proposed SWEM on inference speed and performance in a multi-object segmentation scenario. We set different memory interval (T=5, 10, 15, 30, $+\\infty$) for STCN. The area of the circle represents the number of per-frame memory features involved in matching for each object. SWEM achieves a better trade-off between performance and efficiency than STCN.}\n\\label{fig:Efficiency_davis17}\n\\end{figure}\n\n\\section{Efficiency comparison with STCN}\n\\label{sec:eff}\nSTCN~\\cite{cheng2021stcn} achieves state-of-the-art performance while maintaining an acceptable inference speed (26 FPS with an NVIDIA V100 GPU). However, Similar to STM~\\cite{Oh_2019_ICCV}, STCN still stores template features every T frames endlessly.\nWe compare the efficiency between STCN and proposed SWEM with an NVIDIA GTX 1080ti GPU, which is a cheap and commonly used platform. \n\nFigure~\\ref{fig:Efficiency_davis16} shows the per-frame inference time and memory capacity of STCN and SWEM in a single-object segmentation scenario. It clearly reflects the negative effects of growing memory features on the efficiency and storage during the long-term segmentation. Compared to STCN, our SWEM stores far fewer memory features and has a stable inference time cost.\n\nFigure~\\ref{fig:Efficiency_davis17} shows the efficiency and performance comparison between STCN and SWEM in a multi-object segmentation scenario. The area of the circle represents the number of per-frame per-object memory features. We also set different memory interval (T=5, 10, 15, 30, $+\\infty$) for STCN. When $T=10$, STCN gets a similar inference speed and performance with SWEM, but with much more memory features. When $T=15$, the performance has a significant drop. If STCN only stores features of the first frame ($T=+\\infty$), the overall performance is dropped from 85.4 to 74.1. \n\nThe above results demonstrate the superiority of SWEM in terms of efficiency and performance with common hardware.\n\n\\begin{table}[b]\n\\begin{center}\n\\begin{tabular}{cccccc}\n\\toprule \n  \\multirow{2}*{Method} &\\multirow{2}*{$\\mathcal{G}$} & \\multicolumn{2}{c}{seen} & \\multicolumn{2}{c}{unseen} \\\\\n & & $\\mathcal{J}_M$ $\\uparrow$& $\\mathcal{F}_M$ $\\uparrow$ & $\\mathcal{J}_M$ $\\uparrow$ & $\\mathcal{F}_M$ $\\uparrow$\\\\\n\\midrule\nSTM~\\cite{Oh_2019_ICCV}                & 79.2 & 79.6 & 83.6 & 73.0 & 80.6 \\\\\nCFBI~\\cite{yang_2020_ECCV}             & 81.0 & 80.6 & 85.1 & 75.2 & 83.0 \\\\\nLWL~\\cite{Bhat_2020_ECCV_GCM}          & 81.0 & 79.6 & 83.8 & 76.4 & 84.2 \\\\\nSSTVOS~\\cite{duke2021sstvos}           & 81.8 & 80.9 & 85.3 & 76.6 & 84.4 \\\\\nHMMN~\\cite{seong2021hierarchical}      & 82.5 & 81.7 & 86.1 & 77.3 & 85.0 \\\\\nJOINT~\\cite{mao2021joint}              & 82.8 & 80.8 & 84.8 & \\textbf{79.0} & 86.6 \\\\\nAOT~\\cite{yang2021associating}         & \\textbf{83.6} & \\textbf{82.2} & \\textbf{86.9} & 78.3 & \\textbf{86.9} \\\\\n\\midrule\n\\textbf{SWEM*}                         & 82.6 & 82.0 & 86.1 & 77.2 & 85.2 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\vspace{-0.0 cm}\n\\caption{Comparison with state-of-the-art methods on the YouTube-VOS 2019 validation dataset. We report all of the mean Jaccard ($\\mathcal{J}$), the boundary ($\\mathcal{F}$) scores for seen and unseen categories, and the overall scores $\\mathcal{G}$. Besides, we use `*' to indicate those methods with an inference speed $>20$ FPS. Note SSTVOS, JOINT and AOT are transformer-based methods.}\n\\label{tab:YTVOS19}\n\\vspace{-0.0 cm}\n\\end{table}\n\n\\begin{table*}[t]\n\\begin{center}\n\\begin{tabular}{ccccccc}\n\\toprule\n \\multirow{2}*{\\diagbox[width=10em]{Training Data}{Method}} & \\multicolumn{3}{c}{DAVIS 2016 val $\\mathcal{J}$ \\& $\\mathcal{F}$} & \\multicolumn{3}{c}{DAVIS 2017 val $\\mathcal{J}$ \\& $\\mathcal{F}$} \\\\\n & STM~\\cite{Oh_2019_ICCV} & STCN~\\cite{cheng2021stcn} & \\textbf{SWEM} & STM~\\cite{Oh_2019_ICCV} & STCN~\\cite{cheng2021stcn} & \\textbf{SWEM}\\\\\n\\midrule\nDAVIS                       & 74.5 & 84.6 & \\textbf{88.1}      & 48.6 & 71.2 & \\textbf{77.2} \\\\\nDAVIS + YTVOS               & 88.2 & \\textbf{89.8} & 89.5      & 80.0 & 81.1 & \\textbf{81.9} \\\\\nImage + (DAVIS + YTVOS)     & 89.8 & 91.2 & \\textbf{91.3}      & 81.6 & \\textbf{84.5} & 84.3 \\\\\n\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\caption{Analysis of training on different datasets. We train STM~\\cite{Oh_2019_ICCV}, STCN~\\cite{cheng2021stcn} and proposed SWEM under three different conditions: 1) only training on video data DAVIS, 2) training on video datasets DAVIS and YouTube-VOS, and 3) pretraining on image data first and then training on DAVIS and YouTube-VOS.}\n\\label{tab:ablation_data}\n\\end{table*}\n\n\\section{Comparisons on YouTube-VOS 2019}\n\\label{sec:YTVOS19}\nTo further evaluate the effectiveness of our method, we\nalso carry out experiments on the YouTube-VOS 2019 dataset.\nNote that the YouTube-VOS 2019 validation set contains 507 videos, while the 2018 version has 474 videos.\nSpecifically, our SWEM is trained on the YouTube-VOS 2019 training set and evaluated on the validation set through the official evaluation server. We compare SWEM with recent state-of-the-art methods in Table~\\ref{tab:YTVOS19}, including transformer-based methods SSTVOS~\\cite{duke2021sstvos}, JOINT~\\cite{mao2021joint} and AOT~\\cite{yang2021associating}. SWEM achieves the 82.6\\% overall score while maintaining the real-time inference speed.\n\n\\section{Impact of Training Data}\n\\label{sec:TrainingData}\nTable ~\\ref{tab:ablation_data} presents the performance of various models on DAVIS 2016 and DAVIS 2017 validation sets with different training datasets. \nIt can be seen that both pre-training on image data and using additional YouTube-VOS video data can boost performance. Note that our SWEM trained only on DAVIS data outperforms STM~\\cite{Oh_2019_ICCV} and STCN~\\cite{cheng2021stcn} under the same setting with a large margin. Furthermore, STM~\\cite{Oh_2019_ICCV} obtains $48.6\\%$ $\\mathcal{J}\\&\\mathcal{F}$ on DAVIS 2017 validation set when it is trained only with DAVIS data. Authors claim it is due to the high risk of over-fitting on small datasets. However, STCN gets a much higher overall performance $71.2\\%$ $\\mathcal{J}\\&\\mathcal{F}$ than STM. We argue the reason for this performance gap is that the features involved in matching in STM come from different feature spaces, whereas those features in STCN and SWEM are from the same feature spaces. Moreover, the direct usage of affinity features also accelerates the convergence of SWEM. SWEM gets similar performance with STCN when adopting additional YouTube-VOS video data or image data. It is worth noting that SWEM stores fixed-size (512) memory features which are much fewer than those in STCN. As an inference, a single frame with a size $480\\times 864$ produces 1,620 features.\n\n\\section{Qualitative Results}\n\\label{sec:qualitative}\nWe further provide the qualitative comparison between STM~\\cite{Oh_2019_ICCV} and our SWEM on the validation set of the DAVIS 2017 and YouTube-VOS in Figure~\\ref{fig:vis}, which demonstrates the superiority of our SWEM. \n\n\\begin{figure*}[h]\n\\begin{center}\n\\includegraphics[width=0.95\\linewidth]{figures/vis.pdf}\n\\end{center}\n\\vspace{-0.0 cm}\n   \\caption{The qualitative comparison between STM~\\cite{Oh_2019_ICCV} and our SWEM on the DAVIS 2017 and YouTube-VOS. The obvious failure segmentation is indicated by yellow bounding boxes. Our SWEM is robust with rapid motion and similar distractors.}\n\\label{fig:vis}\n\\vspace{-0.0 cm}\n\\end{figure*}\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2007.08270v1.tex",
        "arXiv-2109.11404v1.tex",
        "arXiv-2208.10128v1.tex"
    ],
    "group_id": "group_107",
    "response": "### Title: Advances in Video Object Segmentation (VOS) Using Memory Networks and Adaptive Clustering Techniques\n\n### Introduction\n\nVideo Object Segmentation (VOS) is a critical task in computer vision that involves tracking and segmenting target objects at the pixel level throughout a video sequence. This task is particularly challenging due to the dynamic nature of objects, including their appearance changes, occlusions, and rapid motion. VOS can be categorized into semi-supervised and unsupervised approaches, with semi-supervised VOS being the focus of this summary. In semi-supervised VOS, the ground truth segmentation mask of the target object is provided in the first frame, and the model must predict the segmentation masks for subsequent frames. This task has seen significant advancements in recent years, with the introduction of space-time memory networks (STM) as a promising solution. However, STM, despite its success, has inherent limitations when applied to VOS, primarily due to its non-local nature, which does not align well with the predominantly local characteristics of VOS. This mismatch poses challenges in handling occlusions and accurately predicting object boundaries, which are critical for high-quality segmentation.\n\nCurrent progress in VOS has been marked by the development of various memory-based and clustering techniques that aim to address these challenges. These methods leverage the concept of Query, Key, and Value (QKV) to store and retrieve information from past frames, aiding in the prediction of object masks in the current frame. However, the non-local matching in STM can lead to incorrect matches, especially when multiple objects in the query frame resemble the target object. Additionally, the quality of ground truth masks near object boundaries often degrades the performance of VOS models, making it essential to refine these boundaries during training.\n\nThis summary will focus on three recent papers that have made significant contributions to the field of semi-supervised VOS. Each paper introduces innovative techniques to improve the performance and efficiency of VOS models. The first paper proposes a Kernelized Memory Network (KMN) that addresses the non-locality issue of STM by incorporating a Gaussian kernel for local matching. The second paper introduces a Hierarchical Memory Matching Network (HMMN) that builds upon the KMN by further refining the pre-training process using the Hide-and-Seek strategy. The third paper presents a Sequential Weighted Expectation-Maximization (SWEM) network, which reduces both inter-frame and intra-frame redundancy through a novel approach that merges sequential and weighted EM algorithms.\n\n### Main Content of Each Paper\n\n#### Paper 1: Kernelized Memory Network for Video Object Segmentation\n\nThe first paper, \"Kernelized Memory Network for Video Object Segmentation,\" proposes a Kernelized Memory Network (KMN) to improve the performance of VOS by addressing the non-locality issue of STM. The authors argue that while STM is designed to handle non-local information, VOS is primarily a local task, where the target object in the query frame usually appears in the same or a nearby location as in the memory frames. To mitigate this mismatch, KMN employs a Gaussian kernel to control the non-local matching between the query and memory frames, ensuring that only the most relevant features are used for prediction.\n\nBefore being trained on real videos, KMN undergoes pre-training on static images using a Hide-and-Seek strategy. This strategy involves hiding random patches of the target object during training to improve the robustness of the model to occlusions and to refine the boundaries of the object segment. The Hide-and-Seek strategy was initially developed for weakly supervised object localization but is adapted here for VOS. This approach not only enhances the model's ability to handle occlusions but also generates cleaner and more accurate boundaries for the object segment.\n\nThe KMN architecture is similar to STM, with two ResNet50 models used to extract Key and Value features from the query and memory frames. The Key and Value features are embedded via a convolutional layer from the res4 feature, which has a $1/16$ resolution relative to the input image. The memory frames are independently embedded and concatenated along the temporal dimension, while the query frame's embedded features are directly used for memory reading. The main innovation in KMN lies in the memory read operation, which includes both Query-to-Memory and Memory-to-Query matching, controlled by a Gaussian kernel.\n\nThe Hide-and-Seek strategy during pre-training significantly improves the performance of KMN, as demonstrated by experiments on DAVIS 2016, DAVIS 2017, and YouTube-VOS 2018 datasets. The results show that KMN surpasses the state-of-the-art methods, even those that use online learning strategies, and achieves a runtime of 0.12 seconds per frame on the DAVIS 2016 validation set. This paper highlights the importance of local matching in VOS and the benefits of using the Hide-and-Seek strategy for pre-training.\n\n#### Paper 2: Hierarchical Memory Matching Network for Video Object Segmentation\n\nThe second paper, \"Hierarchical Memory Matching Network for Video Object Segmentation,\" builds upon the KMN by introducing a Hierarchical Memory Matching Network (HMMN). HMMN further refines the pre-training process using the Hide-and-Seek strategy and incorporates a hierarchical structure in the memory read operation. The hierarchical structure allows for more efficient and accurate retrieval of memory features by dividing the memory into multiple levels based on the temporal distance from the current frame. This hierarchical approach ensures that the most relevant frames are given more weight during the matching process, improving the robustness of the model.\n\nHMMN also employs a top-$k$ guided memory matching module at the res2 stage, reducing the number of $k$ to $k/4$ compared to the res3 stage. This reduction helps in managing the memory more efficiently, preventing the model from becoming overly complex. Additionally, the decoder architecture is modified to include permutation-invariant operations that help in generating segmentation clues from correlations between the current frame and memory bases. This ensures that the model can handle the dynamic nature of objects more effectively.\n\nThe Hide-and-Seek strategy during pre-training is further analyzed, showing that it generates clean boundaries and improves the robustness of the model to occlusions. The authors conduct experiments on DAVIS 2016, DAVIS 2017, and YouTube-VOS 2018 datasets, demonstrating that HMMN outperforms all previous offline-learning methods, even those that use additional YouTube-VOS training data. The results indicate that HMMN achieves state-of-the-art performance with a runtime of 0.10 seconds per frame on the DAVIS 2017 validation set.\n\n#### Paper 3: SWEM: Towards Real-Time Video Object Segmentation with Sequential Weighted Expectation-Maximization\n\nThe third paper, \"SWEM: Towards Real-Time Video Object Segmentation with Sequential Weighted Expectation-Maximization,\" introduces a Sequential Weighted Expectation-Maximization (SWEM) network to achieve real-time VOS while maintaining high performance. SWEM leverages the Expectation-Maximization Attention (EMA) algorithm to construct a compact set of bases for memory features, reducing both inter-frame and intra-frame redundancy. Unlike previous methods that only detect feature redundancy between frames, SWEM merges both intra-frame and inter-frame similar features, ensuring that the model can handle rapid motion and appearance changes more effectively.\n\nSWEM uses adaptive weights for frame features, which allows the model to represent hard samples more accurately and improve the discrimination of templates. The authors argue that this approach is more flexible and can adapt to different scenarios, making the model robust to various challenges in VOS. The network structure includes a ResNet-50 backbone for feature extraction and a ResNet-18 for value feature extraction, with all batch normalization layers frozen. The SWEM algorithm is implemented in a recursive manner, where the numerator and denominator are updated sequentially based on the current frame and previous bases.\n\nThe authors conduct extensive experiments on DAVIS 2016, DAVIS 2017, and YouTube-VOS 2018 datasets, demonstrating that SWEM achieves high efficiency (36 FPS) and high performance (84.3% $\\mathcal{J}\\&\\mathcal{F}$ on DAVIS 2017 validation dataset) without sacrificing accuracy. The results show that SWEM outperforms previous methods, especially those that use growing memory banks, which can lead to increased computational complexity and memory usage. SWEM maintains a fixed number of template features in memory, ensuring stable inference complexity and making it suitable for real-time applications.\n\n### Commonalities and Innovations\n\nAll three papers focus on improving the performance and efficiency of VOS models by addressing the non-locality issue of STM and reducing redundancy in memory features. They share a common goal of achieving accurate and robust segmentation while maintaining real-time inference speeds. However, each paper introduces unique innovations to tackle these challenges.\n\n- **KMN** introduces a Gaussian kernel to control the non-local matching between query and memory frames, ensuring that only the most relevant features are used for prediction. The Hide-and-Seek strategy during pre-training generates clean boundaries and improves the robustness of the model to occlusions.\n- **HMMN** builds upon KMN by introducing a hierarchical structure in the memory read operation, allowing for more efficient and accurate retrieval of memory features. The top-$k$ guided memory matching module at the res2 stage further reduces redundancy and computational complexity.\n- **SWEM** leverages the EMA algorithm to construct a compact set of bases for memory features, reducing both inter-frame and intra-frame redundancy. Adaptive weights for frame features make the model more flexible and robust to hard samples, ensuring that the segmentation performance is not compromised.\n\n### Comparison of Results\n\nThe three papers report their results on commonly used VOS benchmarks, including DAVIS 2016, DAVIS 2017, and YouTube-VOS 2018 datasets. The performance metrics used are region similarity ($\\mathcal{J}$), contour accuracy ($\\mathcal{F}$), and their mean ($\\mathcal{G}$). Table \\ref{tab:results_comparison} summarizes the performance and runtime of the proposed methods and compares them with other state-of-the-art approaches.\n\n| Method       | DAVIS 2016 val $\\mathcal{G}$ | DAVIS 2017 val $\\mathcal{G}$ | YouTube-VOS 2018 val $\\mathcal{G}$ | Runtime (s/frame) |\n|--------------|----------------------------|----------------------------|------------------------------------|-------------------|\n| STM          | 86.5                        | 71.6                       | 79.4                              | 0.16              |\n| KMN          | 87.6                        | 76.0                       | 81.4                              | 0.12              |\n| HMMN         | 88.1                        | 84.7                       | 82.8                              | 0.10              |\n| SWEM         | 88.1                        | 77.2                       | 82.6                              | 0.12              |\n| STM (+YV)    | 89.3                        | 81.7                       | 89.4                              | 0.16              |\n| KMN (+YV)    | 90.5                        | 82.8                       | 84.3                              | 0.12              |\n| HMMN (+YV)   | 91.3                        | 84.7                       | 85.6                              | 0.10              |\n| SWEM (+YV)   | 91.3                        | 84.3                       | 82.6                              | 0.12              |\n\n#### Table \\ref{tab:results_comparison}: Performance and Runtime Comparison of Proposed Methods\n\nThe results show that KMN, HMMN, and SWEM outperform STM and other state-of-the-art methods on both DAVIS and YouTube-VOS datasets. When trained on additional YouTube-VOS data, all three methods achieve even higher performance, with HMMN and SWEM surpassing KMN. The runtime of these methods is also competitive, with KMN and SWEM achieving a runtime of 0.12 seconds per frame, and HMMN achieving a slightly faster runtime of 0.10 seconds per frame.\n\n### Conclusion\n\nThe three papers discussed in this summary present innovative approaches to improve the performance and efficiency of semi-supervised VOS models. KMN addresses the non-locality issue of STM by incorporating a Gaussian kernel for local matching and using the Hide-and-Seek strategy for pre-training. HMMN further refines the pre-training process and introduces a hierarchical structure for memory read operations, ensuring that the most relevant frames are given more weight during the matching process. SWEM leverages the EMA algorithm to construct compact bases for memory features, reducing both inter-frame and intra-frame redundancy, and uses adaptive weights to improve the robustness of the model to hard samples.\n\nThese methods demonstrate significant improvements over previous approaches, achieving state-of-the-art performance on standard benchmarks and maintaining real-time inference speeds. The Hide-and-Seek strategy and the use of adaptive weights are particularly noteworthy, as they address the challenges of occlusions and boundary refinement, which are critical for high-quality segmentation. Future research in this field could explore the integration of these techniques with transformer-based architectures to further enhance the performance and efficiency of VOS models. Additionally, the development of more sophisticated kernel functions and adaptive weighting schemes could lead to even more robust and accurate segmentation results."
}