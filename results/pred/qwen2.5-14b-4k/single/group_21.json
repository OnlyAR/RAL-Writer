{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Mesh Convolution with Continuous Filters \\\\for\n 3D Surface Parsing}\n\n\\begin{document}\n\n\\title{Mesh Convolution with Continuous Filters \\\\for\n 3D Surface Parsing}\n\n\\author{Huan~Lei,\n        Naveed~Akhtar,\n        Mubarak~Shah,\n        and~Ajmal~Mian % <-this % stops a space\n\\thanks{H.~Lei is with the School of Computing, The Australian National University. N.~Akhtar and A.~Mian are with the Department of Computer Science and Software Engineering, The University of Western Australia, 35 Stirling\nHighway, Crawley, Western Australia, 6009. M.~Shah is with the Center for Research in Computer Vision, University of Central Florida, 4328 Scorpius St. Orlando, USA.\nE-mail: dr.huanlei@gmail.com, naveed.akhtar@uwa.edu.au, shah@crcv.ucf.edu, \najmal.mian@uwa.edu.au.}% <-this % stops a space\n}\n\n\\maketitle\n\n\\begin{abstract}\nGeometric feature learning {\\color{black}for 3D surfaces is critical for many applications in computer graphics and 3D vision.} \nHowever, deep learning currently lags in hierarchical modeling of  3D surfaces due to the lack of required operations and/or their efficient implementations. In this paper, we propose a series of modular operations for effective geometric feature learning from 3D triangle meshes. These operations include novel mesh convolutions, efficient mesh decimation and associated mesh (un)poolings. \n{\\color{black}Our mesh convolutions exploit spherical harmonics as orthonormal bases to create continuous  convolutional filters.} \nThe mesh decimation module is GPU-accelerated and able to process batched meshes on-the-fly, while the (un)pooling operations compute features for up/down-sampled meshes. \nWe provide open-source implementation of these operations, collectively termed \\textit{Picasso}.  \n{\\color{black}Picasso supports \\textit{heterogeneous} mesh batching and processing. Leveraging its modular operations, we further contribute a novel hierarchical neural network for perceptual parsing of 3D surfaces, named PicassoNet++.} It achieves highly competitive performance for shape analysis and scene segmentation on prominent 3D benchmarks. The code, data and trained models are available at \\href{https://github.com/EnyaHermite/Picasso}{https://github.com/EnyaHermite/Picasso}.\n\\end{abstract}\n\n\\begin{IEEEkeywords}\n\\color{black}Point cloud, mesh convolution, spherical harmonics, GPU mesh decimation, heterogeneous meshes, surface parsing.\n\\end{IEEEkeywords}\n\n\\section{Introduction}\\label{sec:intro}\n\n\\IEEEPARstart{D}{iscriminative} feature learning for 3D surfaces is fundamentally important for computer graphics and computer vision. \nAlthough deep learning is able to learn impressive features on images and videos \\cite{krizhevsky2017imagenet,he2016deep,liu2016ssd}, {\\color{black}its application does not generalize to \ndata structures that are not homogeneous grids, such as 3D surfaces. This motivates the research direction of geometric deep learning~\\cite{bronstein2017geometric}, which targets irregular data representations, \\ie,~graphs, surfaces,  as inputs of neural networks.}\n\n{\\color{black}\nIn digital devices, 3D surfaces are usually represented as discrete polygon meshes, or more commonly triangle meshes. Triangle meshes hold key information about the topology of their smooth counterpart surfaces. We can, therefore, learn features for perceptual parsing of 3D surfaces from such mesh representations. \nHowever, the variety and heterogeneous nature of atomic components, \\ie,~vertices, edges, facets, in meshes makes the ideal feature learning  challenging~\\cite{hanocka2019meshcnn,schult2020dualconvmesh,lei2021picasso}.}\n\n{\\color{black}\nExisting mesh-based \nneural networks mostly learn features for shape analysis \\cite{boscaini2016learning,hanocka2019meshcnn,monti2017geometric,ranjan2018generating}. \nThose methods handle small shape meshes as graphs and learn feature representations using graph convolutions. The mesh resolutions in their architectures are either fixed or reduced via inefficient mesh decimation algorithm \\cite{garland1999quadric,garland1997surface,rossignac1993multi,zhou2018open3d}.\nWhereas non-hierarchical networks and slow resolution reduction are acceptable for small-scale surface analysis like 3D shapes, they become impractical in large-scale surface parsing of 3D scenes. Pioneering works for large-scale surface parsing divide the intact meshes into smaller blocks and learn features with graph convolution~\\cite{schult2020dualconvmesh}. \nCurrently, applying deep learning to intact scene surfaces is hindered by the absence of amenable mesh convolutions and efficient mesh decimation in the modern libraries \\eg,~Pytorch~\\cite{paszke2019pytorch}, Tensorflow~\\cite{abadi2016tensorflow}.}  \n\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=0.9\\textwidth]{Figs/Lib_demo_usage.pdf}\n\\vspace{-3mm}\n\\caption{(a) An example of building a simple hierarchical mesh network for shape classification using the mesh convolutions and poolings in Picasso. The network comprises  two hierarchical layers, and uses batch size 3 in this example. It accepts batch input as a tuple of $({\\bf V}, {\\bf F}, {\\bf H}_G^0)$, where ${\\bf V}$ denotes concatenated vertices, ${\\bf F}$ are facets, and ${\\bf H}_G^0$ denotes geometry features of the shapes. In this illustration, we decimate the input meshes by reducing their number of vertices by $N_r$. (b) Example configuration of the standard initial layer which considers the input features to comprise both geometry features, ${\\bf H}_G^0$, and  texture features, ${\\bf H}_C^0$. We discuss further details of this figure in the overview of Picasso in  \\S~\\ref{sec:overview}.}\n\\label{fig:picasso_demo_usage}\n\\vspace{-2mm}\n\\end{figure*}\n\n{\\color{black}\nWith this work, we aim to make deep learning more accessible for 3D surface parsing. To achieve this, we propose novel modular operations that suit triangle meshes such as mesh convolutions with continuous filters, GPU-accelerated mesh decimation, and mesh (un)poolings. \nSpecifically, we use spherical harmonics to formulate the convolutional filters as continuous functions of azimuth and elevation angles $(\\theta,\\phi)$. This \nsimplifies the construction of all convolutional kernels in our proposed mesh convolutions, including \\textit{facet2vertex}, \\textit{vertex2facet}, and \\textit{facet2facet} convolution. \nOur formulation parameterizes the angular arguments with face normals for the facet2vertex convolution, while using projected Barycentric coordinates for the vertex2facet and facet2facet convolutions. In addition, we extend the idea of continuous filter modelling to point cloud convolution in ambient spheres by combining spherical harmonics with a function of radius $r$.} Our GPU-accelerated mesh decimation can simplify a batch of meshes on-the-fly for hierarchical feature learning.\nIt allows control over the decimated mesh resolution using the number of vertices. \nWe define the (un)pooling operations based on vertex clusters recorded during the decimation. They are required to generate features for newly-created neurons when the network resolution is altered. \nWe implement the presented modular operations in CUDA, and introduce them collectively as \\textit{Picasso}. Picasso supports feature learning from heterogeneous\nmeshes in both PyTorch and Tensorflow.\nFigure~\\ref{fig:picasso_demo_usage} illustrates an example mesh-based neural network for shape classification\nthat can be built using the Picasso modules. \n\n{\\color{black}Based on the modular operations, we further contribute a hierarchical mesh-based neural network for semantic analysis of 3D shapes and reconstructed scene surfaces. The proposed network, named PicassoNet++, accepts intact triangle meshes (with or without texture) as inputs.  \nPicassoNet++ incorporates a series of significant improvements over the original PicassoNet~\\cite{lei2021picasso}, including reduced utilization of point cloud convolutions, increased network depth, and different skip connections.} Our experiments show that dual convolutions, as used in~\\cite{lei2020spherical}, are unnecessary for high-resolution meshes as mesh convolutions are effective enough to learn distinctive features for them. This leads to significant computational savings.\nTo evaluate PicassoNet++, we follow MeshCNN~\\cite{hanocka2019meshcnn} and use the SHREC \\cite{lian2011shape}, CUBE \\cite{hanocka2019meshcnn}, COSEG \\cite{wang2012active}, Human \\cite{maron2017convolutional}, and FAUST \\cite{bogo2014faust} datasets for shape analysis. We also use the large-scale ShapeNetCore dataset~\\cite{chang2015shapenet} for 3D shape classification, and the S3DIS \\cite{armeni20163d} and ScanNet \\cite{dai2017scannet} datasets for large-scale surface segmentation. Our approach achieves highly competitive results in all cases.\n\nThis article is a significant extension of our preliminary work presented in IEEE CVPR 2021~\\cite{lei2021picasso}. \nThe following outlines our main contributions  beyond the conference work.\n\\begin{itemize}\n\\item {\\color{black}\\textbf{Filter modelling with spherical harmonics:} We simplify the construction of mesh convolutional kernels by using spherical harmonics. This allows the discrete filters to be represented as continuous real-valued functions, rather than discrete values in $\\mathbb{R}$. Since face normals are distributed on the surface of a unit sphere, we model filters for \\textit{facet2vertex} convolutions as continuous functions of their azimuth and elevation angles $(\\theta, \\phi)$. This is superior to the fuzzy modeling of using mixture models \\cite{lei2021picasso} because spherical harmonics are fixed functions and do not require additional training parameters. We also apply this idea to \\textit{vertex2facet} and \\textit{facet2facet} convolutions by computing the angular arguments $(\\theta, \\phi)$ from projected Barycentric coordinates. Additionally, we combine the spherical harmonics with a radial function of radius $r$ to construct continuous filters for point cloud convolution in ambient spheres.\n\n\\vspace{0.5mm}\n\\item \\textbf{PicassoNet++:} We present a novel mesh based neural network that can be applied for general 3D surface parsing from their triangle mesh representations. By establishing the passive role of point cloud convolutions in high-resolution mesh feature learning, we address it in PicassoNet++ to gain a significant computational advantage over \\cite{lei2021picasso} while maintaining the performance. We further improve the network architecture through better design choices of skip connections and sub-network blocks. PicassoNet++ accepts intact meshes as inputs, and is applicable to textured meshes.\nWe incorporate all required functionalities in our network, including reconfiguration of the initial convolutional layer - see Fig.~\\ref{fig:picasso_demo_usage}.} \n\\vspace{0.5mm}\n\\item \\textbf{Extensive evaluation:} We evaluate PicassoNet++ on various 3D benchmarks for surface parsing, including small-scale shape analysis and large-scale scene segmentation. It achieves highly competitive performance on all datasets. We also provide ablation studies for determining the neural network architecture and the truncated degree of spherical harmonics. We release the latest Picasso and PicassoNet++ at \\href{https://github.com/EnyaHermite/Picasso}{https://github.com/EnyaHermite/Picasso} for the broader research community.\n\\item \\textbf{Pytorch extension:} Originally in \\cite{lei2021picasso}, Picasso was only supported in Tensorflow. However, with this work, we also make it available in PyTorch due to the growing popularity of this community. We note that the Picasso released with this article incorporates not only the newly introduced modules for heterogeneous mesh processing, but also compatible modular operations for heterogeneous point cloud processing. We include the point cloud modules by adapting our previous contributions~\\cite{lei2020spherical,lei2020seggcn} to heterogeneous applications. Altogether, Picasso enables convenient building of neural networks for heterogeneous mesh processing in PyTorch and Tensorflow.\n\\end{itemize}\n\n\\section{Related Work}\\label{sec:references}\n{\\color{black}\nWe review convolutional methods for 3D triangle meshes, as well as convolutions over 3D point clouds as they can be applied to the vertices of a triangle mesh. Finally, we discuss the existing algorithms for mesh decimation.\n\n\\vspace{-1mm}\n\\subsection{Convolution on 3D Meshes}\\label{subsec:convolution_mesh}\nMultiple approaches exist that employ convolution on meshes to learn features for small-scale shape analysis. \nThe convolutions are generally performed  on local planar patches defined in the hand-crafted coordinate systems \\cite{boscaini2016learning,masci2015geodesic,monti2017geometric}. These methods either establish the coordinate system using  geodesic level sets \\cite{masci2015geodesic}\nor surface normals and principle curvatures \\cite{boscaini2016learning,monti2017geometric}.\nFor improved correspondence matching, Verma \\etal \\cite{verma2018feastnet} replaced the previous hand-crafted local patches with a learnable\nmapping between graph neighborhoods and filter weights.\nTo reconstruct human facial expressions, Ranjan~\\etal~\\cite{ranjan2018generating} exploited the spectral graph convolutions \\cite{defferrard2016convolutional} with hierarchical mesh-based autoencoders. Gao~\\etal~\\cite{gao2022robust} proposed to learn mesh representations with adaptive attention matrices and anisotropic convolutions.\n\nWhereas other methods focus on learning vertex-wise features,\nMeshCNN~\\cite{hanocka2019meshcnn} introduces convolutional operation that learns edge-wise features for semantic labelling on a mesh. The recent PD-MeshNet~\\cite{milano2020primal} further extracts facet-wise representations by defining convolution on the primal-dual graphs of an input mesh. It reduces network resolution using the graph edge contraction method provided by Pytorch Geometric \\cite{fey2019fast}. HodgeNet~\\cite{smirnov2021hodgenet} proposes to learn the mesh geometry from the spectral domain, which involves computing eigenvalues/eigenvectors and can be time-consuming. SubdivNet~\\cite{hu2022subdivision} uses loop subdivisions to learn features from meshes, whereas the input mesh has to be remeshed~\\cite{liu2020neural} for fine connectivity.\n\nCurrently, only a small number of mesh-based convolutional networks exist for large-scale scene parsing in the real world. TextureNet \\cite{huang2019texturenet} parameterizes the room surface into local planar patches in the 4-RoSy field such that standard CNNs \\cite{krizhevsky2012imagenet} can be applied to extract high-resolution texture information from mesh facets. Schult~\\etal   \\cite{schult2020dualconvmesh} applied the spatial graph convolutions of dynamic filters \\cite{li2018pointcnn,simonovsky2017dynamic,wang2018dynamic,wu2019pointconv} to the union of  neighborhoods in both geodesic and Euclidean domains for vertex-wise feature learning. VMNet \\cite{hu2021vmnet} combines the SparseConvNet \\cite{tang2020searching} with graph convolutional networks to learn merged features from point clouds and meshes. \nGenerally, previous methods explore mesh as an edge-based graph and define the graph convolutions based on its geodesic connections \\cite{milano2020primal,verma2018feastnet,schult2020dualconvmesh,ranjan2018generating}. We instead propose convolutions on the mesh structure itself, following its elementary geometric components, \\ie,~vertices and facets. To promote this more natural perspective, we also contribute computation and memory optimized CUDA implementations \nfor forward and backward propagations of all the mesh convolutions we present in this work. \n\n\\vspace{-1mm}\n\\subsection{Convolution on 3D Point Clouds}\\label{subsec:convolution_pointcloud}\nApplying voxel-grid kernels to dense volumetric representations is the most straightforward solution of transferring CNNs from images to point clouds \\cite{wu20153d,maturana2015voxnet,zeng20163dmatch}.\nHowever, the practical potential of these methods is limited by their cubically growing requirements on memory and computational resources. \nDifferent strategies have been introduced to incorporate sparsity into the dense volumetric CNNs \\cite{EngelckeICRA2017,graham20183d,choy20194d,riegler2017octnet}, among which SparseConvNets \\cite{graham20183d,choy20194d} are currently the best performing architectures.  \nSeveral approaches also explore similar regular-grid kernels for transformed input representations of point clouds, such as TangentConv \\cite{tatarchenko2018tangent}, SplatNet \\cite{su2018splatnet},  UnPNet~\\cite{li2021rethinking}.\nSince PointNet \\cite{qi2017pointnet}, the permutation invariant networks learn features from point clouds using multilayer perceptrons followed by max pooling~\\cite{klokov2017escape,qi2017pointnet,qi2017pointnetplusplus,wu2019pointconv} and the spatial coordinates of points are used as input features.\n\nGraph-based neural networks allow the\nconvolutions to be conducted in either spectral or spatial domain. However, applying the spectral convolutions to point cloud processing is complicated because they demand the graph Laplacians of different input samples to be pre-aligned \\cite{yi2017syncspeccnn}. \nAs a pioneering work in the spatial domain, ECC \\cite{simonovsky2017dynamic} exploits dynamical filters \\cite{de2016dynamic} to \nparameterize the graph convolutional parameters for point cloud analysis. Subsequent works also explored more effective kernel and filter parameterizations \\cite{li2018pointcnn,wang2019attention,wu2019pointconv,du2022novel,huang2022dual}.\nThe discrete kernels  \\cite{lei2019octree,lei2020seggcn,lei2020spherical,thomas2019kpconv} are efficient alternatives to those dynamic kernels as they define the filter parameters directly, avoiding the necessity of indirect filter generation within the network. \nThe spherical kernels \\cite{lei2020seggcn,lei2020spherical} that separate depth-wise and point-wise computations are memory and runtime advantageous, while KPConv \\cite{thomas2019kpconv} is reported to be more competitive than SparseConvNets. \nWe refer interested readers to surveys~\\cite{wu2020comprehensive, li2020deep} for progress in deep learning for graph neural networks and point clouds. \nRecently, researchers have also started to adapt transformers~\\cite{vaswani2017attention} to point cloud processing~\\cite{zhao2021point, liu2022uninet, guo2021pct, wu2022point}.}\n\n\\vspace{-1mm}\n\\subsection{Mesh Decimation}\\label{subec:decimation_mesh}\nHierarchical neural networks induce  multi-scale feature extraction by allowing convolutions to be applied on increasing receptive fields of the input data. Although farthest point sampling (FPS) is widely used to construct hierarchical architectures for point clouds \\cite{lei2020spherical,qi2017pointnetplusplus,wu2019pointconv}, it is inapplicable to mesh processing because of its inability of tracking vertex connections. \nFortunately, the graphics research community has contributed effective  methods for mesh simplification, such as Vertex Clustering (VC) \\cite{rossignac1993multi} and Quadric Error Metrics (QEM)~\\cite{garland1999quadric,garland1997surface}. \nThe two methods are suitable choices for mesh-based neural networks \\cite{ranjan2018generating,schult2020dualconvmesh,hu2021vmnet} to establish hierarchical architectures. Compared to VC, the QEM method is good at reducing mesh resolution\nwhile retaining most of its geometric information, leading to  superior performance \\cite{schult2020dualconvmesh}. In specific, QEM simplifies a mesh via iterative contractions of  vertex pairs, whereas the optimal vertex pair for contraction has to be determined after each iteration. The popular geometric processing library - Open3D  \\cite{zhou2018open3d}, offers simplification functions for both VC and QEM. \nHowever, the  CPU-based implementation is inefficient and also not amenable to operations required for deep learning, \\eg,~batch processing. {\\color{black}Recently, researchers have also started to explore the possibility of achieving mesh simplification using neural networks~\\cite{potamias2022neural}.}\n\nThough superior to VC in performance, the iterative progressive strategy of QEM makes it  impossible to be deployed on GPUs as parallel processes. In this work, we introduce a fast mesh decimation technique based on the QEM algorithm \\cite{garland1997surface}. Compatible to deep learning, our method can process a batch of heterogeneous meshes on-the-fly. In contrast to \\cite{garland1997surface}, it sorts all the vertex pairs only once according to their quadric errors, and groups the vertices to be contracted into \\textit{disjoint} clusters. Except for the grouping process, all other computations in our method get accelerated via parallel GPU computing. \n\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=0.98 \\textwidth]{Figs/VCluster.pdf}\\\\\n\\vspace{-3mm}\n\\caption{Illustration of the vertex clustering process. (a) An input mesh with twelve edges (vertex pairs). We sort the vertex pairs in ascending order according to their quadric errors. (b) Then, we initialize the clusters as $\\{c,d\\},\\{a,g\\},\\{e,f\\}$ using the disjoint vertex pairs $(c,d),(a,g),(e,f)$ - shown by red, green and blue. (c) We group the remaining \nvertex $b$ to the vertex cluster $\\{a,g\\}$ because $(a,b)$ holds the smallest quadric error among all pairs containing $b$, \\ie $(a,b),(b,g),(b,c)$. Finally, the vertex clusters become $\\{c,d\\},\\{a,b,g\\},\\{e,f\\}$. (d) We construct the decimated mesh by applying vertex contraction to each cluster. \nThe target position of contraction is computed as the average location of all vertices in the cluster.} \n\\label{fig:vertex_cluster}\n\\vspace{-2mm}\n\\end{figure*}\n\n\\section{GPU-Accelerated Mesh Decimation}\nTo explore hierarchical neural networks on 3D meshes, there is a need of  efficient mesh decimation technique that suits deep learning for on-the-fly network reduction. The QEM algorithm \\cite{garland1997surface} is effective at simplifying meshes while retaining the decimation quality. However, it applies contractions to each vertex pair iteratively with a global optimal quadric error. \nThe implicit dependencies between the iterative contractions make this method unsuitable for parallel acceleration. \nHence, we propose an enhancement of QEM to enable parallel computing with GPUs. \nIn our method, we do not allow inter-dependent iterative contractions. \nInstead, we group the vertices into multiple disjoint clusters under a reasonable compromise on the \nquadric error cost. \nWe control the clustering process using expected number of vertices in the decimated mesh rather than the number of edges or facets. \nDue to the disjointness of vertex clusters, their contractions are independent of each other and can be executed in parallel. \nWe provide a toy example in Fig.~\\ref{fig:vertex_cluster} to illustrate our procedure of vertex clustering.\n\nIn our method, we establish the vertex pairs for candidate contraction using the end-vertices of mesh geodesic edges only. \nTo prioritize the vertex pairs that contribute to lower quadric errors, they are arranged in ascending order.  \nEach vertex cluster is then initialized as a disjoint vertex pair in the ascending order of the candidates.\nWe summarize our mesh simplification procedure as Algorithm~\\ref{alg:Mesh_decimate_ours} that reduces the number of mesh vertices to nearly a half per-iteration.\nTo handle mesh decimation of arbitrary number of vertices, we allow the core algorithm to be iterated for flexible (${\\geqslant}1$) times.\nIn Algorithm~\\ref{alg:Mesh_decimate_ours}, we present the decimation method for a single mesh as the input for clarity. Our decimation function implementation processes `mini-batches' of multiple meshes. \nWe execute the vertex clustering (\\textit{lines 5--16}) on CPU while all the other operations that require heavy computations are performed on GPU. The clustering process has a time complexity of  $\\mathcal{O}(|\\mathcal{E}|)$,  where $|\\mathcal{E}|$ is the number of edges of the input mesh.\nThe routine penalties, and consistency checks in mesh decimation are excluded in our method to favor runtime efficiency. We compare the runtime of QEM and the proposed decimation algorithm in Fig.~B.1 of the supplementary, where our method is much faster.\n\n\\begin{algorithm}[t]\n\\caption{The GPU-accelerated mesh simplification}\n\\label{alg:Mesh_decimate_ours}\n\\begin{algorithmic}[1]\n\\renewcommand{\\algorithmicrequire}{\\textbf{Input:}}\n\\renewcommand{\\algorithmicensure}{\\textbf{Output:}}\n\\REQUIRE mesh $\\mathcal{T}^i{=}(\\mathcal{V}^i,\\mathcal{F}^i)$;\nnumber of vertices to remove $N_r$. \\hspace{-4mm}\n\\ENSURE decimated mesh ${\\mathcal{T}^o}{=}(\\mathcal{V}^o,\\mathcal{F}^o)$.\n\\vspace{1mm}\n\\STATE  establish a vertex pair $(v_i$, $v_j)$ for each edge. \n\\STATE compute the quadric cost of contracting each pair.\n\\STATE sort all pairs ascendingly based on the quadrics.\n\\STATE set $n_r=0$, and $p(v_i)=\\FALSE, \\forall~v_i \\in \\mathcal{V}^i$. \n\\FOR{each pair ($v_i$, $v_j$)}\n\\IF{$p(v_i)=\\FALSE$, $p(v_j)=\\FALSE$, \\AND $n_r<N_r$}\n\\STATE (a) initialize  $\\{v_i, v_j\\}$ as a new cluster.\n\\STATE (b) set $n_r=n_r+1$, $p(v_i)=\\TRUE$, $p(v_j)=\\TRUE$.\n\\ENDIF\n\\ENDFOR\n\\FOR{each pair ($v_i$, $v_j$)}\n\\IF{$p(v_i)=\\FALSE$ \\OR $p(v_j)=\\FALSE$, \\AND $n_r<N_r$}\n\\STATE (a) place $v_i$, $v_j$ to the same cluster.\n\\STATE (b) set $p(v_i)=\\TRUE$, $p(v_j)=\\TRUE$.\n\\ENDIF\n\\ENDFOR\n\\FOR{each cluster $\\{v_i, v_j, \\dots\\}$}\n\\STATE (a) compute the average position $\\bar{v}$ of the cluster.\n\\STATE (b) contract the cluster to $\\bar{v}$.\n\\ENDFOR\n\\STATE return\n\\end{algorithmic}\n\\footnotetext{We compute $\\bar{v}$ as the average position of all vertices in a cluster.}\n\\end{algorithm}\nIn our implementation, \nwe also record the vertex clustering information with a parameter \\textit{VCluster}, and the vertex mapping between input and output meshes with a parameter \\textit{IOmap}. They are both vectors of the same sizes as the number of vertices $|\\mathcal{V}^i|$ in the input mesh $\\mathcal{T}^i=(\\mathcal{V}^i,\\mathcal{F}^i)$.\nOur decimation function yields those two parameters along with the decimated mesh as they are required in the computations of (un)poolings. \n\n\\vspace{1mm}\n\\noindent \\textbf{(Un)poolings:} The clustering and mapping information  encoded in vectors \\textit{VCluster} and \\textit{IOmap} largely facilitate the (un)pooling computations.  \nConsidering each cluster as a local region or neighborhood, common pooling operations such as `sum'/`average'/`max'/`median'/`weighted' can be directly defined. We provide max($\\cdot$), and average($\\cdot$) poolings to down-sample the features. For unpooling, all vertices in a cluster replicate features of a representative vertex that the cluster is contracted to in the decimated mesh. \nConsider the input and output meshes in Fig.~\\ref{fig:vertex_cluster} as an  example. We compute the feature of vertex `$1$' in the decimated mesh as $h^1=\\max(h^a,h^b,h^g)$ under max pooling, while create the features of $\\{a,b,g\\}$ as $h^a=h^b=h^g=h^1$ in unpooling.\nIn addition to the mesh decimation and (un)poolings, we also introduce convolutional operations that are more compatible to feature learning on triangular meshes than the previous graph convolutions \\cite{schult2020dualconvmesh}.\n\n{\\color{black}\n\\section{Mesh Convolutions}\n\\vspace{2mm}\nWe represent a triangle mesh as $\\mathcal{T}=(\\mathcal{V},\\mathcal{F})$, where $\\mathcal{V}$ and $\\mathcal{F}$ denote the set of vertices and facets respectively. \nLet the spatial coordinates of each vertex $v$ be ${\\bf x}$, while the area and normal of a facet $f$ be $A$ and ${\\bf n}$.\nIn the case of a textured mesh, we denote the texture size of each facet as $K \\times 3$, where $K$ represents the texture resolution and 3 indicates the dimension of colors. Note that, $K$ varies across different facets.\n\\begin{figure*}[!t]\n    \\centering\n\\includegraphics[width=0.95\\textwidth]{Figs/sphere_harmonics.pdf}\n    \\vspace{-3mm}\n    \\caption{\\color{black}Visualization of spherical harmonics of degree $L=2$. We show both the real and imaginary parts for each spherical harmonic $Y_l^m$ with order $m>0$. We approximate each real-valued filter function ${\\mathbf F}(\\theta,\\phi)$ with linear combinations of truncated spherical harmonics. This formulation is applied to parameterize the filters in all of our proposed mesh\nconvolutions, \\ie,  the facet2vertex, vertex2vertex, and\nfacet2facet convolutions.} \n    \\label{fig:sphere_harmonics}\n    \\vspace{-2mm}\n\\end{figure*}\n\nLet $\\{\\mathbf{F}_t\\}_{t=1}^T$ be the discrete filters in a convolutional kernel, where $T$ is the kernel size. In deep learning, to compute the feature of a given item $j$ using convolutions, we consider its receptive field $\\mathcal{N}(j)$ and apply the following \n\\begin{align}\n&g_j =\\frac{1}{\\mathcal{N}(j)}\\sum_{i\\in\\mathcal{N}(j)}\\langle\\mathbf{F}_{t_i},\\mathbf{h}_i\\rangle.\n\\label{eq:basic_Conv}\n\\end{align}\nHere, $t_i$ refers to the filter indexed by its neighboring item $i$ in the receptive field, and $\\mathbf{h}_i$ refers to the input features of item $i$. Let the dimension of input features  be  $C$, it then follows that $g_j\\in\\mathbb{R}$, and ${\\mathbf F}_i, {\\mathbf h}_i\\in\\mathbb{R}^C$. \n\n\\subsection{Filter Modelling with Spherical Harmonics}\\label{subsec:continuous_filter_ylm}\nDiscrete kernels~\\cite{lei2021picasso, thomas2019kpconv, choy20194d, lei2020spherical} require brute-force partitioning of the subspace defined by local receptive fields. To avoid this, we propose formulating the discrete filters $\\{\\mathbf{F}_t\\}_{t=1}^T$ as a \\textit{single continuous} function $\\mathbf{F}$, and we parameterize it using the surface normals of 3D meshes. We note that $\\mathbf{F}{\\in}\\mathbb{R}^C$ is a vector function. Inspired by the characteristics of basis functions, we represent the continuous filter as linear combinations of such functions. Since normals are distributed on the surface of a unit sphere, we choose the spherical harmonics $\\{Y_l^m\\}$ as the basis functions. Thus, the filter function $\\mathbf{F}$ is expressed as \n\\begin{align}\\label{eq:spharm_filter}\n\\mathbf{F}(\\theta,\\phi) &= \\sum_{\\ell=0}^{\\infty}\\sum_{m=-\\ell}^{\\ell}\\boldsymbol{\\alpha}_{\\ell m}Y_{\\ell}^m(\\theta,\\phi),\n\\end{align}\nwhere $\\{\\boldsymbol{\\alpha}_{\\ell m}\\in\\mathbb{C}^C\\}$ are the linear coefficients, and \n\\begin{align}\\label{eq:ylm}\nY_{\\ell}^m(\\theta,\\phi) &= \\sqrt{\\frac{2\\ell+1}{4\\pi}\\frac{(\\ell-m)!}{(\\ell+m)!}}P_{\\ell}^m(\\cos{\\theta})e^{im\\phi},\n\\end{align}\nfor $m\\geq0$. The integers $l$ and $m$ indicate the degree and order of a spherical harmonic, respectively. $P_l^m(\\cdot)$ are associated Legendre polynomials without the Condon–Shortley phase.    \nWe denote the azimuthal angle as $\\phi \\in [0,2\\pi)$ and the polar angle as $\\theta \\in [0,\\pi]$, following the common conventions. In practice, we use truncated spherical harmonics with a finite degree $L$ to approximate the continuous filter functions.\n\nEquation (\\ref{eq:ylm}) implies that the spherical harmonics satisfy the property $Y_l^{-m}(\\theta,\\phi){=}(-1)^mY_l^m(\\theta,\\phi)^*$. Since the filter $\\mathbf{F}(\\theta,\\phi)$ is a \\textit{real-valued} function, we can combine each pair of $Y_l^{-m}$ and $Y_l^m$, and represent $\\mathbf{F}(\\theta,\\phi)$ using truncated spherical harmonics as follows\n\\begin{equation}\\label{eq:real_spharm_filter}\n\\begin{split}\n\\mathbf{F}(\\theta,\\phi)= \\sum_{\\ell=0}^{L}\\Big( &\\sum_{m=1}^{\\ell}\\mathbf{a}_{\\ell m}Y_{\\ell}^m(\\theta,0)\\cos(m\\phi)+\\\\\n&\\sum_{m=1}^{\\ell}\\mathbf{b}_{\\ell m}Y_{\\ell}^m(\\theta,0)\\sin(m\\phi)+\\\\\n&\\mathbf{a}_{\\ell 0}Y_{\\ell}^0(\\theta,\\phi)\\Big).\n\\end{split}\n\\end{equation}\nFigure~\\ref{fig:sphere_harmonics} shows an example of such a spherical harmonic basis with degree $L{=}2$. In this representation, the learnable parameters in our convolutional kernel are the linear coefficients $\\{\\mathbf{a}_{\\ell 0}, \\mathbf{a}_{\\ell m}, \\mathbf{b}_{\\ell m}{\\in}\\mathbb{R}^C|m{\\in}[\\ell], \\ell{\\in}[L]\\}$, where $[\\ell]{=}\\{1,2,\\dots,\\ell\\}$ and similarly $[L]$. We redefine the size of our convolutional kernel $T$ as the number of such coefficients, \\ie, the number of basis functions in Eq.~(\\ref{eq:real_spharm_filter}). This results in the kernel size being $T=(L+1)^2$.\nWe apply the filter modelling in Eq.~(\\ref{eq:real_spharm_filter}) to all of our proposed mesh convolutions, including the \\textit{facet2vertex}, \\textit{vertex2vertex}, and \\textit{facet2facet} convolutions. \n\n\\subsection{Angular Arguments in Different Convolutions}\n\\noindent\\textbf{Facet2vertex convolution.}  We compute features of each vertex by aggregating context information from adjacent facets, rather than neighboring vertices. This avoids transforming a mesh into a graph for context propagation.\nThe facet normal is directional data residing on the surface of a unit sphere. We, therefore, compute the angular arguments $(\\theta,\\phi)$ of its filter functions based on facet normals. \n\nFollowing previous works \\eg~\\cite{chollet2017xception,lei2020spherical}, we define the facet2vertex convolution in a depth-wise separable manner to save computations. Let the learnable coefficients in the kernel be $\\{a_{\\ell 0}, a_{\\ell m}, b_{\\ell m}{\\in}\\mathbb{R}|m{\\in}[\\ell], \\ell{\\in}[L]\\}$, the adjacent facets of vertex $v$ be $\\mathcal{N}(v)$, and the associated features of those facets be $\\{h_f|{f\\in\\mathcal{N}(v)}\\}$. The feature of vertex $v$ is computed as \n\\begin{align}\ng_v =\\frac{1}{\\mathcal{N}(v)}\\sum_{f\\in\\mathcal{N}(v)}F(\\theta_f, \\phi_f){h_f}.\n\\label{eq:F2V_Conv}\n\\end{align}\nWe use ReLU \\cite{nair2010rectified} as the activation function. Considering\nour filter modelling based on normals, the facet2vertex convolution is scale and translation invariant but not rotation invariant.\n\n\\vspace{1mm}\n\\noindent\\textbf{Vertex2facet convolution.} We aggregate features of each facet from its vertices. The vertex2facet convolution also exploits depth-wise separable strategy. Therefore, its definition of learnable parameters is the same as that for the facet2vertex convolution. We compute angular arguments of its filters from Barycentric coordinates of each  vertex of a facet, which are $(1,0,0)$ for vertex $v_1$, $(0,1,0)$ for vertex $v_2$, and $(0,0,1)$ for vertex $v_3$. \nThey correspond to the angular values of  $(\\frac{\\pi}{2},0)$, $(\\frac{\\pi}{2},\\frac{\\pi}{2})$, and $(0,0)$ on the sphere, respectively.\n\nLet $\\{h_1,h_2,h_3\\}$ be the features of vertices $\\{v_1,v_2,v_3\\}$. We compute the feature of facet $f$ as\n\\begin{align}\ng_f=F(\\frac{\\pi}{2},0)h_1+\nF(\\frac{\\pi}{2},\\frac{\\pi}{2}) h_2+F(0,0)h_3.\n\\label{eq:V2F_Conv}\n\\end{align}\nThe Barycentric interpolation in \\cite{lei2021picasso} is no longer retained in the vertex2facet convolution \nas it only makes a minor contribution to  feature extraction but requires additional computations.\nTo propagate local information from vertices to vertices \\cite{qi2017pointnet,wu2019pointconv,lei2020spherical}, we induce a vertex2vertex convolution by combining the vertex2facet and facet2vertex convolutions.  \nFigure~\\ref{fig:v2v_conv} illustrates the notion of facet2vertex, vertex2facet,   vertex2vertex,\nand facet2facet convolutions. \n\n\\vspace{1mm}\n\\noindent\\textbf{Facet2facet convolution.}\nWhen the input mesh is textured, we learn the texture features of each facet based on the colors of all the internal points on the facet. \nLet $\\{{\\bf h}_k\\in\\mathbb{R}^3\\}$ be the input colors of all points on a facet, and the associated Barycentric coordinates of each point be $\\{{\\boldsymbol\\xi}_k=[\\xi_{k1},\\xi_{k2},\\xi_{k3}]^\\intercal|\\xi_{k1}+\\xi_{k2}+\\xi_{k3}=1,~\\xi_{k1},\\xi_{k2},\\xi_{k3}\\geqslant0\\}$. \nA facet of texture resolution $K$ leads to $|\\{{\\bf h}_k\\}|=|\\{{\\boldsymbol\\xi}_k\\}|=K$. \n\nWe do not exploit \ndepth-wise separable strategy in the facet2facet convolution since there are only three (color) channels. \nTherefore, its learnable parameters are defined as $\\{\\mathbf{a}_{\\ell 0}, \\mathbf{a}_{\\ell m}, \\mathbf{b}_{\\ell m}{\\in}\\mathbb{R}^3|m{\\in}[\\ell], \\ell{\\in}[L]\\}$. \nWe calculate the angular arguments for its filters by projecting $\\{{\\boldsymbol\\xi}_k\\}$ on a simplex to a unit sphere. \nWe compute the feature of a facet $f$ whose texture resolution is $K$ as\n\\begin{align}\ng_f&=\\frac{1}{K}\\sum_k \\langle{\\mathbf{F}(\\theta_k,\\phi_k),{\\bf h}_k}\\rangle.\n\\label{eq:F2F_Conv}\n\\end{align}\n\nThe facet2facet convolution is only required at the first convolution layer for extracting texture information from the raw mesh.\nIn the experiments, we utilize Barycentric interpolation~\\cite{coxeter1961introduction} to prepare the mesh textures. \nThe texture resolution $K$ of a facet ${\\bf f}$ is determined by its area $A$, \\ie,~ \n\\begin{equation}\\label{equ:interpolate_num}\nK=\\frac{(\\gamma+1)(\\gamma+2)}{2},~\\text{where}~ \n\\gamma=\\left\\lfloor \\frac{\\alpha(A-A_{\\min})}{A_{\\max}-A_{\\min}}  \\right\\rfloor  + \\beta.\\\\  \n\\end{equation}\nHere, $A_{\\min}, A_{\\max}$ are the minimum and maximum facet areas of the mesh, whereas  $\\alpha,\\beta\\in\\mathbb{Z}_{\\scaleto{\\geqslant0\\mathstrut}{5pt}}$ are  hyper-parameters. \n\n\\subsection{Extension to Point Cloud Convolution}\\label{subsec:pcloud_conv}\nPoint cloud convolution aggregates context information of a point from its nearest neighbors, which are usually constructed using range search \\cite{preparata2012computational}. This results in its receptive field being within an ambient sphere rather than on a sphere surface. The filters in its convolutional kernel should be functions of three arguments $(\\theta,\\phi,r)$, where $r$ indicates the radial variable. For simplicity, we do not use orthonormal basis in 3D space to model the filter $F(\\theta,\\phi,r)$. Instead, we extend the formulation of $F(\\theta,\\phi)$ in Eq. (\\ref{eq:real_spharm_filter}) by adding radial controls. It is noted  that the Wigner D-functions \\cite{cohenspherical}, which are defined for SO(3) of Euler angles, are not applicable in our case.\n\nSimilar to the facet2vertex and vertex2facet convolutions, we exploit depth-wise separable strategy in the point cloud convolution. Finally, the filter $F(\\theta,\\phi,r)$ is defined as \n\\begin{align}\nF(\\theta,\\phi,r) = F(\\theta,\\phi)Z(r) + c_{0}\\big(1-Z(r)\\big),\n\\end{align}\nwhere $Z(r)=\\frac{r}{\\rho},~r\\in[0,\\rho]$ and $\\rho$ is the radius of a sphere. This modeling introduces only an additional parameter $c_0$ to the learnable parameters and significantly simplifies the computation. It also nicely correlates $F(\\theta,\\phi)$ with $F(\\theta,\\phi,r)$, where $F(\\theta,\\phi)\\equiv F(\\theta,\\phi,\\rho)$. The normalized $Z(r)$ makes it easier to use the filter across different scales and spatial locations in a point cloud.}\n\n\\begin{figure}[!t]\n    \\centering\n\\includegraphics[width=0.49\\textwidth]{Figs/MeshConvolutions.pdf}\n    \\vspace{-3mm}\n    \\caption{Mesh convolutions introduced in Picasso. \n    (a) The facet2vertex convolution propagates features from the adjacent facets of a vertex to the vertex itself. (b) The vertex2facet convolution computes the features of a facet based on its three vertices. (c) The facet2facet convolution calculates features of a rendered facet based on the vertices and interpolated points in the facet. For simplicity, we show only three interpolated points on the rendered facet. It corresponds to a setting of $\\gamma=1$ and $K=6$ following Eq.~(\\ref{equ:interpolate_num}). (d) The vertex2vertex convolution is composed of a vertex2facet convolution followed by a facet2vertex convolution. We apply batch normalization to both vertex and facet features. \n    } \n    \\label{fig:v2v_conv}\n    \\vspace{-2mm}\n\\end{figure}\n\n\\subsection{Geometric Features}\\label{subsec:mesh_facet_geometry}\nLet us denote the coordinates of facet vertices by ${\\bf x}_1, {\\bf x}_2, {\\bf x}_3$, the edge lengths of the facet as ${\\boldsymbol \\ell}=(\\ell_1,\\ell_2,\\ell_3)$ and the facet normals as ${\\bf n}$. We compute inner angles of a facet  ${\\boldsymbol \\theta}=(\\theta_1,\\theta_2,\\theta_3)$ as \n\\begin{equation}\\label{equ:inner_angles}\n\\begin{aligned}\n\\theta_1 &= \\frac{\\langle{\\bf x}_2-{\\bf x}_1, {\\bf x}_3-{\\bf x}_1\\rangle}{\\ell_1\\ell_3}, \\\\\n\\theta_2 &= \\frac{\\langle{\\bf x}_1-{\\bf x}_2, {\\bf x}_3-{\\bf x}_2\\rangle}{\\ell_1\\ell_2}, \\\\\n\\theta_3 &= \\frac{\\langle{\\bf x}_1-{\\bf x}_3, {\\bf x}_2-{\\bf x}_3\\rangle}{\\ell_2\\ell_3}.\n\\end{aligned}\n\\end{equation}\nWe form the input feature representation of mesh geometry as $[{\\boldsymbol \\ell}, {\\boldsymbol \\theta}, {\\bf n}]$ on each facet for shapes. For real-world surface data with aligned gravitational-axis (\\eg,~the $z$-axis), we form their facet geometrics as $[{\\boldsymbol \\ell}, {\\boldsymbol \\theta}, {\\bf n}, {\\bf h}]$, where ${\\bf h}$ concatenates the heights of the three vertices, \\eg,~${\\bf h}=[z_1,z_2,z_3]$. \nWe note that the standard input features to our network in \\S~\\ref{sec:network_Pi2} include both mesh geometry and textures, if textures\nare available.\n\n\\section{Picasso overview}\\label{sec:overview}  \nWe combine the operations proposed in this work for mesh processing  with our previously proposed operations for 3D point cloud processing in \\cite{lei2020spherical,lei2020seggcn} into \n\\textit{Picasso}. \nThe previous point cloud operations are improved to handle point clouds of heterogeneous sizes in addition to homogeneous arrays. This improvement has led to seamless integration of our mesh and point cloud operations. Through Picasso, we make geometric deep learning over 3D data accessible to the broader research community. \nWe allow easy integration of the contributed modular operations in 3D domain with the modern deep learning blocks/layers such as ResNet \\cite{he2016deep}, DenseNet \\cite{huang2017densely}, Inception \\cite{Szegedy2015googLeNet} etc.  \nFigure~\\ref{fig:picasso_overview} provides an overview of the major  modules in Picasso.\nTo differentiate this article's contribution from  \\cite{lei2020spherical,lei2020seggcn}, the figure colorizes only the novel operations introduced in this work. These include CUDA-accelerated mesh decimation, pooling, unpooling, and different mesh convolutions.\nWe additionally incorporate a module for GPU-based voxelization of point clouds and meshes in Picasso. The module allows mesh decimation with voxelized vertex clustering to be performed on-the-fly. \nPicasso is supported in both Pytorch \\cite{paszke2019pytorch} and Tensorflow \\cite{abadi2016tensorflow} for different user preferences. \nWe release the code at  \\href{https://github.com/EnyaHermite/Picasso}{https://github.com/EnyaHermite/Picasso}.\n\n\\vspace{1.0mm}\n\\subsection{\\color{black}Example Usage and Heterogeneous Batching}\nTo build a deep convolutional block for feature learning in Picasso, \nmultiple vertex2vertex convolutions can be cascaded within a network layer of the same mesh resolution, similar to the usage of CNN kernels.\nIn Fig.~\\ref{fig:picasso_demo_usage}(left),  an example is shown for constructing a simple hierarchical mesh network using mesh convolutions and poolings.\nThe example network is sequentially composed of an initial convolutional layer, a max pooling layer, one convolutional block, a global pooling and an arbitrary classifier. \nAssume the network uses batch size 3 for training. Let  $\\mathcal{T}_1{=}(\\mathcal{V}_1,\\mathcal{F}_1)$, $\\mathcal{T}_2{=}(\\mathcal{V}_2,\\mathcal{F}_2)$,  $\\mathcal{T}_3{=}(\\mathcal{V}_3,\\mathcal{F}_3)$ be different shapes in a batch, and $\\mathcal{H}^0_1,\\mathcal{H}^0_2,\\mathcal{H}^0_3$ be the input features of $\\mathcal{T}_1, \\mathcal{T}_2, \\mathcal{T}_3$, respectively. The standard input features $\\mathcal{H}^0$ of a mesh comprise geometrics $\\mathcal{H}_G^0$ and textures $\\mathcal{H}_C^0$ on each facet. As shape meshes provided in the example do not contain textures, their input features are simplified to   $\\mathcal{H}^0{=}\\mathcal{H}_G^0$. In Picasso, we customize the network to accept multiple meshes via concatenation. Therefore, the shapes in the batch input are represented as a tuple of $({\\bf V}, {\\bf F}, {\\bf H}^0)$, where\n\\begin{equation}\n{\\bf V}{=\\hspace{-1mm}}\n\\begin{bmatrix}\n\\mathcal{V}_1\\vspace{.5mm}\\\\\n\\mathcal{V}_2\\vspace{.5mm}\\\\\n\\mathcal{V}_3\\vspace{.5mm}\\\\\n\\end{bmatrix}{\\hspace{-.8mm},\\hspace{1.5mm}}\n{\\bf F}{=\\hspace{-1mm}}\n\\begin{bmatrix}\n\\mathcal{F}_1{+}0\\phantom{+|\\mathcal{V}_1|+}\\vspace{.5mm}\\\\\n\\mathcal{F}_2{+}|\\mathcal{V}_1|\\phantom{+|\\mathcal{V}_2|}\\vspace{.5mm}\\\\\n\\mathcal{F}_3{+}|\\mathcal{V}_1|{+}|\\mathcal{V}_2|\\\\\n\\end{bmatrix}{\\hspace{-.8mm},\\hspace{1.5mm}}\n{\\bf H}^0{=}{\\bf H}_G^0{=\\hspace{-1mm}}\n\\begin{bmatrix}\n\\mathcal{H}_{G,1}^0\\vspace{.5mm}\\\\\n\\mathcal{H}_{G,2}^0\\vspace{.5mm}\\\\\n\\mathcal{H}_{G,3}^0\\\\\n\\end{bmatrix}{\\hspace{-.8mm}.}\n\\end{equation}\n\nFor facet concatenations in ${\\bf F}$, we follow the 0-indexing convention. \nWithout textures, the initial layer of the shape example consists of a $1{\\times}1$ convolution followed by a facet2vertex convolution. However, the standard initial layer takes both geometrics ${\\bf H}_G^0$ and textures ${\\bf H}_C^0$ as input features. We show its configurations in  Fig.~\\ref{fig:picasso_demo_usage}(right).\nTo pool the features, the mesh has to be  decimated first such that the pooling operation can proceed. We exploit max pooling in the example, while the convolutional block comprises two vertex2vertex convolutions. \nGlobal pooling induces a single representation for each sample such that the final classification can be applied. \nWe present a basic example network in Fig.~\\ref{fig:picasso_demo_usage} to provide a clear overview of Picasso. Next, we introduce our proposed network, which offers a more advanced example of using the Picasso modules.\n\n\\begin{figure}[t]\n    \\centering \\includegraphics[width=0.45\\textwidth]{Figs/PicassoOverview.pdf}\n    \\vspace{-3mm}\n\\caption{Overview of the major deep learning modules in Picasso. We only colorize the novel modules proposed in this work. Picasso allows feature learning for both heterogeneous 3D mesh and heterogeneous 3D point cloud.}\n\\label{fig:picasso_overview}\n\\vspace{-2mm}\n\\end{figure}\n\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=0.9\\textwidth]{Figs/Pi2.pdf}\n\\vspace{-3mm}\n\\caption{PicassoNet++ for large-scale semantic parsing of complete scenes (\\textbf{top}), and its convolution blocks (\\textbf{bottom}). The network consists of six mesh resolutions including the input $\\mathcal{T}^{0\\sim5}$. \nThe output channels are respectively $32,64,96,128,192,256$ in the encoder and $128,128,96,96,96$ in the decoder. \nThe pooling strides are $4,3,3,2,2$, which can be different for shape analysis. The figure depicts predicting semantics of mesh vertices,  a vertex2facet convolution can be inserted before the final classifier for facet-based predictions. For classification, the decoder is replaced by a global mesh pooling. \nThe bottom row shows (i) the `Initial Convolution' \nwhich propagates input features from facet to vertex; (ii) the `MeshEncoderBlock' that is exploited in high-resolution layers for feature extraction, along the `DualEncoderBlock' that applies to low-resolution layers such that feature extraction can go beyond disconnected components of the mesh, and (iii) the `DecoderBlock' for feature unsampling from low-resolution meshes to high-resolution meshes. \nIn the `Initial Convolution', the facet2facet convolution is not applicable if textures are not provided. We apply `MeshEncoderBlock' to   $\\mathcal{T}^1,\\mathcal{T}^2$ and `DualEncoderBlock' to   $\\mathcal{T}^3,\\mathcal{T}^4,\\mathcal{T}^5$. They repeat $s{=}2$ and $s{=}4$ times, respectively. \n}\n\\label{fig:Pi2}\n\\vspace{-2mm}\n\\end{figure*}\n\n\\vspace{-2mm}\n\\section{PicassoNet++}\\label{sec:network_Pi2}\nBesides extending Picasso beyond the preliminary work in \\cite{lei2021picasso}, this article also considerably enhances PicassoNet~\\cite{lei2021picasso} to introduce a more effective network  PicassoNet++. \nCompared to \\cite{lei2021picasso}, PicassoNet++ is deeper  yet faster for geometric feature learning over 3D meshes. \nWe show its configuration in the Fig.~\\ref{fig:Pi2}(top), which also  includes a decoder part (boxed) for dense\nparsing. For classification, the decoder is replaced with an average global pooling layer. PicassoNet++ takes \\textit{intact} meshes rather than mesh crops as input samples.  \nWe apply strided mesh decimation by specifying the expected vertex size using a stride parameter. This removes the \nconstraint of fixing vertex sizes across different samples. \n\n{\\color{black}\nFor decimated meshes of low resolution, \nPicassoNet++ exploits the point cloud convolution of \\S~\\ref{subsec:pcloud_conv} to extract features across disconnected components of the mesh, similar to \\cite{lei2021picasso}.\nThe Euclidean neighborhood in point cloud convolution allows larger context to be established such that feature learning can go beyond geodesic connections. \nWhereas the previous methods \\cite{lei2021picasso,schult2020dualconvmesh} explore Euclidean neighborhood extensively in every \nnetwork layer, PicassoNet++ employs it only when the input mesh is significantly decimated. This results in considerable computational advantage. \nWe show in \\S~\\ref{subsec:dual_layers_ablation} that point cloud convolution is unnecessary at high-resolution layers since the meshes are already well-connected. \n\nOur network exploits two types of encoder blocks to extract features from the meshes of different resolutions. One is \\textit{mesh} encoder block, which comprises a repetitive building unit that uses only mesh convolutions. The other is \\textit{dual} encoder block, whose repetitive building unit is mesh convolutions accompanied by one point cloud convolution. We use identical feature channels for mesh and point cloud convolutions in the dual encoder blocks. PicassoNet++ employs mesh encoder blocks in high-resolution layers and dual encoder blocks in the low-resolution layers.}\nOur network inherits the primary building units of PicassoNet. However, it applies skip connections to every building unit of the encoder block. Besides, it leverages the concatenation-style skip connection of DenseNet~\\cite{huang2017densely}, instead of the addition-style skip connection of ResNet~\\cite{he2016deep}.\nFigure~\\ref{fig:Pi2}(bottom) depicts the major blocks of PicassoNet++, including its initial convolution, mesh and dual encoder blocks, as well as the decoder blocks. We employ max mesh pooling to down-sample the network features.\n\n{\\color{black}\nPicassoNet++ takes geometry and textures of each facet as input features. This differs from PicassoNet~\\cite{lei2021picasso}, which follows point cloud networks and expects vertex features as inputs.\nTo propagate the input features from facet to vertex, we build the initial layer of PicassoNet++ using a $1{\\times}1$ convolution with a parallel facet2facet convolution, followed by a feature fusion of addition `$\\textcircled{+}$' and a facet2vertex convolution. \nFor the decoder, we use  $1{\\times}1$ convolutions and mesh unpooling of PicassoNet~\\cite{lei2021picasso} to upsample the features. PicassoNet++ applies batch normalization to all of its modular convolutions. \nThe proposed network repeats the building unit of its dual encoder blocks $s{=}$4 times, resulting in a deeper network than PicassoNet. For classification, we replace the single $1{\\times}1$ convolution for the final predictions in dense parsing with two fully connected (FC) layers. \n}\n\n\\section{Experiments}\\label{sec:experiment}\nWe establish the effectiveness of PicassoNet++ by evaluating it for (i)~shape analysis with synthetic meshes and (ii)~semantic scene surface parsing using real-world meshes. We use the \nShapeNetCore \\cite{chang2015shapenet} dataset, along with SHREC \\cite{lian2011shape}, CUBE \\cite{hanocka2019meshcnn}, COSEG \\cite{wang2012active}, HUMAM \\cite{maron2017convolutional} and FAUST \\cite{bogo2014faust} \nfor shape analysis. \nFor real-world scene surface parsing, we employ the large-scale datasets S3DIS \\cite{armeni20163d} and ScanNet \\cite{dai2017scannet}. \nEach dataset is discussed with its related experiments. \nFor all shape analysis experiments in \\S~\\ref{sec:synthetic_data}, we normalize the scales of all shape meshes, but keep the original scales of real-world scene surfaces in \\S~\\ref{sec:realistic_data}. \nThis is because scale information is more important for real-world scenes. \nWe employ the proposed facet geometrics $[{\\boldsymbol \\ell}, {\\boldsymbol \\theta}, {\\bf n}]$ as default  input features for synthetic shapes, and $[{\\boldsymbol \\ell}, {\\boldsymbol \\theta}, {\\bf n}, {\\bf h}]$ as default geometric features for the real-world data. Additionally, real-world data also provides facet textures for experiments.\n\n\\vspace{1mm}\n\\noindent{\\textbf{Data Augmentation:}} We apply standard geometric transformations, \\eg random flipping, scaling and shifting to the mesh vertices. We perform random rotations along the gravitational axis for aligned data in ShapeNetCore, ScanNet and S3DIS, and free data rotation  along all axes for other datasets. We also randomly drop the vertices and facets of meshes to obtain more training data. \nWhen textures are available, we apply color shifting, jittering,\nand standard photometric augmentation techniques as in image processing \\cite{liu2016ssd,choy20194d}.\n\n\\vspace{1mm}\n\\noindent{\\textbf{Network Configuration:}}\nPicassoNet++ contains 6 hierarchical layers of mesh resolutions from $\\mathcal{T}^0$ to $\\mathcal{T}^5$ that use mesh decimation. \nWe set different decimation strides for different datasets, discussed in the respective experiments.\nOur network uses dual encoder blocks only at coarse resolutions $\\mathcal{T}^3,\\mathcal{T}^4,\\mathcal{T}^5$.\nThe range search radii for point-based convolutions of $\\mathcal{T}^3$ to $\\mathcal{T}^5$ are $0.2$, $0.4$, $0.8$, respectively. {\\color{black}We use spherical harmonics of degree $L{=}3$, which results in a kernel size of $T{=}16$ for all mesh convolutions and $T{=}17$ for the extended point cloud convolution. \nWe train the network with Adam Optimizer \\cite{kingma2015adam} and exponential decay. The initial learning rate is 0.001, with 0.98 decay rate after each epoch.} \n\n\\begin{table*}[!t]\n\\centering\n\\caption{\\color{black}Shape analysis performance of our network on the synthetic datasets. \n}\n\\label{tab:shape_analysis}\n\\begin{adjustbox}{width=0.9\\textwidth}\n{\n\\begin{tabular}{l|c|c|c|c|c|c|c|c|c}\n\\hline\n\\multirow{3}{*}{Method}& \\multicolumn{4}{c|}{Classification} & \\multicolumn{4}{c|}{Semantic Labelling} & Correspondence \\\\\n\\cline{2-4}\\cline{5-10} \n& \\multirow{2}{*}{ShapeNetCore}& \\multicolumn{2}{c|}{SHREC} & \\multirow{2}{*}{CUBE} &  \\multicolumn{3}{c|}{COSEG} & \\multirow{2}{*}{HUMAN} &  \\multirow{2}{*}{FAUST} \\\\\n \\cline{3-4}\\cline{6-8} \n & & Split 16 & Split 10 &&  aliens & chairs & vases &  & \\\\\n \\hline\n \\# Train/test samples &40866/10261 &480/120 & 300/300& 5100/660& 169/29&337/60 & 252/45& 381/18& 80/20\\\\\n \\hline\nGI \\cite{sinha2016deep} & --& 96.6& 88.6& -- &  --& --& --&  --&--\\\\\nGWCNN \\cite{ezuz2017gwcnn} & --& 96.6& 90.3& -- &  --& --& --&  --&--\\\\\nPointNet++ \\cite{qi2017pointnetplusplus} & --& --& --& 64.3 &   --& --& --& --&--\\\\\n MeshCNN \\cite{hanocka2019meshcnn} & --& 98.6& 91.0& 92.2 &  96.3& 93.0& 92.4& 85.4&--\\\\\n PD-MeshNet \\cite{milano2020primal} & --& 99.7& 99.1& 94.4&  98.2& 97.2& 95.4 & 85.6&--\\\\\n {\\color{black}HodgeNet~\\cite{smirnov2021hodgenet}} & -- & 99.2& 94.7& --&  96.0& 95.7& 90.3 & 85.0&--\\\\ \n {\\color{black}SubdivNet~\\cite{hu2022subdivision}} &-- & \\textbf{100.0}& \\textbf{100.0}& \\textbf{100.0}&  --& --& -- & \\textbf{91.7}&--\\\\ \n GCNN \\cite{masci2015geodesic}& --& --& -- & --& --& --& --&  --& 65.4\\\\\nACNN \\cite{boscaini2016learning}& --& --&-- & --& --& --& --&  --& 63.0 \\\\\nMoNet \\cite{monti2017geometric} & --& --& -- & --& --& --& --&  --& 90.0\\\\\nPointContrast \\cite{monti2017geometric} & 85.1&--& --& -- & --& --& --& --&  --\\\\\n\\hline\n{\\color{black}PicassoNet++ (Prop.)} &\\textbf{87.3}&\\textbf{100.0} & \\textbf{100.0}& \\textbf{100.0} &\n\\textbf{98.8} & \\textbf{99.5} & \\textbf{95.6} & 91.5 &\n\\textbf{100.0}\\\\\n\\hline\n\\multicolumn{10}{l}{$^{\\dagger}$SubdivNet~\\cite{hu2022subdivision} benefits from remeshing and finer geometric details of input shapes, whereas other methods do not.}\n\\end{tabular}\n}\n\\end{adjustbox}\n\\end{table*}\n\n\\vspace{-2mm}\n\\subsection{Shape Analysis}\n\\label{sec:synthetic_data}\nWe evaluate our network performance on shape classification and facet labelling tasks using synthetic data. The input meshes are decimated with strides 1, 3, 2, 2, 2, respectively on ShapeNetCore and FAUST, while 1, 1.5, 1.5, 1.5, 1.5 on the other datasets due to their limited number of input vertices. \nHere, the first stride 1 indicates the $\\mathcal{T}^0$ is not decimated and it is identical to $\\mathcal{T}^1$. Therefore, the first pooling operation in PicassoNet++ is not applied.\nWe train the network using batch size 6 for FAUST, and 64 for others. A weight decay of $10^{-5}$ is applied to all datasets other than ShapeNetCore for their limited training samples.\n\n\\vspace{1mm}\n\\noindent\\textit{1) Classification}\n\\vspace{1mm}\\\\\n\\noindent\\textbf{ShapeNetCore:} The ShapeNetCore dataset \\cite{chang2015shapenet} is a large-scale and information-rich repository of 3D models collected from online  resources. It contains around 51,000\nshapes of 55 common objects. We follow the original standard split to evaluate the performance of PicassoNet++ for shape classification. In specific, the split specifies $80\\%$ samples for training and $20\\%$ for testing. Table \\ref{tab:shape_analysis} shows that our network outperform the sparse Residual network of PointContrast \\cite{xie2020pointcontrast} by 2.5\\%.\nThis indicates the desirability of processing mesh data with  PicassoNet++ for shape analysis. We prepare the input meshes to our network by uniformly sampling 3,000 points on the raw mesh, and triangulating them using the algorithm provided by \\cite{pointcloud2mesh}. \nWe note that these meshes are not ideal and actual watertight meshes should result in even better performance of our network. \n\n\\vspace{1mm}\n\\noindent\\textbf{SHREC:} The SHREC dataset \\cite{lian2011shape, hanocka2019meshcnn} contains 600 watertight meshes from 30 classes, with 20 samples in each class. Shape classification is defined on split 16 and 10 of the dataset. The split number here indicates the number of training samples  per class. Following the setup in \\cite{hanocka2019meshcnn}, we report the average results over three randomly generated sets. Table \\ref{tab:shape_analysis} shows excellent performance of PicassoNet++.\n\n\\vspace{1mm}\n\\noindent\\textbf{CUBE:} The CUBE\nEngraving dataset \\cite{hanocka2019meshcnn} includes 22 object categories with 200 mesh samples per class. Those samples are created by insetting the MPEG-7 binary\nshapes \\cite{latecki2000shape} into random locations of a cube. \nEach cube consists of about 250 vertices and 500 facets. Table~\\ref{tab:shape_analysis} shows that our network achieves 100\\% accuracy on this dataset.\n\n\\vspace{1mm}\n\\noindent\\textit{2) Semantic Labelling}\n\\vspace{1mm}\\\\\n\\vspace{1mm}\n\\noindent\\textbf{COSEG:} The COSEG dataset~\\cite{wang2012active} defines semantic labelling tasks over three independent categories, \\ie \\textit{aliens}, \\textit{chairs} and \\textit{vases}.\nThe alien category contains 169 training samples, 29 test samples and 4 part labels. The chair category contains 337 training samples, 60 test samples and 3 part labels. The vase category contains 252 training samples, 45 test samples and 4 part labels. \nWe follow \\cite{milano2020primal} and evaluate our network under semantic facet labelling. \nTable \\ref{tab:shape_analysis} reports the consistent superior performance of PicassoNet++.\n\n\\vspace{1mm}\n\\noindent\\textbf{HUMAN:}\nThe HUMAN dataset~\\cite{maron2017convolutional} defines semantic facet labelling as segmenting the human body into 8 parts, which include  \\textit{head}, \\textit{hand}, \\textit{forearm}, \\textit{upperarm}, \\textit{body}, \\textit{thigh}, \\textit{leg} and \\textit{foot}. \nIt contains 381 training samples and 18 test samples. Each mesh sample is composed of 750 vertices and 1,500 facets. Table \\ref{tab:shape_analysis} suggests that the segmentation result of PicassoNet++ outperforms the previous methods by a large margin. \n\n\\vspace{1mm}\n\\noindent\\textit{3) 3D Manifold Correspondence}\n\\vspace{1mm}\\\\\n\\noindent\\textbf{FAUST:} The FAUST\ndataset~\\cite{bogo2014faust} is widely used for correspondence matching of 3D manifold meshes \\cite{masci2015geodesic,boscaini2016learning,monti2017geometric}.\nIt consists of 10 different subjects with 10 different poses each, resulting in 100 watertight meshes with exact ground-truth correspondence. Each\nshape is represented as a mesh with 6,890 vertices and 13,776 facets. The convention is to utilize the first pose of the\nfirst subject (i.e.~the zeroth scan `000') as the reference, the first 80 shapes for training and the rest 20 shapes for testing. We follow MoNet~\\cite{monti2017geometric} and formulate the correspondence task as a multi-class labelling problem. Similar to its configurations in semantic labelling, the proposed network accomplishes this correspondence labelling with softmax function. In specific, the number of classes is defined as 6,890, \\ie \nthe number of vertices in the reference mesh. We report the matching accuracy of different methods for correspondences without geodesic error in Table \\ref{tab:shape_analysis}. \nFeature representation of our network achieves 100\\% accuracy, which is considerably better than other methods.  \n\n\\vspace{-2mm}\n\\subsection{Real-world Datasets}\\label{sec:realistic_data}\nReal-world scene surfaces have heterogeneous vertex and facet sizes, and  varying scales. We decimate the input meshes using strides $4$, $3$, $3$, $2$, $2$, respectively, \nto construct network layers of mesh resolutions from $\\mathcal{T}^1$ to $\\mathcal{T}^5$.\nThe network is trained with batch size 16.\n\n\\begin{table*}[!t]\n\\caption{\\color{black}Performance of PicassoNet++ on the fifth fold (Area 5) of S3DIS dataset. It outperforms PointTransformer while using only half the number of training parameters (2.5M vs. 4.9M). Besides, our results are obtained by taking each \\textit{complete} scene as input. \n}\\label{tab:s3dis_seg_review} \n\\vspace{-1.5mm}\n\\begin{adjustbox}{width=1\\textwidth}\n{\\Huge\\begin{tabular}{l|ccc|ccccccccccccc}\n\\hline\nMethod& OA& mAcc & mIoU & ceiling & floor & wall & beam & column & window & door & table & chair & sofa & bookcase & board & clutter \\\\\n\\hline\n PointNet \\cite{qi2017pointnet}& - &49.0 &41.1 &88.8 &97.3 &69.8 &\\textbf{0.1} &3.9 &46.3 &10.8 &58.9 &52.6  &5.9 &40.3  &26.4 &33.2\\\\\nSEGCloud \\cite{tchapmi2017segcloud} & - &57.4 &48.9 &90.1 &96.1 &69.9 &0.0 &18.4 &38.4 &23.1 &70.4 &75.9 &40.9 &58.4 &13.0 &41.6\\\\\nTangent-Conv \\cite{tatarchenko2018tangent}& 82.5 &62.2 &52.8 &- &- &- &- &- &- &- &- &- &- &- &- &-\\\\\nSPG \\cite{landrieu2017large} & 86.4 &66.5 &58.0 &89.4 &96.9 &78.1 &0.0 &42.8 &48.9 &61.6&75.4 &84.7 &52.6 &69.8  &2.1 &52.2\\\\\nPointCNN \\cite{li2018pointcnn}& 85.9& 63.9& 57.3& 92.3& 98.2 &79.4& 0.0 &17.6& 22.8& 62.1& 74.4& 80.6& 31.7& 66.7& 62.1& 56.7\\\\\nSSP+SPG \\cite{landrieu2019point}& 87.9 & 68.2 &61.7&- &- &- &- &- &- &- &- &- &- &- &- &-\\\\\nGACNet \\cite{wang2019attention}& 87.8 & - &62.9&92.3 &98.3 &81.9 &0.0 &20.4 &59.1 &40.9 &78.5 &85.8 &61.7 &70.8 &74.7 &52.8\\\\\nSPH3D-GCN \\cite{lei2020spherical}& 87.7 &65.9 &59.5 &93.3 &97.1 &81.1 &0.0 &33.2 &45.8 &43.8  &79.7 &86.9 &33.2 &71.5  &54.1 &53.7\\\\\nSegGCN \\cite{lei2020seggcn} & 88.2 &70.4 &63.6 &93.7 &\\textbf{98.6} &80.6 &0.0\n&28.5 &42.6 &74.5  &80.9\n&88.7\n&69.0\n&71.3  &44.4 &54.3\\\\\n MinkowskiNet \\cite{choy20194d} & -& 71.7&65.3 &- &- &- &- &- &- &- &- &- &- &- &- &-\\\\\nKPConv \\cite{thomas2019kpconv}& - & 72.8 &67.1 &92.8& 97.3& 82.4 &0.0& 23.9& 58.0& 69.0& 81.5& \\textbf{91.0}& \\textbf{75.4}& 75.3 & 66.7& 58.9\\\\\nDCM-Net \\cite{schult2020dualconvmesh} & - & 71.2 & 64.0 &92.1 &96.8 &78.6 &0.0 &21.6 &61.7 &54.6 &78.9 &88.7 &68.1 &72.3 &66.5 &52.4\\\\\n{\\color{black}PCT~\\cite{guo2021pct}} &-&67.7 & 61.3 & 92.5 &98.4 &80.6 &0.0 & 19.4 & 61.6& 48.0& 85.2& 76.6& 67.7& 46.2&  67.9& 52.3 \\\\\nPointTransformer\\cite{zhao2021point} & 90.8& 76.5& 70.4& 94.0& 98.5& 86.3& 0.0& 38.0& 63.4& 74.3& \\textbf{89.1}& 82.4& 74.3& \\textbf{80.2}& 76.0& 59.3\\\\\n\\hline\n{\\color{black}PicassoNet~\\cite{lei2021picasso}} & 89.4 &70.9 &64.6 &93.3 &97.7 &83.5 &0.0\n&31.9 &53.4 &69.2  &81.7\n&88.0\n&50.5\n&74.3  &58.2 &57.9\\\\\n{\\color{black}PicassoNet++ (Prop.)} & \\textbf{91.3} &\\textbf{77.2} &\\textbf{71.0} &\\textbf{94.4}&98.4 &\\textbf{87.5} &0.0 &\\textbf{46.9} &\\textbf{63.7} &\\textbf{75.5}  &81.4 &90.3\n&71.3&76.2 &\\textbf{76.7}&\\textbf{61.1}\\\\\n\\hline\n\\end{tabular}}\n\\end{adjustbox}\n\\end{table*}\n\n\\begin{table*}[!t]\n\\centering\n\\caption{\\color{black}Semantic vertex labelling results on the test set of ScanNet. The training parameters of KPConv, MinkowskiNet, and DCM-Net are 25.6M, 29.8M, 76.1M, respectively, whereas PicassoNet++ uses only 2.5M parameters. }\n\\label{tab:scannet_test}\n\\vspace{-1.5mm}\n\\begin{adjustbox}{width=1\\textwidth}\n{\\Huge\\begin{tabular}{l|c|cccccccccccccccccccc}\n\\hline\nMethod & mIoU & floor &wall &chair &sofa &table& door& cab& bed &desk &toil &sink &wind& pic &bkshf &curt &show &cntr &fridg& bath &other\\\\\n\\hline\nSPLATNET$_{\\text{3D}}$ \\cite{su2018splatnet}& 39.3&92.7&69.9&65.6&51.0&38.3&19.7&31.1&51.1&32.8&59.3&27.1&26.7&0.0&60.6&40.5&24.9&24.5&0.1&47.2&22.7\\\\\nTangent-Conv~\\cite{tatarchenko2018tangent}& 43.8&91.8&63.3&64.5&56.2&42.7&27.9&36.9&64.6&28.2&61.9&48.7&35.2&14.7&47.4&25.8&29.4&35.3&28.3&43.7&29.8\\\\\nPointCNN~\\cite{li2018pointcnn} &45.8&94.4&70.9&71.5&54.5&45.6&31.9&32.1&61.1&32.8&75.5&48.4&47.5&16.4&35.6&37.6&22.9&29.9&21.6&57.7&28.5\\\\\nPointConv~\\cite{wu2019pointconv}& 55.6&94.4&76.2&73.9&63.9&50.5&44.5&47.2&64.0&41.8&82.7&54.0&51.5&18.5&57.4&43.3&57.5&43.0&46.4&63.6&37.2\\\\\nSPH3D-GCN~\\cite{lei2020spherical}& 61.0&93.5&77.3&79.2&70.5&54.9&50.7&53.2&77.2&57.0&85.9&60.2&53.4&4.6&48.9&64.3&70.2&40.4&51.0&85.8&41.4\\\\\nKPConv~\\cite{thomas2019kpconv}& 68.4&93.5&81.9&81.4&78.5&61.4&59.4&64.7&75.8&60.5&88.2&\\textbf{69.0}&63.2&18.1&78.4&77.2&80.5&47.3&58.7&84.7&45.0 \\\\\nSegGCN~\\cite{lei2020seggcn}& 58.9&93.6&77.1&78.9&70.0&56.3&48.4&51.4&73.1&57.3&87.4&59.4&49.3&6.1&53.9&46.7&50.7&44.8&50.1&83.3&39.6\\\\\n{\\color{black}CBL~\\cite{tang2022contrastive}} & 69.3 &\t74.3\t&79.4 &\t65.5\t&68.4\t&82.2\t&49.7\t&\\textbf{71.9}\t&62.2\t&61.7\t&\\textbf{97.7}\t&44.7\t&33.9\t&75.0\t&66.4\t&70.3\t&79.0\t&\\textbf{59.6}\t&94.6\t&85.5\t&64.7 \\\\\nMinkowskiNet \\cite{choy20194d} & \\textbf{73.6}&95.1&85.2&84.0&77.2&\\textbf{68.3}&\\textbf{64.3}&70.9&\\textbf{81.8}&\\textbf{66.0}&87.4&67.5&72.7&28.6&\\textbf{83.2}&\\textbf{85.3}&\\textbf{89.3}&52.1&\\textbf{73.1}&\\textbf{85.9}&\\textbf{54.4} \\\\\nDCM-Net~\\cite{schult2020dualconvmesh} & 65.8&94.1&80.3&81.3&72.7&56.8&52.4&61.9&70.2&49.4&82.6&67.5&63.7&\\textbf{29.8}&80.6&69.3&82.1&46.8&51.0&77.8&44.9\\\\\n\\hline\n{\\color{black}PicassoNet++ (Prop.)} & 69.2&\\textbf{95.2}&\\textbf{85.4}&\\textbf{86.6}&\\textbf{81.0}&56.4&62.6&67.7&77.2&50.9&90.3&68.9&\\textbf{72.9}&22.5&78.6&84.8&70.4&51.7&54.5&73.2&53.6 \\\\\n\\hline\n\\end{tabular}}\n\\end{adjustbox}\n\\end{table*}\n\n\\vspace{1mm}\n\\noindent\\textbf{S3DIS.} The Stanford 3D Indoor Spaces (S3DIS) dataset~\\cite{armeni20163d} is a large-scale real-world dataset. It has sparse 3D meshes and dense 3D point clouds of 6 large-scale indoor areas. The data was collected, using the Matterport scanner, from three different buildings in Stanford University campus. \nThe semantic labelling task on this dataset is defined to classify 13  classes, namely \\emph{ceiling, floor, wall, beam, column, window, door, table,\nchair, sofa, bookcase, board}, and \\emph{clutter}. We follow the standard training/testing protocol where Area 5 is used as the test set and the remaining 5 Areas as the training set \\cite{landrieu2017large,li2018pointcnn,qi2017pointnet}. \nPerformance of each method is evaluated  for Overall Accuracy (OA), mean  Accuracy of all classes (mAcc),   Intersection Over Union of each class (IoU) and their average over all classes (i.e.~mIoU). mIoU is normally considered the most reliable among these metrics. \n\nDCM-Net \\cite{schult2020dualconvmesh} prepared its training meshes and labels based on the original meshes with over-tessellation and interpolation. In contrast, we generate the scene meshes by triangulating the labelled point cloud provided in the dataset.  In specific, we voxelize the raw point cloud using a voxel size of 0.03 (3$cm$), and triangulate them into meshes \\cite{pointcloud2mesh}. We guarantee all of the created meshes to be \\textit{edge-manifold}. In this experiment, we utilize the default facet geometrics together with rendered facets of texture resolutions determined by $(\\alpha,\\beta)=(3,1)$ as input features to PicassoNet++.\nWe train and test the network using complete scenes as input samples. It can be noticed from Table~\\ref{tab:s3dis_seg_review} that \nour method significantly outperforms the previous methods. \n{\\color{black}The average inference time of PicassoNet++ in Pytorch is 170 ms\nacross the 68 (voxelized) test samples in Area~5, using a single NVIDIA 3090 GPU.} The final results reported in Table~\\ref{tab:s3dis_seg_review} are computed on the original point cloud. We transfer the voxelized predictions to dense predictions using nearest neighborhood search.\n\n\\vspace{2mm}\n\\noindent\\textbf{ScanNet.}  The ScanNet dataset~\\cite{dai2017scannet} comprises \nreconstructed room meshes from RGB-D video frames, and has rich annotations for semantic vertex labelling.\nIt includes 1,613 meshes in total, among which 1,213 scenes are used for training and 300 scenes for validation. We ignore the 100 test samples in our experiment as their labels are unavailable. The dataset contains 40 class labels, while 20 are recommended for performance evaluation. We train and test our network without cropping the complete scenes into smaller samples.\n\nOur network takes voxelized mesh of grid size 2$cm$ as the input. \n{\\color{black}Similar to the experiments on S3DIS, we use the geometric features together with rendered textures of each facet as input features. The texture resolution is  $(\\alpha,\\beta)=(3,3)$.\nOur results on the validation set of ScanNet is 71.8\\%,} which is 3.6\\% higher than DCM-Net and is very competitive to the 72.2\\% of the top performer MinkowskiNet. MinkowskiNet has 29.8M training parameters \\cite{xie2020pointcontrast}, while our network produces similar results using just {\\color{black}2.5M parameters}. We report the results of our network on the test benchmark of ScanNet in Table~\\ref{tab:scannet_test}, which validates that PicassoNet++ is very competitive to the top performer. {\\color{black}We note that \nPicassoNet++ takes 180 ms on average in Pytorch to process per (voxelized) mesh on a single NVIDIA RTX 3090 GPU.}\n\n\\section{Further Analysis}\n\\label{sec:ablation}\nIn this section, we provide further results to analyze the proposed approach. \n\\vspace{-2mm}\n{\\color{black}\n\\subsection{Varying the Degree of Spherical Harmonics}\nWe study influence of the degree $L$ of the spherical harmonics on the performance of PicassoNet++. The HUMAN dataset is utilized in this analysis. Table~\\ref{tab:diff_spharm_degree} summarizes the resulting network parameters and segmentation accuracy for different values of $L$. It can be noticed that generally, larger values of $L$ (\\eg,~$2$, $3$, $4$) result in more accurate predictions because they introduce more basis functions and parameters, leading to a better fit of a continuous filter. However, too large values of $L$ (\\eg,~$5$) can cause overfitting. By default, PicassoNet++ uses $L=3$. \n\n\\begin{table}[t]\n\\centering\n\\caption{\\color{black}Performance of PicassoNet++ on the \\\\HUMAN dataset using different degrees \\\\of spherical harmonics.}\\label{tab:diff_spharm_degree}\n\\vspace{-1.5mm}\n\\begin{adjustbox}{width=0.48\\textwidth}\n{\n\\large\n\\begin{tabular}{l|c|c|c|c|c|c}\n\\hline\nDegree ($L$) & 5 & 4 & 3 &2 & 1 & 0\\\\\n\\hline \n\\#Parameters &$2.81M$ & $2.67M$& $2.56M$& $2.48M$ & $2.42M$ & $2.38M$ \\\\ \n\\hline\nAccuracy & 91.0 & 91.4& 91.5 & 91.6& 90.9& 91.0\\\\\n\\hline\n\\end{tabular}\n}\n\\end{adjustbox}\n\\vspace{-2mm}\n\\end{table}}\n\n\\vspace{-2mm}\n\\subsection{Is Dual Convolution Always Necessary?}\\label{subsec:dual_layers_ablation}\nIt is known that point cloud convolutions can be time-consuming because of neighborhood search, which accompanies a significant computational burden in processing dense data~\\cite{lei2020spherical}. \nThe DCM-Net and original PicassoNet~\\cite{lei2021picasso}  use dual convolutions in every layer of their networks. \nIn comparison, PicassoNet++ utilizes dual convolutions only in its encoder blocks of coarse resolutions~$\\mathcal{T}^{3\\sim5}$. \nTo consolidate our pruning of point-based convolutions for PicassoNet++, we empirically evaluate if dual convolution is  necessary for every layer. In specific, we alter the PicassoNet++ by either adding point cloud convolutions to its encoders of resolution~$\\mathcal{T}^2$, or removing its existing point cloud convolution from the encoder of resolution~$\\mathcal{T}^3$.\nWe test the performance of these variants, and report their results as well as other details in Table~\\ref{tab:nLayer_dual}. ScanNet dataset is utilized in this experiment. From our findings, we can conclude that point-based convolutions can be eliminated from high resolution layers of the network, without affecting the network performance.  This led us to our eventual configuration of PicassoNet++, which is both effective and efficient. The reported inference time in the Table is for voxelized meshes of grid size 2$cm$. To further confirm our finding, we also conducted a similar  experiment on the HUMAN dataset. The results in Table~\\ref{tab:exp_human_dual} validate  the passive role of point cloud convolutions for dense meshes. \n\n\\begin{table}[!t]\n    \\centering\n     \\caption{\\color{black}Performance and runtime of PicassoNet++ on ScanNet while adding or deleting point cloud convolutions.  The list of NN search radii denotes the radii of neighborhood search for point cloud convolutions from resolution $\\mathcal{T}^2$ to $\\mathcal{T}^5$.}\n    \\label{tab:nLayer_dual}\n    \\vspace{-1.5mm}\n    \\begin{adjustbox}{width=0.47\\textwidth}{\n    \\begin{tabular}{l|c|c|c}\n    \\hline\n    Config & adding & \\textbf{used} & reducing \\\\\n    \\hline\n      Dual Levels &  $(\\mathcal{T}^2, \\mathcal{T}^3, \\mathcal{T}^4, \\mathcal{T}^5)$ & $(\\mathcal{T}^3, \\mathcal{T}^4, \\mathcal{T}^5)$& $(\\mathcal{T}^4, \\mathcal{T}^5)$  \\\\\n      \\hline\n      NN search radii & (0.1,~0.2,~0.4,~0.8)& (0.2,~0.4,~0.8)&  (0.4,~0.8) \\\\ \n      \\hline\n      Inference time (ms) & 240 & 180 & 160 \\\\\n      \\hline \n      mIoU  & 71.8 & 71.8 & 71.0 \\\\\n      \\hline\n      \\multicolumn{4}{l}\n      {$^{\\dagger}$Network runtime is reported in Pytorch.}\n    \\end{tabular}}\n    \\end{adjustbox}\n\\end{table}\n\n\\begin{table}[!t]\n\\centering\n\\caption{\\color{black}PicassoNet++ performance on the HUMAN dataset while adding or deleting point cloud convolutions.\n}\n\\label{tab:exp_human_dual}\n \\vspace{-1.5mm}\n\\begin{adjustbox}{width=0.48\\textwidth}\n{\n\\begin{tabular}{l|c|c|c|c|c}\n\\hline\n Dual &  $(\\mathcal{T}^2, \\mathcal{T}^3, \\mathcal{T}^4, \\mathcal{T}^5)$ &  $(\\mathcal{T}^3, \\mathcal{T}^4, \\mathcal{T}^5)$& $(\\mathcal{T}^4, \\mathcal{T}^5)$ & $(\\mathcal{T}^5)$ & None\\\\\n\\hline\nRadii & (0.1,~0.2,~0.4,~0.8) & (0.2,~0.4,~0.8)&  (0.4,~0.8) & (0.8) & N.A. \\\\ \n\\hline\nAcc &91.4& 91.5 & 91.1 & 90.4 & 88.7   \\\\\n\\hline\n\\end{tabular}\n}\n\\end{adjustbox}\n\\end{table}\n\n\\subsection{2D Embedding of the Shape Features}\nWe visualize the shape features learned by PicassoNet++ for the test samples of ShapeNetCore \\cite{chang2015shapenet} by showing their 2D embeddings in Fig.~\\ref{fig:tSNE_2Dembed_ShapeNetCore} using the t-SNE technique \\cite{van2008visualizing} for dimension reduction. For the feature representations, we use the 256-dimensional output of global pooling in the classification network. \nFrom Fig.~\\ref{fig:tSNE_2Dembed_ShapeNetCore}, it is clear that the shapes of most classes are distinctly represented, such as \\textit{car}, \\textit{bus},  \\textit{guitar}, \\textit{knife}, \\textit{vessel}, \\textit{rifle}, \\textit{faucet},  \\textit{airplane}, \\textit{chair}, \\textit{sofa}, \\textit{table}, etc. We also note that some classes are much closer to each other such that their  closeness is well-justified based on their shapes and semantics. For instance, buses are close to cars and pistols are close to rifles.\n\n\\begin{figure}[!t]\n    \\centering\n\\hspace{-1mm}\\includegraphics[width=0.49\\textwidth]{Figs/tSNE_name.png} \\\\\n    \\vspace{-2mm}\n    \\caption{\\color{black}2D embedding of the shape feature representations learned by PicassoNet++ for the test  samples of ShapeNetCore.}\n    \\label{fig:tSNE_2Dembed_ShapeNetCore}\n\\end{figure}\n\n\\subsection{Semantic Parsing Visualization}\nAs representative examples, we visualize the semantic parsing results of PicassoNet++ for shapes of human bodies \\cite{maron2017convolutional} and surfaces of real-world scenes \\cite{dai2017scannet} in Fig.~\\ref{fig:sem_vis}.\nThe network predicts most of the body parts and scene objects correctly. \nHowever, we see that segmenting parts and objects near boundaries sometimes  cause minor issues for our network. Nevertheless, such errors remain minor and do not occur too frequently. \nAlso notice that one of the test samples of human bodies has incorrect ground-truth label for the right leg. Such ground truth problems can result in a lower accuracy value of an accurate technique like ours. This also indicates that instead of highest prediction performance on a single dataset, highly competitive results across multiple datasets is sometimes more preferable in this domain. PicassoNet++ is able to achieve that.  \n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.49\\textwidth]{Figs/sem_vis_revision.pdf}\n    \\vspace{-2mm}\n    \\caption{\\color{black}The ground truth and our predictions for the shapes of human bodies and real-world scene surfaces. The black colors in the ground truth of textured surfaces indicate unlabelled objects.}\n    \\label{fig:sem_vis}\n     \\vspace{-2mm}\n\\end{figure}\n\n{\\color{black}\n\\section{Limitations}\nOur proposed modules and neural network focus on triangle meshes as inputs. Whereas meshes are often readily available, in some cases, \\eg, outdoor settings~\\cite{hackel2017semantic3d}, \\cite{behley2019semantickitti}, it is not the case. \nThe proposed mesh processing modules are obviously not applicable to those cases.  \nFortunately, significant progress has been made in the field of surface reconstruction utilizing deep learning techniques such as NeRF~\\cite{mildenhall2020nerf,tancik2022block}, SDF~\\cite{park2019deepsdf}, and deep computational geometry~\\cite{chen2022neural,lei2023circnet}. This progress is likely to make meshes more accessible in the future, which will resolve this limitation. \n\nAll of our proposed convolutions are translation-invariant. Furthermore, the vertex2facet and facet2facet convolutions are also rotation-invariant, as they utilize Barycentric coordinates. However, the facet2vertex convolution and the extended point cloud convolution do not hold rotation-invariance due to their dependence on normals and relative locations between the points. This is an intrinsic limitation of the underlying objective of this convolution, which must be considered in its application.  \n} \n\n\\vspace{-3mm}\n\\section{Conclusion}\nWe made two major contributions towards hierarchical neural modeling of heterogeneous 3D meshes. First, we\npresented Picasso - a modular implementation of multiple desired operations for geometric feature learning over 3D meshes. Picasso introduces novel mesh-amenable convolutional operations, mesh (un)poolings and GPU-accelerated mesh decimation. {\\color{black}This article considerably enhances our preliminary version of Picasso by incorporating continuous filter modelling and improved efficiency. Moreover, we also release Pytorch version of Picasso with this article along the Tensorflow support. The second major contribution of this article is our network, PicassoNet++. Enabled by the upgraded Picasso, our network is able to effectively process mesh signals, including primitive geometrics and textures on the facets, as inputs.} It also takes advantage of a new insight provided in this article regarding the passive role of point cloud convolutions in high resolution mesh feature learning. Leveraging that, PicassoNet++ learns features over 3D shapes and scene surfaces efficiently. Through extensive experiments, we established the highly competitive performance of PicassoNet++ for shape analysis and scene parsing.   \n\n \n\n\\begin{thebibliography}{1}\n\n\\bibitem{krizhevsky2017imagenet}\nA.~Krizhevsky, I.~Sutskever and G.~E.~Hinton. \n``Imagenet classification with deep convolutional neural networks,'' in\n\\emph{Advances in neural information processing systems}, 2017, pp. 1097–1105. \n\n\\bibitem{he2016deep}\nK.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image\n  recognition,'' in \\emph{Proceedings of the IEEE conference on computer vision\n  and pattern recognition}, 2016, pp. 770--778.\n\n\\bibitem{liu2016ssd}\nW.~Liu, D.~Anguelov, D.~Erhan, C.~Szegedy, S.~Reed, C.-Y. Fu, and A.~C. Berg,\n  ``{SSD}: Single shot multibox detector,'' in \\emph{European Conference on\n  Computer Vision}, 2016, pp. 21--37.\n\n\\bibitem{verma2018feastnet}\nN.~Verma, E.~Boyer, and J.~Verbeek, ``Feastnet: Feature-steered graph\n  convolutions for 3d shape analysis,'' in \\emph{Proceedings of the IEEE\n  conference on computer vision and pattern recognition}, 2018, pp. 2598--2606.\n\n\\bibitem{hanocka2019meshcnn}\nR.~Hanocka, A.~Hertz, N.~Fish, R.~Giryes, S.~Fleishman, and D.~Cohen-Or,\n  ``Meshcnn: a network with an edge,'' in \\emph{ACM Transactions on Graphics\n  (TOG)}, vol.~38, no.~4, pp. 1--12, 2019.\n\n\\bibitem{schult2020dualconvmesh}\nJ.~Schult, F.~Engelmann, T.~Kontogianni, and B.~Leibe, ``Dualconvmesh-net:\n  Joint geodesic and euclidean convolutions on 3d meshes,'' in\n  \\emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition}, 2020, pp. 8612--8622.\n\n\\bibitem{bronstein2017geometric}\nM.~M. Bronstein, J.~Bruna, Y.~LeCun, A.~Szlam, and P.~Vandergheynst,\n  ``Geometric deep learning: going beyond euclidean data,'' in \\emph{IEEE Signal\n  Processing Magazine}, vol.~34, no.~4, pp. 18--42, 2017.\n\n\\bibitem{yi2016scalable}\nL.~Yi, V.~G. Kim, D.~Ceylan, I.~Shen, M.~Yan, H.~Su, A.~Lu, Q.~Huang,\n  A.~Sheffer, L.~Guibas \\emph{et~al.}, ``A scalable active framework for region\n  annotation in {3D} shape collections,'' in \\emph{ACM Transactions on Graphics},\n  vol.~35, no.~6, p. 210, 2016.\n\n\\bibitem{dai2017scannet}\nA.~Dai, A.~X. Chang, M.~Savva, M.~Halber, T.~Funkhouser, and M.~Nie{\\ss}ner,\n  ``{ScanNet}: Richly-annotated 3d reconstructions of indoor scenes,'' in\n  \\emph{Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition}, 2017, pp. 5828--5839.\n\n\\bibitem{lei2020spherical}\nH.~Lei, N.~Akhtar, and A.~Mian, ``Spherical kernel for efficient graph\n  convolution on 3d point clouds,'' in \\emph{IEEE Transactions on Pattern Analysis\n  and Machine Intelligence}, 2020.\n\n\\bibitem{armeni20163d}\nI.~Armeni, O.~Sener, A.~R. Zamir, H.~Jiang, I.~Brilakis, M.~Fischer, and\n  S.~Savarese, ``{3D} semantic parsing of large-scale indoor spaces,'' in\n  \\emph{Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition}, 2016, pp. 1534--1543.\n\n\\bibitem{boscaini2016learning}\nD.~Boscaini, J.~Masci, E.~Rodol{\\`a}, and M.~Bronstein, ``Learning shape\n  correspondence with anisotropic convolutional neural networks,'' in\n  \\emph{Advances in neural information processing systems}, 2016, pp.\n  3189--3197.\n\n\\bibitem{monti2017geometric}\nF.~Monti, D.~Boscaini, J.~Masci, E.~Rodola, J.~Svoboda, and M.~M. Bronstein,\n  ``Geometric deep learning on graphs and manifolds using mixture model cnns,''\n  in \\emph{Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition}, 2017, pp. 5115--5124.\n\n\\bibitem{ranjan2018generating}\nA.~Ranjan, T.~Bolkart, S.~Sanyal, and M.~J. Black, ``Generating 3d faces using\n  convolutional mesh autoencoders,'' in \\emph{Proceedings of the European\n  Conference on Computer Vision (ECCV)}, 2018, pp. 704--720.\n\n  \\bibitem{gao2022robust}\nZ.~Gao, J.~Yan, G.~Zhai, J.~Zhang, and X.~Yang, ``Robust mesh representation\n  learning via efficient local structure-aware anisotropic convolution,''\n  \\emph{IEEE Transactions on Neural Networks and Learning Systems}, 2022.\n\n\\bibitem{garland1999quadric}\nM.~Garland, ``Quadric-based polygonal surface simplification [thesis],''\n  in \\emph{Pittsburgh: Carnegie Mellon University}, 1999.\n\n\\bibitem{garland1997surface}\nM.~Garland and P.~S. Heckbert, ``Surface simplification using quadric error\n  metrics,'' in \\emph{Proceedings of the 24th annual conference on Computer\n  graphics and interactive techniques}, 1997, pp. 209--216.\n\n\\bibitem{potamias2022neural}\nR.~A.~Potamias, S.~Ploumpis, and S.~Zafeiriou,\n ``Neural mesh simplification,''\n  in \\emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp. 18583--18592, 2022.\n\n\\bibitem{rossignac1993multi}\nJ.~Rossignac and P.~Borrel, ``Multi-resolution 3d approximations for rendering\n  complex scenes,'' in \\emph{Modeling in computer graphics}.\\hskip 1em plus\n  0.5em minus 0.4em\\relax Springer, 1993, pp. 455--465.\n\n\\bibitem{zhou2018open3d}\nQ.-Y. Zhou, J.~Park, and V.~Koltun, ``{Open3D: A modern library for 3D data\n  processing},'' in \\emph{arXiv preprint arXiv:1801.09847}, 2018.\n\n\\bibitem{choy20194d}\nC.~Choy, J.~Gwak, and S.~Savarese, ``4d spatio-temporal convnets: Minkowski\n  convolutional neural networks,'' in \\emph{Proceedings of the IEEE conference on\n  computer vision and pattern recognition}, 2019.\n\n\\bibitem{thomas2019kpconv}\nH.~Thomas, C.~R. Qi, J.-E. Deschaud, B.~Marcotegui, F.~Goulette, and L.~J.\n  Guibas, ``Kpconv: Flexible and deformable convolution for point clouds,'' in \n  \\emph{Proceedings of the IEEE International Conference on Computer Vision},\n  2019.\n\n\\bibitem{qi2017pointnetplusplus}\nC.~R. Qi, L.~Yi, H.~Su, and L.~J. Guibas, ``{PointNet++}: Deep hierarchical\n  feature learning on point sets in a metric space,'' in \\emph{Advances in Neural\n  Information Processing Systems}, 2017.\n\n\\bibitem{abadi2016tensorflow}\nM.~Abadi, P.~Barham, J.~Chen, Z.~Chen, A.~Davis, J.~Dean, M.~Devin,\n  S.~Ghemawat, G.~Irving, M.~Isard \\emph{et~al.}, ``Tensorflow: A system for\n  large-scale machine learning,'' in \\emph{12th $\\{$USENIX$\\}$ Symposium on\n  Operating Systems Design and Implementation ($\\{$OSDI$\\}$ 16)}, 2016, pp.\n  265--283.\n\n\\bibitem{paszke2019pytorch}\nA.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,\n  Z.~Lin, N.~Gimelshein, L.~Antiga \\emph{et~al.}, ``Pytorch: An imperative\n  style, high-performance deep learning library,'' in \\emph{Advances in neural\n  information processing systems}, vol.~32, pp. 8026--8037, 2019.\n\n\\bibitem{huang2019texturenet}\nJ.~Huang, H.~Zhang, L.~Yi, T.~Funkhouser, M.~Nie{\\ss}ner, and L.~J. Guibas,\n  ``Texturenet: Consistent local parametrizations for learning from\n  high-resolution signals on meshes,'' in \\emph{Proceedings of the IEEE\n  Conference on Computer Vision and Pattern Recognition}, 2019, pp. 4440--4449.\n\n\\bibitem{gopal2014mises}\nS.~Gopal and Y.~Yang, ``Von mises-fisher clustering models,'' in\n  \\emph{International Conference on Machine Learning}, 2014, pp. 154--162.\n\n\\bibitem{lei2021picasso}\nH.~Lei, N.~Akhtar, and A.~Mian, ``Picasso: A cuda-based library for deep\n  learning over 3d meshes,'' in \\emph{Proceedings of the IEEE Conference on\n  Computer Vision and Pattern Recognition}, 2021.\n\n\\bibitem{chang2015shapenet}\nA.~X. Chang, T.~Funkhouser, L.~Guibas, P.~Hanrahan, Q.~Huang, Z.~Li,\n  S.~Savarese, M.~Savva, S.~Song, H.~Su \\emph{et~al.}, ``{ShapeNet: An\n  information-rich 3D model repository},'' in \\emph{arXiv preprint\n  arXiv:1512.03012}, 2015.\n\n\\bibitem{lian2011shape}\nZ.~Lian, A.~Godil, B.~Bustos, M.~Daoudi, J.~Hermans, S.~Kawamura, Y.~Kurita,\n  G.~Lavoua, and P.~Dp~Suetens, ``Shape retrieval on non-rigid 3d watertight\n  meshes,'' in \\emph{Eurographics workshop on 3d object retrieval}, 2011.\n\n\\bibitem{wang2012active}\nY.~Wang, S.~Asafi, O.~Van~Kaick, H.~Zhang, D.~Cohen-Or, and B.~Chen, ``Active\n  co-analysis of a set of shapes,'' in \\emph{ACM Transactions on Graphics (TOG)},\n  vol.~31, no.~6, pp. 1--10, 2012.\n\n\\bibitem{maron2017convolutional}\nH.~Maron, M.~Galun, N.~Aigerman, M.~Trope, N.~Dym, E.~Yumer, V.~G. Kim, and\n  Y.~Lipman, ``Convolutional neural networks on surfaces via seamless toric\n  covers.'' in \\emph{ACM Trans. Graph.}, vol.~36, no.~4, pp. 71--1, 2017.\n\n\\bibitem{bogo2014faust}\nF.~Bogo, J.~Romero, M.~Loper, and M.~J. Black, ``Faust: Dataset and evaluation\n  for 3d mesh registration,'' in \\emph{Proceedings of the IEEE Conference on\n  Computer Vision and Pattern Recognition}, 2014, pp. 3794--3801.\n\n\\bibitem{lei2020seggcn}\nH.~Lei, N.~Akhtar, and A.~Mian, ``Seggcn: Efficient 3d point cloud segmentation\n  with fuzzy spherical kernel,'' in \\emph{Proceedings of the IEEE/CVF\n  Conference on Computer Vision and Pattern Recognition}, 2020, pp.\n  11\\,611--11\\,620.\n\n\\bibitem{wu20153d}\nZ.~Wu, S.~Song, A.~Khosla, F.~Yu, L.~Zhang, X.~Tang, and J.~Xiao, ``{3D}\n  {ShapeNets}: A deep representation for volumetric shapes,'' in\n  \\emph{Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition}, 2015, pp. 1912--1920.\n\n\\bibitem{maturana2015voxnet}\nD.~Maturana and S.~Scherer, ``Vox{N}et: A {3D} convolutional neural network for\n  real-time object recognition,'' in \\emph{IEEE/RSJ International Conference on\n  Intelligent Robots and Systems}.\\hskip 1em plus 0.5em minus 0.4em\\relax IEEE,\n  2015, pp. 922--928.\n\n\\bibitem{zeng20163dmatch}\nA.~Zeng, S.~Song, M.~Nie{\\ss}ner, M.~Fisher, J.~Xiao, and T.~Funkhouser,\n  ``{3DMatch}: Learning local geometric descriptors from {RGB-D}\n  reconstructions,'' in \\emph{Proceedings of the IEEE Conference on Computer\n  Vision and Pattern Recognition}, 2017, pp. 199--208.\n\n\\bibitem{EngelckeICRA2017}\nM.~Engelcke, D.~Rao, D.~Zeng~Wang, C.~Hay~Tong, and I.~Posner, ``{Vote3Deep}:\n  Fast object detection in {3D} point clouds using efficient convolutional\n  neural networks,'' in \\emph{IEEE International Conference on Robotics and\n  Automation}, June 2017.\n\n\\bibitem{graham20183d}\nB.~Graham, M.~Engelcke, and L.~van~der Maaten, ``{3D} semantic segmentation\n  with submanifold sparse convolutional networks,'' in \\emph{Proceedings of the\n  IEEE Computer Vision and Pattern Recognition}, pp. 18--22, 2018.\n\n\\bibitem{riegler2017octnet}\nG.~Riegler, A.~Osman~Ulusoy, and A.~Geiger, ``Oct{N}et: Learning deep 3d\n  representations at high resolutions,'' in \\emph{Proceedings of the IEEE\n  Conference on Computer Vision and Pattern Recognition}, 2017, pp. 3577--3586.\n\n\\bibitem{tatarchenko2018tangent}\nM.~Tatarchenko, J.~Park, V.~Koltun, and Q.-Y. Zhou, ``Tangent convolutions for\n  dense prediction in 3d,'' in \\emph{Proceedings of the IEEE Conference on\n  Computer Vision and Pattern Recognition}, 2018, pp. 3887--3896.\n\n\\bibitem{su2018splatnet}\nH.~Su, V.~Jampani, D.~Sun, S.~Maji, E.~Kalogerakis, M.-H. Yang, and J.~Kautz,\n  ``{SPLATNet}: Sparse lattice networks for point cloud processing,'' in\n  \\emph{Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition}, 2018, pp. 2530--2539.\n\n\\bibitem{qi2017pointnet}\nC.~R. Qi, H.~Su, K.~Mo, and L.~J. Guibas, ``{PointNet}: Deep learning on point\n  sets for {3D} classification and segmentation,'' in \\emph{Proceedings of the\n  IEEE Conference on Computer Vision and Pattern Recognition}, pp. 652--660,\n  2017.\n\n\\bibitem{klokov2017escape}\nR.~Klokov and V.~Lempitsky, ``Escape from cells: Deep kd-networks for the\n  recognition of 3d point cloud models,'' in \\emph{Proceedings of the IEEE\n  International Conference on Computer Vision}.\\hskip 1em plus 0.5em minus\n  0.4em\\relax IEEE, 2017, pp. 863--872.\n\n\\bibitem{wu2019pointconv}\nW.~Wu, Z.~Qi, and L.~Fuxin, ``Pointconv: Deep convolutional networks on 3d\n  point clouds,'' in \\emph{Proceedings of the IEEE Conference on Computer\n  Vision and Pattern Recognition}, 2019, pp. 9621--9630.\n\n\\bibitem{yi2017syncspeccnn}\nL.~Yi, H.~Su, X.~Guo, and L.~J. Guibas, ``Syncspeccnn: Synchronized spectral\n  cnn for 3d shape segmentation,'' in \\emph{Proceedings of the IEEE Conference\n  on Computer Vision and Pattern Recognition}, 2017, pp. 2282--2290.\n\n\\bibitem{simonovsky2017dynamic}\nM.~Simonovsky and N.~Komodakis, ``Dynamic edge-conditioned filters in\n  convolutional neural networks on graphs,'' in \\emph{Proceedings of the IEEE\n  Conference on Computer Vision and Pattern Recognition}, 2017.\n\n\\bibitem{de2016dynamic}\nB.~De~Brabandere, X.~Jia, T.~Tuytelaars, and L.~Van~Gool, ``Dynamic filter\n  networks,'' in \\emph{Advances in Neural Information Processing Systems},\n  2016.\n\n\\bibitem{li2018pointcnn}\nY.~Li, R.~Bu, M.~Sun, W.~Wu, X.~Di, and B.~Chen, ``Point{CNN}: Convolution on\n  x-transformed points,'' in \\emph{Advances in Neural Information Processing\n  Systems}, 2018, pp. 820--830.\n\n\\bibitem{wang2019attention}\nL.~Wang, Y.~Huang, Y.~Hou, S.~Zhang, and J.~Shan, ``Graph attention convolution\n  for point cloud semantic segmentation,'' in \\emph{The IEEE Conference on\n  Computer Vision and Pattern Recognition}, June 2019.\n\n  \\bibitem{du2022novel}\nZ.~Du, H.~Ye, and F.~Cao, ``A novel local-global graph convolutional method for\n  point cloud semantic segmentation,'' \\emph{IEEE Transactions on Neural\n  Networks and Learning Systems}, 2022.\n\n\\bibitem{huang2022dual}\nC.-Q. Huang, F.~Jiang, Q.-H. Huang, X.-Z. Wang, Z.-M. Han, and W.-Y. Huang,\n  ``Dual-graph attention convolution network for 3-d point cloud\n  classification,'' \\emph{IEEE Transactions on Neural Networks and Learning\n  Systems}, 2022.\n\n\\bibitem{li2021rethinking}\nS.~Li, Y.~Liu, and J.~Gall, ``Rethinking 3-d lidar point cloud segmentation,''\n  \\emph{IEEE Transactions on Neural Networks and Learning Systems}, 2021.\n\n\\bibitem{wu2020comprehensive}\nZ.~Wu, S.~Pan, F.~Chen, G.~Long, C.~Zhang, and S.~Y. Philip, ``A comprehensive\n  survey on graph neural networks,'' \\emph{IEEE transactions on neural networks\n  and learning systems}, vol.~32, no.~1, pp. 4--24, 2020.\n\n\\bibitem{li2020deep}\nY.~Li, L.~Ma, Z.~Zhong, F.~Liu, M.~A. Chapman, D.~Cao, and J.~Li, ``Deep\n  learning for lidar point clouds in autonomous driving: A review,'' \\emph{IEEE\n  Transactions on Neural Networks and Learning Systems}, vol.~32, no.~8, pp.\n  3412--3432, 2020.\n\n\\bibitem{lei2019octree}\nH.~Lei, N.~Akhtar, and A.~Mian, ``Octree guided cnn with spherical kernels for\n  3d point clouds,'' in \\emph{Proceedings of the IEEE Conference on Computer\n  Vision and Pattern Recognition}, 2019, pp. 9631--9640.\n\n\\bibitem{vaswani2017attention}\nA.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,\n  {\\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in\n  \\emph{Advances in neural information processing systems}, 2017, pp.\n  5998--6008.\n\n\\bibitem{zhao2021point}\nH.~Zhao, L.~Jiang, J.~Jia, P.~Torr, and V.~Koltun, ``Point transformer,'' in \\emph{Proceedings of the IEEE/CVF international conference on\n  computer vision}, 2021.\n\n\\bibitem{wu2022point}\nX. Wu, Y. Lao, L. Jiang, X. Liu, and H. Zhao, ``Point Transformer V2: Grouped Vector Attention and Partition-based Pooling,'' in \\emph{Neural Information Processing Systems}, 2022.\n\n\\bibitem{tang2022contrastive}\nL.~Tang, Y.~Zhan, Z.~Chen, B.~Yu, and D.~Tao, ``Contrastive boundary learning for point cloud segmentation,'' in \\emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\npp. 8489--8499, 2022.\n\n\\bibitem{masci2015geodesic}\nJ.~Masci, D.~Boscaini, M.~Bronstein, and P.~Vandergheynst, ``Geodesic\n  convolutional neural networks on riemannian manifolds,'' in \\emph{Proceedings\n  of the IEEE international conference on computer vision workshops}, 2015, pp.\n  37--45.\n\n\\bibitem{defferrard2016convolutional}\nM.~Defferrard, X.~Bresson, and P.~Vandergheynst, ``Convolutional neural\n  networks on graphs with fast localized spectral filtering,'' in\n  \\emph{Advances in Neural Information Processing Systems}, 2016, pp.\n  3844--3852.\n\n\\bibitem{liu2022uninet}\nJ. Liu, H. Li, G. Song, X. Huang, and Y. Liu,\n``UniNet: Unified Architecture Search with Convolution, Transformer, and MLP,'' in\n\\emph{European Conference on Computer Vision}, 2022.\n\n\\bibitem{guo2021pct},\nM.~Guo, J.~Cai, Z.~Liu, T.~Mu, R.~R.~Martin, and Shi-Min Hu\n``{PCT}: Point cloud transformer,'' in \\emph {Computational Visual Media}, vol.~7, pp. 187-199, 2021.\n\n\\bibitem{milano2020primal}\nF.~Milano, A.~Loquercio, A.~Rosinol, D.~Scaramuzza, and L.~Carlone,\n  ``Primal-dual mesh convolutional neural networks,'' in \\emph{Advances in\n  Neural Information Processing Systems}, 2020.\n\n\\bibitem{smirnov2021hodgenet}\nD.~Smirnov and J.~Solomon, \n  ``{HodgeNet}: Learning Spectral Geometry on Triangle Meshes,'' in \\emph{ACM Transactions on Graphics (TOG)}, vol.~40, no.~4, pp. 166:1--166:11, 2021.\n\n\\bibitem{hu2022subdivision}\nS.-M.~Hu, Zh.~Liu, M.~Guo, J.~Cai, J.~Huang, T.~Mu, and R.~R.~Martin,\n``Subdivision-based mesh convolution networks,''\n in \\emph{ACM Transactions on Graphics (TOG)},\n  vol. 41, no. 3, pp. 1--16, 2022.\n\n\\bibitem{liu2020neural}\nH. D. Liu, V. G. Kim, S. Chaudhuri, N. Aigerman, and A. Jacobson, ``Neural Subdivision,'' in \\emph{\nACM Transactions on Graphics (ToG)}, 2020.\n\n\\bibitem{Nekrasov213DV}\nA.~Nekrasov, J.~Schult, O.~Litany, B.~Leibe, and F.~Engelmann, \n  ``{Mix3D: Out-of-Context Data Augmentation for 3D Scenes},''\n in \\emph{International Conference on 3D Vision (3DV)}, 2021. \n\n\\bibitem{fey2019fast}\nM.~Fey and J.~E. Lenssen, ``Fast graph representation learning with pytorch\n  geometric,'' in \\emph{arXiv preprint arXiv:1903.02428}, 2019.\n\n\\bibitem{krizhevsky2012imagenet}\nA.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``{ImageNet} classification with\n  deep convolutional neural networks,'' in \\emph{Advances in Neural Information\n  Processing Systems}, 2012, pp. 1097--1105.\n\n\\bibitem{wang2018dynamic}\nY.~Wang, Y.~Sun, Z.~Liu, S.~E. Sarma, M.~M. Bronstein, and J.~M. Solomon,\n  ``Dynamic graph cnn for learning on point clouds,'' in \\emph{arXiv preprint\n  arXiv:1801.07829}, 2018.\n\n\\bibitem{hu2021vmnet}\nZ.~Hu, X.~Bai, J.~Shang, R.~Zhang, J.~Dong, X.~Wang, G.~Sun, H.~Fu, and C.~Tai, ``Vmnet: Voxel-mesh network for geodesic-aware 3d semantic\n  segmentation,'' in\n  \\emph{Proceedings of the IEEE International Conference on Computer Vision}, 2021.\n\n\\bibitem{tang2020searching}\nH.~Tang, Z.~Liu, S.~Zhao, Y.~Lin, J.~Lin, H.~Wang, and S.~Han, ``Searching\n  efficient 3d architectures with sparse point-voxel convolution,'' in\n  \\emph{European Conference on Computer Vision}, pp. 685--702, 2020.\n\n\\bibitem{chen2020simple}\nT.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton, ``A simple framework for\n  contrastive learning of visual representations,'' in \\emph{International\n  conference on machine learning},\n  pp. 1597--1607, 2020.\n\n\\bibitem{chollet2017xception}\nF.~Chollet, ``Xception: Deep learning with depthwise separable convolutions,''\n  in \\emph{Proceedings of the IEEE conference on computer vision and pattern\n  recognition}, 2017, pp. 1251--1258.\n\n\\bibitem{nair2010rectified}\nV.~Nair and G.~E. Hinton, ``Rectified linear units improve restricted boltzmann\n  machines,'' in \\emph{International Conference on Machine Learning}, 2010, pp. 807--814.\n\n\\bibitem{coxeter1961introduction}\nH.~S.~M. Coxeter, ``Introduction to geometry,'' 1961.\n\n\\bibitem{huang2017densely}\nG.~Huang, Z.~Liu, L.~Van Der~Maaten, and K.~Q. Weinberger, ``Densely connected\n  convolutional networks,'' in \\emph{Proceedings of the IEEE conference on\n  computer vision and pattern recognition}, 2017, pp. 4700--4708.\n\n\\bibitem{Szegedy2015googLeNet}\nC.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,\n  V.~Vanhoucke, and A.~Rabinovich, ``Going deeper with convolutions,'' in\n  \\emph{Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition}, 2015.\n\n\\bibitem{preparata2012computational}\nF.~P. Preparata and M.~I. Shamos, \\emph{Computational geometry: an\n  introduction}. Springer Science \\&\n  Business Media, 2012.\n\n\\bibitem{cohenspherical}\nT.~S. Cohen, M. Geiger, J. K{\\\"o}hler, and M. Welling,\n  ``Spherical CNNs,'' in \\emph{International Conference on Learning Representations}, 2018.\n\n\\bibitem{srivastava2014dropout}\nN.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov,\n  ``Dropout: a simple way to prevent neural networks from overfitting,'' in \n  \\emph{The journal of machine learning research}, vol.~15, no.~1, pp.\n  1929--1958, 2014.\n\n\\bibitem{kingma2015adam}\nD.~P. Kingma and J.~Ba, ``Adam: A method for stochastic optimization,''\n in \\emph{International Conference on Learning Representations}, 2015.\n\n\\bibitem{sinha2016deep}\nA.~Sinha, J.~Bai, and K.~Ramani, ``Deep learning 3d shape surfaces using\n  geometry images,'' in \\emph{European conference on computer vision}, 2016, pp. 223--240.\n\n\\bibitem{ezuz2017gwcnn}\nD.~Ezuz, J.~Solomon, V.~G. Kim, and M.~Ben-Chen, ``Gwcnn: A metric alignment\n  layer for deep shape analysis,'' in \\emph{Computer Graphics Forum}, vol.~36,\n  no.~5, pp.\n  49--57, 2017.\n\n\\bibitem{xie2020pointcontrast}\nS.~Xie, J.~Gu, D.~Guo, C.~R. Qi, L.~Guibas, and O.~Litany, ``Pointcontrast:\n  Unsupervised pre-training for 3d point cloud understanding,'' in\n  \\emph{European Conference on Computer Vision}, pp. 574--591, 2020.\n\n\\bibitem{pointcloud2mesh}\n``{Surface Reconstruction from Scattered Points, Mathworks 2020},''\n  \\url{https://au.mathworks.com}, accessed 17-Nov-2020.\n\n\\bibitem{latecki2000shape}\nL.~J. Latecki and R.~Lakamper, ``Shape similarity measure based on\n  correspondence of visual parts,'' in \\emph{IEEE Transactions on Pattern Analysis\n  and Machine Intelligence}, vol.~22, no.~10, pp. 1185--1190, 2000.\n\n\\bibitem{tchapmi2017segcloud}\nL.~Tchapmi, C.~Choy, I.~Armeni, J.~Gwak, and S.~Savarese, ``Segcloud: Semantic\n  segmentation of 3d point clouds,'' in \\emph{International Conference on 3D\n  Vision}, pp. 537--547, 2017.\n\n\\bibitem{landrieu2017large}\nL.~Landrieu and M.~Simonovsky, ``Large-scale point cloud semantic segmentation\n  with superpoint graphs,'' in \\emph{Proceedings of the IEEE Conference on\n  Computer Vision and Pattern Recognition}, 2018.\n\n\\bibitem{landrieu2019point}\nL.~Landrieu and M.~Boussaha, ``Point cloud oversegmentation with\n  graph-structured deep metric learning,'' in \\emph{Proceedings of the IEEE\n  conference on computer vision and pattern recognition}, 2019.\n\n\\bibitem{van2008visualizing}\nL.~Van~der Maaten and G.~Hinton, ``Visualizing data using t-sne.''\nin \\emph{Journal of machine learning research}, vol.~9, no.~11, 2008.\n\n  \\bibitem{hackel2017semantic3d}\nT.~Hackel, N.~Savinov, L.~Ladicky, J.~D. Wegner, K.~Schindler, and\n  M.~Pollefeys, ``Semantic{3D}.net: A new large-scale point cloud\n  classification benchmark,'' in \\emph{ISPRS Annals of the Photogrammetry, Remote\n  Sensing and Spatial Information Sciences}, vol. IV-1-W1, pp. 91--98, 2017.\n\n\\bibitem{behley2019semantickitti}\nJ.~Behley, M.~Garbade, A.~Milioto, J.~Quenzel, S.~Behnke, C.~Stachniss, and\n  J.~Gall, ``Semantickitti: A dataset for semantic scene understanding of lidar\n  sequences,'' in \\emph{Proceedings of the IEEE/CVF international conference on\n  computer vision}, 2019, pp. 9297--9307.\n\n\\bibitem{park2019deepsdf}\nJ.~J. Park, P.~Florence, J.~Straub, R.~Newcombe, and S.~Lovegrove, ``Deepsdf:\n  Learning continuous signed distance functions for shape representation,'' in\n  \\emph{Proceedings of the IEEE/CVF conference on computer vision and pattern\n  recognition}, 2019, pp. 165--174.\n\n\\bibitem{mildenhall2020nerf}\nB.~Mildenhall, P.~P. Srinivasan, M.~Tancik, J.~T. Barron, R.~Ramamoorthi, and\n  R.~Ng, ``Nerf: Representing scenes as neural radiance fields for view\n  synthesis,'' in \\emph{European conference on computer vision}, pp. 405--421, 2020.\n\n\\bibitem{tancik2022block}\nM.~Tancik, V.~Casser, X.~Yan, S.~Pradhan, B.~Mildenhall, P.~P. Srinivasan,\n  J.~T. Barron, and H.~Kretzschmar, ``Block-nerf: Scalable large scene neural\n  view synthesis,'' in \\emph{Proceedings of the IEEE/CVF conference on computer vision and pattern\n  recognition}, pp. 8248--8258, 2022.\n\n\\bibitem{chen2022neural}\nZ.~Chen, A.~Tagliasacchi, T.~Funkhouser, and H.~Zhang, \n  ``Neural Dual Contouring,'' in \\emph{ACM Transactions on Graphics (TOG)}, 2022.\n\n\\bibitem{lei2023circnet}\nH.~Lei, R.~Leng, L.~Zheng, and H.~Li, ``CircNet: Meshing 3D Point Clouds with Circumcenter Detection,'' in \\emph{International Conference on Learning Representations}, 2023.\n\\end{thebibliography}\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{}\n\n\\begin{document}\n\n\\title{\\vspace{-3mm}\\LARGE Segment Anything\\vspace{-6mm}}\n\\author{\n{\\normalsize Alexander Kirillov$^{1,2,4}$ \\quad Eric Mintun$^{2}$ \\quad Nikhila Ravi$^{1,2}$\n \\quad Hanzi Mao$^{2}$ \\quad Chloe Rolland$^{3}$ \\quad Laura Gustafson$^{3}$}\\\\[0mm]\n{\\normalsize Tete Xiao$^{3}$ \\hspace{4.65mm} Spencer Whitehead \\hspace{4.65mm} Alexander C. Berg \\hspace{4.65mm}\nWan-Yen Lo \\hspace{4.65mm} Piotr Doll\\'{a}r$^{4}$ \\hspace{4.65mm} Ross Girshick$^{4}$}\\\\[0mm]\n {\\small$^1$project lead \\qquad $^2$joint first author \\qquad $^3$equal contribution \\qquad $^4$directional lead}\\\\[1mm]\n{Meta AI Research, FAIR}\\vspace{-4mm}}\n\n\\twocolumn[{\n\\maketitle\\centering\n\\captionsetup{type=figure}\n\\includegraphics[width=0.99\\textwidth]{figs/teaser.pdf}\\vspace{-2mm}\n\\captionof{figure}{We aim to build a foundation model for segmentation by introducing three interconnected components: a promptable segmentation \\emph{task}, a segmentation \\emph{model} (\\sam) that powers data annotation and enables zero-shot transfer to a range of tasks via prompt engineering, and a \\emph{data} engine for collecting \\sad, our dataset of over 1 billion masks.}\n\\label{fig:teaser}\\vspace{5mm}\n}]\n\n\\maketitle\n\n\\begin{abstract}\\vspace{-3mm}\nWe introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 \\textbf{billion} masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (\\sam) and corresponding dataset (\\sad) of 1B masks and 11M images at \\href{https://segment-anything.com}{https://segment-anything.com} to foster research into foundation models for computer vision.\n\\vspace{-3mm}\n\\end{abstract}\n\n\\section{Introduction}\\label{sec:intro}\n\nLarge language models pre-trained on web-scale datasets are revolutionizing NLP with strong zero-shot and few-shot generalization~\\cite{Brown2020}. These ``foundation models''~\\cite{bommasani2021opportunities} can generalize to tasks and data distributions beyond those seen during training. This capability is often implemented with \\emph{prompt engineering} in which hand-crafted text is used to prompt the language model to generate a valid textual response for the task at hand. When scaled and trained with abundant text corpora from the web, these models' zero and few-shot performance compares surprisingly well to (even matching in some cases) fine-tuned models~\\cite{Brown2020,chowdhery2022palm}. Empirical trends show this behavior improving with model scale, dataset size, and total training compute~\\cite{kaplan2020scaling,Brown2020,chowdhery2022palm,hoffmann2022training}.\n\nFoundation models have also been explored in computer vision, albeit to a lesser extent. Perhaps the most prominent illustration aligns paired text and images from the web. For example, CLIP~\\cite{Radford2021} and ALIGN~\\cite{jia2021scaling} use contrastive learning to train text and image encoders that align the two modalities. Once trained, engineered text prompts enable zero-shot generalization to novel visual concepts and data distributions. Such encoders also compose effectively with other modules to enable downstream tasks, such as image generation (\\eg, DALL·E~\\cite{Ramesh2021}). While much progress has been made on vision and language encoders, computer vision includes a wide range of problems beyond this scope, and for many of these, abundant training data does not exist.\n\nIn this work, our goal is to build \\emph{a foundation model for image segmentation}. That is, we seek to develop a promptable model and pre-train it on a broad dataset using a task that enables powerful generalization. With this model, we aim to solve a range of downstream segmentation problems on new data distributions using prompt engineering.\n\nThe success of this plan hinges on three components: \\textbf{task}, \\textbf{model}, and \\textbf{data}. To develop them, we address the following questions about image segmentation:\n\\begin{enumerate}[itemsep=-0.6mm]\n\\item What \\textbf{task} will enable zero-shot generalization?\n\\item What is the corresponding \\textbf{model} architecture?\n\\item What \\textbf{data} can power this task and model?\n\\end{enumerate}\nThese questions are entangled and require a comprehensive solution. We start by defining a \\emph{promptable segmentation} \\textbf{task} that is general enough to provide a powerful pre-training objective and to enable a wide range of downstream applications. This task requires a \\textbf{model} that supports flexible prompting and can output segmentation masks in real-time when prompted to allow for interactive use. To train our model, we need a diverse, large-scale source of \\textbf{data}. Unfortunately, there is no web-scale data source for segmentation; to address this, we build a ``data engine'', \\ie, we iterate between using our efficient model to assist in data collection and using the newly collected data to improve the model. We introduce each interconnected component next, followed by the dataset we created and the experiments that demonstrate the effectiveness of our approach.\n\n\\paragraph{Task (\\S\\ref{sec:task}).} In NLP and more recently computer vision, foundation models are a promising development that can perform zero-shot and few-shot learning for new datasets and tasks often by using ``prompting'' techniques. Inspired by this line of work, we propose the \\emph{promptable segmentation task}, where the goal is to return a \\emph{valid} segmentation mask given any segmentation \\emph{prompt} (see~\\fig{fig:teaser}{\\color{linkcolor}a}). A prompt simply specifies what to segment in an image, \\eg, a prompt can include spatial or text information identifying an object. The requirement of a valid output mask means that even when a prompt is ambiguous and could refer to multiple objects (for example, a point on a shirt may indicate either the shirt or the person wearing it), the output should be a reasonable mask for at least one of those objects. We use the promptable segmentation task as both a pre-training objective and to solve general downstream segmentation tasks via prompt engineering.\n\n\\paragraph{Model (\\S\\ref{sec:model}).} The promptable segmentation task and the goal of real-world use impose constraints on the model architecture. In particular, the model must support \\emph{flexible prompts}, needs to compute masks in amortized \\emph{real-time} to allow interactive use, and must be \\emph{ambiguity-aware}. Surprisingly, we find that a simple design satisfies all three constraints: a powerful image encoder computes an image embedding, a prompt encoder embeds prompts, and then the two information sources are combined in a lightweight mask decoder that predicts segmentation masks. We refer to this model as the Segment Anything Model, or \\sam (see~\\fig{fig:teaser}{\\color{linkcolor}b}). By separating \\sam into an image encoder and a fast prompt encoder / mask decoder, the same image embedding can be reused (and its cost amortized) with different prompts. Given an image embedding, the prompt encoder and mask decoder predict a mask from a prompt in $\\app$50ms in a web browser. We focus on point, box, and mask prompts, and also present initial results with free-form text prompts. To make \\sam ambiguity-aware, we design it to predict multiple masks for a single prompt allowing \\sam to naturally handle ambiguity, such as the shirt \\vs person example.\n\n\\paragraph{Data engine (\\S\\ref{sec:engine}).} To achieve strong generalization to new data distributions, we found it necessary to train \\sam on a large and diverse set of masks, beyond any segmentation dataset that already exists. While a typical approach for foundation models is to obtain data online~\\cite{Radford2021}, masks are not naturally abundant and thus we need an alternative strategy. Our solution is to build a ``data engine'', \\ie, we co-develop our model with model-in-the-loop dataset annotation (see~\\fig{fig:teaser}{\\color{linkcolor}c}). Our data engine has three stages: \\emph{assisted-manual}, \\emph{semi-automatic}, and \\emph{fully automatic}. In the first stage, \\sam assists annotators in annotating masks, similar to a classic interactive segmentation setup. In the second stage, \\sam can automatically generate masks for a subset of objects by prompting it with likely object locations and annotators focus on annotating the remaining objects, helping increase mask diversity. In the final stage, we prompt \\sam with a regular grid of foreground points, yielding on average \\app100 high-quality masks per image.\n\n\\paragraph{Dataset (\\S\\ref{sec:dataset}).} Our final dataset, \\sad, includes more than \\emph{1B} masks from \\emph{11M} licensed and privacy-preserving images (see \\fig{fig:sa1bvisuals}). \\sad, collected fully automatically using the final stage of our data engine, has 400$\\x$ more masks than any existing segmentation dataset~\\cite{Lin2014,Gupta2019,Zhou2019,OpenImages}, and as we verify extensively, the masks are of high quality and diversity. Beyond its use in training \\sam to be robust and general, we hope \\sad becomes a valuable resource for research aiming to build new foundation models.\n\n\\paragraph{Responsible AI (\\S\\ref{sec:rai}).} We study and report on potential fairness concerns and biases when using \\sad and \\sam. Images in \\sad span a geographically and economically diverse set of countries and we found that \\sam performs similarly across different groups of people. Together, we hope this will make our work more equitable for real-world use cases. We provide model and dataset cards in the \\supp.\n\n\\paragraph{Experiments (\\S\\ref{sec:eval}).} We extensively evaluate \\sam. First, using a diverse new suite of 23 segmentation datasets, we find that \\sam produces high-quality masks from a single foreground point, often only slightly below that of the manually annotated ground truth. Second, we find consistently strong quantitative and qualitative results on a variety of downstream tasks under a zero-shot transfer protocol using prompt engineering, including edge detection, object proposal generation, instance segmentation, and a preliminary exploration of text-to-mask prediction. These results suggest that \\sam can be used out-of-the-box with prompt engineering to solve a variety of tasks involving object and image distributions beyond \\sam's training data. Nevertheless, room for improvement remains, as we discuss in \\S\\ref{sec:disc}.\n\n\\paragraph{Release.} We are releasing the \\sad dataset for research purposes and making \\sam available under a permissive open license (Apache 2.0) at \\href{https://segment-anything.com}{https://segment-anything.com}. We also showcase \\sam's capabilities with an \\href{https://segment-anything.com/demo}{online demo}.\n\n\\newcommand{\\incvisualsrow}[6]{\n\\begin{subfigure}{\\linewidth}\\begin{adjustbox}{width=\\textwidth}{\n\\def\\arraystretch{0.2}\\setlength\\tabcolsep{#1pt}\\begin{tabular}{ccccc}\n\\raisebox{1.2\\normalbaselineskip}[0pt][0pt]{\\rotatebox{90}{\\tiny #2}} &\n\\includegraphics[height=2cm]{figs/sa1b_examples/#3.jpg} &\n\\includegraphics[height=2cm]{figs/sa1b_examples/#4.jpg} &\n\\includegraphics[height=2cm]{figs/sa1b_examples/#5.jpg} &\n\\includegraphics[height=2cm]{figs/sa1b_examples/#6.jpg} \\\\\n\\end{tabular}}\\end{adjustbox}\\end{subfigure}}\n\n\\begin{figure*}[!t]\n\\centering\n\\incvisualsrow{0.5}{$<$50 masks}{9_sa_1192782}{25_sa_864082}{32_sa_8234897}{45_sa_4298678}\n\\incvisualsrow{1.0}{50-100 masks}{67_sa_1468983}{61_sa_2146330}{66_sa_9307564}{65_sa_2889438}\n\\incvisualsrow{0.8}{100-200 masks}{187_sa_7694512}{145_sa_3647402}{116_sa_6137832}{154_sa_2945741}\n\\incvisualsrow{0.8}{200-300 masks}{208_sa_6808005}{230_sa_6506882}{250_sa_9338205}{221_sa_9820072}\n\\incvisualsrow{0.6}{300-400 masks}{360_sa_5537747}{318_sa_7769360}{304_sa_1437195}{317_sa_7651290}\n\\incvisualsrow{0.7}{400-500 masks}{456_sa_11107398}{452_sa_10129735}{438_sa_1596486}{401_sa_10771020}\n\\incvisualsrow{0.3}{$>$500 masks}{783_sa_3805502}{823_sa_11048476}{576_sa_10463977}{579_sa_1232910}\n\\caption{Example images with overlaid masks from our newly introduced dataset, \\textbf{\\sad}. \\sad contains 11M diverse, high-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks. These masks were annotated \\emph{fully automatically} by \\sam, and as we verify by human ratings and numerous experiments, are of high quality and diversity. We group images by number of masks per image for visualization (there are \\app100 masks per image on average).}\n\\label{fig:sa1bvisuals}\n\\end{figure*}\n\n\\section{Segment Anything Task}\\label{sec:task}\n\nWe take inspiration from NLP, where the next token prediction task is used for foundation model pre-training \\emph{and} to solve diverse downstream tasks via prompt engineering~\\cite{Brown2020}. To build a foundation model for segmentation, we aim to define a task with analogous capabilities.\n\n\\paragraph{Task.} We start by translating the idea of a prompt from NLP to segmentation, where a prompt can be a set of foreground / background points, a rough box or mask, free-form text, or, in general, any information indicating what to segment in an image. The \\emph{promptable segmentation task}, then, is to return a \\emph{valid} segmentation mask given any \\emph{prompt}. The requirement of a ``valid'' mask simply means that even when a prompt is \\emph{ambiguous} and could refer to multiple objects (\\eg, recall the shirt \\vs person example, and see \\fig{fig:ambiguity_examples}), the output should be a reasonable mask for at least \\emph{one} of those objects. This requirement is similar to expecting a language model to output a coherent response to an ambiguous prompt. We choose this task because it leads to a natural pre-training algorithm \\emph{and} a general method for zero-shot transfer to downstream segmentation tasks via prompting.\n\n\\paragraph{Pre-training.} The promptable segmentation task suggests a natural pre-training algorithm that simulates a sequence of prompts (\\eg, points, boxes, masks) for each training sample and compares the model's mask predictions against the ground truth. We adapt this method from interactive segmentation~\\cite{xu2016deep,mahadevan2018iteratively}, although unlike interactive segmentation whose aim is to eventually predict a valid mask after enough user input, our aim is to always predict a \\emph{valid mask} for \\emph{any prompt} even when the prompt is \\emph{ambiguous}. This ensures that a pre-trained model is effective in use cases that involve ambiguity, including automatic annotation as required by our data engine \\S\\ref{sec:engine}. We note that performing well at this task is challenging and requires specialized modeling and training loss choices, which we discuss in \\S\\ref{sec:model}.\n\n\\paragraph{Zero-shot transfer.} Intuitively, our pre-training task endows the model with the ability to respond appropriately to any prompt at inference time, and thus downstream tasks can be solved by engineering appropriate prompts. For example, if one has a bounding box detector for cats, cat instance segmentation can be solved by providing the detector's box output as a prompt to our model. In general, a wide array of practical segmentation tasks can be cast as prompting. In addition to automatic dataset labeling, we explore five diverse example tasks in our experiments in \\S\\ref{sec:eval}.\n\n\\begin{figure}[t]\\centering\n\\includegraphics[width=.97\\linewidth]{figs/ambiguity_examples.pdf}\\vspace{-1mm}\n\\caption{Each column shows 3 valid masks generated by \\sam from a single ambiguous point prompt (green circle).}\n\\label{fig:ambiguity_examples}\\vspace{-1mm}\n\\end{figure}\n\n\\paragraph{Related tasks.} Segmentation is a broad field: there's interactive segmentation~\\cite{kass1988snakes,xu2016deep}, edge detection~\\cite{arbelaez2010contour}, super pixelization~\\cite{ren2003learning}, object proposal generation~\\cite{alexe2010object}, foreground segmentation~\\cite{stauffer1999adaptive}, semantic segmentation~\\cite{shotton2006textonboost}, instance segmentation~\\cite{Lin2014}, panoptic segmentation~\\cite{kirillov2019panoptic}, \\etc. The goal of our promptable segmentation task is to produce a broadly capable model that can adapt to \\emph{many} (though not all) existing and \\emph{new} segmentation tasks via prompt engineering. This capability is a form of task generalization~\\cite{da2012learning}. Note that this is different than previous work on multi-task segmentation systems. In a multi-task system, a single model performs a \\emph{fixed} set of tasks, \\eg, joint semantic, instance, and panoptic segmentation~\\cite{zhang2021knet,cheng2022masked,jain2022oneformer}, but the training and test tasks are the same. An important distinction in our work is that a model trained for promptable segmentation can perform a new, different task at inference time by acting as a \\emph{component} in a larger system, \\eg, to perform instance segmentation, a promptable segmentation model is \\emph{combined} with an existing object detector.\n\n\\paragraph{Discussion.} Prompting and composition are powerful tools that enable a single model to be used in extensible ways, potentially to accomplish tasks unknown at the time of model design. This approach is analogous to how other foundation models are used, \\eg, how CLIP~\\cite{Radford2021} is the text-image alignment component of the DALL$\\cdot$E~\\cite{Ramesh2021} image generation system. We anticipate that composable system design, powered by techniques such as prompt engineering, will enable a wider variety of applications than systems trained specifically for a fixed set of tasks. It's also interesting to compare promptable and interactive segmentation through the lens of composition: while interactive segmentation models are designed with human users in mind, a model trained for promptable segmentation can also be composed into a larger algorithmic system as we will demonstrate.\n\n\\begin{figure*}[t]\\centering\n\\includegraphics[width=0.99\\linewidth]{figs/model_diagram.pdf}\\vspace{-1mm}\n\\caption{Segment Anything Model (\\sam) overview. A heavyweight image encoder outputs an image embedding that can then be efficiently queried by a variety of input prompts to produce object masks at amortized real-time speed. For ambiguous prompts corresponding to more than one object, \\sam can output multiple valid masks and associated confidence scores.}\n\\label{fig:model_diagram}\n\\vspace{-1mm}\n\\end{figure*}\n\n\\section{Segment Anything Model}\\label{sec:model}\n\nWe next describe the Segment Anything Model (\\sam) for promptable segmentation. \\sam has three components, illustrated in~\\fig{fig:model_diagram}: an image encoder, a flexible prompt encoder, and a fast mask decoder. We build on Transformer vision models~\\cite{Carion2020,Dosovitskiy2021,cheng2021per,li2022exploring} with specific tradeoffs for (amortized) real-time performance. We describe these components at a high-level here, with details in \\S\\ref{app:model}.\n\n\\paragraph{Image encoder.} Motivated by scalability and powerful pre-training methods, we use an MAE~\\cite{he2022masked} pre-trained Vision Transformer (ViT)~\\cite{Dosovitskiy2021} minimally adapted to process high resolution inputs~\\cite{li2022exploring}. The image encoder runs once per image and can be applied prior to prompting the model.\n\n\\paragraph{Prompt encoder.} We consider two sets of prompts: \\emph{sparse} (points, boxes, text) and \\emph{dense} (masks). We represent points and boxes by positional encodings~\\cite{tancik2020fourier} summed with learned embeddings for each prompt type and free-form text with an off-the-shelf text encoder from CLIP~\\cite{Radford2021}. Dense prompts (\\ie, masks) are embedded using convolutions and summed element-wise with the image embedding.\n\n\\paragraph{Mask decoder.} The mask decoder efficiently maps the image embedding, prompt embeddings, and an output token to a mask. This design, inspired by~\\cite{Carion2020,cheng2021per}, employs a modification of a Transformer decoder block~\\cite{Vaswani2017} followed by a dynamic mask prediction head. Our modified decoder block uses prompt self-attention and cross-attention in two directions (prompt-to-image embedding and vice-versa) to update \\emph{all} embeddings. After running two blocks, we upsample the image embedding and an MLP maps the output token to a dynamic linear classifier, which then computes the mask foreground probability at each image location.\n\n\\paragraph{Resolving ambiguity.} With one output, the model will average multiple valid masks if given an ambiguous prompt. To address this, we modify the model to predict multiple output masks for a single prompt (see \\fig{fig:ambiguity_examples}). We found 3 mask outputs is sufficient to address most common cases (nested masks are often at most three deep: whole, part, and subpart). During training, we backprop only the minimum loss~\\cite{charpiat2008automatic,guzman2012multiple,li2018interactive} over masks. To rank masks, the model predicts a confidence score (\\ie, estimated IoU) for each mask.\n\n\\paragraph{Efficiency.} The overall model design is largely motivated by efficiency. Given a precomputed image embedding, the prompt encoder and mask decoder run in a web browser, on CPU, in \\app50ms. This runtime performance enables seamless, real-time interactive prompting of our model.\n\n\\paragraph{Losses and training.} We supervise mask prediction with the linear combination of focal loss~\\cite{Lin2017a} and dice loss~\\cite{milletari2016v} used in~\\cite{Carion2020}. We train for the promptable segmentation task using a mixture of geometric prompts (for text prompts see \\S\\ref{sec:eval:text_to_mask}). Following~\\cite{sofiiuk2022reviving,forte2020getting}, we simulate an interactive setup by randomly sampling prompts in 11 rounds per mask, allowing \\sam to integrate seamlessly into our data engine.\n\n\\section{Segment Anything Data Engine}\\label{sec:engine}\n\nAs segmentation masks are not abundant on the internet, we built a data engine to enable the collection of our 1.1B mask dataset, \\sad. The data engine has three stages: (1) a model-assisted manual annotation stage, (2) a semi-automatic stage with a mix of automatically predicted masks and model-assisted annotation, and (3) a fully automatic stage in which our model generates masks without annotator input. We go into details of each next.\n\n\\paragraph{Assisted-manual stage.} In the first stage, resembling classic interactive segmentation, a team of professional annotators labeled masks by clicking foreground / background object points using a browser-based interactive segmentation tool powered by \\sam. Masks could be refined using pixel-precise ``brush'' and ``eraser'' tools. Our model-assisted annotation runs in real-time directly inside a browser (using precomputed image embeddings) enabling a truly interactive experience. We did not impose semantic constraints for labeling objects, and annotators freely labeled both ``stuff'' and ``things''~\\cite{adelson2001seeing}. We suggested annotators label objects they could name or describe, but did not collect these names or descriptions. Annotators were asked to label objects in order of prominence and were encouraged to proceed to the next image once a mask took over 30 seconds to annotate.\n\nAt the start of this stage, \\sam was trained using common public segmentation datasets. After sufficient data annotation, \\sam was retrained using only newly annotated masks. As more masks were collected, the image encoder was scaled from ViT-B to ViT-H and other architectural details evolved; in total we retrained our model 6 times. Average annotation time per mask decreased from 34 to 14 seconds as the model improved. We note that 14 seconds is 6.5$\\x$ faster than mask annotation for COCO~\\cite{Lin2014} and only 2$\\x$ slower than bounding-box labeling with extreme points~\\cite{papadopoulos2017extreme,maninis2018deep}. As \\sam improved, the average number of masks per image increased from 20 to 44 masks. Overall, we collected 4.3M masks from 120k images in this stage.\n\n\\paragraph{Semi-automatic stage.} In this stage, we aimed to increase the \\emph{diversity} of masks in order to improve our model's ability to segment anything. To focus annotators on less prominent objects, we first automatically detected confident masks. Then we presented annotators with images prefilled with these masks and asked them to annotate any additional unannotated objects. To detect confident masks, we trained a bounding box detector~\\cite{Ren2015} on all first stage masks using a generic ``object'' category. During this stage we collected an additional 5.9M masks in 180k images (for a total of 10.2M masks). As in the first stage, we periodically retrained our model on newly collected data (5 times). Average annotation time per mask went back up to 34 seconds (excluding the automatic masks) as these objects were more challenging to label. The average number of masks per image went from 44 to 72 masks (including the automatic masks).\n\n\\paragraph{Fully automatic stage.} In the final stage, annotation was \\emph{fully automatic}. This was feasible due to two major enhancements to our model. First, at the start of this stage, we had collected enough masks to greatly improve the model, including the diverse masks from the previous stage. Second, by this stage we had developed the ambiguity-aware model, which allowed us to predict valid masks even in ambiguous cases. Specifically, we prompted the model with a 32$\\x$32 regular grid of points and for each point predicted a set of masks that may correspond to valid objects. With the ambiguity-aware model, if a point lies on a part or subpart, our model will return the subpart, part, and whole object. The IoU prediction module of our model is used to select \\emph{confident} masks; moreover, we identified and selected only \\emph{stable} masks (we consider a mask stable if thresholding the probability map at $0.5-\\delta$ and $0.5+\\delta$ results in similar masks). Finally, after selecting the confident and stable masks, we applied non-maximal suppression (NMS) to filter duplicates. To further improve the quality of smaller masks, we also processed multiple overlapping zoomed-in image crops. For further details of this stage, see \\S\\ref{app:dataset_generation}. We applied fully automatic mask generation to all 11M images in our dataset, producing a total of 1.1B high-quality masks. We describe and analyze the resulting dataset, \\sad, next.\n\n\\begin{figure}[t]\\centering\n\\includegraphics[width=.99\\linewidth]{figs/center_distribution.png}\\vspace{-3mm}\n\\caption{Image-size normalized mask center distributions.}\n\\label{fig:analysis:center_distribution}\\vspace{-2mm}\n\\end{figure}\n\n\\begin{figure*}[t]\\centering\\vspace{-1mm}\n\\includegraphics[width=0.88\\linewidth]{figs/dataset_legend.pdf}\\\\[-2mm]\n\\includegraphics[width=0.32\\linewidth]{figs/dataset_percent_mask_count.pdf}\n\\includegraphics[width=0.32\\linewidth]{figs/dataset_area_distribution.pdf}\n\\includegraphics[width=0.32\\linewidth]{figs/dataset_concavity_distribution_stratified.pdf}\n\\vspace{-3mm}\n\\caption{Dataset mask properties. The legend references the number of images and masks in each dataset. Note, that \\sad has 11$\\x$ more images and 400$\\x$ more masks than the largest existing segmentation dataset Open Images~\\cite{OpenImages}.}\n\\label{fig:analysis}\\vspace{-3mm}\n\\end{figure*}\n\n\\section{Segment Anything Dataset}\\label{sec:dataset}\n\nOur dataset, \\sad, consists of 11M diverse, high-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks collected with our data engine. We compare \\sad with existing datasets and analyze mask quality and properties. We are releasing \\sad to aid future development of foundation models for computer vision. We note that \\sad will be released under a favorable license agreement for certain research uses and with protections for researchers.\n\n\\paragraph{Images}. We licensed a new set of 11M images from a provider that works directly with photographers. These images are high resolution (3300$\\x$4950 pixels on average), and the resulting data size can present accessibility and storage challenges. Therefore, we are releasing downsampled images with their shortest side set to 1500 pixels. Even after downsampling, our images are significantly higher resolution than many existing vision datasets (\\eg, COCO~\\cite{Lin2014} images are \\app480$\\x$640 pixels). Note that most models today operate on much lower resolution inputs. Faces and vehicle license plates have been blurred in the released images.\n\n\\paragraph{Masks}. Our data engine produced 1.1B masks, 99.1\\% of which were generated fully automatically. Therefore, the quality of the automatic masks is centrally important. We compare them directly to professional annotations and look at how various mask properties compare to prominent segmentation datasets. Our main conclusion, as borne out in the analysis below and the experiments in \\S\\ref{sec:eval}, is that our automatic masks are high quality and effective for training models. Motivated by these findings, \\sad \\emph{only includes automatically generated masks.}\n\n\\paragraph{Mask quality.} To estimate mask quality, we randomly sampled 500 images ($\\app$50k masks) and asked our professional annotators to improve the quality of all masks in these images. Annotators did so using our model and pixel-precise ``brush'' and ``eraser'' editing tools. This procedure resulted in pairs of automatically predicted and professionally corrected masks. We computed IoU between each pair and found that 94\\% of pairs have greater than 90\\% IoU (and 97\\% of pairs have greater than 75\\% IoU). For comparison, prior work estimates inter-annotator consistency at 85-91\\% IoU~\\cite{Gupta2019,OpenImages}. Our experiments in \\S\\ref{sec:eval} confirm by human ratings that mask quality is high relative to a variety of datasets and that training our model on automatic masks is nearly as good as using all masks produced by the data engine.\n\n\\begin{figure*}[t]\\centering\n\\includegraphics[width=0.44\\linewidth]{figs/ablation_sa1b_geography.pdf}\\hfill\n\\includegraphics[width=0.54\\linewidth]{figs/ablation_countries_barplot.pdf}\n\\vspace{-3mm}\n\\caption{Estimated geographic distribution of \\sad images. Most of the world's countries have more than 1000 images in \\sad, and the three countries with the most images are from different parts of the world.}\n\\label{fig:geo_distribution}\\vspace{-4mm}\n\\end{figure*}\n\n\\paragraph{Mask properties.} In \\fig{fig:analysis:center_distribution} we plot the spatial distribution of object centers in \\sad compared to the largest existing segmentation datasets. Common photographer biases are present in all datasets. We observe that \\sad has greater coverage of image corners compared to LVIS v1~\\cite{Gupta2019} and ADE20K~\\cite{Zhou2019}, the two most similarly distributed datasets, while COCO~\\cite{Lin2014} and Open Images V5~\\cite{OpenImages} have a more prominent center bias. In \\fig{fig:analysis} (legend) we compare these datasets by size. \\sad has 11$\\x$ more images and 400$\\x$ more masks than the second largest, Open Images. On average, it has 36$\\x$ more masks per image than Open Images. The closest dataset in this respect, ADE20K, still has 3.5$\\x$ fewer masks per image. \\fig{fig:analysis} (left) plots the masks-per-image distribution. Next, we look at image-relative mask size (square root of the mask area divided by image area) in \\fig{fig:analysis} (middle). As expected, since our dataset has more masks per image, it also tends to include a greater percentage of small and medium relative-size masks. Finally, to analyze shape complexity, we look at mask concavity (1 minus mask area divided by area of mask's convex hull) in \\fig{fig:analysis} (right). Since shape complexity is correlated with mask size, we control for the datasets' mask size distributions by first performing stratified sampling from binned mask sizes. We observe that the concavity distribution of our masks is broadly similar to that of other datasets.\n\n\\section{Segment Anything RAI Analysis}\\label{sec:rai}\n\nWe next perform a Responsible AI (RAI) analysis of our work by investigating potential fairness concerns and biases when using \\sad and \\sam. We focus on the geographic and income distribution of \\sad and fairness of \\sam across protected attributes of people. We also provide dataset, data annotation, and model cards in \\S\\ref{app:cards}.\n\n\\begin{table}[t]\\centering\n\\resizebox{\\linewidth}{!}{\n\\tablestyle{3pt}{1.2}\\begin{tabular}{@{}lr|rr|rrr@{}}\n& & \\multicolumn{2}{c|}{\\sad} & \\multicolumn{3}{c}{\\% images} \\\\\n& \\hspace{-9mm} \\# countries & \\#imgs & \\#masks & \\sad & COCO & O.I. \\\\\n\\hline\n\\scriptsize Africa & 54 & 300k & 28M & 2.8\\% & 3.0\\% & 1.7\\% \\\\\n\\scriptsize Asia \\& Oceania & 70 & 3.9M & 423M & 36.2\\% & 11.4\\% & 14.3\\% \\\\\n\\scriptsize Europe & 47 & 5.4M & 540M & 49.8\\% & 34.2\\% & 36.2\\% \\\\\n\\scriptsize Latin America \\& Carib. & 42 & 380k & 36M & 3.5\\% & 3.1\\% & 5.0\\% \\\\\n\\scriptsize North America & 4 & 830k & 80M & 7.7\\% & 48.3\\% & 42.8\\% \\\\\n\\hline\n\\scriptsize high income countries & 81 & 5.8M & 598M & 54.0\\% & 89.1\\% & 87.5\\% \\\\\n\\scriptsize middle income countries & 108 & 4.9M & 499M & 45.0\\% & 10.5\\% & 12.0\\% \\\\\n\\scriptsize low income countries & 28 & 100k & 9.4M & 0.9\\% & 0.4\\% & 0.5\\% \\\\\n\\end{tabular}}\\vspace{-2mm}\n\\caption{Comparison of geographic and income representation. \\sad has higher representation in Europe and Asia \\& Oceania as well as middle income countries. Images from Africa, Latin America \\& Caribbean, as well as low income countries, are underrepresented in all datasets.}\n\\label{tab:region}\\vspace{0mm}\n\\end{table}\n\n\\paragraph{Geographic and income representation.} We infer the country images were photographed in using standard methods (see \\S\\ref{app:rai}). In \\fig{fig:geo_distribution} we visualize the per-country image counts in \\sad (left) and the 50 countries with the most images (right). We note that the top-three countries are from different parts of the world. Next, in Table~\\ref{tab:region} we compare the geographic and income representation of \\sad, COCO~\\cite{Lin2014}, and Open Images~\\cite{OpenImages}. \\sad has a substantially higher percentage of images in Europe and Asia \\& Oceania as well as in middle income countries. All datasets underrepresent Africa as well as low income countries. We note that in \\sad, all regions, including Africa, have at least 28 million masks, 10$\\x$ more than the \\emph{total} number of masks of any previous dataset. Finally, we observe that the average number of masks per image (not shown) is fairly consistent across region and income (94-108 per image).\n\n\\paragraph{Fairness in segmenting people.} We investigate potential fairness concerns across perceived gender presentation, perceived age group, and perceived skin tone by measuring the performance discrepancy of \\sam between groups. We use the More Inclusive Annotations for People (MIAP)~\\cite{schumann2021step} dataset for gender presentation and age and a proprietary dataset for skin tone (see \\S\\ref{app:rai}). Our evaluation uses simulated interactive segmentation with random sampling of 1 and 3 points (see \\S\\ref{app:experimental_design}). Table \\ref{tab:rai_person} (top left) shows results for perceived gender presentation. We note that females have been shown to be underrepresented in detection and segmentation datasets~\\cite{zhao2017}, but observe that \\sam performs similarly across groups. We repeat the analysis for perceived age in Table~\\ref{tab:rai_person} (bottom left), noting that those who are perceived to be younger and older have been shown to be underrepresented in large-scale datasets~\\cite{yang2020towards}. \\sam performs best on those who are perceived older (although the confidence interval is large). Finally, we repeat the analysis for perceived skin tone in Table~\\ref{tab:rai_person} (right), noting that those with lighter apparent skin tones have been shown to be overrepresented and those with darker skin tones underrepresented in large-scale datasets~\\cite{yang2020towards}. As MIAP does not contain perceived skin tone annotations, we use a proprietary dataset that contains annotations for the perceived Fitzpatrick skin type~\\cite{fitzpatrick1988}, which ranges from 1 (lightest skin tone) to 6 (darkest skin tone). While the means vary somewhat, we do not find a significant difference across groups. We believe our findings stem from the nature of the task, and acknowledge biases may arise when \\sam is used as a component in larger systems. Finally, in \\S\\ref{app:rai} we extend the analysis to segmenting clothing where we find an indication of bias across perceived gender presentation.\n\n\\begin{table}[t]\\centering\n\\resizebox{!}{19mm}{\n\\tablestyle{4pt}{1.1}\\begin{tabular}{@{}lcc@{}}\n{} & \\multicolumn{2}{c}{mIoU at} \\\\\n{} & 1 point & 3 points \\\\\n\\hline\n\\multicolumn{3}{@{}l}{\\emph{perceived gender presentation}} \\\\\nfeminine & 54.4\\mypm{1.7} & 90.4\\mypm{0.6} \\\\\nmasculine & 55.7\\mypm{1.7} & 90.1\\mypm{0.6} \\\\\n\\hline\n\\multicolumn{3}{@{}l}{\\emph{perceived age group}} \\\\\nolder & 62.9\\mypm{6.7} & 92.6\\mypm{1.3} \\\\\nmiddle & 54.5\\mypm{1.3} & 90.2\\mypm{0.5} \\\\\nyoung & 54.2\\mypm{2.2} & 91.2\\mypm{0.7} \\\\\n\\end{tabular}}\\hspace{5mm}\n\\resizebox{!}{19mm}{\n\\tablestyle{4pt}{1.1}\\begin{tabular}{@{}lcc@{}}\n{} & \\multicolumn{2}{c}{mIoU at} \\\\\n{} & 1 point & 3 points \\\\\n\\hline\n\\multicolumn{3}{@{}l}{\\emph{perceived skin tone}} \\\\\n1 & 52.9\\mypm{2.2} & 91.0\\mypm{0.9} \\\\\n2 & 51.5\\mypm{1.4} & 91.1\\mypm{0.5} \\\\\n3 & 52.2\\mypm{1.9} & 91.4\\mypm{0.7} \\\\\n4 & 51.5\\mypm{2.7} & 91.7\\mypm{1.0} \\\\\n5 & 52.4\\mypm{4.2} & 92.5\\mypm{1.4} \\\\\n6 & 56.7\\mypm{6.3} & 91.2\\mypm{2.4} \\\\\n\\end{tabular}}\n\\vspace{-2mm}\n\\caption{\\sam's performance segmenting people across perceived gender presentation, age group, and skin tone. 95\\% confidence intervals are shown. Within each grouping, all confidence intervals overlap except older \\vs middle.}\n\\label{tab:rai_person}\\vspace{-3mm}\n\\end{table}\n\n\\section{Zero-Shot Transfer Experiments}\\label{sec:eval}\n\nIn this section, we present \\emph{zero-shot transfer} experiments with \\sam, the Segment Anything Model. We consider five tasks, four of which differ significantly from the promptable segmentation task used to train \\sam. These experiments evaluate \\sam on datasets and tasks that were not seen during training (our usage of ``zero-shot transfer'' follows its usage in CLIP~\\cite{Radford2021}). The datasets may include novel image distributions, such as underwater or ego-centric images (\\eg \\fig{fig:benchmark_examples}) that, to our knowledge, do not appear in \\sad.\n\nOur experiments begin by testing the core goal of promptable segmentation: producing a valid mask from any prompt. We emphasize the challenging scenario of a \\emph{single} foreground point prompt, since it is more likely to be ambiguous than other more specific prompts. Next, we present a sequence of experiments that traverse low, mid, and high-level image understanding and roughly parallel the historical development of the field. Specifically, we prompt \\sam to (1) perform edge detection, (2) segment everything, \\ie object proposal generation, (3) segment detected objects, \\ie instance segmentation, and (4), as a proof-of-concept, to segment objects from free-form text. These four tasks differ significantly from the promptable segmentation task that \\sam was trained on and are implemented via prompt engineering. Our experiments conclude with an ablation study.\n\n\\paragraph{Implementation.} Unless otherwise specified: (1) \\sam uses an MAE~\\cite{he2022masked} pre-trained ViT-H~\\cite{Dosovitskiy2021} image encoder and (2) \\sam was trained on \\sad, noting that this dataset includes only automatically generated masks from the final stage of our data engine. For all other model and training details, such as hyperparameters, refer to \\S\\ref{app:model}.\n\n\\subsection{Zero-Shot Single Point Valid Mask Evaluation}\\label{sec:eval:single_point}\\vspace{-2mm}\n\n\\paragraph{Task.} We evaluate segmenting an object from a \\emph{single} foreground point. This task is ill-posed as one point can refer to multiple objects. Ground truth masks in most datasets do not enumerate \\emph{all} possible masks, which can make automatic metrics unreliable. Therefore, we supplement the standard mIoU metric (\\ie, the mean of all IoUs between predicted and ground truth masks) with a human study in which annotators rate mask quality from 1 (nonsense) to 10 (pixel-perfect). See \\S\\ref{app:benchmark}, \\S\\ref{app:human_study}, and \\S\\ref{app:annotation_guidelines} for additional details.\n\nBy default, we sample points from the ``center'' of ground truth masks (at a maximal value of the mask's interior distance transform), following the standard evaluation protocol in interactive segmentation~\\cite{sofiiuk2022reviving}. Since \\sam is capable of predicting multiple masks, we evaluate only the model's most confident mask by default. The baselines are all single-mask methods. We compare mainly to RITM~\\cite{sofiiuk2022reviving}, a strong interactive segmenter that performs best on our benchmark compared to other strong baselines~\\cite{liu2022simpleclick,chen2022focalclick}.\n\n\\begin{figure*}[t]\\centering\n\\tablestyle{0.9pt}{0.6}\\fontsize{6}{7}\\selectfont\n\\begin{tabular}{cccccccc}\nADE20K~\\cite{Zhou2019} & BBBC038v1~\\cite{cells} & Cityscapes~\\cite{Cordts2016} & DOORS~\\cite{doors} & DRAM~\\cite{DRAM} & EgoHOS~\\cite{egoHos} & GTEA~\\cite{gtea1,gtea2} & Hypersim~\\cite{hypersim} \\\\\n\\includegraphics[height=1.43cm]{figs/datasets/ade20k_ADE_val_00001049.jpg} &\n\\includegraphics[height=1.43cm]{figs/datasets/cells_10k_0e21d7b3eea8cdbbed60d51d72f4f8c1974c5d76a8a3893a7d5835c85284132e.jpg} &\n\\includegraphics[height=1.43cm]{figs/datasets/cityscapes_10k_blurred_munster_000144_000019_leftImg8bit_blurred.jpg} &\n\\includegraphics[height=1.43cm]{figs/datasets/doors_10k_TR_024413.jpg} &\n\\includegraphics[height=1.43cm]{figs/datasets/dram_10k_1905_221847.jpg} &\n\\includegraphics[height=1.43cm]{figs/datasets/egohos_10k_blurred_thu_manicure_RGB_manicure_lzy_1_68.jpg} &\n\\includegraphics[height=1.43cm]{figs/datasets/gtea_10k_blurred_s4_cofhoney_0000000700.jpg} &\n\\includegraphics[height=1.43cm]{figs/datasets/hypersim_10k_frame.jpg}\n\\end{tabular}\\\\[0mm]\n\\tablestyle{1pt}{0.6}\\fontsize{6}{7}\\selectfont\n\\begin{tabular}{cccccccc}\nIBD~\\cite{chen20223D} & iShape~\\cite{iShape} & LVIS~\\cite{Gupta2019} & NDD20~\\cite{ndd20} & NDISPark~\\cite{ndis1,ndis2} & OVIS~\\cite{ovis} & PPDLS~\\cite{plants} & Plittersdorf~\\cite{haucke2022socrates}\\\\\n\\includegraphics[height=1.64cm]{figs/datasets/drones_10k_69-10400-3200-OR.jpg} &\n\\includegraphics[height=1.64cm]{figs/datasets/ishape_10k_3982.jpg} &\n\\includegraphics[height=1.64cm]{figs/datasets/lvis_10k_blurred_000000050844.jpg} &\n\\includegraphics[height=1.64cm]{figs/datasets/ndd20_10k_1599.jpg} &\n\\includegraphics[height=1.64cm]{figs/datasets/ndis_10k_blurred_64_1537102819.jpg} &\n\\includegraphics[height=1.64cm]{figs/datasets/ovis_10k_blurred_img_0000022.jpg} &\n\\includegraphics[height=1.64cm]{figs/datasets/plant_10k_ara2012_plant101_rgb.jpg} &\n\\includegraphics[height=1.64cm]{figs/datasets/plittersdorf_10k_20220213100529_000507.jpg}\n\\end{tabular}\\\\[0mm]\n\\tablestyle{1pt}{0.6}\\fontsize{6}{7}\\selectfont\n\\begin{tabular}{ccccccc}\nSTREETS~\\cite{streets} & TimberSeg~\\cite{timberSeg} & TrashCan~\\cite{hong2020trashcan} & VISOR~\\cite{VISOR,EpicKitchens} & WoodScape~\\cite{yogamani2019woodscape} & PIDRay~\\cite{wang2021towards} & ZeroWaste-f~\\cite{zerowaste} \\\\\n\\includegraphics[height=1.57cm]{figs/datasets/streets_10k_Lake.jpg} &\n\\includegraphics[height=1.57cm]{figs/datasets/timberseg_10k_000050.jpg} &\n\\includegraphics[height=1.57cm]{figs/datasets/trashcan_10k_vid_000343_frame0000015.jpg} &\n\\includegraphics[height=1.57cm]{figs/datasets/visor_10k_blurred_P04_13_frame_0000005102.jpg} &\n\\includegraphics[height=1.57cm]{figs/datasets/woodscapes_10k_03918_MVL.jpg} &\n\\includegraphics[height=1.57cm]{figs/datasets/xray_hard_10k_xray_hard00550.jpg} &\n\\includegraphics[height=1.57cm]{figs/datasets/zerowaste_10k_07_frame_032200.jpg}\n\\end{tabular}\\vspace{-2mm}\n\\caption{Samples from the 23 diverse segmentation datasets used to evaluate \\sam's zero-shot transfer capabilities.}\n\\label{fig:benchmark_examples}\n\\end{figure*}\n\n\\paragraph{Datasets.} We use a newly compiled suite of 23 datasets with diverse image distributions. \\fig{fig:benchmark_examples} lists the datasets and shows a sample from each one (see appendix Table~\\ref{app:tab:datasets_all} for more details). We use all 23 datasets for mIoU evaluation. For the human study, we use the subset listed in \\fig{fig:benchmark_exps}{\\color{linkcolor}b} (due to the resource requirements of such studies). This subset includes both datasets for which \\sam outperforms and underperforms RITM according to automatic metrics.\n\n\\begin{figure*}[t]\\centering\n\\begin{tabular}[b]{@{}c@{}}\n\\resizebox{.49\\linewidth}{!}{\\input{figs/becnhmark_ritm_comparison.pgf}}\\\\[-1mm]\n\\footnotesize (a) \\sam \\vs RITM~\\cite{sofiiuk2022reviving} on 23 datasets\n\\end{tabular}\\hfill\n\\begin{tabular}[b]{@{}c@{}}\n\\includegraphics[width=.49\\linewidth]{figs/human_study_points.pdf}\\\\[-2mm]\n\\footnotesize (b) Mask quality ratings by human annotators\\\\[2mm]\n\\begin{tabular}[b]{@{}c@{}c@{}}\n\\includegraphics[width=0.24\\linewidth,trim={0 2mm 0 0},clip]{figs/becnhmark_ritm_pointer_plot.pdf} &\n\\includegraphics[width=0.24\\linewidth,trim={0 2mm 0 0},clip]{figs/becnhmark_random_pointer_plot.pdf}\\\\[-1mm]\n\\footnotesize (c) Center points (default) & \\footnotesize (d) Random points\n\\end{tabular}\n\\end{tabular}\\vspace{-2mm}\n\\caption{Point to mask evaluation on 23 datasets. (a) Mean IoU of \\sam and the strongest single point segmenter, RITM~\\cite{sofiiuk2022reviving}. Due to ambiguity, a single mask may not match ground truth; circles show ``oracle'' results of the most relevant of \\sam's 3 predictions. (b) Per-dataset comparison of mask quality ratings by annotators from 1 (worst) to 10 (best). All methods use the ground truth mask center as the prompt. (c, d) mIoU with varying number of points. \\sam significantly outperforms prior interactive segmenters with 1 point and is on par with more points. Low absolute mIoU at 1 point is the result of ambiguity.}\n\\label{fig:benchmark_exps}\\vspace{-2mm}\n\\end{figure*}\n\n\\paragraph{Results.} First, we look at automatic evaluation on the full suite of 23 datasets using mIoU. We compare per-dataset results in \\fig{fig:benchmark_exps}{\\color{linkcolor}a} against RITM. \\sam yields higher results on 16 of the 23 datasets, by as much as $\\app$47 IoU. We also present an ``oracle'' result, in which the most relevant of \\sam's 3 masks is selected by comparing them to the ground truth, rather than selecting the most confident mask. This reveals the impact of ambiguity on automatic evaluation. In particular, with the oracle to perform ambiguity resolution, \\sam outperforms RITM on \\emph{all} datasets.\n\nResults of the human study are presented in \\fig{fig:benchmark_exps}{\\color{linkcolor}b}. Error bars are 95\\% confidence intervals for mean mask ratings (all differences are significant; see \\S\\ref{app:human_study} for details). We observe that the annotators consistently rate the quality of \\sam's masks substantially higher than the strongest baseline, RITM. An ablated, ``ambiguity-unaware'' version of \\sam with a single output mask has consistently lower ratings, though still higher than RITM. \\sam's mean ratings fall between 7 and 9, which corresponds to the qualitative rating guideline: ``\\emph{A high score (7-9): The object is identifiable and errors are small and rare (\\eg, missing a small, heavily obscured disconnected component, ...).}'' These results indicate that \\sam has learned to segment valid masks from a single point. Note that for datasets like DRAM and IBD, where \\sam is worse on automatic metrics, \\emph{it receives consistently higher ratings in the human study}.\n\n\\fig{fig:benchmark_exps}{\\color{linkcolor}c} shows additional baselines, SimpleClick~\\cite{liu2022simpleclick} and FocalClick~\\cite{chen2022focalclick}, which obtain lower single point performance than RITM and \\sam. As the number of points increases from 1 to 9, we observe that the gap between methods decreases. This is expected as the task becomes easier; also, \\sam is not optimized for the very high IoU regime. Finally, in \\fig{fig:benchmark_exps}{\\color{linkcolor}d} we replace the default center point sampling with random point sampling. We observe that the gap between \\sam and the baselines grows and \\sam is able to achieve comparable results under either sampling method.\n\n\\subsection{Zero-Shot Edge Detection}\\label{sec:eval:edge}\n\n\\paragraph{Approach.} We evaluate \\sam on the classic low-level task of edge detection using BSDS500~\\cite{martin2001database,arbelaez2010contour}. We use a simplified version of our automatic mask generation pipeline. Specifically, we prompt \\sam with a 16$\\x$16 regular grid of foreground points resulting in 768 predicted masks (3 per point). Redundant masks are removed by NMS. Then, edge maps are computed using Sobel filtering of unthresholded mask probability maps and standard lightweight postprocessing, including edge NMS (see \\S\\ref{app:edges} for details).\n\n\\begin{figure}[t]\\centering\n\\tablestyle{1pt}{0.8}\\begin{tabular}{ccc}\nimage & ground truth & \\sam \\\\\n\\includegraphics[width=0.325\\linewidth]{figs/bsds/images/35028.pdf} &\n\\includegraphics[width=0.325\\linewidth]{figs/bsds/avg-groundTruth/35028.pdf} &\n\\includegraphics[width=0.325\\linewidth]{figs/bsds/4624034_0002-nms/35028.pdf} \\\\\n\\includegraphics[width=0.325\\linewidth]{figs/bsds/images/48017.pdf} &\n\\includegraphics[width=0.325\\linewidth]{figs/bsds/avg-groundTruth/48017.pdf} &\n\\includegraphics[width=0.325\\linewidth]{figs/bsds/4624034_0002-nms/48017.pdf} \\\\\n\\end{tabular}\\vspace{-3mm}\n\\caption{Zero-shot edge prediction on BSDS500. \\sam was not trained to predict edge maps nor did it have access to BSDS images or annotations during training.}\n\\label{fig:edges}\n\\end{figure}\n\n\\begin{table}[t]\n\\centering\n\\tablestyle{9pt}{1.1}\n\\footnotesize\n\\begin{tabular}{@{}lc|x{20}x{20}x{20}|x{9}}\nmethod & year & ODS & OIS & AP & R50 \\\\\n\\hline\nHED~\\cite{xie2015holistically} & 2015& .788 & .808 & .840 & .923 \\\\\nEDETR~\\cite{pu2022edter} & 2022 & .840 & .858 & .896 & .930 \\\\\n\\multicolumn{6}{@{}l}{\\emph{zero-shot transfer methods:}} \\\\\nSobel filter & 1968 & .539 & - & -& - \\\\\nCanny~\\cite{canny1986computational} & 1986 & .600 & .640 & .580 & - \\\\\nFelz-Hutt~\\cite{felzenszwalb2004efficient} & 2004 & .610 & .640 & .560 & - \\\\\n\\sam & 2023 &.768 &.786 & .794 & .928 \\\\\n\\end{tabular}\\vspace{-2mm}\n\\caption{Zero-shot transfer to edge detection on BSDS500.}\n\\label{tab:edges}\n\\end{table}\n\n\\paragraph{Results.} We visualize representative edge maps in \\fig{fig:edges} (see \\fig{fig:more_edges} for more). Qualitatively, we observe that even though \\sam was not trained for edge detection, it produces reasonable edge maps. Compared to the ground truth, \\sam predicts more edges, including sensible ones that are not annotated in BSDS500. This bias is reflected quantitatively in Table~\\ref{tab:edges}: recall at 50\\% precision (R50) is high, at the cost of precision. \\sam naturally lags behind state-of-the-art methods that learn the biases of BSDS500, \\ie, which edges to suppress. Nevertheless, \\sam performs well compared to pioneering deep learning methods such as HED~\\cite{xie2015holistically} (also trained on BSDS500) and significantly better than prior, though admittedly outdated, zero-shot transfer methods.\n\n\\subsection{Zero-Shot Object Proposals}\\label{subsec:proposals}\n\n\\paragraph{Approach.} Next, we evaluate \\sam on the mid-level task of object proposal generation~\\cite{alexe2010object,Sande2011}. This task has played an important role in object detection research, serving as an intermediate step in pioneering systems (\\eg,~\\cite{Sande2011,Girshick2014,Ren2015}). To generate object proposals, we run a slightly modified version of our automatic mask generation pipeline and output the masks as proposals (see \\S\\ref{app:proposals} for details).\n\nWe compute the standard average recall (AR) metric on LVIS v1~\\cite{Gupta2019}. We focus on LVIS because its large number of categories presents a challenging test. We compare to a \\emph{strong} baseline implemented as a ViTDet~\\cite{li2022exploring} detector (with cascade Mask R-CNN~\\cite{He2017,Cai2018} ViT-H). We note that this ``baseline'' corresponds to the ``Detector Masquerading as Proposal generator'' (DMP) method~\\cite{chavali2016object} that was shown to game AR, making it a truly demanding comparison.\n\n\\begin{table}[t]\n\\centering\n\\tablestyle{2.8pt}{1.1}\n\\footnotesize\n\\begin{tabular}{@{}lx{20}|x{20}x{20}x{20}|x{20}x{20}x{12}}\n\\multirow{2}{*}{} & \\multicolumn{7}{c}{mask AR@1000}\\\\\nmethod & all & small & med. & large & freq. & com. & rare \\\\\n\\hline\nViTDet-H~\\cite{li2022exploring} & 63.0 & 51.7 & 80.8 & 87.0 & 63.1 & 63.3 & 58.3 \\\\\n\\multicolumn{8}{@{}l}{\\emph{zero-shot transfer methods:}} \\\\\n\\sam\\ -- single out. & 54.9 & 42.8 & 76.7 & 74.4 & 54.7 & 59.8 & 62.0 \\\\\n\\sam & 59.3 & 45.5 & 81.6 & 86.9 & 59.1 & 63.9 & 65.8 \\\\\n\\end{tabular}\n\\vspace{-2mm}\n\\caption{Object proposal generation on LVIS v1. \\sam is applied zero-shot, \\ie it was not trained for object proposal generation nor did it access LVIS images or annotations.}\n\\label{tab:proposals}\n\\end{table}\n\n\\paragraph{Results.} In Table~\\ref{tab:proposals} we see unsurprisingly that using the detections from ViTDet-H as object proposals (\\ie, the DMP method~\\cite{chavali2016object} that games AR) performs the best overall. However, \\sam does remarkably well on several metrics. Notably, it outperforms ViTDet-H on medium and large objects, as well as rare and common objects. In fact, \\sam only underperforms ViTDet-H on small objects and frequent objects, where ViTDet-H can easily learn LVIS-specific annotation biases since it was trained on LVIS, unlike \\sam. We also compare against an ablated ambiguity-unaware version of \\sam (``single out.''), which performs significantly worse than \\sam on all AR metrics.\n\n\\subsection{Zero-Shot Instance Segmentation}\\label{sec:eval:instseg}\n\n\\paragraph{Approach.} Moving to higher-level vision, we use \\sam as the segmentation module of an instance segmenter. The implementation is simple: we run a object detector (the ViTDet used before) and prompt \\sam with its output boxes. This illustrates \\emph{composing} \\sam in a larger system.\n\n\\paragraph{Results.} We compare the masks predicted by \\sam and ViTDet on COCO and LVIS in Table~\\ref{tab:instance_segmentation}. Looking at the mask AP metric we observe gaps on both datasets, where \\sam is reasonably close, though certainly behind ViTDet. By visualizing outputs, we observed that \\sam masks are often qualitatively better than those of ViTDet, with crisper boundaries (see \\S\\ref{app:instseg} and \\fig{fig:instanceseg}). To investigate this observation, we conducted an additional human study asking annotators to rate the ViTDet masks and \\sam masks on the 1 to 10 quality scale used before. In \\fig{fig:humanstudy:inst} we observe that \\sam consistently outperforms ViTDet in the human study.\n\n\\begin{table}[t]\n\\centering\n\\tablestyle{2.2pt}{1.1}\n\\footnotesize\n\\begin{tabular}{@{}lx{19}x{19}x{19}x{19}|x{19}x{19}x{19}x{12}}\n \\multirow{2}{*}{} & \\multicolumn{4}{c}{COCO~\\cite{Lin2014}} & \\multicolumn{4}{c}{LVIS v1~\\cite{Gupta2019}}\\\\\nmethod & AP & AP\\textsuperscript{S} & AP\\textsuperscript{M} & AP\\textsuperscript{L} & AP & AP\\textsuperscript{S} & AP\\textsuperscript{M} & AP\\textsuperscript{L} \\\\\n\\hline\nViTDet-H~\\cite{li2022exploring} & 51.0 & 32.0 & 54.3 & 68.9 & 46.6 & 35.0 & 58.0 & 66.3\\\\\n\\multicolumn{9}{@{}l}{\\emph{zero-shot transfer methods (segmentation module only):}} \\\\\n\\sam & 46.5 & 30.8 & 51.0 & 61.7 & 44.7 & 32.5 & 57.6 & 65.5\\\\\n\\end{tabular}\n\\vspace{-2mm}\n\\caption{Instance segmentation results. \\sam is prompted with ViTDet boxes to do zero-shot segmentation. The fully-supervised ViTDet outperforms \\sam, but the gap shrinks on the higher-quality LVIS masks. Interestingly, \\sam outperforms ViTDet according to human ratings (see \\fig{fig:humanstudy:inst}).}\n\\label{tab:instance_segmentation}\\vspace{-1mm}\n\\end{table}\n\n\\begin{figure}[t]\\centering\n\\includegraphics[width=.99\\linewidth]{figs/human_study_boxes.pdf}\n\\vspace{-3mm}\n\\caption{Mask quality rating distribution from our human study for ViTDet and \\sam, both applied to LVIS ground truth boxes. We also report LVIS and COCO ground truth quality. The legend shows rating means and 95\\% confidence intervals. Despite its lower AP (Table~\\ref{tab:instance_segmentation}), \\sam has higher ratings than ViTDet, suggesting that ViTDet exploits biases in the COCO and LVIS training data.}\n\\label{fig:humanstudy:inst}\\vspace{-3mm}\n\\end{figure}\n\nWe hypothesize that on COCO, where the mask AP gap is larger and the ground truth quality is relatively low (as borne out by the human study), ViTDet learns the specific biases of COCO masks. \\sam, being a zero-shot method, is unable to exploit these (generally undesirable) biases. The LVIS dataset has higher quality ground truth, but there are still specific idiosyncrasies (\\eg, masks do not contain holes, they are simple polygons by construction) and biases for modal \\vs amodal masks. Again, \\sam is not trained to learn these biases, while ViTDet can exploit them.\n\n\\subsection{Zero-Shot Text-to-Mask}\\label{sec:eval:text_to_mask}\n\n\\paragraph{Approach.} Finally, we consider an even higher-level task: segmenting objects from free-form text. This experiment is a proof-of-concept of \\sam's ability to process text prompts. While we used the exact same \\sam in all prior experiments, for this one \\sam's training procedure is modified to make it text-aware, but in a way that does not require new text annotations. Specifically, for each manually collected mask with area larger than $\\textrm{100}^\\textrm{2}$ we extract the CLIP \\emph{image} embedding. Then, during training, we prompt \\sam with the extracted CLIP image embeddings as its first interaction. The key observation here is that because CLIP's \\emph{image} embeddings are trained to align with its \\emph{text} embeddings, we can train with image embeddings, but use text embeddings for inference. That is, at inference time we run text through CLIP's text encoder and then give the resulting text embedding as a prompt to \\sam (see \\S\\ref{app:text_to_mask} for details).\n\n\\begin{figure}[t]\\centering\n\\tablestyle{1pt}{0.8}\n\\begin{tabular}{c c}\\begin{tikzpicture}\n\\draw (0, 0) node[inner sep=0] {\\includegraphics[width=0.49\\linewidth]{figs/text_to_mask/a_wheel.pdf}};\n\\draw (-0.6, 0.68) node {\\cmark};\n\\end{tikzpicture} &\n\\begin{tikzpicture}\n\\draw (0, 0) node[inner sep=0] {\\includegraphics[width=0.49\\linewidth]{figs/text_to_mask/beaver_tooth_grille.pdf}};\n\\draw (-1.25, 0.68) node {\\cmark};\n\\end{tikzpicture}\\\\\n\\begin{tikzpicture}\n\\draw (0, 0) node[inner sep=0] {\\includegraphics[width=0.49\\linewidth]{figs/text_to_mask/a_wiper.pdf}};\n\\draw (-0.55, 0.67) node {\\xmark};\n\\end{tikzpicture} &\n\\begin{tikzpicture}\n\\draw (0, 0) node[inner sep=0] {\\includegraphics[width=0.49\\linewidth]{figs/text_to_mask/a_wiper_click.pdf}};\n\\draw (-0.97, 0.68) node {\\cmark};\n\\end{tikzpicture}\\\\\n\\begin{tikzpicture}\n\\draw (0, 0) node[inner sep=0] {\\includegraphics[width=0.49\\linewidth]{figs/text_to_mask/wipers.pdf}};\n\\draw (-0.50, 0.67) node {\\xmark};\n\\end{tikzpicture} &\n\\begin{tikzpicture}\n\\draw (0, 0) node[inner sep=0] {\\includegraphics[width=0.49\\linewidth]{figs/text_to_mask/wipers_click.pdf}};\n\\draw (-0.95, 0.68) node {\\cmark};\n\\end{tikzpicture}\\\\\n\\end{tabular}\n\\vspace{-4mm}\n\\caption{Zero-shot text-to-mask. \\sam can work with simple and nuanced text prompts. When \\sam fails to make a correct prediction, an additional point prompt can help.}\n\\label{fig:textprompts}\\vspace{-5mm}\n\\end{figure}\n    \n\n\\paragraph{Results.} We show qualitative results in \\fig{fig:textprompts}. \\sam can segment objects based on simple text prompts like ``a wheel'' as well as phrases like ``beaver tooth grille''. When \\sam fails to pick the right object from a text prompt only, an additional point often fixes the prediction, similar to~\\cite{ding2020phraseclick}.\n\n\\begin{figure*}[t]\\centering\n\\includegraphics[width=0.32\\linewidth]{figs/ablation_data_stages.pdf}\\hfill\n\\includegraphics[width=0.32\\linewidth]{figs/ablation_autodata_scale_log_plot.pdf}\\hfill\n\\includegraphics[width=0.32\\linewidth]{figs/ablation_backbone_params.pdf}\\vspace{-2mm}\n\\caption{Ablation studies of our data engine stages, image encoder scaling, and training data scaling. (Left) Each data engine stage leads to improvements on our 23 dataset suite, and training with only the automatic data (our default) yields similar results to using data from all three stages. (Middle) \\sam trained with \\app10\\% of \\sad and full \\sad is comparable. We train with all 11M images by default, but using 1M images is a reasonable practical setting. (Right) Scaling \\sam's image encoder shows meaningful, yet saturating gains. Nevertheless, smaller image encoders may be preferred in certain settings.}\n\\label{fig:ablations}\\vspace{-3mm}\n\\end{figure*}\n\n\\subsection{Ablations}\\label{sec:eval:ablations}\\vspace{-1mm}\n\nWe perform several ablations on our 23 dataset suite with the single center point prompt protocol. Recall that a single point may be ambiguous and that ambiguity may not be represented in the ground truth, which contains only a single mask per point. Since \\sam is operating in a zero-shot transfer setting there can be systematic biases between \\sam's top-ranked mask \\vs the masks resulting from data annotation guidelines. We therefore additionally report the best mask with respect to the ground truth (``oracle'').\n\n\\fig{fig:ablations} (left) plots \\sam's performance when trained on cumulative data from the data engine stages. We observe that each stage increases mIoU. When training with all three stages, the automatic masks vastly outnumber the manual and semi-automatic masks. To address this, we found that oversampling the manual and semi-automatic masks during training by 10$\\x$ gave best results. This setup complicates training. We therefore tested a fourth setup that uses only the automatically generated masks. With this data, \\sam performs only marginally lower than using all data (\\app0.5 mIoU). Therefore, by default we use only the automatically generated masks to simplify the training setup.\n\nIn \\fig{fig:ablations} (middle) we look at the impact of data volume. The full \\sad contains 11M images, which we uniformly subsample to 1M and 0.1M for this ablation. At 0.1M images, we observe a large mIoU decline under all settings. However, with 1M images, about 10\\% of the full dataset, we observe results comparable to using the full dataset. This data regime, which still includes approximately 100M masks, may be a practical setting for many use cases.\n\nFinally, \\fig{fig:ablations} (right) shows results with ViT-B, ViT-L, and ViT-H image encoders. ViT-H improves substantially over ViT-B, but has only marginal gains over ViT-L. Further image encoder scaling does not appear fruitful at this time.\n\n\\section{Discussion}\\label{sec:disc}\\vspace{-1mm}\n\n\\paragraph{Foundation models.} Pre-trained models have been adapted to downstream tasks since the early days of machine learning~\\cite{thrun1995learning}. This paradigm has become increasingly important in recent years with a growing emphasis on scale, and such models have recently been (re-)branded as ``foundation models'': \\ie models that are ``trained on broad data at scale and are adaptable to a wide range of downstream tasks''~\\cite{bommasani2021opportunities}. Our work correlates well with this definition, though we note that a foundation model for image segmentation is an inherently limited scope, since it represents an important, yet fractional, subset of computer vision. We also contrast one aspect of our approach with~\\cite{bommasani2021opportunities}, which emphasizes the role of \\emph{self-supervised} learning in foundation models. While our model is initialized with a self-supervised technique (MAE~\\cite{he2022masked}), the vast majority of its capabilities come from large-scale \\emph{supervised} training. In cases where data engines can scale available annotations, like ours, supervised training provides an effective solution.\n\n\\paragraph{Compositionality.} Pre-trained models can power new capabilities even beyond ones imagined at the moment of training. One prominent example is how CLIP~\\cite{Radford2021} is used as a \\emph{component} in larger systems, such as DALL$\\cdot$E~\\cite{Ramesh2021}. Our goal is to make this kind of composition straightforward with \\sam. We aim to achieve this by requiring \\sam to predict a valid mask for a wide range of segmentation prompts. The effect is to create a reliable interface between \\sam and other components. For example, MCC~\\cite{wu2023multiview} can easily use \\sam to segment an object of interest and achieve strong generalization to unseen objects for 3D reconstruction from a single RGB-D image. In another example, \\sam can be prompted with gaze points detected by a wearable device, enabling new applications. Thanks to \\sam's ability to generalize to new domains like ego-centric images, such systems work without need for additional training.\n\n\\paragraph{Limitations.} While \\sam performs well in general, it is not perfect. It can miss fine structures, hallucinates small disconnected components at times, and does not produce boundaries as crisply as more computationally intensive methods that ``zoom-in'', \\eg~\\cite{chen2022focalclick}. In general, we expect dedicated interactive segmentation methods to outperform \\sam when many points are provided, \\eg~\\cite{liu2022simpleclick}. Unlike these methods, \\sam is designed for generality and breadth of use rather than high IoU interactive segmentation. Moreover, \\sam can process prompts in real-time, but nevertheless \\sam's overall performance is not real-time when using a heavy image encoder. Our foray into the text-to-mask task is exploratory and not entirely robust, although we believe it can be improved with more effort. While \\sam can perform many tasks, it is unclear how to design simple prompts that implement semantic and panoptic segmentation. Finally, there are domain-specific tools, such as~\\cite{berg2019}, that we expect to outperform \\sam in their respective domains.\n\n\\paragraph{Conclusion.} The Segment Anything project is an attempt to lift image segmentation into the era of foundation models. Our principal contributions are a new task (promptable segmentation), model (\\sam), and dataset (\\sad) that make this leap possible. Whether \\sam achieves the status of a foundation model remains to be seen by how it is used in the community, but regardless we expect the perspective of this work, the release of over 1B masks, and our promptable segmentation model will help pave the path ahead.\n\n\\paragraph{Acknowledgments.} We would like to thank Aaron Adcock and Jitendra Malik for helpful discussion. We thank Vaibhav Aggarwal and Yanghao Li for help with scaling the model. We thank Cheng-Yang Fu, Jiabo Hu, and Robert Kuo for help with data annotation platform. We thank Allen Goodman and Bram Wasti for help in optimizing web-version of our model. Finally, we thank Morteza Behrooz, Ashley Gabriel, Ahuva Goldstand, Sumanth Gurram, Somya Jain, Devansh Kukreja, Joshua Lane, Lilian Luong, Mallika Malhotra, William Ngan, Omkar Parkhi, Nikhil Raina, Dirk Rowe, Neil Sejoor, Vanessa Stark, Bala Varadarajan, and Zachary Winstrom for their help in making the demo, dataset viewer, and other assets and tooling.\n\n{\\footnotesize\\linespread{.975}\\selectfont\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Neural ShDF: Reviving an Efficient and Consistent Mesh Segmentation Method}\n\n\\begin{document}\n\n\\title{Neural ShDF: Reviving an Efficient and Consistent Mesh Segmentation Method}\n\n\\author{Bruno Roy}\n\\email{bruno.roy@autodesk.com}\n\\affiliation{\n  \\institution{Autodesk Research}\n  \\city{Montreal}\n  \\country{Canada}\n}\n\n\\begin{abstract}\nPartitioning a polygonal mesh into meaningful parts can be challenging. Many applications require decomposing such structures for further processing in computer graphics. In the last decade, several methods were proposed to tackle this problem, at the cost of intensive computational times. Recently, machine learning has proven to be effective for the segmentation task on 3D structures. Nevertheless, these state-of-the-art methods are often hardly generalizable and require dividing the learned model into several specific classes of objects to avoid overfitting. We present a data-driven approach leveraging deep learning to encode a mapping function prior to mesh segmentation for multiple applications. Our network reproduces a neighborhood map using our knowledge of the \\textsl{Shape Diameter Function} (ShDF) method using similarities among vertex neighborhoods. Our approach is resolution-agnostic as we downsample the input meshes and query the full-resolution structure solely for neighborhood contributions. Using our predicted ShDF values, we can inject the resulting structure into a graph-cut algorithm to generate an efficient and robust mesh segmentation while considerably reducing the required computation times.\n\\end{abstract}\n\n\\begin{CCSXML}\n<ccs2012>\n   <concept>\n       <concept_id>10010147.10010371.10010396</concept_id>\n       <concept_desc>Computing methodologies~Shape modeling</concept_desc>\n       <concept_significance>500</concept_significance>\n       </concept>\n   <concept>\n       <concept_id>10010147.10010257.10010293.10010294</concept_id>\n       <concept_desc>Computing methodologies~Neural networks</concept_desc>\n       <concept_significance>500</concept_significance>\n       </concept>\n </ccs2012>\n\\end{CCSXML}\n\n\\ccsdesc[500]{Computing methodologies~Shape modeling}\n\\ccsdesc[500]{Computing methodologies~Neural networks}\n\n\\keywords{Mesh segmentation, machine learning, graph neural network, resolution-agnostic.}\n\n\\maketitle\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.85\\linewidth]{Figures/teaser.png}\n    \\caption{Our approach introduces a neural approximation of the \\textit{Shape Diameter Function}. We use the predicted values of our network to provide a highly adaptive and controllable mesh segmentation workflow.}\n    \\label{fig:teaser}\n\\end{figure}\n\n\\section{Introduction}\nMesh segmentation is the process of decomposing a polygonal mesh into meaningful parts. These meaningful parts consist of subsets of vertices or faces that are semantically relevant and informative for further applications. Decomposing such structures is usually performed by defining edge loops (i.e., pairs of vertices) acting as boundaries between these subsets of elements. Although performing this task on a closed manifold mesh seems inherently intuitive, generalizing is still challenging considering that multiple valid solutions exist for any mesh. Moreover, most of the state-of-the-art methods are rather time-consuming.\n\nThe problem with current mesh segmentation methods is that they mostly rely on constraints -- making them highly time-consuming as they require iterative solvers to jointly optimize them. The constraint types are generally cardinality, geometry, and topological, respectively guiding the number of segmented parts, biases towards specific primitive shapes, and sub-mesh connected components. Some of these methods also require the user to select initial seeds to reduce the number of iterations before satisfying the constraints, adding on the total time required to accomplish the mesh segmentation task.\n\nIn recent years, machine learning has revived the problem of segmentation on meshes by proposing much more generalizable approaches and by offering a better understanding of the intrinsic semantics of shapes. Although these approaches have considerably eased this challenging task, the time required to train and cover a wide spectrum of different topologies remains problematic for use on dense meshes.\n\nIn this work, we propose a data-driven method leveraging a correspondence between the surface and its underlying volume to efficiently segment a polygonal mesh. We generate this correspondence by using a measure based on the medial axis transform from the \\textit{Shape Diameter Function} (ShDF). The neural ShDF values are then used as inputs to a graph-cut algorithm providing an efficient and controllable workflow for mesh segmentation. By combining our Neural ShDF with a state-of-the-art graph-cut algorithm, our proposed method is capable of generating a high-quality segmentation for any polygonal mesh at a fraction of the cost. In addition, our approach provides an intuitive way to generate various solutions for mesh segmentation by reusing the neural ShDF values with different sets of parameters throughout the graph-cut steps. The key contributions are as follows:\n\\begin{itemize}\n    \\item{We introduce a neural \\textit{Shape Diameter Function} improving generalization of local feature for mesh segmentation.}\n    \\item{We propose a novel approach reusing neural ShDF values to generate various and unique outcomes -- making it efficient and highly adaptive.}\n    \\item{We propose a resolution-agnostic approach by downsampling the input mesh while solely querying the full-resolution mesh for neighborhoods.}\n    \\item{We provide experimental results on various applications of mesh segmentation for computer graphics workflows.}\n\\end{itemize}\n\n\\begin{figure*}[t]\n  \\centering\n   \\includegraphics[width=\\textwidth]{Figures/workflow.png}\n   \\caption{This is our \\textit{Neural Shape Diameter} approach workflow. We train using both full-resolution (for neighbors when updating nodes by the \\textbf{Messenger}) and coarse-resolution meshes (downsampled for training) with our Encoder-Messenger-Decoder ($\\textbf{EMD}$) network. The $\\textbf{EMD}$ network generates the predicted ShDF values $\\tilde{\\Lambda}$ used by our k-way graph-cut algorithm (i.e., gaussian mixture and clustering on GPU) to partition the final segmentation.}\n   \\label{fig:workflow}\n\\end{figure*}\n\n\\section{Related Work}\nOver the last two decades, mesh segmentation has been used for various applications in computer graphics. This task has proven to benefit many applications in 3D shape analysis such as texture mapping~\\cite{sander2003multi}, 3D shape modeling~\\cite{ji2006easy}, 3D shape retrieval~\\cite{ferreira2010thesaurus}, multi-resolution and mesh compression~\\cite{maglo2011cluster}, and animation~\\cite{yuan2016space}. We refer the reader to this survey for further details~\\cite{rodrigues2018part} on part-based mesh segmentation. Historically, the traditional problem of mesh segmentation has been approached in many ways.\n\\paragraph{\\textbf{Region growing.}}\nOne of the most intuitive and simple approaches for segmentation is the \\textit{region growing} technique. The criterion that determines whether or not an element should be added to a cluster is what mainly differentiates the variations of the available \\textit{region growing} algorithms. Among other criteria: representative planes~\\cite{kalvin1996superfaces}, curvature~\\cite{lavoue2005new}, and convexity~\\cite{chazelle1995strategies, sheffer2007shuffler} were used as conditioners for clustering. Another common variation of the \\textit{region growing} method using multiple source seeds to initiate the growing process~\\cite{levy2002least, sorkine2002bounded, eck1995multiresolution}.\n\\paragraph{\\textbf{Clustering.}} %Combine both hierarchical and iterative clustering algorithms\nMerge operations on clusters is also crucial when it comes to segmentation tasks~\\cite {garland2001hierarchical, attene2006hierarchical, gelfand2004shape, sander2001texture}. Although \\textit{hierarchical clustering} algorithms are similar to \\textit{growing regions} algorithms, these algorithms prioritize set operations between existing clusters in a structured way. \\textit{Iterative clustering} algorithms are stated as parametric as the number of clusters is given a-priori. As opposed to previous methods, \\textit{iterative clustering} methods are focusing on converging towards optimal segmentation given a number of clusters~\\cite{lloyd1982least, hart2000pattern, shlafman2002metamorphosis, cohen2004variational, wu2005structure}.\n\\paragraph{\\textbf{Implicit methods.}}\nThe \\textit{implicit methods} for mesh segmentation are the closest in spirit to ours as they focus on boundaries and correspondences between subsets of elements of the object to segment. Again, the main difference between these algorithms is how they define the boundaries and underlying structures. To mention a few, the most common ways pass through curvature/contours~\\cite{lee2005mesh, levy2002least, mitani2004making}, subdivision~\\cite{katz2003hierarchical, podolak2006planar} (similar to hierarchical clustering), and underlying structures connecting the shape with intrinsic surface properties~\\cite{li2001decomposing, raab2004virtual, lien2006simultaneous}. As a matter of fact, the \\textit{Shape Diameter Function} algorithm~\\cite{shapira2008consistent} is at the intersection of using subdivision and an underlying structure. The intuiting behind the \\textit{Shape Diameter Function} is to produce a diameter measure throughout vertex neighborhoods of the mesh. The resulting measure (i.e., ShDF values) relates to the medial axis transform and provides a volume correspondence of the shape at any given point on the surface. These per-vertex measures are then used as a threshold for the graph-cut algorithm. In our approach, we take advantage of the generalization power of neural networks to estimate per-vertex properties as input to a graph-cut algorithm -- making the latter highly adaptive for mesh segmentation.\n\\paragraph{\\textbf{Data-driven.}}\nNeural networks have been widely used for the segmentation problem on images~\\cite{lai2015deep}, point clouds~\\cite{qi2017pointnet}, and more recently, meshes~\\cite{hanocka2019meshcnn}. Several interesting approaches were proposed lately to tackle the mesh segmentation problem using deep neural network algorithms such as using convolution operators on edges~\\cite{hanocka2019meshcnn}, converting 3D shapes into voxel-based representations~\\cite{wu20153d, wang2017cnn, graham20183d}, and leveraging local features of point clouds~\\cite{qi2017pointnet, qi2017pointnet++}. Nevertheless, these state-of-the-art data-driven methods remain hardly generalizable and often require dividing the learned model into several specific classes of objects to avoid overfitting. In contrast to these methods and Kovacicet et al.~\\cite{kovacic2010fast}, we avoid the computationally expensive part by using predicted ShDF values $\\tilde{\\Lambda}$ to generate the final mesh segmentation. Moreover, learning a mapping function as opposed to directly learning to classify mesh elements~\\cite{kalogerakis2010learning} (e.g., to cluster vertices) makes our approach more robust when used in unknown settings.\n\n\\section{Method}\nAs a formal definition, the traditional mesh segmentation task is described as follows: given a closed manifold mesh $\\textbf{M}$, and $\\textbf{E}$ the set of mesh elements (i.e., vertices, edges, or faces). The segmentation $\\textbf{S}$ of mesh $\\textbf{M}$ is a set of sub-meshes $\\textbf{S}=\\{M_0, ..., M_{n-1}\\}$, where $M_i$ is defined as a subset of elements $e \\in \\textbf{E}$.\n\n\\subsection{Neural ShDF}\nIn the traditional \\textit{Shape Diameter Function} method, the set of sub-meshes $\\textbf{S}$ is obtained by subdividing a graph using the diameter measures $\\lambda_i$ from per-vertex neighborhoods $\\dot{v}_i$ as thresholds for the graph-cut algorithm. As we aim to estimate these ShDF values through a graph neural network $\\textbf{EMD}$, the problem of finding the sub-meshes $M_i$ can be expressed as partitioning $\\textbf{S}$ such that the constraint criteria $\\{C_0, ..., C_n\\}$ are minimized. The predicted ShDF values $\\tilde{\\lambda}_i$ are then used as threshold criteria to a graph-cut algorithm to generate the sub-meshes $\\textbf{S}=\\{M_0, ..., M_{n-1}\\}$.\n\nThe constraint criteria $\\{C_0, ..., C_n\\}$ are defined as two terms: the similarities between the reference ShDF values $\\lambda_i$ and the predicted ShDF values $\\tilde{\\lambda}_i$, and the neighborhood densities $\\rho_i$ of vertices. We express the $L_2$ similarities between the reference ShDF values and the predicted ones as follows:\n\\begin{equation}\n\\label{eq:loss_SDF}\nL_{\\Lambda}=\\frac{1}{n_j}\\sum_{i}^{n_j}{\\alpha\\|\\lambda_i-\\tilde{\\lambda}_i\\|_2},\n\\end{equation}\nwhere $n$ is the number of downsampled surface vertices (i.e., used to query the neighborhoods). Along with the input vertices, we provide to our network $\\textbf{EMD}$ an additional term to weigh in the local mesh density for adaptive resolution meshes. We use the Poisson sampling disk method at the surface of meshes to select the downsampled vertices and compute their neighborhood density. The resulting densities $\\rho_i$ are then used as a scaling factor during the messaging stage within our network to properly propagate the attributes in the current neighborhood.\n\n\\subsection{Model Architecture}\nOur network $\\textbf{EMD}$ architecture is essentially based on the Encode-Message-Decode model. The network is composed of two hidden MLP layers on both Encoder and Messenger with an output size of 128. The resulting Decoder output size matches the downsampled vertices provided as input during training and inference. Our model is trained and loaded for inference using an A6000 GPU with the Adam optimizer for 5M training steps with an exponential learning rate decay from $10^{-3}$ to $10^{-5}$ when passing the threshold of 3M steps.\n\n\\subsubsection{\\textbf{Resolution-Agnostic Graph Network}}\n\\begin{wrapfigure}[7]{r}[0.25\\columnwidth]{4.5cm}\n\\vspace*{-0.6cm}\n\\hspace*{-0.6cm}\n\\includegraphics[width=0.15\\textwidth]{Figures/neighborhood_densities.png} \n\\label{fig:neighborhood}\n\\end{wrapfigure}\nWe handle varying resolutions using two mechanisms: downsampling the input mesh while maintaining full-resolution neighborhoods and using the neighborhood densities $\\rho_i$ for each node $m^i$ as scaling factors when updating the current nodes during the messaging stage. The idea to remain resolution-agnostic is to pass the full-resolution neighborhoods along the downsampled mesh. That way, we can query the vertex neighborhoods by solely using the downsampled mesh vertices to compute the ShDF values $\\tilde{\\Lambda}$. The density values are used to scale the contributions of the neighbors through the edges $e^i$ during the message-passing step in our network. Moreover, as our network architecture requires a known input size, we use a fixed radius with the Poisson disk sampling algorithm to compute the neighborhood densities.\n\n\\subsubsection{\\textbf{Graph Cut}}\n\\label{sec:graph_cut}\nOnce the predicted ShDF values $\\tilde{\\lambda}$ are obtained, we use them as inputs to a fast $k$-way graph-cut algorithm to offer an efficient and flexible way for mesh segmentation. Similarly to Fig.~\\ref{fig:optimizing_params} in the appendices, our approach can leverage the predicted ShDF values using the grid-search method to find the optimal parameters for the mesh segmentation. We use a GPU implementation of the $k$-way graph partitioning as our graph-cut algorithm leveraging the \\href{https://developer.nvidia.com/nvgraph}{nvGRAPH library} from NVIDIA.\n\nSimilarity to~\\cite{shapira2008consistent}, our partitioning algorithm is composed of two steps. The first uses soft-clustering of the mesh elements (faces) to compute $k$ clusters based on their ShDF values, and the second finds the actual partitioning using $k$-way graph-cut to include local mesh geometric properties. Note that $k$, the number of clusters chosen, is more naturally related to the number of levels in the hierarchy and not to the number of parts.\n\n\\begin{figure}[h]\n  \\centering\n   \\includegraphics[width=\\linewidth]{Figures/recursive_refinement.png}\n   \\caption{Example of the recursive refinement step on the \\textsl{Samurai} model.}\n   \\label{fig:refinement}\n\\end{figure}\n\nAs an optional post-processing step, we recursively use our approach as a refinement process to improve the segmentation of detailed parts (as shown in Fig.~\\ref{fig:refinement}). In a complementary manner, we also performed on some of the presented results a post-processing step to reduce the noise close to the boundaries of the dense meshes. We update the boundaries between segmented parts by using an alpha expansion graph-cut algorithm considering both smoothness and concavity measures by looking at the dihedral angles between faces (as shown on the \\textsl{Mayabot} in Fig.~\\ref{fig:smooth_boundaries}).\n\n\\begin{figure}[h]\n  \\centering\n   \\includegraphics[width=\\linewidth]{Figures/smooth_boundaries.png}\n   \\caption{Post-processing leveraging the alpha expansion to smooth the segmentation boundaries.} \n   \\label{fig:smooth_boundaries}\n\\end{figure}\n\n\\begin{figure}[t]\n  \\centering\n   \\includegraphics[width=\\linewidth]{Figures/dataset_v2.png}\n   \\caption{Base mesh and a subset of blend shapes used to generate the Autodesk Character Generator (ACG) dataset.} \n   \\label{fig:dataset}\n\\end{figure}\n\n\\section{Dataset}\nThe dataset used to train our network $\\textbf{EMD}$ is generated using the \\textsl{Autodesk Character Generator}(ACG)\\copyright~tool producing multiple variants of the same base mesh by applying blend shape operations on it. The generated dataset is self-consistent since it is exclusively composed of meshes having the same number of vertices. To train our network to encode resolution-agnostic features, we used a tessellation technique during training to provide multiple versions of the same mesh. We also used a re-meshing method to make our network resilient to consistent input samples (i.e., by changing the positions of selected downsampled vertices). We only use the tessellated and re-meshed meshes for neighborhood contributions during training.\n\nAs shown in Fig.~\\ref{fig:dataset}, we use several blend shapes $\\textbf{B}$ to alter a base mesh into many variants to grow our dataset. The blend shapes $\\textbf{B}$ are divided into two groups: facial $\\textbf{B}^{f}$ and body $\\textbf{B}^{b}$. For both groups, the blend shapes used to augment our dataset are described as either traits $\\textbf{B}_{t}$ or features $\\textbf{B}_{f}$. By permuting these blend shapes, we can generate a large dataset using solely a single base mesh. Additionally, we use an animation skeleton to randomly generate different poses as it may generate different ShDF values for the same mesh. Similarly to \\cite{shapira2008consistent}, we use anisotropic smoothing on the ShDF values to overcome these differences during training. Moreover, we have built an additional custom dataset using TurboSquid (TS) assets (Fig.~\\ref{fig:subset_SDF}) to evaluate our approach with production-ready content.\n\n\\section{Experiments and Results}\nWe evaluated our approach against a variety of state-of-the-art methods to highly its efficiency and precision while remaining controllable for the users. In the following sections, we compared our approach with a few baselines using known segmentation datasets and our own. We also present computation times on several scenarios using our approach as a few ones from previous work. We briefly discuss a particular case where our approach performs better than the state-of-the-art on dense meshes. Lastly, we will present a few applications that we believe would be useful to improve traditional graphics workflows.\n\n\\begin{figure}[h]\n     \\centering\n     \\begin{subfigure}{0.125\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{Figures/compare_error_barrel_p1.png}\n         \\caption{Reference}\n         \\label{fig:sdf_seg}\n     \\end{subfigure}\n     \\hfill\n     \\begin{subfigure}{0.2098\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{Figures/compare_error_barrel_p2.png}\n         \\caption{Ours}\n         \\label{fig:unwrapping_uvs}\n     \\end{subfigure}\n     \\begin{subfigure}{0.125\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{Figures/compare_error_barrel_p3.png}\n         \\caption{Prediction error}\n         \\label{fig:unwrapping_uvs}\n     \\end{subfigure}\n    \\caption{On this barrel, we compare the precision of predicting the ShDF values $\\tilde{\\Lambda}$ (b) with the baseline (a). We also show that our approach generates a fairly low error rate compared to the baseline (white: no error, red: error$<0.1\\%$).}\n    \\label{fig:barrel_error}\n\\end{figure}\n\n\\subsection{Performance analysis}\nAs our approach solely focuses on mesh segmentation, we have limited the analysis to recent methods performing well in that area. We have focused on four datasets to evaluate the precision of our approach: COSEG, Human Body Segmentation (HBS), Autodesk Character Generator, and TurboSquid. We compared the precision of our approach on the segmentation task with PointNet, PointNet++, and MeshCNN.\n\n\\begin{table}[h]\n    \\centering\n    \\begin{tabular}{|l||c|c|c|c|}\n        \\hline\n        \\multirow{2}{*}{\\textbf{Method}} & \\multicolumn{4}{|c|}{\\textbf{Accuracy}}\\\\\\cline{2-5}\n          & \\textbf{COSEG} & \\textbf{HBS} & \\textbf{ACG} & \\textbf{TS}\\\\[0.5ex]\n        \\hline\\hline\n        \\textsl{PointNet} & 91.5\\% & 86.5\\% & 72.4\\% & 58.3\\%\\\\\n        \\hline\n        \\textsl{PointNet++} & 94.7\\% & 90.8\\% & 74.7\\% & 61.2\\%\\\\\n        \\hline\n        \\textsl{MeshCNN} & 97.3\\% & 92.3\\% & 78.3\\% & 72.8\\%\\\\\n        \\hline\n        \\textsl{Ours} & \\textbf{97.1\\%} & \\textbf{94.2\\%} & \\textbf{92.4\\%} & \\textbf{86.6\\%}\\\\\n        \\hline\n    \\end{tabular}\n    \\caption{Evaluations comparing our approach with state-of-the-art methods on multiple datasets.}\n    \\label{table:metrics_STAR}\n\\end{table}\nAs presented in Table~\\ref{table:metrics_STAR}, our approach performs similarly to MeshCNN and slightly outperforms PointNet and PointNet++ on the COSEG (only the \\textsl{Vases} set) and HBS datasets. Also highlighted in that same table, our approach is way more accurate when used on the ACG and TS datasets (even when compared to MeshCNN). This is not surprising as our approach is trained on multiple-resolution samples. By querying the full-resolution neighborhoods and their corresponding downsampled mesh, our approach has proven to be less sensitive to adaptive meshing, which is the case of most of the 3D models contained in the TS dataset.\nLastly, as we aim to speed up the whole traditional mesh segmentation process, we have also compiled a few computation-time results on the presented assets. As shown in Table~\\ref{table:performances}, our neural-based approach shows a speed-up factor of up to 10x compared to the original \\textit{Shape Diameter Function} method in most of the presented assets.\n\\begin{table}[t]\n    \\centering\n    \\begin{tabular}{|l||c|c|c|c|c|}\n        \\hline\n        \\multirow{2}{*}{\\textbf{Mesh}} & \\multirow{2}{*}{\\textbf{Faces}} & \\multicolumn{4}{|c|}{\\textbf{Accuracy}}\\\\\\cline{3-6}\n        &  & \\textbf{ShDF} & \\textbf{Part.} & \\textbf{Ref.} & \\textbf{PP}\\\\[0.5ex]\n        \\hline\\hline\n        \\textsl{ACM Box} & 1k & 96.5\\% & 94.4\\% & - & -\\\\\n        \\hline\n        \\textsl{Gas Can} & 1.5k & 95.3\\% & 96.9\\% & - & -\\\\\n        \\hline\n        \\textsl{Teapot} & 6.3k & 96.9\\% & 99.7\\% & - & -\\\\\n        \\hline\n        \\textsl{Mayabot} & 56k & 92.4\\% & 94.2\\% & 95.3\\% & -\\\\\n        \\hline\n        \\textsl{Samurai} & 242k & 88.1\\% & 89.0\\% & 92.4\\% & 93.1\\%\\\\\n        \\hline\n        \\textsl{Gladiator Hulk} & 1.3M & 87.3\\% & 90.7\\% & 90.7\\% & 91.4\\%\\\\\n        \\hline\n    \\end{tabular}\n    \\caption{Precision metrics obtained on several 3D models. The performances are broken down into four parts: generating ShDF values, partitioning (Part.), refinement (Ref.), and post-processing (PP).}\n    \\label{table:performance_mesh}\n\\end{table}\n\\begin{table*}[h]\n    \\centering\n    \\begin{tabular}{|l||c|c|C{1.6cm}|C{1.6cm}|C{1cm}|C{1.6cm}|C{1.6cm}|C{1cm}|}\n        \\hline\n        \\multirow{2}{*}{\\textbf{Mesh}} & \\multirow{2}{*}{\\textbf{Vertices}} & \\multirow{2}{*}{\\textbf{Faces}} & \\multicolumn{3}{|c|}{\\textbf{Baseline~\\cite{shapira2008consistent}}} & \\multicolumn{3}{|c|}{\\textbf{Ours}}\\\\\\cline{4-9}\n        & & & \\textbf{ShDF} & \\textbf{Partitioning} & \\textbf{Total} & \\textbf{ShDF} & \\textbf{Partitioning} & \\textbf{Total}\\\\[0.5ex]\n        \\hline\\hline\n        \\textsl{ACM Box} & 0.5k & 1k & 441 & 45 & 486 & \\textbf{30} & 26 & \\textbf{56}\\\\\n        \\hline\n        \\textsl{Gas Can} & 0.8k & 1.5k & 662 & 46 & 708 & \\textbf{44} & 26 & \\textbf{70}\\\\\n        \\hline\n        \\textsl{Teapot} & 3.6k & 6.3k & 2836 & 58 & 2894 & \\textbf{190} & 33 & \\textbf{223}\\\\\n        \\hline\n        \\textsl{Mayabot} & 29.5k & 56k & 23239 & 132 & 23371 & \\textbf{1555} & 70 & \\textbf{1624}\\\\\n        \\hline\n        \\textsl{Samurai} & 58.1k & 242k & 45689 & 139 & 45828 & \\textbf{3057} & 133 & \\textbf{3190}\\\\\n        \\hline\n        \\textsl{Gladiator Hulk} & 671k & 1.3M & 528578 & 816 & 529394 & \\textbf{35366} & 408 & \\textbf{35775}\\\\\n        \\hline\n    \\end{tabular}\n    \\caption{The timing results are expressed in milliseconds.}\n    \\label{table:performances}\n\\end{table*}\nFinally, to improve the partitioning part of our approach, we implemented the Gaussian mixture and the clustering to GPU. With our implementation, we almost halved the computation times required by these steps.\n\n\\subsection{Dense meshses}\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{Figures/dense_mesh_hulk.png}\n    \\caption{Example of our approach performed on a dense mesh (source: \\href{https://desirefx.me/3d_models/gladiator-hulk-mesh-3d-model}{Desire FX}). The generated ShDF values (a) are used to segment the mesh (b).}\n    \\label{fig:densemesh}\n\\end{figure}\nWe also demonstrate that our refinement and post-processing steps can improve the results by a few points on dense meshes. With an error rate of below 2\\% compared to the ground truth, we show that our neural ShDF approach performs well on dense meshes. As shown in Table~\\ref{table:performance_mesh}, our approach produces high-precision ShDF values on both the \\textsl{Samurai} and \\textsl{Gladiator Hulk} meshes. We can provide even more precise values using the refinement and post-processing steps. For example, with the \\textsl{Gladiator Hulk} mesh (Fig.~\\ref{fig:densemesh}), we were able to extract small additional details using the refinement step on segmented parts such as the shoulder pad presented in the close-up (b).\n\nOn the \\textsl{Samurai} mesh, we obtained better results of the segmentation after post-processing the boundaries (between each segmented part) to smooth them out using alpha expansion based on the dihedral angles between faces (i.e., to measure smoothness and concavity). We decided not to use the refinement and post-processing steps when the resulting segmentation seemed visually adequate.\n\n\\subsection{Applications}\nAs previously stated, many tasks intuitively require decomposing meshes before further processing. Mesh segmentation finds many applications in 3D shape analysis such as: texture mapping~\\cite{sander2003multi}, 3D shape modeling~\\cite{ji2006easy}, 3D shape retrieval~\\cite{ferreira2010thesaurus}, multi-resolution and mesh compression~\\cite{maglo2011cluster}; and animation~\\cite{yuan2016space}. We refer the reader to this survey for further details~\\cite{rodrigues2018part} on part-based mesh segmentation. For example, Fig.~\\ref{fig:application_element} shows a typical example in which a cube with an engraved logo is decomposed into meaningful parts using our neural ShDF approach (a). Once segmented (b), the parts can then be selected (c) for further manipulations such as: replacing with a different logo, deforming the selected logo, or as shown, scaling up and removing parts of the logo.\n\n\\begin{figure}[h]\n  \\centering\n   \\includegraphics[width=\\linewidth]{Figures/element_application.png}\n   \\caption{A simple application example of altering an engraved logo in 3ds Max.}\n   \\label{fig:application_element}\n\\end{figure}\n\nWe have also experimented to see how effective our approach would be in the case of UV mapping. The unwrapping part is known to be very unintuitive and requires expert knowledge of shape topology to properly unwrap 3D models into a 2D space while preserving the semantic meaning of the parts. Recently, there has been a single attempt to solve this problem by directly predicting UV seams using a graphical attention network (GAT)~\\cite{teimury2020graphseam}. Although promising, this proposed method ended up being rather limited to the training set and was hardly generalizable for production uses. We believe that being able to provide semantically meaningful parts on complex models can partially alleviate a fair portion of the pain of the traditional UV mapping workflow. That way, each simpler part can be unwrapped using only a few cuts and simple projection methods. In Fig.~\\ref{fig:unwrapping}, we show our approach leveraged to facilitate the 3D unfolding task. As shown, the \\textsl{Gas Can} is first segmented into meaningful parts (bottom of Fig.~\\ref{fig:sdf_seg}) before running an automatic unwrapping based on the prominent shape features. Our solution from the segmented object (top of Fig.~\\ref{fig:unwrapping_uvs}) clearly outperformed the one generated using an automatic method in Maya (bottom image).\n\\begin{figure}\n     \\centering\n     \\begin{subfigure}{0.225\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{Figures/application_unwrapping_left.png}\n         \\caption{ShDF value (top) and segmented parts (bottom).}\n         \\label{fig:sdf_seg}\n     \\end{subfigure}\n     \\hfill\n     \\begin{subfigure}{0.225\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{Figures/application_unwrapping_right.png}\n         \\caption{Automatic UV mapping on the whole mesh (top) and recursively on segmented parts (bottom).}\n         \\label{fig:unwrapping_uvs}\n     \\end{subfigure}\n    \\caption{Piece-wise UV unfold.}\n    \\label{fig:unwrapping}\n\\end{figure}\n\n\\section{Conclusion}\nWe have presented a \\textit{Neural Shape Diameter Function} approach allowing us to considerably accelerate high-quality mesh segmentation while remaining adaptive and controllable. Using the generated ShDF values for our method, we can generate multiple alternative segmentation solutions of the same mesh by adjusting the partitioning parameters for our k-way graph-cut algorithm.\n\nAlthough there are powerful graph-cut methods implemented on GPU, the use of adaptive tessellation-generated meshes in production to optimize memory makes them less efficient than deep neural networks for segmenting such irregular structures containing a highly variable number of neighbors per vertex (as opposed to images). While our approach partially mitigates this part by querying the mesh at full resolution, we are confident that using stacked networks could still benefit from this approach by leveraging our current network outputs as inputs to a non-binary classifier. We may investigate that architecture as future work.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{Figures/subset_SDF.png}\n    \\caption{TurboSquid (TS) dataset: subset of assets.}\n    \\label{fig:subset_SDF}\n\\end{figure}\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2112.01801v3.tex",
        "arXiv-2304.02643v1.tex",
        "arXiv-2306.11737v2.tex"
    ],
    "group_id": "group_21",
    "response": "### Title: Advances in Geometric Deep Learning for 3D Surface Parsing and Mesh Segmentation\n\n### Introduction\nGeometric deep learning is a specialized domain within deep learning that focuses on processing non-Euclidean data structures, such as graphs, point clouds, and 3D meshes. This field has seen significant advancements over the past few years, driven by the need to analyze and understand complex 3D shapes and scenes in computer graphics and vision. Historically, the challenge has been to develop deep learning techniques that can handle the irregular and heterogeneous nature of these data structures, which do not conform to the regular grid patterns typical of images and videos. The introduction of convolutional operations on graphs and point clouds has been a crucial step in this direction, enabling the extraction of meaningful features from these structures. However, the application of deep learning to 3D meshes, which are often represented as triangle meshes, has been limited due to the complexity of defining suitable convolutional operations and the lack of efficient mesh processing tools in modern deep learning libraries.\n\nCurrent progress in geometric deep learning includes the development of novel convolutional kernels and pooling operations tailored for 3D meshes, as well as the creation of large-scale datasets for training and evaluating these models. Challenges remain in achieving efficient and effective hierarchical modeling of 3D surfaces, handling the heterogeneous sizes and scales of real-world meshes, and ensuring the robustness and generalization of models across different datasets and tasks. This summary will explore three recent papers that address these challenges by introducing new convolutional operations, efficient mesh processing techniques, and large-scale datasets for 3D surface parsing and mesh segmentation.\n\n### Main Content of Each Paper\n\n#### Paper 1: Mesh Convolution with Continuous Filters for 3D Surface Parsing\nThe first paper, \"Mesh Convolution with Continuous Filters for 3D Surface Parsing,\" introduces a series of modular operations for geometric feature learning from 3D triangle meshes. These operations include novel mesh convolutions, efficient mesh decimation, and associated mesh (un)poolings. The authors propose using spherical harmonics as orthonormal bases to create continuous convolutional filters, which are applied to the mesh structure itself, rather than treating the mesh as an edge-based graph. This approach simplifies the construction of convolutional kernels and allows for the parameterization of filters based on the surface normals of 3D meshes. The paper also presents a hierarchical neural network, named PicassoNet++, which leverages these operations to achieve highly competitive performance in shape analysis and scene segmentation on prominent 3D benchmarks. The authors provide an open-source implementation of these operations, termed Picasso, which supports heterogeneous mesh batching and processing.\n\nThe paper evaluates PicassoNet++ on various 3D benchmarks, including the ShapeNetCore dataset for 3D shape classification and the S3DIS and ScanNet datasets for large-scale surface segmentation. The authors demonstrate that dual convolutions, which are used in previous works, are unnecessary for high-resolution meshes as mesh convolutions are effective enough to learn distinctive features. This leads to significant computational savings. The authors also introduce a GPU-accelerated mesh decimation technique that simplifies a batch of meshes on-the-fly, allowing control over the decimated mesh resolution using the number of vertices. This decimation process is crucial for establishing hierarchical architectures in neural networks, enabling multi-scale feature extraction.\n\n#### Paper 2: Segment Anything\nThe second paper, \"Segment Anything,\" introduces a new task, model, and dataset for image segmentation. The authors propose the \\textit{promptable segmentation} task, which involves returning a valid segmentation mask given any segmentation prompt. This task is designed to enable powerful generalization and to solve a wide range of downstream segmentation problems via prompt engineering. The model, referred to as the Segment Anything Model (\\sam), consists of three components: an image encoder, a flexible prompt encoder, and a fast mask decoder. The image encoder is a Vision Transformer (ViT) pre-trained using a self-supervised method (MAE), while the prompt encoder and mask decoder are lightweight and designed to handle various types of prompts, including points, boxes, and text. The \\sam model is trained on a large and diverse dataset of over 1 billion masks from 11 million licensed and privacy-respecting images, collected using a data engine that iterates between model-assisted manual annotation, semi-automatic annotation, and fully automatic mask generation.\n\nThe authors extensively evaluate the \\sam model on numerous segmentation tasks, including edge detection, object proposal generation, instance segmentation, and text-to-mask prediction. They find that \\sam produces high-quality masks from a single foreground point, often competitive with or even superior to fully supervised models. The model's zero-shot performance is particularly impressive, indicating its ability to generalize to new data distributions and tasks without additional training. The authors also perform a Responsible AI (RAI) analysis, investigating potential fairness concerns and biases when using the dataset and model.\n\n#### Paper 3: Neural ShDF: Reviving an Efficient and Consistent Mesh Segmentation Method\nThe third paper, \"Neural ShDF: Reviving an Efficient and Consistent Mesh Segmentation Method,\" presents a data-driven approach leveraging deep learning to estimate the \\textit{Shape Diameter Function} (ShDF) values for mesh segmentation. The authors introduce a neural network architecture, referred to as the Encode-Message-Decode (EMD) network, which predicts ShDF values for downsampled vertices of a mesh while querying full-resolution neighborhoods. These predicted ShDF values are then used as inputs to a fast $k$-way graph-cut algorithm to partition the mesh into meaningful parts. The approach is resolution-agnostic, meaning it can handle meshes of varying resolutions without requiring retraining. The authors also implement the Gaussian mixture and clustering steps of the graph-cut algorithm on GPU, significantly reducing computation times.\n\nThe authors evaluate their approach on various datasets, including COSEG, Human Body Segmentation (HBS), Autodesk Character Generator (ACG), and TurboSquid (TS). They compare their method with state-of-the-art approaches such as PointNet, PointNet++, and MeshCNN. The results show that their approach is highly accurate and efficient, with a speed-up factor of up to 10x compared to traditional methods. The authors also demonstrate the effectiveness of their approach in applications such as texture mapping, 3D shape modeling, and UV mapping, highlighting its potential to improve traditional graphics workflows.\n\n### Commonalities and Innovations\nAll three papers focus on advancing geometric deep learning techniques for 3D data processing. They introduce novel methods and architectures that improve the efficiency, accuracy, and generalization of models for 3D surface parsing and mesh segmentation. The common theme is the use of deep learning to extract meaningful features from 3D meshes, which are often represented as triangle meshes with varying resolutions and scales.\n\n- **Paper 1** introduces modular operations for mesh processing, including convolutional filters based on spherical harmonics, GPU-accelerated mesh decimation, and (un)pooling operations. The authors also present a hierarchical neural network, PicassoNet++, which achieves highly competitive performance on various 3D benchmarks.\n- **Paper 2** proposes a new task and model for image segmentation, leveraging prompt engineering to enable zero-shot transfer to a wide range of downstream tasks. The authors introduce the Segment Anything Model (\\sam), which is trained on a large dataset of over 1 billion masks and can generate high-quality segmentation masks from various prompts, including points, boxes, and text.\n- **Paper 3** presents a neural network approach to estimate ShDF values for mesh segmentation, using a resolution-agnostic architecture that can handle meshes of varying resolutions. The authors demonstrate the effectiveness of their approach in improving traditional mesh segmentation workflows and show significant speed-ups compared to traditional methods.\n\n### Comparison of Results and Discussion\nThe results from the three papers highlight the effectiveness of their respective approaches in different domains of 3D surface parsing and mesh segmentation. \n\n- **Paper 1** achieves excellent performance on various 3D benchmarks, including ShapeNetCore, SHREC, CUBE, COSEG, HUMAN, and FAUST datasets. The authors report that PicassoNet++ outperforms previous methods, particularly in the context of real-world scene surface parsing using the S3DIS and ScanNet datasets. The network's zero-shot performance is noted to be competitive, indicating its ability to generalize to new data distributions and tasks.\n- **Paper 2** demonstrates that the \\sam model can produce high-quality segmentation masks from a single foreground point, often only slightly below the manually annotated ground truth. The authors also show that \\sam can be used to solve a variety of tasks, including edge detection, object proposal generation, and instance segmentation, under a zero-shot transfer protocol. The model's performance is consistent across different regions and income levels, indicating its potential for equitable use in real-world applications.\n- **Paper 3** achieves high precision in predicting ShDF values and generating meaningful mesh segmentations. The authors report that their approach is more accurate and efficient than traditional methods, particularly on dense meshes. The method is also shown to be effective in applications such as UV mapping, texture mapping, and 3D shape modeling, indicating its potential to improve traditional graphics workflows.\n\n### Conclusion\nThe three papers collectively advance the field of geometric deep learning by introducing new convolutional operations, efficient mesh processing techniques, and large-scale datasets for 3D surface parsing and mesh segmentation. \n\n- **Paper 1** introduces Picasso, a modular library for deep learning over 3D meshes, and PicassoNet++, a hierarchical neural network for perceptual parsing of 3D surfaces. The authors demonstrate that their approach can handle both synthetic and real-world meshes, achieving highly competitive performance on various benchmarks.\n- **Paper 2** presents the Segment Anything Model (\\sam), which is trained on a large dataset of over 1 billion masks and can generate segmentation masks from various prompts. The authors show that \\sam can be used to solve a wide range of segmentation tasks, including edge detection, object proposal generation, and instance segmentation, under a zero-shot transfer protocol.\n- **Paper 3** introduces a neural network approach to estimate ShDF values for mesh segmentation, using a resolution-agnostic architecture that can handle meshes of varying resolutions. The authors demonstrate that their approach is highly accurate and efficient, particularly on dense meshes, and show its potential to improve traditional graphics workflows.\n\nThese advancements suggest that geometric deep learning is becoming more accessible and efficient for processing 3D data, with significant improvements in both performance and computational efficiency. Future research directions could include further exploration of continuous filter modeling, the development of more efficient mesh processing techniques, and the application of these methods to a wider range of 3D datasets and tasks. Additionally, the integration of these techniques into larger systems and the investigation of their potential in new domains, such as 3D reconstruction and animation, could lead to further breakthroughs in computer graphics and vision."
}