{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\n\nLarge language models have unlocked strong multi-task capabilities from reading instructive prompts.\nHowever, recent studies have shown that existing large models still have difficulty with information extraction tasks. \nFor example, gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset, which is significantly lower than the state-of-the-art performance.\nIn this paper, we propose InstructUIE, a unified information extraction framework based on instruction tuning, which can uniformly model various information extraction tasks and capture the inter-task dependency.\nTo validate the proposed method, we introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction datasets in a unified text-to-text format with expert-written instructions.\nExperimental results demonstrate that our method achieves comparable performance to Bert in supervised settings and significantly outperforms the state-of-the-art and gpt3.5 in zero-shot settings.\n\n\\end{abstract}\n\n\\section{Introduction}\n\nLarge language models (LLMs) \\cite{Brown2020LanguageMA,InstructGPT,GPT4} show tremendous promise in generalization within the set of observed tasks through multi-task training and unified encoding \\cite{mishra-etal-2022-cross,wang-etal-2022-super,Longpre2023TheFC}. \nRecent research has revealed a significant performance gap in LLMs when it comes to information extraction (IE) tasks \\cite{ye2023comprehensive,chen2023robust}. \nFor instance, gpt-3.5-turbo achieves an 18.22 F1 score on the Ontonotes dataset, which is far from satisfactory. \nTherefore, it is necessary to explore how to build a unified information extraction (UIE) model with LLMs.\n\n\\begin{figure}[t]\n\\small\n\\centering\n  \\includegraphics[width=3.0in]{images/comparison.pdf}\n  \\caption{Illustration of 3 different paradigms for solving unified information extraction task.}\n \\label{comparison}\n\\end{figure}\n\nRecently, \\citet{UIE} proposed UIE, which uniformly encodes different extraction structures via a structured extraction language, and captures the common IE abilities via a large-scale pretrained text-to-structure model (shown in Figure \\ref{comparison}a). However, UIE requires separate finetune for different downstream tasks. This lead to the poor performance of UIE in low resource settings or facing new label schema, which greatly restricts the application of UIE in real scenarios. \n\\citet{USM} proposed USM, which decouple IE into two basic tasks, token-token linking to extract label-agnostic substructures, and label-token linking to attach substructures to corresponding semantic concepts (shown in Figure \\ref{comparison}b). However, USM presents two major limitations. Firstly, it converts IE into a semantic matching task, which makes it difficult to integrate with generative language model. Secondly, the method requires semantic matching for each word, which leads to a significant increase in training and inference time.\n\n\\begin{figure*}[t]\n\\centering\n  \\includegraphics[width=6.5in]{images/framework.pdf}\n  \\caption{The overview framework of InstructUIE. The input consists of task instructions, options, and text. The output is a more understandable sentence converted from the original label structures.}\n \\label{fig:framework}\n\\end{figure*}\n\nIn this work, we introduce a unified information extraction framework based on multi-task instruction tuning, named InstructUIE (shown in Figure \\ref{comparison}c). Specifically, we reformulate IE tasks as a natural language generation problem. \nFor the source sentence, we design descriptive instructions to enable the model to understand different tasks and employ an option mechanism including all candidate categories as constraints of output space. \nThen, a pre-trained language model is required to generate the target structure and the corresponding type in the form of natural language. \nWe believe that unrestricted decoding would stimulate the latent knowledge of LLMs to complete IE tasks to a larger extent. \nWe further propose auxiliary tasks, which enable the model to capture common structure information and deepen the understanding of diverse semantics. \nSpecifically, we introduce entity span extraction task and entity typing task for named entity recognition (NER) task, entity pair extraction task and entity pair relationship identification task for relation extraction (RE) task, and trigger extraction task and argument extraction task for event extraction (EE) task. \n\nTo evaluate the effectiveness of the proposed model, we have developed a new benchmark called IE INSTRUCTIONS. The benchmark consists of 32 diverse information extraction datasets that have been unified into a text-to-text format, allowing for a consistent and standardized evaluation of various IE tasks \\footnote{The dataset, code, and models can be found at \nhttps://github.com/BeyonderXX/InstructUIE}. \nBased on the benchmark, we conduct experiments on three main IE tasks under the supervised and zero-shot settings. \n\nOur main contributions are summarized as follows:\n\n\\begin{itemize}[leftmargin=*, align=left]\n    \\item We propose an end-to-end framework for universal information extraction â€“ InstructUIE, which leverages natural language instructions to guide large language models for IE tasks.\n    \\item We introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction datasets in a unified text-to-text format with expert-written instructions. \n    \\item Experimental results demonstrate that InstructUIE achieves comparable performance to Bert in a supervised setup. Notably, our method significantly outperforms the current state-of-the-art and GPT-3.5 in a zero-shot setup.\n\\end{itemize}\n\n\\section{Methodology}\nIn this section, we first briefly introduce the setup of instruction tuning. \nThen, we discuss the task meta-information schema and how IE tasks are mapped into our schema. \nNext, we discuss the framework of InstructUIE, which consists of two major parts: task schema and auxiliary tasks.\nFinally, we explain how IE INSTRUCTION is constructed.\n\n\\subsection{Instruction Tuning Background}\nInstruction tuning is a multi-task learning framework that enables the use of human-readable instructions to guide the output of LLMs. Given a source text and task-specific instructions, the model is trained to generate a sequence of tokens representing the desired output structure and its corresponding labels. \n\nIn a supervised setup, the instructions are provided during training for all tasks, and the model is fine-tuned on a set of labeled data for each task. This allows the model to learn task-specific features and optimize for each task. In a zero-shot setup, the instructions are only provided for a subset of tasks during training, and the model is evaluated on unseen tasks without additional fine-tuning. This requires the model to generalize across tasks and use the shared features learned from the instruction tuning framework to infer the output structures for new tasks.\n\n\\subsection{Framework}\nIn this section, we discuss the task meta-information schema and how IE tasks are mapped into our schema. \nNext, we propose auxiliary tasks, which enable the model to capture common structure information and deepen the understanding of diverse semantics.\n\n\\subsubsection{Task Schema}\\label{task schema}\nTo better transfer and utilize the knowledge learned in pre-trained language models, we reformulate the IE tasks to the seq2seq form and solve it through fine-tuning LLMs, as shown in Figure \\ref{fig:framework}. Every task instance is formatted with four properties: task instruction, options, text, and output.\n\n\\textbf{Task Instruction} provides a detailed guide on how to extract the relevant information from the input text and produce the desired output structure. It includes information such as the type of information to be extracted, the format of the output structure, and any additional constraints or rules that need to be followed during the extraction process. The task instruction acts as a bridge between the raw input text and the structured output representation, enabling the model to understand the extraction task and generate accurate and meaningful output. In Table \\ref{prompts_details} in the Appendix we present the list of instructions for each task.\n\n\\textbf{Options} are the output label constraints for a task, which represent the set of possible outputs that can be generated by the model for a given input. These label constraints are specific to each task and provide information on how to map the predicted outputs to the corresponding semantic concepts. For instance, in NER, options could be entity tags such as person, organization, location, or miscellaneous. Similarly, in RE, options could represent the types of relations that can be extracted, such as \"works for\", \"born in\", \"married to\", and so on. In EE, options could represent the event tags that correspond to different types of events, such as \"beginning\", \"end\", \"occurring\", \"ceasing\", and so on. The options provide a structured output space for the model, allowing it to generate outputs that are consistent with the underlying semantic structure of the task.\n\n\\textbf{Text} is the input sentence of a task instance. This sequence is then fed into the pre-trained language model along with the task instruction and options, enabling the model to generate the desired output sequence for the given task.\n\n\\textbf{Output} is the sentence converted from the original tags of the sample. Specifically, for NER, the output format is \"\\textit{entity tag: entity span}\". For RE, the output format is \"\\textit{relationship: head entity, tail entity}\". For EE, the output format is \"\\textit{event tag: trigger word, argument tag: argument span}\". In cases where the input does not contain structural information that matches any of the provided options, we assign a value of \"\\textit{None}\" to the corresponding output sentence.\n\n\\subsubsection{Auxiliary Tasks}\nTo boost the performance in a more fine-grained level, we further design auxiliary tasks to be trained in conjunction with the main task. The auxiliary tasks provide additional information that complements the main task, enabling the model to capture common structures better and deepen the understanding of diverse semantics.\n\nFor the named entity recognition task, we introduce a span extraction task and an entity typing task. The span extraction task is designed to extract the entity span from the input sentence, while the entity typing task is aimed at identifying the type of entity.\n\nFor the relation extraction task, we have introduced an entity pair extraction task and a relation classification task. The entity pair extraction task aims to extract the entity pairs involved in the relationship, while the relation classification task is designed to classify the type of relationship between the entity pairs.\n\nFor the event extraction task, we have introduced a trigger extraction task and an argument extraction task. The trigger extraction task is designed to extract the trigger word that triggers the event, while the argument extraction task aims to extract the associated arguments.\n\n\\subsection{IE INSTRUCTIONS}\nIE INSTRUCTIONS collects 32 publicly available datasets covering three types of IE tasks: NER, RE, and EE. To ensure the diversity of the datasets, we include corpora from various domains, such as science, healthcare, social media, and transportation, in addition to general-domain sources, such as news and Wikidata. Figure \\ref{benchmark_sun} shows the breakdown of the benchmark by task, domain, and size. For detailed dataset statistics and train/test split methods, please refer to Appendix Table \\ref{dataset-details}.\n\nWe carry out the following data processing steps: (1) To address the issue of inconsistent label schemas across different tasks, we unify the names of labels with identical semantics but different names in various datasets. (2) To better test the semantic understanding capabilities of the LLM, we convert labels with underscores, abbreviations, or special formats into natural language formats. For example, we renamed the label \"people person place\\_of\\_birth\" to \"place of birth.\" (3) Following the guidelines outlined in the section \\ref{task schema}, we transform all datasets into a text-to-text format, which ensures a consistent representation of the input-output pairs across all tasks.\n\nOur benchmark provides a standardized evaluation platform for LLMs' performance on IE tasks. This will facilitate a more accurate comparison of various models and contribute to the development of more effective and robust models for IE tasks.\n\n\\begin{figure}[t]\n\\small\n\\centering\n  \\includegraphics[width=3.0in]{images/sun.png}\n  \\caption{Overview of IE INSTRUCTIONS.}\n \\label{benchmark_sun}\n\\end{figure}\n\n\\section{Experiments}\nThis section conducted extensive experiments under supervised and zero-shot settings to validate the effectiveness of InstructUIE. \nWe select 11B FlanT5 \\cite{chung2022scaling} as our backbone model because prior research \\cite{Longpre2023TheFC} has demonstrated that models fine-tuned on instruction-based tasks offer a computationally efficient starting point for new tasks.\nThe details of the experimental setup, datasets, and comparison methods are described in the following parts.\n\n\\begin{table}[t]\n    \\centering\n    \\adjustbox{max width=\\columnwidth}{\n    \\begin{tabular}{c|ccc|c}\n    \\toprule  \n    Dataset & UIE & USM & Bert-base & Ours \\\\\n    \\midrule\n    ACE2005 & 85.78 & 87.14 & \\textbf{87.30} & 86.66 \\\\\n    AnatEM & - & - & 85.82 & \\textbf{90.89} \\\\\n    bc2gm & - & - & 80.90 & \\textbf{85.16} \\\\\n    bc4chemd & - & - & 86.72 & \\textbf{90.30} \\\\\n    bc5cdr & - & - & 85.28 & \\textbf{89.59} \\\\\n    broad twitter & - & - & 58.61 & \\textbf{83.14} \\\\\n    CoNLL2003 & 92.99 & \\textbf{93.16} & 92.40 & 92.94 \\\\\n    FabNER & - & - & 64.20 & \\textbf{76.20} \\\\\n    FindVehicle & - & - & 87.13 & \\textbf{89.47} \\\\\n    GENIA-Ent & - & - & 73.3 & \\textbf{74.71} \\\\\n    HarveyNER & - & - & 82.26 & \\textbf{88.79} \\\\\n    MIT Movie & - & -& 88.78 & \\textbf{89.01} \\\\\n    MIT Restaurant & - & - & 81.02 & \\textbf{82.55} \\\\\n    multiNERD & - & - & 91.25 & \\textbf{92.32} \\\\\n    ncbi-disease & - & - & 80.20 & \\textbf{90.23} \\\\\n    Ontonotes & - & - & \\textbf{91.11} & 90.19 \\\\\n    polyglot-NER & - & - & \\textbf{75.65} & 70.15 \\\\\n    tweetNER7 & - & - & 56.49 & \\textbf{64.97} \\\\\n    wikiann & - & - & 70.60 & \\textbf{85.13} \\\\\n    wikineural & - & - & 82.78 & \\textbf{91.36} \\\\\n    Avg & - & - & 80.09 & \\textbf{85.19} \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\caption{\nOverall results of InstructUIE on NER task. The evaluation metric is Entity F1. For 20 NER datasets, InstructUIE outperforms the Bert model on 17 of them.\n}\n\\label{supervised-result-NER}\n\\end{table}\n\n\\subsection{Experiments on Supervised Settings}\n\\subsubsection{Dataset}\nWe conduct supervised experiments on IE INSTRUCTIONS, including three tasks (named entity extraction, relation extraction, and event extraction). \nDetails of the dataset splitting methods and statistics can be found in Appendix \\ref{data details}.\n\nTo balance the dataset, we apply a sampling strategy \\cite{poolsawad2014balancing}. Specifically, we sample 10,000 examples for each dataset and include all examples for datasets with fewer than 10,000 samples.\n\n\\subsubsection{Baselines}\n\\label{supervise-baseline}\nWe compare the proposed InstructUIE with the following strong baseline models:\n\\begin{itemize}\n\\item \\textbf{UIE} \\cite{UIE} is a unified text-to-structure generation framework that can universally model different IE tasks and adaptively generate targeted structures;\n\\item \\textbf{USM} \\cite{USM} is a unified IE tasks framework, which converts IE tasks to a semantic matching problem;\n\\item \\textbf{Bert} \\cite{devlin-etal-2019-bert}, which are widely used as text encoders for various tasks.\n\\end{itemize}\n\n\\subsubsection{Evaluation Metrics}\nWe use span-based offset Micro-F1 as the primary metric to evaluate the model. For NER task, we follow a span-level evaluation setting, where the entity boundary and entity type must be correctly predicted. For RE task, a relation triple is correct if the model correctly predicts the boundaries of the subject entity, the object entity, and the entity relation. For EE task, we report two evaluation metrics: (1) Event Trigger: an event trigger is correct if the event type and the trigger word are correctly predicted. (2) Event Argument: an event argument is correct if its role type and event type match a reference argument mention.\n\n\\subsubsection{Results}\n\n\\begin{table}[t]\n    \\centering\n    \\adjustbox{max width=\\columnwidth}{\n    \\begin{tabular}{c|ccc}\n    \\toprule  \n    Dataset & UIE & USM & Ours \\\\\n    \\midrule\n    ADE corpus & - & -  & \\textbf{82.31} \\\\\n    CoNLL2004 & 75.00 & \\textbf{78.84}  & 78.48 \\\\\n    GIDS & - & -  & \\textbf{81.98} \\\\\n    kbp37 & - & - & \\textbf{36.14} \\\\\n    NYT & - & - & \\textbf{90.47} \\\\\n    NYT11 HRL & - & - & \\textbf{56.06} \\\\\n    SciERC & 36.53 & 37.36 & \\textbf{45.15} \\\\\n    semeval RE & - & - & \\textbf{73.23} \\\\\n    Avg & - & - & \\textbf{67.98} \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\caption{Overall results of InstructUIE on RE task. The evaluation metric is Relation Strict F1. Our model reaches an average F1 of 67.98\\% on the eight datasets of the RE task and is comparable to the baseline.}\n\\label{supervised-result-RE}\n\\end{table}\n\n\\begin{table}[t]\n    \\centering\n    \\begin{subtable}\n    \\centering\n    \\begin{tabular}{c|ccc|c}\n    \\toprule  \n    Dataset & UIE & USM & Bert-base & Ours \\\\\n    \\midrule\n    ACE2005 & 73.36 & 72.41 & 72.5 & \\textbf{77.13} \\\\\n    CASIE & 69.33 & \\textbf{71.73} & 68.98 & 67.80 \\\\\n    PHEE & - & - & - & \\textbf{70.14} \\\\\n    Avg & - & - & - & \\textbf{71.69} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption*{a. Event Trigger F1}\n    \\label{subtable1}\n    \\end{subtable}\n    \\hspace{0.1cm}\n    \\begin{subtable}\n    \\centering\n    \\begin{tabular}{c|ccc|c}\n    \\toprule  \n    Dataset & UIE & USM & Bert-base & Ours \\\\\n    \\midrule\n    ACE2005 & 54.79 & 55.83 & 59.9 & \\textbf{72.94} \\\\\n    CASIE & 61.30 & 63.26 & 60.37 & \\textbf{63.53} \\\\\n    PHEE & - & - & - & \\textbf{62.91} \\\\\n    Avg & - & - & - & \\textbf{66.46} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption*{b. Event Argument F1}\n    \\label{subtable2}\n    \\end{subtable}\n    \\caption{Overall results of InstructUIE on EE task. The evaluation metric is Event Trigger F1 and Event Argument F1. Our model outperformed USM and UIE on some datasets.}\n    \\label{supervised-result-EE}\n\\end{table}\n\nTabel \\ref{supervised-result-NER}, Tabel \\ref{supervised-result-RE} and \\ref{supervised-result-EE} show the performance of different models for the NER, RE, and EE tasks. \n\\paragraph{Named Entity Recognition} \nOur model achieves an average F1 score of 85.19\\% on 20 NER datasets, surpassing Bert's 80.09\\%. The best performance is on the CoNLL2003 dataset, where InstructUIE achieved an F1 score of 92.94\\%. For 20 NER data sets, InstructUIE outperforms the Bert model on 17 of them. Among them, our model outperforms Bert by more than 5 points on eight datasets. The dataset with the biggest gap is the broad twitter dataset, where InstructUIE outperforms Bert by about 25 points.\n\nIn the ACE2005, Ontonotes, and Polyglot-NER datasets, our model performs slightly worse than Bert. We speculate that this is due to our strategy of sampling only 10,000 training examples for each dataset. The original corpora for these three datasets contain a larger number of training examples, such as 420,000 for Polyglot-NER, of which we only used around 20\\%. The detailed number of training sets for all datasets can be seen in the appendix.\n\nCompared with UIE and USM, our model also achieves comparable results on ACE2005 and CoNLL2003, which are two commonly used datasets. Due to the UIE and USM only test their modelson a small number of commonly used datasets, we are unable to compare our model with these two models on other datasets.\n\n\\begin{table*}[htbp]\n    \\centering\n    \\begin{tabular}{c|ccccccc}\n    \\toprule\n        Model & Movie & Restaurant & AI & Literature & Music & Politics & Science \\\\\n        \\midrule\n        USM & 37.73 & 14.73 & 28.18 & \\textbf{56.00} & 44.93 & 36.10 & 44.09 \\\\\n        InstructUIE & \\textbf{63.00} & \\textbf{20.99} & \\textbf{49.00} & 47.21 & \\textbf{53.16} & \\textbf{48.15} & \\textbf{49.30} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\n    Micro-F1 scores of zero-shot NER on 7 datasets. The best results are in bold. InstructUIE outperforms SOTA by a wide margin on most datasets ranging from 5.21\\% to 25.27\\%.}\n    \\label{zero-shot-NER}\n\\end{table*}\n\n\\begin{table*}[t]\n    \\centering\n    \\begin{tabular}{c|ccc}\n    \\toprule\n         & Model & FewRel & Wiki-ZSL \\\\\n        \\midrule\n        \\multirow{2}*{Baselines} & $ZETT_{T5-small}$ & 30.53 & 31.74 \\\\\n        ~ & $ZETT_{T5-base}$ & 33.71 & 31.17 \\\\\n        \\midrule\n        Ours & InstructUIE & \\textbf{39.55} & \\textbf{ 35.20} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\n    Micro-F1 scores of zero-shot RE on FewRel and Wiki-ZSL. The best results are in bold. InstructUIE outperforms SOTA on both datasets.}\n    \\label{zero-shot-RE}\n\\end{table*}\n\n\\paragraph{Relational Extraction}\nOur model reaches an average F1 of 67.98\\% on the eight datasets of the RE task, among which the NYT data set reaches 90.47\\% F1 score. Among the eight datasets, CoNLL2004 and SciERC datasets are also tested by UIE and USM models. We focus on the analysis of the results of these two datasets. For the SciERC dataset, InstructUIE significantly outperforms UIE and USM by 8.62\\% and 7.79\\% respectively. For the CoNLL2004 dataset, InstructUIE outperforms UIE by more than three points, and lag USM by less than 0.5\\%.\nMoreover, noted that as BERT is usually used for relation classification tasks rather than relation extraction. Therefore, we did not use this baseline in the RE task.\n\n\\paragraph{Event Extraction}\nOur model achieve sota on all datasets except for the Event Trigger F1 metric of the CASIE dataset. On the Event Trigger F1 metric, InstructUIE reaches an average of 71.69\\% on these three datasets, with ACE2005 reaching 77.13\\%, significantly surpassing UIE's 73.36\\%,  USM's 72.41\\% and Bert's 72.5\\%. On the Event Argument F1 metric, InstructUIE beats three baseline models to reach sota on all three datasets. In particular, ACE2005 dataset reaches 72.94\\%, 18 points higher than the UIE and 17 points higher than the USM.\n\n\\subsection{Experiments on Zero-shot Settings}\n\n\\subsubsection{Dataset}\nTo evaluate InstructUIE's zero-shot performance, we train the model on 18 NER datasets and 6 RE datasets and test it on 7 NER datasets and 2 RE datasets. Specifically, we eliminate the datasets for zero-shot experimental testing during the training phase. \nFor the NER task, We use five CrossNER subsets(AI, literature, music, politics, science) \\cite{CrossNERDATASET}, MIT Movie Review, and MIT Restaurant Review \\cite{MITReviewDataset} to test the zero-shot capability of the model. \nFor RE task, we test the zero-shot capability on FewRel \\cite{FewRelDATASET} and Wiki-ZSL \\cite{Wiki-ZSLDATASET}. For FewRel and Wiki-ZSL data sets, we follow the previous work \\cite{relationprompt} and randomly select 5 unseen labels which do not appear in the training set as the test set. In order to reduce the effect of experimental noise, the unseen label selection process is repeated for five different random seeds to produce the test set.\n\nSince the training and testing tasks do not overlap at all and across various domains as well, this setting is challenging.\n\n\\subsubsection{Baselines}\nFor zero-shot Named Entity Recognition and Relational Extraction, we compare InstructUIE with the following strong baselines:\n\\begin{itemize}\n\\item \\textbf{ZETT}\\cite{zett} is a novel framework based on end-to-end generative transformers and outperform previous state-of-the-art models;\n\\item \\textbf{ChatGPT} \\cite{InstructGPT} is also called GPT-3.5-turbo, which is the most capable GPT-3.5 model and optimized for chat;\n\\item \\textbf{UIE} and \\textbf{USM} have been introduced in \\ref{supervise-baseline}.\n\\end{itemize}\n\n\\subsubsection{Results}\n\n\\begin{table*}[htbp]\n    \\centering\n    \\begin{tabular}{c|ccccccc|cc}\n    \\toprule\n        Model & Movie & Restaurant & AI & Literature & Music & Politics & Science & FewRel & Wiki-ZSL \\\\\n        \\midrule\n        davinci & 0.84 & 2.94 & 2.97 & 9.87 & 13.83 & 18.42 & 10.04 & 0.00 & 0.00 \\\\\n        chatgpt & \\textbf{41.00} & \\textbf{37.76} & \\textbf{54.40} & \\textbf{54.07} & \\textbf{61.24} & \\textbf{59.12} & \\textbf{63.00} & \\textbf{9.96} & \\textbf{13.14} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\\label{zero-shot-GPT}\n    Micro-F1 scores of davinci and chatgpt under zero-shot setting.}\n\\end{table*}\n\nTable \\ref{zero-shot-NER} and Table \\ref{zero-shot-RE} show the performance of NER and RE tasks under the zero-shot setting. For the NER task, we can observe that InstructUIE outperforms the current sota model USM in Micro-F1 score on all the datasets except CrossNER\\_Literature, ranging from 5.21\\% to 25.27\\%. For example, compared with the USM model, InstructUIE performs over 20 points better on the MIT Movie Review dataset and the CrossNER\\_AI dataset. Noted that USM is trained on the same task corpus and tested on the label held out, while our model has never seen the task corpus. For the RE task, under the setting of 5 unseen labels, InstructUIE outperforms the current sota model ZETT on both the FewRel and Wiki-ZSL datasets by 5.84\\% and 3.46\\% respectively.\n\nWhen compared to the GPT series model, InstructUIE significantly outperforms Davinci for the NER task but still falls some way short of Chatgpt's results for the NER task. However, for the RE task, our model performs much better than these two GPT series models. Both Davinci and Chatgpt perform poorly, especially with Davinci completely unable to output correct results.\n\nIt is worth mentioning that since Chatgpt is not open source, we have no way of knowing whether the model has seen the two data sets used by the zero-shot setting during training, and we think the huge difference in results for NER and RE tasks may be due to this reason.\n\n\\section{Related Work}\n\n\\subsection{Instruction Tuning}\nInstruction tuning \\cite{mishra-etal-2022-cross, wang-etal-2022-super, Longpre2023TheFC}, a novel paradigm that leverages natural language instructions to guide large language models for downstream tasks, shows tremendous promise in generalization within the set of observed tasks.\nMost recent work \\cite{wang-etal-2022-super,Longpre2023TheFC} on instruction tuning has focused on general NLP tasks such as question answering and text classification, but not specifically on IE tasks.\nWhile some work such as \\cite{Wang2022InstructionNERAM, parmar-etal-2022-boxbart} includes a few IE tasks, those tasks do not provide good coverage of IE tasks and domains. \nNo prior work has examined how training a model on a wide range of IE tasks with various instructions.\nIn this paper, we propose a unified framework for information extraction that involves auxiliary task design as well as specific tuning methods.\n\n\\subsection{Information Extraction}\nInformation extraction is fundamental in natural language processing systems, aiming to extract structured information from unstructured or semi-structured data sources automatically. \nTraditional methods \\cite{wang-etal-2022-miner,yan-etal-2021-unified,CoNLL2004SOTA,NYT11HRLSOTA} for IE typically require the design of specific architectures for different IE tasks, and the models are trained separately. However, training dedicated models for different IE tasks requires a significant amount of labeled data, which can be costly and time-consuming to obtain. Secondly, knowledge learned from one IE task cannot be easily applied to another task, even if the tasks have similar characteristics. \nRecently, \\citet{UIE} proposed UIE, which uniformly encodes different extraction structures via a structured extraction language and captures the common IE abilities via a large-scale pre-trained text-to-structure model. However, UIE requires separate finetune for different downstream tasks. This lead to the poor performance of UIE in low resource settings or facing new label schema. \n\\citet{USM} proposed USM, which decouples IE into two basic tasks, token-token linking and label-token linking.  Unfortunately, USM requires semantic matching for each word, which leads to a significant increase in training and inference time.\nInstructUIE addresses these challenges by utilizing instructive guidance to direct pre-trained large models toward the task, facilitating the efficient and adaptive generation of target structures.\n\n\\section{Conclusion}\nIn this paper, we propose an end-to-end framework for universal information extraction â€“ InstructUIE, which leverages natural language instructions to guide large language models for IE tasks. \nWe further introduce a new benchmark dataset. The benchmark consists of 32 diverse information extraction datasets that have been unified into a text-to-text format, allowing for a consistent and standardized evaluation of various IE tasks.\nExperimental results demonstrate that InstructUIE achieves state-of-the-art results under supervised and zero settings and solves massive tasks using a single multi-task model. \n\n\\section{Appendix}\n\\label{sec:appendix}\n\\subsection{Data Details}\n\\label{data details}\nIE INSTRUCTIONS collects 32 publicly available datasets covering three IE tasks: NER, RE, and EE.\nFor NER(named entity extraction) task, the 21 used datasets includes ACE2004, ACE2005\\cite{ACE2005DATASET}, broad\\_twitter\\_corpus\\cite{broad_twitter_corpusDATASET}, CoNLL2003\\cite{CoNLL03Dataset}, multiNERD\\cite{multiNERDDATASET}, Ontonotes\\cite{OntoNotesDataset}, polyglot-NER\\cite{polyglot-NERDATASET}, tweetNER7\\cite{tweetNER7DATASET}, wikiann\\cite{wikiannDataset}, wikineural\\cite{wikineuralDATASET}, AnatEM\\cite{AnatEM}, bc2gm\\cite{Kocaman2020BiomedicalNE}, bc4chemd\\cite{bc4chemdDATASET}, bc5cdr\\cite{Li2016BioCreativeVC}, FabNER\\cite{Kumar2021FabNERIE}, FindVehicle\\cite{FindVehicle}, GENIA\\cite{GENIANERDATASET}, HarveyNER\\cite{HarveyNERDATASET}, MIT Movie Review\\cite{MITReviewDataset}, MIT Restaurant Review\\cite{MITReviewDataset} and ncbi-disease\\cite{ncbi-diseaseDATASET}. For RE(relation extraction) task, we use 10 datasets including ADE corpus\\cite{ADEcorpusDATASET}, CoNLL2004\\cite{Roth2004ALP}, GIDS\\cite{Jat2018ImprovingDS}, kbp37\\cite{kbp37DATASET}, NYT\\cite{Riedel2010ModelingRA}, NYT11 HRL\\cite{Takanobu2018AHF}, SciERC\\cite{SciERCDATASET}, semeval RE\\cite{Hendrickx2010SemEval2010T8}, FewRel\\cite{FewRelDATASET} and Wiki-ZSL\\cite{Wiki-ZSLDATASET}. For task EE(event extraction), ACE2005\\cite{ACE2005DATASET}, CASIE\\cite{Lu2021Text2EventCS}, GENIA\\cite{Kim2003GENIAC} and PHEE\\cite{Sun2022PHEEAD} are used. \n\nFor the data set with only training set as the original data, we divided it into training set, validation set and test set according to the ratio of 8:1:1. For the data set with only training set and validation set as the original data, we randomly select half of the data in the validation set as the test set and the other half as the new validation set. For other datasets, we adopt the official split.\n\nTabel \\ref{dataset-details} shows detailed datasets statistics. NER refers to Named Entity Recognition task, RE refers to Relation Extraction task, and EE refers to Event Extraction task. |Labels| indicates the number of labels, and $\\#$ is the number of sentences in the specific subset.\n For the |Labels| of event extraction, the number outside the parenthesis indicates the number of event types and the number inside the parenthesis indicates the number of argument types.\n\\begin{table*}[htbp]\n    \\centering\n    \\begin{tabular}{c|c|c|ccc}\n    \\toprule  \n    Task & Dataset & |labels| & \\#Train & \\#Val & \\#Test \\\\\n    \\midrule\n    \\multirow{26}*{NER} & ACE2004 & 7 & 6202 & 745 & 812 \\\\\n    ~ & ACE2005 & 7 & 7299 & 971 & 1060 \\\\\n    ~ & broad\\_twitter\\_corpus & 3 & 5334 & 2000 & 2001 \\\\\n    ~ & CoNLL2003 & 4 & 14041 & 3250 & 3453 \\\\\n    ~ & multiNERD & 16 & 134144 & 10000 & 10000 \\\\\n    ~ & Ontonotes & 18 & 59924 & 8528 & 8262\\\\\n    ~ & polyglot-NER & 3 & 393982 & 10000 & 10000 \\\\\n    ~ & tweetNER7 & 7 & 7111 & 886 & 576 \\\\\n    ~ & wikiann & 3 & 20000 & 10000 & 10000 \\\\\n    ~ & wikineural & 3 & 92720 & 11590 & 11597 \\\\\n    ~ & AnatEM & 1 & 5861 & 2118 & 3830 \\\\\n    ~ & bc2gm & 1 & 12500 & 2500 & 5000 \\\\\n    ~ & bc4chemd & 1 & 30682 & 30639 & 26364 \\\\\n    ~ & bc5cd & 2 & 4560 & 4581 & 4797 \\\\\n    ~ & CrossNER\\_AI & 14 & 100 & 350 & 431 \\\\\n    ~ & CrossNER\\_literature & 12 & 100 & 400 & 416 \\\\\n    ~ & CrossNER\\_music & 13 & 100 & 380 & 465 \\\\\n    ~ & CrossNER\\_politics & 9 & 199 & 540 & 650 \\\\\n    ~ & CrossNER\\_science & 17 & 200 & 450 & 543 \\\\\n    ~ & FabNER & 12 & 9435 & 2182 & 2064 \\\\\n    ~ & FindVehicle & 21 & 21565 & 20777 & 20777 \\\\%å’‹è¿™ä¹ˆå¤š\n    ~ & GENIA & 5 & 15023 & 1669 & 1854 \\\\\n    ~ & HarveyNER & 4 & 3967 & 1301 & 1303 \\\\\n    ~ & MIT Movie Review & 12 & 9774 & 2442 & 2442 \\\\\n    ~ & MIT Restaurant Review & 8 & 7659 & 1520 & 1520 \\\\\n    ~ & ncbi-disease & 1 & 5432 & 923 & 940 \\\\\n    \\midrule\n    \\multirow{10}*{RE} & ADE corpus & 1 & 3417 & 427 & 428 \\\\\n    ~ & CoNLL2004 & 5 & 922 & 231 & 288 \\\\\n    ~ & GIDS & 4 & 8526 & 1417 & 4307 \\\\\n    ~ & kbp37 & 18 & 15917 & 1724 & 3405 \\\\\n    ~ & NYT & 24 & 56196 & 5000 & 5000 \\\\\n    ~ & NYT11 HRL & 12 & 62648 & 149 & 369 \\\\\n    ~ & SciERC & 7 & 1366 & 187 & 397 \\\\\n    ~ & semeval RE & 10 & 6507 & 1493 & 2717 \\\\\n    \\midrule\n    \\multirow{4}*{EE} & ACE2005 & 33(22) & 3342 & 327 & 293 \\\\\n    ~ & CASIE & 5(26) & 3751 & 788 & 1500 \\\\\n    ~ & GENIA & 5(0) & 15023 & 1669 & 1854 \\\\\n    ~ & PHEE & 2(16) & 2898 & 961 & 968 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\\label{dataset-details}\nDetailed datasets statistics.}\n\\end{table*}\n\n\\subsection{Instruction Details}\nTable \\ref{prompts_details} shows prompts for different tasks. NER refers to the named entity recognition task, the object of which is the entity in the output sentence and its corresponding entity type. RE refers to the relation extraction task, the object of which is to extract the relation triplet in the sentence, including the relation name, the head entity and the tail entity. EE refers to the event extraction task. The task objective is to extract the event types, trigger word and arguments in the sentence. ES refers to entity span, the task target is given sentence and entity category options, and output entities that conform to the entity category, but there is no need to output the entity type of each entity; ET refers to entity type identification. The task target is a given sentence, which contains entity and entity category options, and outputs the entity category corresponding to each entity. EP refers to entity pair identification (entity pair). The task target is given sentence and relation category options, and output entity pairs that conform to relation category, but do not need to output its relation category; EPR refers to entity pair relationship identification. The task target is a given sentence, which contains entity pair and relationship category options, and outputs the corresponding relationship category for each entity pair. ES and ET are auxiliary tasks of NER, EP and EPR are auxiliary tasks of RE, and EEA and EET are auxiliary tasks of EE.\n\n\\begin{table*}[htbp]\n    \\centering\n    \\begin{tabular}{m{0.1\\linewidth}m{0.85\\linewidth}}\n    \\toprule    \n        \\textbf{Task} & \\textbf{{Prompts}} \\\\ \\midrule\n        \\multirow{7}*{NER} & Please list all entity words in the text that fit the category. Output format is \"type1: word1; type2: word2\". \\\\ \\\\\n        ~ & Please find all the entity words associated with the category in the given text. Output format is \"type1: word1; type2: word2\". \\\\ \\\\\n        ~ & Please tell me all the entity words in the text that belong to a given category. Output format is \"type1: word1; type2: word2\". \\\\ \n        \n        \\midrule\n        \\multirow{8}*{RE} & Given a phrase that describes the relationship between two words, extract the words and the lexical relationship between them. The output format should be \"relation1: word1, word2; relation2: word3, word4\". \\\\ \\\\\n        ~ & Find the phrases in the following sentences that have a given relationship. The output format is \"relation1: word1, word2; relation2: word3, word4\". \\\\ \\\\\n        ~ & Given a sentence, please extract the subject and object containing a certain relation in the sentence according to the following relation types, in the format of \"relation1: word1, word2; relation2: word3, word4\". \\\\ \n        \n        \\midrule\n        \\multirow{3}*{EE} & Locate the role in the text that participated in the event based on the event type and return it in the event list. \\\\ \\\\\n        ~ & Extract the event information in the text and return them in the event list. \\\\ \n        \n        \\midrule\n        \\multirow{1}*{ES} & Please list all entity words in the text that fit the category. Output format is word1, word2. \\\\ \n        \n        \\midrule\n        \\multirow{1}*{ET} & Given options, please tell me the categories of all the listed entity words.Output format is \"type1: word1; type2: word2\". \\\\ \n        \n        \\midrule\n        \\multirow{1}*{EP} & Please list all entity pairs containing a certain relationship in the given options.Output format is \"word1, word2; word3, word4\". \\\\ \n        \n        \\midrule\n        \\multirow{1}*{EPR} & Given options, please tell me the relationships of all the listed entity pairs.Output format is \"relation1: word1, word2; relation2: word3, word4\". \\\\ \n        \n        \\midrule\n        \\multirow{1}*{EEA} & Given event type and trigger, please tell me the arguments of all the listed option. Output format is \"name: role\". \\\\ \n        \n        \\midrule\n        \\multirow{1}*{EET} & Please tell me event type and its trigger word from given type options. Output format is \"event type: trigger\". \\\\ \\midrule\n    \\end{tabular}\n    \\caption{\\label{prompts_details}\n    Instructions for different tasks.\n    }\n\\end{table*}\n\n\\begin{table*}[htbp]\n    \\centering\n    \\resizebox{\\textwidth}{!}{\n    \\begin{tabular}{c|c|cc}\n    \\toprule  \n    Dataset & Metric & \\multicolumn{2}{c}{Task-specific SOTA Methods} \\\\\n    \\midrule\n    ACE2004 & Entity F1 & \\citet{ACE0405SOTA} & 90.3 \\\\\n    ACE2005-Ent & Entity F1 & \\citet{ACE0405SOTA} & 90.9 \\\\\n    AnatEM & Entity F1 & \\citet{AnatEMSOTA} & 91.65 \\\\\n    bc2gm & Entity F1 & \\citet{bc2gmSOTA} & 88.75 \\\\\n    bc4chemd & Entity F1 & \\citet{bc4chemdSOTA} & 94.39 \\\\\n    bc5cdr & Entity F1 & \\citet{bc5cdrSOTA} & 91.9 \\\\\n    broad\\_twitter\\_corpus & Entity F1 & \\citet{broad_twitter_corpusSOTA} & 74.70 \\\\\n    CoNLL2003 & Entity F1 & \\citet{CoNLL2003SOTA} & 94.60\\\\\n    FabNER & Entity F1 & \\citet{FabNERSOTA} & 88 \\\\\n    FindVehicle & Entity F1 & \\citet{FindVehicleSOTA} & 80.9 \\\\\n    GENIA-Ent & Entity F1 & \\citet{GENIANERSOTA} & 80.80 \\\\\n    HarveyNER & Entity F1 & \\citet{HarveyNERSOTA} & 68.97 \\\\\n    MIT Movie Review & Entity F1 & \\citet{MITMovieSOTA} & 87.31 \\\\\n    MIT Restaurant Review & Entity F1 & \\citet{MITRestaurantSOTA} & 79.6 \\\\\n    multiNERD & Entity F1 & \\citet{multiNERDSOTA} & 85.0 \\\\\n    ncbi-disease & Entity F1 & \\citet{ncbiSOTA} & 90.48 \\\\\n    Ontonotes & Entity F1 & \\citet{OntonotesSOTA} & 92.07 \\\\\n    polyglot-NER & Entity F1 & - \\\\\n    tweetNER7 & Entity F1 & \\citet{tweetNER7SOTA} & 66 \\\\\n    wikiann & Entity F1 & \\citet{wikiannSOTA} & 91.8 \\\\\n    wikineural & Entity F1 & -  \\\\\n    ADE corpus & Relation Strict F1 & \\citet{ADEcorpusSOTA} & 83.9 \\\\\n    CoNLL2004 & Relation Strict F1 & \\citet{CoNLL2004SOTA} & 76.65 \\\\\n    GIDS & Relation Strict F1 & - \\\\\n    kbp37 & Relation Strict F1 & - \\\\\n    NYT & Relation Strict F1 & - \\\\\n    NYT11 HRL & Relation Strict F1 & \\citet{NYT11HRLSOTA} & 55.47 \\\\\n    SciERC & Relation Strict F1 & \\citet{SciERCSOTA} & 38.40 \\\\ \n    semeval RE & Relation Strict F1 & \\\n    \\citet{semevalRESOTA} & 76.00 \\\\\n    ACE2005 & Event Trigger F1 & - & -  \\\\\n    ACE2005 & Event Argument F1 & - & - \\\\\n    CASIE & Event Trigger F1 & - & - \\\\\n    CASIE & Event Argument F1 & - & - \\\\\n    GENIA-Evt & Event Trigger F1 & - & 63.96 \\\\\n    GENIA-Evt & Event Argument F1 & - & - \\\\\n    PHEE & Event Trigger F1 & - & - \\\\\n    PHEE & Event Argument F1 & - & - \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n    \\caption{\nOverall results of InstructUIE on different datasets. InstructUIE perform better or comparable than Bert on popular NER datasets like ACE2005, CoNLL2003, Ontonotes, and tweetNER7. In the RE task, InstructUIE achieved results comparable to the baseline on most datasets. In the EE task, our model outperformed USM, UIE or SOTA on some datasets.\n}\n\\label{supervised-result-withSOTA}\n\\end{table*}\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{UniversalNER: targeted distillation from large language models for open named entity recognition}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nLarge language models (LLMs) have demonstrated remarkable generalizability, such as understanding arbitrary entities and relations. \nInstruction tuning has proven effective for distilling LLMs into more cost-efficient models such as Alpaca and Vicuna. Yet such student models still trail the original LLMs by large margins in downstream applications. \nIn this paper, we explore {\\it targeted distillation} with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction.\nUsing named entity recognition (NER) for case study, we show how ChatGPT can be distilled into much smaller \\longname models for open NER. \nFor evaluation, we assemble the largest NER benchmark to date, comprising 43 datasets across 9 diverse domains such as biomedicine, programming, social media, law, finance.\nWithout using any direct supervision, \\longname attains remarkable NER accuracy across tens of thousands of entity types, outperforming general instruction-tuned models such as Alpaca and Vicuna by over 30 absolute F1 points in average. \nWith a tiny fraction of parameters, \\longname not only acquires ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute F1 points in average. \nRemarkably, \\longname even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as InstructUIE, which uses supervised NER examples.\nWe also conduct thorough ablation studies to assess the impact of various components in our distillation approach. \nWe release the distillation recipe, data, and \\longname models to facilitate future research on targeted distillation.\\footnote{Project page: \\url{https://universal-ner.github.io/}}\n\\end{abstract}\n\n\\section{Introduction}\n\nLarge language models (LLMs) such as ChatGPT~\\citep{ouyang2022training,openai2023gpt4} have demonstrated remarkable generalization capabilities, but they generally require prohibitive cost in training and inference. Moreover, in mission-critical applications such as biomedicine, white-box access to model weights and inference probabilities are often important for explainability and trust. Consequently, instruction-tuning has become a popular approach for distilling LLMs into more cost-efficient and transparent student models. %A general recipe is to finetune pretrained open-source LLMs such as LLaMA~\\cite{} using instruction-following data generated by ChatGPT. \nSuch student models, as exemplified by Alpaca~\\citep{alpaca} and Vicuna~\\citep{vicuna-2023}, have demonstrated compelling capabilities in imitating ChatGPT. However, upon close inspection, they still trail the teacher LLM by a large margin, especially in targeted downstream applications~\\citep{gudibande2023false}. Bounded by limited compute, it is unsurprising that generic distillation can only produce a shallow approximation of the original LLM across all possible applications.\n\nIn this paper, we instead explore {\\it targeted distillation} where we train student models using mission-focused instruction tuning for a broad application class such as open information extraction~\\citep{etzioni2008open}.\nWe show that this can maximally replicate LLM's capabilities for the given application class, while preserving its generalizability across semantic types and domains. \nWe choose named entity recognition (NER) for our case study, as it is one of the most fundamental tasks in natural language processing~\\citep{wu2017clinical,perera2020named}.\nRecent studies~\\citep{wei2023zero,li2023evaluating} show that when there are abundant annotated examples for an entity type, LLMs still fall behind the state-of-the-art supervised system for that entity type. \nHowever, for the vast majority of entity types, there is little annotated data. New entity types constantly emerge, and it is expensive and time-consuming to generate annotated examples, especially in high-value domains such as biomedicine where specialized expertise is required for annotation. \nTrained on pre-specified entity types and domains, supervised NER models also exhibit limited generalizability for new domains and entity types.\n\nWe present a general recipe for targeted distillation from LLMs and demonstrate that for open-domain NER. \nWe show how to use ChatGPT to generate instruction-tuning data for NER from broad-coverage unlabeled web text, and conduct instruction-tuning on LLaMA~\\citep{touvron2023llama} to distill the \\longname models (\\shortname in short).\n\nTo facilitate a thorough evaluation, we assemble the largest and most diverse NER benchmark to date (\\longname benchmark), comprising 43 datasets across 9 domains such as biomedicine, programming, social media, law, finance.\nOn zero-shot NER, LLaMA and Alpaca perform poorly on this benchmark (close to zero F1). Vicuna performs much better by comparison, but still trails ChatGPT by over 20 absolute points in average F1. By contrast, \\longname attains state-of-the-art NER accuracy across tens of thousands of entity types in the \\longname benchmark, outperforming Vicuna by over 30 absolute points in average F1. \nWith a tiny fraction of parameters, \\longname not only replicates ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute points in average F1. \nRemarkably, \\longname even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as InstructUIE~\\citep{wang2023instructuie}, which uses supervised NER examples.\nWe also conduct thorough ablation studies to assess the impact of various distillation components, such as the instruction prompts and negative sampling. \n\n\\eat{\n``Instruction-tuned'' large language models (LLMs) \\cite{ouyang2022training,gpt4} have demonstrated remarkable capabilities of tackling diverse tasks.\nHowever, they %have a huge cost\nusually lead to high costs for training and inference, and %access to these models is limited to text outputs,\ncan only grant a black-box accessibility without providing underlying probabilities, attention weights, or hidden states.\nThe advancements of recent models such as Alpaca \\cite{alpaca} and Vicuna \\cite{vicuna-2023} have shown that potent conversational capabilities can be obtained by distilling existing LLMs into smaller, more manageable models.\nUpon close inspection, there is still a significant performance gap between the distilled models and LLMs, especially on more targeted rigorous evaluations~\\cite{wang2023far,gudibande2023false}.\nIn this paper, we aim to close such a gap. But instead of doing it for all tasks, we focus on a specific task.\nOur goal is to distill LLMs into a smaller model that performs better or equally well on a given task,\nwhich enables a smaller, fully-accessible, and fine-tunable alternative to LLMs.\n\nWe choose named entity recognition (NER) as a case study as NER is one of the fundamental tasks in natural language processing, which directly supports the need for text understanding and structuring in verticals such as news, science, clinics, etc.~\\cite{wu2017clinical,perera2020named}\nRecent studies~\\cite{wei2023zero,li2023evaluating} show that LLMs still fall behind supervised models on the established NER datasets when there are abundant supervision data. \nHowever, only a small amount of entity types have abundant human annotations.\nThere is little to no supervision data available for a vast number of entity types or concepts which are of practical value. And the number keeps growing with emergent entities.\nIt is extremely costly to provide abundant human annotations for new entity types, especially in specialized domains such as clinics and biology that require expert knowledge. \nIn the meanwhile, we find that supervised models can barely transfer their knowledge to new data and entity types.\nIn contrast, LLMs show superior performance in the setting where there is a lack of supervision data~\\cite{wang2023instructuie}.\n\nIn this paper, we propose a recipe for distilling end-task capabilities from LLMs.\nUsing NER as a case study, we successfully distill %task-level\nthese capabilities from LLMs into a much smaller model \\longname that can recognize diverse types of entities or concepts in text corpora from a wide range of domains.\nIn order to rigorously evaluate \\longname, we collect the largest NER benchmark to date, which consists of 43 datasets across 9 domains.\nOur experiments demonstrate that \\longname not only surpasses existing instruction-tuned models at the same size (e.g., Alpaca, Vicuna) by a large margin, but also shows substantially better performance than LLMs (e.g., ChatGPT).\nWhen additional human annotations are available, we can fine-tune \\longname for further improvement at a much smaller cost compared to LLMs.\nThe fine-tuned \\longname significantly outperforms the supervised state of the art. % as well as LLMs such as ChatGPT.\n\nWe release our training recipe, data, and the \\longname model. We are also hosting an interactive demo to enable direct engagement. We hope sharing them can facilitate the study of distilling other tasks' zero-shot capabilities from LLMs.\n\n}\n\\section{Related Work}\n\n\\paragraph{Knowledge distillation.}\nWhile LLMs such as ChatGPT achieve promising results, these models are often black-box and have high computational costs.\nTo address these issues, distilling the task capabilities of LLMs into smaller, more manageable models has emerged as a promising direction.\nKnowledge distillation~\\citep{hinton2015distilling} often revolves around the transfer of knowledge from larger, more complex models to their smaller counterparts.\nRecent work~\\citep{alpaca,vicuna-2023,peng2023instruction} seeks to distill the general abilities of LLMs with the objective of matching, if not surpassing, the performance of the original LLMs.\nParticularly, Alpaca~\\citep{alpaca} automates the generation of instructions~\\citep{wang2022self} and distills the knowledge from a teacher LLM.\nVicuna~\\citep{vicuna-2023} adopts the ShareGPT data, which are comprised of real conversations with ChatGPT conducted by users, thereby providing a more authentic context for distillation.\nAnother line of work~\\citep{smith2022language,jung2023impossible,hsieh-etal-2023-distilling,gu2023distilling} focuses on distilling task-level abilities from LLMs.\nParticularly, \\citet{jung2023impossible} propose an efficient method to distill an order of magnitude smaller model that outperforms GPT-3 on specialized tasks summarization and paraphrasing in certain domains.\n\\citet{hsieh-etal-2022-converting} propose to distill LLMs' reasoning abilities into smaller models by chain-of-the-thought distillation.\nHowever, these studies perform distillation either on certain datasets or domains, while our work focuses on a more general formulation that can be applied to diverse domains.\n\n\\paragraph{Instruction tuning.} \nAs an effective method to adapt LMs to perform a variety of tasks, instruction tuning has attracted an increasing number of community efforts: FLAN~\\citep{chung2022scaling}, T0~\\citep{sanh2021multitask}, and Tk-Instruct~\\citep{wang-etal-2022-super} convert a large set of existing supervised learning datasets into instruction-following format, and then fine-tune encoder-decoder models, showing strong zero-shot and few-shot performance on NLP benchmarks. \\citet{ouyang2022training} crowd-source high-quality instruction data and fine-tune GPT-3 into InstructGPT, enhancing its ability to understand user intention and follow instructions.\nRecent advancements~\\citep{alpaca,vicuna-2023,peng2023instruction} have also led to smaller models that exhibit task-following capabilities, after being fine-tuned on instruction data generated by LLMs, such as ChatGPT or GPT4.\nHowever, these smaller models often struggle to generate high-quality responses for a diverse range of tasks~\\citep{wang2023far}.\nA closer examination on targeted benchmarks reveals a substantial gap between these models to ChatGPT~\\citep{gudibande2023false}. \nOur proposed method, in contrast, focuses on tuning models to excel at a specific type of tasks. \nThe diversity in our instructing-tuning method comes from task labels (e.g., relation types for relation extraction, entity types for NER), rather than instructions.\nBy focusing on task-level capabilities and using NER as a case study, we demonstrate that it is possible to devise a tuning recipe that not only closes the performance gap but also surpasses ChatGPT.\n\\citet{wang2023instructuie} also explore instruction-tuning for information extraction tasks. However, their method relies solely on supervised datasets and yields subpar performance when compared to ChatGPT.\n\\section{Mission-Focused Instruction Tuning}\n\nInstruction tuning~\\citep{ouyang2022training,wei2022finetuned} is a method through which pretrained autoregressive language models \nare finetuned to follow natural language instructions and generate responses.\nExisting work focuses on tuning models to do diverse tasks~\\citep{alpaca,vicuna-2023}. In contrast, we introduce a general recipe for mission-focused instruction tuning, where the pretrained model is tuned for a broad application class such as open information extraction.\n\nIn this paper, we conduct a case study on the NER task, as it is one of the fundamental tasks for knowledge extraction from text.\nThe objective is to learn a model $f: (\\mathcal{X} \\times \\mathcal{T})\\rightarrow \\mathcal{Y}$, where $\\mathcal{X}$ represents the set of inputs, $\\mathcal{T}$ denotes a predefined set of entity types, and $\\mathcal{Y}$ represents the set of entities of a specific type in the given input.\n\n\\subsection{Data Construction}\nA typical instruction-tuning example is made of three parts, including \\texttt{instruction}, \\texttt{input}, and \\texttt{output}, where the diversity of \\texttt{instruction} %is key for\ncauses the models to follow\na wide range of task instructions.\nHowever, for \\emph{mission-focused} instruction tuning, our goal is to tune the model to maximally generalize across semantic types and domains for the targeted application class.\nTherefore, we focus on increasing the diversity of \\texttt{input} rather than \\texttt{instruction}.\n\n\\begin{wrapfigure}[16]{r}{0.5\\textwidth}\n    \\vspace{-10pt}\n    \\begin{AIboxSmall}{\\footnotesize Data Construction Prompt}\n    \\footnotesize\n    \\textbf{System Message:} You are a helpful information extraction system.\n    \n    \\textbf{Prompt:} Given a passage, your task is to extract all entities and identify their entity types. The output should be in a list of tuples of the following format: [(\"entity 1\", \"type of entity 1\"), ... ].\\\\\n\nPassage: \\{input\\_passage\\}\n    \\end{AIboxSmall}\n    \\caption{Data construction prompt for generating entity mentions and their types for a given passage.}\n    \\label{fig:prompt_ent_type_gen}\n\\end{wrapfigure}\n\nWhile earlier work~\\citep{jung2023impossible} employs language models to generate inputs, these models typically assume that the domains of test data are known and prompt LMs to generate data for each domain.\nThis method falls short when applied to distillation for a broad application class, where the distribution of test data is unknown.\nConsequently, it is challenging to generate inputs from LMs that provide wide coverage of the test domains.\n\nTo address this limitation, we propose an alternative: directly sampling inputs from a large corpus across diverse domains, and then using an LLM to generate outputs.\nIn this paper, we sample inputs from the Pile corpus~\\citep{gao2020pile}, which compiles 22 distinct English sub-datasets.\nWe chunk the articles in Pile to passages of a max length of 256 tokens and randomly sample 50K passages as the inputs.\nSubsequently, we use ChatGPT (\\texttt{gpt-3.5-turbo-0301}) to generate entity mentions and their associated types based on the sampled passages.\nTo ensure stability, we set the generation temperature to 0.\nThe specific prompt for constructing the data is shown in Fig.~\\ref{fig:prompt_ent_type_gen}.\nIn this prompt, we do not specify the set of entity types of interest, allowing the LLM to generate outputs encompassing a broad coverage of entity types.\n\n\\begin{table}[!t]\n    \\centering\n    \\small\n    \\begin{tabular}{c|p{0.8\\linewidth}}\n        \\shline\n         Frequency&  Entity types\\\\\n         \\shline\n         \\makecell[tc]{Top 1\\% \\\\ (74\\%)} & person, organization, location, date, concept, product, event, technology, group, medical condition, ...\\\\\n         \\shline\n         \\makecell[tc]{1\\%-10\\% \\\\ (19\\%)}& characteristic, research, county, module, unit, feature, cell, package, anatomical structure, equipment, ... \\\\\n         \\shline\n         \\makecell[tc]{10\\%-100\\% \\\\ (7\\%)}& attribute value, pokemon, immune response, physiology, animals, cell feature, FAC, input device, ward, broadcast, ... \\\\\n         \\shline\n    \\end{tabular}\n    \\caption{Examples of entities across different frequency ranges - top 1\\%, 1-10\\%, and 10-100\\%, along with the percentage of total frequencies for each range.}\n    \\label{tab:entity_types}\n\\end{table}\n\n\\smallskip\n\\noindent\\textbf{Data statistics.} After filtering out unparseable outputs and inappropriate entities, including non-English entities and those classified under 'ELSE' categories, such as None, NA, MISC, and ELSE, our dataset comprises 45,889 input-output pairs, encompassing 240,725 entities and 13,020 distinct entity types.\nWe divide the entity types according to frequency and show the top 10 entity types in each range in Tab.~\\ref{tab:entity_types}.\nThe distribution of these entity types exhibits a heavy tail, where the top 1\\% of entities account for 74\\% of total frequencies.\nWe find that the generated data contain entity types from various domains, ranging from the general domain (e.g., \\textsc{person}) to the clinical domain (e.g., \\textsc{medical condition}).\nMoreover, we observe variations in granularity among the entity types.\nE.g., \\textsc{county} is the subset of \\textsc{location}, and \\textsc{input device} is a subset of \\textsc{product}.\nThese data characteristics offer extensive coverage of entity types, making them suitable for distilling capabilities from LLMs across various domains.\n\n\\smallskip\n\\noindent\\textbf{Definition-based data construction.} Besides entity types, we also prompt ChatGPT to generate entity mentions and define their types using short sentences.\nTo do so, we simply change the prompt in Fig.~\\ref{fig:prompt_ent_type_gen} from ``extract all entities and identify their entity types'' to ``extract all entities and concepts, and \\emph{define their type using a short sentence}''.\nThis method generates a much more diverse set of 353,092 entity types and leads to a tuned model that is less sensitive to entity type paraphrasing (Section~\\ref{ssec:case_study}), but performs worse on standard NER benchmarks (Section~\\ref{ssec:results_distill}).\n\n    \n\n\\subsection{Instruction Tuning}\n\n\\begin{wrapfigure}[20]{r}{0.5\\textwidth}\n    \\vspace{-20pt}\n    \\begin{AIboxSmall}{\\footnotesize Conversation-style Instruct Tuning Template}\n    \\footnotesize\n    A virtual assistant answers questions from a user based on the provided text.\\\\\n    \\texttt{User:} Text: $\\mX_\\text{passage}$\\\\\n    \\texttt{Assistant:} I've read this text.\\\\\n    \\texttt{User:} What describes $\\vt_1$ in the text?\\\\\n    \\texttt{Assistant:} \\fcolorbox{aigold}{aigold}{$\\vy_1$}\\\\\n    ...\\\\\n    \\texttt{User:} What describes $\\vt_T$ in the text?\\\\\n    \\texttt{Assistant:} \\fcolorbox{aigold}{aigold}{$\\vy_T$}\n    \\end{AIboxSmall}\n    \\caption{The conversation-style template that converts a passage with NER annotations into a conversation, where $\\mX_\\text{passage}$ is the input passage, $[\\vt_1,...,\\vt_T]$ are entity types to consider, and $\\vy_i$ is a list of entity mentions that are $\\vt_i$. The conversation is used to tune language models. Only the highlighted parts are used to compute the loss.}\n    \\label{fig:input_in_conv}\n\\end{wrapfigure}\n\nAfter obtaining the data, we apply instruction tuning to smaller models to distill for a broad application class, e.g., diverse entity types in NER.\nOur template, as shown in Fig.~\\ref{fig:input_in_conv}, adopts a conversation-style tuning format.\nIn this approach, the language model is presented with a passage $\\mX_\\text{passage}$ as input.\nThen, for each entity type $\\vt_i$ that appears in the output, we transform it into a natural language query ``\\textit{What describes $\\vt_i$?}''\nSubsequently, we tune the LM to generate a structured output $\\vy_i$ in the form of a JSON list containing all entities of $\\vt_i$ in the passage.\nWe consider $\\vy_1,...,\\vy_T$ as gold tokens and apply a language modeling objective on these tokens.\nOur preliminary experiments show that conversation-style tuning is better than traditional NER-style tuning adopted by \\citet{wang2023instructuie,sun2023pushing}.\n\nBesides one entity type per query, we also consider combining all entity types in a single query, requiring the model to output all entities in a single response.\nDetailed results and discussions can be found in Section~\\ref{ssec:results_distill}.\n\n\\smallskip\n\\noindent\\textbf{Negative sampling.}\nOur data construction process follows an open-world assumption where we allow the model to generate entity types that have appeared in the passage.\nHowever, the generated data do not account for entity types that are not mentioned in the passage, i.e., negative entity types.\nAs a result, it is challenging for us to apply a model trained on this data to a closed-world setting, where one may ask for entity types that do not exist in the passage.\nTo address this potential mismatch, we sample negative entity types from the collection of all entity types that do not appear in the passage as queries and set the expected outputs as empty JSON lists.\nThe sampling of negative entity types is done with a probability proportional to the frequency of entity types in the entire dataset.\nThis approach greatly improves the instruction tuning results, as shown in Section~\\ref{ssec:ablation_study}.\n\n\\smallskip\n\\noindent\\textbf{Supervised finetuning.} When we have additional human annotations, model performance can be further improved with supervised data.\nHowever, a significant challenge arises when training with multiple datasets, as there might be discrepancies in label definitions among these datasets, resulting in label conflicts.\nFor instance, some datasets like ACE~\\citep{walker2006ace} consider personal pronouns (e.g., she, he) as \\textsc{person}, while other datasets like multiNERD~\\citep{tedeschi-navigli-2022-multinerd} do not include pronouns.\n\n\\begin{wrapfigure}[13]{r}{0.5\\textwidth}\n    \\vspace{-20pt}\n    \\begin{AIboxSmall}{\\footnotesize Dataset-specific Instruct Tuning Template}\n    \\footnotesize\n    A virtual assistant answers questions from a user based on the provided text.\\\\\n    \\texttt{User:} \\fcolorbox{aired}{aired}{Dataset: $\\mD$} \\textbackslash n Text: $\\mX_\\text{passage}$\\\\\n    \\texttt{Assistant:} I've read this text.\\\\\n    \\texttt{User:} What describes $\\vt_1$ in the text?\\\\\n    \\texttt{Assistant:} \\fcolorbox{aigold}{aigold}{$\\vy_1$}\\\\\n    ...\\\\\n    \\texttt{User:} What describes $\\vt_T$ in the text?\\\\\n    \\texttt{Assistant:} \\fcolorbox{aigold}{aigold}{$\\vy_T$}\n    \\end{AIboxSmall}\n    \\caption{The dataset-specific instruction tuning template. We add the dataset name $\\mD$ (colored in \\fcolorbox{aired}{aired}{red}) as part of the input to resolve conflicts in label definitions.}\n    \\label{fig:supervised_input_in_conv}\n\\end{wrapfigure}\n\nTo address this issue, we propose to use dataset-specific instruction tuning templates to harmonize the discrepancies in label definitions, as illustrated in Fig.~\\ref{fig:supervised_input_in_conv}.\nSpecifically, we augment the input with an additional field denoting the dataset name $\\mD$.\nBy doing so, the model can learn the dataset-specific semantics of labels.\nDuring inference, we use the respective dataset name in the prompt for the supervised setting, whereas we omit the dataset field from the prompt in the zero-shot setting.\n\\section{Universal NER Benchmark}\nTo conduct a comprehensive evaluation of NER models across diverse domains and entity types, we collect the largest NER benchmark to date.\nThis benchmark encompasses 43 NER datasets across 9 domains, including general, biomedical, clinical, STEM, programming, social media, law, finance, and transportation domains.\nAn overview of data distribution is shown in Fig.~\\ref{fig:data_dist}.\nDetailed dataset statistics are available in Appendix Tab.~\\ref{tab:full_data_statistics}.\n\n\\begin{wrapfigure}[22]{r}{0.5\\textwidth}\n    \\vspace{-15pt}\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{figs/data_dist.pdf}\n    \\caption{Distribution of UniNER benchmark.}\n    \\label{fig:data_dist}\n\\end{wrapfigure}\n\n\\smallskip\n\\noindent\\textbf{Dataset processing.} To make the entity types semantically meaningful to LLMs, we conduct a manual inspection of the labels and convert the original labels into natural language formats.\nFor instance, we replace \\textsc{per} with \\textsc{person}.\nWhile we try to collect a broad coverage of NER datasets, we do not use all entity types.\nThis is because some entity types (e.g., \\textsc{Else}) are not coming from consistent sources across the different datasets.\nTheir annotations often come from different ontologies for different purposes.\nThe choices of entity types and their annotation guidelines are not optimized for holistic or comprehensive assessments, which renders them suboptimal for use as a ``ground truth'' to evaluate a universal NER model.\nTherefore, we remove those labels from the datasets.\nIn addition, some datasets are at the document level and contain very long contexts, which might exceed the input length limit of models.\nTherefore, we split all instances in document-level datasets into sentence-level %tasks.\nones.\n\\section{Experiments}\nThis section presents experimental evaluations of \\longname.\nWe start by outlining experimental settings (Section~\\ref{ssec:experiment_setting}), followed by presenting the results on both distillation and supervised settings (Sections~\\ref{ssec:results_distill} and~\\ref{ssec:results_supervised}).\nFinally, we conduct analysis (Section~\\ref{ssec:ablation_study}) and case study (Section~\\ref{ssec:case_study}) to provide deeper insights into the model's performance.\n\n\\begin{figure*}[t]\n\\begin{subfigure}[t]{0.46\\textwidth}\n\\centering\n\\includegraphics[width=0.99\\textwidth]{figs/zero_shot_domain.pdf}\n\\caption{Comparisons of zero-shot models on different domains. Our distilled models achieve better results than ChatGPT in all evaluated domains.}\n\\label{fig:self-supervised-bar}\n\\end{subfigure}\n\\hfill\n\\begin{subfigure}[t]{0.46\\textwidth}\n\\centering\n\\includegraphics[width=0.97\\textwidth]{figs/zero_shot_ablation.pdf}\n\\caption{Comparisons between \\shortname-7B and two variants. \\shortname-7B-definition is distilled on Pile data prompted with entity type definitions. \\shortname-7B-all-in-one is tuned with the template where all entity types are asked in one query.}\n\\label{fig:self-supervised-ablation}\n\\end{subfigure}\n\\end{figure*}\n\n\\subsection{Experimental Settings}\n\\label{ssec:experiment_setting}\n\\smallskip\n\\noindent\\textbf{Model configurations.}\nWe train models based on LLaMA\\footnote{We also train models based on LLaMA 2~\\citep{touvron2023llama2}. However, no significant difference is observed in our experiments.}~\\citep{touvron2023llama} following the training schedule of~\\citet{vicuna-2023} for a fair comparison.\nConsidering the large size of certain test datasets, we perform evaluation by sampling up to 200,000 passage-query pairs from each dataset.\nWe use strict entity-level micro-$F_1$ in evaluation, requiring both the entity type and boundary to exactly match the ground truth.\n\n\\smallskip\n\\noindent\\textbf{Compared models.}\nWe compare our model (\\shortname{}) against the following models: (1) \\textbf{ChatGPT} (\\texttt{gpt-3.5-turbo-0301}).\nWe use the prompting template in~\\citet{ye2023comprehensive} for NER.\n(2) \\textbf{Vicuna}~\\citep{vicuna-2023} is finetuned with ChatGPT conversations, using LLaMA as the base model.\n(3) \\textbf{InstructUIE}~\\citep{wang2023instructuie} is a supervised model finetuned on diverse information extraction datasets, employing a unified natural language generation objective.\nIt adopts Flan-T5 11B~\\citep{chung2022scaling} as the base model.\n\n\\subsection{Distillation}\n\\label{ssec:results_distill}\nWe first evaluate the models in a zero-shot setting.\nWe compare the performance of ChatGPT, Vicuna, and our model \\shortname, which is distilled from ChatGPT NER annotations on Pile without human-labeled datasets in training.\nResults are shown in Fig.~\\ref{fig:self-supervised-bar}.\\footnote{Due to limited space, we only show the average $F_1$ of all datasets and the average $F_1$ of each domain. See Appendix Fig.~\\ref{fig:add-supervised-bar} for full results.}\nWe observe that our distilled models, namely \\shortname-7B and \\shortname-13B, outperform ChatGPT in terms of average $F_1$.\nThe average $F_1$ scores of \\shortname-7B and \\shortname-13B are 41.7\\% and 43.4\\%, respectively, compared to 34.9\\% for ChatGPT.\nThis demonstrates that our proposed targeted distillation from diverse inputs yields models that have superior performance on a broad application class while maintaining a relatively small model size.\nAdditionally, \\shortname-13B exhibits better performance compared to \\shortname-7B, indicating that fine-tuning on larger models may lead to improved generalization.\nIn terms of domains, both \\shortname-7B and \\shortname-13B outperform ChatGPT on all domains, showing that the improvements exist across various domains.\n\n\\begin{wraptable}{r}{6cm}\n\\vspace{-15pt}\n\\centering\n\\tablestyle{4pt}{1.3}\n\\setlength{\\tabcolsep}{2pt}\n\\begin{tabular}{c|cc|c}\n\\shline\n& BERT- & InstructUIE & \\shortname \\\\\nDataset & base & 11B & 7B \\\\\n\\shline\nACE05 & \\textbf{87.30} & 79.94 & 86.69 \\\\\nAnatEM & 85.82 & 88.52 & \\textbf{88.65} \\\\\nbc2gm & 80.90 & 80.69 & \\textbf{82.42} \\\\\nbc4chemd & 86.72 & 87.62 & \\textbf{89.21} \\\\\nbc5cdr & 85.28 & 89.02 & \\textbf{89.34} \\\\\nBroad Twitter & 58.61 & 80.27 & \\textbf{81.25} \\\\\nCoNLL03 & 92.40 & 91.53 & \\textbf{93.30}\\\\\nFabNER & 64.20 & 78.38 & \\textbf{81.87}\\\\\nFindVehicle & 87.13 & 87.56 & \\textbf{98.30} \\\\\nGENIA & 73.3 & 75.71 & \\textbf{77.54} \\\\\nHarveyNER & \\textbf{82.26} & 74.69 & 74.21 \\\\\nMIT Movie & 88.78 & 89.58 & \\textbf{90.17} \\\\\nMIT Restaurant & 81.02 & \\textbf{82.59} &82.35 \\\\\nMultiNERD & 91.25 & 90.26 & \\textbf{93.73} \\\\\nncbi & 80.20 & 86.21 & \\textbf{86.96} \\\\\nOntoNotes & \\textbf{91.11} & 88.64 & 89.91 \\\\\nPolyglotNER & \\textbf{75.65} & 53.31 & 65.67 \\\\\nTweetNER7 & 56.49 & \\textbf{65.95} & 65.77 \\\\\nWikiANN & 70.60 & 64.47 & \\textbf{84.91} \\\\\nwikiNeural & 82.78 & 88.27 & \\textbf{93.28} \\\\ \\hline\nAvg & 80.09 & 81.16 & \\textbf{84.78} \\\\\n\\shline\n\\end{tabular}\n\\caption{$F_1$ on 20 datasets used in \\citet{wang2023instructuie}. BERT-base results are from~\\citet{wang2023instructuie}. InstructUIE results are from our reevaluation.}\n\\label{tab:supervised-eval}\n\\vspace{-38pt}\n\\end{wraptable}\n\nWe further compare different variations of \\shortname, including (1) \\shortname-all-in-one, where the extraction of all entity types are combined into one query and response, and (2) \\shortname-definition, where queries in instruction tuning data use entity type definitions generated by ChatGPT instead of entity types.\nResults are shown in Fig.~\\ref{fig:self-supervised-ablation}.\nWe observe that both \\shortname-all-in-one and \\shortname-definition underperform \\shortname-type by 3.3\\% and 11.8\\% on average, respectively.\nThe \\shortname-definition variant's decreased performance could be due to its lower consistency with the evaluation datasets, which all adopt words or short phrases as labels instead of sentences.\nThe performance disparity in the \\shortname-all-in-one variant can be potentially attributed to the attention distribution and task complexity.\nWhen the model is required to handle multiple entity types within a single query, it might disperse its attention across these varied types, possibly resulting in less accurate identification for each individual type.\nConversely, by decomposing the task into several simpler ones, each focusing on one entity type at a time, the model might be better equipped to handle the complexity, thus yielding more accurate results.\n\n\\subsection{Supervised Finetuning}\n\\label{ssec:results_supervised}\nWe study whether our models can be further improved using additional human annotations.\nWe compare the performance of ChatGPT, Vicuna, InstructUIE~\\citep{wang2023instructuie}~\\footnote{Please note that the original evaluation script in InstructUIE contains a critical bug. For passages that do not contain any entities, the script adds \\textsc{none} as a placeholder entity and takes it into account when calculating $F_1$. To rectify this error, we re-evaluated InstructUIE using their released checkpoint.}, and \\shortname.\n\n\\smallskip\n\\noindent\\textbf{Out-of-domain evaluation.} We first study whether supervised finetuning leads to better generalization on unseen data.\nWe follow InstructUIE to exclude two datasets CrossNER~\\citep{liu2021crossner} and MIT~\\citep{liu2013asgard} for out-of-domain evaluation, and fine-tune our model using training splits of the remaining datasets in the universal NER benchmark.\nResults are shown in Tab.~\\ref{tab:zero_shot_2}.\nNotably, without any fine-tuning, instruction-tuned \\shortname{} 7B and 13B already surpass ChatGPT, Vicuna, and the supervised fine-tuned InstructUIE-11B by a large margin.\nIf we train our model from scratch only using the supervised data, it achieves an average $F_1$ of 57.2\\%.\nContinual fine-tuning \\shortname-7B using the supervised data achieves the best average $F_1$ of 60.0\\%.\nThese findings suggest that the models' generalization can be further improved with additional human-annotated data.\n\n\\begin{table*}[!t]\n\\centering\n\\tablestyle{4pt}{1.1}\n\\begin{tabular}{l|ccccccc|c}\n\\shline\n\\multicolumn{1}{c|}{Model} & Movie & Restaurant & AI & Literature & Music & Politics & Science & Avg  \\\\ \\shline\n\\emph{Zero-shot} & & & & & & & & \\\\\n\\quad Vicuna-7B& 6.0& 5.3& 12.8& 16.1& 17.0& 20.5& 13.0& 13.0 \\\\\n\\quad Vicuna-13B& 0.9& 0.4& 22.7& 22.7& 26.6& 27.2& 22.0& 17.5 \\\\\n\\quad ChatGPT & 5.3 & 32.8 & 52.4 & 39.8 & \\textbf{66.6} & \\textbf{68.5} & \\textbf{67.0} & 47.5 \\\\\n\\quad \\shortname-7B & 42.4& 31.7& 53.5& 59.4& 65.0 & 60.8& 61.1& 53.4 \\\\ \n\\quad \\shortname-13B & \\textbf{48.7} & \\textbf{36.2} & \\textbf{54.2} & \\textbf{60.9} & 64.5& 61.4 & 63.5 & \\textbf{55.6} \\\\ \\shline\n\\emph{In-domain supervised} & & & & & & & & \\\\\n\\quad InstructUIE-11B  & - & - & 48.4 & 48.8 & 54.4 & 49.9 & 49.4 & - \\\\\n\\quad \\shortname-7B (sup. only) & 54.2& 16.0& 62.3& \\textbf{67.4}& 69.0& 64.5& 66.9& 57.2 \\\\\n\\quad \\shortname-7B (inst-tuned + sup.) & \\textbf{61.2}& \\textbf{35.2}& \\textbf{62.9}& 64.9& \\textbf{70.6}& \\textbf{66.9}& \\textbf{70.8}& \\textbf{61.8} \\\\\n\\shline\n\\end{tabular}\n\\caption{Out-of-domain evaluation on datasets from \\citet{wang2023instructuie}. ``sup. only'' denotes a variant of \\shortname-7B, trained from scratch using in-domain supervised data only and evaluated on out-of-domain datasets.}\n\\label{tab:zero_shot_2}\n\\end{table*}\n\n\\smallskip\n\\noindent\\textbf{In-domain evaluation.}\nWe then study the performance of \\shortname in an in-domain supervised setting, where we fine-tune \\shortname-7B using the same training data as InstructUIE~\\citep{wang2023instructuie}.\nResults are shown in Tab.~\\ref{tab:supervised-eval}.\nOur \\shortname-7B achieves an average $F_1$ of 84.78\\% on the 20 datasets, surpassing both BERT-base and InstructUIE-11B by 4.69\\% and 3.62\\%, respectively.\nThis experiment demonstrates the effectiveness of our model in the supervised setting.\n\n\\subsection{Analysis}\n\\label{ssec:ablation_study}\n\n\\begin{table*}[!ht]\n\\centering\n\\tablestyle{4pt}{1.3}\n\\begin{tabular}{l|ccccccc|c}\n\\shline\nStrategy & Movie & Restaurant & AI & Literature & Music & Politics & Science & Avg  \\\\ \\shline\nNone & 19.1& 19.1& 25.1& 39.5& 42.7& 48.9& 26.2& 31.5 \\\\\nUniform & 42.5& 29.0& 42.5& 53.3& 57.4& 56.8& 52.6& 47.7 \\\\\n\\rowcolor{defaultcolor} Frequency &42.4& 31.7& 53.5& 59.4& 65.0& 60.8& 61.1& 53.4 \\\\\n\\shline\n\\end{tabular}\n\\caption{Ablation study on negative sampling strategies for \\shortname-7B. All models are instruction-tuned on Pile.}\n\\label{tab:neg-sample-ablation}\n\\end{table*}\n\n\\begin{wrapfigure}[23]{r}{0.5\\textwidth}\n\\vspace{-22pt}\n    \\centering\n    \\includegraphics[width=0.48\\textwidth]{figs/data_specific_prompt.pdf}\n    \\caption{Different in $F_1$ between data-specific and original templates in the supervised setting. \\textcolor[HTML]{ff7f0e}{Orange} and \\textcolor[HTML]{1f77b4}{Blue} mark datasets with/without label overlap with other datasets, respectively.}\n    \\label{fig:dataset_specific_inst}\n\\end{wrapfigure}\n\n\\smallskip\n\\noindent\\textbf{Negative sampling strategies.}\nWe experiment with different negative sampling strategies in instruction tuning, including (1) \\emph{no negative sampling}, (2) \\emph{uniform sampling} where entity types are randomly sampled with equal probability for each one, and (3) \\emph{frequency-based sampling} where we sample entity types with probabilities proportional to their frequency in the constructed dataset.\nResults are shown in Tab.~\\ref{tab:neg-sample-ablation}.\nAmong the approaches tested, frequency-based sampling yielded the best results, outperforming no sampling and uniform sampling by 21.9\\% and 5.7\\%, respectively. \nThese findings highlight the crucial role of negative sampling in instruction tuning, with frequency-based sampling emerging as the most effective method for enhancing model performance in our study.\n\n\\smallskip\n\\noindent\\textbf{Dataset-specific template.}\nWe compare the results of our dataset-specific instruction tuning template and the original template in the supervised setting.\nAs shown in Fig.~\\ref{fig:dataset_specific_inst},\nwe find that the data-specific template outperforms the original template on most datasets.\nTo gain deeper insights into the improvements achieved, we further divide the datasets into two categories: those with label (entity type) overlap with other datasets and those without overlap. \nOur analysis reveals that datasets with label overlap demonstrate more substantial improvements.\n\nTo explore this further, we measure $F_1$ score across all evaluation datasets and calculate the difference. Apart from the long-tail entity types that manifest a high variance in results, we identify two entity types where the dataset-specific template outperforms the original template by over 10\\%: \\textsc{facility} (22.0\\%) and \\textsc{time} (12.4\\%). \nIntriguingly, both labels exhibit inconsistencies in their definitions across various datasets.\nThe \\textsc{facility} label has been annotated on pronouns (e.g., it, which) as entities in ACE datasets but are excluded in OntoNotes.\nThe \\textsc{time} label denotes well-defined time intervals (e.g., Christmas) in MultiNERD, but may encompass any general time expressions (e.g., 3 pm) in OntoNotes.\nThis finding suggests that the improvements provided by the data-specific template are particularly effective in resolving label conflicts.\n\n\\begin{table*}[!t]\n\\centering\n\\tablestyle{3pt}{1.2}\n\\begin{tabular}{c|lrcccccc|c}\n\\shline\nPartial match & Model & Movie & Restaurant & AI & Literature & Music & Politics & Science & Avg  \\\\ \\shline\n\\multirow{3}{*}{No}& ChatGPT & 5.3 & 32.8 & 52.4 & 39.8 & 66.6 & 68.5 & 67.0 & 47.5 \\\\\n& \\shortname-7B & 42.4& 31.7& 53.5& 59.4& 65.0& 60.8& 61.1& 53.4 \\\\\n& \\shortname-7B w/ sup & 61.2& 35.2& 62.9& 64.9& 70.6& 66.9& 70.8& 61.8 \\\\\n\\shline\n\\multirow{3}{*}{Yes}& ChatGPT& 5.9& 40.1& 55.7& 42.8& 70.2& 71.7& 70.1& 50.9 \\\\\n& \\shortname-7B & 46.9& 40.3& 57.7& 62.7& 62.9& 63.2& 63.3& 56.7 \\\\\n& \\shortname-7B w/ sup & 65.5& 39.4& 66.2& 67.2& 72.7& 68.9& 73.4& 64.8 \\\\\n\\shline\n\\end{tabular}\n\\caption{Allowing partial match between the prediction and the gold that has overlap increases the results.\nWhen it is allowed, any partial match is regarded as half correct (counted as 0.5 in true positive) when computing $F_1$.\n}\n\\label{tab:loose-$F_1$}\n\\end{table*}\n\n\\smallskip\n\\noindent\\textbf{Evaluation with partial match.}\nWhile using strict $F_1$ as an evaluation metric, we notice that it may underestimate the zero-shot learning capabilities of NER models.\nIn particular, strict $F_1$ penalizes slight misalignments in the boundaries of the extracted entities, which may not necessarily indicate an incorrect understanding of the text. \nFor instance, given the sentence \\textit{any asian cuisine around} and the entity type \\textsc{cuisine}, \\shortname extracts \\textit{asian cuisine} as the named entity, while the ground truth only labels \\textit{asian} as the correct entity.\nHowever, the model's prediction can still be viewed as correct, even though it is deemed incorrect by strict $F_1$.\nTo better estimate the zero-shot abilities, we also consider partial match~\\citep{segura2013semeval} in evaluation.\nIn this context, a prediction that exhibits word overlap with the ground truth is regarded as half correct (counted as 0.5 in true positive) when computing $F_1$.\nResults are shown in Tab.~\\ref{tab:loose-$F_1$}.\nWe find that allowing partial match consistently improves the results.\nBesides, our models is still the best-performing model on average.\n\n\\subsection{Case Study}\n\\label{ssec:case_study}\n\n\\smallskip\n\\noindent \\textbf{Sensitivity to entity type paraphrasing.}\nOne type of entity can be expressed in multiple ways, so it is essential for our model to give consistent predictions given entity types with similar meanings.\nAn example of sensitivity analysis is present in Fig.~\\ref{fig:sensitivity}.\nWe observe that \\shortname-7B-type %is not robust to different entity types, as it\nsometimes fails to recognize entities with similar semantic meanings.\nOn the other hand, \\shortname-7B-definition, despite performing worse on our Universal NER benchmark, exhibits robustness to entity type paraphrasing.\nIt demonstrates that although using definitions may result in lower performance on standard NER benchmarks, it could yield improved performance for less populous entity types.\n\n\\smallskip\n\\noindent \\textbf{Recognition of diverse entity types.}\nWe present an example in Fig.~\\ref{fig:code-ex} showcasing the capabilities of \\shortname in recognizing various entities.\nParticularly, we focus on a novel domain of code and assess \\shortname's ability to extract diverse types of entities within the code.\nDespite minor mistakes (e.g., \\texttt{from\\_pretrained} is not identified as a method), this case study effectively demonstrates our model's capacity to capture entities of various types.\n\n\\section{Conclusion}\nWe present a targeted distillation approach with mission-focused instruction tuning. Using NER as a case study, we train smaller and more efficient models for open-domain NER. The proposed method successfully distills ChatGPT into a smaller model \\longname{}, achieving remarkable NER accuracy across a wide range of domains and entity types without direct supervision. These models not only retain ChatGPT's capabilities but also surpass it and other state-of-the-art systems in NER performance.\n\n\\section*{Acknowledgement}\n\nWenxuan Zhou and Muhao Chen were supported by the NSF Grants IIS 2105329 and ITE 2333736.\n\n\\newpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{GoLLIE: Annotation Guidelines Improve Zero-Shot Information Extraction}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nLarge Language Models (LLMs) combined with instruction tuning have made significant progress when generalizing to unseen tasks. However, they have been less successful in Information Extraction (IE), lagging behind task-specific models. Typically, IE tasks are characterized by complex annotation guidelines that describe the task and give examples to humans. \nPrevious attempts to leverage such information have failed, even with the largest models, as they are not able to follow the guidelines out of the box. \nIn this paper, we propose GoLLIE (\\textbf{G}uideline-f\\textbf{o}llowing \\textbf{L}arge \\textbf{L}anguage Model for \\textbf{IE}), a model able to improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to comply with annotation guidelines.\nComprehensive evaluation empirically demonstrates that GoLLIE is able to generalize to and follow unseen guidelines, outperforming previous attempts at zero-shot information extraction. The ablation study shows that detailed %representative annotation candidates to the \nguidelines are key for good results. \nCode, data, and models are publicly available: \\url{https://github.com/hitz-zentroa/GoLLIE}. %\\footnote{Upon publication.} to facilitate reproducibility of results and further research on this topic.\n\n\\end{abstract}\n\n\\section{Introduction}\n\\begin{figure}[b]\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{images/zero_shot_results.pdf}\n    \\caption{Out of domain zero-shot NER results. GPT results are not available for all domains.}\n    \\label{fig:zero-shot-results}\n\\end{figure}\n\nThe task of Information Extraction (IE) is highly challenging. This challenge is evident in the detailed guidelines, which feature granular definitions and numerous exceptions, that human annotators must follow to perform the task. The performance of current SoTA models heavily depends on the quantity of human-annotated data, as the model learns the guidelines from these examples. However, this performance significantly decreases when tested in new annotation schema \\citep{DBLP:conf/aaai/Liu0YDJCMF21}. The common practice in IE to achieve good results is to manually annotate each new domain and schema from scratch, as almost no transfer exists across application domains. Unfortunately, this is unfeasible, both, in terms of financial cost and human effort.\n\nRecent advancements in Large Language Models (LLM) \\citep{10.1145/3605943} have enabled the development of models capable of generalizing to unseen tasks. Thus, current zero-shot IE systems leverage the knowledge encoded in LLMs to annotate new examples \\citep{sainz-etal-2022-textual,DBLP:journals/corr/abs-2304-08085}. As a by-product of the pre-training process, models possess now a strong representation of what a person or an organization is. Therefore, they can be prompted to extract mentions of those categories from a text. However, this has a clear limitation: not every annotation schema\\footnote{We define schema as the set of labels and their definitions.} defines \"person\" (or any other label) in the same way. For example, ACE05~\\citep{ACE} annotates pronouns as persons, while, CoNLL03 \\citep{tjong-kim-sang-de-meulder-2003-introduction} does not. IE tasks require more information than just label names, they require annotation guidelines. \n\nCurrent LLMs have been trained to follow instructions, but they fail to follow annotation guidelines out of the box. \nFor instance, Figure \\ref{fig:zero-shot-results} shows results on domain-specific zero-shot Named Entity Recognition. The results of gpt-3.5-turbo when prompted with guidelines  \\citep{DBLP:journals/corr/abs-2305-12217} are low, around 20 F1 score on Music or Politics domains. Building a system that enables high-performance zero-shot information extraction, reducing the dependence on costly human annotations, remains an open challenge.  \n\nIn this work, we present \\GoLLIE GoLLIE  (\\textbf{G}uideline-f\\textbf{o}llowing \\textbf{L}arge \\textbf{L}anguage Model for \\textbf{IE}), an LLM fine-tuned to learn how to attend to the guidelines on a small set of well known IE tasks. Comprehensive zero-shot evaluation empirically demonstrates that GoLLIE outperforms the SoTA~\\citep{DBLP:journals/corr/abs-2304-08085}  in zero-shot information extraction (see Figure \\ref{fig:zero-shot-results}). \n\n\\section{Related Work}\nLarge Language Models (LLMs) have made significant advancements toward the development of systems that can generalize to unseen tasks~\\citep{10.1145/3605943}. \\citet{radford2019language} trained LLMs using a vast amount of internet data, finding that pre-trained models given natural language task descriptions can perform tasks such as question answering, machine translation, or summarizing without explicit supervision. Building on this discovery, instruction tuning, often referred to as multitask fine-tuning, has emerged as the leading method to achieve generalization to unseen tasks. This process involves pre-training a model on a massive amount of unlabeled data and subsequently fine-tuning it on a diverse collection of tasks \\citep{DBLP:conf/emnlp/WangMAKMNADASPK22,DBLP:journals/corr/abs-2210-11416} phrased as text-to-text problems \\citep{DBLP:journals/jmlr/RaffelSRLNMZLL20}. A natural language instruction or prompt is given to the model to identify the task it should solve \\citep{DBLP:conf/eacl/SchickS21,DBLP:conf/naacl/ScaoR21}. Research has demonstrated that increasing the parameter count of the language model \\citep{DBLP:conf/nips/BrownMRSKDNSSAA20}, coupled with improvements in the size and quality of the instruction tuning dataset, results in enhanced generalization capabilities \\citep{DBLP:journals/corr/abs-2302-12692,DBLP:journals/corr/abs-2205-01068,DBLP:journals/corr/abs-2204-02311,DBLP:conf/acl/MuennighoffWSRB23,DBLP:journals/corr/abs-2302-13971,DBLP:journals/corr/abs-2307-09288}. LLMs have displayed impressive zero-shot generalization capabilities in various challenging tasks, including coding \\cite{wang2021gpt,black-etal-2022-gpt,DBLP:journals/corr/abs-2308-12950}, common sense reasoning \\cite{DBLP:journals/corr/abs-2302-13971}, and medical applications \\cite{singhal2023large}, among others. \n\nIn the field of Information Extraction (IE), recent shared tasks \\citep{DBLP:conf/semeval/FetahuKCRM23} have shown that encoder-only language models such as XLM-RoBERTa \\citep{DBLP:conf/acl/ConneauKGCWGGOZ20} and mDEBERTA \\citep{DBLP:conf/iclr/HeGC23} remain the most effective models. Attempts to utilize LLMs and natural language instructions for IE have been less successful \\citep{DBLP:conf/semeval/TanHJCL0ZTXH23,DBLP:journals/corr/abs-2308-03279,DBLP:journals/corr/abs-2305-12217}, as their performance lags behind that of encoder-only models. \nBefore the billion parameters LLMs, indirectly supervised methods improve zero-shot IE by utilizing the knowledge learned from tasks like Textual Entailment~\\citep{sainz-etal-2021-label, sainz-etal-2022-textual, sainz-etal-2022-zs4ie} and Question Answering~\\citep{levy-etal-2017-zero}.\n\\cite{obeidat-etal-2019-description} propose an entity typing method that encodes label descriptions from Wikipedia as embeddings using an LSTM, which is then used to score the inputs. Methods that leveraged external knowledge were also successful on fine-grained zero-shot NER~\\citep{chen-etal-2021-empirical}.\n\\cite{DBLP:conf/acl/0001LDXLHSW22} introduced a unified text-to-structure generation that can model different IE tasks universally. \\cite{DBLP:conf/aaai/Lou0DJLH0023} proposed converting IE tasks to a semantic matching problem, allowing their method to generalize  to new domains and label ontologies not seen during training. \\cite{DBLP:journals/corr/abs-2304-08085} framed IE tasks as natural language descriptive instructions and trained an LLM across a diverse range of IE tasks. In evaluations on tasks with unseen label ontologies, their model outperformed other instruction-tuning methods. \n\nMost instruction tuning attempts for IE share a limitation: they only consider label names in the prompts (e.g., \\textit{\"List all the Persons\"}). This poses two major challenges. Firstly, not all datasets share the same definition for labels like \\textit{Person} (some exclude fictional characters or pronouns). Secondly, a label name alone doesn't sufficiently describe complex or less common labels. While there have been attempts to prompt LLMs using guidelines~\\citep{DBLP:journals/corr/abs-2305-12217}, strong prior knowledge of LLMs regarding task labels \\citep{DBLP:conf/acl/BlevinsGZ23} deter the model from adhering to those guidelines. \n\n\\section{Approach}\n\nDifferent from previous approaches, \\GoLLIE GoLLIE forces the model to attend to the details in the guidelines, performing robustly on schemas not seen during training. On this section we deep dive into the details of our approach, describing how the input and output was represented and the regularization techniques used to force the model to attend to the guidelines.\n\n\\subsection{Input-output representation}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=.8712\\linewidth]{images/input-output_representation.pdf}\n    \\caption{Example of the input and output of the model.}\n    \\label{fig:input-output}\n    \\vspace{-0.8em}\n\\end{figure}\n\nWe have adopted a Python code-based representation~\\citep{wang-etal-2023-code4struct, li-etal-2023-codeie} for both the input and output of the model. This approach not only offers a clear and human-readable structure but also addresses several challenges typically associated with natural language instructions. It enables the representation of any information extraction task under a unified format. The inputs can be automatically standardized using Python code formatters such as Black. The output is well-structured and parsing it is trivial. Furthermore, most current LLMs incorporate code in their pre-training datasets, indicating that these models are already familiar with this representation.\n\nFigure~\\ref{fig:input-output} shows the three main parts of the format: schema definition, input text, and output annotations. \\textbf{Schema definition} forms the initial segment of the input. This section contains information about the labels that are represented as Python classes; guidelines, articulated as docstrings; and representative annotation candidates presented in the form of code comments. The number of class definitions corresponds to the number of labels in the dataset. Classes are flexible and vary for each task. For example, classes for a NER dataset merely require an attribute to specify the text span that corresponds to the class. On the other side, more complex tasks such as Event Argument Extraction (EAE) or Slot Filling (SF) demand more class attributes to categorize the task, such as a list of participants in an event (refer to examples in Appendix~\\ref{ap:repr_examples}). \\textbf{Input text} is the second part of the input. The input text is represented as a string variable in Python. \\textbf{Output annotations} is the part generated by the model. The model starts generating after \\texttt{result =}. The annotations are represented as a list of instances of the classes defined on the schema definition part. Parsing the output is straightforward; executing the generated code in Python yields a list containing the result. This ease of parsing the output stands as a significant advantage of our model. A further detailed analysis of the efficiency of this approach is available in Appendix \\ref{sec:scalability}. \n\n\\subsection{Guidelines enhanced representation}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.85\\linewidth]{images/guideline_example.pdf}\n    \\caption{Example of the input representation. (left) An example of an event definition w/o guidelines information. (right) The same example but with guideline information as Python comments.}\n    \\vspace{-0.7em}\n    \\label{fig:guidelines-example}\n\\end{figure}\n\nThe main contribution of this work is the use of the guidelines as part of the inference process to improve the zero-shot generalization. An example of a class definition with and without guidelines is shown in Figure~\\ref{fig:guidelines-example}. Different datasets usually define guidelines in many different ways: some provide a complex definition of a label with several exceptions and special treatments and others just give a few representative candidates of the fillers of the label. To normalize the input format, we included the label definitions as class docstrings and the candidates as a comment for the principal argument (which is usually \\textit{mention} or \\textit{span}). Complex tasks such as EAE or SF require additional definitions for the arguments or slots, to that end,  we included small definitions as comments on each class argument. \nIn this paper, we will refer to the model without guidelines as Baseline and the model with guidelines as \\GoLLIET.\n\n\\subsection{Training regularization}\n\\label{sec:components}\n\nWe want to ensure that the model follows the guidelines and does not just learn to identify specific datasets and perform correctly on them. To do this, we introduce various kinds of noise during training. This stops the model from recognizing particular datasets, recalling specific labels, or attending only to the label names rather than learning to follow the actual description for each label in the guidelines.\n\nWe applied the following regularizations. \\textbf{Class order shuffling}, for each example, the order of the input classes is randomly shuffled. This makes it more difficult for the model to memorize entire task definitions. \\textbf{Class dropout}, we delete some of the input classes randomly. By eliminating few classes from both the input and output, we force the model to learn to only output instances of classes defined in the input. This not only encourages the model to focus on the schema definition but also minimizes the occurrence of hallucinations during inference. \\textbf{Guideline paraphrasing}, we generate variations of the label definitions to prevent the model from easily memorizing them. We also think this will make the method more robust to different variations on the definition. \\textbf{Representative candidate sampling}, similar to what we do with the paraphrases, for each input we sample 5 different candidates from a fixed pool of 10 per class. \\textbf{Class name masking} involves substituting the label class names (e.g., \\textsc{Person}) with placeholders, such as \\textsc{LABEL\\_1}. This prevents the model from exploiting the label names during training and forces it to attend and understand the guidelines. \n\n\\section{Experimental Setup}\n\n\\subsection{Data}\n\nEvaluating zero-shot capabilities requires dividing the data into training and evaluation datasets. However, many benchmarks for Information Extraction are based on the same domain or share part of their schema. To ensure that the zero-shot evaluation is not affected by similar data, we have divided our set of benchmarks based on the domain of the data (a related topic is data contamination, which we discuss in Appendix~\\ref{ap:contamination}). For training we kept mostly datasets from \\textbf{News and Biomedical} domains, for evaluation instead, we used datasets from \\textbf{diverse domains}. This approach helps to avoid introducing any noise into the evaluation process. Among the evaluation datasets we included CrossNER~\\citep{liu2021crossner}, a dataset that is split into many domains, for simplicity, we will call each domain as a separate dataset: AI, Literature, Music, Politics, and Science. Also, we will refer to MIT Movie and MIT Restaurant as Movie and Restaurant. Table~\\ref{tab:datasets} contains the information about the data used in the experiments. \n\n\\begin{table}\n    \\centering\n    \\caption{Datasets used on the experiments. The table shows the domain, tasks and whether are use for training, evaluation or both.}\n    \\resizebox{0.9\\textwidth}{!}{\n        \\begin{tabular}{l|l|ccccc|cc}\n            \\multicolumn{8}{c}{} \\\\\n            \\toprule\n            \\textbf{Dataset} & \\textbf{Domain} & \\textbf{NER} & \\textbf{RE} & \\textbf{EE} & \\textbf{EAE} & \\textbf{SF} & \\textbf{Training} & \\textbf{Evaluation} \\\\\n            \\midrule\n            ACE05 \\citep{ACE} & News & \\checkmark & \\checkmark & \\checkmark & \\checkmark & & \\checkmark & \\checkmark \\\\\n            BC5CDR~\\citep{bc5cdr} & Biomedical & \\checkmark & & & & & \\checkmark & \\checkmark \\\\\n            CoNLL 2003~\\citep{tjong-kim-sang-de-meulder-2003-introduction} & News & \\checkmark & & & & & \\checkmark & \\checkmark \\\\\n            DIANN~\\citep{diann} & Biomedical & \\checkmark & & & & & \\checkmark & \\checkmark \\\\\n            NCBIDisease~\\citep{islamaj-dogan-lu-2012-improved} & Biomedical & \\checkmark & & & & & \\checkmark & \\checkmark \\\\\n            Ontonotes 5~\\citep{pradhan-etal-2013-towards} & News & \\checkmark & & & & & \\checkmark & \\checkmark \\\\\n            RAMS~\\citep{ebner-etal-2020-multi} & News & & & & \\checkmark & & \\checkmark & \\checkmark \\\\\n            TACRED~\\citep{zhang-etal-2017-position} & News & & & & & \\checkmark & \\checkmark & \\checkmark \\\\\n            WNUT 2017~\\citep{derczynski-etal-2017-results} & News & \\checkmark & & & & & \\checkmark & \\checkmark \\\\\n            \\midrule\n            BroadTwitter~\\citep{derczynski-etal-2016-broad} & Twitter & \\checkmark & & & & & & \\checkmark \\\\\n            CASIE~\\citep{casie} & Cybercrime & & & \\checkmark & \\checkmark & & & \\checkmark \\\\\n            CrossNER~\\citep{liu2021crossner} & \\textit{Many} & \\checkmark & & & & & & \\checkmark \\\\\n            E3C~\\citep{Magnini2021} & Biomedical & \\checkmark & & & & & & \\checkmark \\\\\n            FabNER~\\citep{fabner}& Science & \\checkmark & & & & & & \\checkmark \\\\\n            HarveyNER~\\citep{chen-etal-2022-crossroads} & Twitter & \\checkmark & & & & & & \\checkmark \\\\\n            MIT Movie~\\citep{DBLP:conf/icassp/LiuPCG13} & Queries & \\checkmark & & & & & & \\checkmark \\\\\n            MIT Restaurants~\\citep{DBLP:conf/icassp/LiuPCG13} & Queries & \\checkmark & & & & & & \\checkmark \\\\\n            MultiNERD~\\citep{tedeschi-navigli-2022-multinerd} & Wikipedia & \\checkmark & & & & & & \\checkmark \\\\\n            WikiEvents\\citep{li-etal-2021-document} & Wikipedia & \\checkmark & & \\checkmark & \\checkmark & & & \\checkmark \\\\\n            \\bottomrule\n        \\end{tabular}\n    }\n    \\vspace{-0.4em}\n    \n    \\label{tab:datasets}\n\\end{table}\n\nWe have trained the model to perform 5 different tasks: Named Entity Recognition (NER), Relation Extraction (RE), Event Extraction (EE), Event Argument Extraction (EAE), and Slot Filling (SF). However,  we only evaluated the model on the three main tasks of interest: NER, EE, and EAE. The other two tasks are added to the training data to add diversity and improve the flexibility of the model.\n\nA few modifications have been made to two datasets to improve the quality of the model. First, the training data of Ontonotes 5 was reduced drastically as it was automatically annotated. Second, the TACRED dataset was converted from RE to SF to increase the complexity of the task. These modifications make our system not comparable with the state of the art on those tasks. However, our focus of interest is on the zero-shot evaluation and, therefore, the benefits (see Appendix~\\ref{ap:repr_examples}) are more interesting than adding 2 more comparable points on the supervised setup. In the CASIE dataset, we detected that the annotated event spans are inconsistent. The models typically annotate a sub-string rather than the entire span. Therefore, we evaluate all the models based on the predicted event categories, without considering the exact text span. For arguments, we use partial matching.\n\nWe use the guidelines released by the authors of each dataset (More details in Appendix \\ref{sec:human_efford}). When such guidelines are not publicly available, we ask human experts to create them, based on the annotations from the development split. The representative candidates are extracted from the guidelines when available, otherwise, the candidates are sampled from the the train split based on word frequency or manually curated based on the guidelines. Paraphrases are automatically generated using Vicuna 33B v1.3 \\citep{DBLP:journals/corr/abs-2306-05685}. \n\n\\subsection{Language Models and Baselines}\n\n\\paragraph{Backbone LLMs:} \\GoLLIE GoLLIE is a fine-tuned version of Code-LLaMA \\cite{DBLP:journals/corr/abs-2308-12950}. Other backbone LLMs, such as LLaMA~\\citep{DBLP:journals/corr/abs-2302-13971}, LLaMA-2 \\cite{DBLP:journals/corr/abs-2307-09288} or Falcon \\cite{DBLP:journals/corr/abs-2306-01116} were considered during the development, however, as our approach uses code to represent the input and output, Code-LLaMA model worked better on the preliminary experiments. In order to perform fair comparisons the baseline developed in this paper is based on Code-LLaMA as well. All the development of this paper was done with the 7B parameter version of Code-LLama, but, for a scaling analysis, we also trained the 13B and 34B parameter models.\n\n\\paragraph{Training setup:} To train the models we use QLoRA~\\citep{lora, qlora}. LoRA freezes the pre-trained model weights and injects trainable rank decomposition matrices into linear layers of the Transformer architecture. In a preliminary experiment, this setup outperformed fine-tuning the entire model on the zero-shot tasks, while training much faster (more details in Appendix \\ref{ap:lora_full_model_finetuning}). We applied the LoRA to all linear transformer block layers as recommended by \\cite{qlora}. The models were trained for 3 epochs with an effective batch size of 32 and a learning rate of 3e-4 with a cosine scheduler. Our training infrastructure was 2 NVIDIA's A100 with 80gb each. More details about the training are given in the Appendix~\\ref{ap:extended_training_details}.\n\n\\paragraph{Comparable systems:} Our main point of comparison is Instruct-UIE~\\citep{DBLP:journals/corr/abs-2304-08085} as it is the approach closest to our system, but does not use guidelines. Another system considered for comparison is PromptNER \\citep{DBLP:journals/corr/abs-2305-12217}, which proposes to prompt GPT-3.5 and T5 with definitions using Chain-of-Though in order to perform few-shot NER. Different from us, they did not fine-tune the model to attend to the guidelines. For fair comparison, we only considered the zero-shot results reported in the paper. In addition, other SoTA systems are added for comparison when results from Instruct-UIE and PromptNER are not available. Given that our system is designed for the zero-shot scenario, the supervised experiments are intended to verify that our system does not degrade its performance. Thus we selected, for the supervised scenario, those systems among SoTA that share the most comparable setting with us. \n\n\\section{Results}\n\n\\subsection{Supervised evaluation}\n\n\\begin{table}\n    \\centering\n    \\caption{Supervised evaluation results. ``*\" indicates that results are not directly comparable.}\n    \\resizebox{0.77\\textwidth}{!}{\n        \\begin{tabular}{l|r|cc|cc}\n            \\multicolumn{6}{c}{} \\\\\n            \\toprule\n            \\textbf{Dataset} & \\textbf{SoTA} & \\textbf{Baseline} & \\textbf{\\GoLLIE} & \\textbf{\\GoLLIE 13B} & \\textbf{\\GoLLIE 34B} \\\\\n            \\midrule\n            ACE05\\textsubscript{NER} & \\citep{DBLP:journals/corr/abs-2304-08085} 86.6 & 89.1${\\scriptscriptstyle\\pm}$\\tiny{0.2} & 88.1${\\scriptscriptstyle\\pm}$\\tiny{0.6} & 89.4${\\scriptscriptstyle\\pm}$\\tiny{0.2} & \\textbf{89.6}${\\scriptscriptstyle\\pm}$\\tiny{0.1}\\\\\n            \n            ACE05\\textsubscript{RE} & \\citep{lu-etal-2022-unified} 66.1 & 63.8${\\scriptscriptstyle\\pm}$\\tiny{0.6} & 63.6${\\scriptscriptstyle\\pm}$\\tiny{1.8} & 67.5${\\scriptscriptstyle\\pm}$\\tiny{0.5} & \\textbf{70.1}${\\scriptscriptstyle\\pm}$\\tiny{1.5} \\\\\n            \n            ACE05\\textsubscript{EE} & \\citep{lu-etal-2022-unified} \\textbf{73.4} & 71.7${\\scriptscriptstyle\\pm}$\\tiny{0.2} & 72.2${\\scriptscriptstyle\\pm}$\\tiny{0.8} & 70.9${\\scriptscriptstyle\\pm}$\\tiny{1.6} & 71.9${\\scriptscriptstyle\\pm}$\\tiny{1.1} \\\\\n            \n            \n            ACE05\\textsubscript{EAE} & \\citep{lu-etal-2022-unified} *54.8 & 65.9${\\scriptscriptstyle\\pm}$\\tiny{0.7} & 66.0${\\scriptscriptstyle\\pm}$\\tiny{0.8} & 67.8${\\scriptscriptstyle\\pm}$\\tiny{0.9} & \\textbf{68.6}${\\scriptscriptstyle\\pm}$\\tiny{1.2} \\\\\n            \n            BC5CDR & \\citep{zhang2023optimizing} \\textbf{91.9} & 87.5${\\scriptscriptstyle\\pm}$\\tiny{0.2} & 87.5${\\scriptscriptstyle\\pm}$\\tiny{0.2} & 87.9${\\scriptscriptstyle\\pm}$\\tiny{0.1} & 88.4${\\scriptscriptstyle\\pm}$\\tiny{0.2} \\\\\n            \n            CoNLL 2003 & \\citep{lu-etal-2022-unified} 93.0 & 92.9${\\scriptscriptstyle\\pm}$\\tiny{0.1} & 92.8${\\scriptscriptstyle\\pm}$\\tiny{0.3} & 93.0${\\scriptscriptstyle\\pm}$\\tiny{0.2} & \\textbf{93.1}${\\scriptscriptstyle\\pm}$\\tiny{0.1} \\\\\n            \n            DIANN & \\citep{zabala2018hybrid} 74.8 & 80.3${\\scriptscriptstyle\\pm}$\\tiny{0.7} & 79.4${\\scriptscriptstyle\\pm}$\\tiny{1.1} & 82.6${\\scriptscriptstyle\\pm}$\\tiny{1.3} & \\textbf{84.1}${\\scriptscriptstyle\\pm}$\\tiny{1.1} \\\\\n            \n            NCBIDisease & \\citep{DBLP:journals/corr/abs-2304-08085} \\textbf{90.2} & 86.2${\\scriptscriptstyle\\pm}$\\tiny{0.1} & 85.4${\\scriptscriptstyle\\pm}$\\tiny{0.3} & 86.5${\\scriptscriptstyle\\pm}$\\tiny{0.8} & 85.8${\\scriptscriptstyle\\pm}$\\tiny{0.2} \\\\\n            \n            Ontonotes 5 & - & 83.4${\\scriptscriptstyle\\pm}$\\tiny{0.2} & 83.4${\\scriptscriptstyle\\pm}$\\tiny{0.2} & 84.0${\\scriptscriptstyle\\pm}$\\tiny{0.2} & \\textbf{84.6}${\\scriptscriptstyle\\pm}$\\tiny{0.4} \\\\\n            \n            RAMS & \\citep{li-etal-2021-document} 48.6 & 48.9${\\scriptscriptstyle\\pm}$\\tiny{0.4} & 48.7${\\scriptscriptstyle\\pm}$\\tiny{0.7} & 49.6${\\scriptscriptstyle\\pm}$\\tiny{0.1} & \\textbf{51.2}${\\scriptscriptstyle\\pm}$\\tiny{0.3} \\\\\n            \n            TACRED & - & 56.6${\\scriptscriptstyle\\pm}$\\tiny{0.2} & 57.1${\\scriptscriptstyle\\pm}$\\tiny{0.9} & 56.7${\\scriptscriptstyle\\pm}$\\tiny{0.5} & \\textbf{58.7}${\\scriptscriptstyle\\pm}$\\tiny{0.2} \\\\\n            \n            WNUT 2017 & \\citep{wang-etal-2021-improving} \\textbf{60.2} & 53.7${\\scriptscriptstyle\\pm}$\\tiny{0.7} & 52.0${\\scriptscriptstyle\\pm}$\\tiny{0.6}  & 50.5${\\scriptscriptstyle\\pm}$\\tiny{0.9} & 54.3${\\scriptscriptstyle\\pm}$\\tiny{0.4} \\\\\n            \n            \\midrule\n            Average & & 73.3${\\scriptscriptstyle\\pm}$\\tiny{0.1}  & 73.0${\\scriptscriptstyle\\pm}$\\tiny{0.3} & 73.9${\\scriptscriptstyle\\pm}$\\tiny{0.3} & \\textbf{75.0}${\\scriptscriptstyle\\pm}$\\tiny{0.3} \\\\\n            \\bottomrule\n        \\end{tabular}\n    }\n    \\label{tab:main-results}\n    \\vspace{-.55em}\n\\end{table}\n\nThe results on the supervised datasets are shown in Table~\\ref{tab:main-results}. Comparing GoLLIE with the baseline, they both obtain very similar results, with an absolute difference of 0.3 F1 points on average. This is expected, as the baseline model implicitly learns the guidelines for annotating the datasets based on the data distribution during fine-tuning. In addition, despite the noise introduced to GoLLIE fine-tuning in order to generalize from guidelines, the performance is close to that of the baseline.\n\nCompared to other systems our model achieves similar results in general. Focusing on the two datasets where our model under-performs significantly, WNUT and NCBIDisease, we find that task-specific techniques are still needed. For instance,  \\cite{wang-etal-2021-improving} uses external knowledge to detect emergent and rare entities. In the NCBIDisisease dataset, models pre-trained on Biomedical domain corpora achieve the best results \\citep{10.1007/978-3-030-68763-2_48}. \\citep{DBLP:journals/corr/abs-2304-08085} leverages Flan-T5, which has great proficiency on Biomedical domain tasks \\citep{DBLP:journals/corr/abs-2212-13138}.  These improvements, however, are complementary to our proposal.  \n\n\\subsection{Zero-Shot evaluation}\n\\label{sec:zero-shot}\n\\begin{table}\n    \\centering\n    \\caption{Zero-shot evaluation results. ``*\" indicates results obtained using the original code.}\n    \\resizebox{0.75\\textwidth}{!}{\n        \\begin{tabular}{l|r|cc|cc}\n            \\multicolumn{6}{c}{} \\\\\n            \\toprule\n            \\textbf{Dataset} & \\textbf{SoTA} & \\textbf{Baseline} & \\textbf{\\GoLLIE} & \\textbf{\\GoLLIE 13B} & \\textbf{\\GoLLIE 34B} \\\\\n            \\midrule\n            BroadTwitter & - & 39.0${\\scriptscriptstyle\\pm}$\\tiny{0.6} & 49.5${\\scriptscriptstyle\\pm}$\\tiny{0.8} & \\textbf{51.4}${\\scriptscriptstyle\\pm}$\\tiny{1.8} & 50.3${\\scriptscriptstyle\\pm}$\\tiny{2.1} \\\\\n            \n            CASIE\\textsubscript{EE} & - & 33.9${\\scriptscriptstyle\\pm}$\\tiny{6.5} & 59.3${\\scriptscriptstyle\\pm}$\\tiny{2.3} & 62.2${\\scriptscriptstyle\\pm}$\\tiny{0.9} & \\textbf{65.5}${\\scriptscriptstyle\\pm}$\\tiny{1.8} \\\\\n            \n            CASIE\\textsubscript{EAE} & -& 47.9${\\scriptscriptstyle\\pm}$\\tiny{1.4} & 50.0${\\scriptscriptstyle\\pm}$\\tiny{1.1} & 52.6${\\scriptscriptstyle\\pm}$\\tiny{0.2} & \\textbf{55.2}${\\scriptscriptstyle\\pm}$\\tiny{0.5} \\\\\n            \n            AI & \\citep{DBLP:journals/corr/abs-2304-08085} 49.0 & 32.3${\\scriptscriptstyle\\pm}$\\tiny{0.8} & 59.1${\\scriptscriptstyle\\pm}$\\tiny{1.1} & 56.7${\\scriptscriptstyle\\pm}$\\tiny{3.0} & \\textbf{61.6}${\\scriptscriptstyle\\pm}$\\tiny{1.9} \\\\\n            \n            Literature & \\citep{DBLP:journals/corr/abs-2304-08085} 47.2 & 39.4${\\scriptscriptstyle\\pm}$\\tiny{0.7} & \\textbf{62.7}${\\scriptscriptstyle\\pm}$\\tiny{3.2} & 59.7${\\scriptscriptstyle\\pm}$\\tiny{0.3} & 59.1${\\scriptscriptstyle\\pm}$\\tiny{2.6} \\\\\n            \n            Music & \\citep{DBLP:journals/corr/abs-2304-08085} 53.2 & 56.2${\\scriptscriptstyle\\pm}$\\tiny{1.3} & 67.8${\\scriptscriptstyle\\pm}$\\tiny{0.2} & 65.5${\\scriptscriptstyle\\pm}$\\tiny{3.6} & \\textbf{68.4}${\\scriptscriptstyle\\pm}$\\tiny{2.1} \\\\\n            \n            Politics & \\citep{DBLP:journals/corr/abs-2304-08085} 48.2 & 38.3${\\scriptscriptstyle\\pm}$\\tiny{1.1} & 57.2${\\scriptscriptstyle\\pm}$\\tiny{1.0} & 54.4${\\scriptscriptstyle\\pm}$\\tiny{4.1} & \\textbf{60.2}${\\scriptscriptstyle\\pm}$\\tiny{3.0} \\\\\n            \n            Science & \\citep{DBLP:journals/corr/abs-2304-08085} 49.3 & 37.1${\\scriptscriptstyle\\pm}$\\tiny{1.3} & 55.5${\\scriptscriptstyle\\pm}$\\tiny{1.6} & 56.2${\\scriptscriptstyle\\pm}$\\tiny{1.0} & \\textbf{56.3}${\\scriptscriptstyle\\pm}$\\tiny{0.4} \\\\\n            \n            E3C & - & 59.8${\\scriptscriptstyle\\pm}$\\tiny{0.3} & 59.0${\\scriptscriptstyle\\pm}$\\tiny{0.7} & 59.0${\\scriptscriptstyle\\pm}$\\tiny{0.8} & \\textbf{60.0}${\\scriptscriptstyle\\pm}$\\tiny{0.4} \\\\\n            \n            FabNER & - & 06.1${\\scriptscriptstyle\\pm}$\\tiny{0.4} & 24.8${\\scriptscriptstyle\\pm}$\\tiny{0.6} & 25.4${\\scriptscriptstyle\\pm}$\\tiny{0.5} & \\textbf{26.3}${\\scriptscriptstyle\\pm}$\\tiny{0.4} \\\\\n            \n            HarveyNER & - & 23.2${\\scriptscriptstyle\\pm}$\\tiny{0.4} & 37.3${\\scriptscriptstyle\\pm}$\\tiny{1.8} & \\textbf{41.3}${\\scriptscriptstyle\\pm}$\\tiny{0.8} & 38.9${\\scriptscriptstyle\\pm}$\\tiny{0.5} \\\\\n            \n            Movie & \\citep{DBLP:journals/corr/abs-2304-08085} 63.0 & 43.4${\\scriptscriptstyle\\pm}$\\tiny{1.1} & \\textbf{63.0}${\\scriptscriptstyle\\pm}$\\tiny{0.6} & 62.5${\\scriptscriptstyle\\pm}$\\tiny{1.0} & 62.4${\\scriptscriptstyle\\pm}$\\tiny{1.4} \\\\\n            \n            Restaurants & \\citep{DBLP:journals/corr/abs-2304-08085} 21.0 & 31.3${\\scriptscriptstyle\\pm}$\\tiny{2.2}  & 43.4${\\scriptscriptstyle\\pm}$\\tiny{0.8} & 49.8${\\scriptscriptstyle\\pm}$\\tiny{1.4} & \\textbf{52.7}${\\scriptscriptstyle\\pm}$\\tiny{1.6} \\\\\n            \n            MultiNERD & - & 55.0${\\scriptscriptstyle\\pm}$\\tiny{1.1} & 76.0${\\scriptscriptstyle\\pm}$\\tiny{0.7} & \\textbf{77.5}${\\scriptscriptstyle\\pm}$\\tiny{0.3} & 77.2${\\scriptscriptstyle\\pm}$\\tiny{0.6} \\\\\n            \n            WikiEvents\\textsubscript{NER} & \\citep{sainz-etal-2022-zs4ie} *49.1 & 76.9${\\scriptscriptstyle\\pm}$\\tiny{5.1} & 80.7${\\scriptscriptstyle\\pm}$\\tiny{0.7} & 80.2${\\scriptscriptstyle\\pm}$\\tiny{0.7} & \\textbf{81.3}${\\scriptscriptstyle\\pm}$\\tiny{0.5} \\\\\n            \n            WikiEvents\\textsubscript{EE} & \\citep{sainz-etal-2022-zs4ie} *10.4 & 47.5${\\scriptscriptstyle\\pm}$\\tiny{0.4} & 43.0${\\scriptscriptstyle\\pm}$\\tiny{0.6} & 45.7${\\scriptscriptstyle\\pm}$\\tiny{0.8} & \\textbf{47.0}${\\scriptscriptstyle\\pm}$\\tiny{1.9} \\\\\n            \n            WikiEvents\\textsubscript{EAE} & \\cite{sainz-etal-2022-textual} 35.9 & 51.6${\\scriptscriptstyle\\pm}$\\tiny{0.5} & 51.9${\\scriptscriptstyle\\pm}$\\tiny{0.4} & \\textbf{52.5}${\\scriptscriptstyle\\pm}$\\tiny{1.2} & 50.7${\\scriptscriptstyle\\pm}$\\tiny{0.4} \\\\\n            \n            \\midrule\n            Average SoTA & 42.6 & 45.4${\\scriptscriptstyle\\pm}$\\tiny{0.5} & 58.4${\\scriptscriptstyle\\pm}$\\tiny{0.5} & 58.3${\\scriptscriptstyle\\pm}$\\tiny{0.7} & \\textbf{60.0}${\\scriptscriptstyle\\pm}$\\tiny{1.0} \\\\\n            \n            Average all & - & 42.3${\\scriptscriptstyle\\pm}$\\tiny{0.2} & 55.3${\\scriptscriptstyle\\pm}$\\tiny{0.2} & 56.0${\\scriptscriptstyle\\pm}$\\tiny{0.2} & \\textbf{57.2}${\\scriptscriptstyle\\pm}$\\tiny{0.5} \\\\\n            \\bottomrule\n        \\end{tabular}\n    }\n    \\label{tab:zero-results}\n    \\vspace{-.5em}\n\\end{table}\n\nThe results on the zero-shot are shown in Table~\\ref{tab:zero-results}. Overall, compared to the baseline, \\textbf{the results are improved significantly when using guidelines} on almost every dataset, with an absolute difference of 13 F1 points on average. Despite dividing the evaluation benchmarks based on the domain, there is always some overlap between labels of train and evaluation benchmarks. For instance, the datasets E3C and WikiEvents share a large part of their schema with datasets like BC5CDR, ACE05, and RAMS. This phenomenon is reflected in the results.\n\nGoLLIE surpasses by a large margin the current zero-shot SoTA methods Instruct-UIE~\\citep{DBLP:journals/corr/abs-2304-08085} and Entailment based IE~\\citep{sainz-etal-2022-zs4ie}.\nCompared to Instruct-UIE, the main differences are the backbone model, the amount of training data, and, the use or not of the guidelines. Instruct-UIE leverages the 11B FlanT5 which is a T5 fine-tuned on 473 NLP datasets. With respect to the data, Instruct-UIE leverages a total of 34 IE datasets (counting different tasks as datasets) from diverse domains, we only leverage 12 datasets. Contrary to our method they do not use guideline information. Still, our method performs significantly better suggesting that the guidelines have an important effect on the results. \n\nPromptNER~\\citep{DBLP:journals/corr/abs-2305-12217} also adds some definition information into the prompt in order to perform zero-shot NER. We compare our approach with them (represented as GPT-3.5) in Figure~\\ref{fig:zero-shot-results}. Although their approach leverages guidelines too, our approach performs significantly better on all datasets, showing that LLMs (even with 175B parameters) struggle to follow guidelines. They solve this by adding examples in the context but are still far behind on a comparable setting (T5-XXL).\n\n\\begin{wrapfigure}{r}{0.42\\textwidth}\n    \\centering\n    \\vspace{-1.5em}\n    \\includegraphics[width=1.0\\linewidth]{images/barplot.pdf}\n    \\vspace{-1.0em}\n    \\caption{Seen vs unseen label zero-shot performance, results aggregated from all datasets.}\n    \\label{fig:seen_vs_unseen}\n\\end{wrapfigure}\n\n\\paragraph{Seen vs unseen labels:} Not all labels in the zero-shot datasets are unseen; there is an overlap between the labels in the training and zero-shot datasets. Although these labels may have very different annotation guidelines, we also report results on the set of labels to which it has not been exposed during training, to better understand the generalization capabilities of GoLLIE. The list of seen and unseen labels, as well as an extended analysis is available in Appendix \\ref{sec:seen_vs_unseen}. Figure \\ref{fig:seen_vs_unseen} aggregates the F1 scores across datasets for seen and unseen labels in the zero-shot scenario. All models exhibit slightly lower performance on unseen labels. For the baseline model, the performance drop is more pronounced. In contrast, GoLLIE demonstrates better generalization ability, showing a smaller gap in F1 scores between the seen and unseen labels. Also, the gap is smaller as the parameter count of our model increases. %This indicates GoLLIE's enhanced capability in generalizing to new, unseen labels, underscoring its effectiveness in zero-shot scenarios.\n\n\\paragraph{Model scaling:} Recent research has shown that increasing the parameter count of language models leads to improved generalization capabilities \\cite{DBLP:conf/nips/BrownMRSKDNSSAA20}. Higher parameter count yields superior average zero-shot performance. However, some datasets and tasks greatly benefit from a larger LLM, while others do not. We believe that some datasets do not see benefits from increasing the LLM size because their performance is hindered by the issues with the guidelines that we discuss in Section \\ref{sec:Ablation}. While, in general, larger models achieve better results in both supervised and zero-shot settings, GoLLIE with a 7B parameter backbone already exhibits strong zero-shot capabilities. %As described in Appendix \\ref{ap:extended_training_details}, the 13B and 34B parameter versions require orders of magnitude more compute. Thus, one must carefully weigh the benefits of enhanced performance against the significantly increased computational costs and resources required\n\n\\subsection{Ablation study}\n\\label{sec:Ablation}\n\n\\begin{wraptable}{r}{0.4\\textwidth}\n    \\centering\n    \\vspace{-2.2em}\n    \\caption{Ablation results.}\n    \\adjustbox{max width=0.9\\linewidth}{\\begin{tabular}{l|cl}\n        \\multicolumn{2}{c}{} \\\\\n        \\toprule\n        \\textbf{Model} & \\multicolumn{1}{c}{\\textbf{F1}} & \\multicolumn{1}{c}{\\textbf{p-value}} \\\\\n        \\midrule\n        \\GoLLIET & 55.3${\\scriptscriptstyle\\pm}$\\tiny{0.2} & - \\\\\n        \\midrule\n        w/o Shuffling & 55.9${\\scriptscriptstyle\\pm}$\\tiny{0.2} & $7.2e^{-2}$ \\\\\n        w/o Paraphrases & 54.8${\\scriptscriptstyle\\pm}$\\tiny{0.2} & $1.1e^{-1}$ \\\\\n        w/o Masking & 54.6${\\scriptscriptstyle\\pm}$\\tiny{0.6} & $1.0e^{-1}$ \\\\\n        w/o Dropout & 54.0${\\scriptscriptstyle\\pm}$\\tiny{0.2} & $4.0e^{-3}$ \\\\\n        w/o Candidates & 49.9${\\scriptscriptstyle\\pm}$\\tiny{0.2} & $2.2e^{-10}$ \\\\\n        w/o \\textit{all} (baseline) & 42.3${\\scriptscriptstyle\\pm}$\\tiny{0.1} & $5.1e^{-13}$ \\\\\n        \\bottomrule\n    \\end{tabular}}\n    \\vspace{-.5em}\n    \\label{tab:ablation}\n\n\\end{wraptable}\n\nWe have performed an ablation to see the contribution of several components in the zero-shot evaluation. We analyzed the different regularization techniques proposed in Section~\\ref{sec:components}. Additionally, we represent the baseline, i.e. when removing all components including guidelines, as \"w/o \\textit{all}\".\nAlong with the mean zero-shot F1 we also provide the one-sided p-value with respect to \\GoLLIE GoLLIE. \n\nThe class order shuffling, guideline paraphrasing, and class name masking seem to have no significant contribution to the final result, while class dropout although significant improvements are small. As further explained in Appendix \\ref{ap:extended_training_details}, the loss is only computed over the result tokens, inherently limiting the model's potential to overfit to the guidelines. In contrast, the representative annotation items give a stronger signal to the model. We see how definitions and representative candidates from the guidelines are complementary and help to improve each other. \n\n\\section{Error analysis} \n\nIn this section, we aim to better understand the effect of prompting LLMs with guidelines. We focus on specific labels across various datasets, with the results displayed in Table \\ref{tab:error_analysis}. Our analysis covers both successful and unsuccessful cases of entity labeling by GoLLIE. For the latter, we also aim to identify the reasons why the model fails to correctly label these entities. Further analyses on malformed outputs or hallucinations are discussed in Appendix~\\ref{ap:hallucinations}.\n\n\\begin{table}[!htb]\n\\centering\n\\vspace{-1.0em}\n\\small\n    \\caption{This table shows the F1 scores for specific labels from different datasets. The guideline column is a small summary of the actual guideline used to prompt the model.}\n    \\adjustbox{max width=.90\\linewidth}{\\begin{tabular}{@{}llp{7.5cm}cc@{}}\n        \\multicolumn{4}{c}{} \\\\\n        \\toprule\n        \\multicolumn{1}{l}{\\textbf{Dataset}} & \\multicolumn{1}{l}{\\textbf{Label}} &  \\multicolumn{1}{l}{\\textbf{Guideline}} & \\textbf{Baseline} & \\GoLLIE \\\\ \\midrule\n        \\rowcolor{ForestGreen!10} MultiNERD & Media & Titles of films, books, songs, albums, fictional characters and languages. &  13.6 & 69.1 \\\\ \n        \\rowcolor{ForestGreen!10} \\rule{0pt}{2.25ex} CASIE & Vul. Patch & When a software company addresses a vulnerability by releasing an update. & 27.7 & 70.5 \\\\\n        \\rowcolor{ForestGreen!10} \\rule{0pt}{2ex} Movie & Trailer & Refers to a short promotional video or preview of a movie. & 00.0 & 76.4 \\\\\n        \\rowcolor{ForestGreen!10} \\rule{0pt}{2ex} AI & Task & Particular research task or problem within a specific AI research field.  & 02.7 & 63.9 \\\\ \n        \\rowcolor{CornflowerBlue!10} \\rule{0pt}{2ex} MultiNERD & Time & Specific and well-defined time intervals, such as eras, historical periods, centuries, years and important days. &  01.4 & 03.5 \\\\\n        \\rowcolor{Thistle!10} \\rule{0pt}{2ex} Movie & Plot & Recurring concept, event, or motif that plays a significant role in the development of a movie. & 00.4 & 05.1 \\\\\n        \\rowcolor{Thistle!10} \\rule{0pt}{2ex} AI & Misc & Named entities that are not included in any other category. & 01.1 & 05.2 \\\\\n        \\rowcolor{Thistle!10} \\rule{0pt}{2ex} Literature & Misc & Named entities that are not included in any other category.  &  03.7 & 30.8 \\\\\n        \\rowcolor{Thistle!10} \\rule{0pt}{2ex} Literature & Writer & Individual actively engaged in the creation of literary works. &   04.2 & 65.1 \\\\\n        \\rowcolor{Thistle!10} \\rule{0pt}{2ex} Literature & Person & Person name that is not a writer. & 33.5 & 49.4 \\\\\n        \\rowcolor{Thistle!10} \\rule{0pt}{2ex} Science & Scientist & A person who is studying or has expert knowledge of a natural science field.  &  02.1 & 05.8 \\\\\n        \\rowcolor{Thistle!10} \\rule{0pt}{2ex} Science & Person & Person name that is not a scientist. &   46.1 & 45.9 \\\\\n        \\rowcolor{Thistle!10} \\rule{0pt}{2ex} Politics & Polit. Party & Organization that compete in a particular country's elections. & 11.2 & 34.9 \\\\ \\bottomrule\n    \\end{tabular}}\n    \\label{tab:error_analysis}\n\\end{table}\n\n\\paragraph{The details are in the guidelines:} Labels such as \\textsc{Media}, \\textsc{VulnerabilityPatch}, \\textsc{Trailer}, and \\textsc{Task} are inherently polysemous, making it challenging to determine the appropriate categorization based solely on the label name. As a result, the baseline struggles to effectively classify items under these labels due to having insufficient information. Conversely, GoLLIE successfully follows the guidelines, underscoring their utility.\n\n\\paragraph{When the annotations do not comply with the guidelines:} In the case of the \\textsc{Time} label of the MultiNERD dataset, we found that our model labels years as \\textsc{Time} entities. This is correct according to the annotation guidelines. Surprisingly, years are not labeled as entities in the dataset. In this case, GoLLIE successfully follows the guidelines; unfortunately, the dataset annotations do not.\n\n\\paragraph{Ambiguous labels:} The \\textsc{Miscellaneous} category, used by CoNLL03 and CrossNER datasets, refers to any named entity that is not included in the predefined categories set by the dataset. This definition is highly ambiguous and serves as a catch-all for various elements that do not fit into any of the predefined categories. Similarly,  the \\textsc{Plot} category of the Movie dataset is used to label a wide range of elements. For example, events in a movie (e.g., murder, horse racing), characters (e.g., vampires, zombies), and the country of origin (e.g., British), among others. This lack of specificity hinders the development of consistent rules or guidelines for tagging such elements \\citep{ratinov-roth-2009-design}, which is a problem for humans and machines alike. As a consequence, GoLLIE also fails to label them accurately. \n\n\\paragraph{Conflicts Between Fine-Grained and Coarse Entities:} The CrossNER dataset introduces two labels for person names within each domain. For example, in the Science domain, the labels \\textsc{Scientist} and \\textsc{Person} are used. The former is used to label any person that is not a Scientist. Similarly, the Literature domain includes the labels \\textsc{Writer} and \\textsc{Person}. The guidelines assist GoLLIE in correctly labeling entities as \\textsc{Writer}. However, GoLLIE still categorizes individuals as \\textit{Person} even when they are \\textit{Scientist}, despite the guidelines. This is not technically incorrect, as every scientist is, by definition, also a person. \n\n\\paragraph{Strong Label Preconceptions:} In its Political domain set, CrossNER includes the label \\textsc{Political Party}. GoLLIE outperforms the baseline, once again demonstrating the utility of providing the model with guidelines. However, we often find that the model categorizes political parties as organizations. As listed in Table \\ref{tab:datasets}, most of the pre-training datasets originate from the news domain, where political parties are a common entity. However, none of the fine-tuning datasets include the \\textsc{Political Party} entity; they are instead categorized as \\textsc{Organization}. Consequently, during inference, the model consistently labels political parties as organizations. We believe this issue can be resolved by expanding the number and diversity of the fine-tuning datasets.\n\nIn summary, we anticipate that \\textbf{GoLLIE will perform well on labels with well-defined and clearly bounded guidelines}. On the other hand, ambiguous labels or very coarse labels pose challenges. In this regard, we believe that GoLLIE would benefit from learning to follow instructions such as \\textit{\"Label always the most specific class\"} or \\textit{\"Annotate this class in the absence of other specific class\"}. We also expect that GoLLIE would benefit from expanding the number and diversity of the pre-training datasets.\n\n\\section{Conclusions}\nIn this paper, we introduce \\GoLLIET, an LLM specifically fine-tuned to comply with annotation guidelines that were devised to help humans annotate the dataset. A comprehensive zero-shot evaluation empirically demonstrates that annotation guidelines are of great value for LLMs, as GoLLIE successfully leverages them. GoLLIE achieves better zero-shot results than previous attempts at zero-shot IE which do not leverage the guidelines, or use models not finetuned for following guidelines. \n\nGoLLIE is a significant progress towards the development of models that can generalize to unseen IE tasks. In the future, we plan to enhance GoLLIE by using a larger and more diverse set of pre-training datasets. We will also improve the model's performance with ambiguous and coarse labels by expanding the set of instructions that the model can follow. \n\n\\section*{Acknowledgments}\n\nThis work has been partially supported by %the HiTZ center and \nthe Basque Government (Research group funding IT-1805-22 and ICL4LANG project, grant no. KK-2023/00094). We are also thankful to several MCIN/AEI/10.13039/501100011033 projects: (i) DeepKnowledge (PID2021-127777OB-C21) and by FEDER, EU; (ii) Disargue (TED2021-130810B-C21) and European Union NextGenerationEU/PRTR; (iii) AWARE (TED2021-131617B-I00) and European Union NextGenerationEU/PRTR. This work has also been partially funded by the LUMINOUS project (HORIZON-CL4-2023-HUMAN-01-21-101135724). Oscar Sainz is supported by a doctoral grant from the Basque Government (PRE\\_2023\\_2\\_0137). Rodrigo Agerri currently holds the RYC-2017-23647 fellowship (MCIN/AEI/10.13039/501100011033 and by ESF Investing in your future).\n\n\\clearpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2304.08085v1.tex",
        "arXiv-2308.03279v2.tex",
        "arXiv-2310.03668v5.tex"
    ],
    "group_id": "group_78",
    "response": "### Summary of Recent Advances in Information Extraction with Large Language Models\n\n#### Title\nRecent Advances in Information Extraction with Large Language Models: InstructUIE, UniversalNER, and GoLLIE\n\n#### Introduction\nInformation Extraction (IE) is a fundamental task in Natural Language Processing (NLP) that involves the automatic identification and extraction of structured information from unstructured text. IE tasks include Named Entity Recognition (NER), Relation Extraction (RE), and Event Extraction (EE), among others. Historically, these tasks have been approached with specialized models designed for each task, often requiring extensive labeled data for training. However, recent advancements in Large Language Models (LLMs) have introduced new paradigms for IE, leveraging the multi-task capabilities of these models to handle a wide range of tasks with fewer resources. Despite these advancements, LLMs still face challenges in achieving high performance on IE tasks, particularly in zero-shot settings where they are evaluated on tasks without any prior training data.\n\nCurrent research focuses on enhancing the performance of LLMs on IE tasks through instruction tuning and targeted distillation. Instruction tuning involves training LLMs to follow natural language instructions, enabling them to perform specific tasks more accurately. Targeted distillation, on the other hand, aims to fine-tune smaller models to replicate the performance of larger LLMs on specific tasks, reducing computational costs while maintaining high accuracy. This summary examines three recent papers that explore these approaches: \"InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction,\" \"UniversalNER: targeted distillation from large language models for open named entity recognition,\" and \"GoLLIE: Annotation Guidelines Improve Zero-Shot Information Extraction.\"\n\n#### Main Content of Each Paper\n\n**InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction**\nInstructUIE proposes a unified framework for information extraction tasks using instruction tuning. The authors reformulate IE tasks as natural language generation problems, where the model is guided by task-specific instructions and options to generate structured outputs. The framework includes auxiliary tasks to capture common structure information and deepen the understanding of diverse semantics. For instance, entity span extraction and entity typing are introduced for NER, entity pair extraction and relationship identification for RE, and trigger extraction and argument extraction for EE.\n\nTo evaluate InstructUIE, the authors introduce IE INSTRUCTIONS, a benchmark of 32 diverse IE datasets in a unified text-to-text format. They conduct experiments in both supervised and zero-shot settings, comparing InstructUIE with existing models like Bert, UIE, and USM. In the supervised setting, InstructUIE achieves comparable performance to Bert, while in the zero-shot setting, it outperforms these models significantly. For example, on the NER task, InstructUIE outperforms Bert by over 5 points on eight datasets and by about 25 points on the broad Twitter dataset. On the RE task, InstructUIE achieves an average F1 score of 67.98% across eight datasets, surpassing UIE and USM on several datasets. For EE, InstructUIE outperforms UIE and USM on some datasets, demonstrating its effectiveness in capturing diverse semantic information.\n\n**UniversalNER: targeted distillation from large language models for open named entity recognition**\nUniversalNER explores targeted distillation to train smaller models for open-domain NER tasks. The authors use mission-focused instruction tuning to generate a diverse set of NER examples from broad-coverage unlabeled web text. They then fine-tune LLaMA, a smaller model, to replicate the capabilities of larger LLMs like ChatGPT. The key innovation is the use of diverse inputs to ensure the model generalizes well across different domains and entity types. The authors also introduce negative sampling and class dropout to prevent the model from overfitting to specific datasets or labels.\n\nUniversalNER is evaluated on a benchmark of 43 diverse NER datasets across nine domains, including biomedicine, programming, social media, law, and finance. The results show that UniversalNER outperforms existing instruction-tuned models like Alpaca and Vicuna by over 30 absolute F1 points on average. When compared to ChatGPT, UniversalNER achieves a higher F1 score on most datasets, with an average F1 score of 41.7% for the 7B version and 43.4% for the 13B version. The authors also conduct ablation studies to assess the impact of different components in their distillation approach, finding that class dropout and negative sampling significantly improve the model's performance.\n\n**GoLLIE: Annotation Guidelines Improve Zero-Shot Information Extraction**\nGoLLIE introduces a method for fine-tuning LLMs to follow annotation guidelines, improving their zero-shot performance on IE tasks. The authors use a Python code-based representation for both input and output, allowing the model to generate structured outputs that are easier to parse. They incorporate guidelines into the model's training process through various regularization techniques, such as class order shuffling, guideline paraphrasing, and class name masking. These techniques prevent the model from memorizing specific datasets or labels, encouraging it to learn the underlying guidelines for annotation.\n\nGoLLIE is evaluated on a diverse set of IE datasets, including NER, RE, EE, EAE, and SF tasks. The results show that GoLLIE outperforms the baseline model by an average of 13 F1 points on zero-shot datasets. For example, on the Broad Twitter dataset, GoLLIE achieves an F1 score of 49.5%, compared to 39.0% for the baseline. On the CASIE dataset, GoLLIE achieves an F1 score of 59.3% for EE, compared to 33.9% for the baseline. The authors also perform an ablation study to assess the contribution of different regularization techniques, finding that representative annotation candidates are the most effective component in improving the model's performance.\n\n#### Commonalities and Innovations\nAll three papers leverage the multi-task capabilities of LLMs to improve performance on IE tasks. InstructUIE reformulates IE tasks as natural language generation problems, using task-specific instructions and options to guide the model. UniversalNER uses targeted distillation to train smaller models for open-domain NER tasks, focusing on diverse inputs and negative sampling to improve generalization. GoLLIE introduces a Python code-based representation for input and output, incorporating annotation guidelines into the model's training process through various regularization techniques.\n\nThe key innovation in InstructUIE is the introduction of auxiliary tasks to capture common structure information and deepen the understanding of diverse semantics. UniversalNER's main contribution is the targeted distillation approach, which trains smaller models to replicate the capabilities of larger LLMs on NER tasks. GoLLIE's innovation lies in its use of annotation guidelines to improve zero-shot performance, addressing the challenge of label inconsistencies across different datasets.\n\n#### Comparison of Results and Discussion\nThe results of the three papers show that each method has its strengths and weaknesses. InstructUIE achieves comparable performance to Bert in supervised settings and outperforms state-of-the-art models and GPT-3.5 in zero-shot settings. UniversalNER outperforms existing instruction-tuned models by over 30 absolute F1 points on average, and even surpasses ChatGPT in zero-shot settings. GoLLIE demonstrates significant improvements in zero-shot performance, with an average F1 score of 55.3% for the 7B version and 57.2% for the 34B version, compared to 42.3% for the baseline.\n\nThe differences in results can be attributed to the specific approaches and datasets used in each study. InstructUIE uses a unified framework for various IE tasks, while UniversalNER focuses on NER tasks. GoLLIE incorporates annotation guidelines into the model's training process, addressing the challenge of label inconsistencies. The use of diverse inputs and negative sampling in UniversalNER, and the Python code-based representation in GoLLIE, contribute to their superior performance in zero-shot settings.\n\n#### Conclusion\nThe three papers demonstrate significant progress in leveraging LLMs for information extraction tasks. InstructUIE introduces a unified framework for IE tasks using instruction tuning and auxiliary tasks, achieving state-of-the-art results in zero-shot settings. UniversalNER explores targeted distillation to train smaller models for NER tasks, outperforming existing instruction-tuned models and ChatGPT in zero-shot settings. GoLLIE incorporates annotation guidelines into the model's training process, improving zero-shot performance on IE tasks.\n\nFuture research directions include expanding the set of pre-training datasets for GoLLIE to improve performance on ambiguous and coarse labels, and further exploring the use of auxiliary tasks in InstructUIE to enhance generalization across different IE tasks. Additionally, combining the strengths of these methods, such as the use of guidelines in GoLLIE and auxiliary tasks in InstructUIE, could lead to even more robust and accurate models for information extraction. The development of more efficient and effective distillation techniques, as proposed in UniversalNER, could also reduce the computational costs associated with training large language models for specific tasks."
}