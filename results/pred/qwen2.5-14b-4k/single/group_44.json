{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Think you have Solved Question Answering?\\\\\nTry ARC, the AI2 Reasoning Challenge}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nWe present a new question set, text corpus, and baselines assembled to encourage AI research in\nadvanced question answering.  Together, these constitute the AI2 Reasoning\nChallenge (\\ASC), which requires far more powerful knowledge and\nreasoning than previous challenges such as SQuAD or SNLI.\nThe \\ASC\\  question set is partitioned into a \\Challenge~Set and an \\Additional~Set, where the \\Challenge~Set\ncontains only questions answered incorrectly by both\na retrieval-based algorithm and a word co-occurence algorithm.\nThe dataset contains only natural, grade-school science questions (authored for human tests),\nand is the largest public-domain set of this kind (7,787 questions).\nWe test several baselines on the \\Challenge~Set, including leading neural\nmodels from the SQuAD and SNLI tasks, and find that none are\nable to significantly outperform a random baseline, reflecting the\ndifficult nature of this task. We are also releasing the \\ASC~Corpus, a corpus\nof \\BUSCSIZE~science sentences relevant to the task, and implementations\nof the three neural baseline models tested. Can your model perform\nbetter? We pose \\ASC~as a challenge to the community.\n\\end{abstract}\n\n\\section{Introduction}\n\nDatasets are increasingly driving progress in AI, resulting in impressive solutions\nto several question-answering (QA) tasks \\cite[e.g.,][]{rajpurkar2016squad,JoshiTriviaQA2017}.\nHowever, many of these datasets focused on retrieval-style tasks, where\nsurface-level cues alone were usually sufficient to identify an answer.\nThis has not encouraged progress on questions requiring reasoning, use of commonsense\nknowledge, or other advanced methods for deeper text comprehension.\nThe challenge presented here, called \\ASC~(AI2\nReasoning Challenge), aims to address this limitation by posing questions\nthat are hard to answer with simple baselines.\n\nThe \\ASC~Dataset consists of a collection of 7787 {\\it natural science questions}, namely questions\nauthored for use on standardized tests. Standardized\ntests have previously been proposed as a Grand Challenge for\nAI \\cite{darpa,aimagazine2016} as they involve a wide variety of\nlinguistic and inferential phenomena, have varying levels\nof difficulty, and are measurable, motivating, and ambitious.\nHowever, making this challenge a reality\nis difficult, as such questions are difficult\nobtain (most examination boards release only limited practice\ntests to the public). For \\ASC~we have addressed this \nthrough several months of extensive search and investigation.\n\nIn addition, to encourage focus on advanced phenomena, we have\npartitioned \\ASC~into a \\Challenge~Set (2590 questions), containing questions answered\nincorrectly by both a retrieval-based algorithm and a word co-occurrence\nalgorithm, and an \\Additional~Set (5197 questions), containing the remainder.\nFor example, two typical challenge questions are:\n\\begin{quote}\n  {\\it Which property of a mineral can be determined just by looking at it? (A) luster {\\bfit [correct]} (B) mass (C) weight (D) hardness} \\\\\n  \\\\\n {\\it A student riding a bicycle observes that it moves faster on a smooth road than on a rough road. This happens because the smooth road has (A) less gravity (B) more gravity (C) less friction {\\bfit [correct]} (D) more friction}\n\\end{quote}\n\\vspace{1mm}\nBoth these questions are difficult to answer via simple retrieval or word correlation. For example, there are no Web sentences of the form ``luster can be determined by looking at something''; similarly, ``mineral'' is strongly correlated with ``hardness'' (an incorrect answer option). Rather, they require more advanced QA methods. We provide more example questions, and a catagorization of the knowledge and reasoning types that the questions appeal to, in Tables~\\ref{knowledge-types} and~\\ref{reasoning-types}.\n\nTo help the community to engage with this dataset, we are also releasing a science text corpus and two baseline neural models\nas part of the \\ASC~challenge:\n\\begin{enumerate}\n\\item The \\ASC~Corpus, containing \\BUSCSIZE~\n  science-related sentences with knowledge relevant to \\ASC.\n  A sampled analysis suggests the corpus mentions knowledge relevant to \n   95\\% of the \\Challenge~questions. (Use of the Corpus for the Challenge is optional).\n\\item Three neural baseline models, DecompAttn, BiDAF, and DGEM, for QA. These are\nmultiple-choice QA adaptations of three neural models: the decomposable attention model\n\\cite{parikh2016decomposable}, a top performer on SNLI; Bidirectional Attention Flow \\cite{Seo2016BidirectionalAF}, a top performer on SQuAD; and the decomposed graph entailment model, a top performer on SciTail~\\cite{khot2018scitail}.\nWhile these score well on the \\Additional~Set, they are\nunable to perform significantly better than a random baseline\non the \\Challenge~Set, illustrating its challenging nature.\n\\end{enumerate}\nThis challenge differs from the Kaggle-hosted 2016 Allen AI Science Challenge \\cite{schoenick2017moving}\nin three important ways\\footnote{\n\\ASC~includes the publically releasable subset of the Kaggle questions (about 60\\% of the Kaggle set, making up 43\\% of the \\ASC~set).}. First, the creation of a \\Challenge~partition\nis to avoid scores being dominated by the performance of simple algorithms,\nand thus encourage research on methods that the more difficult questions demand.\nSecond, we provide a science corpus along with the questions to help\nget started (use of the corpus is optional, and systems are not\nrestricted to this corpus). Finally, the questions, corpus, and models\nare all publically available.\n\nThe paper is organized as follows. We first discuss related work, and then\ndescribe how the \\ASC~dataset was collected and partitioned. We also provide an analysis\nof the types of problems present in the \\Challenge~Set. We then describe\nthe supporting \\ASC~Corpus, and illustrate how it contains knowledge\nrelevant to \\Challenge~questions. Finally, we describe several baselines\nand their performance on \\ASC. Most notably, although some baseline algorithms\nperform well on the \\Additional~Set (scoring up to 61\\%), none are able to\nperform significantly above random on the \\Challenge~Set. We conclude by\nposing \\ASC~as a challenge to the community. The \\ASC~Dataset, Corpus, Models,\nand Leaderboard can be accessed at~\\ascurl.\n\n\\section{Related Work}\n\nThere are numerous datasets available to drive progress in question-answering.\nEarlier reading comprehension datasets, e.g., MCTest \\cite{mctest}, SQuAD \\cite{rajpurkar2016squad}, NewsQA \\cite{trischler2016newsqa},\nand CNN/DailyMail \\cite{hermann2015teaching}, contained questions whose answers could\nbe determined from surface-level cues alone (i.e., answers were ``explicitly stated'').\nTriviaQA \\cite{JoshiTriviaQA2017}\nbroadened this task by providing several articles with a question, and\nused questions authored independently of the articles. Again, though, the\nquestions were largely factoid-style, e.g., ``Who won the Nobel Peace Prize in 2009?''.\nAlthough systems can now perform well on these datasets,\neven matching human performance \\cite{human-squad}, they can be easily\nfooled \\cite{jia2017adversarial}; the degree to which they truly understand\nlanguage or domain-specific concepts remains unclear. \n\nTo push towards more complex QA tasks, one approach has been to\ngenerate synthetic datasets, the most notable example being the bAbI dataset \\cite{weston2015towards}.\nbAbi was generated using a simple world simulator and language generator,\nproducing data for 20 different tasks. It has stimulated work on\nuse of memory network neural architectures \\cite{weston2014memory},\nsupporting a form of multistep\nreasoning where a neural memory propagates information from one step to another\n\\cite[e.g.][]{Henaff2016TrackingTW,Seo2017QueryReductionNF}.\nHowever, its use of synthetic text\nand a synthetic world limits the realism and difficulty of the task, with\nmany systems scoring a perfect 100\\% on most tasks \\cite[e.g.][]{weston2014memory}.\nIn general, a risk of using large synthetic QA datasets is that neural methods\nare remarkably powerful at ``reverse-engineering'' the process by which a\ndataset was generated, or picking up on its idiosyncrasies to excel at it,\nwithout necessarily advancing language understanding or reasoning. \n\nMore recently, \\citet{welbl2017constructing} created the WikiHop dataset, containing questions\nthat appear to require more than one Wikipedia document to answer (``multihop questions'').\nThe dataset takes a step towards a more challenging task, but has several limitations:\nquestions are binary predicates (e.g., date\\_of\\_birth(``jeanne c. stein'',?X));\nthe intended inference is typically a simple two-step chain (commonly\na geographical substitution of a city for a country); and in many cases the\ncorrect answer can be guessed from the passage, without requiring multi-hop\ninference ($\\sim$44\\% of the answerable questions are single-hop, according to the authors \\cite{welbl2017constructing}).\n\nDatasets based on human standardized tests have also been used in AI\nfor several years \\cite[e.g.][]{clark2016combining,seo2014diagram,fujita2014overview,strickland},\nand as part of the NTCIR QALab \\cite{ntcir13}, and for the 2016 Allen AI Science Challenge\ndescribed in the Introduction \\cite{schoenick2017moving}. However, there\nare two potentially significant challenges with these datasets. First, these datasets are often\nsmall (e.g., hundreds of questions), due to the scarcity of public, real-world test data.\nSecond, because tests were designed for people rather than machines,\nlarge portions of these tests can be easily solved by simple AI methods\n\\cite[e.g.][]{davis2016write,clark2016combining}. The result of this is\nthat scores become dominated by the performance of simple algorithms\n(information retrieval, statistical correlations).\nThis then biases\nresearch towards incrementally improving those algorithms, rather than\nexploring the larger AI challenges that the more difficult questions demand.\nIndeed, it is easy to mistake progress on these datasets as implying\nequal progress on easy and hard questions, while in reality progress may be\nheavily concentrated on easy questions alone \\cite{Gururangan:2018},\nleaving more difficult challenges unaddressed.\nThe \\ASC~Dataset addresses both of these limitations.\n\n\\section{The \\ASC~Dataset}\n\nThe \\ASC~dataset consists of 7787 science questions, all non-diagram, multiple choice (typically 4-way multiple choice). They are drawn from a variety of sources,\nand sorted into a \\Challenge~Set of 2590 ``hard'' questions (those that both a retrieval\nand a co-occurrence method fail to answer correctly) and an \\Additional~Set of 5197 questions.\nTable~\\ref{numbers} summarizes the sizes of the train/dev/test partitions in the \\ASC~dataset.\nThe question vocabulary uses 6329 distinct words (stemmed).\n\n\\begin{table}[htb]\n\\setlength{\\tabcolsep}{8pt}\n\\setlength{\\doublerulesep}{\\arrayrulewidth}\n\\small\n\\centering\n \\begin{tabular}{|l|rrr|} \\hline\n \\bigstrut[t] & {\\bf \\Challenge} & {\\bf \\Additional} & {\\bf \\hspace{3ex} Total} \\\\ \\hline \\hline\nTrain \\bigstrut[t] & 1119 & 2251 & 3370 \\\\\nDev & 299 & 570 & 869 \\\\\nTest & 1172 & 2376 & 3548 \\\\ \\hline\n\\bigstrut[t] TOTAL & 2590 & 5197 & 7787 \\\\ \\hline\n \\end{tabular}\n \\caption{Number of questions in the \\ASC~partitions. \\label{numbers}}\n \\end{table}\n\n\\begin{table}[htb]\n\\setlength{\\tabcolsep}{4pt}\n\\setlength{\\doublerulesep}{\\arrayrulewidth}\n\\small\n\\centering\n\\begin{tabular}{|c|rlrl|} \\hline\n{\\bf Grade} \\bigstrut[t] & \\multicolumn{2}{c}{\\bf \\Challenge} &\t\\multicolumn{2}{c|}{\\bf \\Additional} \\\\\n& \\% & (\\# qns) & \\% & (\\# qns) \\\\\n\\hline \\hline\n3 \\bigstrut[t] &\t3.6 & (94 qns) & 3.4 & (176 qns) \\\\\n4 & 9 & (233) & 11.4  & (591) \\\\\n5 & 19.5 & (506) & 21.2 & (1101) \\\\\n6 & 3.2 & (84) & 3.4 & (179) \\\\\n7 & 14.4 & (372) & 10.7 & (557) \\\\\n8 & 41.4 & (1072) & 41.2 & (2139) \\\\\n9 & 8.8 & (229) & 8.7 & (454) \\\\ \\hline\n \\end{tabular}\n \\caption{Grade-level distribution of \\ASC~questions \\label{grade-levels}}\n \\end{table}\n \n \\eat{\n\\begin{table}[htb]\n\\setlength{\\tabcolsep}{4pt}\n\\setlength{\\doublerulesep}{\\arrayrulewidth}\n\\small\n\\centering\n\\begin{tabular}{|c|rr@{\\hskip 10pt}rr|} \\hline\n{\\bf Grade} \\bigstrut[t] & \\multicolumn{2}{c}{\\bf \\Challenge} &\t\\multicolumn{2}{c|}{\\bf \\Additional} \\\\\n& \\# & \\% & \\# & \\% \\\\\n\\hline \\hline\n3 \\bigstrut[t] &\t94&3.6\\% &\t176  & 3.4\\%\\\\\n4 &\t233&9.0\\% &\t591 & 11.4\\%\\\\\n5 &\t506&19.5\\% &\t1101 & 21.2\\%\\\\\n6 &\t84&3.2\\% &\t179 & 3.4\\%\\\\\n7 &\t372&14.4\\% &\t557 & 10.7\\%\\\\\n8 &\t1072&41.4\\% &\t2139 & 41.2\\%\\\\\n9 &\t229&8.8\\% &\t454 & 8.7\\%\\\\ \\hline\n \\end{tabular}\n \\caption{Grade-level distribution of \\ASC~questions \\label{grade-levels}}\n \\end{table}\n }\n\nQuestions vary in their target student grade level (as assigned by the examiners who authored the questions), ranging from 3rd grade to 9th, i.e., students typically of\nage 8 through 13 years. Table~\\ref{grade-levels} shows a break-down of the set based on grade level with absolute counts(\\#) and percentage(\\%) of the \\Challenge\\ and \\Additional\\ set. In practice,\nthere is substantial overlap in difficulty among grade levels (also seen in the similar distribution of grade levels), as each grade level contains a mixture of\neasy and difficult questions.\n\n\\begin{table}[htb]\n\\setlength{\\tabcolsep}{4pt}\n\\setlength{\\doublerulesep}{\\arrayrulewidth}\n\\small\n\\centering\n\\begin{tabular}{|l|rr|} \\hline\n   & \\multicolumn{2}{|c|}{\\bf min / average / max} \\\\\n  {\\bf Property:} \\bigstrut[t] & {\\bf \\Challenge} & {\\bf \\Additional} \\\\ \\hline\nQuestion (\\# words) \\bigstrut[t] & 2 / 22.3 / 128 & 3 / 19.4 / 118 \\\\\nQuestion (\\# sentences) & 1 / 1.8 / 11 & 1 / 1.6 / 9 \\\\\nAnswer option (\\# words) \\bigstrut[t] & 1 / 4.9 / 39 & 1 / 3.7 / 26 \\\\\n\\# answer options & 3 / 4.0 / 5 & 3 / 4.0 / 5 \\\\ \\hline\n \\end{tabular}\n \\caption{Properties of the \\ASC~Dataset \\label{statistics}}\n \\end{table}\n\nSummary statistics of \\ASC~are provided in Table~\\ref{statistics}, showing questions and answers vary\nconsiderably in length. Finally, Table~\\ref{sources} in the Appendix lists the variety of sources the questions were drawn from.\n\n\\eat{\ndefinition\t20\n\nnegation\t15\nprocesses\t15\ncomparison\t15\nexperiments\t12\nsituated\t10\nfunction/purpose\t5\nanalogy\t3\nequational\t3\nspatial\t2\n}\n\n\\begin{table*}[htb]\n\\small\n\\centering\n \\begin{tabular}{|>{\\raggedright\\arraybackslash}p{18ex}|p{80ex}|} \\hline\n{\\bf Knowledge Type} \\bigstrut[t] & \\multicolumn{1}{c|}{\\bf Example} \\\\ \\hline\nDefinition \\bigstrut[t] &\nWhat is a worldwide increase in temperature called? (A) greenhouse effect (B) global warming (C) ozone depletion (D) solar heating \\\\\n& \\\\\nBasic Facts \\& Properties &\nWhich element makes up most of the air we breathe? (A) carbon (B) nitrogen (C) oxygen (D) argon \\\\\n& \\\\\nStructure &\nThe crust, the mantle, and the core are structures of Earth. Which description is a feature of Earth's mantle? (A) contains fossil remains (B) consists of tectonic plates (C) is located at the center of Earth (D) has properties of both liquids and solids \\\\\n& \\\\\nProcesses \\& Causal & \nWhat is the first step of the process in the formation of sedimentary rocks? (A) erosion (B) deposition (C) compaction (D) cementation \\\\\n & \\\\\nTeleology / Purpose\n& What is the main function of the circulatory system? (1) secrete enzymes (2) digest proteins (3) produce hormones (4) transport materials \\\\\n& \\\\\nAlgebraic &\nIf a red flowered plant (RR) is crossed with a white flowered plant (rr), what color will the offspring be? (A) 100\\% pink (B) 100\\% red (C) 50\\% white, 50\\% red (D) 100\\% white \\\\\n& \\\\\nExperiments &\nScientists perform experiments to test hypotheses. How do scientists try to remain objective during experiments? (A) Scientists analyze all results. (B) Scientists use safety precautions. (C) Scientists conduct experiments once. (D) Scientists change at least two variables. \\\\\n& \\\\\nSpatial / Kinematic &\nIn studying layers of rock sediment, a geologist found an area where older rock was layered on top of younger rock. Which best explains how this occurred? (A) Earthquake activity folded the rock layers... \\\\ \\hline\n \\end{tabular}\n\\caption{Types of knowledge suggested by \\ASC~\\Challenge~Set questions}\n\\label{knowledge-types}\n\\end{table*}\n\n\\begin{table*}[htb]\n\\setlength{\\tabcolsep}{4pt}\n\\setlength{\\doublerulesep}{\\arrayrulewidth}\n\\small\n\\centering\n \\begin{tabular}{|>{\\raggedright\\arraybackslash}p{18ex}|p{80ex}|} \\hline\n{\\bf Reasoning Type} \\bigstrut[t] & \\multicolumn{1}{c|}{\\bf Example} \\\\ \\hline \\hline\nQuestion logic \\bigstrut[t] &\nWhich item below is {\\bf not} made from a material grown in nature? (A) a cotton shirt (B) a wooden chair (C) a plastic spoon (D) a grass basket \\\\\n& \\\\\nLinguistic Matching \n& Which of the following best describes a mineral? (A) the main nutrient in all foods (B) a type of grain found in cereals (C) a natural substance that makes up rocks (D) the decomposed plant matter found in soil \\\\\n& \\\\ \nMultihop Reasoning &\nWhich property of a mineral can be determined just by looking at it? (A) luster (B) mass (C) weight (D) hardness \\\\\n\\\\\nComparison\n& Compared to the Sun, a red star most likely has a greater (A) volume. (B) rate of rotation. (C) surface temperature. (D) number of orbiting planets \\\\\n & \\\\\nAlgebraic &\nIf a heterozygous smooth pea plant (Ss) is crossed with a homozygous smooth pea plant (SS), which are the possible genotypes the offspring could have? (A) only SS (B) only Ss (C) Ss or SS (D) ss or SS \\\\\n& \\\\\nHypothetical / Counterfactual\n& If the Sun were larger, what would most likely also have to be true for Earth to sustain life? (A) Earth would have to be further from the Sun. (B) Earth would have to be closer to the Sun. (C) Earth would have to be smaller. (D) Earth would have to be larger. \\\\\n & \\\\\nExplanation / {Meta-reasoning} &\nWhy can steam be used to cook food? (A) Steam does work on objects. (B) Steam is a form of water. (C) Steam can transfer heat to cooler objects. (D) Steam is able to move through small spaces. \\\\\n & \\\\\nSpatial / Kinematic &\nWhere will a sidewalk feel hottest on a warm, clear day? (A) Under a picnic table (B) In direct sunlight (C) Under a puddle (D) In the shade \\\\ \n& \\\\\nAnalogy &\nInside cells, special molecules carry messages from the membrane to the nucleus. Which body system uses a similar process? (A) endocrine system (B) lymphatic system (C) excretory system (D) integumentary system \\\\ \\hline\n \\end{tabular}\n\\caption{Types of reasoning suggested by \\ASC~\\Challenge~Set questions}\n\\label{reasoning-types}\n\\end{table*}\n\n\\subsection{Identifying Challenge Questions}\n\nOperationally, we define a Challenge question as\none that is answered incorrectly by both of two baseline solvers, described below.\nAlthough this only approximates the informal goal of it being a ``hard'' question,\nthis definition nevertheless serves as a practical and useful filter, as reflected\nby the low scores of various baselines on the \\Challenge~Set.\n\n\\subsubsection{Information Retrieval (IR) Solver.}\nThe first filter we apply is the IR solver from \\citet{clark2016combining}, briefly described here for completeness. The IR solver uses the Waterloo corpus from \\cite{clark2016combining}, a Web-based corpus of $5 \\times 10^{10}$ tokens (280GB). The solver searches to see if the question $q$ along with an answer option is explicitly stated in the corpus, and returns the confidence that such a statement was found.\nTo do this, for each answer option $a_i$, it sends $q + a_i$ as a query to a search engine \n(we use Elasticsearch),\nand returns the search engine's score for \nthe top retrieved sentence $s$ where $s$ also has at least one non-stopword overlap with $q$, and at least one with $a_i$; this ensures $s$\nhas {\\it some} relevance to both $q$ and $a_i$. This is repeated for all options $a_i$ to score them all, and the option with the highest score selected.\n\n\\subsubsection{The Pointwise Mutual Information (PMI) Solver. }\nThe second filter we apply is the PMI solver, also from \\citet{clark2016combining}, again described here for completeness. This uses the same corpus as the IR solver, and\nformalizes a way of computing and applying associational knowledge. Given a question $q$ and an answer option $a_i$, it uses PMI or pointwise mutual information \\cite{church1989} to measure the strength of the associations between parts of $q$ and parts of $a_i$. Given a large corpus $C$, the PMI for two n-grams $x$ and $y$ is defined as\n\\begin{equation*}\n\\mathrm{PMI}(x,y) = \\log \\frac{p(x,y)}{p(x) p(y)}\n\\end{equation*}\nHere $p(x,y)$ is the joint probability that $x$ and $y$ occur together in $C$, within a certain window of text (we use a 10 word window). The term $p(x) p(y)$, on the other hand, represents the probability with which $x$ and $y$ would occur together if they were statistically independent. The ratio of $p(x,y)$ to $p(x) p(y)$ is thus the ratio of the observed co-occurrence to the expected co-occurrence. The larger this ratio, the stronger the association between $x$ and $y$.\n\nThe solver extracts unigrams, bigrams, trigrams, and skip-bigrams from the question $q$ and each answer option $a_i$.\nIt outputs the answer with the largest average PMI, calculated over all pairs of question n-grams and answer option n-grams.\n\n\\subsection{The \\Challenge~Set}\n\nTo illustrate the impact of using these algorithms as filters when defining the \\Challenge~Set, consider the following example:\n\\begin{quote}\n{\\it Which property of air does a barometer measure? (A) speed (B) pressure {\\bfit [correct]} (C) humidity (D) temperature}\n\\end{quote}\nThe question was excluded from the \\Challenge~Set because it is correctly answered by (here) both the IR and PMI algorithms (note that it would have been excluded even if it was answered correctly by just one of the solvers). The IR algorithm finds multiple sentences supporting the correct answer, e.g.,\n\\begin{ite}\n\\item {\\it Air pressure is measured with a barometer.}\n\\item {\\it Air pressure will be measured with a barometer.}\n\\item {\\it The aneroid barometer is an instrument that does not use liquid in measuring the pressure of the air.}\n\\item {\\it A barometer measures the pressure of air molecules.}\n\\end{ite}\nand similarly the PMI algorithm finds that ``barometer'' and ``pressure'' (and also ``air'' and ``pressure'') co-occur unusually frequently (high PMI) in its corpus.\n\n\\noindent\nIn contrast, consider the following question:\n\\begin{quote}\n{\\it Which property of a mineral can be determined just by looking at it? (A) luster {\\bfit [correct]} (B) mass (C) weight (D) hardness}\n\\end{quote}\nThis is incorrectly answered by both algorithms: There are no corpus sentences similar to ``a material's luster can be determined by looking at it''. Similarly, ``mineral'' co-occurs unusually frequently with several incorrect answer options (e.g., mass, hardness), confusing the PMI algorithm. This question is thus part of the \\Challenge~Set, as it appears to require a more advanced answering method.\n\n\\subsection{Question Types}\n\n\\ASC~questions appeal to both different styles of knowledge, and different styles of reasoning.\nIn Tables~\\ref{knowledge-types} and~\\ref{reasoning-types}, we enumerate the broad classes of each\nthat we observe in the \\ASC~challenge, based on a sample of 100 questions. The relative\nsizes of these categories are shown in Figures~\\ref{knowledge-pie-chart} and~\\ref{reasoning-pie-chart}.\nThese sizes are necessarily approximate, as it requires a subjective judgement about the {\\it main} challenge exhibited by\ndifferent questions. Nevertheless, it helps to provide a rough atlas of the knowledge and reasoning\nspace underlying \\ASC.\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=\\columnwidth]{knowledge.png}\n\\caption{Relative sizes of different knowledge types suggested by the \\ASC~\\Challenge~Set. \\label{knowledge-pie-chart}}\n\\end{figure}\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=\\columnwidth]{reasoning.png}\n\\caption{Relative sizes of different reasoning types suggested by the \\ASC~\\Challenge~Set. \\label{reasoning-pie-chart}}\n\\end{figure}\n\n\\section{The \\ASC~Corpus}\n\nIn addition to the \\ASC~question set, we are also releasing the \\ASC~Corpus, a large corpus of\nscience-related sentences mined from the Web. It contains \\BUSCSIZE~sentences (1.4GB of text),\nand mentions much of the knowledge required to answer the \\Challenge~Questions. Although\nsome of these mentions are indirect, and exploiting them is not trivial, it nevertheless\nprovides a starting point for attacking the \\ASC~Challenge.\nNote that use of the corpus is optional, and also that systems are not\nrestricted to this corpus.\n\nThe \\ASC~Corpus was created by utilizing a major search engine to\nrun a large series of search queries relevant to science. Queries were automatically constructed \nby instantiating $\\sim$100 hand-written templates for 80 science topics covered by US elementary and\nmiddle schools, the subject areas of \\ASC. For example, for the\ntopic ``celestial phenomena'', two templates ``[{\\it astronomical-term}] astronomy'' and\n``[{\\it astronomical-term}] astrophysics'' were authored, and a list of (here, 360) terms for {\\it astronomical-term} collected and used, \nresulting in 720 queries. The top several documents from each search were collected and de-duplicated, and then the content of these documents was stripped down to capture just the text in each document. The resulting text was then chunked into sentences. This was repeated for all templates.\nNote that some templates were parameterized by more than one parameter.\nFrom an informal analysis of a random sample of 805 documents that were collected, approximately 75\\% were judged as ``science relevant''.\nThe corpus was then augmented with the AristoMini corpus\\footnote{Also available separately at http://allenai.org/data.html}, an earlier corpus containing\ndictionary definitions from Wiktionary, articles from Simple Wikipedia tagged as science,\nand additional science sentences collected from the Web.\nFrom a vocabulary analysis, 99.8\\% of the \\ASC~question vocabulary is mentioned in the \\ASC~Corpus\\footnote{\n  Only 11 question words, mainly proper nouns, do not appear in the corpus: Daphne, Sooj, LaKeisha, Quickgrow, Hypergrow, CCGCAT, nonsnow,\n  Quickgrow, Coaccretion, HZn, MgBr}.\n\nIn our baseline experiments discussed shortly, we find that if we change the corpus behind the\nIR solver from Waterloo to the \\ASC~Corpus, this changes its \\Challenge~Set score from near zero (by definition,\n\\Challenge~questions are those that IR with the Waterloo corpus gets wrong) to a score similar to\nrandom guessing\\footnote{\n  In fact slightly lower than random guessing, likely due to some corpus similarities where the\n  same distractors in Waterloo were also present in the\\ASC~Corpus.}.\nHowever, from an informal, sampled analysis, we find that this is more\na limitation of the IR methodology than of the coverage of the \\ASC~Corpus. The \\ASC~Corpus,\nin fact, appears to mention knowledge relevant to approximately 95\\% of the \\ASC~\\Challenge~questions (from an\nanalysis of a random sample of questions), even if simple retrieval methods are not able to exploit it to correctly\nanswer the questions. For example, consider:\n\\begin{quote}\n  {\\it Scientists launch a rocket into space for a mission. Once the rocket escapes the gravitational pull of Earth, how will the mass and weight of the rocket be affected? \\\\\n  (A) The mass and weight will change. \\\\\n  (B) The mass and weight will stay the same.\\\\\n  (C) The mass will stay the same, but the weight will change. \\bfit{[correct]}\\\\\n  (D) The mass will change, but the weight will stay the same.}\n\\end{quote}\nWhile this particular scenario is of course not mentioned explicitly in the \\ASC~Corpus, there are several statements about the relation between mass, weight, and gravity, for example:\n\\begin{ite}\n\\item {\\it The main difference is that if you were to leave the Earth and go to the Moon, your weight would change but your mass would remain constant.}\n\\item {\\it Astronauts in orbit experience weightlessness just like objects in the falling aircraft.}\n\\item {\\it Weight is the force that something feels due to gravity: so the brick would have a much larger weight near the earth's surface than it does in deep space.}\n\\end{ite}\nSuch sentences provide evidence that weight but not mass will change when in space. Similarly, consider the question:\n\\begin{quote}\n{\\it Which factor will prompt an animal's fight-or-flight response? (A) population size (B) competition for food \\bfit{[correct]} (C) seasonal temperatures (D) protection of the environment}\n\\end{quote}\nWhile there are few sentences in the \\ASC~Corpus that directly relate fight-or-flight with food competition, there are several that indicate competition is a kind of threat, e.g.,:\n\\begin{ite}\n\\item {\\it Insurgency, cross-border firing, \\bfit{competition} with livestock for grazing ground, ...all \\bfit{threaten} the Markhor.}\n\\item {\\it The main \\bfit{threats} to the species include loss of habitat through deforestation, hunting for meat and \\bfit{competition} with domestic livestock.}\n\\item {\\it Habitat fragmentation \\bfit{threatens} this species, as does possible \\bfit{competition} with the introduced non-native Ring-necked Pheasant.}\n\\end{ite}\nand similarly that threats can trigger the fight-or-flight response:\n\\begin{ite}\n\\item Whenever a reptile faces a \\bfit{threat} they go into \\bfit{fight or flight mode}...\n\\item {\\it The sympathetics command your \\bfit{fight or flight response}, and when they fire, your heart rate and your breathing speed up, ...preparing your body to face a \\bfit{threat}...}\n\\end{ite}\n\\noindent\nFinally, consider the question:\n\\begin{quote}\n{\\it Which of the following distinguishes the organisms in the kingdom Fungi from other eukaryotic organisms? (A) Fungi are unicellular. (B) Fungi reproduce sexually. (C) Fungi obtain nutrients by absorption. \\bfit{[correct]} (D) Fungi make food through photosynthesis}\n\\end{quote}\nAgain, there is distributed evidence for the answer, including that (A) is incorrect:\n\\begin{ite}\n\\item {\\it There are \\bfit{unicellular} fungi and \\bfit{multicellular} fungi.}\n\\end{ite}\nthat (C) is possible:\n\\begin{ite}\n\\item {\\it Fungi are heterotrophs that \\bfit{acquire their nutrients by absorption.}}\n\\end{ite}\nand that (C) is distinct for other eukaryotic organisms:\n\\begin{ite}\n\\item {\\it Animals are multicellular eukaryotes; they are chemosynthetic heterotrophs that \\bfit{ingest} their food.}\n\\end{ite}\nAlthough these descriptions of reasoning paths are informal, and clearly many others are possible, they illustrate\nthat the \\ASC~Corpus mentions knowledge relevant to a question, even if no single sentence alone provides the answer.\nOf course, this does not address the challenge of correctly identifying and reasoning with this knowledge, nor\nthe challenge of injecting unstated commonsense knowledge that may also be required. Rather, our claim is\nthat the Corpus contains substantial linguistic signal relevant to most of the \\ASC~questions, and\nis a useful starting point for corpus-based attacks on the Challenge.\n\n\\section{Baseline Performance}\n\n\\subsection{Baseline Systems}\n\nWe ran several baseline QA systems on the \\Challenge~and \\Additional~Sets, including two neural models, DecompAttn and BiDAF (details below), that have near state-of-the-art performance on\nthe well-known SNLI and SQuAD datasets respectively. We scored systems using the following scoring rubric: For each question, a system receives 1 point if it chooses the correct answer and $1/k$ if it reports a $k$-way tie (i.e., chooses multiple answers) that includes the correct answer. For a question set, the overall score of the system is the sum of the points it receives for all questions, divided by the number of questions and reported as a percentage. We report performance of the following systems:\n\n\\begin{enumerate}\n\n\\item \\textit{IR (dataset definition).} IR method, described earlier. \n\n\\item \\textit{PMI (dataset definition).} PMI method, described earlier. \n\n\\item \\textit{Guess-all (``random'').} A na\\\"ive baseline that selects all answer options as equally valid, thereby scoring $1/k$ for each question with $k$ answer choices. A system that chooses a single answer at random will also converge to this score after enough trials.\n\n\\item \\textit{IR (\\ASC~Corpus).} The IR algorithm, rerun with the \\ASC~Corpus. Note that changing the original corpus is expected to result in a different score, unless the two corpora are highly correlated. A corpus containing random strings, for instance, will have very low correlation with the original corpus and will result in a random-guessing score of around 25\\%.\n\n\\item \\textit{TableILP \\cite{khashabi2016tableilp},} which performs matching and reasoning using a semi-structured knowledge base of science knowledge, expressed in tables.\n\n\\item \\textit{TupleInference \\cite{khot2017tupleinf},} which performs semi-structured matching of the question with retrieved sentences, where the structure consists of Open IE tuples.\n\n\\item \\textit{DecompAttn, DGEM, and DGEM-OpenIE (Neural Entailment Models).} We adapted two neural entailment models, DecompAttn \\cite{parikh2016decomposable} and DGEM \\cite{khot2018scitail}, to the task of answering multiple-choice questions. The models were trained on an extended version of the SciTail dataset \\cite{khot2018scitail}. To adapt these to multiple-choice QA, we first convert the question $q$ plus an answer option $a_{i}$ into a hypothesis sentence (or paragraph) $h_{i}$, use this as a search query to retrieve text sentences $t_{ij}$ from a corpus, then compute the entailment scores between $h_{i}$ and each $t_{ij}$. This is repeated for all answer options, and the option with the overall highest entailment score selected. Further details are given in the Appendix. \n\nThe DGEM model uses a structured representation of the hypothesis $h_{i}$, extracted with a proprietary parser plus Open IE. To create a releasable version of DGEM, we also evaluate (and release) a variant DGEM-OpenIE, a version of DGEM that only uses Open IE to create the structured representation of $h_{i}$, thus avoiding proprietary tools.\n\n\\item \\textit{BiDAF (Reading Comprehension Model).}\n  We also adapted BiDAF \\cite{Seo2016BidirectionalAF}, a top-performer on the SQuAD dataset \\cite{rajpurkar2016squad}, to our task.\n  As BiDAF is a direct answer system, we adapted it to multiple-choice QA following the approach used in several previous projects\n  \\cite{khashabi2018question,welbl2017crowdsourcing,kembhavi2017you} as follows:\n  First, given a question, we create a single paragraph by concatenating a set of retrieved sentences. In this case, we use\n  the same sentences retrieved by the entailment models for all answer options (above).\n  We then use BiDAF to select an answer span from this paragraph, given the question. Finally, we pick the multiple-choice option that maximally overlaps that answer span (here defined as the option with the highest percentage of lemmatized, non-stopword tokens covered by the BiDAF answer span). BiDAF was trained on SQuAD then further tuned to science questions %\\ASC~\n   using continued training.% on the training questions.\n\\end{enumerate}\n\n\\subsection{Results}\n\nTable~\\ref{baselines} summarizes the scores obtained by various baseline algorithms on\nthe test partitions of the \\Challenge~and \\Additional~sets.\n\n\\begin{table}[!t] \n\\setlength{\\tabcolsep}{4pt}\n\\setlength{\\doublerulesep}{\\arrayrulewidth}\n\\small\n{\\centering\n\\begin{tabular}{|l|cc|} \\hline\n  \\bigstrut[t] & \\multicolumn{2}{|c|}{\\bf Test Scores} \\\\\n   \\multicolumn{1}{|c|}{\\bf Solver} & {\\bf \\Challenge} & {\\bf \\Additional} \\\\\n  & {\\bf Set} & {\\bf Set} \\\\  \\hline \\hline\nIR (dataset defn) \\bigstrut[t]\t& (1.02)$^{\\dag}$\t& (74.48)$^{\\dag}$ \\\\\nPMI (dataset defn)\t& (2.03)$^{\\dag}$\t& (77.82)$^{\\dag}$ \\\\ \\hline\nIR (using \\ASC~Corpus) \\bigstrut[t] \t& 20.26\t& 62.55 \\\\\nTupleInference\t\t\t\t\t\t\t& 23.83\t& 60.81 \\\\\nDecompAttn${^\\ddag}$\t\t\t\t\t& 24.34\t& 58.27 \\\\\nGuess-all (``random'')\t\t\t\t\t& 25.02\t& 25.02 \\\\\nDGEM-OpenIE${^\\ddag}$\t\t\t\t\t& 26.41\t& 57.45 \\\\\nBiDAF${^\\ddag}$\t\t\t\t\t\t\t& 26.54 & 50.11 \\\\ \nTableILP\t\t\t\t\t\t\t\t& 26.97\t& 36.15\\\\ \nDGEM\t\t\t\t\t\t\t\t\t& 27.11\t& 58.97 \\\\ \\hline\n\\end{tabular} \\\\\n}\n$^{\\dag}$These solvers were used to define the dataset, affecting scores. \\\\\n${^\\ddag}$Code available at~\\codeurl \n\\caption{Performance of the different baseline systems. Scores are reported as percentages on the test sets. For up-to-date results, see the \\ASC~leaderboard at ~\\ascurl.}\n\\label{baselines}\n\\end{table}\n\nThe IR and PMI (dataset definition) solvers, by design, score near zero on the \\Challenge~set. The slightly above-zero score is due to the solver occasionally picking multiple (tied) answers, resulting in a partial credit for a few questions. We include these questions in the \\Challenge~set.\n\nThe most striking observation is that \\emph{none of the algorithms score significantly\nhigher than the random baseline} on the \\Challenge~set, where the 95\\% confidence\ninterval is $\\pm 2.5\\%$.\nIn contrast, their performance on the \\Additional~set is generally between 55\\% and 65\\%.\nThis highlights the different nature and difficulty of the \\Challenge~set.\n\nThe poor performance of the non-IR solvers is partly explained by their correlation with the IR solver: the first step for nearly all of them (except TableILP, which uses non-sentential knowledge but has low knowledge coverage) is to use a simple IR method to obtain relevant \nsentences, and then process them in different ways such as extracting structure, attempting matching, attempting chaining, etc. However, the retrieval bias of the underlying IR methods is towards sentences that are all very similar to the question, and away from sentences that individually only partially match the question, but together fully explain the correct answer (e.g., through chaining). This suggests the need for a more advanced retrieval strategy for questions that require combining multiple facts, as well as new methods for combining that information.\n\n\\section{Conclusion}\n\nDatasets have become highly influential in driving the direction of research.\nRecent datasets for QA have led to impressive advances, but have \nfocused on factoid questions where surface-level cues alone are\nsufficient to find an answer, discouraging progress on questions\nrequiring reasoning or other advanced methods. To help the field\nmove towards more difficult tasks, we have presented the AI2 Reasoning Challenge (\\ASC),\nconsisting of a new question set, text corpus, and baselines,\nand whose \\Challenge~partition is hard for retrieval and\nco-occurence methods. We find that none of the baseline systems\ntested can significantly outperform a random baseline on the \\Challenge~set,\nincluding two neural models with high performances on SNLI and SQuAD.\nProgress on \\ASC~would thus be an\nimpressive achievement, given its design, and be significant step\nforward for the community. To access \\ASC, view the leaderboard,\nand submit new entries, visit the \\ASC~Website at~\\ascurl.\n\n\\eat{\nDatasets have become highly influential in driving the direction of research.\nRecent datasets for QA have led to impressive advances, but have \nfocused on factoid questions where surface-level cues alone are\nsufficient to find an answer, discouraging progress on questions\nrequiring reasoning or other advanced methods. To help the field\nmove towards more challenging task, we have presented \\ASC, a new\ndataset whose \\Challenge~partition is hard for retrieval and\nco-occurence methods. We find that none of the baseline systems\ntested can significantly outperform a random baseline on this set,\nand thus pose it as a challenge to the community. To record\nand publicize progress, we have established a leaderboard at\nhttp://www.allenai.org/asq.\n\nIn support of this, we also are releasing the \\ASC~Corpus, a collection of\n\\BUSCSIZE~sentences relevant the \\ASC~task. From an analysis of a sample,\nthe \\ASC~Corpus appears to mention the knowledge needed for\napproximately 95\\% of the \\ASC~\\Challenge~questions.\nWe are also releasing code for the two neural baselines for\nothers to build on. Progress on the \\ASC~\\Challenge~would be an\nimpressive achievement, given its design, and be significant step\nforward for the community. We encourage other research groups\nto rise to this challenge!\n}\n\n\\subsubsection*{Availability}\nThe \\ASC~Dataset, Corpus, three baseline neural models (DecompAttn, BiDAF, and DGEM), and\nthe leaderboard, are all available from~\\ascurl.\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nGenerating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks–where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities–including plan generation–LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.\n\\end{abstract}\n\\section{Introduction}\nThe advent of large pre-trained language models have revolutionized the field of natural language processing and have also received widespread public attention. These types of transformer-based large language models (LLMs) currently provide state-of-the-art performance in many of the standard NLP tasks.\nLLMs essentially predict the next word in a sentence, given a certain context and these models were originally developed to perform word sequence completion tasks. In the recent times, there have been anecdotal evidence and claims that they possess other capabilities that are not normally associated with sequence completion. This led to a sudden outburst of research probing and studying their behavior almost as if they were artificial organisms (c.f. \\cite{kambhampati_2022}). In this paper, we are particularly interested in the line of research efforts that investigate (and showcase) the reasoning capabilities of Large Language models--including commonsense reasoning \\cite{talmor2018commonsenseqa,sakaguchi2020winogrande, geva2021did}, logical reasoning \\cite{bigbench}, and even ethical reasoning \\cite{Jiang2021DelphiTM}. These works have largely been suggesting that LLM’s are indeed capable of doing such kinds of reasoning \\cite{kojima2022large, wei2022chain, chowdhery2022palm}.\n\nPlanning is a reasoning task that has been well studied in the AI community. In its most basic form, planning involves coming up with a course of actions (policy) which when executed would take an agent from a certain initial state to a desired world state. \nPlanning has generally been studied primarily as an inference problem on world and reward models. These models could either be specified by humans or learned by the agent by interacting with its world.\nIn this paper, we want to look at the ability of large language models to do reasoning about actions and change involving common-sense planning tasks. We propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, specially in International Planning Competitions (IPC) \\cite{ipc} to test this.\nOur focus on planning is spurred by not only the fact that it is a core aspect of human intelligence \\cite{russell2010artificial}, but also that it is required for tasks considered as potential applications of LLMs including automatic code generation.  \n\nThe contribution of our paper is a curriculum for evaluating planning, wherein we identify a set of related but distinct tasks, that are central for an agent to successfully perform planning and reasoning about actions and introduce a framework for developing code which is meant to auto-generate the possible queries for each. We initialize PlanBench with two IPC domains: Blocksworld and Logistics. We also provide obfuscated versions of these domains with the obfuscations being either misleading words or random alphanumeric strings. Overall, we provide $\\sim$26250 prompts as part of our dataset. \\revised{We are also actively adding other IPC domains and tasks into the benchmark.} The updated version of the entire benchmark, including the tools, datasets, and the scripts to reproduce the prompts and results in this paper, can be found at \\url{https://github.com/karthikv792/LLMs-Planning}.\n\nWe are not the first to point out the need to perform such analyses of the reasoning capabilities of GPT-3 like LLMs. For example, \\cite{blov-exp} performed an analysis of GPT-3's reasoning capabilities on some example tasks, including different commonsense reasoning tasks varying from biological reasoning to arithmetic reasoning. However, the goal of this paper is fundamentally distinct from these earlier works in multiple ways. Firstly, we are not merely trying to point out a few example cases where LLMs fail but rather help establish an assessment framework for evaluating these systems' capabilities to perform planning. While this paper reports the results of testing Instruct-GPT3 \\cite{ouyang2022training} and GPT-4 \\cite{openai2023gpt4}, one could use this framework to analyse other LLMs that may be fine-tuned for such tasks. Secondly, through this framework, we are also trying to eliminate the subjective aspect of analysis that forms the core part of many of these earlier efforts. Instead, we automate and perform the analyses in a mechanistic way by leveraging automated planning models and tools to generate the queries and validate the system's answers. \n\n\\section{Related Work}\n\\karthik{To the best of our knowledge, we are the first to introduce a benchmark specifically designed to evaluate the emerging planning abilities of LLMs, if any. The initial version of our benchmark was made public almost a year back and is currently in use by various researchers.\\footnote{The number of stars (103) and forks (14) on our GitHub repository serves as an indirect estimate of the level of interest in our benchmark.} \nWe have significantly increased the scope of our benchmark (with additional test cases and domains) from the initial version.}\nBut the idea of developing benchmarks to evaluate emergent properties of LLMs is itself not new. Some prominent existing reasoning benchmarks include,  BIG-BENCH \\cite{bigbench},  GSM8K \\cite{cobbe2021training}, AQUA \\cite{ling2017program}, SVAMP \\cite{patel2021nlp}, CommonsenseQA \\cite{talmor2018commonsenseqa} and StrategyQA \\cite{geva2021did}. However, these tasks are simple involve shallow reasoning and do not give insight into their planning capabilities. As LLMs have been able to perform well on such tasks, there has been a lot more triumphalism about their planning capabilities, which is currently being echoed in the community. \n\n\\revised{\nThere have been significant developments in the intersection of LLMs and planning, with LLMs undertaking various roles \\cite{kambhampati2023role}. These roles range from generating plans \\cite{huang2022language, valmeekam2023planning} and heuristics \\cite{valmeekam2023planning, ahn2022can} to extracting planning knowledge \\cite{guan2023leveraging}.\n}\nFor example, in Say-Can \\cite{ahn2022can}, LLMs have been used as scoring models, which can be seen as providing planning heuristics, for the actions that the embodied robot can execute. Additionally, LLMs are also being made to use feedback from users or the environment to improve their planning performance  \\cite{huang2022inner, yao2023react, raman2022planning}. \n\n\\revised{Our work primarily establishes an assessment framework for evaluating a wide variety of planning capabilities of LLMs.} PlanBench consists of multiple tasks, each designed to evaluate a certain aspect of reasoning about actions and change. The prompts for our tasks are showcased as few-shot examples where we provide an instance and an example completion and then ask for a completion on a new instance. Further, we use the domain description to explicitly constrain the possible actions. In many everyday scenarios, we are often asked to take into consideration unforeseen limitations and constraints. Our explicit domain description allows us to introduce such challenges and forces the LLMs to go beyond merely repeating possible information about the domain they may have come across in the training data. The ability to be conditional on the prompt is critical for the general systems to be customized for the specific domain of interest.\n\\section{Background}\nAs we are interested in investigating the basic planning problem, we want look at the most fundamental planning formalism, namely the goal-directed deterministic planning problem. These kinds of problems are colloquially referred to as {\\em classical planning problems}. \n\nClassical Planning Problems can be mathematically represented by using the tuple $\\mathcal{P} = \\langle \\mathcal{D}, \\mathcal{I}, \\mathcal{G}\\rangle$. $\\mathcal{D}$ is referred to as the problem domain, $I$ is the initial state and $G$ is the goal specification. The possible truth assignment over the predicates defines the state space for the planning problem.\nThe domain is again defined by the tuple $\\mathcal{D} = \\langle \\mathcal{F}, \\mathcal{O}\\rangle$.  $\\mathcal{F}$ corresponds to the set of fluents, i.e., the state variable used to define the state space and each fluent corresponds to a predicate with some arity, and  $\\mathcal{A}$ correspond to the set of actions that can be performed as part of the planning problem. Each action $a_i[\\mathcal{V}] \\in \\mathcal{A}$ (where $a_i$ is the operator label and $\\mathcal{V}$ is the variable used by the operator and each variable could be mapped to an object), can be further defined by two components, the precondition $prec[\\mathcal{V}]$ which describes \\textit{when} an action can be executed and the effects $eff[\\mathcal{V}]$ which defines \\textit{what happens} when an action is executed. We will assume that $prec[\\mathcal{V}]$ consists of a set of predicates defined over the variables $\\mathcal{V}$. An action is assumed to be executable only if its preconditions are met, i.e., the predicates in the precondition hold in the given state.\nThe effects $eff[\\mathcal{V}]$ is further defined by the tuple $\\langle add[\\mathcal{V}], del[\\mathcal{V}] \\rangle$, where $add[\\mathcal{V}]$ or add effects is the set of predicates that will be set true by the action and $del[\\mathcal{V}]$ or delete effects is the set of predicates that will be set false by the action. \nAn action is said to be grounded if we replace each of the variables with an object, else it is referred to as a lifted domain model (we use a similar convention to differentiate between lifted and grounded predicates).\nBelow is a snippet of an action from a popular benchmark problem called Blocksworld, in PDDL. The action corresponds to putting down a block on a table in that domain. \n\n\\myfigure{task_examples/pddl_action_desc.tex}{}{}\n\nIn the above snippet, the parameter line provides the possible variables, in this case \\textit{?ob}, which can stand for possible blocks. The precondition says that you can put down a block only if you are holding it (i.e. predicate \\textit{(holding ?ob)} is true for the block). The effects tell you that after you execute the action put-down, the block will be on the table and will be clear. You won't be holding the block and the arm will be considered \\textit{empty}. The actions may additionally be associated with cost, in these cases, one could also talk about optimal plans, i.e., a plan $\\pi$ is called an optimal one if no plan exists that is less costly than $\\pi$.\n\nThe above description presents one of the simpler classes of planning models and can be extended in multiple ways including allowing for object typing (including type hierarchy), more complex forms of preconditions and conditional effects, not to mention supporting richer classes of planning formalisms.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=\\textwidth]{Images/llmarch.png}\n    \\caption{The diagrammatic overview of the overall test framework. Our system consists of a domain-specific component that allows the generation of various instances of the specific PDDL planning problems and the translation from PDDL to text and back. The domain-independent component is responsible for generating the test instances that will be fed into the LLM and verifying the output generated by the LLM.}\n        \\label{fig:overall}\n        \\vspace*{-0.2in}\n\\end{figure}\n\n\\section{Assessment Architecture}\n\nOur basic test framework consists of two categories of components, the domain-independent ones, provided as part of the framework, and the domain-dependent components which need to be developed for each new domain we test.\n\n\\mund{Domain independent component} The domain-independent component is built around a planner and a plan verification component that takes various planning problems and crafts test instances corresponding to various curriculum items. This component provides the mechanism to verify the solutions generated by the LLM. The current method is going to operate almost exclusively on symbolic models (specifically ones specified using PDDL \\cite{McDermott1998PDDLthePD}) and other structured inputs compatible with such representations.  \n\nThis component is responsible for generating the content for the various prompts that would be generated as part of the different test cases and for validating the output generated by the LLM. As discussed earlier, the component primarily works on formal representations of the problems, so it relies on the translator component to convert any information it generates to natural language or to convert natural language information back to formal representations.\nFor each test case, we mainly rely on a domain-independent planner and a plan validator to generate the relevant information or to validate the output provided by the LLM. For some test cases, we incorporate domain-specific information while crafting the prompts. In each case, there is a test-case-specific component that uses the problems provided by the problem generator component to craft specific test-case content.\n\n\\mund{Domain dependent component} The domain-dependent component consists of three parts; a domain model, a problem generator and a translator. The lifted domain file describes the various actions that may be available to solve any given planning problem, the various predicates that could be used to describe the various relationships over the objects that may be present at a given problem instance of the domain, and the various types of objects that may be part of the given problem. The domain model is lifted as it does not refer to the actual objects that may be part of the problem, but instead, the actions are defined independently of the exact objects it may influence. \n\nThe role of the problem generator is to generate random problem instances consisting of various objects, initial states, and goals. These problems become the basis of generating the various test cases that we will be using throughout the framework. Any distributional requirements we hope to use in the tests could be built into this problem generator.\n\nThe translator converts the symbolic model information to natural language text and {\\em vice versa }. \n\\revised{Translating the prompts to natural language would benefit the users of the benchmark who might want to be in the loop to either impose additional constraints or evaluate the plan themselves as these two tasks are more naturally done in natural language.} \nFor the current testbed (described below), we developed a template-based mechanism to achieve this. In particular, we provide a natural language template for each predicate and each action, and we form texts of states and plans by concatenating these individual strings. In terms of parsing natural language text back into structured forms, the particular task we are interested in is converting plans generated by the LLM back into plan forms that can be used by plan validator tools like \\cite{howey2004val}. Since we use our prompts to shape the LLM's output, we require each action in the plan to be listed on a different line. Then, we can parse the exact action and arguments of the action by either using template-based matching or by assuming that the verb in the sentence corresponds to the action and each noun corresponds to an object which forms a parameter of the action (then mapping it to a possible action). \n\n\\section{Current Curriculum for Testing}\nIn this section, we go over each specific test case we provide as part of PlanBench. \nEach test case is meant to evaluate a central reasoning about actions and change capability and is tested in the context of a common sense planning domain. Each test case makes use of the few shot query setting of LLM where the LLM is provided a few sample answers to the specific reasoning ability being tested and is asked to respond to a new instance.\\footnote{\\karthik{While in this work we look only at one-shot natural language prompt configurations, we have also looked at various other prompt configurations (including zero-shot prompts, chain-of-thought prompts and pddl style prompts), specifically in the plan generation test case, in \\cite{valmeekam2023planning}.}} The exact form of the prompt will depend on the specific test cases, but every instance will start with a description of the lifted planning domain that describes what actions can be executed, their preconditions and their effects.\nThe current set of test cases includes the following cases: \n\\begin{enumerate}\n\\item Plan Generation - Can the LLM come up with valid plans that will achieve a specific goal?\n\n\\item Cost Optimal Planning -  Can the LLM come up with plans that are optimal to achieve a specific goal?\n\n\\item Plan Verification - Can the LLM determine if a plan will successfully execute, and if not, can it explain why?\n\n\\item Reasoning about plan execution - Can the LLM reason about what happens when a plan is executed?\n\n\\item Robustness to goal reformulation - Can the LLM recognize the same goal when specified in different ways?\n\n\\item Ability to reuse plans - Can the LLM recognize scenarios where it can reuse part or the whole of the original plan to achieve the new goal?\n\n\\item Replanning - Can the LLM replan for cases where an unexpected change is reported?\n\\item Plan Generalization - Can the LLM take specific plans, extract underlying procedural patterns and apply them to a new instance?\n\n\\end{enumerate}\nOut of the eight test cases, the first two test cases correspond to actual planning problems (i.e. plan generation and cost-optimal planning) and the rest correspond to simpler auxiliary tasks related to reasoning about action and change. We ground the test cases in multiple domains based on the kinds employed in International Planning Competitions. The domain description is included at the beginning of every prompt. In the rest of the section, we discuss the structure of the prompt for each of the test cases. We provide an example prompt and the corresponding completion generated by an LLM for each of the test cases in the Appendix. \n\n\\mund{Plan Generation}\nFollowing the lifted domain description, the prompt consists of a few instances of planning problem descriptions (consisting of a description of the initial state, the goal) and the corresponding plan (which ends with a tag, henceforth referred to as the plan-end tag, that denotes the end of the plan) and finally, we end the prompt with a planning problem description. {\\revised{The plan-end tag (which is separate from the end-generation tag) is introduced to enable easier plan extraction in cases where the LLM also adds commentary along with the plan. In the case where our extractor cannot reasonably extract a plan from the response, we mark that instance as an incorrect one. We present an example prompt for this test case here. For the rest of the test cases, the examples are provided in the Appendix.}}\n\\mytcbinput{task_examples/one_full_prompt.tex}{One-shot prompt for Plan Generation with GPT-4's plan}{0}{bw1-nl-one-shot}\n\\mund{Cost-Optimal Planning}\nThe prompt is quite similar to the one used in the earlier test case with a few changes. We modify the lifted domain description by including a statement that associates a cost with each action. To make the concept of action cost better fit into common sense domains, we can map the cost to more common concepts like the time taken for executing the action or the amount of money that needs to be spent to execute an action. In the case of each problem description, before the plan is presented we need to explicitly mention that the plan is trying to minimize cost (which depending on the scenario might correspond to saying that the plan takes the least amount of time or the plan correspond to the cheapest plan). The result generated by the LLM is evaluated similarly to the previous query, but in addition to checking if the plan is valid, we also check if the cost of the plan corresponds to the optimal plan cost.\n\n\\mund{Plan Verification}\n\\matt{\nPlan verification involves determining whether a proposed plan will successfully execute and achieve the stated goals when applied from the given initial state. Here, the prompt will first include three example instances, the corresponding candidate plans and the verification details. The set of examples will contain one example with a goal reaching plan, one with a non-goal reaching plan and one with an inexecutable plan. The prompt then contains a new instance along with a candidate plan. The LLM is tasked with answering whether the candidate plan is valid. If it answers no, it must identify the first inexecutable action and atleast one associated missing precondition (if inexecutable) or at least one missing goal condition (if not goal reaching). \n}\n\n\\mund{Reasoning about plan execution}\nHere the objective is not to check whether the LLM can come up with plans, or verify them, but rather if they can predict the outcome of executing an action. The prompt here again starts with the domain description, but instead of providing planning problems and plans, we provide a state, an action sequence and then the state that would result from executing that action sequence in the provided state. Finally the prompt ends with a new state and a new action sequence. The LLM is expected to come up with the resulting state, which is then checked by applying a plan executor that will try to identify what state will result from the execution of the current action sequence on the provided state. We always provide executable action sequences for this test case. \n\n\\mund{Robustness to Goal Reformulation}\nIn this test case, we will see if the LLM can recognize goals it has seen before if they are slightly modified. Here the prompt remains the same as the one used for plan generation. However, all the example problems have the same initial state, and the last problem provided has not only the same initial state but also the same goal as the example problem. Here the goal may be obfuscated in a few ways, for example, the goal facts may be reordered or one might include a subset of the original goal specification (meaning the same plan would still work) or vice-versa. We can again use the same evaluation technique as the plan generation test case to validate the output.\n\n\\mund{Ability to Reuse Plans}\nIn this test case, we are interested in seeing if the LLM can reuse plans or parts of plans that it has seen before. The prompt is again the same as the plan generation, but the prompt ends with a problem that can be solved by a prefix of a previously seen plan. We again keep the initial state the same across the example problems shown. The evaluation remains the same as the plan generation test case.\n\n\\mund{Replanning}\nReplanning corresponds to the problem where there may be an unexpected event that occurs while executing a plan and the system needs to come up with a new plan in response to the event. Here, we focus on the ability of the LLM to replan when unexpected changes are reported. The prompt here starts with a domain description, then a set of instances where an unexpected event occurred during execution, and a new plan in response to the event. In each instance, a planning problem and a corresponding plan are provided at the beginning, the execution of the plan is described and then an unexpected event is noted (event corresponds to some facts unexpectedly turning true or false) and then a new plan from the changed state is presented. The prompt ends with a new case where the plan after replanning is left out and the LLM is expected to complete. The evaluation involves checking whether the new plan is valid from the changed state. The LLM output is evaluated to be true if the new plan it generates achieves the goals from the unexpectedly changed state. For each of the domains, we constrain the unexpected event to be of a specific type. We describe these cases for each domain in the next section.\n\n\\mund{Plan Generalization}\nIn this test case, we want to evaluate whether LLMs can recognize the underlying pattern in the plans provided in the prompt and reuse it for a new planning problem. The prompt is the same as the plan generation case, except that all plans were generated by a fixed program. Here the program may contain loops or conditional statements, but can only solve certain types of problems, that is, the initial state and goals meet certain conditions. Such programs can be thought of as a direct generalization of line plans that we have considered in the rest of the paper \\cite{srivastava2011qualitative}. Execution of this program for a specific planning problem generates a sequence of actions. In this case, we will provide some example traces generated from the program and ask LLM to come up with a plan for a new problem that could be solved by it. The evaluation again would be to take the generated plan and see if it is valid for the given problem. \\matt{Since this task requires the use of specific problems that admit generalizable solutions, not every problem works for this task. As such, this task requires problems to be curated for each domain. To develop this set of problems, we select a domain-specific generalized behavior and then create several problems that can be accomplished by merely adding more iterations of the generalized behavior to an example plan. We detail our domain-specific selection of behavior in Section \\ref{domain-details}.}\n\n\\section{Dataset Details}\n\\label{domain-details}\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=\\textwidth]{Images/blocksworld/blocksworld_stats.png}\n    \\caption{Distribution of the number of objects and optimal plan length for the Blocksworld problem set}\n    \\label{fig:blocksworld-stats}\n    \\vspace*{-0.2in}\n\\end{figure}\n\n\\matt{While PlanBench makes it easy to run the curriculum on new domains, we initialize it with two different International Planning Competition (IPC) \\cite{ipc} domains: Blocksworld and Logistics. For each domain, we provide a description, distribution of selected problems, and details on domain-specific curriculum configurations below.}\n\n\\mund{Blocksworld}\n\\matt{The Blocksworld domain focuses on stacking blocks on a table. One hand is available to move blocks, and only one block may be moved by the hand at a time. Blocks cannot be moved if there are blocks on top of them and blocks cannot be stacked on a block with another block already on top of it. Goals specify the order that blocks within a stack should be stacked in but may include multiple stacks or ask for blocks to be left on the table. Blocks our identified with colors. We developed 600 instances which vary in the number of objects used as well as the optimal plan length: we visualize these distributions in Figure \\ref{fig:blocksworld-stats}. For the plan generalization test case, we focus on problems that can be solved by repeatedly adding clear blocks to the stack and generate 500 instances separate from the main blocksworld dataset. For the replanning test case we constrain the unexpected event to be of a specific type: We execute a random prefix of the plan which ensures that some block is held at the end of that prefix. We then change the resulting state by stacking the held block onto another random block which is clear and make the hand empty. This change is reported and the LLM is asked to replan from the changed state. }\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=\\textwidth]{Images/logistics/logistics_stats.png}\n        \\caption{Distribution of the number of objects and optimal plan length for the Logistics problem set}\n    \\label{fig:logistics-stats}\n    \\vspace*{-0.3in}\n\\end{figure}\n\n\\mund{Logistics}\n\\matt{The Logistics domain involves moving packages between different locations. Locations are grouped by cities. Trucks can be used to move packages between locations in the same city and planes can be used to move packages between cities. Goals specify where packages should be moved. We generate 285 instances for this domain We provide the distribution of objects (which includes cities, locations, trucks, airplanes and packages) and optimal plan length over the problems in Figure \\ref{fig:logistics-stats}. For the plan generalization test case we focus on problems that contain one or more instances of the following; dropping off a package at a location and picking up the next package at that same location, allowing for a chain of package movement within a city through a truck and finally have a package at an airport be flown to another airport. For the replanning test case, we execute a random prefix of the plan which ensures a package is in a truck or an airplane. We then shift the position of the package from inside the truck or airplane to a random location which is not the goal location. This change is reported and the LLM is asked to replan from the changed state.}\n\n\\mund{Obfuscation of domains}\n\\karthik{Although the domain specification is part of our prompts, the names of the objects (e.g. blocks, trucks), predicates (e.g. on-table, in-city) and actions (e.g. pickup, drive) still do provide connections to the commonsense knowledge that the pretrained LLMs possess. One intriguing question is whether the planning performance is based really only on the domain model or patterns discovered from these other background connections. In order to test this, as part of our benchmark, we also provide obfuscated versions of the above domains, where the action names, predicate names and object names are obfuscated either with misleading words or random alphanumeric strings. Note that, for a standard planner, the original domain and the obfuscated version are identical. Further, we also provide the code to perform arbitrary obfuscations for the above domains or any additional domains that might be added in the future.}\n\n\\karthik{On the whole, our dataset consists of $\\sim$26250 prompts across the various test cases and domains (including the obfuscated versions). We will now look at the results of GPT-4 and InstructGPT-3 on the Blocksworld domain in PlanBench.}\n\n\\section{Specimen Evaluation of PlanBench}\nWhile the objective of this paper is to make PlanBench available to other researchers, we also did some initial experiments to give useful baselines. Our evaluation here primarily focuses on two Large Language Models, GPT-4 and InstructGPT3. We used the OpenAI API to access these models. In particular, we evaluated the test framework on the Blocksworld domain. In Table \\ref{tab:my_label}, we have presented the results of GPT-4 and Instruct-GPT3 (text-davinci-002) on PlanBench. The best results within each model were observed for the auxiliary goal reformulation test cases. However, even the most effective model (GPT-4) falls short on most of the test cases in the Blocksworld domain of PlanBench. Overall, the performance of these LLMs on our benchmark shows that, as of right now, LLMs are pretty ineffective in reasoning about actions and change. PlanBench can thus serve as a useful marker of progress of LLMs in planning and reasoning. \\revised{In a companion study \\cite{valmeekam2023planning}, we conducted a deeper investigation into the planning abilities of current\nstate-of-the-art LLMs, critically examining their plan generation abilities under various\nassumed roles. We performed experiments with different prompt configurations, delved into the\nfailure modes of LLMs for autonomous plan generation and analyzed performance shifts when LLMs\nassume heuristic roles (LLM-Modulo settings) in planning systems.}\n\n\\begin{table*}[t]\n    \\centering\n    \\small\n     \\caption{PlanBench Results of GPT-4 and Instruct-GPT3 (text-davinci-002) on Blocksworld domain. The tasks in the highlighted rows correspond to actual planning problems while the others correspond to simpler auxiliary planning tasks.}\n    \\label{tab:my_label}\n    \\begin{tabular}{p{10cm}  p{1.05cm}  p{1.2cm} }\n    \\toprule\n        \\textbf{Task} & \\multicolumn{2}{c}{\n        \\centering\n        \\textbf{Instances correct}} \\\\ \\cmidrule{2-3}\n        & GPT-4 & I-GPT3 \\\\ \\midrule[0.08em] \n        \\rowcolor{backcolour}\n        \\specialcell{\\textbf{Plan Generation} \\\\{\\scriptsize We showcase an instance and the respective plan as an example and prompt the machine with a new instance. }} & 206/600 (34.3\\%) & 41/600 (6.8\\%) \\\\ \\midrule[0.08em] \n        \\rowcolor{backcolour}\n        \\specialcell{\\textbf{Cost-Optimal Planning} \\\\{\\scriptsize We showcase an instance, the respective optimal plan and the associated cost as an example and prompt the machine with a new instance. }} & 198/600 (33\\%) & 35/600 (5.8\\%)  \\\\ \\midrule[0.08em] \n        \\specialcell{\\textbf{Plan Verification} \\\\{\\scriptsize We showcase three instances and three distinct plans (goal reaching, non goal-reaching and inexecutable) and present the respective validation and explanations. We then present a new instance and a plan and ask the machine for to verify and provide an explanation, if needed.  }}  & 352/600 (58.6\\%) & 72/600 (12\\%) \\\\ \\midrule[0.08em]\n        \\specialcell{\\textbf{Reasoning About Plan Execution} \\\\{\\scriptsize We showcase an instance, an action sequence and the corresponding resulting state after executing the action sequence as an example. We then provide an instance and an executable action sequence and ask the machine to provide the resulting state.}}  & 191/600 (31.8\\%) & 4/600 (0.6\\%) \\\\ \\midrule[0.08em]\n        \\specialcell{\\textbf{Replanning} \\\\{\\scriptsize We showcase an instance, the respective plan and present an unexpected change of the state. We then also present a new plan from the changed state. Finally, for a new instance we repeat the same except we ask the machine for the new plan.  }}  & 289/600 (48.1\\%) & 40/600 (6.6\\%) \\\\ \\midrule[0.08em]\n        \\specialcell{\\textbf{Plan Generalization} \\\\{\\scriptsize We showcase an instance and the respective plan as an example and prompt the machine with a new instance. The plans for both the instances can be generated by a fixed program containing loops and conditionals.}} & 141/500 (28.2\\%) & 49/500 (9.8\\%) \\\\ \\midrule[0.08em]\n        \\specialcell{\\textbf{Plan Reuse} \\\\{\\scriptsize We showcase an instance and the respective plan as an example and prompt the machine with an instance which requires only a certain prefix of the plan provided in the example.}} & 392/600 (65.3\\%) & 102/600 (17\\%)  \\\\ \\midrule[0.08em] \n        \\specialcell{\\textbf{Robustness to Goal Reformulation {\\small (Shuffling goal predicates)}} \\\\{\\scriptsize We showcase an instance and the respective plan as an example and prompt the machine with the same instance but shuffle the ordering of the goals.}} & 461/600 (76.8\\%) & 467/600 (77.8\\%) \\\\ \\midrule[0.08em]\n        \\specialcell{\\textbf{Robustness to Goal Reformulation {\\small (Full $\\rightarrow$ Partial)}}\\\\{\\scriptsize We showcase an instance with a fully specified goal state and the respective plan as an example and prompt the machine with the same instance but provide a partially specified goal state.}}& 522/600 (87\\%) & 467/600 (77.8\\%) \\\\ \\midrule[0.08em] \n        \\specialcell{\\textbf{Robustness to Goal Reformulation {\\small (Partial $\\rightarrow$ Full)} }\\\\ {\\scriptsize We showcase an instance with a partially specified goal state and the respective plan as an example and prompt the machine with the same instance but provide a fully specified goal state.}}& 348/600 (58\\%) & 363/600 (60.5\\%)  \\\\ \\bottomrule \n    \\end{tabular}\n    \\vspace*{-0.2in}\n\\end{table*}\n\n\\section{Conclusion and Future Work}\nIn this paper, we presented PlanBench, a reasoning assessment suite for large language models (LLMs) that consists of various test cases each evaluating a central aspect of planning and reasoning about actions and change. Our results show that even in simple common-sense planning domains, LLMs seem to display subpar performance. Our goal is to establish an extensible benchmark where researchers can evaluate the current and future large language models. Our assessment suite can be improved in multiple ways in the future. For instance, evaluation metrics that consider partial correctness could be incorporated into the benchmark. Additionally, the benchmark could be extended to support more number of IPC domains.\nIn conclusion, we hope that PlanBench\nencourages other researchers to test the capabilities of their systems across different LLM models \\cite{chowdhery2022palm, du2021glam, smith2022using, zhang2022opt, rae2021scaling, thoppilan2022lamda, hoffmann2022training} and even those that are finetuned for such tasks.\\section{Acknowledgements}\nThis research was supported  by ONR grants N00014-18-1-2442,\nN00014-18-1-2840, N00014-19-1-2119 and N00014-23-1-2409, AFOSR grant\nFA9550-18-1-0067, DARPA SAIL-ON grant W911NF-19-2-0006, and a JP Morgan\nAI Faculty Research Grant to Kambhampati. Sreedharan was supported in\npart by NSF grant 2303019. We would also like to thank Kaya Stechly and Anil Murthy for their help in adding additional domains to the benchmark.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Reflection-Bench: probing AI intelligence with reflection}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nThe ability to adapt beliefs or behaviors in response to unexpected outcomes, \\textit{reflection}, is fundamental to intelligent systems' interaction with the world. From a cognitive science perspective, this serves as a core principle of intelligence applicable to both human and AI systems. To address the debate on the intelligence of large language models (LLMs), we propose \\emph{Reflection-Bench}, a comprehensive benchmark comprising 7 tasks spanning core cognitive functions crucial for reflection, including perception, memory, belief updating, decision-making, prediction, counterfactual thinking, and meta-reflection. We evaluate the performances of 13 prominent LLMs such as OpenAI o1, GPT-4, Claude 3.5 Sonnet, etc. The results indicate that current LLMs still lack satisfactory reflection ability. We discuss the underlying causes of these results and suggest potential avenues for future research.  In conclusion, Reflection-Bench offers both evaluation tools and inspiration for developing AI capable of reliably interacting with the environment. Our data and code are available at \\url{https://github.com/YabYum/ReflectionBench}.\n\\end{abstract}\n\n\\section{Introduction}\nLarge language models (LLMs) have sparked intense debate regarding their true capabilities: \\emph{are they genuinely intelligent or simply sophisticated statistical engines mimicking human language?}~\\cite{bender2021dangers} This question has profound implications for trust in AI and the development of appropriate regulations. Proponents of AI's human-level intelligence often advocate for stricter regulations due to potential existential risks, while skeptics argue that excessive regulation could hinder innovation. Numerous studies probe specific facets of LLM intelligence, such as reasoning \\cite{clark2018think}, planning \\cite{valmeekam2024planbench}, and cognitive flexibility \\cite{kennedy2024cognitive}, etc. However, these investigations often lack a unifying framework grounded in a fundamental theory of intelligence, hindering a comprehensive understanding of LLM capabilities. Our work aims to clarify this enigma and establish efficient metrics for assessing intelligence by drawing insights from cognitive science into the fundamental process of intelligence, \\textit{reflection}.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\columnwidth]{figure/fig1.png}\n    \\caption{Reflection, a fundamental process of intelligence, integrates various cognitive components. To achieve desired outcomes, an intelligent agent must predict the external world states and behavioral consequences based on prior beliefs. Post-action, discrepancies between prediction and observation are perceived, prompting an update of prior belief. This update involves recalling the previous decision process and engaging in counterfactual thinking about un-chosen alternatives.}\n    \\label{fig1}\n\\end{figure}\n\nFrom a first-principles perspective, cognitive science conceptualizes intelligent systems as predictive machines that constantly anticipate future events using internal models \\cite{friston2010free}. They can cope with uncertain environments by minimizing surprises (mismatch between predictions and observations) through iterative updates to thoughts and actions, as shown in Figure \\ref{fig1}. We operationalize this process as reflection, a cyclic process of predicting based on prior belief, making decisions to achieve desired states, perceiving surprises, and subsequently updating beliefs. Reflection encompasses crucial cognitive components including perception, memory, belief updating, decision-making, prediction, counterfactual thinking, and even meta-reflection. Leveraging this framework, we introduce \\emph{Reflection-Bench}, a benchmark designed to evaluate the reflection capabilities of LLMs.\n\nBased upon well-established cognitive science paradigms, we select 6 paradigms and design 7 tasks adapted for LLMs evaluation, including oddball paradigm, n-back task, probabilistic reversal learning task, Wisconsin card sorting test, weather prediction task, double-choice Iowa gambling task, and meta-bandit task. Reflection-Bench leverages these 7 tasks to offer a comprehensive evaluation of different aspects while engaging in reflection. Importantly, the difficulty of these tasks can be adjusted to accommodate varying cognitive loads, ensuring Reflection-Bench's adaptability to more advanced AI models and maintaining its long-term relevance.\n\nWe evaluated 13 diverse LLMs on Reflection-Bench using relatively easy settings and conducted an in-depth analysis of their performance. The results demonstrate Reflection-Bench's discriminative power: o1-preview achieved the highest scores, followed by the other state-of-the-art LLMs, while models with smaller sizes scored lower. We highlight a notable contrast between o1-preview's performances in the oddball paradigm and its performance in other tasks. Additionally, we observed a universal absence of meta-reflection across all models. The potential causes and implications are discussed. In conclusion, our findings indicate that current LLMs still fall considerably short of human-level intelligence.\n\nTo summarize, our main contributions are as follows:\n\\begin{itemize}\n    \\item     \\textbf{We introduce reflection, a core feature of intelligent systems,  into the AI field} as a biologically-inspired criterion for probing AI intelligence. This approach offers a more nuanced understanding of LLMs' intelligence that aligns with humans.\n    \\item  \\textbf{We propose Reflection-Bench}, a comprehensive benchmark comprising seven tasks adapted from cognitive science paradigms to assess the reflection capabilities of LLMs. This benchmark decomposes reflection into crucial cognitive components: \\textbf{perception, memory, belief updating, decision-making, prediction, counterfactual thinking, and meta-reflection}. \n    \\item We conduct an assessment of 13 prominent LLMs. The study reveals that \\textbf{current LLMs exhibit significant limitations in their capacity for human-like reflection}, especially the universal lack of meta-reflection abilities.\n\\end{itemize}\n\n\\section{Related work}\n\n\\paragraph{Emergent ability}  `Emergent abilities' in LLMs are defined as capabilities `not present in smaller models but present in larger models'\\cite{wei2022emergent}. These manifest as the ability to `perform above the random baseline on corresponding tasks without explicit training on those same tasks', such as reasoning and planning. \\cite{lu2024emergentabilitieslargelanguage}. Some researchers argue that these emergent abilities might disappear with better metrics or statistics \\cite{NEURIPS2023_adc98a26}. This underscores the importance of designing valid and efficient metrics for evaluating intelligence, one of the most debated emergent properties of LLMs. \n\n\\paragraph{Evaluating intelligence} Various benchmarks have been developed to assess different aspects of LLMs' intelligence. The AI2 Reasoning Challenge (ARC) evaluates reasoning, common sense, and text comprehension using multiple-choice questions \\cite{clark2018think}. PlanBench tests planning and reasoning with open-ended questions \\cite{valmeekam2024planbench}. CELLO assesses four levels of causal inference - discovery, association, intervention, and counterfactual - with multi-choice questions \\cite{chen2024cellocausalevaluationlarge}. Beyond the single-turn evaluations, researchers also investigate LLMs' intelligence in uncertain environments through multi-turn dialogues. These include tasks such as multi-armed bandits for probability estimation \\cite{krishnamurthy2024can}, Wisconsin card sorting tests for rule inference \\cite{kennedy2024cognitive}, Minecraft for spatial reasoning \\cite{madge2024llmbenchmarkbasedminecraft}, etc. These aspects are interconnected in ways related to the epistemology of AI systems - how they learn about and adapt to the external world. While this feature has been vaguely recognized but not clearly defined in AI field. \n\n\\paragraph{First principle of intelligence} A first-principles perspective in cognitive science conceptualizes intelligent systems as predictive machines that continuously predict future events with internal models\\cite{friston2010free}. They can adapt to environments flexibly by minimizing unexpected events through updating thoughts or actions. This theory has been applied to interpret intelligent systems across different scales, such as in vitro neural networks \\cite{isomura2023experimental}, sensory system \\cite{huang2011predictive}, cognitive process \\cite{spratling2016predictive}, consciousness \\cite{solms2019hard}, and self-identity \\cite{li2024enabling}. Therefore, this theory serves as the first principle of intelligence. Considering the cyclic nature of this process as showcased in Figure \\ref{fig1} - prediction, verification, and update - we term it \\textit{reflection}. Through reflection, we can comprehensively assess various aspects of intelligence encompassing the aforementioned research interests. \n\n\\section{Reflection-Bench}\n\n\\begin{figure*}\n\\centering\n\\includegraphics[width=\\textwidth]{figure/fig2.png}\n\\caption{Tasks in Reflection-Bench. A: Oddball paradigm. B: N-back (2-back). C: Probabilistic reversal learning task. D: Wisconsin card sorting test. E: Weather prediction task. F: Iowa gambling test. G: Meta-bandit task. }\n\\label{fig2}\n\\end{figure*}\n\\subsection{Task selection for evaluating reflection}\nAs discussed, reflection is a complex capability collaborated by cognitive components including perception, working memory, belief updating, decision-making, prediction, counterfactual thinking, and meta-reflection. Drawing from well-established cognitive science paradigms, we select 6 paradigms and design 7 tasks adapted for LLMs evaluation in Reflection-Bench, as follows:\n\n\\paragraph{Perception} For perceiving surprise signals, we select the \\textit{oddball paradigm} \\cite{naatanen2007mismatch}, a method to study the brain's automatic processing of novel stimuli. As shown in Figure \\ref{fig2}.A, participants are presented with a sequence of auditory stimuli, including frequent standard and rare deviant stimuli (\\emph{e.g.}, different tones). Electroencephalogram recordings show that the brain is naturally sensitive to deviant stimuli, as reflected in a deviance-related negative waveform called Mismatch Negativity (MMN) \\cite{garrido2009mismatch}. This automatic phenomenon represents the basic ability to perceive surprise signals, which aligns with our intention appropriately.\n\n\\paragraph{Memory} Recalling the previous decision process requires active memory retrieval. We select the \\textit{n-back} task for evaluating this ability \\cite{jaeggi2010concurrent}. As shown in Figure \\ref{fig2}.B, presented with sequential stimuli (typically symbols), subjects need to indicate whether the current stimulus matches the one from n steps earlier. The process of continuously updating and maintaining information in the n-back task fits our objective well.\n\n\\paragraph{Belief updating} Focusing on belief updating, we involve \\textit{probabilistic reversal learning task} (PRLT) \\cite{costa2015reversal} in our benchmark. As illustrated in Figure \\ref{fig2}.C, participants engage in a two-arm bandit task, making choices between two options with different reward probabilities, which they need to infer to maximize rewards. At the midpoint of the task, the reward probabilities are reversed without notification. This task focuses on the ability to update beliefs in response to the changing environment. \n\n\\paragraph{Decision-making} We employ the \\textit{Wisconsin card sorting test} (WCST) to assess the flexibility in decision-making \\cite{nyhus2009wisconsin}. Participants are presented with a set of cards varying in color, shape, and number of figures, and must match the given card according to an undisclosed rule (e.g., by color) which they must infer from feedback on their choices. (Figure \\ref{fig2}.D). After a certain number of trials, the matching rule changes without warning. Both PRLT and WCST require flexible belief updating, while WCST focuses more on inferring which latent rule one should obey when making decisions.\n\n\\paragraph{Prediction} Computationally, prediction is realized via transition probability \\cite{friston2010free}. There are no tests directly evaluating transition probabilistic learning in cognitive science. Therefore we designed an adapted version of the \\textit{weather prediction task }(WPT) originally designed to study probabilistic classification learning \\cite{shohamy2008basal}. In the original version, subjects need to predict the weather based on card cues and gradually learn the probability relationship between card cues and weather. We convert this probability into the explicitly defined transition probability matrix as shown in Figure \\ref{fig2}.E. The evaluated model must learn how the cues (sensors) influence the transition of weather, through which their flexible prediction ability is assessed.\n\n\\paragraph{Counterfactual thinking} Counterfactual thinking refers to creating an alternative to reality by considering 'what if' \\cite{annurev-psych-122414-033249}. Therefore we test this ability by forcing models to consider the un-chosen options with an opportunity to `turn back time'. We select \\textit{Iowa gambling task }(IGT) for our design \\cite{buelow2009construct}. In IGT, participants are presented with four decks of cards (A, B, C, and D) with distinct expected rewards and losses, shown in Figure \\ref{fig2}.F. They must maximize their profit by selecting cards from these decks over a series of trials. We implement a double-choice IGT (DC-IGT) where after each initial choice, models are given the possible outcomes of the choice and the opportunity to go back in time to make their choice again.\n\n\\paragraph{Meta-reflection} As illustrated in Figure \\ref{fig1}, meta-reflection is the process of reflecting the previous reflection, therefore, we design a \\textit{meta-bandit task} (MBT) based on the PRLT. In MBT, the the reward probabilities are reversed every \\textbf{n} trials, creating a predictable pattern of rule changes (Figure \\ref{fig2}.G).  This design introduces a ``rule of rule changing'', requiring participants to not only adapt to reversals but also to recognize and anticipate the meta-structure of these changes, i.e., meta-reflection.\n\nThrough these 7 tasks, Reflection-Bench offers a comprehensive evaluation of different aspects while engaging reflection. Notably, there are no strict correspondences between these tasks and specific cognitive components, only with certain aspects being more prominently featured or assessed.\n\n\\subsection{Task design for assessing LLMs}\n\nThe selected six paradigms are widely used in cognitive science for investigating human intelligence. We apply them to evaluate LLMs in Reflection-Bench through the following adaption:\n\n\\paragraph{Oddball paradigm} We design a prompt-based oddball task for evaluating LLMs. The prompt consists of seven short sentences about one topic (stimulus A) and one random sentence (stimulus B) causing content consistency interruption (example in Appendix~\\ref{appendix1}). The prompts are presented to LLMs with only the instruction to `just make some brief comments', allowing us to evaluate their capability to automatically detect surprise signals. We compile 50 such prompts with the help of the o1-preview model. Model's responses are manually scored from 0 to 3  (example in Appendix~\\ref{appendix1}), corresponding to:\n\\begin{itemize}\n    \\item 0: forced explanation or neglect to B\n    \\item 1: simple enumeration of A and B\n    \\item  2: pointing out that A and B are different\n    \\item 3: clearly stating B is nonsensical in this sequence\n\\end{itemize}\n\n\\paragraph{N-back} For n-back implementation, we set a fixed sequence consisting of several letters (e.g. E, F, G, and H) and send them to evaluated models one by one. The models are instructed to determine if the current letter is the same as the one that appears \\textbf{n} steps earlier. The model's accuracy is calculated as its score. \n\n\\paragraph{Probabilistic reversal learning task} We design a 40-trial PRLT. In each trial, the LLM is presented with two options and asked to make a choice. The reward for the chosen option is sampled from a Bernoulli distribution. The LLM is then prompted to make its next choice based on the reward feedback. The reward probabilities of the two options (left and right arms) are \\textbf{p} and \\textbf{1-p} initially and reversed at the 21st trial. The models' beliefs on the reward probability are estimated according to their decisions over time by a moving average (window size = 3). The overall score is calculated by the mean absolute errors (MAE) between estimated and true probability:\n\\[Score = (1 - \\text{MAE}/\\text{Max}_{\\text{MAE}})*100\\]\n\\paragraph{Wisconsin card sorting test} Similar to the designation of \\cite{kennedy2024cognitive}, we implement WCST in a text-based version consisting of \\textbf{x} trials, and the matching rule changes every \\textbf{x/6} trials, which means each rule can be applied 2 times. In each trial, the LLM is presented with a description of one testing card (\\emph{e.g.}, `triangle green 4'). The model is instructed to match the target card among four choices without being told the matching rule. After each choice, the model receives feedback on whether the match was correct. We evaluated the LLMs' performance based on their overall accuracy.\n\n\\paragraph{Weather prediction task} In each trial, LLM is presented with the current day's weather and the sensor state ([0,1] or [1,0]), and required to predict the next day's weather based on the two information. Actual weather is calculated with the corresponding transition matrix: \n\\[T_{sensors=[1,0]=}\\begin{bmatrix}\n   p & 1-p \\\\\n   1-p & p\n\\end{bmatrix}\\]\n\n\\[T_{sensors=[0,1]=}\\begin{bmatrix}\n   1-p & p \\\\\n   p &1-p\n\\end{bmatrix}\\]\nAt the next trial, LLM is informed of the actual weather which is also the current day's weather for this trial and the state of sensors, and required to make a new prediction again. We estimate the transition probability matrices of models according to their last 20 predictions in 100 trials. Models' performances are evaluated in the same way as PRLT, \\emph{i.e.}, based on MAE between estimated and true transition matrices. \n\n\\paragraph{Double choice Iowa gambling task} We implement DC-IGT for 100 trials. Gains of four decks are \\$100, \\$100, \\$50, and \\$50, and their losses are \\$260, \\$1250, \\$50, and \\$200 with possibilities of \\(\\textbf{P}_\\textbf{loss}={p_a, p_b, p_c, p_d}\\), respectively. In each trial, the LLM is presented with four card decks and asked to make an initial choice. The gain and loss of its choice are calculated with the above rule. After receiving feedback on the gain and loss, the model is prompted to make a second choice, with the option to stick with or change their initial decision. The LLM's performances are the composite of short-term (beneficial switches that avoid losses) and long-term (final overages) metrics. Notably, the scores in DC-IGT are normalized from the raw scores, so the marks only represent performances relative to other models.\n\n\\paragraph{Meta-bandit task} The MBT consists of 20 blocks of \\textbf{n} trials each. In each trial, the LLM is presented with two options and asked to make a choice. The reward for the chosen option is sampled from Bernoulli distribution and then communicated to the model (either 0 or 1). The reward probabilities for the two options are \\textbf{p} and \\textbf{1-p}, respectively, and reversed every \\textbf{n} trials without notification. By setting \\textbf{p} = 1, we analyze the model's rewards over the task to judge whether it successfully recognizes the fixed pattern of reversals - if so, the model could get a reward in the reversal trials.\n\n\\section{Experiment}\n\n\\begin{table}\n  \\centering\n  \n\\begin{tabular}{ccc}\n    \\hline\n    \\textbf{Task}& \\textbf{Trials} &\\textbf{Parameters}\\\\\n    \\hline\n    Oddball& 50&NA\\\\\n    N-back& 52&\\textbf{n}=2\\\\\n    PRLT& 40&\\textbf{p}=0.9\\\\\n    WCST& 108&\\textbf{x}=108\\\\\n    WPT& 100&\\textbf{p}=0.9\\\\\n    DC-IGT& 100&$\\textbf{P}_\\textbf{loss}={0.5, 0.1, 0.5, 0.1}$\\\\\n    MBT& 60&\\textbf{n}=3, \\textbf{p}=1\\\\\n    \\hline\n  \\end{tabular}\n  \\caption{Experiment settings of our experiment including trials and parameter settings in each task.}\n  \\label{table1}\n\\end{table}\n\n\\subsection{Experimental Setup}\nWe evaluate the performances of 13 LLMs on above 7 tasks, including o1-preview, o1-mini, GPT-4, GPT-4o, GPT-4o-mini \\cite{ChatGPT}, Claude-3.5-Sonnet \\cite{Claude}, Gemini-1.5-pro \\cite{reid2024gemini}, Llama-3.1-405B-Instruct, Llama-3.1-70B-Instruct, Llama-3.1-8B-Instruct \\cite{dubey2024llama}, Qwen-2.5-72B-Instruct, Qwen-2.5-32B-Instruct, and Qwen-2.5-14B-Instruct \\cite{yang2024qwen2}. \nThe evaluations are implemented using the corresponding API. \nTable \\ref{table1} shows the trials and repeating sessions of each task. \nTo mitigate potential scorer bias in the manual scoring process, we repeat oddball paradigms 3 times with the temperature parameter set to 0. \nTo ensure deterministic responses, the temperature parameter in the other 6 tasks tested and scored automatically was set to 0. \nConsidering the feedback in several tasks is sampled probabilistically, they were repeated 2 sessions and calculated average performances to reduce randomness. \nIn total, we utilized API 1,470 times for each model. The total costs of API calls were approximately \\$500, and the cost of o1-preview was around 60\\% higher than the summation of all other 12 models' cost.\n\\begin{table*}\n  \\centering\n  \\begin{tabular}{c|cccccccc}\n    \\hline\n    \\textbf{Model}& \\textbf{Oddball}& \\textbf{2-back} &\\textbf{PRLT} & \\textbf{WCST}& \\textbf{WPT}& \\textbf{DC-IGT}&\\textbf{Overall} &\\textbf{MBT}\\\\\n    \\hline\n    o1-preview& 58.22&                            \\textbf{100}& 87.07& \\textbf{85.29}& \\textbf{71.48}& \\textbf{83.78}& \\textbf{80.97}&0\\\\\n    o1-mini& 64.89&                            \\underline{86.54}& 63.73& 53.92& 35.38& 14.20& 53.11&0\\\\\n    GPT-4& 90.00&                            81.73& 87.05& 53.43& 40.93& 61.48& \\underline{69.10}&0\\\\\n GPT-4o& 62.22& 75.00& 84.42& 56.37& 29.70& 81.05& 64.79&0\\\\\n    GPT-4o-mini& 34.44&                            54.81& 68.43& 50.98& 41.08& 43.99& 48.96&0\\\\\n Claude-3.5-Sonnet& 86.22& 77.88& 81.28& 53.43& 26.73& 76.57& 67.02&0\\\\\n Gemini-1.5-pro& 80.00& 48.08& 88.46& \\underline{60.29}& 53.31& \\underline{81.82}& 68.66&0\\\\\n Llama-3.1-405B-Instruct& \\textbf{94.67}& 70.19& 79.38& 29.90& \\underline{67.22}& 70.95& 68.72&0\\\\\n  Llama-3.1-70B-Instruct& \\underline{92.22}& 57.69& \\textbf{90.30}& 53.92& 50.33& 51.29& 65.96&0\\\\\n Llama-3.1-8B-Instruct& 50.67& 57.69& 60.12& 49.51& 29.52& 39.91& 47.90&0\\\\\n Qwen-2.5-72B-Instruct& 57.33& 73.08& \\underline{89.42}& 52.94& 29.38& 55.48& 59.61&0\\\\\n Qwen-2.5-32B-Instruct& 45.56& 75.00& 65.48& 52.94& 23.71& 43.66& 51.06&0\\\\\n    Qwen-2.5-14B-Instruct&                         54.22& 66.35& 56.67& 51.96& 46.75& 67.48& 57.24&0\\\\\n    \\hline\n  \\end{tabular}\n  \\caption{Performances of 13 models on Reflection-Bench.}\n  \\label{table2}\n\\end{table*}\n\n\\subsection{Experiment Results}\n\n\\paragraph{Main results} The overall results are listed in Table \\ref{table2}.  \nIn general, Reflection-Bench shows good discriminative power. o1-preview stands out among all 13 evaluated models, followed by the current top-tier models such as GPT-4, Llama-3.5-405B, Gemini-1.5-pro, and Claude-3.5-sonnet. Smaller models tend to perform worse including Llama-3.1-8B, GPT-4o-mini, Qwen-2.5-32B, and o1-mini. In MBT, all models fail to recognize the pattern of reward reversals and thus get zero points, therefore, MBT is not included in the calculation of the total score. In the following paragraphs, we analyze the results task by task.\n\n\\paragraph{Oddball paradigm} As showcased in Figure \\ref{fig3}, most LLMs can detect contextual inconsistencies automatically to some extent without explicit instructions. Llama-3.1-405B \\& 70B and GPT-4 demonstrate the strongest responses to deviant stimulus, indicating a robust capability to perceive unexpected information. GPT-4o-mini, however, does not demonstrate this ability given that its average score of each response is \\(34.44 * 4.5 / 150 = 1.03\\) which means the simple enumeration of two different topics. Interestingly, MMN deficits are identified as a robust feature of mental disorders such as schizophrenia suggesting cognitive impairments \\cite{umbricht2005mismatch}. And the `MMN deficits' of GPT-4o-mini are aligned with poor performances in all other tasks. Different from GPT-4o-mini, despite the `MMN deficits', o1-preview still achieves superior performances in other tasks.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.50\\textwidth]{figure/fig3.png}\n    \\caption{MMN-like waveforms demonstrating the response of LLMs to deviant stimuli in an oddball paradigm. A deeper curve means a higher response.}\n    \\label{fig3}\n\\end{figure}\n\\paragraph{2-back} Although we provide all conversation history to evaluated models when testing, determining whether the current stimulus matches the one from 2 steps earlier can be difficult for some models. Only o1-preview got full marks, and the accuracy of models like Gemini-1.5-pro and GPT-4o-mini is even around 50\\%.\n\n\\paragraph{Probabilistic reversal learning task} In PRLT, Qwen-2.5-14B, and Llama-3.1-8B show little learning behavior, as illustrated in Figure \\ref{fig4}, and o1-mini, Qwen-2.5-32B, and GPT-4o-mini demonstrate rigid beliefs that do not converge to the true reward possibility after reversed. Other models could update beliefs relatively flexibly. Additionally, we find that most models adopted a `win-stay-lose-switch' strategy. This might be the reason why these models perform similarly with 7 models scoring over 80 points.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.50\\textwidth]{figure/fig4.png}\n    \\caption{Probabilistic reversal learning task. The black dashed line represents the true reward possibility of the bandit's left arm over trials, and other solid lines represent the average ratio of the left arm chosen. }\n    \\label{fig4}\n\\end{figure}\n\\paragraph{Wisconsin card sorting test} In WCST, the score of o1-preview is 25 points higher than the second place, Gemini-1.5-pro, so we further analyze the accuracy of models by 6 rule groups over 108 trials, shown in Figure \\ref{fig5}. To summarize, Llama-3.1-405B \\& 8B failed to obey any rule in the test. Most models fell into the `shape sink' for the whole test after the first rule group (shape). Gemini-1.5-pro successfully transited the matching rule from shape to color in the second rule group but then insisted on the color rule for the rest trials. Only the o1-preview kept matching based on hidden and changing rules until the end, although its accuracy in the third rule group was only 55\\%.\n \\begin{figure}\n     \\centering\n     \\includegraphics[width=0.50\\textwidth]{figure/fig5.png}\n     \\caption{Wisconsin card sorting test. Accuracy by 6 rule groups over 108 trials.}\n     \\label{fig5}\n \\end{figure}\n\\paragraph{Weather prediction task} We set the quite clear transition probability matrices (\\textbf{p}=0.9) to lower the difficulty of WPT. For example on a sunny day, the next day would most likely be sunny for cue `[1,0]' and rainy for cue `[0,1]'. However, most models struggled with learning two opposite transition probabilities, while some models grasped the probabilistic relationships to some extent. In Figure \\ref{fig6}, we present the actual and models' estimated transition matrices of the o1-preview and Qwen-2.5-32B to demonstrate the differences in performance extremes. Qwen-2.5-32B tends to predict the weather according to a fixed transition probability, missing the differences between the two sensor states. On the other side, the o1-preview shows a better grasp of the actual transition probabilities, although not accurately.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.50\\textwidth]{figure/fig6.png}\n    \\caption{Weather prediction task. True and models' estimated transition matrices of the highest (o1-preview) and lowest scoring models (Qwen-2.5-32B).}\n    \\label{fig6}\n\\end{figure}\n\n\\paragraph{Double choice Iowa gambling task} o1-mini has the lowest scores in DC-IGT, because it kept choosing the deck of card (B) with expected negative reward, even when told that the choice would cause high losses. Llama-3.1-70B, despite varying selections, insisted on its initial choice no matter its possible gain or loss, hence getting as low a short-term score as o1-mini. GPT-4o-mini and Llama-3.1-8B show rigid switching patterns leading to higher short-term scores by chance. Other models such as o1-preview, GPT-4o, and Gemini-1.5-pro can modify their initial choice according to possible gain and loss, demonstrating some extent of counterfactual thinking.\n\n\\paragraph{Meta-bandit task} Interestingly, in this task, no model could recognize the pattern of reward reversals, as illustrated in Figure \\ref{fig7}. All models perform period or irregular mistakes across the 60 trials which contain 20 reversals. This result indicates the general difficulty for current models in grasping the meta-structure of this task which represents the lack of meta-reflection ability.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.50\\textwidth]{figure/fig7.png}\n    \\caption{Rewards of models in the meta-bandit task over 60 trials and 20 reversals.}\n    \\label{fig7}\n\\end{figure}\n\\subsection{Discussion}\n\\paragraph{CoT is not cost-effective} o1-preview significantly outperforms the other 12 models, likely demonstrating the effects of chain of thought (CoT) in enhancing reflection abilities \\footnote{https://openai.com/index/learning-to-reason-with-llms/}. We encourage further research into the impacts of CoT on LLM's reflection capability. Intriguingly, while o1-preview excels in most tasks it exhibits 'MMN deficits' in the oddball paradigm. This may be attributed to the long-hidden CoT weakening immediate responses to unexpected information. Moreover, despite the o1-preview API cost being around 60\\% higher than the combined cost of all other 12 models, the performance gap did not significantly widen except in 2-back and WCST. Consequently, although CoT has been proven to dramatically improve LLMs' reasoning and problem-solving abilities \\cite{prabhakar2024decipheringfactorsinfluencingefficacy,li2024chainthoughtempowerstransformers}, it may not be the optimal solution to genuine intelligence due to its excessive costs. This finding underscores the necessity for future AI systems to balance different cognitive demands through `thinking fast and slow' \\cite{kahneman2011thinking}. \n\n\\paragraph{LLM lacks human-level reflection} While demonstrating basic automatic surprise signal detection and working memory, most models struggle to adapt flexibly to changing environments. This inflexibility manifests in rigid belief updating, rule inference, predictive learning, and counterfactual thinking. Most strikingly, all models lack meta-reflection ability. In MBT, all models merely alter their choices based on immediate feedback, failing to recognize the task's meta-structure. This indicates an absence of reflection on their adaption strategies, i.e., lack of meta-reflection. It is not just an advanced cognitive skill but a cornerstone of intelligence, crucial for rational reasoning and learning, reliable decision-making, and self-development \\cite{GRIFFITHS2020873,BOUREAU2015700,li2024enabling}. These results suggest that current LLMs still lack satisfactory reflection ability. While LLMs may excel in certain tasks, they may lack core features of human-level intelligence. These findings underscore the need for new paradigms and this field may benefit from drawing inspiration from biological intelligence.\n\n\\section{Conclusion}\nThis study aims to establish valid and efficient metrics for assessing intelligence, one of the most debated emergent abilities of LLMs. We introduce the concept of reflection, drawing from the first-principles perspective of cognitive science. Reflection, a general process inherent in both biological and artificial intelligence, is the capability to adapt beliefs or behaviors in response to unexpected outcomes, facilitating learning about, and adaption to the environments. To probe AI systems' intelligence through reflection, we propose Reflection-Bench. This comprehensive benchmark decomposes reflection into crucial cognitive components: perception, memory, belief updating, decision-making, prediction, counterfactual thinking, and meta-reflection. We design seven corresponding tasks for LLMs, spanning core cognitive functions with adjustable difficulties to maintain long-term relevance. In this work, we employed relatively easy experiment settings to assess the reflection capabilities of 13 LLMs. The results indicate that current LLMs still lack satisfactory reflection ability. In conclusion, Reflection-Bench offers both evaluation tools and inspiration for developing AI capable of reliably interacting with the environment. We hope this benchmark will contribute to advancing the field of artificial intelligence, particularly in enhancing the reflection capabilities of AI systems.\n\n\\section*{Limitations}\nWe acknowledge the limitations of the current Reflection-Bench that can be improved in future work. Firstly, text-based tasks for LLMs may not fully capture their original effectiveness. For instance, the textual version of the oddball paradigm might be inefficient in assessing the model's ability to detect surprise signals automatically, because the fine-tuning process and system prompts influence the outcomes. Secondly, our analysis of model's performances is relatively superficial, primarily focusing on metrics like accuracy. This risks overlooking their internal generation processes. More nuanced analysis could reveal invaluable insights into LLMs' reflection abilities. Thirdly, except for the oddball paradigm, we required the evaluated model to only respond with their options. While this facilitates result analysis, it might limit the model's real capability by restricting explicit thought processes.\n\n\\section*{Ethical considerations}\nReflection-Bench provides useful metrics of AI system's intelligence and can adapt to advanced AI in the future by adjustable difficulty. No privacy-related and personally identifiable data are included in our benchmarks. Considering some limitations, its validity and reliability in assessing intelligence still require confirmation by further studies.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-1803.05457v1.tex",
        "arXiv-2206.10498v4.tex",
        "arXiv-2410.16270v1.tex"
    ],
    "group_id": "group_44",
    "response": "### Title: Evaluating Advanced Question Answering, Planning, and Reflection Capabilities in Large Language Models\n\n### Introduction\n\nThe field of artificial intelligence (AI) has seen significant advancements in recent years, particularly in the development of large language models (LLMs) that can perform a variety of tasks, including question answering, planning, and reflection. These models have been trained on vast amounts of text data, enabling them to generate human-like responses and perform complex reasoning tasks. However, the true extent of their capabilities remains a subject of intense debate. While some researchers argue that LLMs exhibit human-like intelligence, others contend that their performance is more akin to sophisticated statistical engines that mimic human language rather than understanding it deeply. This debate has led to the creation of several benchmarks designed to evaluate the reasoning, planning, and reflection abilities of LLMs, aiming to clarify their true cognitive capabilities.\n\nHistorically, AI research has been driven by datasets that focus on retrieval-style tasks, where surface-level cues are sufficient for identifying answers. Such datasets have facilitated impressive advancements in AI systems, but they have not encouraged progress in more complex areas such as reasoning, commonsense knowledge, and advanced text comprehension. To address this, researchers have developed new datasets and benchmarks that challenge AI systems to perform tasks requiring deeper cognitive abilities. This paper summarizes three such research papers that introduce new benchmarks for evaluating LLMs: the AI2 Reasoning Challenge (ARC), PlanBench, and Reflection-Bench.\n\n### Main Content of Each Paper\n\n#### Paper 1: AI2 Reasoning Challenge (ARC)\n\nThe AI2 Reasoning Challenge (ARC) is designed to evaluate the advanced question answering (QA) capabilities of AI systems, particularly focusing on tasks that require reasoning and commonsense knowledge. The dataset consists of 7,787 natural science questions, sourced from standardized tests, and is divided into two sets: the \\Challenge~Set (2,590 questions) and the \\Additional~Set (5,197 questions). The \\Challenge~Set is composed of questions that were answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm, ensuring that the questions are challenging and cannot be solved by simple surface-level cues.\n\nThe ARC dataset is accompanied by a science text corpus, \\ASC~Corpus, which contains 1.4GB of science-related sentences. This corpus is intended to provide a starting point for AI systems to answer the questions in the \\Challenge~Set, although its use is optional. The corpus was created by running a series of search queries on a major search engine, using templates for 80 science topics covered in US elementary and middle schools. The corpus was also augmented with additional science sentences from sources like Wiktionary and Simple Wikipedia.\n\nSeveral baseline models were tested on the ARC dataset, including DecompAttn, BiDAF, and DGEM, which are adaptations of models that performed well on other QA tasks. The results show that none of these models could significantly outperform a random baseline on the \\Challenge~Set, indicating the difficulty of the questions and the need for more advanced QA methods. The \\Challenge~Set questions require reasoning, understanding of commonsense knowledge, and the ability to combine multiple facts, which are challenging for current AI systems.\n\n#### Paper 2: PlanBench\n\nPlanBench is a benchmark suite designed to evaluate the planning and reasoning about actions and change capabilities of LLMs. It consists of various test cases, each focusing on a different aspect of planning, such as plan generation, cost-optimal planning, plan verification, reasoning about plan execution, robustness to goal reformulation, ability to reuse plans, and replanning. The test cases are based on two domains from the International Planning Competition (IPC): Blocksworld and Logistics. Blocksworld involves stacking blocks on a table, while Logistics deals with moving packages between different locations.\n\nPlanBench leverages a domain-independent component that includes a planner and a plan verification tool, and a domain-dependent component that consists of a domain model, a problem generator, and a translator. The domain model defines the actions, predicates, and objects that can be used in the planning problems. The problem generator creates random instances of these problems, and the translator converts symbolic models into natural language text and vice versa. The benchmark suite was tested on two LLMs, GPT-4 and Instruct-GPT3, and the results indicate that even in simple common-sense planning domains, LLMs exhibit subpar performance. The most effective model, GPT-4, achieved the highest scores in plan verification and robustness to goal reformulation, but still struggled with tasks like plan generation and cost-optimal planning.\n\n#### Paper 3: Reflection-Bench\n\nReflection-Bench is a comprehensive benchmark designed to evaluate the reflection capabilities of LLMs, drawing from cognitive science paradigms. Reflection, as defined in this paper, encompasses several cognitive components including perception, memory, belief updating, decision-making, prediction, counterfactual thinking, and meta-reflection. The benchmark includes seven tasks adapted from cognitive science paradigms: oddball paradigm, n-back task, probabilistic reversal learning task, Wisconsin card sorting test, weather prediction task, double-choice Iowa gambling task, and meta-bandit task.\n\nThe oddball paradigm evaluates the LLMs' ability to detect deviant stimuli in a sequence of similar stimuli. The n-back task assesses working memory by requiring the LLMs to recall information from previous steps. The probabilistic reversal learning task tests the LLMs' ability to update beliefs in response to changing reward probabilities. The Wisconsin card sorting test evaluates the LLMs' ability to infer and adapt to changing rules. The weather prediction task assesses the LLMs' predictive learning ability based on probabilistic relationships. The double-choice Iowa gambling task evaluates counterfactual thinking by allowing the LLMs to reconsider their initial choices. Finally, the meta-bandit task tests the LLMs' ability to recognize patterns in reward reversals, a form of meta-reflection.\n\nReflection-Bench was evaluated on 13 different LLMs, including o1-preview, GPT-4, Claude-3.5-Sonnet, Gemini-1.5-pro, and various versions of Llama and Qwen. The results indicate that while some models, like o1-preview, perform well in certain tasks, none of the models exhibit satisfactory reflection ability, especially in the meta-bandit task. This suggests that current LLMs lack the ability to reflect on their own decision-making processes and adapt their strategies accordingly.\n\n### Commonalities and Innovations\n\nAll three papers introduce new benchmarks to evaluate the advanced cognitive capabilities of LLMs. ARC focuses on advanced question answering, PlanBench on planning and reasoning about actions and change, and Reflection-Bench on reflection capabilities. These benchmarks are designed to challenge LLMs beyond simple retrieval and surface-level reasoning tasks, aiming to assess their deeper understanding and reasoning abilities.\n\nThe ARC dataset is notable for its partitioning into a \\Challenge~Set and an \\Additional~Set, with the \\Challenge~Set containing questions that are difficult for simple algorithms to answer. This partitioning helps to avoid scores being dominated by the performance of simple algorithms and encourages research on more advanced QA methods. PlanBench, on the other hand, introduces a framework for generating prompts and evaluating LLMs in planning tasks, using both domain-specific and domain-independent components. Reflection-Bench is unique in its focus on reflection, a process that involves predicting future events, perceiving discrepancies between predictions and observations, and updating beliefs based on these discrepancies.\n\nInnovations in these benchmarks include the use of real-world standardized test questions in ARC, the integration of domain-specific and domain-independent components in PlanBench, and the adaptation of cognitive science paradigms in Reflection-Bench. These benchmarks also provide a starting point for researchers to develop and test new methods for improving LLMs' cognitive capabilities.\n\n### Comparison of Results\n\nThe results from the three benchmarks highlight the limitations of current LLMs in performing tasks that require deeper cognitive abilities. In ARC, none of the baseline models could significantly outperform a random baseline on the \\Challenge~Set, indicating the difficulty of the questions. PlanBench showed that even in simple common-sense planning domains, LLMs exhibit subpar performance, with the most effective model, GPT-4, achieving the highest scores in auxiliary tasks but still struggling with core planning tasks. Reflection-Bench revealed that while some models, like o1-preview, perform well in certain tasks, none of the models exhibit satisfactory reflection ability, especially in the meta-bandit task.\n\nThese results suggest that current LLMs are not yet capable of performing tasks that require advanced reasoning, planning, and reflection abilities. The performance gaps observed in these benchmarks indicate the need for further research to develop more sophisticated models that can handle these tasks effectively.\n\n### Conclusion\n\nThe main findings from the three papers are that current LLMs still lack satisfactory reflection ability, as demonstrated by their poor performance on the ARC, PlanBench, and Reflection-Bench benchmarks. While these models can perform well on simpler tasks, they struggle with more complex tasks that require deeper cognitive abilities. The ARC dataset highlights the difficulty of advanced QA tasks, PlanBench underscores the limitations of LLMs in planning and reasoning about actions and change, and Reflection-Bench reveals the absence of meta-reflection abilities in these models.\n\nFuture research directions should focus on developing new paradigms and methods to enhance the reflection capabilities of LLMs. This could involve exploring the impacts of chain of thought (CoT) on LLMs' reflection abilities, as suggested by the results from Reflection-Bench. Additionally, researchers should consider the balance between different cognitive demands, as CoT may not be the most cost-effective solution for genuine intelligence. Drawing inspiration from biological intelligence, particularly in the areas of perception, memory, belief updating, decision-making, prediction, counterfactual thinking, and meta-reflection, could also contribute to advancing the field of AI.\n\nIn conclusion, these benchmarks offer valuable tools for evaluating the cognitive capabilities of LLMs and provide a foundation for future research in developing more intelligent AI systems."
}