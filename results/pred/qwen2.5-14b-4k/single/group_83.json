{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable Makeup Transfer}\n\n\\begin{document}\n\n\\vspace{-5mm}\n\\newcommand\\myfigure{\n\\centering\n    \\includegraphics[width=0.95\\linewidth]{figures/first_big.pdf}\n\\captionof{figure}{Our model allows users to control both the shade of makeup and facial parts to transfer. The first row on the left shows the results of only transferring partial makeup style from the reference. The second row shows the results with controllable shades. Moreover, our method can perform makeup transfer between images that have different poses and expressions, as shown on the right part of the figure. Best viewed in color.}\n\\label{p1}\n}\n\n\\title{PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable Makeup Transfer}\n\n\\author{Wentao Jiang\\textsuperscript{\\rm 1}, Si Liu\\textsuperscript{\\rm 1}, Chen Gao\\textsuperscript{\\rm 2,4}, Jie Cao\\textsuperscript{\\rm 3,4}, Ran He\\textsuperscript{\\rm 3,4}, Jiashi Feng\\textsuperscript{\\rm 5}, Shuicheng Yan\\textsuperscript{\\rm 6} \\\\ \n\\textsuperscript{\\rm 1}School of Computer Science and Engineering, Beihang University \\\\\n\\textsuperscript{\\rm 2}Institute of Information Engineering, Chinese Academy of Sciences \\\\\n\\textsuperscript{\\rm 3}Institute of Automation, Chinese Academy of Sciences \\\\\n\\textsuperscript{\\rm 4}University of Chinese Academy of Sciences \n\\textsuperscript{\\rm 5}National University of Singapore \n\\textsuperscript{\\rm 6}YITU Tech \\\\\n{\\tt\\small \\{jiangwentao, liusi\\}@buaa.edu.cn, gaochen@iie.ac.cn, jie.cao@cripac.ia.ac.cn} \\\\  {\\tt\\small rhe@nlpr.ia.ac.cn, elefjia@nus.edu.sg, shuicheng.yan@yitu-inc.com}\n}\n\n\\maketitle\n \n\\begin{abstract} \n    In this paper, we address the makeup transfer task, which aims to transfer the makeup from a reference image to a source image. Existing methods have achieved promising progress in constrained scenarios, but transferring between images with large pose and expression differences is still challenging. Besides, they cannot realize customizable transfer that allows a controllable shade of makeup or specifies the part to transfer, which limits their applications. To address these issues, we propose Pose and expression robust Spatial-aware GAN (PSGAN). It first utilizes Makeup Distill Network to disentangle the makeup of the reference image as two spatial-aware makeup matrices. Then, Attentive Makeup Morphing module is introduced to specify how the makeup of a pixel in the source image is morphed from the reference image. With the makeup matrices and the source image, Makeup Apply Network is used to perform makeup transfer. Our PSGAN not only achieves state-of-the-art results even when large pose and expression differences exist but also is able to perform partial and shade-controllable makeup transfer. We also collected a dataset containing facial images with various poses and expressions for evaluations.\n\\end{abstract}\n \n\\vspace{-4mm}\n\\section{Introduction}\n\nWe explore the makeup transfer task, which aims to transfer the makeup from an arbitrary reference image to a source image. It is widely demanded in many popular portrait beautifying applications.\nMost existing makeup transfer methods \\cite{Li2018BeautyGANIF,ChenBeautyGlow2019,Chang2018PairedCycleGANAS,Gu2019LADNLA} are  based on Generative Adversarial Networks (GANs) \\cite{Goodfellow2014GenerativeAN}. They generally use face parsing maps and/or facial landmarks as a preprocessing step to facilitate the subsequent processing and adopt the framework of CycleGAN \\cite{Zhu2017UnpairedIT} which is trained on unpaired sets of images, i.e., non-makeup images and with-makeup images.\n\n \nHowever, existing methods mainly have two limitations. \nFirstly, they only work well on frontal facial images with neutral expression since they lack a specially designed module to handle the misalignment of images and overfit on frontal images. \nWhile in practical applications, an ideal method should be \\emph{pose and expression robust}, which is able to generate high-quality results even if source images and reference images show different poses and expressions. \nSecondly, the existing methods cannot perform customizable makeup transfer since they encode makeup styles into low dimension vectors which lose the spatial information. An ideal makeup transfer method need be capable of realizing \\emph{partial} and \\emph{shade-controllable} makeup transfer. Partial transfer indicates transferring the makeup of specified facial regions separately, e.g., eye shadows or lipstick. Shade-controllable transfer means the shade of the transferred makeup can be controllable from light to heavy.\n\nTo solve these challenges, we propose a novel Pose and expression robust Spatial-aware GAN, which consists of a Makeup Distill Network (MDNet), an Attentive Makeup Morphing (AMM) module and a Makeup Apply Network (MANet).\nDifferent from the previous approaches that simply input two images into the  network or recombine makeup latent code and identity latent code to perform transfer, we design PSGAN to transfer makeup through scaling and shifting the feature map for only once, inspired by style transfer methods \\cite{Huang2017ArbitraryST,Dumoulin2016ALR}.\nComparing with general style transfer, makeup transfer is more difficult since the human perception system is very sensitive to the artifacts on faces. Also, makeup styles contain subtle details in each facial region instead of general styles. To this end, we propose MDNet to disentangle the makeup from the reference image into two makeup matrices, i.e., the coefficient matrix $\\gamma$ and bias matrix $\\beta$ which both have the same spatial dimensions with visual features. These matrices embed the makeup information and serve as the shifting and scaling parameters. Then, $\\gamma$ and $\\beta$ are morphed and adapted to the source image by the AMM module which calculates an attentive matrix $A$ to produce adapted makeup matrices $\\gamma^{\\prime}$ and $\\beta^{\\prime}$. \nThe AMM module utilizes the face parsing maps and facial landmarks to build the pixel-wise correspondences between source images and reference images, which solves the misalignment of faces. \nFinally, the proposed MANet conducts makeup transfer through applying pixel-wise multiplication and addition on visual features using $\\gamma^{\\prime}$ and $\\beta^{\\prime}$.\n\nSince the makeup style has been distilled in a spatial-aware way, \\emph{partial transfer} can be realized by applying masks pixel-wisely according to the face parsing results.\nFor example, in the top left panel of Figure \\ref{p1}, the lip gloss, skin and eye shadow can be individually transferred from the reference image to the source image.\n\\emph{Shade-controllable transfer} can be realized through multiplying the weights of makeup matrices by coefficient within $[0,1]$.\nAs shown in the bottom left panel of Figure \\ref{p1}, where the makeup shade is increasingly heavier. \nMoreover, the novel AMM module effectively assists the generating of  \\emph{pose and expression robust} results, as shown in the right part of  Figure \\ref{p1}. We also directly apply transfer to every frame of facial videos and still get nice and consistent results.\nWith the three novel components, PSGAN satisfies the requirements we pose for an ideal customizable makeup transfer method. \n\nWe make the following contributions in this paper:\n\\begin{itemize}\n   \\item To our best knowledge, PSGAN is the first to simultaneously realize partial, shade-controllable, and pose/expression robust makeup transfer, which facilitates the applications in the real-world environment.\n   \\item A MDNet is introduced to disentangle the makeup from the reference image as two makeup matrices. The spatial-aware makeup matrices enable the flexible partial and shade-controllable transfer.\n   \\item An AMM module that adaptively morphs the makeup matrices to source images is proposed, which enables pose and expression robust transfer.\n   \\item A new Makeup-Wild dataset containing images with diverse poses and expressions is collected for better evaluations.\n\\end{itemize}\n\n\\begin{figure*}[!t]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{figures/framework.pdf}\n    \\caption{\n        \\textbf{(A)} Illustration of PSGAN framework. MDNet distills makeup matrices from the reference image. AMM module applies the adapted makeup matrices to the output feature map of the third bottleneck of MANet to achieve makeup transfer. \n        \\textbf{(B)} Illustration of AMM module. Green blocks with 136 (68$\\times$2) channels indicate relative position features of the pixels, which are then concatenated with $C$-channel visual features. Thus, the attention map is computed for each pixel in the source image through the similarity of relative positions and visual appearances. The adapted makeup matrices $\\gamma^{\\prime}$ and $\\beta^{\\prime}$ are produced by the AMM module, which are then multiplied and added to feature maps of MANet element-wisely. The orange and the gray blocks in the figure indicate visual features with makeup and without makeup. \\textbf{(C)} Attention maps for a specific red point in the source image. Note that we only calculate attentive values for pixels that belong to the same facial region. Thus, there are no response values on the lip and eye of the reference image.\n    }\n    \\vspace{-3mm}\n    \\label{framework}\n\\end{figure*} \n\n\\section{Related Work}\n\\subsection{Makeup Transfer}\nMakeup transfer has been studied a lot these years \\cite{Tong2007ExampleBasedCT,Guo2009DigitalFM,Li2015SimulatingMT,Liu2016MakeupLA,Liu2014WowYA,Alashkar2017ExamplesRulesGD}.\nBeautyGAN \\cite{Li2018BeautyGANIF} first proposed a GAN framework with dual input and output for makeup transfer and removal simultaneously. They also introduced a makeup loss that matches the color histogram in different parts of faces for instance-level makeup transfer. BeautyGlow \\cite{ChenBeautyGlow2019} proposed a similar idea on the Glow framework and decomposed makeup component and non-makeup component. PairedCycleGAN \\cite{Chang2018PairedCycleGANAS} employed an additional discriminator to guide makeup transfer using pseudo transferred images generated by warping the reference face to the source face. LADN \\cite{Gu2019LADNLA} leveraged additional multiple overlapping local discriminators for dramatic makeup transfer.\nHowever, the above approaches often fail on transferring in-the-wild images and cannot adjust transfer precisely and partially, which limits their applications, such as the makeup transfer in videos.\n\n\\subsection{Style Transfer}\n\nStyle transfer has been investigated extensively \\cite{Gatys2015ANA,Gatys2016PreservingCI,Johnson2016PerceptualLF,Luan2017DeepPS,Taigman2016UnsupervisedCI}. \\cite{Gatys2016ImageST} proposed to derive image representations from CNN, which can be separated and recombined to synthesize images. Some methods are developed to solve the fast style transfer problem. \\cite{Dumoulin2016ALR} found the vital role of normalization in style transfer networks and achieved fast style transfer by the conditional instance normalization. While their methods can only transfer a fixed set of styles and cannot adapt to arbitrary new styles. Then, \\cite{Huang2017ArbitraryST} proposed adaptive instance normalization (AdaIN) that aligns the mean and variance of the content features with those of the style features and achieved arbitrary style transfer. Here, we propose spatial-aware makeup transfer for each pixel rather than transferring a general style from the reference.\n\n \n\\subsection{Attention Mechanism}\nAttention mechanism has been utilized in many areas \\cite{Xu2015ShowAA,Mnih2014RecurrentMO,Hu2017SqueezeandExcitationN,Rush2015ANA}. \\cite{Vaswani2017AttentionIA} proposed the attention mechanism in the natural language processing area by leveraging a self-attention module to compute the response at a position in a sequence (e.g., a sentence) by attending to all positions and taking their weighted average in an embedding space.  \\cite{Wang2017NonlocalNN} proposed the non-local network, which is to compute the response at a position as a weighted sum of the features at all positions. Inspired by these works, we explore the application of attention module by calculating the attention between two feature maps. Unlike the non-local network that only considers visual appearance similarities, our proposed AMM module computes the weighted sum of another feature map by considering both visual appearances and locations.\n\\vspace{-2mm}\n\n\\section{PSGAN}\n\\subsection{Formulation}\n\nLet $X$ and $Y$ be the source image domain and the reference image domain. \nAlso, we utilize $\\left\\{x^{n}\\right\\}_{n=1, \\ldots, N}, x^{n} \\in X$ and $\\left\\{y^{m}\\right\\}_{m=1, \\ldots, M}, y^{m} \\in Y$  to represent the examples of two domains respectively.  Note that paired datasets are not required. That is, the source and reference images have different identities. \nWe assume $x$ is sampled from $X$ according to the distribution $\\mathcal{P}_{X}$ and  $y$ is sampled from $Y$ according to the distribution $\\mathcal{P}_{Y}$.\nOur proposed PSGAN learns a transfer function $G: \\{x, y\\} \\rightarrow \\tilde{x}$, where the transferred image $\\tilde{x}$ has the makeup style of the reference image $y$ and preserves the identity of the source image $x$. \n\n\\subsection{Framework}\n\n\\textbf{Overall.} The framework of PSGAN is shown in Figure \\ref{framework} (A). \nMathematically, it is formulated as $\\tilde{x}= G(x, y)$.\nIt can be divided into three parts.\n1) \\emph{Makeup distill network.} MDNet extracts the makeup style from the reference image $y$ and represents it as two makeup matrices $\\gamma$ and $\\beta$, which have the same height and width as the feature map.\n2)  \\emph{Attentive makeup morphing module.}  Since source images and reference images may have large discrepancies in expressions and poses, the extracted makeup matrices cannot be directly applied to the source image $x$. We then propose an AMM module to morph the two makeup matrices to two new matrices $\\gamma^{\\prime}$ and $\\beta^{\\prime}$ which are adaptive to the source image by considering the similarities between pixels of the source and reference.\n3) \\emph{Makeup apply network.} The adaptive makeup matrices $\\gamma^{\\prime}$ and $\\beta^{\\prime}$ are applied to the bottleneck of the MANet to perform makeup transfer with pixel-level guidance by element-wise multiplication and addition.\n\n\\textbf{Makeup distill network.} The MDNet utilizes the encoder-bottleneck architecture used in  \\cite{Choi2017StarGANUG} without the decoder part. \nIt disentangles the makeup related features, e.g., lip gloss, eye shadows, from the intrinsic facial features, e.g., facial shape, the size of eyes.\nThe makeup related features are represented as two makeup matrices  $\\gamma$ and $\\beta$, which are used to transfer the makeup by pixel-level operations. As shown in Figure \\ref{framework} (B), the output feature map of MDNet $\\mathbf{V_y}\\in \\mathbb{R}^{C \\times H \\times W}$ is fed into two $1\\times1$ convolution layers to produce $\\gamma \\in \\mathbb{R}^{1\\times H \\times W}$ and $\\beta \\in \\mathbb{R}^{1 \\times H \\times W}$, where $C$, $H$ and $W$ are the number of channels, height and width of the feature map.\n\n\\textbf{Attentive makeup morphing module.} \nSince the source and reference images may have different poses and expressions, the obtained spatial-aware $\\gamma$ and $\\beta$ cannot be applied directly to the source image. The proposed AMM module calculates an attentive matrix $A \\in \\mathbb{R}^{HW \\times HW}$ to specify how a pixel in the source image $x$ is morphed from the pixels in the reference image $y$, where $A_{i,j}$ indicates the attentive value between the $i$-th pixel $x_i$ in image $x$ and the $j$-th pixel $y_j$ in image $y$.\n\nIntuitively, makeup should be transferred between the pixels with similar relative positions on the face, and the attentive values between these pixels should be high. For example, the lip gloss region of the transferred result $\\tilde{x}$ should be sampled from the corresponding lip gloss region of the reference image $y$.\nTo describe the relative positions, we take the facial landmarks as anchor points. The relative position feature of pixel $x_i$ is represented by $\\mathbf{p}_i \\in \\mathbb{R}^{136}$, which is reflected in the differences of coordinates between pixel $x_i$  and $68$ facial landmarks, calculated by\n\\begin{equation}\n    \\begin{split}\n    \\mathbf{p}_i= [f(x_i)-f(l_1), f(x_i)-f(l_2), \\dots, f(x_i) - f(l_{68}),\\\\ g(x_i)-g(l_1), g(x_i)-g(l_2), \\dots, g(x_i) - g(l_{68})],\n    \\label{p}\n    \\end{split}\n\\end{equation}\nwhere $f(\\cdot)$ and $g(\\cdot)$ indicate the coordinates on $x$ and $y$ axes, $l_i$ indicates the $i$-th facial landmark obtained by the 2D facial landmark detector \\cite{Zhang2016JointFD}, which serves as the anchor point when calculating $\\mathbf{p}_i$. In order to handle faces that occupy different sizes in images, we divide $\\mathbf{p}$ by its two-norm (i.e., $\\frac{\\mathbf{p}}{\\left\\| \\mathbf{p} \\right\\|}$) when calculating the attentive matrix.\n\nMoreover, to avoid unreasonable sampling pixels with similar relative positions but different semantics, we also consider the visual similarities between pixels (e.g., $x_i$ and $y_j$), which are denoted as the similarities between $\\mathbf{v}_i$  and $\\mathbf{v}_j$ that extracted from the third bottleneck of MANet and MDNet respectively.\nTo make the relative position to be the primary concern, we multiply the visual features by a weight when calculating $A$.\nThen, the relative position features are resized and concatenated with the visual features along the channel dimension.\nAs Figure \\ref{framework} (B) shows, the attentive value $A_{i,j}$ is computed by considering the similarities of both visual appearances and relative positions via\n\\begin{equation}\nA_{i, j} = \\frac{\\exp \\left( [w\\mathbf{v}_i, \\frac{\\mathbf{p}_i}{\\left\\| \\mathbf{p}_i \\right\\|} ]^T [w\\mathbf{v}_j, \\frac{\\mathbf{p}_j}{\\left\\| \\mathbf{p}_j \\right\\|}]  \\right) \\mathbb{I}  (m^i_x = m^j_y) }{\\sum_{j} \\exp \\left( [w\\mathbf{v}_i, \\frac{\\mathbf{p}_i}{\\left\\| \\mathbf{p}_i \\right\\|} ]^T [w\\mathbf{v}_j, \\frac{\\mathbf{p}_j}{\\left\\| \\mathbf{p}_j \\right\\|}] \\right) \\mathbb{I}  (m^i_x = m^j_y)},\n\\label{equ5}\n\\end{equation}\nwhere $[\\cdot, \\cdot]$ denotes the concatenation operation, $\\mathbf{v} \\in \\mathbb{R}^{C}$ and $\\mathbf{p} \\in \\mathbb{R}^{136} $ indicate the visual features and relative position features, $w$ is the weight for visual features.\n$\\mathbb{I}(\\cdot)$ is an indicator function whose value is $1$ if the inside formula is true, $m_x, m_y \\in \\{0, 1, \\dots, N-1\\}^{H \\times W}$ are the face parsing map of source image $x$ and reference image $y$, where $N$ stands for the number of facial regions ($N$ is $3$ in our experiments including eyes, lip and skin), $m^i_x$ and $m^j_y$ indicate the facial regions that $x_i$ and $x_j$ belong to. Note that we only consider the pixels belonging to same facial region, i.e., $m^i_x = m^j_y$ , by applying indicator function $\\mathbb{I}(\\cdot)$.\n\nGiven a specific point that marked in red in the lower-left corner of the nose in the source image, \nthe middle image of Figure \\ref{framework} (C) shows its attention map by reshaping a specific row of the attentive matrix $A_{i,:} \\in \\mathbb{R}^{1 \\times HW}$ to $H \\times W$.\nWe can see that only the pixels around the left corner of the nose have large values. After applying softmax, attentive values become more gathered.\nThis verifies that our proposed AMM module is able to locate semantically similar pixels to attend.\n\nWe multiply attentive matrix $A$ by the $\\gamma$ and $\\beta$, and get the morphed makeup matrices $\\gamma^{\\prime}$ and $\\beta^{\\prime}$. \nMore specifically, the matrices $\\gamma^\\prime$ and $\\beta^\\prime$ are computed by\n\\begin{equation}\n\\begin{split}\n\\gamma^\\prime_i = \\sum_{j} A_{i, j} \\gamma_j ; \\\\\n\\beta^\\prime_i = \\sum_{j} A_{i, j} \\beta_j ,\n\\label{equ4}\n\\end{split}\n\\end{equation}\nwhere $i$ and $j$ are the pixel index of $x$ and $y$. After that, the matrix $\\gamma^{\\prime}  \\in \\mathbb{R}^{1 \\times H \\times W}$ and $\\beta^{\\prime} \\in \\mathbb{R}^{1 \\times H \\times W}$ are duplicated and expanded along the channel dimension to produce the makeup tensors $\\Gamma^{\\prime}  \\in \\mathbb{R}^{C \\times H \\times W}$ and $B^{\\prime}  \\in \\mathbb{R}^{C \\times H \\times W}$, which will be the input of MANet.\n\n\\textbf{Makeup apply network.} MANet utilizes a similar encoder-bottleneck-decoder architecture as \\cite{Choi2017StarGANUG}. As shown in Figure \\ref{framework} (A), the encoder part of MANet shares the same architecture with MDNet, but they do not share parameters.\nIn the encoder part, we use instance normalizations that have no affine parameters that make the feature map to be a normal distribution.\nIn the bottleneck part, the morphed makeup tensors $\\Gamma^{\\prime}$ and $B^{\\prime}$ obtained by the AMM module are applied to the source image feature map $\\mathbf{V_x}\\in \\mathbb{R}^{C \\times H \\times W}$. \nThe activation values of the transferred feature map $\\mathbf{V_x}^{\\prime}$ are calculated by\n\\begin{equation}\n\\mathbf{V_x}^{\\prime} = \\Gamma^{\\prime} \\mathbf{V_x} +B ^{\\prime}.\n\\label{equ1}\n\\end{equation}\nEq. (\\ref{equ1}) gives the function of makeup transfer. The updated feature map $\\mathbf{V_x}^{\\prime}$ is then fed to the subsequent decoder part of MANet to produce the transferred result.\n\n\\subsection{Objective Function}\n\n\\textbf{Adversarial loss.} We utilize two discriminators $D_X$ and $D_Y$ for the source image domain $X$ and the reference image domain $Y$, which try to discriminate between generated images and real images and thus help the generators synthesize realistic outputs. Therefore, the adversarial loss $L_D^{adv}$, $L_G^{adv}$ for discriminator and generator are computed by\n\\begin{equation}\n    \\begin{gathered}\n        \\begin{aligned} L_{D}^{adv} &= - \\mathbb{E}_{x \\sim \\mathcal{P}_{X}}\\left[\\log D_{X}(x)\\right] - \\mathbb{E}_{y \\sim \\mathcal{P}_{Y}}\\left[\\log D_{Y}(y)\\right] \\\\ &- \\mathbb{E}_{x \\sim \\mathcal{P}_{X}, y \\sim \\mathcal{P}_{Y}}\\left[\\log \\left(1-D_{X}(G(y, x))\\right)\\right] \\\\ &- \\mathbb{E}_{x \\sim \\mathcal{P}_{X}, y \\sim \\mathcal{P}_{Y}}\\left[\\log \\left(1-D_{Y}(G(x, y))\\right)\\right] \n        \\end{aligned} \\\\\n        \\begin{aligned} L_{G}^{adv} = &- \\mathbb{E}_{x \\sim \\mathcal{P}_{X}, y \\sim \\mathcal{P}_{Y}}\\left[\\log \\left(D_{X}(G(y, x))\\right)\\right] \\\\ &- \\mathbb{E}_{x \\sim \\mathcal{P}_{X}, y \\sim \\mathcal{P}_{Y}}\\left[\\log \\left(D_{Y}(G(x, y))\\right)\\right] \n        \\end{aligned}\n    \\end{gathered}\n\\end{equation}\n \n\\textbf{Cycle consistency loss.} Due to the lack of triplets data (source image, reference image, and transferred image), we train the network in an unsupervised way. Here, we introduce the cycle consistency loss proposed by \\cite{Zhu2017UnpairedIT}. We use the L1 loss to constrain the reconstructed images and define the cycle consistency loss $L_G^{cyc}$ as\n\\begin{equation}\n    \\begin{aligned}\n        L_G^{cyc} &=\\mathbb{E}_{x \\sim \\mathcal{P}_{X}, y \\sim \\mathcal{P}_{Y}}\\left[ \\left\\| G(G(x, y), x) - x \\right\\|_{1} \\right]  \\\\ &+\\mathbb{E}_{x \\sim \\mathcal{P}_{X}, y \\sim \\mathcal{P}_{Y}}\\left[ \\left\\| G(G(y, x), y) - y \\right\\|_{1} \\right].\n    \\end{aligned}\n\\end{equation}\n\n\\textbf{Perceptual loss.} When transferring the makeup style, the transferred image is required to preserve personal identity. Instead of directly measuring differences at pixel-level, we utilize a VGG-16 model pre-trained on ImageNet to compare the activations of source images and generated images in the hidden layer. Let $F_l(\\cdot)$ denote the output of the $l$-th layer of VGG-16 model. We introduce the perceptual loss $L_G^{per}$ to measure their differences using L2 loss:\n\\begin{equation}\n    \\begin{aligned}\n        L_G^{per} &=\\mathbb{E}_{x \\sim \\mathcal{P}_{X}, y \\sim \\mathcal{P}_{Y}}\\left[ \\left\\| F_l(G(x, y)) - F_l(x) \\right\\|_{2} \\right]  \\\\ &+\\mathbb{E}_{x \\sim \\mathcal{P}_{X}, y \\sim \\mathcal{P}_{Y}}\\left[ \\left\\| F_l(G(y, x)) - F_l(y) \\right\\|_{2} \\right].\n    \\end{aligned}\n\\end{equation}\n\n\\textbf{Makeup loss.} To provide coarse guidance for makeup transfer, we utilize the makeup loss proposed by \\cite{Li2018BeautyGANIF}. Specifically, we perform histogram matching on the same facial regions of $x$ and $y$ separately and then recombine the results, denoted as $HM(x, y)$. As a kind of pseudo ground truth, $HM(x, y)$ preserves the identity of $x$ and has a similar color distribution with $y$. Then we calculate the makeup loss $L_G^{make}$ as coarse guidance by\n\\begin{equation}\n    \\begin{aligned}\n        L_G^{make} &= \\mathbb{E}_{x \\sim \\mathcal{P}_{X}, y \\sim \\mathcal{P}_{Y}}\\left[ \\left\\| G(x, y) - HW(x, y) \\right\\|_{2} \\right]  \\\\ &+\\mathbb{E}_{x \\sim \\mathcal{P}_{X}, y \\sim \\mathcal{P}_{Y}}\\left[ \\left\\| G(y, x) - HW(y, x) \\right\\|_{2} \\right].\n    \\end{aligned}\n\\end{equation}\n\n\\textbf{Total loss.} The loss $L_D$ and $L_G$ for discriminator and generator of our approach can be expressed as\n\\begin{equation}\n    \\begin{gathered}\n            L_D = \\lambda_{adv} L_D^{adv} \\\\\n            L_G = \\lambda_{adv} L_G^{adv} + \\lambda_{cyc} L_G^{cyc} + \\lambda_{per} L_G^{per} + \\lambda_{make} L_G^{make},\n    \\end{gathered} \n\\end{equation}\nwhere $\\lambda_{adv}$, $\\lambda_{cyc}$, $\\lambda_{per}$, $\\lambda_{make}$ are the weights to balance the multiple objectives. \n\n\\section{Experiments}\n\n\\subsection{Data Collection}\nSince the existing makeup datasets only consist of frontal facial images with neutral expressions, we collect a new Makeup-Wild dataset that contains facial images with various poses and expressions as well as complex backgrounds to test methods in the real-world environment. We collect data from the Internet and then manually remove images with frontal face or neutral expression. After that, we crop and resize the images to be $256 \\times 256$ resolution without alignment.\nFinally, $403$ with-makeup images and $369$ non-makeup images are collected to form the Makeup-Wild dataset.\n\n\\subsection{Experimental Setting and Details}\nWe train our network using the training part of the MT (Makeup Transfer) dataset \\cite{Li2018BeautyGANIF,ChenBeautyGlow2019} and test it on the testing part of MT dataset and the Makeup-Wild dataset. MT dataset contains $1,115$ non-makeup images and $2,719$ with-makeup images which are mostly well-aligned, with the resolution of $361 \\times 361$ and the corresponding face parsing results. We follow the splitting strategy of \\cite{Li2018BeautyGANIF} to form the train/test set and conduct frontal face experiments in the test set of MT dataset since the examples in the test set are well-aligned frontal facial images. \nTo further prove the effectiveness of PSGAN for handling pose and expression differences, we use the Makeup-Wild dataset as an extra test set. Note that we only train our network using the training part of the MT dataset for a fair comparison.\n\nFor all experiments, we resize the images to 256$\\times$256, and utilize the $relu\\_4\\_1$ feature layer of VGG-16 for calculating perceptual loss. The weights of different loss functions are set as $\\lambda_{adv} = 1$, $\\lambda_{cyc} = 10$, $\\lambda_{per} = 0.005$, $\\lambda_{make} = 1$, and the weight for visual feature in AMM is set to $0.01$. We train the model for 50 epochs optimized by Adam \\cite{kingma2014adam} with learning rate of 0.0002 and batch size of 1.\n\n\\subsection{Ablation Studies}\n\n\\textbf{Attentive makeup morphing module.}\nIn PSGAN,  AMM module  morphs the distilled makeup matrices $\\gamma$ and $\\beta$ to $\\gamma^{\\prime}$, $\\beta^{\\prime}$. It alleviates the pose and expression differences between source and reference images. \nThe effectiveness of the AMM module is shown in Figure \\ref{ab1}.\nIn the first row,  the pose of source and reference images are very different.  The bangs of the reference image are transferred to the skin of the source image without AMM. By applying AMM, the pose misalignment is well solved. \nA similar observation can be found in the second row: the expressions of source and reference images are smiling and neutral respectively, while the lip gloss is applied to the teeth region without the AMM module shown in the third column. After integrating AMM, lip gloss is applied to the lip region, bypassing the teeth area.\nThe experiments demonstrate that the AMM module can specify how a pixel in the source image is morphed from pixels of the reference instead of mapping the makeup from the same location directly. \n\n\\begin{figure}[!t]\n    \\includegraphics[width=1\\linewidth]{figures/semantic_ablation.pdf}\n    \\caption{Without AMM module, the  makeup transfer results (the 3rd column) are bad due to pose and expression differences between source and reference images.}\n    \\label{ab1}\n \\end{figure}\n\n\\textbf{The weight of visual feature in calculating $A$.} In the AMM module, we calculate the attentive matrix $A$ by considering both the visual features $\\mathbf{v}$ and relative positions $\\mathbf{p}$ using Eq. (\\ref{equ5}). \nFigure \\ref{ab2} demonstrates that if only relative positions are considered by setting the weight to zero, the attentive maps in the second column are similar to a 2D Gaussian distribution.\nIn the first column of Figure \\ref{ab2}, the red point on the skin of the source may wrongly receive makeup from the nostrils area in the reference image (1st row). The attention map also crosses the face boundary and covers the earrings (2nd row) which is unreasonable.\nBesides, larger weights will lead to scattered and unreasonable attention maps, as shown in the last column.\nAfter considering the appearance feature appropriately by setting the weight to $0.01$, the attention maps focus more on the skin and also bypass the nostrils as well as background.\n\n \\begin{figure}[!t]\n    \\includegraphics[width=1\\linewidth]{figures/visual_ablation_w.pdf}\n    \\caption{Given a red point on the skin, the corresponding attention maps with different weights on visual features are shown. Without using visual features, attention maps fail to avoid nostrils (1st row, 2nd column) and wrongly crosses the facial boundary (2nd row, 2nd column). While a larger weight leads to scattered and unreasonable attention maps.}\n    \\label{ab2}\n \\end{figure}\n\n \\begin{figure}[!t]\n    \\includegraphics[width=1\\linewidth]{figures/partial2_2.pdf}\n    \\caption{Given the source image (2nd column), the transferred images (3rd column) are generated by transferring the lipstick from reference 1 and other makeup from reference 2. }\n    \\label{partial}\n \\end{figure}\n\n\\begin{figure*}%[!htb]\n   \\includegraphics[width=1\\linewidth]{figures/interpolated2.pdf}\n   \\caption{Results of interpolated makeup styles. If only one reference is used, adjusting the shade of makeup can be realized (1st row). If two references are used (1st column and last column), the makeup of the transferred images is gradually changing from reference 1 towards reference 2 from left to right (2nd rows). }\n   \\label{interpolated}\n   \\vspace{-2mm}\n\\end{figure*}\n\n\\subsection{Partial and Interpolated Makeup Transfer}\nSince the makeup matrices $\\gamma$ and $\\beta$ are spatial-aware, the partial and interpolated transfer can be realized during testing.\nTo achieve partial makeup generation, we compute the new makeup matrices by weighting the matrices using the face parsing results.\nLet $x$, $y_1$, and $y_2$ denote a source image and two reference images. We can obtain $\\Gamma^{\\prime}_x$, $B^{\\prime}_x$ and $\\Gamma^{\\prime}_{y_1}$, $B^{\\prime}_{y_1}$ as well as $\\Gamma^{\\prime}_{y_2}$, $B^{\\prime}_{y_2}$ by feeding the images to MDNet.\nIn addition, we can obtain the face parsing mask $m_x$ of $x$ through the existing deep learning method \\cite{Zhao2016PyramidSP}.\nSuppose we want to mix the lipstick from $y_1$ and other makeup from $y_2$, we can first obtain the binary mask of the lip, denoted as $m^{l}_x \\in \\{0, 1\\}^{H \\times W}$.\nThen, PSGAN can realize partial makeup transfer by assigning different makeup parameters on different pixels.\nBy modifying Eq. (\\ref{equ1}), the partial transferred feature map $\\mathbf{V_x}^{\\prime}$ can be calculated by\n    \\begin{equation}\n        \\mathbf{V_x}^{\\prime} = (m^{l}_x \\Gamma^{\\prime}_{y_1} + (1-m^{l}_x)\\Gamma^{\\prime}_{y_2}) \\mathbf{V_x}  + (m^{l}_x B^{\\prime}_{y_1}  + (1-m^{l}_x) B^{\\prime}_{y_2}).\n        \\label{equ2}\n        \\end{equation}\nFigure \\ref{partial} shows the results by mixing the makeup styles from two references partially. The results on the third column recombine the makeup of lip from reference 1 and other part of makeup from reference 2, which are natural and realistic. Also, only transferring the lipstick from reference 1 and remain other parts unchanged can be achieved by assigning $x = y_2$. The new feature of partial makeup makes PSGAN realize the flexible partial makeup transfer.\n\nMoreover, we can interpolate the makeup with two reference images by a coefficient $\\alpha \\in [0,1]$. We first get the makeup tensors of two references $y_1$ and $y_2$, and then compute the new parameters by weighting them  with the coefficient $\\alpha$. The resulted feature map $\\mathbf{V_x}^{\\prime}$ is calculated by\n\\begin{equation}\n\\mathbf{V_x}^{\\prime} = (\\alpha \\Gamma^{\\prime}_{y_1} + (1-\\alpha)\\Gamma^{\\prime}_{y_2}) \\mathbf{V_x}  + (\\alpha B^{\\prime}_{y_1} + (1-\\alpha) B^{\\prime}_{y_2}).\n\\label{equ3}\n\\end{equation}\nFigure \\ref{interpolated} shows the interpolated makeup transfer results with one and two reference images. By feeding the new makeup tensors into MANet, we yield a smooth transition between two reference makeup styles. Similarly, we can adjust the shade of transfer using only one reference image by assigning $x = y_1$.\nThe generated results demonstrate that our PSGAN can not only control the shade of makeup transfer but also generate a new style of makeup by mixing the makeup tensors of two makeup styles.\n\nWe can also perform partial and interpolated transfer simultaneously by leveraging both the face parsing maps and coefficient thanks to the design of spatial-aware makeup matrices. The above experiments have demonstrated that PSGAN broadens the application range of makeup transfer significantly.\n\n\\begin{figure*}[!ht]\n   \\includegraphics[width=1\\linewidth]{figures/res_all.pdf}\n   \\caption{Qualitative comparison. PSGAN is able to generate realistic images with the same makeup styles as the reference.}\n   \\label{res1}\n       \\vspace{-2mm}\n\\end{figure*} \n\n\\subsection{Comparison}\n \nWe conduct comparison with general image-to-image translation methos DIA \\cite{Liao2017VisualAT} and CycleGAN \\cite{Zhu2017UnpairedIT} as well as state-of-the-art makeup transfer methods BeautyGAN (BGAN) \\cite{Li2018BeautyGANIF}, PairedCycleGAN (PGAN) \\cite{Chang2018PairedCycleGANAS}, BeautyGlow (BGlow) \\cite{ChenBeautyGlow2019} and LADN \\cite{Gu2019LADNLA}. Current makeup transfer methods leverage face parsing maps \\cite{Chang2018PairedCycleGANAS,ChenBeautyGlow2019,Li2018BeautyGANIF} and facial landmarks \\cite{Gu2019LADNLA} for training and realize different functions as shown in Table \\ref{t1}.\n\\begin{table}[!t]\n    \\centering\n    \\small\n     \n    \\begin{tabular}{@{}c|ccc@{}}\n        \\toprule\n        \\multirow{2}{*}{Method} & \\multicolumn{3}{c}{Functions} \\\\ \\cmidrule(l){2-4} \n        & Shade    & Part    & Robust   \\\\ \\midrule\n        BGAN \\cite{Li2018BeautyGANIF}    &       &      &    \\\\ \\midrule\n        PGAN \\cite{Chang2018PairedCycleGANAS}     &       &      &    \\\\ \\midrule\n        BGlow \\cite{ChenBeautyGlow2019}   & \\checkmark &      &    \\\\ \\midrule\n        LADN \\cite{Gu2019LADNLA}   & \\checkmark &      &    \\\\ \\midrule\n        PSGAN   & \\checkmark & \\checkmark & \\checkmark \\\\ \\bottomrule\n    \\end{tabular}\n    \\vspace{1mm}\n    \\caption{Analysises of existing methods. ``Shade'', ``Part'' and ``Robust'' indicate shade-controllable, partial and pose/expression robust transfer respectively.}\n    \\label{t1}\n\\end{table}\n\n\\begin{table}[!t]\n    \\centering\n    \\begin{tabular}{@{}cccccc@{}}\n      \\toprule\n      Test set & PSGAN & BGAN & DIA & CGAN & LADN\\\\ \\midrule\n      MT       &  \\textbf{61.5}  &  32.5 &  3.25 & 2.5  &  0.25 \\\\ \\midrule\n      M-Wild   & \\textbf{83.5} & 13.5 & 1.75 & 1.25 & 0.0 \\\\ \\bottomrule\n    \\end{tabular}\n    \\vspace{1mm}\n    \\caption{Ratio selected as best (\\%).}\n    \\label{t2}\n    \\vspace{-4mm}\n\\end{table}\n\n\\textbf{Quantitative Comparison.} We conduct a user study for quantitative evaluation on Amazon Mechanical Turk (AMT) that use BGAN, CGAN, DIA, and LADN as baselines. For a fair comparison, we only compare with methods whose code and pre-train model are released since we cannot guarantee a perfect re-implementation. We randomly select 20 source images and 20 reference images from both the MT test set and Makeup-Wild (M-Wild) dataset. After using the above methods to perform makeup transfer between these images, we obtain 800 images for each method. Then, 5 different workers are asked to choose the best images generated by five methods through considering image realism and the similarity with reference makeup styles. The generated images are shown in random order for a fair comparison. Table \\ref{t2} shows the human evaluation results. Our PSGAN outperforms other methods by a large margin, especially on the M-Wild test set.\n\n\\textbf{Qualitative Comparison.} Figure \\ref{res1} shows the qualitative comparison of PSGAN with other state-of-the-art methods on frontal faces in neutral expressions. Since the code of BeautyGlow and PairedCycleGAN is not released, we follow the strategy of BeautyGlow which cropped the results from corresponding papers. \nThe result produced by DIA has an unnatural color on hair and background since it performs transfer in the whole image.\nComparatively, the result of CycleGAN is more realistic than that of DIA, but CycleGAN can only synthesize general makeup which is not similar to the reference.\nThe current makeup transfer methods outperform the previous methods. However, BeautyGlow fails to preserve the color of pupils and does not have the same foundation makeup as reference. We also use the pre-trained model released by the author of LADN, which produces blurry transfer results and unnatural background.\nCompared to the baselines, our method is able to generate vivid images with the same makeup styles as reference.\n\n\\begin{figure}[!t]\n    \\includegraphics[width=1\\linewidth]{figures/wild_comparsion.pdf}\n    \\caption{Qualitative comparison on M-Wild test set.}\n    \\label{wild}\n    \\vspace{-3mm}\n\\end{figure}\n\nWe also conduct a comparison on the M-Wild test set with the state-of-the-art method (BeautyGAN and LADN) that provide code and pre-trained model, as shown in Figure \\ref{wild}.\nSince the current methods lack an explicit mechanism to guide the direction of make transfer at the pixel-level and also overfit on frontal images, the makeup is applied in the wrong region of the face when dealing with images with different poses and expressions. For example, the lip gloss is transferred to the skin on the first row of Figure \\ref{wild}. In the second row, other methods fail to perform transfer on faces with different sizes. However, the proposed AMM module can accurately assign the makeup for every pixel through calculating the similarities, which makes our results look better.\n\n\\subsection{Video Makeup Transfer}\nTo transfer makeup for a person in the video is a challenging and meaningful task, which has wide prospects in the applications. However, the pose and expression of a face in the video are continuously changing which brings extra difficulties. To examine the effectiveness of our method, we simply perform makeup transfer on every frame of the video, as shown in Figure \\ref{video}. By incorporating the design of PSGAN, we receive nice and stable transferred results.\n\n\\begin{figure}[!t]\n   \\includegraphics[width=1\\linewidth]{figures/video.pdf}\n   \\caption{Video makeup transfer results of PSGAN.}\n   \\label{video}\n   \\vspace{-2mm}\n\\end{figure}\n \n\\section{Conclusion}\nIn order to bring makeup transfer to real-world applications, we propose the Pose and expression robust Spatial-Aware GAN (PSGAN) that first distills the makeup style into two makeup matrices from the reference and then leverages an Attentive Makeup Morphing (AMM) module to conduct makeup transfer accurately. \nThe experiments demonstrate our approach can achieve state-of-the-art transfer results on both frontal facial images and facial images that have various poses and expressions. Also, with the spatial-aware makeup matrices, PSGAN can transfer the makeup partially and adjust the shade of transfer, which greatly broadens the application range of makeup transfer. Moreover, we believe our novel framework can be used in other conditional image synthesis problems that require customizable and precise synthesis.\n\n{\\small\n}\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{EleGANt: Exquisite and Locally Editable GAN for Makeup Transfer}\n\n\\begin{document}\n\n\\pagestyle{headings}\n\\mainmatter\n\\def\\ECCVSubNumber{5430}  % Insert your submission number here\n\n\\title{EleGANt: Exquisite and Locally Editable GAN for Makeup Transfer} % Replace with your title\n\n\\begin{comment}\n\\titlerunning{ECCV-22 submission ID \\ECCVSubNumber} \n\\authorrunning{ECCV-22 submission ID \\ECCVSubNumber} \n\\author{Anonymous ECCV submission}\n\\institute{Paper ID \\ECCVSubNumber}\n\\end{comment}\n\n\\titlerunning{EleGANt: Exquisite and Locally Editable GAN for Makeup Transfer}\n\\author{Chenyu Yang\\inst{1}\\and\nWanrong He\\inst{1}\\and\nYingqing Xu\\inst{1}\\textsuperscript{(\\textrm{\\Letter})}\\and\nYang Gao\\inst{1,2}\\textsuperscript{(\\textrm{\\Letter})}\n}\n\n\\authorrunning{C. Yang et al.}\n\\institute{Tsinghua University, Beijing, China\\\\\n\\email{\\{yangcy19, hwr19\\}@mails.tsinghua.edu.cn\\\\ \n\\{yqxu, gaoyangiiis\\}@tsinghua.edu.cn} \\and\nShanghai Qi Zhi Institute, Shanghai, China\n}\n\\maketitle\n\\let\\thefootnote\\relax\\footnotetext{\\textsuperscript{\\textrm{\\Letter}}Corresponding author}\n\n\\begin{figure}[!htbp]\n\\vskip -0.2cm\n\\setlength{\\abovecaptionskip}{-0.1cm}\n\\setlength{\\belowcaptionskip}{-0.6cm} \n\\centering\n\\includegraphics[width=0.96\\textwidth]{figures/teaser.pdf}\n\\caption{Our proposed EleGANt generates makeup faces with exquisite details. It supports flexible control such as (a) global shade control and (b) customized local editing.}\n\\label{fig-teaser}\n\\end{figure}\n\n\\begin{abstract}\nMost existing methods view makeup transfer as transferring color distributions of different facial regions and ignore details such as eye shadows and blushes. Besides, they only achieve controllable transfer within predefined fixed regions. This paper emphasizes the transfer of makeup details and steps towards more flexible controls. To this end, we propose Exquisite and locally editable GAN for makeup transfer (EleGANt). \nIt encodes facial attributes into pyramidal feature maps to preserves high-frequency information. It uses attention to extract makeup features from the reference and adapt them to the source face, and we introduce a novel Sow-Attention Module that applies attention within shifted overlapped windows to reduce the computational cost. Moreover, EleGANt is the first to achieve customized local editing within arbitrary areas by corresponding editing on the feature maps. Extensive experiments demonstrate that EleGANt generates realistic makeup faces with exquisite details and achieves state-of-the-art performance. The code is available at \\href{https://github.com/Chenyu-Yang-2000/EleGANt}{https://github.com/Chenyu-Yang-2000/EleGANt}. \n\\keywords{Makeup Transfer, Sow-Attention, GAN Controls}\n\\end{abstract}\n\n\\section{Introduction}\nMakeup transfer aims to transfer the makeup from an specific reference image to a source image. It has tremendous value in practical scenarios, for instance, cosmetics try-on and marketing. The goal of makeup transfer is two-fold: (1) Precisely transferring the makeup attributes from the reference to the source. The attributes include low-frequency color features and high-frequency details such as the brushes of eye shadow and blushes on the cheek. (2) Preserving the identity of the source, involving shapes, illuminations, and even subtle wrinkles. Controllable transfer further meets the requirement in practice: it allows users to design customized makeup according to their preferences.\n\nDeep learning approaches, especially GAN-based models \\cite{BeautyGAN,PairedCycleGAN,LADN,PSGAN,SCGAN}, have been widely employed in this task. They mainly adopt the CycleGAN \\cite{CycleGAN} framework that is trained on unpaired non-makeup and with-makeup images, with extra supervision, e.g., a makeup loss term \\cite{BeautyGAN,PairedCycleGAN,FAT}, to guide the reconstruction of a specific makeup on the source face. \nNotwithstanding the demonstrated success, existing approaches mainly view makeups as color distributions and largely ignore the spatial, high-frequency information about details. Some methods \\cite{BeautyGAN,LADN} cannot tackle the pose misalignment between the two faces, while others represent makeups by matrices of limited size \\cite{PSGAN} or 1D-vectors \\cite{SCGAN}, resulting in high-frequency attributes being smoothed.\nMeanwhile, the commonly used objective, histogram matching \\cite{BeautyGAN}, only imposes constraints on color distributions without incorporating any spatial information. Besides, existing controllable models \\cite{PSGAN,SCGAN} only achieve editing the makeup within some fixed regions, for instance, skin, lip, and eyes, instead of arbitrary customized regions.\n\nTo address these issues, we propose Exquisite and locally editable GAN for makeup transfer (EleGANt). On one hand, we focus on high-frequency information to synthesize makeup faces with rich and delicate details.\nEleGANt encodes facial attributes of different frequencies into feature maps of a pyramid structure. To tackle misaligned head poses, it uses QKV-attention to extract makeup features from the reference and adapt them to the source face by pixel-wise correspondence. \nWe employ a high-resolution feature map to preserve high-frequency attributes and further propose a novel Sow-Attention Module to reduce the computational cost. It computes attention efficiently within local windows and uses shifted overlapped windowing schemes to ensure the continuity of the output. The network is trained with a newly designed pseudo ground truth which comprises both color and spatial information.\nOn the other hand, the high-res makeup feature maps support precise editing to control the makeup style and shade within arbitrary customized areas. To our best knowledge, EleGANt is the first makeup transfer network to achieve this free-style local editing.\n\nFig. \\ref{fig-teaser} exhibits the great capability and controllability of EleGANt. It generates realistic makeup images with high-fidelity colors and high-quality details. Global shade control (Fig. \\ref{fig-teaser}(a)) and customized local editing (Fig. \\ref{fig-teaser}(b)) can be realized by manipulating the makeup feature maps. Extensive experiments demonstrate the superiority of EleGANt compared with the existing methods.\n\nThe contributions of this paper can be summarized as follows:\n\\begin{itemize}\n    \\item We propose EleGANt, a fully automatic makeup transfer network with the most flexible control among existing methods. To our best knowledge, it is the first to achieve customized local editing of makeup style and shade.\n    \\item EleGANt uses a pyramid structure with a high-resolution feature map to preserve high-frequency makeup features beyond color distributions. It achieves state-of-the-art performance, especially in processing makeup details.\n    \\item A novel Sow-Attention Module that computes attention within shifted overlapped windows is introduced, which guarantees the continuity of the output and reduces the computational cost for high-resolution inputs.\n\\end{itemize}\n\n\\section{Related Work}\n\\subsection{Makeup Transfer}\nMakeup transfer has been studied in computer vision for a decade. Traditional methods \\cite{Example-1,Example-2,tradition-1,tradition-2,tradition-3} utilized image processing techniques. Later, CycleGAN \\cite{CycleGAN} and its variants \\cite{StarGAN,FDIT} were widely used for image-to-image translation tasks such as facial attribute transfer. However, these methods focus on domain-level rather than instance-level transfer, and they do not well maintain some facial attributes, e.g., shape and pose, that keep unchanged during makeup.\n\nInspired by the successes in GANs, makeup transfer was formulated as an asymmetric domain translation problem in PairedCycleGAN \\cite{PairedCycleGAN}. They also employed an additional discriminator to guide makeup transfer with pseudo transferred images. %generated by image warping. \nBeautyGAN \\cite{BeautyGAN} introduced a dual input/output GAN for simultaneous makeup transfer and removal and a color histogram matching loss for instance-level makeup transfer. BeautyGlow \\cite{BeautyGlow} utilized the Glow framework to disentangle the latent features into makeup and non-makeup components. LADN \\cite{LADN} leveraged multiple and overlapping local discriminators to ensure local details consistency. PSGAN \\cite{PSGAN} and FAT \\cite{FAT} proposed to use attention mechanism to handle misaligned facial poses and expressions. Lately, SCGAN \\cite{SCGAN} attempted to eliminate the spatial misalignment problem by encoding makeup styles into component-wise style-codes. \n\nHowever, the existing methods have limitations in processing makeup details: some methods \\cite{BeautyGAN,LADN} cannot tackle the pose misalignment; \\cite{SCGAN} encodes the makeups into 1D-vectors, thus discarding a large proportion of spatial information; the high cost of pixel-wise attention in \\cite{PSGAN,FAT} limited the size of feature maps then harmed the preservation of details.\nEleGANt surpasses these approaches by using a high-res feature map and an efficient Sow-Attention Module. Besides, unlike previous models \\cite{PSGAN,SCGAN} that can only adjust the makeup in a fixed set of regions, our EleGANt supports customized local editing in arbitrary regions.\n\n\\subsection{Style Transfer}\nStyle transfer can be regarded as a general form of makeup transfer, and it has been investigated extensively since the rise of deep convolutional neural networks \\cite{DIA,NeuralTransfer,PerceptualLoss,phototransfer}. However, these methods either require a time-consuming optimization process or can only transfer a fixed set of styles. \nThen \\cite{AdaIN} proposed adaptive instance normalization (AdaIN) that matched the mean and variance of the content features with those of the style features and achieved arbitrary style transfer. Since style transfer methods do not consider the face-specific semantic correspondence and lack local manipulation and controllability, even the state-of-the-art algorithms \\cite{style-1,style-2,style-3} cannot fit makeup transfer applications.\n\n\\subsection{Attention Mechanism}\n\nAttention mechanism has been widely used in the computer vision area. Early works \\cite{vattn-1,vattn-2} simply apply it to images where each pixel attends to every other pixel, but they do not scale to large input sizes due to a quadratic cost in the number of pixels. Many variants have been tried so far to make attention more efficient and applicable to high-resolution images, e.g., tokenization on patches \\cite{ViT,T2T}, attention in local regions \\cite{local-attn-1,Swin,ViL,Twins}, and pyramid architecture \\cite{PVT,PiT,R2L}. Since these modules are plugged into discriminative networks for image representations, they are not exactly suitable for generative tasks. Meanwhile, sparse attention such as block-wise \\cite{block-1,block-2}, axial-wise \\cite{axial-1,GODIVA}, and nearby attention \\cite{NUWA} has been introduced into visual synthesis models, but neither global nor regional attributes of the makeup can be completely encoded due to the sparsity.\n\nFor makeup transfer, naive pixel-wise attention employed by \\cite{PSGAN,FAT} suffers from significant computational overhead and is hard to scale to inputs of larger size. To address this problem, EleGANt uses a novel Sow-Attention Module that performs attention within shifted overlapped windows. \n\n\\section{Methodology}\n\\subsection{Formulation}\nLet $X$ and $Y$ be the non-makeup image domain and the\nmakeup image domain, where $\\{x^n\\}_{n=1,\\dots,N},x^n\\in X$ and $\\{y^m\\}_{m=1,\\dots,M},y^m\\in Y$ denote the examples of two domains respectively. We assume no paired data is available, i.e., the non-makeup and makeup images have different identities. Our proposed EleGANt aims to learn a transfer function $\\mathcal{G}$: given a source image $x$ and a reference image $y$, $\\hat{x}=\\mathcal{G}(x,y)$, where $\\hat{x}$ is the transferred image with the makeup style of $y$ and the face identity of $x$.  \n\n\\subsection{Network Architecture}\n\\label{sec:arch}\n\\begin{figure}[t]\n    \\setlength{\\belowcaptionskip}{-0.2cm} \n    \\centering\n    \\includegraphics[width=0.75\\textwidth]{architecture_figures/architecture.pdf}\n\\caption{Overall structure of our proposed EleGANt. Facial Attribute Encoder (FAEnc) constructs pyramidal feature maps. Makeup Transfer Module (MTM) yields low-res and high-res makeup feature maps, $\\Gamma_L$ and $\\Gamma_H$, by Attention and Sow-Attention modules respectively. Makeup Apply Decoder (MADec) applies the makeup feature maps to the source to generate the final result.}\n    \\label{fig-arch}\n\\end{figure}\n\\subsubsection{Overall.}\nThe architecture of EleGANt is shown in Fig. \\ref{fig-arch}, which consists of three components: (1) \\textit{Facial Attribute Encoder.} FAEnc encodes the facial attributes into feature maps of pyramid structure. High-res feature maps $X_H$, $Y_H$ mainly contain high-frequency information about sharp edges and details, while low-res ones $X_L$, $Y_L$ contain more low-frequency information related to colors and shadows. (2) \\textit{Makeup Transfer Module}. In MTM, an Attention Module is applied to the low-res feature maps and yields a low-res makeup feature map $\\Gamma_L$, while a Sow-Attention Module is for the high-res one $\\Gamma_H$. These two modules extract makeup features of different frequencies and utilize attention to make them spatially aligned with the source face to tackle the misalignment between the two faces. (3) \\textit{Makeup Apply Decoder.} MADec applies the two makeup feature maps $\\Gamma_L$ and $\\Gamma_H$ to corresponding feature maps of the source respectively by element-wise multiplication and generates the final result. \n\n\\subsubsection{Attention Module.} \nSince the two faces may have discrepancies in expressions and poses, the makeup attributes of the reference face need to be adapted to the source face. We employ a QKV-cross-attention similar to Transformer \\cite{Transformer} to model the pixel-wise correspondence between the two faces. Formally, given a pair of feature maps extracted from the source and reference, $X,Y\\in \\mathbb{R}^{HW\\times C}$, where $C$, $H$ and $W$ are the number of channels, height and width of the feature map, we compute the attentive matrix $A\\in \\mathbb{R}^{HW\\times HW}$ to specify how a pixel on $X$ corresponds to its counterparts on $Y$:\n\\begin{equation}\n\\begin{aligned}\n    A=softmax\\left(\\frac{\\widetilde{X}Q(\\widetilde{Y}K)^T}{\\sqrt{C}}\\right)\n\\end{aligned}\n\\end{equation}\nwhere $K,Q\\in\\mathbb{R}^{C\\times C}$ \nare learnable parameters, \nand $\\widetilde{X},\\widetilde{Y}$ are the feature maps combined with positional embedding to introduce spatial information. Here, we adopt Landmark Embedding \\cite{FAT} that concatenates a vector representing the relative positions to the facial landmarks with the visual features for each pixel.\n\nMakeup features are extracted by a $1\\times1$-Conv with weights $V\\in\\mathbb{R}^{C\\times C}$ from $Y$. They consequently maintain the spatial correspondence with $Y$. Then the attentive matrix $A$ is applied to align them with the spatial distribution of $X$. The aligned features are represented by a makeup feature map $\\Gamma\\in\\mathbb{R}^{HW\\times C}$:\n\\begin{equation}\n\\begin{aligned}\n    \\Gamma =A(YV)\n\\end{aligned}\n\\end{equation}\nAfter that, the makeup feature map $\\Gamma$ becomes the input of MADec and then is applied to the source feature map $X$ by element-wise multiplication: \n\\begin{equation}\n\\begin{aligned}\n    \\widehat{X} = \\Gamma \\odot X\n\\end{aligned}   \n\\end{equation}\nBefore being fed into MADec, $\\Gamma$ can be globally or locally manipulated to achieve controllable transfer, which will be discussed in detail in Sec. \\ref{sec:control}. \n \n\n\\subsubsection{Sow-Attention Module.} \nTo avoid high-frequency information being smoothed and support precise local editing, we utilize feature maps of high resolution. However, the above pixel-wise attention is not practicable here due to the high quadratic cost. We propose a novel shifted overlapped windowing attention (Sow-Attention) to reduce the complexity. As illustrated in Fig. \\ref{fig:sow}, the Sow-Attention Module obtains the makeup feature map $\\Gamma$ by three steps:\n\n(1) \\textit{Coarse alignment.} We employ Thin Plate Splines (TPS) to warp  $Y$ into $Y'$ to be coarsely aligned with the source face. Specifically, TPS is determined by $N$ control points whose coordinates in the original space and the target space are denoted as $C = \\{c_i\\}_{i=1}^N$ and $C' = \\{c'_i\\}_{i=1}^N$, respectively. Here, $C$ is set to be the coordinates of $N$ landmark points \\cite{landmark} of the reference face and $C'$ to be those of the source face, and then TPS warps $Y$ into $Y'$ to fit $C$ into $C'$. We use the parameterized grid sampling \\cite{STN} to perform 2D TPS that is differentiable w.r.t input $Y$ and the formulation from \\cite{ASTER} to obtain required parameters.\n\n(2) \\textit{Attention.} Since $X$ and $Y'$ are coarsely aligned, local attention is enough for a point on $X$ to capture makeup information from the neighbor region on $Y'$. To avoid the boundary issue of non-overlapped windows that leads to artificial edges in the output image (see Sec. \\ref{sec:ablation}), we perform attention in shifted overlapped windows. As depicted in Fig. \\ref{fig:sow}, $w_1$, $w_2$, $w_3$ and $w_4$ represent 4 partitioning schemes that split the feature maps the into overlapped windows of size $S$ with an $S/2$ shift. A QKV-attention is shared by all windows, and the result computed within window $w_j$ is denoted as $\\Gamma^{w_j}\\in\\mathbb{R}^{S^2\\times C}$, where $C$ is the number of channels. An alternative view is that a pixel cross-attends to the four windows it belongs to, for instance, the pixel $x_i$ marked in Fig. \\ref{fig:sow} attends to the windows $\\{w_j\\}_{j=1}^4$ on $Y'$ and obtains four vectors $\\Gamma^{w_j}(x_i)\\in \\mathbb{R}^{C},j=1,2,3,4$.\n\n\\begin{figure}[t]\n    \\setlength{\\belowcaptionskip}{-0.1cm}\n    \\centering\n    \\includegraphics[width=0.99\\textwidth]{architecture_figures/sow2.pdf}\n\\caption{Illustration of the Sow-Attention Module. Attention is computed within shifted overlapped windows across coarsely aligned feature maps, and the outputs are aggregated by weighted sum. A darker color indicates a larger weight.}\n\\label{fig:sow}\n\\end{figure}\n(3) \\textit{Aggregation.} For each pixel $x_i$, the four vectors derived from previous attention, $\\{\\Gamma^{w_j}(x_i),\\ j:x_i\\in w_j\\}$, are aggregated into one vector as the final output. We conduct this by a weighted sum:\n\\begin{equation}\n\\begin{aligned}\n\\Gamma(x_i)=\\sum_{j:x_i\\in w_j}\\Gamma^{w_j}(x_i)\\cdot W(x_i,w_j)\n\\end{aligned}\n\\label{eq:sum}\n\\end{equation}\nwhere the weight $W(x_i,w_j)$ is determined by the relative position of $x_i$ to $w_j$. $W(x_i,w_j)$ should guarantee that the output is spatially continuous both inter and intra windows. Besides, if $x_i$ is closer to the center of $w_j$, $\\Gamma^{w_j}(x_i)$ will contain more information about its neighbor region, and $W(x_i,w_j)$ is expected to be larger. We choose a ``bilinear'' form that works well in practice:\n\\begin{equation}\n\\begin{aligned}\n    W(x_i,w_j)=\\frac{\\left|\\left(S-2\\left(\\mathrm{x}(x_i)-\\mathrm{x}(c_{w_j})\\right)\\right)\\left(S-2\\left(\\mathrm{y}(x_i)-\\mathrm{y}(c_{w_j})\\right)\\right)\\right|}{S^2}\n\\end{aligned}\n\\label{eq:weight}\n\\end{equation}\nwhere $c_{w_j}$ denotes the center of window $w_j$, $\\mathrm{x}(\\cdot)$ and $\\mathrm{y}(\\cdot)$ is the x-coordinate and y-coordinate of a point respectively. Eq. \\ref{eq:sum} and Eq. \\ref{eq:weight} can also be interpreted as a kind of ``bilinear interpolation'': the attention results from different windows are ``interpolated'' regarding their centers as anchor points. \n\nSow-Attention reduces the cost of pixel-wise attention from $O\\left((HW)^2\\right)$ to $O\\left(HWS^2\\right)$. Generally, $S=H/8$ would be enough, and then the complexity is reduced by a factor of 16 since the attention is performed with four partitioning schemes and the cost of each is $(1/8)^2$ of the original attention. \n\n\\subsection{Makeup Loss with Pseudo Ground Truth}\nDue to the lack of paired makeup and non-makeup images, we adopt the CycleGAN \\cite{CycleGAN} framework to train the network in an unsupervised way. Nevertheless, normal GAN training only drives the generator to produce realistic images with a general makeup. Therefore, to guide the reconstruction of specific makeup attributes of the reference on the source face, extra supervision, i.e., a makeup loss term for the generator, is introduced into the total objective $L_{total}$:\n\\begin{equation}\n\\begin{aligned}\n    L_{total}&=\\lambda_{a d v} (L_{\\mathcal{G}}^{a d v} +  L_{\\mathcal{D}}^{a d v})+\\lambda_{c y c} L_{\\mathcal{G}}^{c y c}+\\lambda_{per} L_{\\mathcal{G}}^{per}+\\lambda_{make} L_{\\mathcal{G}}^{make}\n\\end{aligned}\n\\end{equation}\nwhere $L^{a d v},L^{c y c},L^{per}$ are adversarial loss \\cite{GAN}, cycle consistency loss \\cite{CycleGAN}, and perceptual loss \\cite{PerceptualLoss} (formulations are summarized in App. %C).\n\\ref{sec:objective}). \nThe makeup loss $L_{\\mathcal{G}}^{make}$ is defined with pseudo ground truth (PGT), namely, images that are synthesized independently of the generator and serve as the training objective:\n\\begin{equation}\n\\begin{aligned}\nL_{\\mathcal{G}}^{m a k e} &=\\|\\mathcal{G}(x, y)-P G T(x, y)\\|_{1} +\\|\\mathcal{G}(y, x)-P G T(y, x)\\|_{1} .\n\\end{aligned}\n\\end{equation}\nwhere $PGT(x,y)$ has the makeup of $y$ and the face identity of $x$. \nAs shown in Fig. \\ref{fig:PGT_Diff_strat}, typical strategies for PGT generation include histogram matching \\cite{BeautyGAN,PSGAN} and TPS warping \\cite{FAT}. Histogram matching equalizes the color distribution of the source face with that of the reference, but it suffers from extreme color differences and discards all spatial information. TPS warps the reference face into the shape of the source face by aligning detected landmarks, but it may result in artifacts of stitching and distortion and also mix in unwanted shadows. The imprecision of these PGTs will consequently cause a sub-optimal transfer. \n\\begin{figure}[t]\n    \\setlength{\\belowcaptionskip}{-0.2cm}\n    \\centering\n    \\includegraphics[width=0.79\\textwidth]{pgt_figures/pgt_compare.pdf}\n    \\caption{Different strategies for pseudo ground truth (PGT) generation. Our PGT has more accurate details (e.g., eye shadows) than histogram matching and fewer artifacts and distortions than TPS warping. The final generated image has better quality than PGTs, e.g., there are no jags on the edge of the lip and artifacts on the forehead.}\n    \\label{fig:PGT_Diff_strat}\n\\end{figure}\n\n\\begin{figure}[!htbp]\n    \\setlength{\\belowcaptionskip}{-0.2cm}\n    \\centering\n    \\includegraphics[width=0.92\\textwidth]{pgt_figures/pgt.pdf}\n    \\caption{The pipeline of our PGT generation. It utilizes histogram matching for color matching, TPS warping for detail matching, and annealing factors for image blending.}\n    \\label{fig:PGT}\n\\end{figure}\nTo address these issues, we propose a novel strategy that incorporates both color and spatial information and avoids misleading signals. Although our PGT is not comparable with generated images in quality, it provides sufficient guidance complementary to the GAN training. As illustrated in Fig. \\ref{fig:PGT}, it consists of two stages: color matching and detail matching.\n\n(1) \\textit{Color Matching.}\nWe adopt histogram matching \\cite{BeautyGAN} to replicate the makeup color of the reference $y$ to the source $x$. The color distributions in the skin, lip, and eye shadow regions are separately equalized between $x$ and $y$. \n\n(2) \\textit{Detail Matching.}\nWe employ TPS transformation to incorporate spatial information into the PGT. Specifically, skin, lip, and eye shadow regions of the reference $y$ are separately warped to fit the source $x$ using corresponding facial landmarks, and blended with their counterparts on the color matching result. The blending factors $\\alpha^D_{skin}$, $\\alpha^D_{eyes}$, and $\\alpha^D_{lip}$ anneal during the training process to emphasize colors or details in different stages of training for better results. See App. %D.3\n\\ref{sec:pgt} \nfor implementation details. \n\n\\section{Experiments}\n\\subsection{Experiment Settings}\nWe use the MT (Makeup Transfer) dataset \\cite{BeautyGAN} which contains 1115 non-makeup images and 2719 makeup images to train our model. We follow the strategy of \\cite{BeautyGAN} to split the train/test set. All images are resized to $256\\times256$ before training. More implementation details and results are given in supplementary materials.\n\nWe conduct comparisons of EleGANt with general style transfer methods: DIA \\cite{DIA}, CycleGAN \\cite{CycleGAN} as well as makeup transfer methods: BeautyGAN \\cite{BeautyGAN}, BeautyGlow \\cite{BeautyGlow}, LADN \\cite{LADN}, PSGAN \\cite{PSGAN}, Spatial FAT \\cite{FAT}, and SCGAN \\cite{SCGAN}. Since the implementation of BeautyGlow and Spatial FAT is not released, we follow \\cite{BeautyGAN} and take the results from corresponding papers. Table \\ref{tab:func} summarizes the functions of open-source makeup transfer models. Our EleGANt demonstrates the greatest capability and flexibility among all methods. It can precisely transfer makeup details and is the first to achieve customized local editing.\n\n\\begin{table}[!htbp]\n\\setlength{\\belowdisplayskip}{0cm}\n\\setlength{\\abovecaptionskip}{0cm}\n\\centering\n\\caption{Analyses of EleGANt with existing open-source methods. ``Misalign.'': robust transfer with large spatial misalignment between the two faces. ``Detail'': precise transfer with high-quality details. ``Shade'': shade-controllable transfer. ``Part'': partial transfer for lip, eye, and skin regions. ``Local.'': local editing within arbitrary areas.}\n\\label{tab:func}\n\\begin{tabular}{c|cc|ccc}\n\\hline\n\\multirow{2}{*}{Method} & \\multicolumn{2}{c|}{Capability}        & \\multicolumn{3}{c}{Controllability}                            \\\\ \\cline{2-6} \n                        & \\multicolumn{1}{c|}{\\;Misalign.\\;} & \\;Detail\\; & \\multicolumn{1}{c|}{\\;Shade\\;} & \\multicolumn{1}{c|}{\\;Part\\;} & \\;Local.\\; \\\\ \\hline\nBeautyGAN \\cite{BeautyGAN}\\;   &  &  &  &  &  \\\\\nLADN \\cite{LADN}\\;   &  &  & \\checkmark &  &  \\\\\nPSGAN \\cite{PSGAN}\\; & \\checkmark &  & \\checkmark & \\checkmark &  \\\\\nSCGAN \\cite{SCGAN}\\;  & \\checkmark &  & \\checkmark & \\checkmark &  \\\\\nEleGANt (ours)\\; & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\checkmark \\\\ \\hline\n\\end{tabular}\n\\vskip -0.7cm\n\\end{table}\n\n\\subsection{Qualitative Comparison.}\nFig. \\ref{fig:model_comparison} shows the results on images with frontal faces in neutral expressions and light makeups. The results of DIA have unnatural colors on the hair and shadows on the face. CycleGAN synthesizes realistic images, but it simply performs domain transfer without recovering any makeup of the reference. BeautyGlow fails to transfer the correct color of skin and lip. Severe artifacts and blurs exist in the results of LADN. Recent works such as PSGAN, SCGAN, and Spatial FAT can generate more visually acceptable results. However, PSGAN and SCGAN suffer from the color bleeding problem, e.g., blurs at the edge of the lips. Spatial FAT generates results with richer details, but there are still apparent artifacts. Compared to existing methods, our proposed EleGANt generates the most realistic images with natural light and shadow while synthesizing high-fidelity makeup colors and high-quality details.\n\n\\begin{figure}[t]\n    \\setlength{\\abovedisplayskip}{0cm}\n    \\setlength{\\abovecaptionskip}{-0.2cm}\n    \\setlength{\\belowcaptionskip}{0.1cm}\n    \\centering\n    \\includegraphics[width=1.0\\textwidth]{figures/model_compare2.pdf}\n    \\caption{Qualitative comparisons with existing methods. EleGANt generates the most precise transferred result with the desired makeup and high-quality details.}\n    \\label{fig:model_comparison}\n\\end{figure}\n\\begin{figure}[!t]\n    \\setlength{\\abovedisplayskip}{0cm}\n    \\setlength{\\belowcaptionskip}{-0.2cm}\n    \\centering\n    \\includegraphics[width=0.90\\textwidth]{figures/compare2.pdf}\n    \\caption{Comparison on images with misaligned poses and complex makeups. Our proposed EleGANt generates the most exquisite details (e.g., the shapes and colors of the eye shadows), the most natural colors and shadows, and the fewest artifacts.}\n    \\label{fig:robuts_comparison}\n\\end{figure}\n\nTo test the effectiveness on complex makeups and robustness against spatial misalignment, we compare our method with makeup transfer models that have released code. The results are shown in Fig. \\ref{fig:robuts_comparison}. None of the existing methods can precisely transfer makeup details such as the shapes and colors of eye shadows (the 1st, 2nd, and 4th rows) due to the loss of high-frequency information. Our EleGANt synthesizes these details with the highest quality with the help of high-resolution feature maps. Existing methods fall short in cases that the two faces have large discrepancies in pose or illumination, resulting in unnatural colors and shadows (the 3rd and 4th rows) in the transferred images. The results of EleGANt have natural colors and shadows consistent with the faces. Besides, images generated by EleGANt have the fewest artifacts (4th row) among all of the methods. More samples of comparison are provided in App. %E.2. \n\\ref{sec:add-compare}. \n\n\\subsection{Quantitative Comparison. }\nWe conduct a user study to quantitatively evaluate the generation quality and the transfer precision of different models. For a fair comparison, we compare our EleGANt with the methods whose code and pre-train model are available: BeautyGAN, LADN, PSGAN, and SCGAN. We randomly selected 20 generated images from the test split of the MT dataset. Totally 40 participants were asked to evaluate these samples in three aspects: ``visual quality'', ``detail processing'' (the quality and precision of transferred details), and ``overall performance'' (considering the visual quality, the fidelity of transferred makeup, etc.). They then selected the best one in each aspect. Table \\ref{tab:user} demonstrates the results of the user study. Our EleGAnt outperforms other methods in all aspects, especially in synthesizing makeup details.\n\n\\begin{table}[!htbp]\n\\setlength{\\belowdisplayskip}{0cm}\n\\setlength{\\abovecaptionskip}{0cm}\n\\centering\n\\caption{User study results (ratio ($\\%$) selected as the best). ``Quality'', ``Detail'' and ``Overall'' denote the three aspects for evaluation: visual quality, detail processing, and overall performance.}\n\\label{tab:user}\n\\begin{tabular}{c|ccccc}\n\\hline\n        & \\;BeautyGAN\\; & \\;LADN\\; & \\;PSGAN\\; & \\;SCGAN\\; & \\;EleGANt (ours)\\; \\\\ \\hline\n\\;Quality\\; &  $6.75$ & $1.88$ & $11.13$ & $21.25$ & $\\mathbf{59.00}$ \\\\\n\\;Detail\\; & $5.38$ & $2.75$ & $11.25$ & $14.38$ & $\\mathbf{66.25}$ \\\\ \\hline\n\\;\\textbf{Overall}\\; &  $4.75$ & $2.75$ & $9.88$ & $20.38$ & $\\mathbf{62.25}$ \\\\ \\hline\n\\end{tabular}\n\\vskip -0.7cm\n\\end{table}\n\n\\subsection{Controllable Makeup Transfer}\\label{sec:control}\nSince the makeup feature maps $\\Gamma_H$ and $\\Gamma_L$ spatially correspond to the source face and are combined with the source feature maps by element-wise multiplication, controllability can be achieved by interpolating those makeup feature maps.\n\n\\subsubsection{Partial and Interpolated Makeup Transfer.}\n Transferring the makeup in predefined parts of the face, for instance, lip and skin, can be realized automatically by masking makeup feature maps using face parsing results. Specifically, let $x$ denote the source image, $y_i$ denote the reference image we would like to transfer for part $i$, and $M_i^x\\in [0,1]^{H\\times W}$ is the corresponding parsing mask, the partial transferred feature maps are calculated as: \n\n\\begin{equation}\n\\begin{aligned}\n\\Gamma_* &= M_i^x\\odot \\Gamma_*^{y_i} + (1- M_i^x) \\odot \\Gamma_*^x\n\\end{aligned}\n\\end{equation}\nwhere $*\\in \\{H,L\\}$, $\\Gamma_*^y$ is the makeup feature map with $x$ being the source and $y$ being the reference, the mask is expanded along the channel dimension, and $\\odot$ denotes the Hadamard product. Interpolating the makeup feature maps can control the shade of makeup or fuse makeup from multiple references. Given a source image $x$, two reference images $y_1,y_2$, and a coefficient $\\alpha^S\\in[0,1]$, we first get the makeup feature maps of $y_1$ and $y_2$ w.r.t. $x$, and interpolate them by \n\\begin{equation}\n\\begin{aligned}\n\\Gamma_*&= \\alpha^S \\Gamma_*^{y_1} + (1-\\alpha^S) \\Gamma_*^{y_2}\\ ,\\ *\\in\\{H,L\\}\n\\end{aligned}\n\\end{equation}\n If we want to adjust the shade of makeup using a single reference style, just simply set $y_2=x$, then $\\alpha^S$ indicates the intensity. We also perform partial and interpolated makeup transfer simultaneously by leveraging both area masks and shade intensity, and the results are shown in Fig. \\ref{fig:control}. \n \\begin{figure}[t]\n    \\setlength{\\abovecaptionskip}{-0.1cm}\n    \\setlength{\\belowcaptionskip}{0cm}\n    \\centering\n    \\includegraphics[width=1.0\\textwidth]{figures/control2.pdf}\n    \\caption{Partial and interpolated makeup transfer. The first row applies a global transfer. The second row only transfers the lipsticks and eye shadows of the two references. }\n    \\label{fig:control}\n\\end{figure}\n\n\\begin{figure}[t]\n    \\setlength{\\abovedisplayskip}{0cm}\n    \\setlength{\\abovecaptionskip}{-0.1cm}\n    \\setlength{\\belowcaptionskip}{-0.2cm}\n    \\centering\n    \\includegraphics[width=1.0\\textwidth]{figures/paint.pdf}    \\caption{Customized local editing. In the first row, we select the areas around the eyes and cheek to adjust the eye shadow and blush. In the second row, we select the areas of upper and lower eye shadow and assign them with different references.}\n    \\label{fig:paint}\n\\end{figure}\n\\subsubsection{Customized Local Editing.}\nThe general controls can be formulated as: given a set of $k$ reference images $\\{y_i\\}_{i=1}^k$ with corresponding masks $\\{M_i^x\\}_{i=1}^k$ for the area to apply makeup and coefficients $\\{\\alpha^S_i\\}_{i=1}^k$ to specify the shade, the fused makeup feature maps are computed by \n\\begin{equation}\n\\label{eq:free}\n\\begin{aligned}\n\\Gamma_* &= \\sum_{i=1}^k \\alpha^S_i M_i^x\\odot \\Gamma_*^{y_i} + \\left(1-\\sum_{i=1}^k \\alpha^S_i M_i^x\\right)\\odot\\Gamma_*^x\\ ,\\ *\\in\\{H,L\\}\n\\end{aligned}\n\\end{equation}\nUnlike %PSGAN \\cite{PSGAN} and \nSCGAN \\cite{SCGAN} that restrict the areas for partial transfer to skin, lip and eye shadow, our EleGANt is more interactive and flexible. In Eq. \\ref{eq:free}, $M_i^x$ can be the mask of any arbitrary area, and it has the same size as the makeup feature map. Therefore, we can specify customized regions to edit their makeup styles and shades, and the paint-board for region selection is of the same resolution as the high-res makeup feature map $\\Gamma_H$. Though the masks need to be down-sampled when applied to the low-res $\\Gamma_L$, the makeup details are dominantly determined by $\\Gamma_H$, whose high resolution guarantees the precision of the controls. Fig. \\ref{fig:paint} illustrates customized local editing of makeup style and shade. \n\n\\subsection{Ablation Study}\n\\label{sec:ablation}\n\\begin{figure}[t]\n    \\setlength{\\abovecaptionskip}{-0.1cm}\n    \\setlength{\\belowcaptionskip}{0.1cm}\n    \\centering\n    \\includegraphics[width=1.0\\textwidth]{figures/ablation_arch.pdf}\n    \\caption{Ablation study of architecture design. ``w/o Sow.'' denotes performing attention in non-overlapped windows instead of shifted overlapped windows. Unnatural colors are marked by red boxes. Detail missing and blurs are marked by green boxes.}\n    \\label{fig-abla-arch}\n\\end{figure}\n\\begin{figure}[t]\n    \\setlength{\\abovecaptionskip}{0.1cm}\n    \\setlength{\\belowcaptionskip}{-0.2cm}\n    \\centering\n    \\includegraphics[width=0.92\\textwidth]{figures/ablation_pgt2.pdf}\n    \\caption{Ablation study of PGT generation. We train the network with different PGT settings and compare the final results generated by the network. ``w/o his.'' denotes generating PGT without histogram matching. ``w/o anneal'' indicates using fixed blending factors rather than annealing during the training process.}\n    \\label{fig-abla-pgt}\n\\end{figure}\n\n\\subsubsection{Architecture Design.} \nOur EleGANt utilizes high-resolution feature maps to preserve high-frequency information as well as attention in shifted overlapped windows which reduces the complexity and avoids the boundary issue. Here we conduct an ablation study to evaluate their effectiveness. As shown in Fig. \\ref{fig-abla-arch}, without the high-res feature maps, detailed facial attributes, e.g., eye shadows of the reference (the 1st and 2nd rows) and freckles of the source (2nd row), are lost or smoothed during the transfer. When replacing Sow-attention with attention in non-overlapped windows, the outputs are discontinuous on the boundaries of the windows. Fig. \\ref{fig-abla-arch} shows that there are color blocks nearby the eyebrow (1st row) and on the forehead (2nd row). \n\n\\subsubsection{PGT Generation.} To confirm our proposed strategy for pseudo ground truth generation, we conduct ablation studies on all techniques we employ. Qualitative comparisons are demonstrated in Fig. \\ref{fig-abla-pgt}. The model will only yield images with general instead of particular makeups if we do not use PGT for supervision. Spatial information cannot be injected into PGT without TPS transformation, so the model fails to transfer makeup details (e.g., the eye shadows are missing in the first row and inaccurate in the second row.) due to the lack of corresponding supervision. Without histogram matching or annealing, artifacts introduced by TPS warping and image blending remain in the PGT, which are then learned by the model (e.g., stitching traces nearby the lip of the first row and on the forehead of the second row). Besides, the model learns to directly copy the shadows of the reference to the source face when only guided by TPS-warped images.\n\n\\section{Conclusion}\nIn this paper, we emphasize that makeup transfer is beyond transferring color distributions. \nWe propose Exquisite and locally editable GAN for makeup transfer (EleGANt) to improve the synthesis of details and step towards more flexible controls. It utilizes high-res feature maps to preserve high-frequency attributes, and a novel Sow-Attention Module performs attention within shifted overlapped to reduce the computational cost. The model is trained with a newly designed objective that leverages both color and spatial information. Besides partial and interpolated makeup transfer, it is the first to achieve customized local makeup editing within arbitrary regions. Extensive experiments demonstrate the superiority of EleGANt compared with existing approaches. It can generate realistic images with exquisite details, despite various facial poses and complex makeup styles. %The robustness and flexibility of our EleGANt perfectly meet the requirement of makeup transfer applications in practice. \nBesides, we believe that the attention scheme with shifted overlapped windowing (Sow-Attention) would be helpful for other tasks and networks. \n\nAlthough our model succeeds in daily makeup transfer, it fails in some cases such as extreme makeup. The MT dataset \\cite{BeautyGAN} we currently use has limited resolution and diversity of makeup styles and skin tones. This may be addressed if more data is available and we leave this for future work.\n\n\\bigskip\\noindent\\textbf{Acknowledgements.} This work is supported by the Ministry of Science and Technology of the People´s Republic of China, the 2030 Innovation Megaprojects ``Program on New Generation Artificial Intelligence'' (Grant No. 2021AAA0150000). This work is also supported by a grant from the Guoqiang Institute, Tsinghua University. Thanks to Steve Lin for his pre-reading and constructive suggestions.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{ScGAN: A Generative Adversarial Network to Predict Hypothetical Superconductors}\n\n\\begin{document}\n\n\\title{ScGAN: A Generative Adversarial Network to Predict Hypothetical Superconductors}\n\\date{\\today}\n\n\\author{Evan Kim}\n    \\email{evan.e.kim@gmail.com}\n\\affiliation{Tesla STEM High School, Redmond, WA 98053, USA}\n \n \\author{S.V. Dordevic}\n    \\email{dsasa@uakron.edu}\n\\affiliation{Department of Physics, The University of Akron, Akron, OH 44325, USA}\n\n\\begin{abstract}\nDespite having been discovered more than three decades ago, High Temperature Superconductors (HTSs) lack both an explanation for their mechanisms and a systematic way to search for them. To aid this search, this project proposes ScGAN, a Generative Adversarial Network (GAN) to efficiently predict new superconductors. ScGAN was trained on compounds in OQMD and then transfer learned onto the SuperCon database or a subset of it. Once trained, the GAN was used to predict superconducting candidates, and approximately 70\\% of them were determined to be superconducting by a classification model--a 23-fold increase in discovery rate compared to manual search methods. Furthermore, more than 99\\% of predictions were novel materials, demonstrating that ScGAN was able to potentially predict completely new superconductors, including several promising HTS candidates. This project presents a novel, efficient way to search for new superconductors, which may be used in technological applications or provide insight into the unsolved problem of high temperature superconductivity.\n\\end{abstract}\n\n\\maketitle\n\n\\section{Introduction}\n\\label{introduction}\n\nIn recent years, superconductors have been applied in a variety of important technologies such as power transmission lines, MRI magnets, Maglev trains, and quantum computers. Quantum computers are especially important as they are expected to solve problems that are too computationally expensive for current classical computers. However, to be used in these applications the superconductors must be cooled below their critical temperatures ($T_c$), which for most current superconductors are very low. For instance, Google's quantum computer must be maintained at $0.02 \\;\\mathrm{K}$, which severely limits its general use \\cite{Arute2019}. This points to a growing need for superconductors with higher $T_c$, \nwhich has made them an active research topic for the last couple of decades. \n\nThe mechanisms of superconductivity in most materials with relatively high $T_c$ is not fully understood, which means there is no systematic way to search for new materials or to predict their critical temperatures \\cite{HIRSCH20151}. Thus, the current procedure for finding HTSs is essentially trial--and--error, which is extremely inefficient. This was exemplified in a recent study, which found that only about $3\\%$ of the approximately $1000$ materials surveyed were superconducting \\cite{Hosono2015}. Furthermore, the study failed to find any superconductors with $T_c>60 \\; \\mathrm{K}$. This extreme inefficiency means that the likelihood of manually finding new superconductors, especially HTSs, is extremely low. \n\nTo address this difficulty, the use of computational tools in superconductivity research has become popular in recent years \\cite{Bedolla_2020}. In particular, there have been several studies utilizing machine learning to predict whether a given chemical compound will be superconducting or not. The earliest such study was done by \\citeauthor{Stanev2018} in which two random forest-based models were built: a classification model for predicting superconductivity and a regression model for predicting superconducting transition temperature \\cite{Stanev2018}. The models were successfully applied on the SuperCon database, achieving a 92\\% accuracy in classification and a $R^2=0.88$ for regression. They did run across one limitation of their machine learning model, however. When trained on a certain class of superconductors (e.g. cuprates), the model was unable to make good predictions on other classes of superconductors (e.g. pnictides).\n\nFollowing this pioneering study, there have been several other studies applying machine learning methods, such as a K-nearest neighbors algorithm (Ref.~\\cite{ROTER20201353689}) and a deep learning model (Ref.~\\cite{PhysRevB.103.014509}). The K-nearest neighbors algorithm reported improvements on the previous study from Ref.~\\cite{Stanev2018} in terms of overall performance: an $R^2$ of $0.93$ and a classification accuracy of $96.5\\%$. The deep learning model, on the other hand, showed that it might be possible to overcome the limitation that \\citeauthor{Stanev2018} faced, as they were able to make predictions about pnictide superconductors from the training data that did not contain them.\n\nThe way each of these previous studies predicted new superconducting materials was by running their trained model on a database of known existing chemical compounds and finding which compounds the model indicated could be superconducting. This procedure has several notable limitations. First, these studies miss out on the vast chemical composition space that is not already contained in existing databases. Second, commonly used databases (such as ICSD and OQMD) contain mostly stoichiometric compounds, whereas many superconductors are non-stoichiometric (cuprates and pnictides, in particular), and so many possibilities were missed in that manner as well (see for example Table 3 in Ref.~\\cite{Stanev2018} and Table 1 in Ref.~\\cite{ROTER20201353689}). Finally, the discovery of superconductors in the lab usually does not happen that way; they are discovered by synthesizing new materials and testing them, rather than checking the known ones.\n\nIn order to overcome these limitations, in this work we employ Generative Adversarial Networks (GANs) \\cite{goodfellow2014}. Generative models refer to a general class of machine learning models which are able to generate things that resemble the input dataset. They have proven to be extremely powerful, and in recent years have found numerous applications such as science, engineering, medicine, art, video games, deepfakes, etc. \\cite{DBLP:ganreview}. In most cases GANs performed better than other generative models, such as variational autoencoders, because they are able to learn more hidden rules in the input data set. For example, \\citeauthor{Dan2020} \\cite{Dan2020} reported a GAN model which generated new chemical compounds with 92.53\\% novelty and 84.5\\% validity. Another work, \\citeauthor{sym12111889} \\cite{sym12111889} applied GANs to the SuperCon dataset, but for the purpose of characterization rather than prediction. \n\nIn this work, we combine the general idea of GANs with the previous superconductor models, and propose the first GAN to predict new superconductors (ScGAN). Our ScGANs are based on chemical composition only, and are able to generate new superconducting materials with high novelty and validity. The paper is organized as follows. In Section \\ref{methodology} we present the details of the creation of the ScGANs. In section \\ref{results} the main results of the study are discussed. In particular, we present a list of hypothetical superconducting materials generated by our ScGANs, as well as their predicted critical temperatures. Finally, in Section \\ref{conclusions} we summarize the most important findings. \n\n\\section{Methodology}\n\\label{methodology}\n\nAs stated in the introduction, we chose the GAN as our generative model. Its structure is shown in Fig. \\ref{fig:ganarch}. A GAN is composed of two competing neural networks, the generator and discriminator. The generator takes in random noise and generates a ``fake'' compound, which the discriminator attempts to determine if it is real or not using existing data of real compounds. Each of them then updates their parameters based on the performance of the discriminator. The two networks improve their performance iteratively until a generator can generate realistic looking compounds, while the discriminator can detect unrealistic looking ones \\cite{goodfellow2014}.\n\n\\begin{figure*}\n        \\centering\n        \\includegraphics[scale=0.6]{ganarch.png}\n        \\caption{The architecture of the GAN model used. Y$_{1.2}$Ba$_{0.8}$CuO$_4$ and O$_{7.25}$Ca$_{0.62}$Cu$_{2.13}$Sr$_{1.91}$Y$_{0.37}$Bi$_{1.71}$ \n        are examples of real (from SuperCon) and ``fake'' superconductors, respectively. }\n        \\label{fig:ganarch}\n    \\end{figure*}\n    \n    \n    \nFig. \\ref{fig:trainflowchart} depicts the training/testing process of our GANs and it has three main stages. The main idea is to have the model first learn general chemical composition rules by training off of a larger dataset (OQMD database) of general chemical compounds and then transfer learning it onto a (much smaller) dataset of known superconducting materials (SuperCon). The idea of transfer learning is to allow a model to learn even with a limited amount of data, which is the case here as the general compounds dataset (OQMD) has on the order of $\\sim 10^6$ data points, while the superconductor datasets will only have on the order of $\\sim 10^4$ data points.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[scale=0.4]{trainflowchart_final.png}\n    \\caption{The overall training / testing process from the data to the final GAN model. It is composed of three main stages: (a) data processing, (b) training on the OQMD dataset, and finally (c) transfer learning onto the superconductor dataset (the whole SuperCon, or a part of it).}\n    \\label{fig:trainflowchart}\n\\end{figure}\n\n\\subsection{Data Collection}\n\nData were sourced from two open-source databases: SuperCon \\cite{supercon.nims.go.jp} and the Open Quantum Materials Database (OQMD) \\cite{Saal2013,Kirklin2015}. SuperCon is the largest database for superconducting materials with around $30,000$ superconductors before filtering. Similar to what was done in some previous studies \\cite{ROTER20201353689,ROTER20221354078}, \nwe only used the chemical compositions of the materials extracted from SuperCon. On the other hand, OQMD is a much larger database with around $10^6$ DFT-calculated compounds, most of which are not superconductors.\n\n\\subsection{Data Processing}\n\\label{sec:dataprocess}\n\\begin{figure}\n    \\centering\n    \\includegraphics[scale=0.28]{matrixfigure.png}\n    \\caption{Chemical composition data represented as a matrix \\cite{ROTER20201353689} in $\\mathbb{R}^{96\\times m}$, where $m$ is the number of datapoints. Each column is a single compound, with each entry representing the number of each element present in the compound. Note that that the numbers in the matrix are for illustration purposes only; they do not represent any real compounds or superconductors.}\n    \\label{fig:matrixrepresentation}\n\\end{figure}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[scale=1]{onehot_data_figure.png}\n    \\caption{An ``adjusted one-hot'' encoding of the chemical composition as a matrix in $\\mathbb R ^{96 \\times 8}$. The idea here is that the amount of each element is encoded through both the vertical location of the yellow box and the value in it, which allows the GAN to learn the chemical compositions better.}\n    \\label{fig:dataf}\n\\end{figure}\n\n\\begin{table}\n    \\centering\n    \\begin{ruledtabular}\n    \\begin{tabular}{ccc}\n        Class & Quantity & Percentage  \\\\\n        \\hline\n        Cuprates & 7,304 & 44.4\\% \\\\\n        Pnictides & 1,436 & 8.7\\% \\\\\n        Others & 7,749 & 47.0\\% \\\\\\hline\n        Everything & 16489 & 100\\%\n    \\end{tabular}\n    \\end{ruledtabular}\n    \\caption{Distribution of the superconductors in our filtered version of SuperCon by class. Each of these served as different training sets.\n    We notice that the number of pnictide entries is much smaller compared with cuprates and others. }\n    \\label{tab:class_distributions}\n\\end{table}\n\nBefore filtering the data, all compounds had to be expressed in a common format so that unwanted datapoints could be detected regardless of small differences in formatting across the databases, such as the ordering of elements. First, all datapoints were formatted as $1 \\times 96$ matrices\n\\cite{ROTER20201353689} (the $96$ is the maximum atomic number present across all the compounds in the database) and then combined so that each dataset was a matrix in $\\mathbb{R}^{96 \\times m}$, where $m$ is the number of data points (see Fig.~\\ref{fig:matrixrepresentation}). Then, both datasets were filtered for duplicates, which reduced OQMD to around $800,000$ datapoints. The SuperCon dataset required further processing as a number of entries were incomplete, and they were either corrected or removed, leaving around 16,000 datapoints. Lastly, the SuperCon dataset was split into three different groups (classes): cuprates, pnictides (iron-based) and others (anything else that is neither a curpate nor a pnictide). Table \\ref{tab:class_distributions} has the quantitative counts of these different groups. The purpose of this was to test the model's ability to learn the different classes of superconductors, especially the high temperature classes of cuprates and pnictides. \n\nOnce the data was filtered, the chemical compositions were then transformed into a form for the GAN to train on. A previous study, Ref.~\\cite{Dan2020}, used a one-hot encoding for general chemical compounds. However, that encoding was designed for stoichiometric compounds, i.e. for integer values of parameters, \nand many superconductors are non-stoichiometric, i.e. have decimal compositions. Instead, we propose the use of a ``adjusted one-hot'' encoding that works for decimals,\nin which each compound is represented by a 96 $\\times$ 8 matrix of real numbers (Fig.~\\ref{fig:dataf}). As shown in the figure, from the $1 \\times 96$ vectors in the columns of \nFig.~\\ref{fig:matrixrepresentation}, each nonzero component of that vector $\\mathbf{v}_i$ was expanded into an 8-dimensional vector with the following process. First the nearest integer $k$ between $1$ and $7$ inclusive to $\\mathbf{v}_i$ was found. Then, the matrix values were set as \n\\begin{equation}\n    A_{mi} = \\delta_{mk} \\cdot \\mathbf{v}_i/k,\n\\end{equation}\nwhere $\\delta$ here is the Kronecker delta, we index from $0$ (top left is $A_{00}$), and $m$ ranges from $0$ to $7$. For zero components ($\\mathbf{v}_i = 0$), a $1$ was simply placed at $A_{0i}$. We point out that we tested other encodings, such as the one from Ref.~\\cite{PhysRevB.103.014509}, but these were susceptible to mode collapse. The encoding proposed in this work successfully encodes decimal values both through the actual matrix entry and its location.\n\n\\subsection{Model}\n\n    \nA GAN is a type of generative model that has two competing neural networks, a generator and a discriminator, as shown in Fig.~\\ref{fig:ganarch}. Traditional GANs, however, can suffer from issues such as mode collapse and gradient vanishing, so the Wasserstein GAN with Gradient Penalty \\cite{DBLP:journals/corr/GulrajaniAADC17} was used instead. In the Wasserstein GAN with gradient penalty, the loss functions are\n\\begin{align}\n    \\mathrm{Loss}_D &= \\underset{\\boldsymbol{\\tilde{x}} \\sim \\mathbb P_g}{\\mathbb E} [D ( \\boldsymbol{\\tilde{x}})] - \\underset{\\boldsymbol{x} \\sim \\mathbb P_r}{\\mathbb E} [D(\\boldsymbol{x})]\\\\ &\\quad + \\lambda \\underset{\\boldsymbol{\\hat{x}} \\sim \\mathbb P_{\\boldsymbol{\\hat x}}}{\\mathbb E} [(\\| \\nabla_{\\boldsymbol{\\hat x}} D(\\hat{\\boldsymbol x}) \\| - 1)^2], \\nonumber \\\\\n    \\mathrm{Loss}_G &= - \\underset{\\boldsymbol{\\tilde{x}} \\sim \\mathbb P_g}{\\mathbb E} [D ( \\boldsymbol{\\tilde{x}})].\n\\end{align}\nHere $D(x)$ represents the output of the discriminator and $\\mathbb{E}$ is the expectation value (average). Then the parameters are updated with an optimizer,\n\\begin{align}\n            w &\\leftarrow w + \\alpha  \\cdot \\operatorname{Optimizer}(w,\\nabla_w \\mathrm{Loss}_D),\\\\\n            \\theta &\\leftarrow \\theta - \\alpha \\cdot \\operatorname{Optimizer}(\\theta,\\nabla_\\theta \\mathrm{Loss}_G),\n\\end{align}\nwhere $w$ are the discriminator's parameters, $\\theta$ is the generator's parameters, and $\\alpha$ is the learning rate. RMSProp was chosen as the optimizer \\cite{hintonrmsprop}, after testing out several other options, such as Adam \\cite{kingma2014adam}.\n\n\\subsubsection{Training}\nThe model was first trained on the OQMD dataset for 400 epochs. It was then transfer learned onto the SuperCon dataset or a subset of it, on which it would train for another 500 epochs. Transfer learning onto four different datasets (cuprates, pnictides, others, and everything together) resulted in four different versions of the GAN. Afterwards, the testing procedure and the data analysis were conducted, and then the hyperparameters were updated based on the results. The training curves for the final model on each of these sets are displayed in Fig.~\\ref{fig:training_curves}. Notably, they were all able to converge and stabilize over the 500 epochs.\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[scale=0.8]{training_curves.png}\n    \\caption{The generator loss against training epoch for each of the four datasets the GAN trained on: (a) All of SuperCon; (b) Others (i.e. not cuprates or pnictides); (c) Cuprates; and (d) Pnictides.}\n    \\label{fig:training_curves}\n\\end{figure*}\n\n\\subsubsection{Testing}\n\nAfter each training process, $5,000$ hypothetical compounds were generated from the versions of the GAN trained on the smaller sets and $30,000$ from the version trained on everything. The generated predictions were then inspected with various quality checks. Each compound was first tested for validity using the charge neutrality and electronegativity check features of the \\texttt{SMACT} package \\cite{DAVIES2016617}. The package tests the compound for electronegativity and charge balance to determine whether it is a valid compound or not. Each prediction was then checked for uniqueness---whether the compound showed up earlier in the generated list---and novelty---whether the compound was in the training dataset. These three checks looked at the general quality of the model. Then, more specific to superconductivity, each compound was run through the model from Ref.~\\cite{ROTER20201353689} to check whether it is a superconductor or not, as well as to \npredict its critical temperature. Of course, to be sure of superconductivity, the compounds must be synthesized and tested. Lastly, the formation energy of each generated compound was calculated using the model from Ref.~\\cite{Jha2018}, which indicates the stability of the compound. These tests will be discussed further in Section \\ref{results}, along with the actual results. \n\n\\subsection{Clustering}\n\nIn order to further assess the quality of predictions,\nclustering analysis was performed. Clustering is an unsupervised machine learning \ntechnique whose main goal is to unveil hidden patterns in the data.\nIt was recently applied to superconductors from SuperCon database \\cite{ROTER20221354078}.\nDepending on the data set, different clustering algorithms were used, such as k-means, hierarchical, Gaussian mixtures, \nself-organizing maps, etc. The results showed that in case of superconductors, clustering methods can achieve, \nand in some cases exceed, human level performance. \n\nIn order to visualize clustering results, different techniques can be used. It was shown that for \nsuperconductors the so-called t-SNE produces the best results \\cite{ROTER20221354078}.\nt-SNE is a non-linear dimensionality reduction\ntechnique which allows higher dimensional data to be represented in 2D or 3D \\cite{vanDerMaaten2008}. \nIn case of superconductors, the data points are $96$-dimensional (each compound is represented by a 1 $\\times$ 96 matrix), \nas discussed in Section \\ref{sec:dataprocess}. t-SNE reduces these dimensions down to either two or three, which allows easy visualization.  \nWe point out, however, that these reduced dimensions do not have any physical meaning. \n\n\\section{Results}\n\\label{results}\n\nAfter training, from the four different versions of the GAN, we generated four superconductor candidate lists with either 5,000 or 30,000 chemical compositions. We then ran the predicted superconductors through a series of tests to evaluate their quality. The first few were general tests, and the rest were in the context of superconductivity using existing computational models, as experimentally testing all of them is unfeasible.\n\n\\subsection{Duplicates and Novelty}\n\nWe first screened the output for duplicates within the generated sets themselves and then for duplicates between the generated set and the dataset of known superconductors that it trained on. The results are tabulated in Table~\\ref{tab:novelty}. As seen in the table, the number of repeats within the generated samples were relatively low (high uniqueness), with the exception of pnictides, which had more duplicates than the rest. This is likely due to the fact that the dataset of pnictides was significantly smaller than the rest (see Table~\\ref{tab:class_distributions}). We speculate that this overall low rate of duplicates stems from the fact that the model is able to handle decimals (see Section IIB and Fig.4), which opens up a large composition space for it to explore.\n\nThe percent of predicted compounds that were novel, i.e. not already known to exist, listed in Table~\\ref{tab:novelty} is also very high across all four GANs. These two results demonstrate that all versions of ScGAN can generate a \\textit{diverse} array of \\textit{new} compositions. \n\n\\begin{table}\n    \\centering\n    \\begin{ruledtabular}\n    \\begin{tabular}{ccc}\n        GAN Version & Novel \\% & Unique \\%\\\\\n        \\hline\n        Entirety of SuperCon & 99.69\\% & 96.78\\% \\\\\n        Cuprate  & 99.74\\% &  92.98\\%\\\\\n        Pnictides  & 99.32\\% & 58.74\\% \\\\\n        Others &  98.89\\% & 91.58 \\%\n    \\end{tabular}\n    \\end{ruledtabular}\n    \n    \\caption{The percentage of generated predictions that were novel (not in the training set) and  unique (distinct from others in the given generated set) for each of the versions of ScGAN trained on the given training datasets on the left.}\n    \\label{tab:novelty}\n\\end{table}\n\n\\subsection{Formation Energy}\n\nAs mentioned in the previous section, we found the formation energies of the predicted compounds from the GANs using ElemNet \\cite{Jha2018}.\nIt was indicated in Ref.~\\cite{Jha2018} that a negative value of formation energy is a good indicator of stability, i.e. the possibility of being \nsynthesized in the lab. In Fig.~\\ref{fig:form_energy} we display the values of formation energy for all predictions. We see from the \ndistributions of formation energies that most of predicted compounds have calculated formation energies less than zero. Even though this does not provide definitive proof of stability, it is a general indication that most of the predicted compounds are stable and \ncan be synthesized in the lab.\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[scale=0.8]{formation_energy_figures_fixedy.png}\n    \\caption{Distribution of the formation energies of the predicted compounds from the four versions of the GAN: (a) everything, (b) others, (c) cuprates, (d) pnictides.}\n    \\label{fig:form_energy}\n\\end{figure*}\n\n\\subsection{Superconductivity}\n\nAs a next test, we ran the predicted compounds through the $K$-Nearest Neighbors (KNN) classification model from Ref.~\\cite{ROTER20201353689} for predicting superconductivity based on elemental composition, in order to to check if the predictions of two machine learning models (GAN and KNN) would agree. However, the probabilistic nature of the machine learning model had to be taken into account. If $p_{sc}$ is the proportion of predictions that came up superconducting according to the model, then we can estimate $\\rho_{sc}$, the true proportion of superconducting entries, using Bayesian statistics. Denoting $\\textit{\\textsf{tp}}$ and $\\textit{\\textsf{fp}}$ as the true positive and false positive rates of the classification model, respectively, we can write\n\\begin{equation}\n    \\rho_{sc} \\cdot \\textit{\\textsf{tp}} + (1-\\rho) \\cdot \\textit{\\textsf{fp}} \\approx p_{sc}.\n\\end{equation}\nSolving for $\\rho_{sc}$ gives\n\\begin{equation}\\label{eq:estimate}\n    \\rho_{sc} \\approx \\frac{p_{sc} - \\textit{\\textsf{fp}}}{\\textit{\\textsf{tp}} - \\textit{\\textsf{fp}}}.\n\\end{equation}\nThe true positive and false positive rates here are reported from Ref.~\\cite{ROTER20201353689}:  $\\textit{\\textsf{tp}} = 98.69\\%$ and $\\textit{\\textsf{fp}} = 16.94\\%$. The output percentages along with the estimates of the true proportions calculated from equation \\ref{eq:estimate} are tabulated in Table~\\ref{tab:supercon_percentages}.\n\n\\begin{table}\n    \\centering\n    \\begin{ruledtabular}\n    \\begin{tabular}{ccc}\n    GAN Version & Output \\% & True \\% Estimate \\\\\n    \\hline\n    Entirety of SuperCon & 74.50 \\% & 70.42\\%  \\\\\n    Cuprates  & 75.76\\% & 71.95\\% \\\\\n    Pnictides & 72.44\\% & 67.89\\% \\\\\n    Others & 69.58\\% & 64.39\\% \\\\\n    \\end{tabular}\n    \\end{ruledtabular}\n    \\caption{The percentages of generated predictions that were determined by the KNN model to be superconducting for different training sets along with the estimated real percentage of the predictions that were superconducting. The true percentages were estimated according to Eq.~\\ref{eq:estimate}.}\n    \\label{tab:supercon_percentages}\n\\end{table}\nAll four GANs achieved very high percentages of superconducting material according to the KNN model, especially when compared to the 3\\% figure from the manual search in Ref.~\\cite{Hosono2015}. However, the only definite test of superconductivity can come from experimental measurements.\n\n\\subsection{Critical Temperature Estimates}\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[scale=0.8]{tc_figures_fixedy.png}\n    \\caption{Distributions of the critical temperatures of the predictions of the four different versions of ScGAN: trained on (a) everything, (b) others, (c) cuprates, (d) pnictides. }\n    \\label{fig:tc_dists}\n\\end{figure*}\n\nWe also calculated the critical temperatures of our predictions using the regression model from Ref.~\\cite{ROTER20201353689}. Similar to the superconductivity tests in the previous subsection, these calculated values can only be taken as approximations. However, the regression model can still provide us with a general understanding of the capabilities of ScGAN. The critical temperature outputs from the model are summarized in Table \\ref{tab:critical_temps} and the distributions are in shown in Fig.~\\ref{fig:tc_dists}. While the distributions are somewhat broad, the GANs were still able to find several superconductors with predicted critical temperatures higher than $100 \\, \\mathrm{K}$, which exceeds the manual search maximum of $58\\, \\mathrm{K}$ (though that search was mostly restricted to pnictides) and several of the previous machine learning approaches \\cite{ROTER20201353689,Stanev2018}. This is not surprising, as those previous searches were limited to existing databases of stoichiometric compounds, which meant that these forward design approaches could only produce a limited number of candidates, mostly with low critical temperatures.\n\n\\begin{table}[t]\n    \\centering\n    \\begin{ruledtabular}\n    \\begin{tabular}{cccc}\n    Training Data & Average $T_c$ & Standard Dev. & Max $T_c$ \\\\\n    \\hline\n    Entirety of SuperCon & $6.53 \\, \\mathrm{K}$ & $11.76\\,\\mathrm{K}$ & $123.25\\,\\mathrm K$  \\\\\n    Cuprates  & $59.34 \\, \\mathrm{K}$ & $24.78\\,\\mathrm{K}$ & $133 \\,\\mathrm K$ \\\\\n    Pnictides & $20.41 \\, \\mathrm K$ & $13.69 \\, \\mathrm K$ & $51.98 \\, \\mathrm K$ \\\\\n    Others & $72.68 \\, \\mathrm{K}$ & $21.24 \\,\\mathrm{K}$ & $116.55\\,\\mathrm K$ \\\\\n    \\end{tabular}\n    \\end{ruledtabular}\n    \\caption{Summary statistics of the predicted critical temperatures of the generated predictions that were determined by the regression model for different training sets.}\n    \\label{tab:critical_temps}\n\\end{table}\n\n\\subsection{Ability to Learn Features}\n\nWe then looked at the types of superconductors that were generated by the different versions of ScGAN. The distributions are given in Table \\ref{tab:by_class}, and we can see that each versions of ScGAN generated mostly superconductors that matched their training data. This indicates the GAN was able to detect the different underlying features behind these different major classes of superconductors.\n\n\\begin{table}\n    \\centering\n    \\begin{ruledtabular}\n    \\begin{tabular}{cccc}\n        Training Data & Cuprate \\% & Pnictide \\% & Other \\%\\\\\n        \\hline\n        Cuprate  & 92.76\\% &  0.06\\% & 7.18\\% \\\\\n        Pnictides  & 0.02 \\% & 99.84 \\% & 0.14\\% \\\\\n        Others &  0.14\\% & 0.6 \\% & 99.26\\%\n    \\end{tabular}\n    \\end{ruledtabular}\n    \n    \\caption{The distribution of the predicted superconductors across the different classes of superconductors for the different versions of the GAN.}\n    \\label{tab:by_class}\n\\end{table}\n\n\\subsection{Clustering results}\n\nIn Fig.~\\ref{fig:tsne} we display the results of \nclustering analysis on three sets of predictions: panel (a)\nfor cuprates, panel (b) for pnictides and panel (c) for \nothers. The results are visualized with the help of t-SNE.\nAs pointed out above, the two t-SNE dimensions, Y$_1$ and Y$_2$, do not have any physical\nmeaning. Full circles of different colors represent different families of superconductors\nfrom SuperCon database, whereas purple open circles represent GAN \npredictions. As can be seen from all three panels, GANs\nwere able to generate new superconductors from all known families \nof cuprates, pnictides and other superconductors. However,\nGANs did not predict any new {\\it families} of \nsuperconductors. \n\n\\begin{figure*}\n    \\centering\n\\vspace{-2cm}\n\\includegraphics[scale=0.5]{clustering.png}\n\\vspace{-2cm}\n    \\caption{Clustering of the predicted compounds \nfrom various versions of the GAN: (a) cuprates, (b) pnictides and (c) others.\nFull circles represent the data points from SuperCon and purple open circles are GAN\npredictions.}\n    \\label{fig:tsne}\n\\end{figure*}\n\n\\subsection{Promising Candidates}\n\nAfter running through the $T_c$ prediction model, we manually identified the ones that looked the most promising, including some with very high critical temperatures. It turns out that most of these were cuprates, which is not surprising, considering that superconductors with highest critical temperatures in the training set are cuprates. We then checked the Crystallography Open Database (COD) \\cite{Vaitkus2021} to see if those compounds were in the database. The ones listed in Table~\\ref{tab:predictions} could not be found neither in COD nor in SuperCon, showing that this model overcomes the limitations of the previous forward design models and finds completely new superconductors. A more comprehensive list of predictions is available upon reasonable request.\n\n\\begin{table}\n    \\centering\n    \\begin{ruledtabular}\n    \\begin{tabular}{lcc}\n        Compound & Predicted $T_c$ & Class \\\\ \\hline\n$\\mathrm{PrCaBiSr_{2}Cu_{2}O_{7.46}}$ & $104.6  \\, \\mathrm K$ & Cuprates \\\\\n$\\mathrm{YTiSr_{2}Cu_{2.74}O_{6.76}}$ & $91.7  \\, \\mathrm K$ & Cuprates \\\\\n$\\mathrm{TeYSr_{2}Cu_{2}O_{7.75}}$ & $89.8  \\, \\mathrm K$ & Cuprates \\\\\n$\\mathrm{TlCaSr_{2}Cu_{2}O_{7.82}}$ & $73.9  \\, \\mathrm K$ & Cuprates \\\\\n$\\mathrm{YCaBa_{2}ZnCu_{2.36}O_{7.54}}$ & $71.5  \\, \\mathrm K$ & Cuprates \\\\\n$\\mathrm{HgCsSrCa_{2}Cu_{2.56}O_{8.66}}$ & $69.8  \\, \\mathrm K$ & Cuprates \\\\\n$\\mathrm{GdCaRuSr_{1.83}Cu_{2}O_{8.71}}$ & $40.8  \\, \\mathrm K$ & Cuprates \\\\\n$\\mathrm{C_{2.52}Ni_{0.92}Y_{0.71}Th}$ & $85.3  \\, \\mathrm K$ & Others \\\\\n$\\mathrm{Si_{0.62}V_{0.91}Zr_{0.83}}$ & $84.7  \\, \\mathrm K$ & Others \\\\\n$\\mathrm{Al_{2.34}Te_{0.64}Ir_{1.07}}$ & $84.7  \\, \\mathrm K$ & Others \\\\\n$\\mathrm{Be_{0.16}Si_{1.09}V_{2.67}Y_{1.72}}$ & $62.4  \\, \\mathrm K$ & Others \\\\\n$\\mathrm{Cu_{1.13}Nb_{3.0}Sb_{0.72}Ir_{1.05}}$ & $59.4  \\, \\mathrm K$ & Others \\\\\n$\\mathrm{Ga_{0.62}Nb_{2.88}Sn_{0.65}Te_{0.79}}$ & $40.8  \\, \\mathrm K$ & Others \\\\\n$\\mathrm{B_{1.73}C_{1.03}Ni_{1.12}Y_{0.66}Pt_{0.64}}$ & $40.8  \\, \\mathrm K$ & Others \\\\\n$\\mathrm{RuTeSeFe}$ & $35.6  \\, \\mathrm K$ & Pnictides \\\\\n$\\mathrm{TeSSeFe_{1.05}}$ & $31.  0\\, \\mathrm K$ & Pnictides \\\\\n$\\mathrm{CeCoAs_{2.15}Fe_{1.39}}$ & $23.3  \\, \\mathrm K$ & Pnictides \\\\\n$\\mathrm{CeThPAsFe_{1.59}}$ & $12.2  \\, \\mathrm K$ & Pnictides \\\\\n$\\mathrm{GaPrCa_{2.58}As_{12.44}Fe_{6.34}}$ & $11.9  \\, \\mathrm K$ & Pnictides \\\\\n$\\mathrm{NdOAsFe}$ & $4.5  \\, \\mathrm K$ & Pnictides \\\\\n\n        \n\n    \\end{tabular}\n    \\end{ruledtabular}\n    \\caption{Promising superconductor candidates generated by ScGANs, that do not exist in current databases. Also shown are their predicted critical temperatures \\cite{ROTER20201353689}.}\n    \\label{tab:predictions}\n\\end{table}\n\n\\section{Conclusion}\n\\label{conclusions}\n\nFor decades the search for new superconductors has relied on the serendipity of material scientists to synthesize a new material with superconducting proprieties. This paper introduced a novel method to search for superconductors---discovering candidates with a generative adversarial network. In contrast to previous computational methods which attempted to predict new superconductors off of existing datasets, this model predicted compounds directly. This ``inverse design'' approach proved to be far more powerful than manual search methods and previous computational methods, with the model being able to generate thousands of candidates with a wide-range of critical temperatures that lied outside of existing databases (both superconductor and general inorganic compound databases). Even while only training on chemical compositions, more than $70\\%$ of the GANs predictions were cross-checked with a separate model to be potentially superconducting (the only way to know for sure, however, would be to synthesize and check these compounds in a lab). Of these, several were promising HTS candidates listed in Table \\ref{tab:predictions}. We point out that previous models would have been unable to find such candidates as they were outside of current databases.\n\nWhile the compounds generated were new, our clustering showed that the GAN did not generate any new families of superconductors. However, it was still able to generate non-stoichiometric compounds, widening the scope of the computational search. Future studies should look into some improvements that can be made, such as being able to account for charge neutrality and crystal structure in the compound encodings. Chemical checks on the predicted compounds (using SMACT as detailed earlier) revealed that while there was electronegativity balance, charge balance was not always exact for the superconductor candidates. Furthermore, crystal structure has been known to play a significant role in superconductivity, so including it in calculations would most likely result in improvements. However, this may prove to be a difficult endeavor as crystal structure is not well-documented in existing databases. Active transfer learning could also be attempted to narrow down the predictions to high temperature superconductors only \\cite{Kim2021}. However, even without these possible improvements, the model at its current version is still very promising and can be applied to search for superconductors, starting with the candidates identified in this paper.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-1909.06956v2.tex",
        "arXiv-2207.09840v1.tex",
        "arXiv-2209.03444v1.tex"
    ],
    "group_id": "group_83",
    "response": "### Summary of Makeup Transfer and Superconductor Prediction Using Generative Adversarial Networks (GANs)\n\n#### Title: Makeup Transfer and Superconductor Prediction Using GANs: A Comparative Analysis\n\n#### Introduction\n\nMakeup transfer and superconductor prediction are two distinct but equally challenging tasks within the broader field of computer vision and materials science, respectively. Makeup transfer aims to transfer the makeup style from a reference image to a source image, preserving the identity of the source while applying makeup from the reference. This task has gained significant attention due to its applications in portrait beautifying and makeup try-on services. Superconductor prediction, on the other hand, seeks to identify new superconducting materials with high critical temperatures ($T_c$), which are crucial for technological advancements in various fields such as power transmission and quantum computing. While the search for superconductors has traditionally relied on empirical methods and manual synthesis, recent advancements in machine learning have introduced computational tools to aid in this search, including the use of Generative Adversarial Networks (GANs).\n\nIn the field of makeup transfer, early methods focused on transferring color distributions across predefined regions of the face, such as the skin, lips, and eyes. However, these methods often failed to handle large pose and expression differences between source and reference images, leading to misalignment and artifacts. Additionally, they lacked the capability to perform customizable transfer, such as adjusting the shade of makeup or specifying the part to transfer. Similarly, in the domain of superconductor prediction, traditional methods have been limited by the need for extensive databases and the inability to explore non-stoichiometric compounds, which are often overlooked in existing databases.\n\nCurrent progress in makeup transfer has seen the development of GAN-based methods that address some of these limitations, such as BeautyGAN and PairedCycleGAN, which use face parsing maps and facial landmarks to guide the transfer. These methods have improved the realism of transferred images but still struggle with pose and expression robustness. In superconductor prediction, machine learning models have been employed to predict the likelihood of a compound being superconducting and its critical temperature based on its chemical composition. However, these models often suffer from mode collapse and are limited by the stoichiometric nature of the compounds in existing databases.\n\nThis summary will focus on three recent research papers that explore the use of GANs for makeup transfer and superconductor prediction. The first paper, \"PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable Makeup Transfer,\" proposes a novel framework for makeup transfer that is robust to pose and expression differences and supports customizable transfer. The second paper, \"EleGANt: Exquisite and Locally Editable GAN for Makeup Transfer,\" introduces an approach that emphasizes the transfer of makeup details and supports flexible controls. The third paper, \"ScGAN: A Generative Adversarial Network to Predict Hypothetical Superconductors,\" presents a GAN-based model for predicting new superconductors, capable of generating non-stoichiometric compounds and identifying promising HTS candidates.\n\n#### Main Content of Each Paper\n\n**PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable Makeup Transfer**\n\nThe PSGAN paper addresses the limitations of existing makeup transfer methods by proposing a framework that is robust to pose and expression differences and supports customizable transfer. The framework consists of three main components: Makeup Distill Network (MDNet), Attentive Makeup Morphing (AMM) module, and Makeup Apply Network (MANet). MDNet disentangles the makeup style from the reference image into two spatial-aware makeup matrices, $\\gamma$ and $\\beta$, which serve as shifting and scaling parameters. The AMM module then morphs these matrices to adapt to the source image by calculating an attentive matrix $A$ that specifies how each pixel in the source image is morphed from the reference image. This is achieved by considering both the visual features and relative positions of pixels, ensuring that makeup is transferred between semantically similar regions. MANet applies the morphed makeup matrices to the source image feature map to generate the final transferred image.\n\nThe authors introduce a new dataset, Makeup-Wild, containing facial images with various poses and expressions to test the robustness of their method. They demonstrate that PSGAN outperforms existing methods in terms of pose and expression robustness, detail preservation, and customizable transfer. The method is also shown to work effectively on facial videos, generating consistent results across frames.\n\n**EleGANt: Exquisite and Locally Editable GAN for Makeup Transfer**\n\nThe EleGANt paper aims to improve the synthesis of makeup details and achieve more flexible controls in makeup transfer. The proposed model utilizes a pyramid structure with high-resolution feature maps to preserve high-frequency information about makeup details. It employs a QKV-attention mechanism to extract makeup features from the reference and adapt them to the source face, ensuring that makeup is transferred accurately despite pose and expression differences. The authors introduce a novel Sow-Attention Module that computes attention within shifted overlapped windows, reducing computational cost and ensuring output continuity.\n\nEleGANt supports partial and interpolated makeup transfer, allowing users to mix makeup styles from different references and adjust the shade of makeup. The model is also capable of customized local editing within arbitrary regions, which is a significant advancement over previous methods. The authors conduct extensive experiments to demonstrate the effectiveness of their approach, showing that EleGANt generates realistic images with exquisite details and outperforms existing methods in terms of detail preservation and controllability.\n\n**ScGAN: A Generative Adversarial Network to Predict Hypothetical Superconductors**\n\nThe ScGAN paper presents a novel GAN-based model for predicting new superconductors. The model is trained on a large dataset of general chemical compounds (OQMD) and then transfer learned onto a smaller dataset of known superconductors (SuperCon). This approach allows the model to generate a diverse array of new chemical compositions, including non-stoichiometric compounds, which are often overlooked in existing databases. The predictions are then evaluated for validity, uniqueness, novelty, superconductivity, and critical temperature using various computational models and databases.\n\nThe authors demonstrate that ScGAN can generate thousands of potential superconductors with a wide range of critical temperatures, many of which are outside the scope of existing databases. They also show that the model can learn the underlying features of different classes of superconductors, generating new candidates from all known families of cuprates, pnictides, and other superconductors. While the model does not generate new families of superconductors, it widens the scope of the computational search by including non-stoichiometric compounds.\n\n#### Commonalities and Innovations\n\nAll three papers utilize GANs to address specific challenges within their respective fields. PSGAN and EleGANt both aim to improve the robustness and detail preservation of makeup transfer, while ScGAN uses GANs for the forward design of superconductors. The common theme is the use of advanced techniques, such as attention mechanisms and pyramid structures, to enhance the capabilities of GANs.\n\n**PSGAN Innovations:**\n- **Makeup Distill Network (MDNet):** Disentangles makeup style into two spatial-aware matrices, $\\gamma$ and $\\beta$.\n- **Attentive Makeup Morphing (AMM) Module:** Morphs makeup matrices to adapt to the source image by considering both visual features and relative positions.\n- **Makeup Apply Network (MANet):** Applies the morphed makeup matrices to the source image feature map to generate the final result.\n\n**EleGANt Innovations:**\n- **Pyramidal Feature Maps:** Preserves high-frequency information about makeup details.\n- **Sow-Attention Module:** Computes attention within shifted overlapped windows to reduce computational cost and ensure output continuity.\n- **Customized Local Editing:** Supports flexible control over arbitrary regions, enabling users to edit specific areas of the face.\n\n**ScGAN Innovations:**\n- **Adjusted One-Hot Encoding:** Encodes non-stoichiometric compounds, allowing the model to explore a broader chemical composition space.\n- **Transfer Learning:** Trains on a large dataset of general compounds and then transfer learns onto a smaller dataset of known superconductors.\n- **Formation Energy Calculation:** Uses ElemNet to predict the stability of generated compounds, indicating their potential for synthesis in the lab.\n\n#### Comparison of Results\n\nBoth PSGAN and EleGANt demonstrate significant improvements over existing methods in terms of pose and expression robustness, detail preservation, and controllability. PSGAN achieves state-of-the-art results on both frontal facial images and images with various poses and expressions, while EleGANt generates images with exquisite details and supports flexible local editing. The user study conducted by EleGANt shows that it outperforms existing methods in all aspects, especially in detail processing.\n\nScGAN, on the other hand, shows promise in the field of superconductor prediction by generating thousands of potential superconductors with a wide range of critical temperatures. The model is able to predict non-stoichiometric compounds, which are often overlooked in existing databases. However, the clustering results indicate that while the model can generate new compounds, it does not predict new families of superconductors. This suggests that further improvements are needed to fully leverage the potential of GANs in superconductor prediction.\n\n#### Conclusion\n\nThe three papers highlight the versatility and potential of GANs in addressing complex tasks within their respective fields. PSGAN and EleGANt both significantly improve the robustness and detail preservation of makeup transfer, while EleGANt introduces a novel Sow-Attention Module that reduces computational cost and supports flexible local editing. ScGAN demonstrates the potential of GANs in predicting new superconductors, capable of generating non-stoichiometric compounds and identifying promising HTS candidates. However, the model's limitations in predicting new families of superconductors indicate the need for further research.\n\nFuture work in makeup transfer could focus on improving the realism of transferred images in extreme cases, such as extreme makeup styles and complex backgrounds. In superconductor prediction, future research could explore the inclusion of crystal structure and charge neutrality in compound encodings to further refine the predictions. Additionally, active transfer learning could be employed to narrow down the predictions to high temperature superconductors only, enhancing the practical applicability of the model.\n\nThe use of advanced techniques, such as attention mechanisms and pyramid structures, in GANs has shown significant promise in both makeup transfer and superconductor prediction. These innovations not only improve the quality of generated images but also broaden the scope of the computational search, making GANs a valuable tool in both fields."
}