{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Graph Contrastive Learning with Augmentations}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n\nGeneralizable, transferrable, and robust representation learning on graph-structured data remains a challenge for current graph neural networks (GNNs).  \nUnlike what has been developed for convolutional neural networks (CNNs) for image data, self-supervised learning and pre-training are\nless explored for GNNs.\nIn this paper, we propose a graph contrastive learning (GraphCL) framework\nfor learning unsupervised \nrepresentations of graph data.\nWe first design four types of graph augmentations to incorporate various priors. We then systematically \nstudy the impact of\nvarious combinations of graph augmentations on multiple datasets, in four different settings: semi-supervised, unsupervised, and transfer learning as well as adversarial attacks. The results show that, even without tuning augmentation extents nor using sophisticated GNN architectures, our GraphCL framework can produce graph representations of similar or better generalizability, transferrability, and robustness compared to state-of-the-art methods. We also investigate the impact of parameterized graph augmentation extents and patterns, and observe further performance gains in preliminary experiments. \nOur codes are available at: \\url{https://github.com/Shen-Lab/GraphCL}.\n\\end{abstract}\n\n\\renewcommand{\\thefootnote}{\\fnsymbol{footnote}}\n\\footnotetext[1]{Equal contribution.}\n\\renewcommand{\\thefootnote}{\\arabic{footnote}}\n\n\\vspace{-1em}\n\\section{Introduction} \\label{introduction}\nGraph neural networks (GNNs) \\cite{kipf2016semi,velivckovic2017graph,xu2018powerful}, following a neighborhood aggregation scheme, are increasingly popular for graph-structured data. Numerous variants of GNNs have been proposed to  achieve state-of-the-art performances in graph-based tasks, such as node or link classification \\cite{kipf2016semi,velivckovic2017graph,you2020l2,liu2020towards,zou2019layer}, link prediction \\cite{zhang2018link} and graph classification \\cite{ying2018hierarchical,xu2018powerful}. Intriguingly, in most scenarios of graph-level tasks, GNNs are trained end-to-end under supervision.  For GNNs,  there is little exploration (except \\cite{hu2019pre}) of (self-supervised) pre-training, a technique commonly used as a regularizer in training deep architectures that suffer from gradient vanishing/explosion \\cite{erhan2009difficulty,glorot2010understanding}. The reasons behind the intriguing phenomena could be that most studied graph datasets, as shown in \\cite{dwivedi2020benchmarking}, are often limited in size and GNNs often have shallow architectures to avoid over-smoothing \\cite{li2018deeper} or ``information loss'' \\cite{oono2019graph}.  \n\nWe however argue for the necessity of exploring GNN pre-training schemes.  Task-specific labels can be extremely scarce for graph datasets (e.g. in biology and chemistry labeling through wet-lab experiments is often resource- and time-intensive) \\cite{zitnik2018prioritizing,hu2019pre}, and pre-training can be a promising technique to mitigate the issue, as it does in convolutional neural networks (CNNs) \\cite{goyal2019scaling,kolesnikov2019revisiting,chen2020simple}.  As to the conjectured reasons for the lack of GNN pre-training: first, real-world graph data can be huge and even benchmark datasets are recently getting larger \\cite{dwivedi2020benchmarking,hu2020obg}; second, even for shallow models, pre-training could initialize parameters in a ``better\" attraction basin  around a local minimum associated with better generalization \\cite{glorot2010understanding}. \nTherefore, we emphasize the significance of GNN pre-training.\n\nCompared to CNNs for images, there are unique challenges of designing GNN pre-training schemes for graph-structured data.  Unlike geometric information in images, rich structured information of various contexts exist in graph data \\cite{velivckovic2018deep,sun2019infograph} %\\red{(not sure these are still proper references for the whole sentence)} \\blue{(I think this two paper enforce more on structure information in graph?)}, \nas graphs are abstracted representations of raw data with diverse nature (e.g. molecules made of chemically-bonded atoms and networks of socially-interacting people).  It is thus difficult to design a GNN pre-training scheme generically beneficial to down-stream tasks. A na\\\"ive GNN pre-training scheme for graph-level tasks is to reconstruct the vertex adjacency information (e.g. GAE \\cite{kipf2016variational} and GraphSAGE \\cite{hamilton2017inductive} in network embedding).  This scheme can be very limited (as seen in \\cite{velivckovic2018deep} and our Sec. \\ref{sec:exp_sota}) because it over-emphasizes proximity that is not always beneficial \\cite{velivckovic2018deep}, and could hurt structural information \\cite{ribeiro2017struc2vec}. Therefore, a well designed pre-training framework is needed to capture highly heterogeneous information in graph-structured data.% \\red{(this sentence needs revision again)}.\n\nRecently, in visual representation learning, contrastive learning has renewed a surge of interest \\cite{wu2018unsupervised,ye2019unsupervised,ji2019invariant,chen2020simple,he2020momentum}. Self-supervision with handcrafted pretext tasks \\cite{noroozi2016unsupervised,carlucci2019domain,trinh2019selfie,chen2020adversarial} relies on heuristics to design, and thus could limit the generality of the learned representations. In comparison, contrastive learning aims to learn representations by maximizing feature consistency under differently augmented views, that exploit data- or task-specific augmentations \\cite{herzig2019learning}, to inject the desired feature invariance. \nIf extended to pre-training GCNs, this framework can potentially  overcome the aforementioned limitations of proximity-based pre-training methods \\cite{kipf2016variational,hamilton2017inductive,you2020does,jin2020self,zhu2020self,zhang2020graph,hu2020gpt,liu2020self}. However, it is not straightforward to be directly applied outside visual representation learning and demands significant extensions to graph representation learning, leading to our innovations below.   \n\n\\textbf{Contributions.}\nIn this paper, we have developed contrastive learning with augmentations for GNN pre-training to address the challenge of data heterogeneity in graphs.\n(i) Since data augmentations are the prerequisite for constrastive learning but are under-explored in graph-data \\cite{verma2019graphmix}, we first design four types of graph data augmentations, each of which imposes certain prior over graph data and parameterized for the extent and pattern.\n(ii) Utilizing them to obtain correlated views, we propose a novel graph contrastive learning framework (GraphCL) for GNN pre-training,\nso that representations invariant to specialized perturbations can be learned for diverse graph-structured data.\nMoreover, we show that GraphCL actually performs mutual information maximization, and the connection is drawn between GraphCL and recently proposed contrastive learning methods that we demonstrate that GraphCL can be rewritten as \\textit{a general framework} unifying a broad family of contrastive learning methods on graph-structured data.\n(iii) Systematic study is performed to assess the performance of contrasting different augmentations on various types of datasets, revealing the rationale of the performances and providing the guidance to adopt the framework for specific datasets.\n(iv) Experiments show that GraphCL achieves state-of-the-art performance in the settings of semi-supervised learning, unsupervised representation learning and transfer learning. It additionally boosts robustness against common adversarial attacks. \n\n\\section{Related Work} \\label{related_work}\n\\textbf{Graph neural networks.}\nIn recent years, graph neural networks (GNNs) \\cite{kipf2016semi,velivckovic2017graph,xu2018powerful} have emerged as a promising approach for analyzing graph-structured data. \nThey follow an iterative neighborhood aggregation (or message passing) scheme to capture the structural information within nodes' neighborhood.\nLet $\\mathcal{G} = \\{ \\mathcal{V}, \\mathcal{E} \\}$ denote an undirected graph,\nwith $\\boldsymbol{X} \\in \\mathbb{R}^{|\\mathcal{V}| \\times N}$ as the feature matrix\nwhere $\\boldsymbol{x}_n = \\boldsymbol{X}[n, :]^T$ is the $N$-dimensional attribute vector of the node $v_n \\in \\mathcal{V}$.\nConsidering a $K$-layer GNN $f(\\cdot)$, the propagation of the $k$th layer is represented as:\n\\begin{equation}\n    \\boldsymbol{a}_n^{(k)} = \\mathrm{AGGREGATION}^{(k)}(\\{ \\boldsymbol{h}_{n'}^{(k-1)}: n' \\in \\mathcal{N}(n) \\}), \\:\n    \\boldsymbol{h}_n^{(k)} = \\mathrm{COMBINE}^{(k)}(\\boldsymbol{h}_{n}^{(k-1)}, \\boldsymbol{a}_n^{(k)}),\n\\end{equation}\nwhere $\\boldsymbol{h}_n^{(k)}$ is the embedding of the vertex $v_n$ at the $k$th layer with $\\boldsymbol{h}_n^{(0)} = \\boldsymbol{x}_n$,\n$\\mathcal{N}(n)$ is a set of vertices adjacent to $v_n$,\nand $\\mathrm{AGGREGATION}^{(k)}(\\cdot)$ and $\\mathrm{COMBINE}^{(k)}(\\cdot)$ are component functions of the GNN layer.\nAfter the $K$-layer propagation, the output embedding for $\\mathcal{G}$ is summarized on layer embeddings through the READOUT function. Then a multi-layer perceptron (MLP) is adopted for the graph-level downstream task (classification or regression):\n\\begin{equation} \\label{eq:gnn}\n    f(\\mathcal{G}) = \\mathrm{READOUT}(\\{ \\boldsymbol{h}_{n}^{(k)}: v_n \\in \\mathcal{V}, k \\in K \\}), \\: \\boldsymbol{z}_\\mathcal{G} = \\mathrm{MLP}(f(\\mathcal{G})).\n\\end{equation}\n\nVarious GNNs have been proposed \\cite{kipf2016semi,velivckovic2017graph,xu2018powerful}, achieving state-of-the-art performance in graph tasks.\n\n\\textbf{Graph data augmentation.}\nAugmentation for graph-structured data still remains under-explored,\nwith some work along these lines but requiring prohibitive additional computation cost \\cite{verma2019graphmix}.\nTraditional self-training methods \\cite{verma2019graphmix,li2018deeper} utilize the trained model to annotate unlabelled data;\n\\cite{ding2018semi} proposes to train a generator-classifier network in the adversarial learning setting to generate fake nodes; and\n\\cite{deng2019batch,feng2019graph}\ngenerate adversarial perturbations to node feature over the graph structure.\n\n\\textbf{Pre-training GNNs.}\nAlthough (self-supervised) pre-training is a common and effective scheme for convolutional neural networks (CNNs) \\cite{goyal2019scaling,kolesnikov2019revisiting,chen2020simple},\nit is rarely explored for GNNs.\nOne exception \\cite{hu2019pre} is restricted to studying pre-training strategies in the transfer learning setting, \nWe argue that a pre-trained GNN is not easy to transfer, due to the diverse fields that graph-structured data source from.\nDuring transfer, substantial domain knowledge is required for both pre-training and downstream tasks, otherwise it might lead to negative transfer \\cite{hu2019pre,rosenstein2005transfer}.\n\n\\textbf{Contrastive learning.}\nThe main idea of contrastive learning is to make representations agree with each other under proper transformations, raising a recent surge of interest in visual representation learning \\cite{becker1992self,wu2018unsupervised,ye2019unsupervised,ji2019invariant,chen2020simple}.\nOn a parallel note, for graph data, \ntraditional methods trying to reconstruct the adjacency information of vertices \\cite{kipf2016variational,hamilton2017inductive} can be treated as a kind of ``local contrast'',\nwhile over-emphasizing the proximity information at the expense of the structural information \\cite{ribeiro2017struc2vec}.\nMotivated by \\cite{belghazi2018mine,hjelm2018learning},\n\\cite{ribeiro2017struc2vec,sun2019infograph,peng2020self} propose to perform contrastive learning between local and global representations to better capture structure information.\nHowever, graph contrastive learning has not been explored from the perspective of enforcing perturbation invariance as \\cite{ji2019invariant,chen2020simple} have done.\n\n\\section{Methodology}\n\\subsection{Data Augmentation for Graphs}\n\\label{sec:graph_data_augmentation}\nData augmentation aims at creating novel and realistically rational data through applying certain transformation without affecting the semantics label. It still remains  under-explored for graphs except some with expensive computation cost (see Sec. \\ref{related_work}).\nWe focus on graph-level augmentations.  Given a graph $\\mathcal{G} \\in \\{ \\mathcal{G}_m: m \\in M \\}$\nin the dataset of $M$ graphs,\nwe formulate the augmented graph $\\hat{\\mathcal{G}}$ satisfying: $\\hat{\\mathcal{G}} \\sim q(\\hat{\\mathcal{G}} | \\mathcal{G})$, \nwhere $q(\\cdot | \\mathcal{G})$ is the augmentation distribution conditioned on the original graph,\nwhich is pre-defined, representing the human prior for data distribution. For instance for image classification, the applications of rotation and cropping encode the prior that people will acquire the same classification-based semantic knowledge from the rotated image or its local patches \\cite{xie2019unsupervised,berthelot2019mixmatch}. \n\nWhen it comes to graphs, the same spirit could be followed. However, one challenge as stated in Sec. \\ref{introduction} is that graph datasets are abstracted from diverse fields \nand therefore there may not be universally appropriate data augmentation as those for  images.\nIn other words, for different categories of graph datasets some data augmentations might be more desired than others. \nWe mainly focus on three categories:\nbiochemical molecules (e.g. chemical compounds, proteins) \\cite{hu2019pre}, social networks \\cite{kipf2016semi} and image super-pixel graphs \\cite{dwivedi2020benchmarking}. Next, we propose four general data augmentations for graph-structured data and discuss the intuitive \npriors that they introduce.\n\\begin{table}[ht] \\vspace{-1em}\n\\scriptsize\n \\caption{Overview of data augmentations for graphs.\n }\n \\label{tab:data_augmentation}\n \\centering\n \\begin{tabular}{c | c | c} \n  \\hline\n  \\hline\n  \\textbf{Data augmentation} & \\textbf{Type} & \\textbf{Underlying Prior} \\\\\n  \\hline\n  \\hline\n  Node dropping & Nodes, edges & Vertex missing does not alter semantics. \\\\\n  \\hline\n  Edge perturbation & Edges & Semantic robustness against connectivity variations. \\\\\n  \\hline\n  Attribute masking & Nodes & Semantic robustness against losing partial attributes. \\\\\n  \\hline\n  Subgraph & Nodes, edges & Local structure can hint the full semantics. \\\\\n  \\hline\n  \\hline\n \\end{tabular} % \\vspace{-1em}\n\\end{table}\n\n\\textbf{Node dropping.}\nGiven the graph $\\mathcal{G}$, node dropping will randomly discard certain portion of vertices along with their connections.\nThe underlying prior enforced by it is that missing part of vertices does not affect the semantic meaning of $\\mathcal{G}$. Each node's dropping probability follows a default i.i.d. uniform distribution (or any other distribution).  \n\n\\textbf{Edge perturbation.}\nIt will perturb the connectivities in $\\mathcal{G}$ through randomly adding or dropping certain ratio of edges.\nIt implies that the semantic meaning of $\\mathcal{G}$ has certain robustness to the edge connectivity pattern variances. We also follow an i.i.d. uniform distribution to add/drop each edge.\n\n\\textbf{Attribute masking.}\nAttribute masking prompts models to recover masked vertex attributes using their context information, i.e., the remaining attributes. The underlying assumption is that missing partial vertex attributes does not affect the model predictions much.  \n\n\\textbf{Subgraph.} This one samples a subgraph from $\\mathcal{G}$ using  random walk (the algorithm is summarized in Appendix A). It assumes that the semantics of $\\mathcal{G}$ can be much preserved in its (partial) local structure. \n\nThe default augmentation (dropping, perturbation, masking and subgraph) ratio is set at 0.2. % for all graph data augmentations. Later on we will also present ablation studies on them.\n\n\\begin{figure}[t] \n    \\centering \n    \\includegraphics[width=0.85\\linewidth]{CLGCN_new.pdf}\n    \\caption{\\small{A framework of graph contrastive learning. Two graph  augmentations $q_i(\\cdot | \\mathcal{G})$ and $q_j(\\cdot | \\mathcal{G})$ are sampled from an augmentation pool $\\mathcal{T}$ and applied to input graph $\\mathcal{G}$. A shared GNN-based encoder $f(\\cdot)$ and a projection head $g(\\cdot)$ are trained to maximize the agreement between representations $z_i$ and $z_j$ via a contrastive loss.}} \n    \\label{fig:graphcl} \n    \\vspace{-0.5em}\n\\end{figure}\n\n\\subsection{Graph Contrastive Learning}\nMotivated by recent contrastive learning developments in visual representation learning (see Sec. \\ref{related_work}),\nwe propose a graph contrastive learning framework (GraphCL) for (self-supervised) pre-training of GNNs.\nIn graph contrastive learning, pre-training is performed through maximizing the agreement between two augmented views of the same graph via a contrastive loss in the latent space as shown in Fig. \\ref{fig:graphcl}.\nThe framework consists of the following four major components:\n\n(1) \\textbf{Graph data augmentation.}\nThe given graph $\\mathcal{G}$ undergoes graph data augmentations to obtain two correlated views $\\hat{\\mathcal{G}}_i, \\hat{\\mathcal{G}}_j$, as a positive pair,\nwhere $\\hat{\\mathcal{G}}_i \\sim q_i(\\cdot | \\mathcal{G}), \\hat{\\mathcal{G}}_j \\sim q_j(\\cdot | \\mathcal{G})$ respectively.\nFor different domains of graph datasets, how to strategically select data augmentations matters (Sec. \\ref{sec:aug}).\n\n(2) \\textbf{GNN-based encoder.}\nA GNN-based encoder $f(\\cdot)$ (defined in \\eqref{eq:gnn}) extracts graph-level representation vectors $\\boldsymbol{h}_i, \\boldsymbol{h}_j$ for augmented graphs $\\hat{\\mathcal{G}}_i, \\hat{\\mathcal{G}}_j$.\nGraph contrastive learning does not apply any constraint on the GNN architecture.\n\n(3) \\textbf{Projection head.}\nA non-linear transformation $g(\\cdot)$ named projection head maps augmented representations to another latent space where the contrastive loss is calculated, as advocated in \\cite{chen2020simple}.\nIn graph contrastive learning, a two-layer perceptron (MLP) is applied to obtain $\\boldsymbol{z}_i, \\boldsymbol{z}_j$.\n\n(4) \\textbf{Contrastive loss function.}\nA contrastive loss function $\\mathcal{L}(\\cdot)$ is defined to enforce maximizing the consistency between positive pairs $\\boldsymbol{z}_i, \\boldsymbol{z}_j$ compared with negative pairs.\nHere we utilize the normalized temperature-scaled cross entropy loss (NT-Xent) \\cite{sohn2016improved,wu2018unsupervised,oord2018representation}.\n\nDuring GNN pre-training, a minibatch of $N$ graphs are randomly sampled and processed through contrastive learning, resulting in $2N$ augmented graphs and corresponding contrastive loss to optimize,\nwhere we re-annotate $z_i, z_j$ as $z_{n,i}, z_{n,j}$ for the $n$th graph in the minibatch.\nNegative pairs are not explicitly sampled but generated from the other $N-1$ augmented graphs within the same minibatch as in \\cite{chen2017sampling,chen2020simple}.\nDenoting the cosine similarity function as $\\mathrm{sim}(\\boldsymbol{z}_{n,i}, \\boldsymbol{z}_{n,j}) = \\boldsymbol{z}_{n,i}^\\mathsf{T} \\boldsymbol{z}_{n,j} / \\lVert \\boldsymbol{z}_{n,i} \\rVert \\lVert \\boldsymbol{z}_{n,j} \\rVert$, NT-Xent for the $n$th graph is defined as:\n\\begin{equation} \\label{eq:cl_loss}\n    \\ell_{n} = -\\mathrm{log}\\frac{\\mathrm{exp}(\\mathrm{sim}(\\boldsymbol{z}_{n,i}, \\boldsymbol{z}_{n,j}) / \\tau)}{\\sum_{n'=1, n' \\neq n}^N \\mathrm{exp}(\\mathrm{sim}(\\boldsymbol{z}_{n,i}, \\boldsymbol{z}_{n',j}) / \\tau)},\n\\end{equation}\nwhere $\\tau$ denotes the temperature parameter.\nThe final loss is computed across all positive pairs in the minibatch.\nThe proposed graph contrastive learning is summarized in Appendix A.  % \\ref{alg:graphcl}.\n\n\\textbf{Discussion.}\nWe first show that GraphCL can be viewed as one way of mutual information maximization between the latent representations of two kinds of augmented graphs. The full derivation is in Appendix F, with the loss form rewritten as below:\n\\begin{equation} \\label{eq:general_formulation}\n    \\ell = \\mathbb{E}_{\\mathbb{P}_{\\hat{\\mathcal{G}}_i}} \\{ - \\mathbb{E}_{\\mathbb{P}_{(\\hat{\\mathcal{G}}_j | \\hat{\\mathcal{G}}_i)}} T(f_1(\\hat{\\mathcal{G}}_i), f_2(\\hat{\\mathcal{G}}_j)) +  \\mathrm{log} (\\mathbb{E}_{\\mathbb{P}_{\\hat{\\mathcal{G}}_j}} e^{T(f_1(\\hat{\\mathcal{G}}_i), f_2(\\hat{\\mathcal{G}}_j))}) \\}.\n\\end{equation}\nThe above loss essentially maximizes a lower bound of the mutual information between $\\boldsymbol{h}_i = f_1(\\hat{\\mathcal{G}}_i), \\boldsymbol{h}_j = f_2(\\hat{\\mathcal{G}}_j)$ that the compositions of $(f_1, \\hat{\\mathcal{G}}_i), (f_2, \\hat{\\mathcal{G}}_j)$ determine our desired views of graphs.\nFurthermore, we draw the connection between GraphCL and recently proposed contrastive learning methods that we demonstrate that GraphCL can be rewrited as a general framework unifying a broad family of contrastive learning methods on graph-structured data, through reinterpreting \\eqref{eq:general_formulation}.\nIn our implementation, we choose $f_1 = f_2$ and generate $\\hat{\\mathcal{G}}_i, \\hat{\\mathcal{G}}_j$ through data augmentation, while with various choices of the compositions result in \\eqref{eq:general_formulation} instantiating as other specific contrastive learning algorithms including \\cite{velickovic2019deep,ren2019heterogeneous,park2020unsupervised,sun2019infograph,peng2020graph,hassani2020contrastive,qiu2020gcc} also shown in in Appendix F.\n\n\\section{The Role of Data Augmentation in Graph Contrastive Learning}\\label{sec:aug}\n\n\\begin{wraptable}{r}{80mm}\n\\vspace{-1em}\n\\small\n \\caption{\\small{Datasets statistics.}}\n \\label{tab:statistics}\n \\centering\n \\resizebox{0.58\\textwidth}{!}{\n \\begin{tabular}{c | c | c | c | c } \n  \\hline\n  \\hline\n  Datasets & Category & Graph Num. & Avg. Node & Avg. Degree \\\\\n  \\hline\n  \\hline\n  NCI1 & Biochemical Molecules & 4110 & 29.87 & 1.08 \\\\\n  PROTEINS & Biochemical Molecules & 1113 & 39.06 & 1.86 \\\\\n  \\hline\n  COLLAB & Social Networks & 5000 & 74.49 & 32.99 \\\\\n  RDT-B & Social Networks & 2000 & 429.63 & 1.15 \\\\\n  \\hline\n  \\hline\n \\end{tabular}}% \\vspace{-1em}\n\\end{wraptable}\n\nIn this section, we assess and rationalize the role of data augmentation for graph-structured data in our GraphCL framework.\nVarious pairs of augmentation types are applied, as illustrated in Fig. \\ref{fig:augvsaug}, to three categories of graph datasets (Table \\ref{tab:statistics}, and we leave the discussion on superpixel graphs in Appendix C).  Experiments are performed in the semi-supervised setting, following the pre-training \\& finetuning approach \\cite{chen2020simple}.  \nDetailed settings are in Appendix B.\n\n\\subsection{Data Augmentations are Crucial. Composing Augmentations Benefits.}\nWe first examine whether and when applying (different) data augmentations helps graph contrastive learning in general.  We summarize the results in Fig.~\\ref{fig:augvsaug} using the accuracy gain compared to training from scratch (no pre-training).  And we list the following \\textbf{Obs}ervations.\n\n\\begin{figure}[t] \n    \\centering \n    \\includegraphics[width=1\\linewidth]{aug2.pdf}\n    \\caption{\\small{Semi-supervised learning accuracy gain (\\%) when contrasting different augmentation pairs, compared to training from scratch, under four datasets: NCI1, PROTEINS, COLLAB, and RDT-B. Pairing ``Identical\" stands for a no-augmentation baseline for contrastive learning, where the positive pair diminishes and the negative pair consists of two non-augmented graphs. \n    Warmer colors indicate better performance gains.\n    The baseline training-from-scratch accuracies are 60.72\\%, 70.40\\%, 57.46\\%, 86.63\\% for the four datasets respectively. } %\\textcolor{red}{I don't understand what contrasting two ``identical\" means.} \\blue{(just simply contrast between two unaugmented data, intuitively do nothing but push all data point away from each other)}\n    } \n    \\vspace{-1em}\n    \\label{fig:augvsaug} \n\\end{figure}\n\n\\textbf{Obs. 1. Data augmentations are crucial in graph contrastive learning.}\nWithout any data augmentation graph contrastive learning  is not helpful and often worse compared with training from scratch, judging from the accuracy losses in the upper right corners of Fig. \\ref{fig:augvsaug}.    \nIn contrast, composing an original graph and its appropriate augmentation can benefit the downstream performance.  Judging from the top rows or the right-most columns in Fig. \\ref{fig:augvsaug}, graph contrastive learning with single best augmentations achieved considerable improvement without exhaustive hyper-parameter tuning:  1.62\\% for NCI1, 3.15\\% for PROTEINS, 6.27\\% for COLLAB, and 1.66\\% for RDT-B.  \n\nThe observation meets our intuition. Without augmentation,\ngraphCL simply compares two original samples as a negative pair (with the positive pair loss becoming zero), leading to homogeneously pushes all graph representations away from each other,\nwhich is non-intuitive to justify.\nImportantly, when appropriate augmentations are applied, the corresponding priors on the data distribution are instilled, enforcing the model to learn representations invariant to the desired perturbations through maximizing the agreement between a graph and its augmentation.  \n\n\\begin{figure}[!htb] \n    \\centering \n    \\includegraphics[width=1\\linewidth]{obs1.pdf}\n    \\caption{\\small{Contrastive loss curves for different augmentation pairs.\n    In the two figures of the left attribute masking is contrasted with other augmentations and that of the right for edge perturbation, where contrasting the same augmentations always leads to the fastest loss descent.}}\n    \\vspace{-1em}\n    \\label{fig:cl_loss} \n\\end{figure}\n\n\\textbf{Obs. 2. Composing different augmentations benefits more.} \nComposing augmentation pairs of a graph rather than the graph and its augmentation further improves the performance: the maximum accuracy gain was 2.10\\% for NCI1, 3.15\\% for PROTEINS, 7.11\\% for COLLAB, and 1.85\\% for RDT-B.  Interestingly, applying augmentation pairs of the same type  (see the diagonals of Fig. \\ref{fig:augvsaug}) does not usually lead to the best performance (except for node dropping),  \ncompared with augmentation pairs of different types (off-diagonals).  \nSimilar observations were made in visual representation learning  \\cite{chen2020simple}. % that no single augmentation suffices to learn good representations.  \nAs conjectured in \\cite{chen2020simple}, composing different augmentations avoids the learned features trivially overfitting low-level ``shortcuts'', making features more generalizable.\n\nHere we make a similar conjecture that contrasting isogenous graph pairs augmented in different types presents a harder albeit more useful task for graph representation learning.\nWe thus plot the contrastive loss curves composing various augmentations (except subgraph) together with attribute masking or edge perturbation  for NCI1 and PROTEINS. Fig. \\ref{fig:cl_loss} \nshows that, with augmentation pairs of different types, the contrastive loss always descents slower than it does with pairs of the same type, when the optimization procedure remains the same. This result indicates that composing augmentation pairs of different types does correspond to a ``harder\" contrastive prediction task.\nWe will explore in Sec.~\\ref{sec:over_simple} how to quantify a ``harder\" task in some cases and whether it always helps. %the model avoid learning the shortcut information and benefit the learned representations. \\red{(Yang come back to revise again after completing Sec. 4)} \n\n\\subsection{The Types, the Extent, and the Patterns of Effective Graph Augmentations}\nWe then note that the (most) beneficial combinations of augmentation types can be dataset-specific, which matches our intuition as  graph-structured data are of highly heterogeneous nature (see Sec. \\ref{introduction}). We summarize our observations and derive insights below.  And we further analyze the impact of the extent and/or the pattern of given types of graph augmentations.   \n\n\\textbf{Obs. 3. Edge perturbation benefits social networks but hurts some  biochemical molecules.}\nEdge perturbation as one of the paired augmentations improves  the performances for social-network data COLLAB and ROT-B as well as biomolecule data PROTEINS, but hurts the other biomolecule data NCI1. %For instance, the combination of identical transformation and edge perturbation achieves 3.10\\%, 0.85\\%, and 1.01\\% improvement for COLLAB, RDT-B, and PROTEINS, respectively, while doing 2.43\\% worse for NCI1 (Fig.~\\ref{fig:augvsaug}). This observation highlights the art of graph augmentations.  \nWe hypothesize that, compared to the case of social networks, the ``semantemes'' of some biomolecule data are more sensitive to individual edges.   Specifically, a single-edge change in NCI1  corresponds to a removal or addition of a covalent bond, which can drastically change the identity and even the validity of a compound, let alone its property for the down-stream semantemes.  \nIn contrast the semantemes of social networks are more tolerant to individual edge perturbation \\cite{dai2018adversarial,zugner2018adversarial}. Therefore, for chemical compounds, edge perturbation demonstrates a prior that is conceptually incompatible with the domain knowledge and empirically unhelpful for down-stream performance.\n\nWe further examine whether the extent or strength of edge perturbation can affect the conclusion above.  We evaluate the downstream performances on representative examples NCI1 and COLLAB.  And we use the combination of the original graph (``identical'') and edge perturbation of various ratios in our GraphCL framework. Fig. \\ref{fig:ratiovsperformance}A shows that edge perturbation worsens the NCI1 performances regardless of augmentation strength, confirming that our earlier conclusion was insensitive to the extent of edge perturbation.  Fig. \\ref{fig:ratiovsperformance}B suggests that edge perturbation could improve the COLLAB performances more with increasing augmentation strength.  \n\n\\textbf{Obs. 4. Applying attribute masking achieves better performance in denser graphs.} For the social network datasets, composing the identical graph and attribute masking achieves 5.12\\% improvement for COLLAB (with higher average degree) while only 0.17\\% for RDT-B.\nSimilar observations are made for the denser PROTEINS versus NCI1.  To assess the impact of augmentation strength on this observation, we perform similar experiments on RDT-B and COLLAB, by composing the identical graph and its attributes masked to various extents.  Fig.~\\ref{fig:ratiovsperformance}C and D show that, masking less for the very sparse RDT-B does not help, although masking more for the very dense COLLAB does.  \n\n\\begin{figure}[!htb] \n    \\centering \n    \\includegraphics[width=1\\linewidth]{obs2.pdf}\n    \\caption{\\small{Performance versus augmentation strength. Left two figures implemented edge perturbation with different ratios. The right two figures apply attribute masking with different masking ratios.}}\n    \\vspace{-0.5em}\n    \\label{fig:ratiovsperformance} \n\\end{figure} % \\vspace{-1em}\n\nWe further hypothesize that masking patterns also matter, and masking more hub nodes with high degrees benefit denser graphs, because GNNs cannot reconstruct the missing information of isolated nodes, \naccording to the message passing mechanism \\cite{gilmer2017neural}.  To test the hypothesis, we perform an experiment to mask nodes with more connections with higher probability on denser graphs PROTEINS and COLLAB.  Specifically, we adopt a masking distribution $\\mathrm{deg}_n^\\alpha$ rather than the uniform distribution, where $\\mathrm{deg}_n$ is the degree of vertex $v_n$ and $\\alpha$ is the control factor. A positive $\\alpha$ indicates more masking for high-degree nodes.  Fig. \\ref{fig:controlratiovsperformance}C and D \nshowing that, for very dense COLLAB, there is an apparent upward tendency on performance if masking nodes with more connections.  \n\n\\textbf{Obs. 5. Node dropping and subgraph are generally beneficial across datasets.}\nNode dropping and subgraph, especially the latter, seem to be generally beneficial in our studied datasets.\nFor node dropping, the prior that missing certain vertices (e.g. some  hydrogen atoms in chemical compounds or edge users for social networks) does not alter the semantic information is emphasized,\nintuitively fitting for our cognition.  \nFor subgraph, previous works \\cite{velivckovic2018deep,sun2019infograph} show that enforcing local (the subgraphs we extract) and global information consistency is helpful for representation learning, which explains the observation. Even for chemical compounds in NCI1, subgraphs can represent structural and functional ``motifs'' important for the down-stream semantemes.   \n\nWe similarly examined the impact of node dropping patterns by adopting the non-uniform distribution as mentioned in changing attribute-masking patterns.  Fig.~\\ref{fig:controlratiovsperformance}B shows that, for the dense social-network COLLAB graphs, more GraphCL improvements were observed while dropping hub nodes more in the range considered.  Fig.~\\ref{fig:controlratiovsperformance}A shows that, for the not-so-dense PROTEINS graphs, changing the node-dropping distribution away from uniform does not necessarily help.  \n\n\\begin{figure}[ht] \n    \\centering \n    \\includegraphics[width=1\\linewidth]{obs3.pdf}\n    \\caption{\\small{Performance versus augmentation patterns. Node dropping and  attribute masking are performed with various control factors (negative to positive: dropping/masking more low-degree vertices to high-degree ones).}}\n    \\label{fig:controlratiovsperformance} \n\\end{figure} % \\vspace{-0.5em}\n\n\\vspace{-1em}\n\\subsection{Unlike ``Harder'' Ones, Overly Simple Contrastive Tasks Do Not Help.}\n\\label{sec:over_simple}\nAs discussed in Obs. 2, ``harder'' contrastive learning might benefit more, where the ``harder'' task is achieved by composing augmentations of different types.  \nIn this section we further explore quantifiable difficulty in relationship to parameterized augmentation strengths/patterns and assess the impact of the difficulty on performance improvement.  \n\nIntuitively, larger  dropping/masking ratios or control factor $\\alpha$ leads to harder contrastive tasks, which did result in better COLLAB performances (Fig.~\\ref{fig:ratiovsperformance} and \\ref{fig:controlratiovsperformance}) in the range considered.  Very small ratios or negative $\\alpha$, corresponding to overly simple tasks, We also design subgraph variants of increasing difficulty levels and reach similar conclusions. More details are in Appendix D.\n\n\\textbf{Summary.} In total, we decide the augmentation pools for Section 5 as: node dropping and subgraph for biochemical molecules; all for dense social networks; and all except attribute masking for sparse social networks. Strengths or patterns are default even though varying them could help more.  \n\n\\section{Comparison with the State-of-the-art Methods}\n\\label{sec:exp_sota}\nIn this section, we compare our proposed (self-supervised) pre-training framework, GraphCL, with state-of-the-art methods (SOTAs) in the settings of semi-supervised, unsupervised \\cite{sun2019infograph} and transfer learning \\cite{hu2019pre} on graph classification (for node classification experiments please refer to Appendix G).\nDataset statistics and training details for the specific settings are in Appendix E.\n\n\\textbf{Semi-supervised learning.}\nWe first evaluate our proposed framework in the semi-supervised learning setting on graph classification \\cite{chen2019powerful,xu2018powerful} on the benchmark TUDataset \\cite{Morris+2020}.\nSince pre-training \\& finetuning in semi-supervised learning for the graph-level task is unexplored before, we take two conventional network embedding methods as pre-training tasks for comparison:\nadjacency information reconstruction (we refer to GAE \\cite{kipf2016variational} for implementation) and local \\& global representation consistency enforcement (refer to Infomax \\cite{sun2019infograph} for implementation).\nBesides, the performance of training from scratch and that with augmentation (without contrasting) is also reported.\nWe adopt graph convolutional network (GCN) with the default setting in \\cite{chen2019powerful} as the GNN-based encoder which achieves comparable SOTA performance in the fully-supervised setting.\nTable \\ref{tab:semi-supervised} shows that GraphCL outperforms  traditional pre-training schemes.\n\n\\begin{table}[!htb] \\vspace{-1em}\n\\scriptsize\n\\begin{center}\n\\caption{\\small{Semi-supervised learning with pre-training \\& finetuning.\n\\darkred{Red} numbers indicate the best performance and the number that overlap with the standard deviation of the best performance (comparable ones). 1\\% or 10\\% is label rate; baseline and Aug. represents training from scratch without and with augmentations, respectively.\n}}\n\\label{tab:semi-supervised}\n\\resizebox{1\\textwidth}{!}{\n\\begin{tabular}{c | c  c  c | c  c  c c | c  c }\n    \\hline\n    \\hline\n    Dataset & NCI1 & PROTEINS & DD & COLLAB & RDT-B & RDT-M5K & GITHUB & MNIST & CIFAR10 \\\\\n    \\hline\n    \\hline\n    1\\% baseline & 60.72$\\pm$0.45 & - & - & 57.46$\\pm$0.25 & - & - & 54.25$\\pm$0.22 & 60.39$\\pm$1.95 & 27.36$\\pm$0.75 \\\\\n    1\\% Aug. & 60.49$\\pm$0.46 & - & - & 58.40$\\pm$0.97 & - & - & 56.36$\\pm$0.42 & 67.43$\\pm$0.36 & 27.39$\\pm$0.44 \\\\\n    1\\% GAE & 61.63$\\pm$0.84 & - & - & 63.20$\\pm$0.67 & - & - & \\darkred{59.44}$\\pm$0.44 & 57.58$\\pm$2.07 & 21.09$\\pm$0.53 \\\\\n    1\\% Infomax & \\darkred{62.72}$\\pm$0.65 & - & - & 61.70$\\pm$0.77 & - & - & 58.99$\\pm$0.50  & 63.24$\\pm$0.78 & 27.86$\\pm$0.43\\\\\n    \\hdashline\n    1\\% GraphCL & \\darkred{62.55}$\\pm$0.86 & - & - & \\darkred{64.57}$\\pm$1.15 & - & - & 58.56$\\pm$0.59  & \\darkred{83.41}$\\pm$0.33 & \\darkred{30.01}$\\pm$0.84 \\\\\n    \\hline\n    10\\% baseline & 73.72$\\pm$0.24 & 70.40$\\pm$1.54 & 73.56$\\pm$0.41 & 73.71$\\pm$0.27 & 86.63$\\pm$0.27 & 51.33$\\pm$0.44 & 60.87$\\pm$0.17 & 79.71$\\pm$0.65 & 35.78$\\pm$0.81 \\\\\n    10\\% Aug. & 73.59$\\pm$0.32 & 70.29$\\pm$0.64 & 74.30$\\pm$0.81 & 74.19$\\pm$0.13 & 87.74$\\pm$0.39 & 52.01$\\pm$0.20 & 60.91$\\pm$0.32 & 83.99$\\pm$2.19 & 34.24$\\pm$2.62 \\\\\n    10\\% GAE & 74.36$\\pm$0.24 & 70.51$\\pm$0.17 & 74.54$\\pm$0.68 & \\darkred{75.09}$\\pm$0.19 & 87.69$\\pm$0.40 & \\darkred{53.58}$\\pm$0.13 & 63.89$\\pm$0.52 & 86.67$\\pm$0.93 & 36.35$\\pm$1.04 \\\\\n    10\\% Infomax & \\darkred{74.86}$\\pm$0.26 & 72.27$\\pm$0.40 & \\darkred{75.78}$\\pm$0.34 & 73.76$\\pm$0.29 & 88.66$\\pm$0.95 & \\darkred{53.61}$\\pm$0.31 & \\darkred{65.21}$\\pm$0.88 & 83.34$\\pm$0.24 & 41.07$\\pm$0.48 \\\\\n    \\hdashline\n    10\\% GraphCL & \\darkred{74.63}$\\pm$0.25 & \\darkred{74.17}$\\pm$0.34 & \\darkred{76.17}$\\pm$1.37 & 74.23$\\pm$0.21 & \\darkred{89.11}$\\pm$0.19 & 52.55$\\pm$0.45 & \\darkred{65.81}$\\pm$0.79 & \\darkred{93.11}$\\pm$0.17 & \\darkred{43.87}$\\pm$0.77 \\\\\n    \\hline\n    \\hline\n\\end{tabular}}\n\\end{center}\n\\end{table}\n\n\\textbf{Unsupervised representation learning.}\nFurthermore, GraphCL is evaluated in the unsupervised representation learning following \\cite{narayanan2017graph2vec,sun2019infograph}, where unsupervised methods generate graph embeddings that are fed into a down-stream SVM classifier \\cite{sun2019infograph}. \nAside from SOTA graph kernel methods that graphlet kernel (GL), Weisfeiler-Lehman sub-tree kernel (WL) and deep graph kernel (DGK), we also compare with four unsupervised graph-level representation learning methods as node2vec \\cite{grover2016node2vec}, sub2vec \\cite{adhikari2018sub2vec}, graph2vec \\cite{narayanan2017graph2vec} and InfoGraph \\cite{sun2019infograph}.\nWe adopt graph isomorphism network (GIN) with the default setting in \\cite{sun2019infograph} as the GNN-based encoder which is SOTA in representation learning.\nTable \\ref{tab:unsupervised} shows GraphCL outperforms in most cases except on datasets with small graph size (e.g. MUTAG and IMDB-B consists of graphs with average node number less than 20).\n\n\\begin{table}[ht] \\vspace{-1em}\n\\scriptsize\n\\begin{center}\n\\caption{\\small{Comparing classification accuracy on top of graph representations learned from graph kernels, SOTA representation learning methods, and GIN pre-trained with GraphCL. The compared numbers are from the corresponding papers under the same experiment setting.}}\n\\label{tab:unsupervised}\n\\resizebox{1\\textwidth}{!}{\n\\begin{tabular}{c | c  c c  c | c  c  c c }\n    \\hline\n    \\hline\n    Dataset & NCI1 & PROTEINS & DD & MUTAG & COLLAB & RDT-B & RDT-M5K & IMDB-B \\\\\n    \\hline\n    \\hline\n    GL & - & - & - & 81.66$\\pm$2.11 & - & 77.34$\\pm$0.18 & 41.01$\\pm$0.17 & 65.87$\\pm$0.98 \\\\\n    WL & \\darkred{80.01}$\\pm$0.50 & 72.92$\\pm$0.56 & - & 80.72$\\pm$3.00 & - & 68.82$\\pm$0.41 & 46.06$\\pm$0.21 & \\darkred{72.30}$\\pm$3.44 \\\\\n    DGK & \\darkred{80.31}$\\pm$0.46 & 73.30$\\pm$0.82 & - & 87.44$\\pm$2.72 & - & 78.04$\\pm$0.39 & 41.27$\\pm$0.18 & 66.96$\\pm$0.56 \\\\\n    \\hline\n    node2vec & 54.89$\\pm$1.61 & 57.49$\\pm$3.57 & - & 72.63$\\pm$10.20 & - & - & - & - \\\\\n    sub2vec & 52.84$\\pm$1.47 & 53.03$\\pm$5.55 & - & 61.05$\\pm$15.80 & - & 71.48$\\pm$0.41 & 36.68$\\pm$0.42 & 55.26$\\pm$1.54 \\\\\n    graph2vec & 73.22$\\pm$1.81 & 73.30$\\pm$2.05 & - & 83.15$\\pm$9.25 & - & 75.78$\\pm$1.03 & 47.86$\\pm$0.26 & 71.10$\\pm$0.54 \\\\\n    InfoGraph & 76.20$\\pm$1.06 & \\darkred{74.44}$\\pm$0.31 & 72.85$\\pm$1.78 & \\darkred{89.01}$\\pm$1.13 & \\darkred{70.65}$\\pm$1.13 & 82.50$\\pm$1.42 & 53.46$\\pm$1.03 & \\darkred{73.03}$\\pm$0.87 \\\\\n    \\hline\n    GraphCL & 77.87$\\pm$0.41 & \\darkred{74.39}$\\pm$0.45 & \\darkred{78.62}$\\pm$0.40 & 86.80$\\pm$1.34 & \\darkred{71.36}$\\pm$1.15 & \\darkred{89.53}$\\pm$0.84 & \\darkred{55.99}$\\pm$0.28 & 71.14$\\pm$0.44 \\\\\n    \\hline\n    \\hline\n\\end{tabular}}\n\\end{center}\n\\end{table}\n\n\\textbf{Transfer learning.}\nLastly, experiments are performed on transfer learning on molecular property prediction in chemistry and protein function prediction in biology following \\cite{hu2019pre},\nwhich pre-trains and finetunes the model in different datasets to evaluate the transferability of the pre-training scheme.\nWe adopt GIN with the default setting in \\cite{hu2019pre} as the GNN-based encoder which is SOTA in transfer learning.\nExperiments are performed for 10 times with mean and standard deviation of ROC-AUC scores (\\%) reported as \\cite{hu2019pre}.\nAlthough there is no universally beneficial pre-training scheme especially for the out-of-distribution scenario in transfer learning (Sec. \\ref{introduction}),\nTable \\ref{tab:transfer} shows that GraphCL still achieves SOTA performance on 5 of 9 datasets compared to the previous best schemes.\n\n\\begin{table}[ht] \\vspace{-1em}\n\\scriptsize\n\\begin{center}\n\\caption{\\small{Transfer learning comparison with different manually designed pre-training schemes, where the compared numbers are from \\cite{hu2019pre}.}\n}\n\\label{tab:transfer}\n\\resizebox{1\\textwidth}{!}{\n\\begin{tabular}{c | c  c c  c c  c  c c | c }\n    \\hline\n    \\hline\n    Dataset & BBBP & Tox21 & ToxCast & SIDER & ClinTox & MUV & HIV & BACE & PPI \\\\\n    \\hline\n    \\hline\n    No Pre-Train & 65.8$\\pm$4.5 & 74.0$\\pm$0.8 & 63.4$\\pm$0.6 & 57.3$\\pm$1.6 & 58.0$\\pm$4.4 & 71.8$\\pm$2.5 & 75.3$\\pm$1.9 & 70.1$\\pm$5.4 & 64.8$\\pm$1.0 \\\\\n    \\hline\n    Infomax & 68.8$\\pm$0.8 & 75.3$\\pm$0.5 & 62.7$\\pm$0.4 & 58.4$\\pm$0.8 & 69.9$\\pm$3.0 & \\darkred{75.3}$\\pm$2.5 & 76.0$\\pm$0.7 & 75.9$\\pm$1.6 & 64.1$\\pm$1.5 \\\\\n    EdgePred & 67.3$\\pm$2.4 & 76.0$\\pm$0.6 & \\darkred{64.1}$\\pm$0.6 & 60.4$\\pm$0.7 & 64.1$\\pm$3.7 & 74.1$\\pm$2.1 & 76.3$\\pm$1.0 & \\darkred{79.9}$\\pm$0.9 & 65.7$\\pm$1.3 \\\\\n    AttrMasking & 64.3$\\pm$2.8 & \\darkred{76.7}$\\pm$0.4 & \\darkred{64.2}$\\pm$0.5 & \\darkred{61.0}$\\pm$0.7 & 71.8$\\pm$4.1 & \\darkred{74.7}$\\pm$1.4 & 77.2$\\pm$1.1 & \\darkred{79.3}$\\pm$1.6 & 65.2$\\pm$1.6 \\\\\n    ContextPred & 68.0$\\pm$2.0 & 75.7$\\pm$0.7 & \\darkred{63.9}$\\pm$0.6 & \\darkred{60.9}$\\pm$0.6 & 65.9$\\pm$3.8 & \\darkred{75.8}$\\pm$1.7 & \\darkred{77.3}$\\pm$1.0 & \\darkred{79.6}$\\pm$1.2 & 64.4$\\pm$1.3 \\\\\n    \\hline\n    GraphCL & \\darkred{69.68}$\\pm$0.67 & 73.87$\\pm$0.66 & 62.40$\\pm$0.57 & \\darkred{60.53}$\\pm$0.88 & \\darkred{75.99}$\\pm$2.65 & 69.80$\\pm$2.66 & \\darkred{78.47}$\\pm$1.22 & 75.38$\\pm$1.44 & \\darkred{67.88}$\\pm$0.85 \\\\\n    \\hline\n    \\hline\n\\end{tabular}}\n\\end{center}\n\\end{table}\n\n\\textbf{Adversarial robustness.}\nIn addition to generalizability, we claim that GNNs also gain robustness using GraphCL.\nThe experiments are performed on synthetic data to classify the component number in graphs, facing the RandSampling, GradArgmax and RL-S2V attacks following the default setting in \\cite{dai2018adversarial}.\nStructure2vec \\cite{dai2016discriminative} is adopted as the GNN-based encoder as in \\cite{dai2018adversarial}.\nTable \\ref{tab:adv_graph} shows that GraphCL boosts GNN robustness compared with training from scratch, under three evasion attacks.\n\n\\begin{table}[!htb] \\vspace{-1em}\n\\scriptsize\n\\begin{center}\n\\caption{\\small{Adversarial performance under three adversarial attacks for GNN with different depth (following the protocol in \\cite{dai2018adversarial}). \\darkred{Red} numbers indicate the best performance.}}\n\\label{tab:adv_graph}\n\\begin{tabular}{c | c  c | c  c | c  c}\n    \\hline\n    \\hline\n     & \\multicolumn{2}{c|}{Two-Layer} & \\multicolumn{2}{c|}{Three-Layer} & \\multicolumn{2}{c}{Four-Layer} \\\\\n    \\cline{2-7}\n    Methods & No Pre-Train & GraphCL & No Pre-Train & GraphCL & No Pre-Train & GraphCL \\\\\n    \\hline\n    \\hline\n    Unattack & 93.20 & \\darkred{94.73} & 98.20 & \\darkred{98.33} & 98.87 & \\darkred{99.00} \\\\\n    \\hline\n    RandSampling & 78.73 & \\darkred{80.68} & 92.27 & \\darkred{92.60} & 95.13 & \\darkred{97.40} \\\\\n    GradArgmax & \\darkred{69.47} & 69.26 & 64.60 & \\darkred{89.33} & 95.80 & \\darkred{97.00} \\\\\n    RL-S2V & \\darkred{42.93} & 42.20 & 41.93 & \\darkred{61.66} & 70.20 & \\darkred{84.86} \\\\\n    \\hline\n    \\hline\n\\end{tabular}\n\\end{center}\n\\end{table} \\vspace{-0.5em}\n\n\\section{Conclusion}\nIn this paper, we perform explicit study to explore contrastive learning for GNN pre-training, facing the unique challenges in graph-structured data.\nFirstly, several graph data augmentations are proposed  with the discussion of each of which on introducing certain human prior of data distribution.\nAlong with new augmentations, we propose a novel graph contrastive learning framework (GraphCL) for GNN pre-training to facilitate invariant representation learning along with rigorous theoretical analysis.  \nWe systematically assess and analyze the influence of data augmentations in our proposed framework, revealing the rationale and guiding the choice of augmentations.\nExperiment results verify the state-of-the-art performance of our proposed framework in both generalizability and robustness.\n\n \n\n\\section*{Broader Impact}\n\nEmpowering deep learning for reasoning and predicting over graph-structured data  is of broad interests and wide applications, such as recommendation systems, neural architecture search, and drug discovery.  \nThe proposed graph contrastive learning framework with augmentations contributes a general framework that can potentially benefit the effectiveness and  efficiency of graph neural networks through model pre-training.  The numerical results and analyses would also inspire the design of proper augmentations toward positive knowledge transfer on downstream tasks.  \n\n{\\small\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Adversarial Graph Augmentation \nto Improve \\\\ Graph Contrastive Learning}\n\n\\begin{document}\n\n\\maketitle\n\\vspace{-5mm}\n\\begin{abstract}\n\\vspace{-2mm}\nSelf-supervised learning of graph neural networks (GNN) is in great need because of the widespread label scarcity issue in real-world graph/network data.\nGraph contrastive learning (GCL), by training GNNs to maximize the correspondence between the representations of the same graph in its different augmented forms, may yield robust and transferable GNNs even without using labels. However, GNNs trained by traditional GCL often risk capturing redundant graph features and thus may be brittle and provide sub-par performance in downstream tasks. \nHere, we propose a novel principle, termed adversarial-GCL (\\textit{AD-GCL}), which enables GNNs to avoid capturing redundant information during the training by optimizing adversarial graph augmentation strategies used in GCL. \nWe pair AD-GCL with theoretical explanations and design a practical instantiation based on trainable edge-dropping graph augmentation.\nWe experimentally validate AD-GCL\\footnote{\\url{https://github.com/susheels/adgcl}} by comparing with the state-of-the-art GCL methods and achieve performance gains of up-to~14\\% in unsupervised, ~6\\% in transfer, and~3\\% in semi-supervised learning settings overall with 18 different benchmark datasets for the tasks of molecule property regression and classification, and social network classification.\n\\end{abstract}\n\n\\vspace{-3mm}\n\\section{Introduction}\n\\vspace{-2mm}\n\\label{sec:intro}\nGraph representation learning (GRL) aims to encode graph-structured data into low-dimensional vector representations, which has recently shown great potential in many applications in biochemistry, physics and social science~\\cite{senior2020improved,shlomi2020graph,hamilton2020book}. Graph neural networks (GNNs), inheriting the power of neural networks~\\cite{hornik1989multilayer,cybenko1989approximation}, have become the almost {\\em de facto} encoders for GRL~\\cite{scarselli2008graph, chami2020machine, zhang2020deep_survey, hamilton2017representation}. GNNs have been mostly studied in  cases with supervised end-to-end training~\\cite{kipf2016semi,dai2016discriminative,velickovic2018graph,zhang2018end,xu2018powerful,morris2019weisfeiler,li2020distance}, where a large number of task-specific labels are needed. However, in many applications, annotating labels of graph data takes a lot of time and resources~\\cite{hu2019strategies,sun2019infograph}, e.g., identifying pharmacological effect of drug molecule graphs requires living animal experiments~\\cite{vogel2002drug}. Therefore, recent research efforts are directed towards studying self-supervised learning for GNNs, where only limited or even no labels are needed~\\cite{kipf2016variational,grover2019graphite,gmi,dgi,sun2019infograph,you2020graph, hassani2020contrastive, xie2021self, liu2021graph,zhang2020motif, thakoor2021bootstrapped,zhu2020graph,qiu2020gcc}. %For this, two major principles are typically adopted.\n\nDesigning proper self-supervised-learning principles for GNNs is crucial, as they drive what information of graph-structured data will be captured by  GNNs and may heavily impact their performance in downstream tasks. Many previous works adopt the edge-reconstruction principle to match traditional network-embedding requirement~\\cite{belkin2003laplacian,perozzi2014deepwalk,grover2016node2vec,ribeiro2017struc2vec}, where the edges of the input graph are expected to be reconstructed based on the output of GNNs~\\cite{hamilton2017inductive,kipf2016variational,grover2019graphite}. %These works include unsupervised GraphSAGE~\\cite{hamilton2017inductive}, GAE~\\cite{kipf2016variational} and Graphite~\\cite{grover2019graphite}. \nExperiments showed that these GNN models learn to over-emphasize node proximity~\\cite{dgi} and may lose subtle but crucial structural information, thus failing in many tasks including node-role classification~\\cite{henderson2012rolx,donnat2018learning,ribeiro2017struc2vec,li2020distance} and graph classification~\\cite{hu2019strategies}. \n \nTo avoid the above issue, graph contrastive learning (GCL) has attracted more attention recently~\\cite{xie2021self,dgi,gmi, liu2021graph,sun2019infograph,hassani2020contrastive,zhang2020motif,thakoor2021bootstrapped,zhu2020graph,qiu2020gcc}. GCL leverages the mutual information maximization principle (InfoMax)~\\cite{linsker1988self} that aims to maximize the correspondence between the representations of a graph (or a node) in its different augmented forms~\\cite{sun2019infograph,hassani2020contrastive,you2020graph,zhang2020motif,thakoor2021bootstrapped,zhu2020graph,qiu2020gcc}. Perfect correspondence indicates that a representation precisely identifies its corresponding graph (or node) and thus the encoding procedure does not decrease the mutual information between them.\n \nHowever, researchers have found that the InfoMax principle may be risky because it may push encoders to capture redundant information that is irrelevant to the downstream tasks: Redundant information suffices to identify each graph to achieve InfoMax, but encoding it yields brittle representations and may severely deteriorate the performance of the encoder in the downstream tasks~\\cite{tschannen2019mutual}. This observation reminds us of another principle, termed information bottleneck (IB)~\\cite{tishby2000information,tishby2015deep,goldfeld2020information,alemi2016deep,vdb,betavae}. As opposed to InfoMax, IB asks the encoder to capture the \\emph{minimal sufficient} information for the downstream tasks. Specifically, IB minimizes the information from the original data while maximizing the information that is relevant to the downstream tasks. As the redundant information gets removed, the encoder learnt by IB tends to be more robust and transferable. Recently, IB has been applied to GNNs~\\cite{wu2020graph,yu2021recognizing}. But IB needs the knowledge of the downstream tasks that may not be available. \n \nHence, a natural question emerges: \\emph{When the knowledge of downstream tasks are unavailable, how to train GNNs that may remove redundant information?} Previous works highlight some solutions by designing data augmentation strategies for GCL but those strategies are typically task-related and sub-optimal. They either leverage domain knowledge~\\cite{hassani2020contrastive,zhang2020motif,zhu2020graph}, \\textit{e.g.}, node centralities in network science or molecule motifs in bio-chemistry, or depend on extensive evaluation on the downstream tasks, where the best strategy is selected based on validation performance~\\cite{you2020graph, zhu2020graph}. \n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1.0\\textwidth]{figures/ad-gcl-pipeline.pdf}\n    \\vspace{-4mm}\n    \\caption{\\small{The AD-GCL principle and its instantiation based on learnable edge-dropping augmentation. AD-GCL contains two components for graph data encoding and graph data augmentation. The GNN encoder $f(\\cdot)$ maximizes the mutual information between the original graph $G$ and the augmented graph $t(G)$ while the GNN augmenter optimizes the augmentation  $T(\\cdot)$ to remove the information from the original graph. The instantiation of AD-GCL proposed in this work uses edge dropping: An edge $e$ of $G$ is randomly dropped according to $\\text{Bernoulli}(\\omega_e)$, where $\\omega_e$ is parameterized by the GNN augmenter. }}%\\jen{ would be helpful to include ``minimize'' somewhere wrt to T in this figure. Maybe instead of \"remove info\" it could be \"minimize info\"?}}\\pan{I will definitely address this. I tried to avoid Infomin in~\\cite{tian2020makes}} } %In practice, mutual infomation is maximized by minimizing the NT-Xent loss (Eq.~\\ref{eq:loss}) and vice versa. \\pan{Can you send me the slide or sth used to draw this figure? I plan to revise it and put it into the intro.}} \n    \\label{fig:ad-gcl-pipeline}\n    \\vspace{-3mm}\n\\end{figure}\n\nIn this paper, we approach this question by proposing a novel principle that pairs GCL with adversarial training, termed \\emph{AD-GCL}, as shown in Fig.\\ref{fig:ad-gcl-pipeline}. We particularly focus on training self-supervised GNNs for graph-level tasks, though the idea may be generalized for node-level tasks. AD-GCL consists of two components: The first component contains a GNN encoder, which adopts InfoMax to maximize the correspondence/mutual information between the representations of the original graph and its augmented graphs. The second component contains a GNN-based augmenter, which aims to optimize the augmentation strategy to decrease redundant information from the original graph as much as possible. AD-GCL essentially allows the encoder capturing the minimal sufficient information to distinguish graphs in the dataset. We further provide theoretical explanations of AD-GCL. We show that with certain regularization on the search space of the augmenter, AD-GCL can yield a lower bound guarantee of the information related to the downstream tasks, while simultaneously holding an upper bound guarantee of the redundant information from the original graphs, which matches the aim of the IB principle. We further give an instantiation of AD-GCL: The GNN augmenter adopts a task-agnostic augmentation strategy and will learn an input-graph-dependent non-uniform-edge-drop probability to perform graph augmentation.\n\nFinally, we extensively evaluate AD-GCL on 18 different benchmark datasets for molecule property classification and regression, and social network classification tasks in different setting viz. unsupervised learning (Sec.~\\ref{exp:unsup}), transfer learning (Sec.~\\ref{exp:transfer}) and semi-supervised learning (Sec.~\\ref{exp:semisup}) learning. AD-GCL achieves significant performance gains in relative improvement and high mean ranks over the datasets compared to state-of-the-art baselines. %to further support the effectiveness of our principle through its instantiation. \nWe also study the theoretical aspects of AD-GCL with apt experiments and analyze the results to offer fresh perspectives (Sec.~\\ref{exp:reg_analysis}): Interestingly, we observe that AD-GCL outperforms traditional GCL based on non-optimizable augmentation across almost the entire range of perturbation levels.  \n\\vspace{-3mm}\n\\section{Notations and Preliminaries}\n\\vspace{-2mm}\nWe first introduce some preliminary concepts and notations for further exposition. In this work, we consider attributed graphs $G =(V,E)$ where $V$ is a node set and $E$ is an edge set. $G$ may have node attributes $\\{X_{v} \\in \\mathbb{R}^F \\mid v \\in V\\}$ and edge attributes $\\{X_{e} \\in \\mathbb{R}^F \\mid e \\in E\\}$ of dimension $F$. We denote the set of the neighbors of a node $v$ as $\\mathcal{N}_v$. \\vspace{-2mm}\n\\paragraph{Learning Graph Representations.} \nGiven a set of graphs $G_i$, $i=1,2,...,n$, in some universe $\\mathcal{G}$, the aim is to learn an encoder $f:\\mathcal{G}\\rightarrow \\mathbb{R}^d$, where $f(G_i)$ can be further used in some downstream task. %Here, we call $f:\\mathcal{G}\\rightarrow \\mathbb{R}^d$ as the \\emph{encoder}. \nWe also assume that $G_i$'s are all IID sampled from an unknown distribution $\\mathbb{P}_{\\mathcal{G}}$ defined over $\\mathcal{G}$. %Specifically,  $f(G_i): \\mathcal{G} \\rightarrow \\mathbb{R}^d$. \nIn a downstream task, each $G_i$ is associated with a label $y_i\\in\\mathcal{Y}$. Another model $q:\\mathbb{R}^d\\rightarrow \\mathcal{Y}$ will be learnt to predict $Y_i$ based on $q(f(G_i))$. We assume $(G_i, Y_i)$'s are IID  sampled from a distribution $\\mathbb{P}_{\\mathcal{G}\\times \\mathcal{Y}} = \\mathbb{P}_{\\mathcal{Y}|\\mathcal{G}}\\mathbb{P}_{\\mathcal{G}}$, where $\\mathbb{P}_{\\mathcal{Y}|\\mathcal{G}}$ is the conditional distribution of the graph label in the downstream task given the graph. \n\\paragraph{Graph Neural Networks (GNNs).} In this work, we focus on using GNNs, message passing GNNs in particular~\\cite{gilmer2017neural}, as the encoder $f$. For a graph $G=(V,E)$, every node $v\\in V$ will be paired with a node representation $h_v$ initialized as $h_v^{(0)}=X_v$. These representations will be updated by a GNN. During the $k^{\\text{th}}$ iteration, each $h_v^{(k-1)}$ is updated using $v'$s neighbourhood information expressed as,\n\n\\vspace{-4mm}\n\\begin{equation}\n\\label{eq:gnn_iteration}\n   \\small h_v^{(k)} = \\text{UPDATE}^{(k)} \\Bigg( h_v^{(k-1)}, \\; \\text{AGGREGATE}^{(k)} \\Big( \\big\\{(h_u^{(k-1)}, X_{uv})  \\mid u \\in \\mathcal{N}_v\\big\\} \\Big) \\Bigg)\n\\end{equation}\n where $\\text{AGGREGATE}(\\cdot)$ is a trainable function that maps the set of node representations and edge attributes $X_{uv}$ to an aggregated vector, $\\text{UPDATE}(\\cdot)$ is another trainable function that maps both $v\\text{'s}$ current representation and the aggregated vector to $v\\text{'s}$ updated representation.\nAfter $K$ iterations of Eq.~\\ref{eq:gnn_iteration}, the graph representation is obtained by pooling the final set of node representations as,\n\\begin{equation}\n    \\label{eq:pooling}\n    f(G):\\triangleq h_{G} = \\text{POOL}\\big(\\{ h_v^{(K)} \\mid v \\in V \\}\\big)\n\\end{equation}\nFor design choices regarding aggregation, update and pooling functions we refer the reader to \\cite{zhang2020deep_survey, chami2020machine, hamilton2020book}. \\vspace{-5mm}\n\\paragraph{The Mutual Information Maximization Principle.} \nGCL is built upon the InfoMax principle~\\cite{linsker1988self}, which prescribes to learn an encoder $f$ that maximizes the mutual information or the correspondence between the graph and its representation. The rationale behind GCL is that a graph representation $f(G)$ should capture the features of the graph $G$ so that representation can distinguish this graph from other graphs. Specifically, the objective of GCL follows\n\\begin{equation} \\label{eq:infomax}\n   \\text{InfoMax:}\\quad \\max_{f} I(G ; f(G)), \\quad \\text{where } G\\sim \\mathbb{P}_{\\mathcal{G}}.\n\\end{equation}\nwhere $I(X_1;X_2)$ denotes the mutual information between two random variables $X_1$ and $X_2$~\\cite{cover2012elements}. %The learnt encoder $f$ can yield graph representations that are used for the downstream tasks.\n\nNote that the encoder $f(\\cdot)$ given by GNNs is not injective in the graph space $\\mathcal{G}$ due to its limited expressive power~\\cite{xu2018powerful,morris2019weisfeiler}. Specifically, for the graphs that cannot be distinguished by $1$-WL test~\\cite{weisfeiler1968reduction}, GNNs will associate them with the same representations. We leave more discussion on 1-WL test in Appendix~\\ref{apd:1wl}. In contrast to using CNNs as encoders, one can never expect GNNs to identify all the graphs in $\\mathcal{G}$ based their representations, which introduces a unique challenge for GCL. \n\\vspace{-3mm}\n\\section{Adversarial Graph Contrastive Learning}\n\\vspace{-2mm}\nIn this section, we introduce our adversarial graph contrastive learning (AD-GCL) framework and one of its instantiations based on edge perturbation. \n\\vspace{-3mm}\n\\subsection{Theoretical Motivation and Formulation of AD-GCL}\n\\vspace{-1mm}\n\\label{sec:ad-gcl}\n\nThe InfoMax principle in Eq.~\\ref{eq:infomax} could be problematic in practice for general representation learning. Tschannen et al. have shown that for image classification, representations capturing the information that is entirely irrelevant to the image labels are also able to maximize the mutual information but such representations are definitely not useful for image classification~\\cite{tschannen2019mutual}. A similar issue can also be observed \nin graph representation learning, as illustrated by Fig.\\ref{fig:bad-exm}: We consider a binary graph classification problem with graphs in the dataset ogbg-molbace~\\cite{hu2020open}. Two GNN encoders with exactly the same architecture are trained to keep mutual information maximization between graph representations and the input graphs, but one of the GNN encoders in the same time is further supervised by random graph labels. Although the GNN encoder supervised by random labels still keeps one-to-one correspondance between every input graph and its representation (i.e., mutual information maximization), we may observe significant performance degeneration of this GNN encoder when evaluating it over the downstream ground-truth labels.  \nMore detailed experiment setup is left in Appendix~\\ref{apd:exp_settings_motivation}. %but note that as graph representation is not an injective mapping from the graph space $\\mathcal{G}$.\n\n\\begin{wrapfigure}{r}{0.38\\textwidth}\n\\vspace{-7mm}\n  \\begin{center}\n    \\includegraphics[width=0.38\\textwidth]{figures/bad_example_new.pdf}\n  \\end{center}\n  \\vspace{-4mm}\n  \\caption{\\small{Two GNNs keep the mutual information maximized between graphs and their representations. Simultaneously, they get supervised by ground-truth labels (green) and random labels (blue) respectively. %is supervised by ground-truth labels (green) \\textit{v.s.} random labels (blue) respectively keeps the mutual information maximization between graphs and their representations maximized.  training supervised by ground-truth labels (green) \\textit{v.s.} random labels (blue) respectively while. \n  The curves show their testing performance on predicting ground-truth labels. }} %Dataset ogbg-molbace~\\cite{hu2020open} is used. %and Appendix~\\ref{apd:exp_settings_motivation} contains detailed experiment setup.}}\n  \\vspace{-2mm}\n  \\label{fig:bad-exm}\n\\end{wrapfigure}\n\nThis observation inspires us to rethink what a good graph representation is. Recently, the information bottleneck has applied to learn graph representations~\\cite{wu2020graph,yu2021recognizing}. Specifically, the objective of graph information bottleneck (GIB) follows \\vspace{-1mm}\n\\begin{align} \\label{eq:GIB}\n  \\text{GIB:}\\quad  \\max_{f} I(f(G); Y) - \\beta I(G; f(G)),\n\\end{align} \n\\text{where } $(G,Y)\\sim \\mathbb{P}_{\\mathcal{G}\\times \\mathcal{Y}}, \\beta \\text{ is a positive constant.}$ Comparing Eq.~\\ref{eq:infomax} and Eq.~\\ref{eq:GIB}, we may observe the different requirements between InfoMax and GIB: InfoMax asks for maximizing the information from the original graph, while GIB asks for minimizing such information but simultaneously maximizing the information that is relevant to the downstream tasks. As GIB asks to remove redundant information, GIB naturally avoids the issue encountered in Fig.\\ref{fig:bad-exm}. Removing extra information also makes GNNs trained w.r.t. GIB robust to adverserial attack and strongly transferrable~\\cite{wu2020graph,yu2021recognizing}.  \n\nUnfortunately, GIB requires the knowledge of the class labels $Y$ from the downstream task and thus does not apply to self-supervised training of GNNs where there are few or no labels. Then, the question is how to learn robust and transferable GNNs in a self-supervised way. \n\nTo address this, we will develop a GCL approach that uses adversarial learning to  avoid capturing redundant information during the representation learning. \nIn general, GCL methods use graph data augmentation (GDA) processes to perturb the original observed graphs and decrease the amount of information they encode. %; GCL makes an assumption that \\emph{relevant information should be kept during GDA while irrelevant information is more fragile and thus tends to be removed}. \nThen, the methods apply InfoMax over perturbed graph pairs (using different GDAs) to train an encoder $f$ to capture the remaining information. \n\\begin{definition}[Graph Data Augmentation (GDA)]\nFor a graph $G \\in \\mathcal{G}$, $T(G)$ denotes a graph data augmentation of $G$, which is a distribution defined over $\\mathcal{G}$ conditioned on $G$. We use $t(G)\\in \\mathcal{G}$ to denote a sample of $T(G)$. \n\\end{definition}\\vspace{-2mm}\nSpecifically, given two ways of GDA $T_1$ and $T_2$, the objective of GCL becomes\n\\begin{equation} \\label{eq:gcl-fix-aug}\n  \\text{GDA-GCL:}\\,  \\max_{f} I(f(t_1(G)) ; f(t_2(G))), \\, \\text{where } G\\sim \\mathbb{P}_{\\mathcal{G}}, t_i(G)\\sim T_i(G), i\\in\\{1,2\\}.\n\\end{equation}\nIn practice, GDA processes are often pre-designed based on either domain knowledge or extensive evaluation, and improper choice of GDA may severely impact the downstream performance~\\cite{you2020graph,hu2019strategies}. %as a predefined random perturbation of the input graph. \n We will review a few GDAs adopted in existing works in Sec.\\ref{sec:related}. \n \n In contrast to previous predefined GDAs, our idea, inspired by GIB, is to {\\em learn} the GDA process (over a parameterized family), so that the encoder $f$ can capture the \\textbf{minimal information } that is sufficient to identify each graph. % from the others.\n \n\\vspace{-1mm}\n\n\\paragraph{AD-GCL:} %In contrast to those predefined GDAs, our idea, inspired by GIB, is to optimize GDA so that the encoder $f$ can capture the minimal sufficient information that may identify each graph from the others. %from different ways of GDA. \nWe optimize the following objective, over a GDA family $\\mathcal{T}$ (defined below).\n\\begin{equation} \\label{eq:ad-gcl}\n  \\text{AD-GCL:}\\quad  \\min_{T\\in \\mathcal{T}} \\max_{f} I(f(G) ; f(t(G))), \\quad \\text{where } G\\sim \\mathbb{P}_{\\mathcal{G}}, t(G)\\sim T(G),\n\\end{equation}\n\\begin{definition}[Graph Data Augmentation Family]  Let $\\mathcal{T}$ denote a family of different GDAs $T_{\\Phi}(\\cdot)$, where $\\Phi$ is the parameter in some universe. A $T_{\\Phi}(\\cdot)\\in \\mathcal{T}$ is a specific GDA with parameter $\\Phi$. \n\\end{definition} \\vspace{-0.5mm}\nThe min-max principle in AD-GCL aims to train the encoder such that even with a very aggressive GDA (i.e., where $t(G)$ is very different from $G$), the mutual information / the correspondence between the perturbed graph and the original graph can be maximized. %\\jen{It's not clear what you mean by {\\em aggressive} here, can you relate it to the family of GDAs? or does the parametrization of $T$ need to include the ratio of edges? }%Intuitively, AD-GCL pushes the encoder to capture the minimal sufficient information to identify different graphs in $\\mathcal{G}$.\nCompared with the two GDAs adopted in GDA-GCL (Eq.\\ref{eq:gcl-fix-aug}), AD-GCL views the original graph $G$ as the anchor while pushing its perturbation $T(G)$ as far from the anchor as it can. The automatic search over $T \\in \\mathcal{T}$ saves a great deal of effort  evaluating different combinations of GDA as adopted in \\cite{you2020graph}. %, which becomes another big advantage of AD-GCL.\n\n\\vspace{-2mm}\n\\paragraph{Relating AD-GCL to the downstream task.} %AD-GCL by definition is agnostic to the downstream task $Y$. However, we can show that if the GDA family $\\mathcal{T}$ is correlated with $Y$ to some extent, $f$ learnt by AD-GCL is expected to behave well to predict $Y$. \\jen{I don't think it's correct to say the whole family $\\mathcal{T}$ is correlated with $Y$. It has to be something more like there is one or more $T \\in \\mathcal{T}$ that doesn't (tends not to) perturb the graph info that is correlated with $Y$. Or relate it to the ratio of perturbed content. }\nNext, we will theoretically characterize the property of the encoder trained via AD-GCL.  %However, we can show that if the GDA family $\\mathcal{T}$ is correlated with $Y$ to some extent, $f$ learnt by AD-GCL is expected to behave well to predict $Y$.\nThe analysis here not only further illustrates the rationale of AD-GCL but helps design practical $\\mathcal{T}$ when some knowledge of $Y$ is accessible. But note that our analysis does not make any assumption on the availability of $Y$.\n\nNote that GNNs learning graph representations is very different from CNNs learning image representations because GNNs are never injective mappings between the graph universe $\\mathcal{G}$ and the representation space $\\mathbb{R}^d$, because the expressive power of GNNs is limited by the 1-WL test~\\cite{weisfeiler1968reduction,xu2018powerful,morris2019weisfeiler}. So, we need to define a quotient space of $\\mathcal{G}$ based on the equivalence given by the 1-WL test. \\vspace{-1mm}\n\n\\begin{definition}[Graph Quotient Space]\nDefine the equivalence $\\cong$ between two graphs $G_1\\cong G_2$ if $G_1,\\,G_2$ cannot be distinguished by the 1-WL test. Define the quotient space $\\mathcal{G}' = \\mathcal{G}/\\cong$. %\\jen{I'm not sure how to read this, does it mean that $\\mathcal{G}'$ is the reduced space of graphs, where all (WL) isomorphic graphs are grouped together (ie all $G' \\in \\mathcal{G}'$ are unique by WL test)? }\n\\end{definition}\\vspace{-1mm}\nSo every element in the quotient space, i.e., $G' \\in \\mathcal{G}'$, is a representative graph from a family of graphs that cannot be distinguished by the 1-WL test. Note that our definition also allows attributed graphs.\n\\begin{definition}[Probability Measures in $\\mathcal{G}'$]\nDefine $\\mathbb{P}_{\\mathcal{G}'}$ over the space $\\mathcal{G}'$ such that $\\mathbb{P}_{\\mathcal{G}'}(G') = \\mathbb{P}_{\\mathcal{G}}(G\\cong G')$ for any $G'\\in\\mathcal{G}'$. Further define $\\mathbb{P}_{\\mathcal{G}'\\times \\mathcal{Y}} (G', Y') = \\mathbb{P}_{\\mathcal{G}\\times \\mathcal{Y}} (G\\cong G', Y=Y')$. Given a GDA $T(\\cdot)$ defined over $\\mathcal{G}$, define a distribution on $\\mathcal{G}'$, $T'(G') = \\mathbb{E}_{G\\sim \\mathbb{P}_{\\mathcal{G}}}[T(G)| G\\cong G']$ for $G'\\in \\mathcal{G}'$. %\\jen{Are you assuming something about isomorphism in the GDA family here? eg. that two WL isomorphic graphs will have same augmentation in expectation? If so, we might need to include that in Def 2. }\n\\end{definition}\\vspace{-1mm}\n\nNow, we provide our theoretical results and give their implication. The proof is in the Appendix~\\ref{apd:prf}.\n\\begin{theorem} \\label{thm:main}\nSuppose the encoder $f$ is implemented by a GNN as powerful as the 1-WL test. Suppose $\\mathcal{G}$ is a countable space and thus $\\mathcal{G'}$ is a countable space. Then, the optimal solution $(f^*, T^*)$ to AD-GCL satisfies, letting $T'^{*}(G') = \\mathbb{E}_{G\\sim \\mathbb{P}_{\\mathcal{G}}}[T^*(G)| G\\cong G']$, \\vspace{-1mm}\n\\begin{enumerate}[leftmargin=*]\n    \\item $I(f^*(t^*(G)); G\\,|\\,Y) \\leq  \\min_{T\\in\\mathcal{T}} I(t'(G'); G') - I(t'^*(G'); Y) $, where $t'(G')\\sim T'(G')$, $t'^*(G')\\sim T'^{*}(G')$, $(G,Y)\\sim \\mathbb{P}_{\\mathcal{G}\\times \\mathcal{Y}}$ and $(G',Y)\\sim \\mathbb{P}_{\\mathcal{G}'\\times \\mathcal{Y}}$.\n    \\item $I(f^*(G); Y)\\geq I(f^*(t'^*(G')); Y) = I(t'^*(G'); Y)$, where  $t'^*(G')\\sim T'^*(G')$, $(G,Y)\\sim \\mathbb{P}_{\\mathcal{G}\\times \\mathcal{Y}}$ and $(G',Y)\\sim \\mathbb{P}_{\\mathcal{G}'\\times \\mathcal{Y}}$.\n\\end{enumerate}\n\\vspace{-1mm}\n\\end{theorem}\n\nThe statement 1 in Theorem~\\ref{thm:main} guarantees a upper bound of the information that is captured by the representations but irrelevant to the downstream task, which matches our aim. This bound has a form very relevant to the GIB principle (Eq.\\ref{eq:GIB} when $\\beta=1$), since $ \\min_{T\\in\\mathcal{T}} I(t'(G'); G') - I(t'^*(G'); Y) \\geq \\min_{f} [I(f(G); G) - I(f(G); Y)]$,\nwhere $f$ is a GNN encoder as powerful as the 1-WL test. But note that this inequality also implies that the encoder given by AD-GCL may be worse than the optimal encoder given by GIB ($\\beta =1$). This makes sense as GIB has the access to the downstream task $Y$.\n\nThe statement 2 in Theorem~\\ref{thm:main} guarantees a lower bound of the mutual information between the learnt representations and the labels of the downstream task. As long as the GDA family $\\mathcal{T}$ has a good control, $I(t'^*(G'); Y)\\geq \\min_{T\\in\\mathcal{T}}I(t'(G');Y)$ and $I(f^*(G); Y)$ thus cannot be too small. This implies that it is better to regularize when learning over $\\mathcal{T}$. In our instantiation, based on edge-dropping augmentation (Sec.~\\ref{sec:instantiation}), we regularize the ratio of dropped edges per graph. \n\n\\vspace{-1mm}\n\\subsection{Instantiation of AD-GCL via Learnable Edge Perturbation} \\label{sec:instantiation}\n\\vspace{-1mm}\nWe now introduce a practical instantiation of the AD-GCL principle (Eq.~\\ref{eq:ad-gcl}) based on learnable edge-dropping augmentations as illustrated in Fig.~\\ref{fig:ad-gcl-pipeline}. \n(See Appendix~\\ref{apd:algo} for a summary of AD-GCL in its algorithmic form.)\nThe objective of AD-GCL has two folds: (1) Optimize the encoder $f$ to maximize the mutual information between the representations of the original graph $G$ and its augmented graph $t(G)$; (2) Optimize the GDA $T(G)$ where $t(G)$ is sampled to minimize such a mutual information. We always set the encoder as a GNN $f_{\\Theta}$ with learnable parameters $\\Theta$ and next we focus on the GDA, $T_{\\Phi}(G)$ that has learnable parameters $\\Phi$. \\vspace{-1mm}\n\n\\paragraph{Learnable Edge Dropping GDA model $T_{\\Phi}(\\cdot)$.}\nEdge dropping is the operation of deleting some edges in a graph. As a proof of concept, we adopt edge dropping to formulate the GDA family $\\mathcal{T}$. Other types of GDAs such as node dropping, edge adding and feature masking can also be paired with our AD-GCL principle. %while edge dropping has already given good enough performance in our experiments. \nInterestingly, in our experiments, edge-dropping augmentation optimized by AD-GCL has already achieved much better performance than any pre-defined random GDAs even carefully selected via extensive evaluation~\\cite{you2020graph} (See Sec.\\ref{sec:exp}). %We attribute such improvement to the AD-GCL principle. \nAnother reason that supports edge dropping is due to our Theorem~\\ref{thm:main} statement 2, which shows that good GDAs should keep some information related to the downstream tasks. Many GRL downstream tasks such as molecule classification only depends on the structural fingerprints that can be represented as subgraphs of the original graph~\\cite{duvenaud2015convolutional}. Dropping a few edges may not change those subgraph structures %of the original graph allows those subgraphs being kept \nand thus keeps the information sufficient to the downstream classification. But note that this reasoning does not mean that we leverage domain knowledge to design GDA, as the family $\\mathcal{T}$ is still broad and the specific GDA still needs to be optimized. Moreover, experiments show that our instantiation also works extremely well on social network classification and molecule property regression, where the evidence of subgraph fingerprints may not exist any more.  \\vspace{-1mm}\n\n\\paragraph{Parameterizing $T_{\\Phi}(\\cdot)$.} For each $G=(V,E)$, we set $T_{\\Phi}(G)$, $T\\in \\mathcal{T}$ as a random graph model \\cite{gilbert1959random, erdds1959random} conditioning on $G$. Each sample $t(G)\\sim T_{\\Phi}(G)$ is a graph that shares the same node set with $G$ while the edge set of $t(G)$ is only a subset of $E$. Each edge $e \\in E$ will be associated with a random variable $p_e\\sim$ Bernoulli$(\\omega_e)$, where $e$ is in $t(G)$ if $p_e=1$ and is dropped otherwise. \n\nWe parameterize the Bernoulli weights $\\omega_e$ by leveraging another GNN, \\textit{i.e.,} the \\emph{augmenter}, to run on $G$ according to Eq.\\ref{eq:gnn_iteration} of $K$ layers, get the final-layer node representations $\\{h_v^{(K)}|v\\in V\\}$ and set\n\\begin{equation}\\label{eq:w_e}\n    \\omega_{e} = \\text{MLP} ([h_u^{(K)};h_z^{(K)}]), \\quad \\text{where }  e=(u,z) \\,\\text{and}\\,\\{ h_v^{(K)} \\mid v \\in V \\} = \\text{GNN-augmenter}(G)\n\\end{equation}\n\nTo train $T(G)$ in an end-to-end fashion, %we require the gradients $\\frac{\\partial p_{e}}{\\partial \\omega_{e}}$ to be smooth and well defined. \nwe relax the discrete $p_{e}$ to be a continuous variable in $[0,1]$ and utilize the Gumbel-Max reparametrization trick \\cite{maddison2016concrete, jang2016categorical}. Specifically, % which is crucial for backpropagation when dealing with. Specifically, \n$p_{e} = \\text{Sigmoid}((\\log \\delta - \\log(1-\\delta) + \\omega_{e})/\\tau)$, where $\\delta \\sim \\text{Uniform(0,1)}$. \nAs temperature hyper-parameter $\\tau \\rightarrow 0$, $p_{e}$ gets closer to being binary. Moreover, the gradients $\\frac{\\partial p_{e}}{\\partial \\omega_{e}}$ are smooth and well defined. This style of edge dropping based on a random graph model has also been used for parameterized explanations of GNNs \\cite{luo2020parameterized}. \\vspace{-1mm}\n\n\\begin{comment}\nfor a graph $G = (V, E)$, let $p_{ij}$ be a binary indicator that specifies if edge $(i,j) \\in E$ is perturbed i.e., $p_{ij} = 1$ if edge $(i,j)$ is perturbed and 0 otherwise. The probability of edge being perturbed follows a Bernoulli distribution i.e., $p_{ij} \\sim \\text{Bernoulli}(\\omega_{ij})$. This allows us to parameterize the GDA as $T_{\\Omega} (G)$, where $\\Omega$ holds parameters $\\omega_{ij}, \\forall (i,j) \\in E$ for $G$. Now, taking into account our MI estimator, encoder and parametrized GDA, we can rewrite the AD-GCL principle\n\nObjective (2) requires a parametrized GDA family $\\mathcal{T}$ which allows us to search over its space. To make this objective tractable, we design $\\mathcal{T}$ as an edge perturbation family within the paradigm of a random graph model \\cite{gilbert1959random, erdds1959random} where each edge is perturbed following a Bernoulli distribution.\n\nFor a graph $G = (V, E)$, let $p_{ij}$ be a binary indicator that specifies if edge $(i,j) \\in E$ is perturbed i.e., $p_{ij} = 1$ if edge $(i,j)$ is perturbed and 0 otherwise. The probability of edge being perturbed follows a Bernoulli distribution i.e., $p_{ij} \\sim \\text{Bernoulli}(\\omega_{ij})$. This allows us to parameterize the GDA as $T_{\\Omega} (G)$, where $\\Omega$ holds parameters $\\omega_{ij}, \\forall (i,j) \\in E$ for $G$. Now, taking into account our MI estimator, encoder and parametrized GDA, we can rewrite the AD-GCL principle as,\n\\begin{equation} \\label{eq:ad-gcl-edge-perturb}\n  \\min_{\\Omega} \\max_{\\Theta} I_{\\text{NCE}}(f_{\\Theta}(G) ; f_{\\Theta}(t(G))), \\quad \\text{where } G\\sim \\mathbb{P}_{\\mathcal{G}}, t(G)\\sim T_{\\Omega}(G), \n\\end{equation}\nTo implement $T_{\\Omega}(\\cdot)$ and optimize $\\Omega$ in practice with gradient based optimization, we make use of another network called \\textit{augmentor}, which is composed of a separate GNN with parameters $\\Phi_1$ followed by an MLP edge model with parameters $\\Phi_2$. Concretely, for graph $G$, the GNN of the \\textit{augmentor} following Eq.~\\ref{eq:gnn_iteration} iteratively for K layers gives final node representations and another MLP provides the parameters $\\Omega$ for perturbing edges: \n\\begin{equation}\n    \\omega_{ij} = \\text{MLP}_{\\Phi_2} ([h_i^{(K)};h_j^{(K)}]) \\quad \\text{where } \\{ h_v^{(K)} \\mid v \\in V \\} = \\text{GNN}_{\\Phi_1}(G)\n\\end{equation}\nThis augmentor with combined parameters $\\Phi = \\{\\Phi_1, \\Phi_2\\}$ provides us with a continuous $\\omega_{ij} \\in \\mathbb{R}$ parameter for each edge, constituting our $\\Omega$.\n\\begin{equation} \\label{eq:ad-gcl-encoder-augmentor}\n  \\min_{\\Phi} \\max_{\\Theta} I_{\\text{NCE}}(f_{\\Theta}(G) ; f_{\\Theta}(t(G))), \\quad \\text{where } G\\sim \\mathbb{P}_{\\mathcal{G}}, t(G)\\sim T_{\\Phi}(G), \n\\end{equation}\nIn this instantiation, generating samples $t(G)$ from the distribution defined by $T_{\\Omega}(G)$ equivalently means drawing samples $p_{ij}$ from $\\text{Bernoulli}(\\omega_{ij})$. Because our aim is to do the learning (finding $\\omega_{ij}$) in an end-to-end fashion, we require the gradients $\\frac{\\partial p_{ij}}{\\partial \\omega_{ij}}$ to be smooth and well defined. For that, we first relax the discrete $p_{ij}$ to be a continuous variable in $[0,1]$ and utilize the well-known Gumbel-Max reparametrization trick \\cite{maddison2016concrete, jang2016categorical} which is crucial for backpropagation when dealing with . Specifically, \n\\begin{equation}\n\\label{eq:p_ij}\n    p_{ij} = \\text{Sigmoid}((\\log \\delta - log(1-\\delta) + \\omega_{ij})/\\tau) \\quad \\text{where } \\delta \\sim \\text{Uniform(0,1)} \n\\end{equation}\nAs temperature hyper-parameter $\\tau \\rightarrow 0$, $p_{ij}$ gets closer to being binary.\n\n\\end{comment}\n\n\\paragraph{Regularizing $T_{\\Phi}(\\cdot)$.} As shown in Theorem~\\ref{thm:main}, a reasonable GDA should keep a certain amount of information related to the downstream tasks (statement 2). Hence, we expect the GDAs in the edge dropping family $\\mathcal{T}$ not to perform very aggressive perturbation. Therefore, we regularize the ratio of edges being dropped  per graph by enforcing the following constraint: For a graph $G$ and its augmented graph $t(G)$, we add $\\sum_{e\\in E} \\omega_e/|E|$ to the objective, where $\\omega_e$ is defined in Eq.\\ref{eq:w_e} indicates the probability that $e$ gets dropped. \n\nPutting everything together, the final objective is as follows.\n\\begin{equation} \\label{eq:ad-gcl-encoder-augmentor-reg}\n  \\min_{\\Phi} \\max_{\\Theta} I(f_{\\Theta}(G) ; f_{\\Theta}(t(G))) + \\lambda_{\\text{reg}} \\mathbb{E}_{G}\\big[\\sum_{e \\in E} \\omega_e/|E|\\big],\\, \\text{where } G\\sim \\mathbb{P}_{\\mathcal{G}}, t(G)\\sim T_{\\Phi}(G).\n\\end{equation}\n\n\\vspace{-3mm}\nNote $\\Phi$ corresponds to the learnable parameters of the augmenter GNN and MLP used to derive the $\\omega_{e}$'s and $\\Theta$ corresponds to the learnable parameters of the GNN $f$. \n\\vspace{-2mm}\n\\paragraph{Estimating the objective in Eq.\\ref{eq:ad-gcl-encoder-augmentor-reg}.} In our implementation, the second (regularization) term is easy to estimate empirically. For the first (mutual information) term, we adopt InfoNCE as the estimator~\\cite{oord2018representation, tian2019contrastive, poole2019variational}, which is known to be a lower bound of the mutual information and is frequently used for contrastive learning~\\cite{oord2018representation,tschannen2019mutual,chen2020simple}. Specfically, during the training, given a minibatch of $m$ graphs $\\{G_i\\}_{i=1}^m$, let $z_{i, 1} = g(f_{\\Theta}(G_i))$ and $z_{i, 2} = g(f_{\\Theta}(t(G_i)))$ where $g(\\cdot)$ is the projection head implemented by a 2-layer MLP as suggested in~\\cite{chen2020simple}. With $sim(\\cdot, \\cdot)$ denoting cosine similarity, we estimate the mutual information for the mini-batch as follows.\n\\begin{equation}\n    \\label{eq:loss}\n    \\small{I(f_{\\Theta}(G) ; f_{\\Theta}(t(G))) \\rightarrow \\hat{I}= \\frac{1}{m}\\sum_{i= 1}^{m} \\log \\frac{\\exp(sim(z_{i,1}, z_{i,2}))}{\\sum_{i^\\prime = 1, i^\\prime \\neq i }^{m}\\exp(sim(z_{i,1}, z_{i^\\prime,2}))}}\n\\end{equation}\n\n\\vspace{-6mm}\n\\section{Related Work}\n\\vspace{-2mm}\n\\label{sec:related}\nGNNs for GRL is a broad field and gets a high-level review in the Sec.~\\ref{sec:intro}. Here, we focus on the topics that are most relevant to graph contrastive learning (GCL). \n\nContrastive learning (CL)~\\cite{linsker1988self,becker1992self, henaff2020data, oord2018representation, tian2019contrastive,hjelm2018learning} was initially proposed to train CNNs for image representation learning and has recently achieved great success~\\cite{chen2020simple,chen2020big}. GCL applies the idea of CL on GNNs. In contrast to the case of CNNs, GCL trained using GNNs posts us new fundamental challenges. An image often has multiple natural views, say by imposing different color filters and so on. Hence, different views of an image give natural contrastive pairs for CL to train CNNs. However, graphs are more abstract and the irregularity of graph structures typically provides crucial information. Thus, designing contrastive pairs for GCL must play with irregular graph structures and thus becomes more challenging. Some works use different parts of a graph to build contrastive pairs, including nodes \\textit{v.s.} whole graphs~\\cite{velivckovic2018deep,sun2019infograph}, nodes \\textit{v.s.} nodes~\\cite{peng2020graph}, nodes \\textit{v.s.} subgraphs~\\cite{jiao2020sub, hu2019strategies}.\nOther works adopt graph data augmentations (GDA) such as edge perturbation~\\cite{qiu2020gcc} to generate contrastive pairs. Recently. GraphCL~\\cite{you2020graph} gives an extensive study on different combinations of GDAs including node dropping, edge perturbation, subgraph sampling and feature masking. Extensive evaluation is required to determine good combinations. MVGRL~\\cite{hassani2020contrastive} and GCA~\\cite{zhu2020graph} leverage the domain knowledge of network science and adopt network centrality to perform GDAs. Note that none of the above methods consider optimizing augmentations.  In contrast, our principle AD-GCL provides theoretical guiding principles to optimize augmentations. Very recently, JOAO~\\cite{you2021graph} adopts a bi-level optimization framework sharing some high-level ideas with our adversarial training strategy but has several differences: 1) the GDA search space in JOAO is set as different types of augmentation with uniform perturbation, such as uniform edge/node dropping while we allow augmentation with non-uniform perturbation. 2) JOAO relaxes the GDA combinatorial search problem into continuous space via Jensens inequality and adopts projected gradient descent to optimize. Ours, instead, adopts Bayesian modeling plus reparameterization tricks to optimize. The performance comparison between AD-GCL and JOAO for the tasks investigated in Sec.~\\ref{sec:exp} is given in Appendix~\\ref{apd:joao_compare}. %Augmentation schemes are generally guided by vague principles and for success requires extensive domain knowledge. \n\nTian et al.~\\cite{tian2020makes} has recently proposed the InfoMin principle that shares some ideas with AD-GCL but there are several fundamental differences. Theoretically, InfoMin needs the downstream tasks to supervise the augmentation. Rephrased in our notation, the optimal augmentation $T_{IM}(G)$ given by InfoMin (called the sweet spot in~\\cite{tian2020makes}) needs to satisfy $I(t_{IM}(G);Y) = I(G;Y)$ and $I(t_{IM}(G);G|Y)=0$, $t_{IM}(G)\\sim T_{IM}(G)$, neither of which are possible without the downstream-task knowledge. Instead, our Theorem~\\ref{thm:main} provides more reasonable arguments and creatively suggests using regularization to control the tradeoff. Empirically, InfoMin is applied to CNNs while AD-GCL is applied to GNNs. AD-GCL needs to handle the above challenges due to irregular graph structures and the limited expressive power of GNNs~\\cite{xu2018powerful,morris2019weisfeiler}, which InfoMin does not consider. \n\n\\begin{comment}\n\\subsection{Graph Representation Learning}\nOwing to the widespread use of graphs to represent data from diverse domains \\cite{applications}, machine learning methods for various downstream tasks like node-level, subgraph-level, graph-level classification/regression and link prediction have become extremely important. A significant amount of work has been done towards this goal \\cite{}. The common goal in all these methods is to build representations in latent space that can capture the structure and various attributes of a graph that can be useful for downstream tasks. In particular, Graph Neural Networks (GNNs) \\cite{scarselli2008graph, bruna2014spectral,gilmer2017neural, kipf2017semi, morris2019weisfeiler} making use of the message passing paradigm, learn useful node-level and graph-level representations with much of the focus in supervised settings. There is now growing interest to build self-supervised graph representation learning methods given the prevalent label scarcity issue. For a comprehensive review of the design and principles of GNNs we refer the reader to \\cite{chami2020machine, zhang2020deep_survey, hamilton2020book}.\n\nEarliest works in self-supervised graph representation learning (SS-GRL) were inspired by language models which learn word representation by predicting their context \\cite{mikolov2013distributed}. They have been adapted to build node and graph level embeddingsby using hand-crafted procedures like random walks, adjacency reconstruction and Weisfeiler-Lehman relabelling for providing suitable context~\\cite{perozzi2014deepwalk,grover2016node2vec,ribeiro2017struc2vec, narayanan2017graph2vec,hamilton2017inductive, kipf2016variational} .\n\n\\subsection{Graph Contrastive Learning (GCL)}\nContrastive Learning (CL)~\\cite{becker1992self, henaff2020data, oord2018representation, tian2019contrastive} aims to learn an encoder that can differentiate similar data instances (views) from dissimilar instances and has found great success in self-supervised language and image representation learning~\\cite{}. It provides a natural learning metric based on the mutual information maximization between correlated instances~\\cite{linsker1988self}. For graphs, with the advent of GNNs as the primary encoder of choice, CL has been adopted but, defining the contrastive pairs for graphs plays a very significant role in the learning outcome and has been guided by heuristics and extensive evaluation. These strategies include, contrasting node-level and graph-level pairs as done in DGI~\\cite{velivckovic2018deep}, InfoGraph~\\cite{sun2019infograph}, contrasting node-node pairs and including edge information as in GMI~\\cite{peng2020graph}, contrasting node and sub-graph pairs~\\cite{jiao2020sub, hu2019strategies}. The above methods use simple shuffling of node features or edge permutations to construct dissimilar instances. Different from them MVGRL~\\cite{hassani2020contrastive} uses graph diffusion kernels to generate augmented graphs and contrasts node-graph pairs. GCC~\\cite{qiu2020gcc} contrasts sampled sub-graph pairs, GraphCL~\\cite{you2020graph} contrasts graph-graph pairs and augmentations like node dropping, edge perturbation, subgraph sampling and feature masking are designed. It is to be noted that these augmentations are not conditioned on the input graph and thus requires very extensive evaluation when deploying them in practice. Finally, GCA~\\cite{zhu2020graph} takes a step in the direction of creating augmentations that are adaptive to both structure and attributes of a graph but, resort to non-learnable augmentation schemes that are based on degree, eigenvector and PageRank centrality. In addition this method is designed with the specific objective of node classification. As evident, a common thread is that none of the above methods have any theoretical guiding principles for defining these contrastive pairs or generating augmentations. Augmentation schemes are generally guided by vague principles and for success requires extensive domain knowledge. Improperly designed augmentations can inject too much or too little information that risk learning sub-optimal representations. Different from them our principle AD-GCL, guides the automatic generation of augmentations that can be most beneficial for GCL. \n\nHere I will cite and discuss \\cite{tian2020makes}\n\n\\subsection{Information Bottleneck}\nI will work on this. Here I will cite and discuss \\cite{tian2020makes}\n\\end{comment}\n\n\\vspace{-3mm}\n\\section{Experiments and Analysis}\n\\vspace{-3mm}\n\\label{sec:exp}\nThis section is devoted to the empirical evaluation of the proposed instantiation of our AD-GCL principle. Our initial focus is on unsupervised learning which is followed by analysis of the effects of regularization. We further apply AD-GCL to transfer and semi-supervised learning. Summary of datasets and training details for specific experiments are provided in Appendix~\\ref{apd:datasets} and \\ref{apd:exp_settings} respectively.\n\n\\vspace{-3mm}\n\\subsection{Unsupervised Learning}\n\\vspace{-3mm}\n\\label{exp:unsup}\nIn this setting, an encoder (specifically GIN~\\cite{xu2018how}) is trained with different self-supervised methods to learn graph representations, which are then evaluated by feeding these representations to make prediction for the downstream tasks. We use datasets from Open Graph Benchmark (OGB)~\\cite{hu2020open}, TU Dataset~\\cite{Morris+2020} and ZINC~\\cite{dwivedi2020benchmarking} for graph-level property classification and regression. More details regarding the experimental setting are provided in the Appendix~\\ref{apd:exp_settings}.\n\nWe consider two types of AD-GCL, where one is with a fixed regularization weight $\\lambda_{\\text{reg}}=5$ (Eq.\\ref{eq:ad-gcl-encoder-augmentor-reg}), termed AD-GCL-FIX, and another is with $\\lambda_{\\text{reg}}$ tuned over the validation set among $\\{0.1, 0.3, 0.5, 1.0, 2.0, 5.0, 10.0\\}$, termed AD-GCL-OPT. AD-GCL-FIX assumes any information from the downstream task as unavailable while AD-GCL-OPT assumes the augmentation search space has some weak information from the downstream task. A full range of analysis on how $\\lambda_{\\text{reg}}$ impacts AD-GCL will be investigated in Sec.~\\ref{exp:reg_analysis}. We compare AD-GCL with three unsupervised/self-supervised learning baselines for graph-level tasks, which include randomly initialized untrained GIN (RU-GIN)~\\cite{xu2018how}, InfoGraph~\\cite{sun2019infograph} and GraphCL~\\cite{you2020graph}. Previous works~\\cite{you2020graph, sun2019infograph} show that they generally outperform graph kernels~\\cite{kriege2020survey,yanardag2015deep,shervashidze2011weisfeiler} and network embedding methods~\\cite{grover2016node2vec, perozzi2014deepwalk, narayanan2017graph2vec,adhikari2018sub2vec}.\n\nWe also adopt GCL with GDA based on non-adversarial edge dropping (NAD-GCL) for ablation study. NAD-GCL drops the edges of a graph uniformly at random. We consider NAD-GCL-FIX and NAD-GCL-OPT with different edge drop ratios. NAD-GCL-GCL adopts the edge drop ratio of AD-GCL-FIX at the saddle point of the optimization (Eq.\\ref{eq:ad-gcl-encoder-augmentor-reg}) while NAD-GCL-OPT optimally tunes the edge drop ratio over the validation datasets to match AD-GCL-OPT.\nWe also adopt fully supervised GIN (F-GIN) to provide an anchor of the performance. We stress that all methods adopt GIN~\\cite{xu2018how} as the encoder. Except F-GIN, all methods adopt a downstream \\emph{linear} classifier or regressor with the same hyper-parameters for fair comparison. Adopting \\emph{linear models} was suggested by~\\cite{tschannen2019mutual}, which explicitly attributes any performance gain/drop to the quality of learnt representations.  \n\n\\begin{table}[t]\n\\centering\n\\renewcommand{\\arraystretch}{1.5}\n\\newcommand{\\STAB}[1]{\\begin{tabular}{@{}c@{}}#1\\end{tabular}}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{clcccc|ccccc}\n\\hline\n& Dataset                                      & NCI1           & PROTEINS            & MUTAG          & DD                                      & COLLAB           & RDT-B            & RDT-M5K          & IMDB-B           & IMDB-M           \\\\ \\hline\n\n& F-GIN & 78.27 $\\pm$ 1.35 & 72.39 $\\pm$ 2.76 & 90.41 $\\pm$ 4.61 & 74.87 $\\pm$ 3.56 & 74.82 $\\pm$ 0.92 & 86.79 $\\pm$ 2.04 & 53.28 $\\pm$ 3.17 & 71.83 $\\pm$ 1.93 & 48.46 $\\pm$ 2.31 \\\\ \\hline\n\\multirow{3}{*}{\\STAB{\\rotatebox[origin=c]{90}{Baselines}}}&RU-GIN~\\cite{xu2018how}& 62.98 $\\pm$ 0.10 & 69.03 $\\pm$ 0.33 & 87.61 $\\pm$ 0.39 & 74.22 $\\pm$ 0.30  & 63.08 $\\pm$ 0.10 & 58.97 $\\pm$ 0.13 & 27.52 $\\pm$ 0.61 & 51.86 $\\pm$ 0.33 & 32.81 $\\pm$ 0.57 \\\\\n& InfoGraph~\\cite{sun2019infograph}& 68.13 $\\pm$ 0.59 & 72.57 $\\pm$ 0.65 & 87.71 $\\pm$ 1.77 & 75.23 $\\pm$ 0.39                                     & 70.35 $\\pm$ 0.64 & 78.79 $\\pm$ 2.14 & 51.11 $\\pm$ 0.55 & 71.11 $\\pm$ 0.88 & 48.66 $\\pm$ 0.67 \\\\\n& GraphCL ~\\cite{you2020graph}& 68.54 $\\pm$ 0.55 & 72.86 $\\pm$ 1.01 & 88.29 $\\pm$ 1.31 & 74.70 $\\pm$ 0.70                                     & 71.26 $\\pm$ 0.55 & 82.63 $\\pm$ 0.99 & 53.05 $\\pm$ 0.40 & 70.80 $\\pm$ 0.77 & 48.49 $\\pm$ 0.63 \\\\\\hline\n\\multirow{2}{*}{\\STAB{\\rotatebox[origin=c]{90}{AB-S}}}& NAD-GCL-FIX & 69.23 $\\pm$ 0.60 & 72.81 $\\pm$ 0.71 & 88.58 $\\pm$ 1.58 & 74.55 $\\pm$ 0.55              & 71.56 $\\pm$ 0.58 & 83.41 $\\pm$ 0.66 & 52.72 $\\pm$ 0.71 & 70.94 $\\pm$ 0.77 & 48.33 $\\pm$ 0.47 \\\\\n& NAD-GCL-OPT & 69.30 $\\pm$ 0.32 & 73.18 $\\pm$ 0.71 & 89.05 $\\pm$ 1.06 & 74.55 $\\pm$ 0.55             & 72.04 $\\pm$ 0.67 & 83.74 $\\pm$ 0.76 & 53.43 $\\pm$ 0.26 & 71.94 $\\pm$ 0.59 & 49.01 $\\pm$ 0.93 \\\\\\hline\\hline\n\\multirow{2}{*}{\\STAB{\\rotatebox[origin=c]{90}{Ours}}} \n\n& AD-GCL-FIX & \\bf{69.67 $\\pm$ 0.51}$^\\star$ & \\bf{73.59 $\\pm$ 0.65} & \\bf{89.25 $\\pm$ 1.45} & 74.49 $\\pm$ 0.52 & \\textbf{73.32 $\\pm$ 0.61}$^\\star$ & \\textbf{85.52 $\\pm$ 0.79}$^\\star$ & 53.00 $\\pm$ 0.82 & \\textbf{71.57 $\\pm$ 1.01} & \\textbf{49.04 $\\pm$ 0.53} \\\\ \n\n& AD-GCL-OPT & \\bf{69.67 $\\pm$ 0.51}$^\\star$ & \\bf{73.81 $\\pm$ 0.46}$^\\star$ & \\textbf{89.70 $\\pm$ 1.03} & 75.10 $\\pm$ 0.39 & \\textbf{73.32 $\\pm$ 0.61}$^\\star$ & \\textbf{85.52 $\\pm$ 0.79}$^\\star$ & \\textbf{54.93 $\\pm$ 0.43}$^\\star$ & \\textbf{72.33 $\\pm$ 0.56}$^\\star$ & \\textbf{49.89 $\\pm$ 0.66}$^\\star$ \\\\ \\hline\\hline\n\\end{tabular}%\n}\\\\\n\\vspace{3mm}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{clcccc|ccccc}\n\\hline\n& Task                                         & \\multicolumn{4}{c|}{Regression (Downstream Classifier - Linear Regression + L2)} & \\multicolumn{5}{c}{Classification (Downstream Classifier - Logistic Regression + L2)}       \\\\ \\cline{1-11} \n& Dataset                                      & molesol            & mollipo           & molfreesolv        & \\multicolumn{1}{|c|}{ZINC-10K}           & molbace          & molbbbp          & molclintox       & moltox21         & molsider         \\\\\n& Metric                                       & \\multicolumn{3}{c}{RMSE (shared) ($\\downarrow$)}                     & \\multicolumn{1}{|c|}{MAE ($\\downarrow$) }& \\multicolumn{5}{c}{ROC-AUC \\% (shared) ($\\uparrow$)}                                               \\\\ \\hline\n\n&F-GIN                         & 1.173 $\\pm$ 0.057  & 0.757 $\\pm$ 0.018 & 2.755 $\\pm$ 0.349  & \\multicolumn{1}{|c|}{0.254 $\\pm$ 0.005}  & 72.97 $\\pm$ 4.00 & 68.17 $\\pm$ 1.48 & 88.14 $\\pm$ 2.51 & 74.91 $\\pm$ 0.51 & 57.60 $\\pm$ 1.40 \\\\ \\hline\n\\multirow{3}{*}{\\STAB{\\rotatebox[origin=c]{90}{Baselines}}} & RU-GIN~\\cite{xu2018how}                              & 1.706 $\\pm$ 0.180  & 1.075 $\\pm$ 0.022 & 7.526 $\\pm$ 2.119  & \\multicolumn{1}{|c|}{0.809 $\\pm$ 0.022}  & 75.07 $\\pm$ 2.23 & 64.48 $\\pm$ 2.46 & 72.29 $\\pm$ 4.15 & 71.53 $\\pm$ 0.74 & 62.29 $\\pm$ 1.12 \\\\\n\n&InfoGraph~\\cite{sun2019infograph}                                    & 1.344 $\\pm$ 0.178  & 1.005 $\\pm$ 0.023 & 10.005 $\\pm$ 4.819 & \\multicolumn{1}{|c|}{0.890 $\\pm$ 0.017}  & 74.74 $\\pm$ 3.64 & 66.33 $\\pm$ 2.79 & 64.50 $\\pm$ 5.32 & 69.74 $\\pm$ 0.57 & 60.54 $\\pm$ 0.90 \\\\\n\n&GraphCL~\\cite{you2020graph}                                      & 1.272 $\\pm$ 0.089  & 0.910 $\\pm$ 0.016 & 7.679 $\\pm$ 2.748  & \\multicolumn{1}{|c|}{0.627 $\\pm$ 0.013}  & 74.32 $\\pm$ 2.70 & 68.22 $\\pm$ 1.89 & 74.92 $\\pm$ 4.42 & 72.40 $\\pm$ 1.01 & 61.76 $\\pm$ 1.11 \\\\ \n\n\\hline\n\\multirow{2}{*}{\\STAB{\\rotatebox[origin=c]{90}{AB-S}}} & NAD-GCL-FIX                   & 1.392 $\\pm$ 0.065  & 0.952 $\\pm$ 0.024 & 5.840 $\\pm$ 0.877  & \\multicolumn{1}{|c|}{0.609 $\\pm$ 0.010}  & 73.60 $\\pm$ 2.73 & 66.12 $\\pm$ 1.80 & 73.32 $\\pm$ 3.66 & 71.65 $\\pm$ 0.94 & 60.41 $\\pm$ 1.48 \\\\ \n\n&NAD-GCL-OPT             & 1.242 $\\pm$ 0.096  & 0.897 $\\pm$ 0.022 & 5.840 $\\pm$ 0.877  & \\multicolumn{1}{|c|}{0.609 $\\pm$ 0.010}  & 73.69 $\\pm$ 3.67 & 67.70 $\\pm$ 1.78 & 74.40 $\\pm$ 4.92 & 71.65 $\\pm$ 0.94 & 61.14 $\\pm$ 1.43 \\\\ \n\n\\hline\\hline\n\\multirow{2}{*}{\\STAB{\\rotatebox[origin=c]{90}{Ours}}}&AD-GCL-FIX     & \\textbf{1.217 $\\pm$ 0.087}  & \\textbf{0.842 $\\pm$ 0.028}$^\\star$ & \\textbf{5.150 $\\pm$ 0.624}$^\\star$  & \\multicolumn{1}{|c|}{\\textbf{0.578 $\\pm$ 0.012}$^\\star$} & \\textbf{76.37 $\\pm$ 2.03} & 68.24 $\\pm$ 1.47 & \\textbf{80.77 $\\pm$ 3.92} & 71.42 $\\pm$ 0.73 & \\textbf{63.19 $\\pm$ 0.95} \\\\\n\n&AD-GCL-OPT & \\textbf{1.136 $\\pm$ 0.050}$^\\star$  & \\textbf{0.812 $\\pm$ 0.020}$^\\star$ & \\textbf{4.145 $\\pm$ 0.369}$^\\star$ & \\multicolumn{1}{|c|}{\\textbf{0.544 $\\pm$ 0.004}$^\\star$}  & \\textbf{77.27 $\\pm$ 2.56} & \\textbf{69.54 $\\pm$ 1.92} & \\textbf{80.77 $\\pm$ 3.92} & \\textbf{72.92 $\\pm$ 0.86} & \\textbf{63.19 $\\pm$ 0.95} \\\\ \\hline \\hline\n\\end{tabular}%\n}\n\n\\caption{\\small{Unsupervised learning performance for (TOP) biochemical and social network classification in TU datasets~\\cite{Morris+2020} (Averaged accuracy $\\pm$ std. over 10 runs) and (BOTTOM) chemical molecules property prediction in OGB datasets~\\cite{hu2020open} (mean $\\pm$ std. over  10 runs). % \n\\textbf{Bold}/\\textbf{Bold}$^\\star$ indicats our methods outperform baselines with $\\geq$ 0.5/$\\geq$ 2 std respectively. Fully supervised (F-GIN) results are shown \\textbf{only} for placing GRL methods in perspective. Ablation-study (AB-S) results do not count as baselines.}}\n\\label{tab:unsupervised_learning_ogbg}\n\\vspace{-5mm}\n\\end{table}\nTables~\\ref{tab:unsupervised_learning_ogbg} %and \\ref{tab:unsupervised_learning_social} \nshow the results for unsupervised graph level property prediction in social and chemical domains respectively.  %a random edge drop augmentation strategy using GCL. \nWe witness the big performance gain of AD-GCL as opposed to all baselines across all the datasets. Note GraphCL utilizes extensive evaluation to select the best combination of augmentions over a broad GDA family including node-dropping, edge dropping and subgraph sampling. Our results indicate that such extensive evaluation may not be necessary while optimizing  the augmentation strategy in an adversarial way is greatly beneficial.\n\nWe stress that edge dropping is not cherry picked as the search space of augmentation strategies. Other search spaces may even achieve better performance, while an extensive investigation is left for the future work. \n\nMoreover, AD-GCL also clearly improves upon the performance against its non-adversarial counterparts (NAD-GCL) across all the datasets, which further demonstrates stable and significant advantages of the AD-GCL principle. Essentially, the input-graph-dependent augmentation learnt by AD-GCL yields much benefit.\nFinally, we compare AD-GCL-FIX with AD-GCL-OPT. %The take-away is that, %our framework with a large $\\lambda_{\\text{reg}}$ is comparable to the optimized $\\lambda_{\\text{reg}}$ version \nInterestingly, two methods achieve comparable results though AD-GCL-OPT is sometimes better. This observation implies that the AD-GCL principle may be robust to the choice of $\\lambda_{\\text{reg}}$ and thus motivates the analysis in the next subsection. Moreover, weak information from the downstream tasks indeed help with controlling the search space and further betters the performance. We also list the optimal $\\lambda_{\\text{reg}}$'s of AD-GCL-OPT for different datasets in Appendix~\\ref{apd:more_results_opt_reg} for the purpose of comparison and reproduction.\n\\subsubsection{Note on the linear downstream classifier}\nWe find that the choice of the downstream classifier can significantly affect the evaluation of the self-supervised representations. InfoGraph~\\cite{sun2019infograph} and GraphCL~\\cite{you2020graph} adopt a non-linear SVM model as the downstream classifier. Such a non-linear model is more powerful than the linear model we adopt and thus causes some performance gap between the results showed in Table~\\ref{tab:unsupervised_learning_ogbg} (TOP) and (BOTTOM) and their original results (listed in Appendix~\\ref{apd:unsup_nonlinear} as Table~\\ref{tab:unsup_tu_non_linear_eval_results}). We argue that using a non-linear SVM model as the downstream classifier is unfair, because the performance of even a randomly initialized untrained GIN (RU-GIN) is significantly improved  (comparing results from Table~\\ref{tab:unsupervised_learning_ogbg} (TOP) to Table~\\ref{tab:unsup_tu_non_linear_eval_results} ). Therefore, we argue for adopting a linear classifier protocol as suggested by~\\cite{tschannen2019mutual}.\nThat having been said, our methods (both AD-GCL-FIX and AD-GCL-OPT) still performs significantly better than baselines in most cases, even when a non-linear SVM classifer is adopted, as shown in Table~\\ref{tab:unsup_tu_non_linear_eval_results}. Several relative gains are there no matter whether the downstream classifier is a simple linear model (Tables~\\ref{tab:unsupervised_learning_ogbg}) or a non-linear SVM model (Table~\\ref{tab:unsup_tu_non_linear_eval_results}). AD-GCL methods significantly outperform InfoGraph in 5 over 8 datasets and GraphCL in 6 over 8 datasets. This further provides the evidence for the effectiveness of our method. Details on the practical benefits of linear downstream models can be found in Appendix~\\ref{apd:unsup_nonlinear}.\n\n\\vspace{-3mm}\n\\subsection{Analysis of Regularizing the GDA Model}\n\\vspace{-2mm}\n\\label{exp:reg_analysis}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/reg_analysis.pdf}\n    \\caption{\\small{(a) $\\lambda_{\\text{reg}}$ \\textit{v.s.} expected edge drop ratio $\\mathbb{E}_\\mathcal{G}[\\sum_{e} \\omega_e/|E|]$ (measured at saddle point of Eq.\\ref{eq:ad-gcl-encoder-augmentor-reg}). (b) Training dynamics of expected drop ratio for $\\lambda_{\\text{reg}}$. (c) Validation performance for graph classification \\textit{v.s.} edge drop ratio. Compare AD-GCL and GCL with non-adversarial edge dropping. The markers on AD-GCL's performance curves show the $\\lambda_{\\text{reg}}$ used.}}% \\sus{make in high dpi setting for making legible }}\n    \\label{fig:reg_term_analysis}\n    \\vspace{-4mm}\n\\end{figure}\n\nHere, we study how different $\\lambda_{\\text{reg}}$'s impact the expected edge drop ratio of AD-GCL at the saddle point of Eq.\\ref{eq:ad-gcl-encoder-augmentor-reg} and further impact the model performance on the validation datasets. Due to the page limitation, we focus on classification tasks in the main text while leaving the discussion on regression tasks in the Appendix~\\ref{apd:more_results_reg_analysis}. Figure~\\ref{fig:reg_term_analysis} %and~\\ref{fig:perf_vs_edge_drop} \nshows the results.\n\nAs shown in Figure~\\ref{fig:reg_term_analysis}(a), a large $\\lambda_{\\text{reg}}$ tends to yield a small expected edge drop ratio at the convergent point, which matches our expectation. $\\lambda_{\\text{reg}}$ ranging from 0.1 to 10.0 corresponds to dropping almost everything (80\\% edges) to nothing (<10\\% edges). The validation performance in %Figure~\\ref{fig:perf_vs_edge_drop} \nFigure~\\ref{fig:reg_term_analysis}(c) is out of our expectation. We find that for classification tasks, the performance of the encoder is extremely robust to different choices of $\\lambda_{\\text{reg}}$'s when trained w.r.t. the AD-GCL principle, though the edge drop ratios at the saddle point are very different. However, the non-adversarial counterpart NAD-GCL is sensitive to different edge drop ratios, especially on the molecule dataset (e.g., ogbg-molclitox, ogbg-molbbbp). We actually observe the similar issue of NAD-GCL across all molecule datasets (See Appendix~\\ref{apd:more_results_perf_vs_edge_drop}). More interesting aspects of our results appear at the extreme cases. When $\\lambda_{\\text{reg}}\\geq 5.0$, the convergent edge drop ratio is close to 0, which means no edge dropping, but AD-GCL still significantly outperforms naive GCL with small edge drop ratio. When $\\lambda_{\\text{reg}}=0.3$, the convergent edge drop ratio is greater than 0.6, which means dropping more than half of the edges, but AD-GCL still keeps reasonable performance. We suspect that such benefit comes from the training dynamics of AD-GCL (examples as shown in Figure~\\ref{fig:reg_term_analysis}(b)). Particularly, optimizing augmentations allows for non-uniform edge-dropping probability. During the optimization procedure, AD-GCL pushes high drop probability on redundant edges while low drop probability on critical edges, which allows the encoder to differentiate redundant and critical information. This cannot be fully explained by the final convergent edge drop ratio % may not tell the entire story. \n and motivates future investigation of AD-GCL from a more in-depth theoretical perspective. \n\\vspace{-3mm}\n\\subsection{Transfer Learning}\n\\vspace{-2mm}\n\\label{exp:transfer}\n\\begin{table}[t]\n\\centering\n\\renewcommand{\\arraystretch}{1.25}\n\\newcommand{\\STAB}[1]{\\begin{tabular}{@{}c@{}}#1\\end{tabular}}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{lcccccccc|c}\n\\hline\nPre-Train Dataset   & \\multicolumn{8}{c|}{\\begin{tabular}[c]{@{}c@{}} ZINC 2M\\end{tabular}}                                                                                                                              & \\begin{tabular}[c]{@{}c@{}}PPI-306K\\end{tabular} \\\\ \\hline\nFine-Tune Dataset    & BBBP                                & Tox21                                & SIDER                                & ClinTox                              & BACE                                 & \\multicolumn{1}{c}{HIV} & MUV              & ToxCast & PPI                                                                                       \\\\ \\hline\nNo Pre-Train & 65.8 $\\pm$ 4.5                      & 74.0 $\\pm$ 0.8                       & 57.3 $\\pm$ 1.6                       & 58.0 $\\pm$ 4.4                       & 70.1 $\\pm$ 5.4                       & 75.3 $\\pm$ 1.9          & 71.8 $\\pm$ 2.5   &  63.4 $\\pm$ 0.6 & 64.8 $\\pm$ 1.0                                                                            \\\\\nEdgePred~\\cite{hu2019strategies}     & 67.3 $\\pm$ 2.4                      & 76.0 $\\pm$ 0.6                       & 60.4 $\\pm$ 0.7                       & 64.1 $\\pm$ 3.7                       & 79.9 $\\pm$ 0.9                       & 76.3 $\\pm$ 1.0          & 74.1 $\\pm$ 2.1   & 64.1$\\pm $ 0.6 & 65.7 $\\pm$ 1.3                                                                            \\\\\nAttrMasking~\\cite{hu2019strategies}  & 64.3 $\\pm$ 2.8                      & 76.7 $\\pm$ 0.4                       & 61.0 $\\pm$ 0.7                       & 71.8 $\\pm$ 4.1                       & 79.3 $\\pm$ 1.6                       & 77.2 $\\pm$ 1.1          & 74.7 $\\pm$ 1.4   & 64.2 $\\pm$ 0.5 & 65.2 $\\pm$ 1.6                                                                            \\\\\nContextPred~\\cite{hu2019strategies}  & 68.0 $\\pm$ 2.0                      & 75.7 $\\pm$ 0.7                       & 60.9 $\\pm$ 0.6                       & 65.9 $\\pm$ 3.8                       & 79.6 $\\pm$ 1.2                       & 77.3 $\\pm$ 1.0          & 75.8 $\\pm$ 1.7   & 63.9 $\\pm$ 0.6& 64.4 $\\pm$ 1.3                                                                            \\\\\nInfoGraph~\\cite{sun2019infograph}    & 68.8 $\\pm$ 0.8                      & 75.3 $\\pm$ 0.5                       & 58.4 $\\pm$ 0.8                       & 69.9 $\\pm$3.0                        & 75.9 $\\pm$ 1.6                       & 76.0 $\\pm$ 0.7          & 75.3 $\\pm$ 2.5   & 62.7 $\\pm$ 0.4 & 64.1 $\\pm$ 1.5                                                                            \\\\\nGraphCL~\\cite{you2020graph}      & 69.68 $\\pm$ 0.67                    & 73.87 $\\pm$ 0.66                     & 60.53 $\\pm$ 0.88                     & 75.99 $\\pm$ 2.65                     & 75.38 $\\pm$ 1.44                     & 78.47 $\\pm$ 1.22        & 69.8 $\\pm$ 2.66  &  62.40 $\\pm$ 0.57 & 67.88 $\\pm$ 0.85                                                                          \\\\ \\hline\\hline\nAD-GCL-FIX         & \\multicolumn{1}{l}{70.01 $\\pm$1.07} & \\multicolumn{1}{l}{76.54 $\\pm$ 0.82} & \\multicolumn{1}{l}{\\textbf{63.28 $\\pm$ 0.79}} & \\multicolumn{1}{l}{\\textbf{79.78 $\\pm$ 3.52}} & \\multicolumn{1}{l}{78.51 $\\pm$ 0.80} & 78.28 $\\pm$ 0.97        & 72.30 $\\pm$ 1.61 & 63.07 $\\pm$ 0.72 & \\textbf{68.83 $\\pm$ 1.26} \\\\\n\\centering{Our Ranks} & 1 & 2 & 1 & 1 & 4 & 2 & 5 & 5 &  1 \\\\\n\\hline\\hline\n\\end{tabular}%\n}\n\\caption{\\small{Transfer learning performance for chemical molecules property prediction (mean ROC-AUC $\\pm$ std. over  10 runs). \\textbf{Bold} indicates our methods outperform baselines with $\\geq$ 0.5 std..} }%Compared numbers are from~\\cite{you2020graph, hu2019strategies}.}\n\\vspace{-5mm}\n\\label{tab:transfer_learning}\n\\end{table}Next, we evaluate the GNN encoders trained by AD-GCL on transfer learning to predict chemical molecule properties and biological protein functions. We follow the setting in~\\cite{hu2019strategies} and use the same datasets: GNNs are pre-trained on one dataset using self-supervised learning and later fine-tuned on another dataset to test out-of-distribution performance. Here, we only consider AD-GCL-FIX as AD-GCL-OPT is only expected to have better performance. We adopt baselines including no pre-trained GIN (\\textit{i.e.,} without self-supervised training on the first dataset and with only fine-tuning), InfoGraph~\\cite{sun2019infograph}, GraphCL~\\cite{you2020graph}, three different pre-train strategies in~\\cite{hu2019strategies} including edge prediction, node attribute masking and context prediction that utilize edge, node and subgraph context respectively. More detailed setup is given in Appendix~\\ref{apd:exp_settings}. \n\nAccording to Table~\\ref{tab:transfer_learning}, AD-GCL-FIX significantly outperforms baselines in 3 out of 9 datasets and achieves a mean rank of 2.4 across these 9 datasets which is better than all baselines. Note that although AD-GCL only achieves 5th on some datasets, AD-GCL still significantly outperforms InfoGraph~\\cite{sun2019infograph} and GraphCL~\\cite{you2020graph}, both of which are strong GNN self-training baselines. In contrast to InfoGraph~\\cite{sun2019infograph} and GraphCL~\\cite{you2020graph}, AD-GCL achieves some performance much closer to those baselines (EdgePred, AttrMasking and ContextPred) based on domain knowledge and extensive evaluation in~\\cite{hu2019strategies}. This is rather significant as our method utilizes only edge dropping GDA, which again shows the effectiveness of the AD-GCL principle.\n\n\\vspace{-2mm}\n\\subsection{Semi-Supervised Learning}\n\\vspace{-1mm}\n\\label{exp:semisup}\n\\begin{table}[t]\n\\centering\n\\renewcommand{\\arraystretch}{1.25}\n\\resizebox{0.8\\textwidth}{!}{%\n\\begin{tabular}{lccc|ccc}\n\\hline\nDataset            & NCI1             & PROTEINS         & DD               & COLLAB           & RDT-B            & RDT-M5K          \\\\ \\hline\nNo Pre-Train      & 73.72 $\\pm$ 0.24 & 70.40 $\\pm$ 1.54 & 73.56 $\\pm$ 0.41 & 73.71$\\pm$ 0.27  & 86.63 $\\pm$ 0.27 & 51.33 $\\pm$ 0.44 \\\\\nSS-GCN-A & 73.59 $\\pm$ 0.32 & 70.29 $\\pm$ 0.64 & 74.30 $\\pm$ 0.81 & 74.19 $\\pm$ 0.13 & 87.74 $\\pm$ 0.39 & 52.01 $\\pm$ 0.20 \\\\\nGAE~\\cite{kipf2016variational}           & 74.36 $\\pm$ 0.24 & 70.51 $\\pm$ 0.17 & 74.54 $\\pm$ 0.68 & 75.09 $\\pm$ 0.19 & 87.69 $\\pm$ 0.40 & 53.58 $\\pm$ 0.13 \\\\\nInfoGraph~\\cite{sun2019infograph}     & 74.86 $\\pm$ 0.26 & 72.27 $\\pm$ 0.40 & 75.78 $\\pm$ 0.34 & 73.76 $\\pm$ 0.29 & 88.66 $\\pm$ 0.95 & 53.61 $\\pm$ 0.31 \\\\\nGraphCL~\\cite{you2020graph}      & 74.63 $\\pm$ 0.25 & 74.17 $\\pm$ 0.34 & 76.17 $\\pm$ 1.37 & 74.23 $\\pm$ 0.21 & 89.11 $\\pm$ 0.19 & 52.55 $\\pm$ 0.45 \\\\ \\hline\nAD-GCL-FIX        & \\textbf{75.18 $\\pm$ 0.31} &  73.96 $\\pm$ 0.47 & \\textbf{77.91 $\\pm$ 0.73}$^\\star$ & \\textbf{75.82 $\\pm$ 0.26}$^\\star$ & \\textbf{90.10 $\\pm$ 0.15}$^\\star$ & 53.49 $\\pm$ 0.28 \\\\ \nOur Ranks & 1 &  2  & 1 & 1 & 1  &  3 \\\\\n\\hline\n\\end{tabular}%\n}\n\\caption{\\small{Semi-supervised learning performance with 10\\% labels on TU datasets~\\cite{Morris+2020} (10-Fold Accuracy (\\%)$\\pm$ std over 5 runs). \\textbf{Bold}/\\textbf{Bold}$^\\star$ indicate our methods outperform baselines with $\\geq$ 0.5 std/ $\\geq$ 2 std respectively.} } %Compared number from~\\cite{you2020graph}.}\n\\label{tab:semi_supervised_learning}\n\\vspace{-5mm}\n\\end{table}Lastly, we evaluate AD-GCL on semi-supervised learning for graph classification on the benchmark TU datasets~\\cite{Morris+2020}. We follow the setting in \\cite{you2020graph}:  GNNs are pre-trained on one dataset using self-supervised learning and later fine-tuned based on 10\\% label supervision on the same dataset.  Again, we only consider AD-GCL-FIX and compare it with several baselines in \\cite{you2020graph}: 1) no pre-trained GCN, which is directly trained by the 10\\% labels from scratch, 2) SS-GCN-A, a baseline that introduces more labelled data by creating random augmentations and then gets trained from scratch, 3) a predictive method GAE~\\cite{kipf2016variational} that utilizes adjacency reconstruction in the pre-training phase, and GCL methods, 4) InfoGraph~\\cite{sun2019infograph} and 5) GraphCL~\\cite{you2020graph}. Note that here we have to keep the encoder architecture same and thus  AD-GCL-FIX adopts GCN as the encoder. Table~\\ref{tab:semi_supervised_learning} shows the results. AD-GCL-FIX significantly outperforms baselines in 3 out of 6 datasets and achieves a mean rank of 1.5 across these 6 datasets, which again demonstrates the strength of AD-GCL. \\vspace{-3mm}\n\\section{Conclusions}\n\\vspace{-2mm}\nIn this work we have developed a theoretically motivated, novel principle: \\textit{AD-GCL} that goes a step beyond the conventional InfoMax objective for self-supervised learning of GNNs. The optimal GNN encoders that are agnostic to the downstream tasks are the ones that capture the minimal sufficient information to identify each graph in the dataset. To achieve this goal, AD-GCL suggests to better graph contrastive learning via optimizing graph augmentations in an adversarial way.  Following this principle, we developed a practical instantiation based on learnable edge dropping. We have extensively analyzed and demonstrated the benefits of AD-GCL and its instantiation with real-world datasets for graph property prediction in unsupervised, transfer and semi-supervised learning settings.\n\n\\begin{ack}\nWe greatly thank the actionable suggestions given by reviewers and the area chair.\nS.S. and J.N. are supported by the National Science Foundation under contract numbers CCF-1918483 and IIS-1618690. P.L. is partly supported by the 2021 JP Morgan Faculty Award and the National Science Foundation (NSF) award HDR-2117997.\n\\end{ack}\n\n\\newpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{AutoGCL: Automated Graph Contrastive Learning via \\\\ Learnable View Generators}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nContrastive learning has been widely applied to graph representation learning, where the view generators play a vital role in generating effective contrastive samples. Most of the existing contrastive learning methods employ pre-defined view generation methods, e.g., node drop or edge perturbation, which usually cannot adapt to input data or preserve the original semantic structures well. To address this issue, we propose a novel framework named \\emph{\\underline{Auto}mated \\underline{G}raph \\underline{C}ontrastive \\underline{L}earning} (AutoGCL) in this paper. Specifically, AutoGCL employs a set of learnable graph view generators orchestrated by an auto augmentation strategy, where every graph view generator learns a probability distribution of graphs conditioned by the input. While the graph view generators in AutoGCL preserve the most representative structures of the original graph in generation of every contrastive sample, the auto augmentation learns policies to introduce adequate augmentation variances in the whole contrastive learning procedure. Furthermore, AutoGCL adopts a joint training strategy to train the learnable view generators, the graph encoder, and the classifier in an end-to-end manner, resulting in topological heterogeneity yet semantic similarity in the generation of contrastive samples. Extensive experiments on semi-supervised learning, unsupervised learning, and transfer learning demonstrate the superiority of our AutoGCL framework over the state-of-the-arts in graph contrastive learning. In addition, the visualization results further confirm that the learnable view generators can deliver more compact and semantically meaningful contrastive samples compared against the existing view generation methods. Our code is available at \\url{https://github.com/Somedaywilldo/AutoGCL}.\n\\end{abstract}\n\n\\section{Introduction}\n\nGraph neural networks (GNNs)~\\cite{kipf2016gcn,velivckovic2017gat,xu2018gin,hamilton2017graphsage} are gaining increasing attention in the realm of graph representation learning. By generally following a recursive neighborhood aggregation scheme, GNNs have shown impressive representational power in various domains, such as point clouds~\\cite{shi2020pointgnn}, social networks~\\cite{fan2019gnnsocial}, chemical analysis~\\cite{de2018molgan}, and so on. Most of the existing GNN models are trained in an end-to-end supervised fashion, which relies on a high volume of fine-annotated data. However, labeling graph data requests a huge amount of effort from professional annotators with domain knowledge. To alleviate this issue, GAE~\\cite{kipf2016vgae} and GraphSAGE~\\cite{hamilton2017graphsage} have been proposed to exploit a naive unsupervised pretraining strategy that reconstructs the vertex adjacency information. Some recent works~\\cite{hu2019pretraingnn,you2020sslgcn} introduce self-supervised pretraining strategies to GNNs which further improve the generalization performance. \n\nMore recently, with developments of contrastive multi-view learning in computer vision \\cite{he2020moco, chen2020simclr,tian2019cmc} and natural language processing \\cite{yang2019xlnet,logeswaran2018efficient}, some self-supervised pretraining approaches perform as good as (or even better than) supervised methods.\nIn general, contrastive methods generate training views using data augmentations, where views of the same (positive pairs) input are concentrated in the representation space with~% through mutual information (MI) maximization,\nviews of different inputs (negative pairs) pushed apart. %By following this paradigm in graph learning, \nTo work on graphs, DGI \\cite{velivckovic2018dgi} has been proposed to treat both graph-level and node-level representations of the same graph as positive pairs, pursuing consistent representations from local and global features. CMRLG \\cite{hassani2020cmcgnn} achieves a similar goal by grouping adjacency matrix (local features) and its diffusion matrix (global features) as positive pairs. GCA \\cite{zhu2020gca} generates the positive view pairs through sub-graph sampling with the structure priors with node attributes randomly masked. GraphCL~\\cite{you2020graphcl} offers even more strategies for augmentations, such as node dropping and edge perturbation. While above attempts incorporate contrastive learning into graphs, they usually fail to generate views with respect to the semantic of original graphs or adapt augmentation policies to specific graph learning tasks.\n\nBlessed by the invariance of image semantics under various transformation, image data augmentation has been widely used~\\cite{cubuk2019autoaugment} to generative contrastive views. However, the use of graph data augmentation might be ineffective here, as transformations on a graph might severely disrupt its semantics and properties for learning. In the meanwhile, InfoMin~\\cite{tian2020goodview} improves contrastive learning for vision tasks and proposes to replace image data augmentation with a flow-based generative model for contrastive views generation. Thus, learning a probability distribution of contrastive views conditioned by an input graph might be an alternative to simple data augmentation for graph contrastive learning but still requests non-trivial efforts, as the performance and scalability of common graph generative models are poor in real-world scenarios. \n\n\\begin{table}[!tb]\n    \\caption{\n    An overview of graph augmentation methods. The explanation of these properties can be found in Section \\ref{sec-insight}.\n    }\n    \\vspace{-0.3cm}\n    \\begin{minipage}{1\\linewidth}\n    \\centering\n    \\resizebox{1\\hsize}{!}{\n        \\begin{tabular}{cccccccc}\n        \\hline\n        Property           & CMRLG          & GRACE         & GraphCL       & GCA           & JOAO          & AD-GCL        & Ours \\\\\n        \\hline\n        Topological        & $\\checkmark$   & $\\checkmark$  & $\\checkmark$  & $\\checkmark$  & $\\checkmark$  & $\\checkmark$  & $\\checkmark$  \\\\\n        Node  Feature      & -              & $\\checkmark$  & $\\checkmark$  & $\\checkmark$  & $\\checkmark$  & -             & $\\checkmark$  \\\\\n        Label-preserving   & -              & -             & -             & -             & -             & -             & $\\checkmark$  \\\\\n        Adaptive           & -              & -             & -             & $\\checkmark$  & $\\checkmark$  & $\\checkmark$  & $\\checkmark$  \\\\\n        Variance           & -              & $\\checkmark$  & $\\checkmark$  & $\\checkmark$  & $\\checkmark$  & $\\checkmark$  & $\\checkmark$  \\\\\n        Differentiable     & -              & -             & -             & -             & -             & $\\checkmark$  & $\\checkmark$  \\\\\n        Efficient   BP     & -              & -             & -             & -             & -             & $\\checkmark$  & $\\checkmark$  \\\\\n        \\hline\n        \\end{tabular}\n    }\n    \\end{minipage}\n\\label{tab-aug-compare}\n\\vspace{-0.6cm}\n\\end{table}\n\nIn this work, we propose a learnable graph view generation method, namely AutoGCL, to address above issues via learning a probability distribution over node-level augmentations. While the conventional pre-defined view generation methods, such as random dropout or graph node masking, may inevitably change the semantic labels of graphs and finally hurt contrastive learning, AutoGCL adapts to the input graph such that it can well preserve the semantic labels of the graph. In addition, thanks to the gumbel-softmax trick \\cite{jang2016gumbelsoftmax}, AutoGCL is end-to-end differentiable yet providing sufficient variances for contrastive samples generation. We further propose a joint training strategy to train the learnable view generators, the graph encoders, and the classifier in an end-to-end manner. The strategy includes the view similarity loss, the contrastive loss, and the classification loss. It makes the proposed view generators generate augmented graphs that have similar semantic information but with different topological properties. In Table \\ref{tab-aug-compare}, we summarize the properties of existing graph augmentation methods, where AutoGCL dominates in the comparisons.\n\nWe conduct extensive graph classification experiments using semi-supervised learning, unsupervised learning, and transfer learning tasks to evaluate the effectiveness of AutoGCL. The results show that AutoGCL improves the state-of-the-art graph contrastive learning performances on most of the datasets. In addition, we visualize the generated graphs on MNIST-Superpixel dataset \\cite{monti2017mnistsuperpix} and reveal that AutoGCL could better preserve semantic structures of the input data than existing pre-defined view generators.\n\nOur contributions can be summarized as follows.\n\\begin{itemize}\n    \\item We propose a graph contrastive learning framework with learnable graph view generators embedded into an auto augmentation strategy. To the best of our knowledge, this is the first work to build learnable generative node-wise augmentation policies for graph contrastive learning. \n    \n    \\item We propose a joint training strategy for training the graph view generators, the graph encoder, and the graph classifier under the context of graph contrastive learning in an end-to-end manner.\n    \n    \\item We extensively evaluate the proposed method on a variety of graph classification datasets with semi-supervised, unsupervised, and transfer learning settings. The t-SNE and view visualization results also demonstrate the effectiveness of our method.\n\\end{itemize}\n\n\\section{Related Work}\n\n\\subsection{Graph Neural Networks}\nDenote a graph as $g=(V, E)$ where the node features are $\\boldsymbol{x}_v$ for $v \\in V$. In this paper, we focus on the graph classification task using Graph Neural Networks (GNNs). GNNs generate node-level embedding $\\boldsymbol{h}_v$ through aggregating the node features $\\boldsymbol{x}_v$ of its neighbors. Each layer of GNNs serves as an iteration of aggregation, such that the node embedding after the $k$-th layers aggregates the information within its $k$-hop neighborhood. The $k$-th layer of GNNs can be formulated as\n\\begin{small}\n    \\begin{align}\n    \\boldsymbol{a}_v^{(k)} &= \\text{AGGREGATE}^{(k)}  ( \\{ \\boldsymbol{h}_u^{(k-1)} : u \\in \\mathcal{N}(v) \\} ) \\\\\n    \\boldsymbol{h}_v^{(k)} &= \\text{COMBINE}^{(k)} ( \\boldsymbol{h}_v^{(k-1)}, \\boldsymbol{a}_v^{(k)} )\n    \\end{align}\n\\end{small}\nFor the downstream tasks such as graph classification, the graph-level representation $\\boldsymbol{z}_g$ is obtained via the READOUT function and MLP layers as\n\\begin{small}\n    \\begin{align}\n    F(g) &= \\text{READOUT}(\\{ \\boldsymbol{h}_n^{(k)}: v_n \\in \\mathcal{V} \\} ) \\\\\n    \\boldsymbol{z}_g &= \\text{MLP} (  F(g) )\n    \\end{align}\n\\end{small}\nIn this work we follow the existing graph contrastive learning literature to employ two state-of-the-art GNNs, \\textit{i.e.}, GIN \\cite{xu2018gin} and ResGCN \\cite{chen2019gfn}, as our backbone GNNs. \n\n\\subsection{Pre-training Graph Neural Networks}\n\nPre-training GNNs on graph datasets still remains a challenging task, since the semantics of graphs are not straightforward, and the annotation of graphs (proteins, chemicals, etc.) usually requires professional domain knowledge. It is very costly to collect large-scale and fine-annotated graph datasets like ImageNet \\cite{krizhevsky2012imagenet}. An alternative way is to pre-train the GNNs in an unsupervised manner. The GAE \\cite{kipf2016vgae} first explored the unsupervised GNN pre-training by reconstructing the graph topological structure. GraphSAGE \\cite{hamilton2017graphsage} proposed an inductive way of unsupervised node embedding by learning the neighborhood aggregation function. Pretrain-GNN \\cite{hu2019pretraingnn} conducted the first systematic large-scale investigation of strategies for pre-training GNNs under the transfer learning setting. It proposed self-supervised pre-training strategies to learn both the local and global features of graphs. However, the benefits of graph transfer learning may be limited and lead to negative transfer \\cite{rosenstein2005negtransfer}, as the graphs from different domains actually differ a lot in their structures, scales and node/edge attributes. Therefore, many of the following works started to explore an alternative approach, \\textit{i.e.}, the contrastive learning, for GNNs pre-training.\n\n\\subsection{Contrastive Learning}\n\nIn recent years, contrastive learning (CL) has received considerable attention among the self-supervised learning approaches, and a series of CL methods including SimCLR \\cite{chen2020simclr} and MoCo-v2 \\cite{chen2020mocov2} even outperform the supervised baselines. Through minimizing the contrastive loss \\cite{hadsell2006clloss}, the views generated from the same input (\\textit{i.e.}, positive view pairs) are pulled close in the representation space, while the views of different inputs (\\textit{i.e.}, negative view pairs) are pushed apart. Most of the existing CL methods \\cite{he2020moco,zbontar2021barlowtwins,chen2020simclr, grill2020byol} generate views using data augmentation, which is still challenging and under-explored for the graph data. Instead of data augmentation, DGI \\cite{velivckovic2018dgi} treated the graph-level and node-level representations of the same graph as positive view pairs. CMRLG \\cite{hassani2020cmcgnn} achieved an analogical goal by treating the adjacency matrix and the diffusion matrix as positive pairs. More recently, the GraphCL framework \\cite{you2020graphcl} employed four types of graph augmentations, including node dropping, edge perturbation\\footref{foot-edge-perturb}, sub-graph sampling\\footref{foot-subgraph}, and node attribute masking\\footref{foot-attr-mask}, enabling the most diverse augmentations by far for graph view generation. GCA \\cite{zhu2020gca} used sub-graph sampling and node attribute masking as augmentations and introduced a prior augmentation probability based on the node centrality measures, enabling more adaptiveness than GraphCL \\cite{you2020graphcl}, but the prior is not learnable.\n\n\\subsection{Learnable Data Augmentation}\n\nAs mentioned above, data augmentation is a significant component of CL. The existing literature \\cite{chen2020simclr,you2020graphcl} has revealed that the optimal augmentation policies are task-dependent and the choice of augmentations makes a considerable difference to the CL performance. The researchers have explored to automatically discover the optimal policy for image augmentations in the computer vision field. For instance, AutoAugment \\cite{cubuk2019autoaugment} firstly optimized the combination of augmentation functions through reinforcement learning. Faster-AA \\cite{hataya2020fasteraa} and DADA \\cite{li2020dada} proposed a differentiable augmentation optimization framework following the DARTS \\cite{liu2018darts} style. \n\nHowever, the learnable data augmentation methods are barely explored for CL except the InfoMin framework \\cite{tian2020goodview}, which claims that good views of CL should maintain the label information as well as minimizing the mutual information of positive view pairs. InfoMin employs a flow-based generative model as the view generator for data augmentation and trains the view generator in a semi-supervised manner. However, transferring this idea to graph is a non-trivial task since current graph generative models are either of limited generation qualities \\cite{kipf2016vgae} or designed for specific tasks such as the molecular data \\cite{de2018molgan, madhawa2019graphnvp, wang2021molecular}. To make graph augmentations adaptive to different tasks, JOAO \\cite{you2021joao} learns the sampling distribution of pre-defined augmentations in a Bayesian manner, but the augmentations themselves are still not learnable. AD-GCL \\cite{suresh2021adgcl} first proposed a learnable edge dropping augmentation and employs adversarial training strategy, but node-level augmentations are not considered, and the strategy will not ensure to generate label-preserving augmentations.\n\nIn this work we build a learnable graph view generator that learns a probability distribution over the node-level augmentations. Compared to the existing graph CL methods, our method well preserves the semantic structures of original graphs. Moreover, it is end-to-end differentiable and can be efficiently trained. \n\n\\section{Methodology}\n\n\\subsection{What Makes a Good Graph View Generator?}\n\\label{sec-insight}\n\nOur goal is to design a learnable graph view generator that learns to generate the augmented graph view in data-driven manner. Although various graph data augmentation methods have been proposed, there is less discussion on what makes a good graph view generator? From our perspective, an ideal graph view generator for data augmentation and contrastive learning should satisfy the following properties: (1) It supports both the augmentations of the graph \\textbf{topology} and the \\textbf{node feature}. (2) It is \\textbf{label-preserving}, \\textit{i.e.}, the augmented graph should maintain the semantic information in the original graph.\n(3) It is \\textbf{adaptive} to different data distributions and scalable to large graphs. (4) It provides \\textbf{sufficient variances} for contrastive multi-view pre-training. (5) It is \\textbf{end-to-end differentiable} and \\textbf{efficient} enough for fast gradient computation via \\textbf{back-propagation (BP)}.\n\nHere we provide an overview of the augmentation methods proposed in existing literature of graph contrastive learning in Table \\ref{tab-aug-compare}. CMRLG \\cite{hassani2020cmcgnn} applies diffusion kernel to get different topological structures. GRACE \\cite{zhu2020grace} uses random edge dropping and node attribute masking\\footnote{Randomly mask the attributes of certain ratio of nodes. \\label{foot-attr-mask}}. GCA \\cite{zhu2020gca} uses node dropping and node attribute masking along with a structural prior. GraphCL \\cite{you2020graphcl} proposes the most flexible set of graph data augmentations so far, including node dropping, edge perturbation\\footnote{Randomly replace certain ratio of edges with random edges. \\label{foot-edge-perturb}}, sub-graph\\footnote{Randomly select a connected subgraph of certain size. \\label{foot-subgraph}}, and attribute masking\\footref{foot-attr-mask}. We provide a detailed ablation study and analysis of GraphCL augmentations with different augmentation ratios in Section 1.1 of the supplementary. JOAO \\cite{you2021joao} optimizes the augmentation sampling policy of GraphCL in a Bayesian manner. AD-GCL \\cite{suresh2021adgcl} designs a learnable edge dropping augmentation.\n\nIn this work, we propose a learnable view generator to address all the above issues. Our view generator includes both augmentations of node dropping and attribute masking, but it is much more flexible since both two augmentations can be simultaneously employed in a node-wise manner, without the need of tuning the \\emph{``aug ratio''}. Besides the concern of model performance, another reason for not incorporating edge perturbation in our view generator is, the generation of edges through the learnable methods (\\textit{e.g.}, VGAE \\cite{kipf2016vgae}) requires to predict the full adjacency matrix that contains $O(N^2)$ elements, which is a heavy burden for back-propagation when dealing with large-scale graphs.\n\n\\subsection{Learnable Graph View Generator}\n\n\\begin{figure}[t]\n    \\begin{center}\n    \\includegraphics[width=1\\linewidth]{figures/fig-view-generator.pdf}\n    \\end{center}\n    \\vspace{-0.5cm}\n    \\caption{The architecture of our learnable graph view generator. The GNN layers embed the original graph to generate a distribution for each node. The augmentation choice of each node is sampled from it using the gumbel-softmax.\n    }%, and guides the generation of the augmented graph view. }\n    \\vspace{-0.5cm}\n    \\label{fig-view-generator}\n\\end{figure}\n\nFigure \\ref{fig-view-generator} illustrates the scheme of our proposed learnable graph view generator. We use GIN \\cite{xu2018gin} layers to get the node embedding from the node attribute. For each node, we use the embedded node feature to predict the probability of selecting a certain augment operation. The augmentation pool for each node is drop, keep, and mask. We employ the gumbel-softamx \\cite{jang2016gumbelsoftmax} to sample from these probabilities then assign an augmentation operation to each node. Formally, if we use $k$ GIN layers as the embedding layer, we denote $\\boldsymbol{h}_v^{(k)}$ as the hidden state of node $v$ at the $k$-th layer and $\\boldsymbol{a}_v^{(k)}$ as the embedding of node $v$ after the $k$-th layer. For node $v$, we have the node feature $\\boldsymbol{x}_v $, the augmentation choice $f_v$, and the function $\\text{Aug}(\\boldsymbol{x}, f)$ for applying the augmentation. Then the augmented feature $\\boldsymbol{x}_v^{'}$ of node $v$ is obtained via\n\\begin{small}\n    \\begin{align}\n        \\boldsymbol{h}_v^{(k-1)} &= \\text{COMBINE}^{(k)} ( \\boldsymbol{h}_v^{(k-2)}, \\boldsymbol{a}_v^{(k-1)} ) \\\\\n        \\boldsymbol{a}_v^{(k)} &= \\text{AGGREGATE}^{(k)}  ( \\{ \\boldsymbol{h}_u^{(k-1)} : u \\in \\mathcal{N}(v) \\} ) \\\\\n        f_v &= \\text{GumbelSoftmax} ( \\boldsymbol{a}_v^{(k)} ) \\\\\n        \\boldsymbol{x}_v^{'} &=  \\text{Aug}(\\boldsymbol{x}_v, f_v)\n    \\end{align}\n\\end{small}\nThe dimension of the last layer k is set as the same number of possible augmentations for each node. $\\boldsymbol{a}_v^{(k)}$ denotes the probability distribution for selecting each kind of augmentation. $f_v$ is a one-hot vector sampled from this distribution via gumbel-softmax and it is differentiable due to the reparameterization trick. The augmentation applying function $\\text{Aug}(\\boldsymbol{x}_v, f_v)$ combines the node attribute $\\boldsymbol{x}_v$ and $f_v$ using differentiable operations (e.g. multiplication), so the gradients of the weights of the view generator are kept in the augmented node features and can be computed using back-propagation. For the augmented graph, the edge table is updated using $f_v$ for all $v\\in V$, where the edges connected to any dropped nodes are removed. As the edge table is only the guidance for node feature aggregation and it does not participate in the gradient computation, it does not need to be updated in a differentiable manner. Therefore, our view generator is end-to-end differentiable. The GIN embedding layers and the gumbel-softmax can be efficiently scaled up for larger graph datasets and more augmentation choices.\n\n\\subsection{Contrastive Pre-training Strategy}\n\nSince the contrastive learning requires multiple views to form a positive view pair, we have two view generators and one classifier for our framework. According to InfoMin principle \\cite{tian2020goodview}, a good positive view pair for contrastive learning should maximize the label-related information as well as minimizing the mutual information (similarity) between them. To achieve that, our framework uses two separate graph view generators and trains them and the classifier in a joint manner. \n\n\\begin{figure*}[t]\n    \\begin{center}\n    \\includegraphics[width=1\\textwidth]{figures/fig-framework.pdf}\n    \\end{center}\n    \\vspace{-0.6cm}\n    \\caption{The proposed AutoGCL framework is composed of three parts: (1) two \\emph{view generators} that generate different views of the original graph, (2) a \\emph{graph encoder} that extracts the features of graphs and (3) a \\emph{classifier} that provides the graph outputs.}\n    \\vspace{-0.6cm}\n    \\label{fig-framework}\n\\end{figure*}\n\n\\subsubsection{Loss Function Definition}\n\nHere we define three loss functions, contrastive loss $\\mathcal{L}_{\\text{cl}}$, similarity loss $\\mathcal{L}_{\\text{sim}}$, and classification loss $\\mathcal{L}_{\\text{cls}}$. For contrastive loss, we follow the previous works \\cite{chen2020simclr,you2020graphcl} and use the normalized temperature-scaled cross entropy loss (NT-XEnt) \\cite{sohn2016clloss}. Define the similarity function $\\text{sim}(\\boldsymbol{z}_1,\\boldsymbol{z}_2)$ as\n\\begin{small}\n    \\begin{align}\n    \\text{sim}(\\boldsymbol{z}_1,\\boldsymbol{z}_2) = \\frac{\\boldsymbol{z}_1 \\cdot \\boldsymbol{z}_2}{ {\\lVert \\boldsymbol{z}_1 \\rVert}_2 \\cdot {\\lVert \\boldsymbol{z}_2 \\rVert}_2 }\n    \\end{align}\n\\end{small}\nSuppose we have a data batch made up of $N$ graphs. We pass the batch to the two view generators to obtain $2N$ graph views. We regard the two augmented views from the same input graph as the positive view pair. We use $\\mathbbm{1}_{[k \\neq i]} \\in \\{0, 1\\}$ to denote the indicator function. We denote the contrastive loss function for a positive pair of samples $(i, j)$ as $\\ell(i,j)$, the contrastive loss of this data batch as $\\mathcal{L}_{\\text{cl}}$, the temperature parameter as $\\tau$, then we have\n\\begin{small}\n    \\begin{align}\n    \\ell_{(i,j)} &= - \\log \\frac{\\exp ( \\text{sim} (\\boldsymbol{z}_i, \\boldsymbol{z}_j) / \\tau )}{ \\sum_{k=1}^{2N} \\mathbbm{1}_{[k \\neq i]} \\exp(\\text{sim} (\\boldsymbol{z}_i, \\boldsymbol{z}_k) / \\tau ) } \\\\\n    \\mathcal{L}_{\\text{cl}} &= \\frac{1}{2N} \\sum_{k=1}^{N} [\\ell(2k-1, 2k) + \\ell(2k, 2k-1)]\n    \\end{align}\n\\end{small}\nThe similarity loss is used to minimize the mutual information between the views generated by the two view generators. During the view generation process, we have a sampled state matrix $S$ indicting each node's corresponding augmentation operation (see Figure \\ref{fig-view-generator}). For a graph $G$, we denote the sampled augmentation choice matrix of each view generator as $A_1, A_2$, then we formulate the similarity loss $\\mathcal{L}_{\\text{sim}}$ as\n\\begin{small}\n    \\begin{align}\n    \\mathcal{L}_{\\text{sim}} = \\text{sim}(A_1, A_2)\n    \\end{align}\n\\end{small}\nFinally, for the classification loss, we directly use the cross entropy loss ($\\ell_\\text{cls}$). For a graph sample $g$ with class label $y$, we denote the augmented view as $g_1$ and $g_2$ and the classifier as $F$. Then the classification loss $\\mathcal{L}_{\\text{cls}}$ is formulated as\n\\begin{small}\n    \\begin{align}\n    \\mathcal{L}_{\\text{cls}} &= \\ell_\\text{cls}(F(g), y) + \\ell_\\text{cls}(F(g_1), y) + \\ell_\\text{cls}(F(g_2), y)\n    \\end{align}\n\\end{small}\n$\\mathcal{L}_{\\text{cls}}$ is employed in the semi-supervised pre-training task to encourage the view generator to generate label-preserving augmentations. \n\n\\subsubsection{Naive Training Strategy}\n\nFor unsupervised learning and transfer learning tasks, we use a naive training strategy (naive-strategy). Since we do not know the label in the pre-training stage, the $\\mathcal{L}_{\\text{sim}}$ is not used because it does not make sense to just encourage the views to be different without keeping the label-related information. This could lead to generating useless or even harmful view samples. We just train the view generators and the classifier jointly to minimize the $\\mathcal{L}_{\\text{cl}}$ in the pre-training stage. \n\nAlso, we note that the quality of the generated views will not be as good as the original data. During the $\\mathcal{L}_{\\text{cl}}$ minimization, instead of just minimizing the $\\mathcal{L}_{\\text{cl}}$ between two augmented views like GraphCL \\cite{you2020graphcl}, we also make use of the original data. By pulling the original data and the augmented views close in the embedding space, the view generator are encouraged to preserve the label-related information. The details are described in Algorithm \\ref{algo-naive}.\n\n\\setlength{\\textfloatsep}{10pt} % Remove \\textfloatsep\n\\begin{algorithm}[tb]\n\\begin{small}\n    \\caption{Naive training strategy (naive-strategy).}\n        \\begin{algorithmic}[1]\n            \\State Initialize weights of the two view generator $G_1$, $G_2$\n            \\State Initialize weights of the classifer $F$\n            \\While{not reached maximum epochs}\n                \\For{mini-batch $x$ from unlabeled data}\n                    \\State Get augmentation $x_1 = G_1(x), x_2 = G_2(x)$\n                    \\State Sample two views $v_1, v_2$ from $\\{x, x_1, x_2\\}$\n                    \\State $\\mathcal{L} = \\mathcal{L}_{\\text{cl}} (v_1, v_2)$\n                    \\State Update the weights of $G_1, G_2, F$ to minimize $\\mathcal{L}$\n                \\EndFor\n            \\EndWhile\n            \\While{not reached maximum epochs}\n                \\For{mini-batch $x$ from labeled data}\n                    \\State $\\mathcal{L} = \\mathcal{L}_{\\text{cls}}(x)$\n                    \\State Update the weights of $F$ to minimize $\\mathcal{L}$\n                \\EndFor\n            \\EndWhile\n        \\end{algorithmic}\n    \\label{algo-naive}\n\\end{small}\n\\vspace{-0.1cm}\n\\end{algorithm}\n\n\\subsubsection{Joint Training Strategy}\n\nFor semi-supervised learning tasks, we proposed a joint training strategy, performs contrastive training and supervised training alternately. This strategy generates label-preserving augmentation and outperforms the naive-strategy, the experiment results and detailed analysis is shown in Section \\ref{sec-semi-exp} and Section \\ref{sec-joint-strategy-analysis}.\n\nFor the joint-strategy, during the unsupervised training stage, we fix the view generators, and train the classifer by contrastive learning using unlabeled data. During the supervised training stage, we jointly train the view generator with the classifier using labeled data. By simultaneously optimizing $\\mathcal{L}_{\\text{sim}}$ and $\\mathcal{L}_{\\text{cls}}$, the two view generator are encouraged to generated label-preserving augmentations, yet being different enough from each other. The unsupervised training stage and supervised training stage are repeated alternately. This is very different from previous graph contrastive learning methods. Previous work like GraphCL \\cite{you2020graphcl} use the pre-training/fine-tuning strategy, which first minimizes the contrastive loss ($\\mathcal{L}_{\\text{cl}}$) until convergence using the unlabeled data and then fine-tunes it with the labeled data. \n\n\\begin{algorithm}[tb]\n    \\begin{small}\n        \\caption{Joint training strategy (joint-strategy).}\n        \\begin{algorithmic}[1]\n            \\State Initialize weights of $G_1$, $G_2$, $F$.\n            \\While{not reached maximum epochs}\n                \\For{mini-batch $x$ from unlabeled data}\n                    \\State Fix the weights of $G_1, G_2$\n                    \\State Get augmentation $x_1 = G_1(x), x_2 = G_2(x)$\n                    \\State Sample two views $v_1, v_2$ from $\\{x, x_1, x_2\\}$\n                    \\State $\\mathcal{L} = \\mathcal{L}_{\\text{cl}} (v_1, v_2)$\n                    \\State Update the weights of $F$ to minimize $\\mathcal{L}$\n                \\EndFor\n                \\For{mini-batch $x$ from labeled data}\n                    \\State Get augmentation $x_1 = G_1(x), x_2 = G_2(x)$\n                    \\State $\\mathcal{L} = \\mathcal{L}_{\\text{cls}}(x, x_1, x_2) + \\lambda \\cdot \\mathcal{L}_{\\text{sim}}(x_1, x_2)$\n                    \\State Update the weights of $G_1, G_2, F$ to minimize $\\mathcal{L}$\n                \\EndFor\n            \\EndWhile\n        \\end{algorithmic}\n        \\label{algo-joint}\n    \\end{small}\n\\vspace{-0.1cm}\n\\end{algorithm}\nHowever, we found that for graph contrastive learning, the pre-training/fine-tuning strategy are more likely to cause over-fitting in the fine-tuning stage. And minimizing the $\\mathcal{L}_{\\text{cl}}$ too much may have negative effect for the fine-tuning stage (see Section \\ref{sec-joint-strategy-analysis}). We speculate that minimizing the $\\mathcal{L}_{\\text{cl}}$ too much will push data points near the decision boundary to be too closed to each other, thus become more difficult the classifer to separate them. Because no matter how well we train the GNN classifer, there are still mis-classified samples due to the natural overlaps between the data distribution of different classes. But in the contrastive pre-training state, the classifer is not aware of whether the samples being pulled together are really from the same class.\n\n\\begin{table*}\n        \\caption{Comparison with the existing methods for unsupervised learning. The \\textbf{bold} numbers denote the best performance and the the numbers in \\textcolor{blue}{blue} represent the second best performance.}\n        \\vspace{-0.3cm}\n        \\fontsize{5}{5.5}\\selectfont\n        \\resizebox{1\\textwidth}{!}{\n            \\begin{tabular}{ccccccccc}\n            \\hline\n            Model   & MUTAG       & PROTEINS   & DD & NCI1       & COLLAB & IMDB-B     & REDDIT-B   & REDDIT-M-5K \\\\\n            \\hline\n            GL        & 81.662.11  & -          & -  & -          & -      & 65.870.98 & 77.340.18 & 41.010.17  \\\\\n            WL        & 80.723.00  & 72.920.56 & -  & 80.010.50 & -      & 72.303.44 & 68.820.41 & 46.060.21  \\\\\n            DGK       & 87.442.72  & 73.300.82 & -  & \\textcolor{blue}{80.310.46} & -      & 66.960.56 & 78.040.39 & 41.270.18  \\\\\n            \\hline\n            node2vec  & 72.6310.20 & 57.493.57 & -  & 54.891.61 & -      & -          & -          & -           \\\\\n            sub2vec   & 61.0515.80 & 53.035.55 & -  & 52.841.47 & -      & 55.261.54 & 71.480.41 & 36.680.42  \\\\\n            graph2vec & 83.159.25  & 73.302.05 & -  & 73.221.81 & -      & 71.100.54 & 75.781.03 & 47.860.26  \\\\\n            \\hline\n            InfoGraph       & \\textbf{89.011.13} & \\textcolor{blue}{74.440.31} & 72.851.78 & 76.201.06 & 70.651.13 & \\textcolor{blue}{73.030.87} & 82.501.42 & 53.461.03 \\\\\n            GraphCL  & 86.801.34 & 74.390.45 & \\textbf{78.620.40} & 77.870.41 & \\textcolor{blue}{71.361.15} & 71.140.44 & \\textbf{89.530.84} & \\textcolor{blue}{55.990.28} \\\\\n            JOAOv2  & - & 71.250.85 & 66.911.75 & 72.990.75 & 70.402.21 & 71.600.86 & 78.351.38 & 45.572.86 \\\\\n            AD-GCL  & - & 73.590.65 & 74.490.52 & 69.670.51 & \\textbf{73.320.61} & 71.571.01 & 85.520.79 & 53.000.82 \\\\\n            Ours & \\textcolor{blue}{88.641.08} & \\textbf{75.800.36} & \\textcolor{blue}{77.570.60} & \\textbf{82.000.29} & 70.120.68 & \\textbf{73.300.40} & \\textcolor{blue}{88.581.49} & \\textbf{56.750.18} \\\\\n            \\hline\n            \\end{tabular}\n        }\n    \\vspace{-0.3cm}\n    \\label{tab-unsup-exp}\n\\end{table*}    \n\n\\begin{table*}\n    \\caption{Comparison with the existing methods for transfer learning. The \\textbf{bold} numbers denote the best performance and the numbers in \\textcolor{blue}{blue} denote the second best performance.}\n    \\vspace{-0.3cm}\n    \\fontsize{5}{5.5}\\selectfont\n    \\resizebox{1\\textwidth}{!}{\n        \\begin{tabular}{lcccccccc}\n        \\hline\n        Model                 & BBBP       & Tox21      & ToxCast    & SIDER      & ClinTox    & MUV        & HIV        & BACE       \\\\\n        \\hline\n        \\emph{No  Pretrain}     & 65.84.5   & 74.00.8   & 63.40.6   & 57.31.6   & 58.04.4   & 71.82.5   & 75.31.9   & 70.15.4   \\\\ \\hline\n        Infomax                 & 68.80.8   & 75.30.5   & 62.70.4   & 58.40.8   & 69.93.0   & 75.32.5   & 76.00.7   & 75.91.6   \\\\\n        EdgePred                & 67.32.4   & 76.00.6   & \\textcolor{blue}{64.10.6}   & 60.40.7   & 64.13.7   & 74.12.1   & 76.31.0   & \\textcolor{blue}{79.90.9}   \\\\\n        AttrMasking             & 64.32.8   & \\textbf{76.70.4}   & \\textbf{64.20.5}   & 61.00.7   & 71.84.1   & 74.71.4   & 77.21.1   & 79.31.6   \\\\\n        ContextPred             & 68.02.0   & 75.70.7   & 63.90.6   & 60.90.6   & 65.93.8   & \\textcolor{blue}{75.81.7}   & 77.31.0   & 79.61.2   \\\\\n        GraphCL                 & 69.680.67 & 73.870.66 & 62.400.57 & 60.530.88 & 75.992.65 & 69.802.66 & \\textbf{78.471.22} & 75.381.44 \\\\\n        JOAOv2                 & \\textcolor{blue}{71.390.92} & 74.270.62 & 63.160.45 & 60.490.74 & 80.971.64 & 73.671.00 & 77.511.17 & 75.491.27 \\\\\n        AD-GCL                 & 70.011.07 & \\textcolor{blue}{76.540.82} & 63.070.72 & \\textbf{63.280.79} & \\textcolor{blue}{79.783.52} & 72.301.61 & 78.280.97 & 78.510.80 \\\\\n        Ours & \\textbf{73.360.77} & 75.690.29 & 63.470.38 & \\textcolor{blue}{62.510.63} & \\textbf{80.993.38} & \\textbf{75.831.30} & \\textcolor{blue}{78.350.64} & \\textbf{83.261.13} \\\\\n        \\hline\n        \\end{tabular}\n    }\n    \\vspace{-0.6cm}\n    \\label{tab-transfer-exp}\n\\end{table*}\n\nTherefore, we propose a new semi-supervised training strategy, namely the joint-strategy by alternately minimizing the $\\mathcal{L}_{\\text{cl}}$ and $\\mathcal{L}_{\\text{cls}} + \\mathcal{L}_{\\text{cls}}$. Minimizing $\\mathcal{L}_{\\text{cls}} + \\mathcal{L}_{\\text{cls}}$ is inspired by InfoMin \\cite{tian2020goodview}, so as to make the two view generator to keep label-related information while having less mutual information. \nHowever, since we only have a small portion of labeled data to train our view generator, it is still beneficial to use the original data just like the naive-strategy. Interestingly, since we need to minimize $\\mathcal{L}_{\\text{cls}}$ and $\\mathcal{L}_{\\text{sim}}$ simultaneously, a weight $\\lambda$ can be applied to better balance the optimization, but actually we found setting $\\lambda=1$ works pretty well during the experiments in Section \\ref{sec-sota}. The detailed training strategy is described in Algorithm \\ref{algo-joint}. And the overview of our whole framework is shown in Figure \\ref{fig-framework}.\n\n\\section{Experiment}\n\n\\subsection{Comparison with State-of-the-Art Methods}\n\\label{sec-sota}\n    \n\\subsubsection{Unsupervised Learning}\n\nFor the unsupervised graph classification task, we contrastively train a representation model using unlabeled data, then fix the representation model and train the classifier using labeled data. Following GraphCL \\cite{you2020graphcl}, we use a 5-layer GIN with a hidden size of 128 as our representation model, and use an SVM as our classifier. We train the GIN with a batch size of 128 and a learning rate of 0.001. There are 30 epochs of contrastive pre-training under the naive-strategy. We perform a 10-fold cross validation on every dataset. For each fold, we employ 90\\% of the total data as the unlabeled data for contrastive pre-training, and 10\\% as the labeled testing data. We repeat every experiment for 5 times using different random seeds. \n\nWe compare with the kernel-based methods like graphlet kernel (GL) \\shortcite{shervashidze2009graphlet}, Weisfeiler-Lehman sub-tree kernel (WL) \\shortcite{shervashidze2011wl} and deep graph kernel (DGK) \\shortcite{yanardag2015dgk}, and other unsupervised graph representation methods like node2vec \\cite{grover2016node2vec}, sub2vec \\cite{adhikari2018sub2vec}, graph2vec \\cite{narayanan2017graph2vec} also the contrastive learning methods like InfoGraph \\cite{sun2019infograph}, GraphCL \\cite{you2020graphcl}, JOAO \\cite{you2021joao} and AD-GCL \\cite{suresh2021adgcl}. Table \\ref{tab-unsup-exp} show the comparison among different models for unsupervised learning. Our proposed model achieves the best results on PROTEINS, NCI1, IMDB-binary, and REDDIT-Multi-5K datasets and the second best performances on MUTAG, DD, and REDDIT-binary datasets, outperforming current state-of-the-art contrastive learning methods GraphCL, JOAO and AD-GCL.\n\n\\subsubsection{Transfer Learning}\n\nWe also evaluate the transfer learning performance of the proposed method. A strong baseline method for graph transfer learning is Pretrain-GNN \\cite{hu2019pretraingnn}. The network backbone of Pretrain-GNN, GraphCL, JOAO, AD-GCL and our method is a variant of GIN \\cite{xu2018gin}, which incorporates the edge attribute. We perform 100 epochs of supervised pre-training on the pre-processed ChEMBL dataset (\\cite{mayr2018chemdataset, gaulton2012chembl}), which contains 456K molecules with 1,310 kinds of diverse and extensive biochemical assays. \n\nWe perform 30 epochs of fine-tuning on the 8 chemistry evaluation subsets. We use a hidden size of 300 for the classifier, a hidden size of 128 for the view generator. We train the model using a batch size of 256 and a learning rate of 0.001. The results in Table \\ref{tab-transfer-exp} are the meanstd of the ROC-AUC scores from 10 reps. Infomax, EdgePred, AttrMasking, ContextPred are the manually designed pre-training strategies from Pretrain-GNN \\cite{hu2019pretraingnn}. \n\nTable \\ref{tab-transfer-exp} presents the comparison among different methods. Our proposed method achieves the best performance on most dataset, such as  BBBP, ClinTox, MUV, and BACE, and compared with the current SoTA model AD-GCL \\cite{suresh2021adgcl}, our method performs considerably better, for example, on BACE dataset, the accuracy raises from 78.510.80 to 83.261.13. Considering all datasets, the average gain of using our proposed method is around 1.5\\%. Interestingly, AttrMasking achieves the best performance on Tox21 and ToxCast, which is slightly better than our method. One possible reason is that attributes are important for classification in Tox21 and ToxCast datasets.\n\n\\subsubsection{Semi-Supervised Learning}\n\\label{sec-semi-exp}\n\n\\begin{table*}\n    \\caption{Comparison with existing methods and different strategies for semi-supervised learning. \\textbf{Bold} numbers denote the best performance and the numbers in \\textcolor{blue}{blue} denote the second best performance. \\textcolor{red}{Red} is our default setting for joint training strategy.}\n    \\vspace{-0.3cm}\n    \\resizebox{1\\textwidth}{!}{\n        \\begin{tabular}{lcccccccc}\n        \\hline\n        Model                   & PROTEINS   & DD         & NCI1       & COLLAB     & GITHUB     & IMDB-B     & REDDIT-B   & REDDIT-M-5K \\\\\n        \\hline\n        \\emph{Full Data}        & 78.251.61 & 80.733.78 & 83.651.16 & 83.440.77 & 66.891.04 & 76.604.20 & 89.952.06 & 55.592.24  \\\\ \n        \\hline\n        10\\% Data               & 69.726.71 & 74.365.86 & 75.162.07 & 74.342.00 & 61.051.57 & 64.804.92 & 76.755.60 & 49.713.20  \\\\\n        10\\%   GCA              & 73.855.56 & 76.744.09 & 68.732.36 & 74.322.30 & 59.243.21 & \\textbf{73.704.88} & 77.156.96 & 32.9510.89 \\\\\n        10\\%   GraphCL Aug Only & 70.715.63 & 76.484.12 & 70.972.08 & 73.562.52 & 59.801.94 & 71.105.11 & 76.454.83 & 47.334.02 \\\\\n        10\\%   GraphCL CL       & 74.214.50 & 76.655.12 & 73.162.90 & 75.502.15 & \\textcolor{blue}{63.511.02} & 68.105.15 & 78.052.65 & 48.091.74 \\\\\n        10\\%   JOAOv2           & 73.310.48 & 75.810.73 & 74.860.39 & 75.530.18 & \\textbf{66.660.60} & - & \\textcolor{blue}{88.790.65} & \\textcolor{blue}{52.710.28} \\\\\n        10\\%   AD-GCL           & 73.960.47 & \\textbf{77.910.73} & \\textbf{75.180.31} & 75.820.26 & - & - & \\textbf{90.100.15} & \\textbf{53.490.28} \\\\\n        10\\%   Our Aug Only     & \\textcolor{blue}{75.495.15} & 77.164.53 & 73.332.86 & 75.921.93 & 60.651.04 & \\textcolor{blue}{71.902.88} & 79.652.84 & 47.972.22 \\\\\n        10\\%   Our CL Naive     & 74.573.29 & 75.554.76 & 73.222.48 & \\textcolor{blue}{76.602.15} & 60.951.32 & 71.002.91 & 79.104.38 & 46.712.64 \\\\\n        10\\%   Our CL Joint ($\\mathcal{L}_\\text{cls}$) & 74.662.58 & 76.575.08 & 71.781.61 & 75.382.15 & 60.391.50 & 70.604.17 & 78.903.11 & 46.893.13 \\\\\n        10\\%   Our CL Joint ($\\mathcal{L}_\\text{cls}$+$\\mathcal{L}_\\text{sim}$)     & 75.123.35 & 76.233.57 & 72.552.72 & 75.602.08 & 60.181.75 & 71.703.86 & 79.252.88 & 47.512.51 \\\\\n        10\\%   Our CL Joint ($\\mathcal{L}_\\text{cl}$ +$\\mathcal{L}_\\text{cls}$ )     & 74.753.35 & 76.823.85 & 73.072.31 & 76.182.46 & 61.751.30 & 71.505.32 & 78.354.21 & 47.732.69 \\\\\n        10\\%   \\textcolor{red}{Our CL Joint} ($\\mathcal{L}_\\text{cl}$ +$\\mathcal{L}_\\text{cls}$+$\\mathcal{L}_\\text{sim}$)     & \\textbf{75.652.40} & \\textcolor{blue}{77.504.41} & \\textcolor{blue}{73.752.25} & \\textbf{77.161.48} & 62.461.51 & \\textcolor{blue}{71.904.79} & 79.803.47 & 49.912.70 \\\\\n        \\hline\n        \\end{tabular}\n        }\n    \\vspace{-0.5cm}\n    \\label{tab-semi-exp}\n\\end{table*}\n\nWe perform semi-supervised graph classification task on TUDataset \\cite{morris2020tudataset}. %Following GraphCL \\cite{you2020graphcl}, \nFor our view generator, we use a 5-layer GIN with a hidden size of 128 as the embedding model. We use ResGCN \\shortcite{chen2019gfn} with a hidden size of 128 as the classifier. For GraphCL, we use the default augmentation policy \\emph{random4}, which randomly selects two augmentations from node dropout, edge perturbation, subgraph, and attribute masking for every mini-batch. All augmentation ratios are set to $0.2$, which is also the default setting in GraphCL.\n\nWe employ a 10-fold cross validation on each dataset. For each fold, we use 80\\% of the total data as the unlabeled data, 10\\% as labeled training data, and 10\\% as labeled testing data. For the augmentation only (Aug Only) experiments, we only perform 30 epochs of supervised training with augmentations using labeled data. For the contrastive learning experiments of GraphCL and our naive-strategy, we perform 30 epochs of contrastive pre-training followed by 30 epochs of supervised training. For our joint-strategy, there is 30 joint epochs of contrastive training and supervised training. \n\nTable \\ref{tab-semi-exp} compares the performances obtained by different training strategies: augmentation only (Aug only), naive-strategy \n(CL naive) and joint-strategy (CL joint). We also conducted an ablation study of our joint loss function. The proposed CL joint approach achieves relatively high accuracy on most datasets, for example, on PROTEINS and COLLAB datasets, using joint strategy obtains the best performance. In terms of other datasets, using joint strategy could also achieves the second best performances. Looking at the comparison among Aug only, CL naive and CL joint, CL joint is superior to the other two approaches, in particular to CL naive. \n\n\\subsection{Effectiveness of Learnable View Generators}\n\\label{sec-mnist-exp}\n\nIn this section, we demonstrate the superiority of learnable graph augmentation policies over the fixed ones. Since the graph datasets are usually difficult to be manually classified and visualized, we trained a view generator on MNIST-Superpixel dataset \\cite{monti2017mnistsuperpix} to verify that our graph view generator is able to effectively capture the semantic information in graphs than GraphCL \\cite{you2020graphcl}, since MNIST-Superpixel graphs have clear semantics which does not require any domin knowledge. The visualization result is shown in Figure \\ref{fig-mnist-superpixel-vis}.\n\nHere we jointly trained the view generators with the classifier until the test accuracy (evaluated on generated views) reached $90\\%$. Since our only topological augmentation is node dropping. So we compared the view of GraphCL's node dropping augmentation, and use the default setting $\\text{aug\\_ratio}=2$. Figure \\ref{fig-mnist-superpixel-vis} shows that, our view generator are more likely to keep key nodes in the original graph, preserving its semantic feature, yet providing enough variance for contrastive learning. Details of the MNIST-Superpixel dataset and more visualization examples are shown in Section 1.2 of the supplementary.\n\n\\begin{figure}[tbp]\n    \\begin{center}\n    \\includegraphics[width=1\\linewidth]{figures/fig-mnist-superpixel-one-col.pdf}\n    \\end{center}\n    \\vspace{-0.4cm}\n    \\caption{View visualization on the MNIST-Superpixel dataset. Redness reflects the magnitude of node attribute.}\n    \\vspace{-0.1cm}\n    \\label{fig-mnist-superpixel-vis}\n\\end{figure}\n\n\\subsection{Analysis for Joint Training Strategy}\n\\label{sec-joint-strategy-analysis}\n\nWe compared the naive-strategy (Algorithm \\ref{algo-naive}) with the joint-strategy (Algorithm \\ref{algo-joint}). We trained on COLLAB \\shortcite{collab} dataset, which have 5000 social network graphs of 3 classes, the average nodes and edges are 74.49 and 2457.78. Here we use 5-layer GIN \\cite{xu2018gin} as the backbone for both the view generator and the classifier. For naive-strategy, there is 30 epochs of contrastive pretrain using 80\\% unlabeled data and 30\\% of fine-tuning using 10\\% of data. For joint-strategy, there is 30 epochs of joint training. The learning curves are shown in Section 1.3 of the supplementary. Our results show that the joint strategy considerably alleviate the over-fitting effect, and our label-preserving view generator is very effective. We also visualize the process for learning the embedding for each strategy using t-SNE \\cite{van2008tsne} in the supplementary. We found that joint-strategy leads to better representation much faster since labeled data is used for supervision, also this supervision signal could benefit view generator learning. \n\n\\section{Conclusion}\nIn this paper, we presented a learnable data augmentation approach to graph contrastive learning, where we employed GIN to generate different views of the original graphs. To preserve the semantic label of the input graph, we developed a joint learning strategy, which alternately optimizes view generators, graph encoders and the classifier. We also conducted extensive experiments on a number of datasets and tasks, such as semi-supervised learning, unsupervised learning and transfer learning, where results demonstrate the advantage of our proposed method outperforming counterparts on most datasets and tasks. In addition, we visualized the generated graph views, which could preserve discriminative structures of input graphs, benefiting classification. Finally, the t-SNE visualization illustrated that the proposed joint training strategy could be a better choice for semi-supervised graph representation learning.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2010.13902v3.tex",
        "arXiv-2106.05819v4.tex",
        "arXiv-2109.10259v2.tex"
    ],
    "group_id": "group_72",
    "response": "### Title: Innovations in Graph Contrastive Learning with Augmentations: A Comparative Analysis\n\n### Introduction\nGraph Neural Networks (GNNs) have emerged as a powerful tool for analyzing graph-structured data, which are abstract representations of entities and their interactions in various domains, such as social networks, biochemical molecules, and point clouds. Unlike Convolutional Neural Networks (CNNs) that have been extensively explored for image data, GNNs face unique challenges in pre-training and self-supervised learning due to the abstract nature and heterogeneity of graph data. Current GNNs often rely on supervised learning, which requires a significant amount of labeled data, a resource that is often scarce and costly to obtain, especially in fields like biochemistry and social science. This scarcity has led to a growing interest in developing self-supervised learning strategies for GNNs, aiming to improve their generalizability, transferability, and robustness without the need for extensive labels. \n\nThe history of GNNs dates back to the early 2000s when Graph Neural Networks (GNNs) were first introduced as a way to leverage the power of neural networks for graph data. Since then, numerous variants have been proposed, each aiming to capture the structural information within graphs more effectively. However, the exploration of self-supervised learning and pre-training for GNNs has been limited, primarily due to the challenges in designing effective data augmentation strategies and contrastive learning frameworks that can generalize across diverse graph datasets. \n\nRecent advancements in visual representation learning, particularly in contrastive learning, have inspired researchers to apply similar principles to GNNs. Contrastive learning aims to learn representations by maximizing the consistency between differently augmented views of the same data, which can be particularly beneficial for GNNs as it helps them capture more robust and transferable features. However, the application of contrastive learning to GNNs is not straightforward, as graph data lack the geometric consistency found in images, and the irregularity of graph structures presents unique challenges.\n\nThis summary focuses on three recent papers that explore different aspects of graph contrastive learning (GCL) with augmentations. The first paper introduces a framework for GCL, proposing four types of graph augmentations to facilitate the learning of invariant representations. The second paper presents an adversarial approach to optimize graph augmentations, aiming to capture minimal sufficient information for the downstream tasks. The third paper introduces a learnable view generator for GCL, allowing the augmentation strategy to be data-driven and adaptive to the input graph's structure and attributes. Each paper addresses the challenges of GCL in its unique way, contributing to the broader goal of developing more effective and efficient GNNs through self-supervised learning.\n\n### Main Content of Each Paper\n\n#### Paper 1: Graph Contrastive Learning with Augmentations\nThe first paper introduces a novel framework for graph contrastive learning (GraphCL) that leverages data augmentations to improve the learning of unsupervised representations for graph data. The authors argue that traditional methods for pre-training GNNs, such as adjacency information reconstruction and local-global consistency enforcement, can be limited and may not capture the full structural information of graphs. To address this, they propose four types of graph augmentations: node dropping, edge perturbation, attribute masking, and subgraph sampling. These augmentations are designed to introduce various priors about the graph data distribution, allowing the model to learn representations that are invariant to specific perturbations. \n\nThe GraphCL framework consists of a GNN-based encoder and a projection head, which are trained to maximize the agreement between representations of the original graph and its augmented views using a contrastive loss. The authors conduct extensive experiments to evaluate the effectiveness of GraphCL in semi-supervised, unsupervised, and transfer learning settings, as well as its robustness against adversarial attacks. They find that GraphCL outperforms state-of-the-art (SOTA) methods in most cases, achieving significant gains in accuracy and robustness. The authors also provide theoretical insights into GraphCL, showing that it can be viewed as a form of mutual information maximization and can unify various contrastive learning methods for graphs.\n\n#### Paper 2: Adversarial Graph Augmentation to Improve Graph Contrastive Learning (AD-GCL)\nThe second paper proposes an adversarial approach to graph contrastive learning (AD-GCL), which aims to optimize graph data augmentations (GDA) in an adversarial manner. The authors argue that traditional GCL methods, which rely on predefined GDA strategies, may capture redundant information that is irrelevant to downstream tasks, leading to brittle representations. To overcome this issue, they introduce a principle that pairs GCL with adversarial training, allowing the GNN encoder to capture minimal sufficient information to distinguish graphs in the dataset. \n\nThe AD-GCL framework consists of two components: a GNN encoder that maximizes the mutual information between the original graph and its augmented views, and a GNN-based augmenter that optimizes the augmentation strategy to minimize the information from the original graph. The authors provide theoretical guarantees for AD-GCL, showing that it can yield a lower bound guarantee of information relevant to downstream tasks while simultaneously holding an upper bound guarantee of redundant information. They instantiate AD-GCL using a learnable edge-dropping augmentation strategy, which is optimized to remove redundant information while preserving the structural information necessary for downstream tasks. \n\nExperiments on 18 benchmark datasets for molecule property classification and regression, and social network classification tasks, demonstrate that AD-GCL significantly outperforms traditional GCL methods across various settings, including unsupervised, transfer, and semi-supervised learning. The authors also analyze the impact of regularization on the edge-dropping ratio, showing that AD-GCL is robust to different choices of regularization and can achieve better performance even when the edge-dropping ratio is very low or very high. \n\n#### Paper 3: AutoGCL: Automated Graph Contrastive Learning via Learnable View Generators\nThe third paper introduces an automated graph contrastive learning (AutoGCL) framework that employs learnable view generators to adaptively generate augmented views of graphs. The authors argue that existing GCL methods often rely on predefined augmentation strategies, which may not be optimal for all types of graph data and downstream tasks. To address this, they propose a learnable view generator that predicts the probability of selecting each type of augmentation (node dropping, edge perturbation, or attribute masking) for each node in the graph. \n\nThe AutoGCL framework consists of two view generators, a GNN encoder, and a classifier, all of which are trained in an end-to-end manner. The authors define three loss functions: contrastive loss, similarity loss, and classification loss. The contrastive loss is used to pull the representations of the same graph close in the embedding space, while the similarity loss is used to minimize the mutual information between the views generated by the two view generators. The classification loss is used to encourage the view generators to generate label-preserving augmentations. \n\nThe authors conduct extensive experiments to evaluate the effectiveness of AutoGCL in unsupervised, transfer, and semi-supervised learning settings. They find that AutoGCL outperforms existing GCL methods on most datasets, achieving the best performance on several datasets. They also provide visualizations of the generated graph views, showing that the learnable view generators can better preserve the semantic structures of the original graphs compared to predefined augmentation strategies. \n\n### Commonalities and Innovations\nAll three papers focus on improving the performance of GNNs through self-supervised learning and contrastive learning. They propose different strategies to generate augmented views of graphs, which are then used to train the GNN encoder to learn more robust and transferable representations. \n\nThe common theme across the papers is the use of data augmentations to create multiple views of the same graph, which are then used to train the GNN encoder through contrastive learning. However, each paper introduces unique innovations to address the challenges of GCL. GraphCL proposes a framework that systematically studies the impact of different combinations of graph augmentations on downstream tasks, providing theoretical and empirical insights into the design of effective augmentations. AD-GCL introduces an adversarial training strategy to optimize the augmentation process, allowing the GNN encoder to capture minimal sufficient information for downstream tasks. AutoGCL employs learnable view generators to adaptively generate augmented views of graphs, providing a more flexible and data-driven approach to GCL.\n\n### Comparison of Results\nThe results of the three papers are compared across different settings, including unsupervised learning, transfer learning, and semi-supervised learning. Table \\ref{tab:unsupervised} shows that GraphCL outperforms traditional pre-training schemes in semi-supervised learning on graph classification tasks, achieving comparable or better performance than InfoMax and GAE methods. Table \\ref{tab:transfer} demonstrates that GraphCL also achieves state-of-the-art performance in transfer learning on molecular property prediction and protein function prediction tasks. \n\nTable \\ref{tab-unsup-exp} in Paper 3 shows that AutoGCL outperforms existing GCL methods on most datasets, achieving the best performance on several datasets. Table \\ref{tab-transfer-exp} in Paper 3 also demonstrates that AutoGCL achieves the best performance on most transfer learning tasks, with significant gains over existing methods. Table \\ref{tab-semi-exp} in Paper 3 shows that AutoGCL achieves the best or second-best performance on most semi-supervised learning tasks, with the joint training strategy outperforming the naive training strategy.\n\nThe authors of Paper 2 provide a detailed analysis of the impact of regularization on the edge-dropping ratio, showing that AD-GCL is robust to different choices of regularization and can achieve better performance even when the edge-dropping ratio is very low or very high. This analysis highlights the flexibility and robustness of AD-GCL, which can adapt to different graph structures and augmentation policies.\n\n### Conclusion\nThe three papers contribute to the field of graph contrastive learning by introducing novel frameworks and strategies for generating augmented views of graphs. GraphCL provides a systematic study of the impact of different graph augmentations on downstream tasks, while AD-GCL introduces an adversarial training strategy to optimize the augmentation process. AutoGCL employs learnable view generators to adaptively generate augmented views of graphs, providing a more flexible and data-driven approach to GCL.\n\nThe results of the papers demonstrate the effectiveness of these approaches in improving the performance of GNNs in various settings, including unsupervised learning, transfer learning, and semi-supervised learning. The authors of each paper provide theoretical insights into their proposed methods, highlighting the importance of mutual information maximization and the need for adaptive augmentation strategies.\n\nFuture research directions include further exploring the impact of different augmentation strategies on specific types of graph data and downstream tasks, as well as developing more efficient and scalable methods for training learnable view generators. Additionally, the integration of these methods with existing GNN architectures and the application of GCL to new domains, such as recommendation systems and neural architecture search, could further enhance the effectiveness and efficiency of GNNs in self-supervised learning.\n\n\\begin{table}[ht]\n\\centering\n\\scriptsize\n\\caption{Comparison of performance across different learning settings.}\n\\label{tab:unsupervised}\n\\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}\n\\hline\n\\textbf{Dataset} & \\textbf{NCI1} & \\textbf{PROTEINS} & \\textbf{DD} & \\textbf{MUTAG} & \\textbf{COLLAB} & \\textbf{RDT-B} & \\textbf{RDT-M5K} & \\textbf{IMDB-B} & \\textbf{IMDB-M} & \\textbf{MNIST} \\\\\n\\hline\n\\textbf{Baseline} & 73.72$\\pm$0.24 & 70.40$\\pm$1.54 & 73.56$\\pm$0.41 & 90.41$\\pm$4.61 & 73.71$\\pm$0.27 & 86.63$\\pm$0.27 & 51.33$\\pm$0.44 & 71.83$\\pm$1.93 & 48.46$\\pm$2.31 & 79.71$\\pm$0.65 \\\\\n\\hline\n\\textbf{GraphCL} & 74.63$\\pm$0.25 & 74.17$\\pm$0.34 & 76.17$\\pm$1.37 & 86.80$\\pm$1.34 & 74.23$\\pm$0.21 & 89.11$\\pm$0.19 & 52.55$\\pm$0.45 & 75.38$\\pm$1.44 & 43.87$\\pm$0.77 & 93.11$\\pm$0.17 \\\\\n\\hline\n\\textbf{AD-GCL} & 75.18$\\pm$0.31 & 73.96$\\pm$0.47 & 77.91$\\pm$0.73 & 89.70$\\pm$1.03 & 75.82$\\pm$0.26 & 85.52$\\pm$0.79 & 54.93$\\pm$0.43 & 72.33$\\pm$0.56 & 49.89$\\pm$0.66 & 90.10$\\pm$0.15 \\\\\n\\hline\n\\textbf{AutoGCL} & 75.65$\\pm$2.40 & 77.50$\\pm$4.41 & 73.75$\\pm$2.25 & 88.64$\\pm$1.08 & 76.18$\\pm$2.46 & 90.10$\\pm$0.15 & 56.75$\\pm$0.18 & 71.90$\\pm$4.79 & 49.91$\\pm$2.70 & 90.10$\\pm$0.15 \\\\\n\\hline\n\\end{tabular}\n\\end{table}\n\nThe main findings of the papers are summarized in Table \\ref{tab:unsupervised}. GraphCL and AD-GCL achieve state-of-the-art performance in semi-supervised learning, while AutoGCL outperforms existing methods in most datasets for unsupervised and transfer learning. The joint training strategy proposed in AutoGCL is particularly effective in semi-supervised learning, alleviating over-fitting and improving the performance of the GNN encoder.\n\n### Future Research Directions\nFuture research could explore the integration of these methods with existing GNN architectures to further improve their performance and robustness. Additionally, the application of GCL to new domains, such as recommendation systems and neural architecture search, could provide new insights into the effectiveness of these methods in different contexts. The development of more efficient and scalable methods for training learnable view generators could also enhance the practicality of these approaches, making them more accessible for real-world applications. Finally, the exploration of the impact of different augmentation strategies on specific types of graph data and downstream tasks could further refine the design of GCL methods and improve their performance in diverse settings."
}