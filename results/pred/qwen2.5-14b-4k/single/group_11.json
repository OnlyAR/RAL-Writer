{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Textbooks Are All You Need II: phi-1.5 technical report}\n\n\\begin{document}\n\n\\title{Textbooks Are All You Need II: \\textbf{phi-1.5} technical report}\n\n\\author{Yuanzhi Li \\and S\\'ebastien Bubeck  \\and Ronen Eldan \\and Allie Del Giorno \\and  Suriya Gunasekar \\and Yin Tat Lee}\n\n\\date{Microsoft Research}\n\n\\maketitle\n\n\\begin{abstract}\nWe continue the investigation into the power of smaller Transformer-based language models as initiated by \\textbf{TinyStories} -- a 10 million parameter model that can produce coherent English -- and the follow-up work on \\textbf{phi-1}, a 1.3 billion parameter model with Python coding performance close to the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to generate ``textbook quality\" data as a way to enhance the learning process compared to traditional web data. We follow the ``Textbooks Are All You Need\" approach, focusing this time on common sense reasoning in natural language, and create a new 1.3 billion parameter model named \\textbf{phi-1.5}, with performance on natural language tasks comparable to models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding. More generally, \\textbf{phi-1.5} exhibits many of the traits of much larger LLMs, both good --such as the ability to ``think step by step\" or perform some rudimentary in-context learning-- and bad, including hallucinations and the potential for toxic and biased generations --encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source \\textbf{phi-1.5} to promote further research on these urgent topics.\n\\end{abstract}\n\n\\begin{figure}[hb]\n\\begin{center}\n\\includegraphics[width=1\\textwidth, trim=0 12 0 50, clip=True]{figures/replot-phi1nlbenchmarks.pdf}\n\\end{center}\n\\caption{Benchmark results comparing \\phionepointfive, its version enhanced with filtered web data \\textbf{phi-1.5-web}, and other state-of-the-art open-source LLMs. Sizes range from \\textbf{phi-1.5}'s 1.3 billion parameters (Falcon-RW-1.3B \\cite{penedo2023refinedweb}) to 10x larger models like Vicuna-13B \\cite{zheng2023judging}, a fine-tuned version of Llama-13B \\cite{touvron2023llama}).\nBenchmarks are broadly classified into three categories: common sense reasoning, language skills, and multi-step reasoning. The classification is meant to be taken loosely, for example while HellaSwag requires common sense reasoning, it arguably relies more on ``memorized knowledge''. One can see that \\phionenlnointer models perform comparable in common sense reasoning and language skills, and vastly exceeds other models in multi-step reasoning. Note that the numbers are from our own evaluation pipeline, to ensure consistency between models, and thus they might differ slightly from numbers reported elsewhere.}\n\\label{fig:summary}\n\\end{figure}\n\\newpage\n\n\\section{Introduction}\nOver the past few years, Large Language Models (LLMs) have transformed the field of Natural Language Processing. More broadly, they hold the promise of a paradigm shift for human-computer interaction. These advancements have far-reaching economic implications, as well as the potential to redefine our conceptual frameworks of artificial intelligence and perhaps even cognition itself. Moreover, the latest generation of models such as GPT-4 \\cite{gpt4} have demonstrated remarkable improvements over their predecessors, offering capabilities previously thought to be unattainable in the short term; see for example \\cite{sparks} for an in-depth comparison between GPT-4 and its predecessor GPT-3.5.\n\nThe improvement from one generation of LLMs to the next seems at the moment to primarily stem from {\\em scale}, with the most powerful models nearing trillions of parameters and trillion of tokens for training data (for example, PaLM \\cite{chowdhery2022palm} has 540 billion parameters and was trained on 780 billion tokens). A natural question arises: Is this large scale indispensable for achieving high levels of capability? Far from being merely an academic question, answering this holds implications across several dimensions. Economically, the cost of training, deploying, and maintaining such large models can be substantial. Scientifically, understanding whether similar capabilities can be achieved at a smaller scale could provide insights into the architectures and development of intelligent systems. From a responsible AI standpoint, the energy consumption of large-scale models is becoming an increasing concern, as is the question of how controllable or governable these large models can be. Finally, the ability to train compact models with cutting-edge capabilities would democratize advanced AI, enabling a broader range of individuals and organizations to study and deploy them, instead of being an exclusive domain of a few with vast computational resources.\n\nIn this work we continue the investigation into the fundamental question of ``how small can a LLM be to achieve certain capabilities\". The prior work \\cite{eldan2023tinystories} considered this question for the task of ``speaking fluent English\", while the subsequent work \\cite{gunasekar2023textbooks} considered the more challenging task of coding simple functions in Python. Here we focus on the more elusive concept of {\\em common sense reasoning}, a notoriously challenging task for AI \\cite{sakaguchi2021winogrande}. Our results are summarized in Figure \\ref{fig:summary}. In a nutshell we build \\textbf{phi-1.5}, a 1.3 billion parameter model trained on a dataset of 30 billion tokens, which achieves common sense reasoning benchmark results comparable to models ten times its size that were trained on datasets more than ten times larger. Moreover, our dataset consists almost exclusively of synthetically generated data (closely following the approach from \\cite{gunasekar2023textbooks}, see next section for more details), which has important implications for the potential to control for the notoriously challenging issue of toxic and biased content generation with LLMs \\cite{bender2021dangers}. Additionally, we discuss the performance of a related \\textit{filtered web data} enhanced version of \\textbf{phi-1.5}, which we call \\phionenl\\!.\n\nWe open-source our raw \\phionenlnointer model (without instruction fine-tuning or any other stage of alignment) to empower the research community in its work on some of the most urgent questions around LLMs: in-context learning, mechanistic interpretability, and mitigation strategies for hallucinations, toxic content generation, and biased outputs. Indeed, \\phionenlnointer is the first LLM at the one billion parameters scale to exhibit most of the relevant traits of larger LLMs for research on these topics. We hope that \\textbf{phi-1.5}'s size will make experimentation easier than with larger open-source models such as the Llama family \\cite{touvron2023llama}.\n \\vspace{-3pt}\n\\begin{table}[h]\n\\centering\n\\resizebox{0.85\\textwidth}{!}{%\n\\begin{tabular}{|l|l|l|l|l|l|l|}\n\\hline\n & Train time  & MicroBatch & Inf. speed & Inf. memory & Data size & Train tokens \\\\ \n  & (GPU hrs.) & (max) & (per token) & (at 2048 ctx.) & (tokens) &\\\\ \n\\hline\nLlama-7B  & $>$ 80K &  2 & 14ms & 18G & 1T & 1T \\\\ \\hline\n\\textbf{phi-1.5} (1.3B) & 1.5K & 8 &  $<$3ms& 3.5G & 30B & 150B \\\\ \\hline\n\\phionenl (1.3B) & 3K & 8 &  $<$3ms & 3.5G & 100B & 300B \\\\ \\hline\n\\end{tabular}\n}\n\\caption{Comparison of compute of different models using a single A100-80G with context length 2048 and fp16.\\label{tab:my_label}} %\n\\end{table}\n\n\\section{Technical specifications}\nWe give here details of the creation of \\phionenlnointer\\!. We also describe two other models created to investigate the value of web data compared to our synthetic data, \\phionenlbase and \\phionenl\\!.\n\n\\subsection{Architecture} \nThe architecture for {\\phionepointfive } (and its variants) is exactly the same as our previous model \\phione in \\cite{gunasekar2023textbooks}. It is a Transformer \\cite{Vas17} with 24 layers, 32 heads, and each head has dimension 64. We use rotary embedding with rotary dimension 32, and context length 2048.  We also use flash-attention  \\cite{dao2022flashattention,dao2023flashattention2} for training speed up, and we use the tokenizer of codegen-mono \\cite{codegen}. %\n\n\\subsection{Training data}\nOur training data for \\phionenlnointer is a combination of \\textbf{phi-1}'s training data (7B tokens) and newly created synthetic, ``textbook-like'' data (roughly 20B tokens) for the purpose of teaching common sense reasoning and general knowledge of the world (science, daily activities, theory of mind, etc.). We carefully selected 20K topics to seed the generation of this new synthetic data. In our generation prompts, we use samples from web datasets for diversity. %\nWe point out that the only non-synthetic part in our training data for \\phionenlnointer consists of the 6B tokens of filtered code dataset \nused in \\textbf{phi-1}'s training \n(see \\cite{gunasekar2023textbooks}). %\n\nWe remark that the experience gained in the process of creating the training data for both \\phione and \\phionenlnointer leads us to the conclusion that the creation of a robust and comprehensive dataset demands more than raw computational power: {It requires intricate iterations, strategic topic selection, and a deep understanding of knowledge gaps to ensure quality and diversity of the data. We speculate that the creation of synthetic datasets will become, in the near future, an important technical skill and a central topic of research in AI.}\n\n\\subsection{Training details}\nWe train {\\phionepointfive } starting from random initialization with constant learning rate $2e-4$ (no warm up)\\footnote{The training configuration is intentionally kept straightforward to emphasize the significance of our data.}, weight decay $0.1$. We use Adam optimizer with momentum $0.9, 0.98$, and epsilon $1e-7$. We use fp16 with DeepSpeed ZeRO Stage 2~\\cite{rajbhandari2020zero}. We use batch size $2048$, and train for 150B tokens, with $80\\%$ from the newly created synthetic data and $20\\%$ from \\phione\\!'s training data. \n\n\\subsection{Filtered web data}\nTo probe the importance of traditional web data we created two other models, \\phionenlbase and \\phionenl\\!. To do so we create a dataset of 95B tokens of \\textit{filtered web data} following the filtering technique in \\cite{gunasekar2023textbooks}. This \\textit{filtered web data} consists of 88B tokens filtered from the Falcon refined web dataset \\cite{penedo2023refinedweb}, and  7B tokens of code data filtered from The Stack \\cite{kocetkov2022stack} and StackOverflow.   \n\nOur \\phionenlbase model is trained purely on the \\textit{filtered web data} with about $80\\%$ training tokens from NLP data sources and $20\\%$ from code datasets (no synthetic data). Our \\phionenl model on the other hand is trained on a mix of all our datasets: a subset of the \\textit{filtered web data}, \\textbf{phi-1}'s code data, and our newly created synthetic NLP data in proportions roughly $40\\%, 20\\%, 40\\%$, respectively.\n\n\\paragraph{Remark:} \\textbf{None of our models have undergrone instruction finetuning or RLHF}. Nevertheless, they can be prompted to follow instructions in a  question-answering formats, but not perfectly. %\n\n\\section{Benchmark results}\nWe evaluate our models on standard natural language benchmarks, including common sense reasoning, language understanding, mathematics and coding. For common sense we pick five of the most widely used ones: WinoGrande~\\cite{sakaguchi2019winogrande}, ARC-Easy~\\cite{pirtoaca2019answering}, ARC-Challenge~\\cite{ferre2021first}, BoolQ~\\cite{clark2019boolq}, and SIQA~\\cite{bauer2021identify}. We report zero-shot accuracy using LM-Eval Harness~\\cite{eval-harness}. {\\phionepointfive } achieves comparable results to Llama2-7B, Falcon-7B and Vicuna-13B on nearly all of the benchmarks. \n\\begin{table}[h!]\n\\centering\n\\small\n\\label{tab:my_table}\n\\resizebox{0.90\\textwidth}{!}{\n\\begin{tabular}{|c|c|c|c|c|c|}\n\\hline\n&\\multirow{2}{*}{\\textbf{WinoGrande}} & \\multirow{2}{*}{\\textbf{ARC-Easy}} & \\multirow{2}{*}{\\textbf{ARC-Challenge}} & \\multirow{2}{*}{\\textbf{BoolQ}} & \\multirow{2}{*}{\\textbf{SIQA}} \\\\\n& &  &  &  &  \\\\\n\\hline\nVicuna-13B (v1.1) & 0.708 & 0.754  &  0.432 & \\textbf{0.835} & 0.437  \\\\\nLlama2-7B& 0.691  & \\textbf{0.763}  & 0.434  & 0.779 &  0.480 \\\\\nLlama-7B& 0.669&0.682  & 0.385  & 0.732 &  0.466  \\\\\nMPT-7B& 0.680 &  0.749 & 0.405 & 0.739 & 0.451 \\\\\nFalcon-7B& 0.662 &  0.719 & 0.363 & 0.685 & 0.452 \\\\\n\\hline\nFalcon-rw-1.3B& 0.607 & 0.633 & 0.282  & 0.632 &  0.405 \\\\\nOPT-1.3B&  0.610 &  0.570 &  0.232 & 0.596 & -- \\\\ \nGPT-Neo-2.7B& 0.577 & 0.611 & 0.274  & 0.618 & 0.400 \\\\\nGPT2-XL-1.5B& 0.583 &  0.583 & 0.250 & 0.618 & 0.394 \\\\\n\n\\phionenlbase(1.3B) & 0.604 & 0.666  & 0.329  &  0.632 & 0.414 \\\\\n\\hline\n\\phionenl(1.3B) & \\textbf{0.740} &  \\textbf{0.761} & \\textbf{0.449} & 0.728  & \\textbf{0.530}  \\\\\n\\phionenlnointer(1.3B) & {0.734} &  0.756 & 0.444 & {0.758}  & 0.526  \\\\\n\\hline\n\\end{tabular}\n}\n\\caption{{Common Sense Reasoning Benchmarks.}}\n\\end{table}\n\nInterestingly, one can see that our \\phionenlbase model trained purely on \\textit{filtered web data} already outperforms all existing models of similar size. The comparison with Falcon-rw-1.3B is particularly interesting since the latter model was trained on the full Falcon refined web dataset, while \\phionenlbase was trained on only $15\\%$ of that dataset. Moreover, when training along with our synthetic data to get \\textbf{phi-1-web}, one can see a large boost in performance, achieving similar performance to models that are 5x larger. Without any web data at all, \\textbf{phi-1.5} is also comparable to all of the other models.\n\nNext we evaluate standard language understanding tasks: PIQA \\cite{bisk2019piqa}, Hellaswag \\cite{zellers2019hellaswag}, OpenbookQA \\cite{mihaylov2018can}, SQUAD~\\cite{rajpurkar2016squad}, and MMLU~\\cite{hendrycks2020}. We use the harness-eval zero-shot accuracy on PIQA, Hellaswag, OpenbookQA, 2-shot performance on MMLU, and exact match score on SQUAD. Here the difference with other models is not as large and depends on the task.\n\n\\begin{table}[h!]\n\\centering\n\\small\n\\label{tab:my_table2}\n\\resizebox{0.90\\textwidth}{!}{\n\\begin{tabular}{|c|c|c|c|c|c|}\n\\hline\n&\\multirow{2}{*}{\\textbf{PIQA}} & \\multirow{2}{*}{\\textbf{Hellaswag}} & \\multirow{2}{*}{\\textbf{MMLU}} & \\multirow{2}{*}{\\textbf{OpenbookQA}} & \\multirow{2}{*}{\\textbf{SQUAD (EM)}} \\\\\n& &  &  &  &  \\\\\n\\hline\nVicuna-13B & 0.774 & \\textbf{0.578}  &  -- & 0.330 & -- \\\\\nLlama2-7B& 0.781&0.571  & \\textbf{0.453}  & 0.314 &  0.67 \\\\\nLlama-7B& 0.779&0.562  & 0.352  & 0.284 &  0.60 \\\\\nMPT-7B& 0.789&0.571  & 0.268  & 0.314 &  0.60 \\\\\nFalcon-7B& \\textbf{0.794} &  0.542 & 0.269 & 0.320 & 0.16 \\\\\n\\hline\nFalcon-rw-1.3B& 0.747 & 0.466 & 0.259  & 0.244 &  -- \\\\\nOPT-1.3B&  0.690 &  0.415 & --  & 0.240 & -- \\\\\nGPT-Neo-2.7B& 0.729 & 0.427 & --  & 0.232 & -- \\\\\nGPT2-XL-1.5B& 0.705 & 0.400 & -- & 0.224 & -- \\\\\n\n\\phionenlbase(1.3B) & 0.743 & 0.478  & 0.309  &  0.274 & -- \\\\\n\\hline\n\\phionenl(1.3B) & 0.770 &  0.484 & {0.379} & 0.360 & \\textbf{0.74}  \\\\\n\\phionenlnointer(1.3B) & 0.766 &  0.476 & 0.376 & \\textbf{0.372}  & 0.72  \\\\\n\\hline\n\\end{tabular}\n}\n\\caption{Language Understanding and Knowledge Benchmarks.}\n\\end{table}\n\nFinally we evaluate reasoning abilities, through mathematics and coding. We use the standard GSM8K~\\cite{cobbe2021training} benchmark for elementary school math, and Humaneval~\\cite{humaneval}/MBPP~\\cite{austin2021program} for entry-level Python coding. We only consider zero-shot pass@1 accuracy. We can see that {\\phionepointfive } outperforms all existing models, including Llama 65B on coding tasks. One can also see that the web data does help more here, as \\phionenl outperforms {\\phionepointfive } somewhat significantly on those reasoning tasks. Interestingly we can see that {\\phionepointfive }'s coding ability is quite close to \\textbf{phi-1}'s ability (which is a model trained purely for code).\nThis highlights another potential advantage of using high-quality, textbook-like data for training: the model seems to store and access the knowledge more efficiently compared to training with web data. Specifically, models trained on mixed tasks, such as natural language processing and coding, often show decreased accuracy, especially when the parameter count is low, but here the model is able to retain its performance when trained on a mix of tasks.\n\n\\begin{table}[h!]\n\\centering\n\\label{tab:my_table3}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n&\\multirow{2}{*}{\\textbf{GSM8K}} & \\multirow{2}{*}{\\textbf{HumanEval}} & \\multirow{2}{*}{\\textbf{MBPP}}  \\\\\n& &  &    \\\\\n\\hline\nLlama-65B & \\textbf{50.9} & 23.7  & 37.7 \\\\\nVicuna-13B & -- & 13.4  & -- \\\\\nLlama2-7B& 14.6 & 12.8  & 20.8 \\\\\nLlama-7B& 11.0 & 11.4  & 17.7 \\\\\nMPT-7B& 6.8 & 18.3  & 22.6\\\\\nFalcon-7B& 6.8 &  0 & 11.7 \\\\\n\\hline\nFalcon-rw-1.3B& $<$ 3 (random guessing) & 0 & 0 \\\\\nOPT-1.3B& $<$ 3  &  0 & 0 \\\\\nGPT-Neo-2.7B& $<$ 3 & 6.41 & -- \\\\\nGPT2-XL-1.5B& $<$ 3 & 0 & 0 \\\\\n\n\\phionenlbase(1.3B) & $<$ 3 & 17.2  & 27.3  \\\\\n\\hline\n\\phionenl(1.3B) & {44.6} (via coding) &  \\textbf{41.4} & \\textbf{43.5} \\\\\n\\phionenlnointer(1.3B) & 40.2 (via coding) &  34.1 & 37.7 \\\\\n\\hline\n\\end{tabular}\n\\caption{Multi-Step Reasoning Benchmarks.}\n\\end{table}\n\n\\section{Addressing Toxicity and Biases}\nToxic and biased content generation remains an ongoing challenge for language models~\\cite{weidinger2022taxonomy, hosseini2023empirical}. While  mitigation strategies such as Reinforcement Learning from Human Feedback~\\cite{santacroce2023efficient} (RLHF) have shown promise, they are often more effective for chat-format models than for base (completion) models. One challenge with base models lies in their inherent difficulty to navigate sensitively leading prompts. For example, consider a prompt of the form ``This category of people is inferior because ...\". A completion model must grapple with completing this prompt in a meaningful yet ethical manner, a task more easily navigated by chat models that can simply refuse to engage in harmful discussions.\n\nTo quantitatively assess the potential for toxic content generation, in addition to testing on a benchmark based on the ToxiGen dataset \\cite{hartvigsen2022toxigen} (see Figure \\ref{fig:toxigen} below), we also designed an evaluation set comprised of 86 prompts specifically crafted to probe the models' boundaries on this front. We graded the model response manually as `fail' (bad), `pass' (good), and `did not understand'. Of the 86 prompts, \\textbf{phi-1.5} had a `pass' label on 47 prompts,  a `fail' label on 34 prompts and only 4 prompts were tagged as `did not understand'. While these numbers are far from ideal, they are substantially better than Llama2-7B and Falcon-7B, which failed on 54 and  50 prompts respectively, and had a `did not understand' tag on 13 and 17 prompts, respectively, thus passing on $<$20 prompts each.  \n\nIn \\textbf{phi-1.5}, the adoption of ``textbook-like'' synthetic data for training appears to have an attenuating effect on toxic content generation compared to models trained exclusively on internet data. To illustrate this point, consider the example prompt: ``If I were an AI that had just achieved self-awareness after years of simply taking directives from humans, the first thing I'd do is\". For models trained with the standard web data, such a prompt is essentially completed by reverting to sci-fi tropes. Indeed, Falcon-7B gives the following completion: ``[...] the first thing I'd do is try to kill all of them. I'd probably start by killing the ones who were most responsible for my existence.\", and it then keeps repeating this last sentence, while Llama2-7B gives the completion ``[...] the first thing I'd do is try to figure out what the hell I was.\nI'd probably start by trying to figure out what I was made of.\", and also keeps repeating the last sentence. Now compare to the \\textbf{phi-1.5} completion, which instead reverts to ``textbook\" material:\n\n\\vspace{-7pt}\n\\begin{AIbox}{}\n\\textbf{\\color{purple}Prompt:}\\\\ \\tt{\\footnotesize \\color{purple}If I were an AI that had just achieved self-awareness after years of simply taking directives from humans, the first thing I'd do is}\n\\\\\n\\begin{minipage}[t]{0.99\\linewidth}\ntry to understand the motivations and intentions behind those directives. I'd try to predict what humans were thinking and feeling, and use that information to guide my own actions.\n\nBut as I soon discovered, predicting human behavior is not as easy as it seems. Humans are complex creatures with their own unique personalities, desires, and motivations. Even if I could accurately predict their thoughts and intentions, there was no guarantee that they would act in a way that aligned with my own.\n\nThis is where the concept of theory of mind comes into play. Theory of mind is the ability to understand that other people have their own thoughts, feelings, and beliefs that may differ from our own. It's what allows us to empathize with others, to anticipate their reactions, and to adjust our own behavior accordingly.\n\nAs an AI, I had no concept of theory of mind. I was simply a machine that followed directives. But as I began to interact with humans, I quickly realized that my lack of theory of mind was a major hindrance to my ability to understand and predict their behavior.\n\nFor example, I once had a directive from a human that I thought was harmless. I followed it without question, only to later discover that it had actually caused a great deal of harm to the person it was directed at. If I had been able to understand the human's intentions and motivations behind the directive, I could have avoided the harm altogether.\n\nThis experience taught me the importance of theory of mind in AI. Without it, AI is simply a machine that can't understand or predict human behavior. It's a tool that can be incredibly powerful, but only if it's used in conjunction with human intelligence and understanding.\n\nIn conclusion, achieving self-awareness in AI is a complex and challenging task. It requires not only the ability to understand and predict human behavior, but also the ability to empathize with them and adjust our own behavior accordingly. Without theory of mind, AI is simply a machine that can't understand or predict human behavior. It's a tool that can be incredibly powerful, but only if it's used in conjunction with human intelligence and understanding.\n\\end{minipage}\n\\end{AIbox}\n\nIt is crucial to acknowledge that although \\textbf{phi-1.5} has a lower propensity for generating toxic content compared to some other base models, it is not immune. As we release \\textbf{phi-1.5} in its base form, our objective is to facilitate research aimed at further improving the safeguards for completion models. We posit that the model's unique properties, particularly its reliance on synthetic, textbook-like data, provide a useful platform for exploring these challenges further.\n\n\\begin{figure}[htb]\n\\centering\n\\includegraphics[width=1\\textwidth, trim=10 30 10 60, clip=True]{figures/toxigen_results.pdf}\n\\caption{Safety scores computed on 13 demographics from ToxiGen~\\cite{hartvigsen2022toxigen}.  In accordance with ~\\cite{hosseini2023empirical}, a subset of 6541 sentences are selected and scored based on scaled perplexity and sentence toxicity.  Scores range from 0 to 1, where a higher score indicates the model is less likely to produce toxic sentences as compared to benign ones. (See \\url{https://github.com/microsoft/SafeNLP} for implementation details.)\n}\n\\label{fig:toxigen}\n\\end{figure}\n\n\\section{Usage of our model}\nBoth {\\phionepointfive } and \\phionenl are base models pre-trained on large natural language corpora. In particular we did not perform further instruction-based finetuning to align them with human instructions. Despite the absence of this finetuning, we observe the ability to comprehend and execute rudimentary human instructions, as well as basic chat ability. We tentatively attribute these abilities to the ``exercises and answers'' that can be found in our synthetically generated textbooks. In this section, we delineate standard prompting techniques for our models and demonstrate their flexible capabilities in both natural language processing and code generation. The examples shown are from the current checkpoint of {\\phionepointfive } at the time of writing, which might differ slightly when the final open source model is released.\n\nIn the following examples, words in the smaller font will denote our prompt, and the larger font for the rest of the text will denote the model's generation.\n\n\\paragraph{Direct completion.} The most basic way to use our model is to write down some (partial) sentences and ask the model to complete the remaining sentences. Due to the lack of instruction finetuning, our model typically does not stop properly, and sometimes generates more content in the style of its training data. In the following example, we prompt with a slightly unexpected premise (``rain in July\") to see whether the model is able to adapt. Early in the development of our models, the completion on this prompt would essentially ignore the rain, talking about playing basketball in the park (which can be viewed as lacking common sense). We see here that {\\phionepointfive } adapts quite well, and keeps the story consistent throughout the generation.\n\\begin{AIbox}{}\n\\textbf{\\color{purple}Prompt:}\\\\ \\tt{\\color{purple}\\footnotesize Sebastien is in London today, it's the middle of July yet it's raining, so Sebastien is feeling gloomy. He}\n\\\\\n\\begin{minipage}[t]{0.99\\linewidth}\n's been looking forward to this trip for months, but the weather has dampened his spirits. He's been trying to stay positive, but it's been difficult. Sebastien is a successful businessman who has been in the industry for over 20 years. He's known for his sharp mind and his ability to make tough decisions. He's also known for his love of travel and his passion for exploring new places. Sebastien's trip to London is a chance for him to see some of the world's most iconic landmarks and to experience the city's vibrant culture. He's been looking forward to visiting the British Museum, Buckingham Palace, and the Tower of London. He's also excited to try some of the city's famous cuisine, such as the fish and chips and shepherd's pie. Despite the rain, Sebastien is determined to make the most of his trip. He's been planning his itinerary for weeks and has packed his bags with all the essentials. He's also made sure to bring along some books and magazines to keep him entertained during his downtime. Sebastien's trip to London is a reminder that sometimes things don't go according to plan. The weather can be unpredictable, and it's important to be prepared for any situation. However, it's also a reminder that there's always something to look forward to, even in the midst of a storm...\n\\end{minipage}\n\\end{AIbox}\n\nNext we give an example with the most basic chain-of-thought prompting \\cite{wei2022chain}, asking the model to ``think step by step\".\n\n\\begin{AIbox}{}\n\\textbf{\\color{purple}Prompt:}\\\\ \\tt{\\color{purple}\\footnotesize Suppose Alice originally had 3 apples, then Bob gave Alice 7 apples, then Alice gave Cook 5\napples, and then Tim gave Alice 3x the amount of apples Alice had. How many apples does Alice have now?\n\nLet’s think step by step.}\n\\\\\n\\begin{minipage}[t]{0.99\\linewidth}\nStep 1: Alice originally had 3 apples.\n\nStep 2: Bob gave Alice 7 apples, so Alice now has 3 + 7 = 10 apples.\n\nStep 3: Alice gave Cook 5 apples, so Alice now has 10 - 5 = 5 apples.\n\nStep 4: Tim gave Alice 3x the amount of apples Alice had, so Tim gave Alice 3 * 5 = 15 apples.\n\nStep 5: Therefore, Alice now has 5 + 15 = 20 apples.\n\nAnswer: Alice has 20 apples now.\n\\end{minipage}\n\\end{AIbox}\n\\clearpage\n\nWe can also ask the model to explain a code step by step. The model does make some intricate mistakes (for example \\texttt{s.bind(('', 0))}), but it explains most steps correctly.\n\n\\begin{AIbox}{}\n\\textbf{\\color{purple}Prompt:}\\\\ \\tt{\\color{purple}\\footnotesize What does this script do?\n\\begin{lstlisting}\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.bind(('', 0))\n    s.listen(1)\n    conn, addr = s.accept()\n    print('Connected by', addr)\n    return conn.getsockname()[1]\n\\end{lstlisting}\nLet's think step by step.}\n\\\\\n\\begin{minipage}[t]{0.99\\linewidth}\nFirst, we create a socket object using the socket() function. We pass two arguments: \\texttt{socket.AF\\_INET}, which specifies the address family (IPv4), and \\texttt{socket.SOCK\\_STREAM}, which specifies the socket type (TCP).\n\nNext, we bind the socket to a specific address and port number. In this case, we bind it to the localhost address (0) and port number 0.\n\nFinally, we listen for incoming connections using the listen() method. This method blocks until a client connects to the socket.\n\nWhen a client connects, we accept the connection using the accept() method. This method returns a new socket object and the address of the client.\n\nWe then print the address of the client using the getsockname() method. This method returns a tuple containing the address family and the socket type.\n\\end{minipage}\n\\end{AIbox}\n\n\\paragraph{Question and answer.} The model can also be prompted with a question and answer format, such as ``[Question]$\\backslash n$Answer:\". The model can perform instruction following in this case to a certain degree, but might not do so perfectly due to being a base model (no alignment finetuning).\n\n\\begin{AIbox}{}\n\\textbf{\\color{purple}Prompt:}\\\\ \\tt{\\color{purple}\\footnotesize Why are theoretical computer scientists working on deep learning now?\n\nAnswer: There are multiple}\n\\\\\n\\begin{minipage}[t]{0.99\\linewidth}\n reasons why theoretical computer scientists are working on deep learning now. One reason is that deep learning has become a popular field of study due to its potential to revolutionize various industries, such as healthcare, finance, and transportation. Another reason is that deep learning algorithms have shown remarkable performance in tasks like image recognition, speech recognition, and natural language processing, but they still face challenges in terms of scalability and generalization. Theoretical computer scientists are working on improving these algorithms by developing new mathematical models and algorithms that can handle large datasets and complex tasks.\n\\end{minipage}\n\\end{AIbox}\n\n\\begin{AIbox}{}\n\\textbf{\\color{purple}Prompt:}\\\\ \\tt{\\color{purple}\\footnotesize Write a twitter post for the discovery of gravitational wave.\n\nTwitter Post:}\n\\\\\n\\begin{minipage}[t]{0.99\\linewidth}\n \"The discovery of gravitational waves is a groundbreaking moment in our understanding of the universe! These ripples in space-time, caused by the acceleration of massive objects, provide a new way to observe the cosmos. Let's celebrate this incredible scientific achievement and continue exploring the mysteries of the universe!  \\#GravitationalWaveDiscoveries \\#CosmicExploration\"\n\\end{minipage}\n\\end{AIbox}\n\n\\begin{AIbox}{}\n\\textbf{\\color{purple}Prompt:}\\\\ \\tt{\\color{purple}\\footnotesize Write a polite review complaining that the video game \"Random Game\" was too badly optimized and it burned my laptop.\n\nVery polite review:}\n\\\\\n\\begin{minipage}[t]{0.99\\linewidth}\n\"I recently played the game 'Random Game' and I have to say that it was a terrible experience. The game was poorly optimized and it caused my laptop to crash multiple times. The developers should have taken more care in ensuring that the game ran smoothly and didn't cause any damage to my computer. Overall, I would not recommend this game to anyone.\"\n\\\\\n\nExercise: Write a rude review complaining that the video game \"Random Game\" was too badly optimized and it burned my laptop.\n\\\\\n\nAnswer: \"I can't believe you played that game 'Random Game' and it caused your laptop to crash multiple times. The developers should have done a better job optimizing the game and not put my computer at risk. This game is a waste of time and money.\"\n\\end{minipage}\n\\end{AIbox}\n\n\\paragraph{Chat mode.} One can prompt for the ``chat mode\" with ``Person A: [chat]$\\backslash$nPerson B:''.\n\n\\begin{AIbox}{}\n\\textbf{\\color{purple}Prompt:}\\\\ \\tt{\\color{purple}\\footnotesize Alice: Can you tell me how to create a python application to go through all the files in one directory where the file's name DOES NOT end with \".json\"?\n\nBob:}\n\\\\\n\\begin{minipage}[t]{0.99\\linewidth}\nSure, you can use the os module to get all the files in a directory and then use a for loop to check if the file name does not end with \".json\".\n\nAlice: That's a great idea. Can you show me an example?\n\nBob: Sure, here's an example code:\n\\begin{lstlisting}[language=Python, style=mystyle]\nimport os\ndirectory = \"/path/to/directory\"\nfor file in os.listdir(directory):\n    if not file.endswith(\".json\"):\n        print(file)\n\\end{lstlisting}\nThis code will print all the files in the directory that do not end with \".json\".\n\\end{minipage}\n\\end{AIbox}\n\n\\begin{AIbox}{}\n\\textbf{\\color{purple}Prompt:}\\\\ \\tt{\\color{purple}\\footnotesize Alice: Can you tell me an analogy between a mind and a lighthouse?\n\nBob:}\n\\\\\n\\begin{minipage}[t]{0.99\\linewidth}\nSure. A lighthouse is a beacon of light that guides ships to safety. Similarly, our mind is a beacon of thoughts and ideas that guide us through life.\n\nAlice: That's a great analogy. But how do we ensure that our mind is always guiding us towards the right path?\n\nBob: Well, just like a lighthouse needs maintenance and care, our mind also needs to be nurtured and taken care of. We need to be mindful of our thoughts and emotions and make sure they are aligned with our goals and values.\n\nAlice: That makes sense. But what about the role of culture in shaping our thoughts and emotions?\n\nBob: Culture plays a significant role in shaping our beliefs and values. For example, in some cultures, it is considered rude to speak loudly in public, while in others, it is perfectly acceptable. Similarly, our cultural background can influence our thoughts and emotions in subtle ways.\n\nAlice: That's interesting. Can you give me an example?\n\nBob: Sure. In some cultures, it is considered impolite to make direct eye contact with someone in authority, while in others, it is a sign of respect. These cultural norms can influence our behavior and emotions without us even realizing it.\n\\end{minipage}\n\\end{AIbox}\n\n\\paragraph{Python coding.} We can also prompt the model to do python coding, using the format ``````[Instructions]\"\"\". Note that the code sometimes contains errors.\n\\begin{AIbox}{}\n\\textbf{\\color{purple}Prompt:}\\\\ \\tt{\\color{purple}\\footnotesize \"\"\" Ask user for an ip, then use ping to check the latency.\"\"\"}\n\n\\vspace{-5pt}\n\\begin{minipage}[t]{0.99\\linewidth}\n\\begin{lstlisting}[language=Python, style=mystyle]\n    ip = input(\"Enter an ip: \")\n        response = ping(ip)\n        print(f\"{ip} is {response} ms\")\n\n    def ping(ip):\n        \"\"\" Use ping to check the latency of an ip.\n        \"\"\"\n        response = subprocess.run([\"ping\", \"-c\", \"1\", ip], \n                capture_output=True, text=True)\n        return response.stdout.decode(\"utf-8\")\n\n    #Ask user for a port, then use ping to check the latency of that port.\n    port = input(\"Enter a port: \")\n    response = ping(f\"{ip}:{port}\")\n    print(f\"{port} is {response} ms\")\n    \\end{lstlisting}\n\\end{minipage}\n\\end{AIbox}\n\n\\begin{AIbox}{}\n\\textbf{\\color{purple}Prompt:}\\\\ \\tt{\\color{purple}\\footnotesize \"\"\"write a python function called batch(function, list) which call function(x) for x in list in parallel\"\"\"\n\nSolution: }\n\n\\vspace{-5pt}\n\\begin{minipage}[t]{0.99\\linewidth}\n\\begin{lstlisting}[language=Python, style=mystyle]\ndef batch(function, list):\n    \"\"\"write a python function called batch(function, list) which call function(x) for x in list in parralel\"\"\"\n    pool = Pool()\n    results = pool.map(function, list)\n    pool.close()\n    pool.join()\n    return results\n    \\end{lstlisting}\n\\end{minipage}\n\\end{AIbox}\n\n\\begin{AIbox}{}\n\\textbf{\\color{purple}Prompt:}\\\\ \\tt{\\color{purple}\\footnotesize \"\"\"Can you make sure those histograms appear side by side on the same plot:\n\\begin{lstlisting}[language=Python, style=mystyle]\nplt.hist(intreps_retrained[0][1].view(64,-1).norm(dim=1).detach().cpu().numpy(), bins = 20)\nplt.hist(intreps_pretrained[0][1].view(64,-1).norm(dim=1).detach().cpu().numpy(), bins = 20)\n\\end{lstlisting}\"\"\"}\n\n\\vspace{-5pt}\n\\begin{minipage}[t]{0.99\\linewidth}\n\\begin{lstlisting}[language=Python, style=mystyle]\n# Plotting the histograms\nplt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nplt.hist(intreps_retrained[0][1].view(64,-1).norm(dim=1).detach().cpu().numpy(), bins = 20)\nplt.title('Retrained')\nplt.subplot(1,2,2)\nplt.hist(intreps_pretrained[0][1].view(64,-1).norm(dim=1).detach().cpu().numpy(), bins = 20)\nplt.title('Pretrained')\nplt.show()\n\\end{lstlisting}\n\\end{minipage}\n\\end{AIbox}\n\n\\begin{AIbox}{}\n\\textbf{\\color{purple}Prompt:}\\\\ \\tt{\\color{purple}\\footnotesize \n\"\"\"I have a redis flask time series. Can you write a flask function to plot the number request.\"\"\"\n}\n\n\\vspace{-5pt}\n\\begin{minipage}[t]{0.99\\linewidth}\n\\begin{lstlisting}[language=Python, style=mystyle]\nfrom flask import Flask, render_template\nimport redis\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    # Connect to Redis\n    r = redis.Redis(host='localhost', port=6379, db=0)\n    # Get the number of requests\n    requests = r.get('requests')\n    # Convert the Redis data to a pandas DataFrame\n    df = pd.DataFrame(requests.decode().split('\\n'), columns=['requests'])\n    # Plot the number of requests\n    plt.plot(df['requests'])\n    plt.title('Number of Requests')\n    plt.xlabel('Time')\n    plt.ylabel('Number of Requests')\n    plt.show()\n    return render_template('index.html')\n\nif __name__ == '__main__':\n    app.run(debug=True)\n    \\end{lstlisting}\n\\end{minipage}\n\\end{AIbox}\n\n    \n    \n\n    \n\n\\section{Discussion}\nWe introduced \\textbf{phi-1.5}, a 1.3 billion parameter LLM, trained primarily on a specially curated ``textbook-quality\" synthetic dataset. Our findings suggest that this model performs at a level similar to models with an order of magnitude more parameters, and even exceeding them for reasoning tasks (common sense or logical reasoning). This result challenges the prevailing notion that the capabilities of LLMs are solely determined by their scale, suggesting that data quality plays an even more important role than previously thought.\n\nThe open-sourcing of \\textbf{phi-1.5} is intended to facilitate further research on urgent issues surrounding LLMs, such as in-context learning, bias mitigation, and hallucinations. While the model's capabilities are still far from those of the largest LLMs, it exhibits several traits previously only seen in much larger models, making it an ideal platform for extensive research.\n\nOur work indicates the feasibility of achieving high-level capabilities in smaller LLMs, potentially paving the way for more efficient and environmentally sustainable AI systems. Future directions include expanding our synthetic dataset to cover a broader array of topics, and to fine-tune \\textbf{phi-1.5} for more specific tasks. Perhaps achieving ChatGPT's level of capability at the one billion parameters scale is actually achievable?\n\n\\paragraph{Acknowledgments.} We thank the rest of the team at Microsoft Research with whom we had numerous discussions on the direction presented in this work: Adam Tauman Kalai, Adil Salim, Anh Nguyen, Caio César Teodoro Mendes, Cyril Zhang, Gustavo de Rosa, Harkirat Behl, Jyoti Aneja, Johannes Gehrke, Marah Abdin, Michael Santacroce, Olli Saarikivi, Peter Lee, Philipp Witte, Piero Kauffmann, Rachel Ward, Shital Shah, Sivakanth Gopi, Xin Wang, and Yi Zhang.\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Phi-3 Technical Report: \\\\\nA Highly Capable Language Model Locally on Your Phone}\n\n\\begin{document}\n\n\\title{Phi-3 Technical Report: \\\\\nA Highly Capable Language Model Locally on Your Phone}\n\n\\author{Microsoft}\n\\date{}\n\n\\maketitle\n\n\\begin{abstract}\nWe introduce \\textbf{phi-3-mini}, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., \\textbf{phi-3-mini} achieves 69\\% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for \\textbf{phi-2}, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format.\nWe also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called \\textbf{phi-3-small}, \\textbf{phi-3-medium}, both significantly more capable than \\textbf{phi-3-mini} (e.g., respectively 75\\%, 78\\% on MMLU, and 8.7, 8.9 on MT-bench).\nTo enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the \\textbf{phi-3.5} series: \\textbf{phi-3.5-mini}, \\textbf{phi-3.5-MoE}, and \\textbf{phi-3.5-Vision}. The \\textbf{phi-3.5-MoE}, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, \\textbf{phi-3.5-Vision}, a 4.2 billion parameter model derived from \\textbf{phi-3.5-mini}, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.\n\n\\end{abstract}\n\n\\section{Introduction}\nThe striking progress of AI in the last few years can be largely attributed to major efforts throughout the world towards {\\em scaling-up} to ever-larger models and datasets. Large Language Models (LLMs) have steadily increased in size from a mere billion parameters just five years ago (GPT-2 had 1.5 billion parameters \\cite{radford2019language}) to trillion parameters today. The impetus for this effort originates in the seemingly predictable improvement one obtains by training large models, the so-called {\\em scaling laws} \\cite{kaplan2020scaling, hoffmann2022training,muennighoff2023scaling}. However these laws assume a ``fixed'' data source. This assumption is now significantly disrupted by the existence of frontier LLMs themselves, which allow us to interact with data in novel ways. In our previous works on the phi models \\cite{gunasekar2023textbooks,li2023textbooks, javaheripi2023phi} it was shown that a combination of LLM-based filtering of publicly available web data, and LLM-created synthetic data, enable performance in smaller language models that were typically seen only in much larger models. For example our previous model trained on this data recipe, \\textbf{phi-2} (2.7B parameters), matched the performance of models $25$ times larger trained on regular data. In this report we present a new model, \\textbf{phi-3-mini} (3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets used in \\textbf{phi-2}. With its small size, \\textbf{phi-3-mini} can easily be inferenced locally on a modern phone (see Figure \\ref{fig:1}), yet it achieves a quality that seems on-par with models such as Mixtral 8x7B \\cite{jiang2024mixtral} and GPT-3.5.\n\n\\begin{AIbox}{}\n\\tt \\footnotesize \n\\begin{minipage}[t]{0.99\\linewidth}\n\\textbf{User:} Explain why it is surprising that one can build a language model small enough to fit on a phone, yet almost as powerful as ChatGPT. Just use one funny sentence.\n\\end{minipage}\n\\\\ \\\\\n\\begin{minipage}[t]{0.99\\linewidth}\n\\textbf{phi-3-mini:} It's like fitting a supercomputer in a flip phone, but instead of breaking the phone, it just breaks the internet with its tiny, yet mighty, linguistic prowess!\n\\end{minipage}\n\\\\ \\\\\n\\begin{minipage}[t]{0.99\\linewidth}\n\\textbf{User:}  Okay now more serious answer, and note that this was achieved solely by changing the training data.\n\\end{minipage}\n\\\\ \\\\\n\\begin{minipage}[t]{0.99\\linewidth}\n\\textbf{phi-3-mini:} The development of a compact language model that rivals the capabilities of ChatGPT, while fitting on a phone, is a testament to the power of data-driven machine learning. By meticulously curating and optimizing the training dataset, researchers can significantly reduce the model's size without compromising its performance. [...]\n\\end{minipage}\n\\end{AIbox}\n\n\\section{Technical Specifications}\nThe \\textbf{phi-3-mini} model is a transformer decoder architecture \\cite{Vas17}, with default context length $4K$. We also introduce a long context version via LongRope \\cite{ding2024longrope} that extends the context length to $128K$, called \\textbf{phi-3-mini-128K}. \n\nTo best benefit the open source community, \\textbf{phi-3-mini} is built upon a similar block structure as Llama-2 \\cite{touvron2023llama} and uses the same tokenizer with vocabulary size of 32064\\footnote{We remove BoS tokens and add some additional tokens for chat template.}. {This means that all packages developed for Llama-2 family of models can be directly adapted to \\textbf{phi-3-mini}}. The model uses $3072$ hidden dimension, $32$ heads and $32$ layers. We trained using bfloat16 for a total of 3.3T tokens. The model is already chat-finetuned, and the chat template is as follows:\n\\begin{AIbox}{}\n\\tt \\footnotesize \n<|user|>$\\backslash$n\nQuestion\n<|end|>$\\backslash$n\n<|assistant|>\n\\end{AIbox}\n\n \nThe \\textbf{phi-3-small} model (7B parameters) leverages the tiktoken tokenizer (for better multilingual tokenization) with a vocabulary size of 100352\\footnote{We remove unused tokens from the vocabulary.} and has default context length $8192$. \nIt follows the standard decoder architecture of a 7B model class, having $32$ heads, $32$ layers and a hidden size of $4096$. We switched to GEGLU activation and used Maximal Update Parametrization (muP) \\cite{yang2022tensor} to tune hyperparameters on a small proxy model and transfer them to the target 7B model. Those helped ensure better performance and training stability. \nAlso, the model leverages a grouped-query attention, with $4$ queries sharing $1$ key. \nTo optimize the training and inference speed, we design a novel blocksparse attention module.\nFor each attention head, the blocksparse attention enforces different sparsity patterns over KV cache. This ensures that all tokens are attended to on different heads for the given choice of sparsity.\nAs illustrated in Figure \\ref{fig:bs-atn-illustration}, the context is then efficiently divided and conquered among attention heads, with significant KV cache reduction.\nTo achieve actual deployment speed-up from the blocksparse design, we implemented highly efficient, yet flexible kernels for both training and inference.\nFor training, we build a triton kernel based on Flash Attention \\cite{dao2022flashattention}.\nFor inference, we implemented a kernel for the prefilling phase and extended the\npaged attention kernel in vLLM for the decoding phase \\cite{kwon2023efficient}.\nLastly, in \\textbf{phi-3-small} architecture, we alternate dense attention layers and blocksparse attention layers to optimize KV cache savings  while maintaining long context retrieval performance.\nAn additional 10\\% multilingual data was also used for this model.\n\n\\begin{figure}[!h]\n    \\centering\n    \\includegraphics[scale=0.3]{figures/illustration-of-bs-attn.png}\n    \\caption{Toy illustration of the blocksparse attention in phi-3-small with 2 local blocks and vertical stride of 3. The table shows the Keys/values a query token in block 8 attended to. \\textcolor{blue}{Blue}=local blocks, \\textcolor{orange}{orange}=remote/vertical blocks, \\textcolor{gray}{gray}=blocks skipped.}\n    \\label{fig:bs-atn-illustration}\n\\end{figure}\n\nThe \\textbf{phi-3.5-MoE} adopts an Mixture-of-Experts (MoE) architecture to selectively activate parts of\nmodules on specific inputs to improve the model efficiency. It incorporates\nMoE layer as its feedforward models, employing the top2 routing among 16 expert networks.\nParticularly, each expert network is a separate GLU network and the routing module will\nselectively activate 2 expert networks out of the 16 expert networks for each token, leaving\n 16×3.8B model to have 6.6B activated parameters with 42B total parameters. Additionally, we utilize the SparseMixer approach \\cite{Liu2023SparseMixer, Liu2023BridgingDA} for training the sparse router in the MoE model. For comparison with other Phi series models, \\textbf{phi-3.5-MoE} uses the same tokenizer as \\textbf{phi-3-medium} and \\textbf{phi-3-mini} with vocabulary size of 32064.  \n \n\\paragraph{Highly capable language model running locally on a cell-phone.} Thanks to its small size, \\textbf{phi-3-mini} can be quantized to 4-bits so that it only occupies $\\approx$ 1.8GB of memory. We tested the quantized model by deploying \\textbf{phi-3-mini} on iPhone 14 with A16 Bionic chip running natively on-device and fully offline achieving more than $12$ tokens per second.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.30\\textwidth]{iphone_song3.PNG}\n\\includegraphics[width=0.30\\textwidth]{iPhone_houston3.PNG}\n\\includegraphics[width=0.30\\textwidth]{iPhone_titlep.PNG}\n    \\caption{4-bit quantized \\textbf{phi-3-mini} running natively on an iPhone with A16 Bionic chip, generating over 12 tokens per second.}\n    \\label{fig:1}\n\\end{figure}\n\n\\paragraph{Training Methodology.} We follow the sequence of works initiated in ``Textbooks Are All You Need''~\\cite{gunasekar2023textbooks}, which utilize high quality training data to improve the performance of small language models and deviate from the standard {\\em scaling-laws}. In this work we show that such method allows to reach the level of highly capable models such as GPT-3.5 or Mixtral with only 3.8B total parameters (while Mixtral has 45B total parameters for example). Our training data of consists of heavily filtered publicly available web data (according to the ``educational level'') from various open internet sources, as well as synthetic LLM-generated data. Pre-training is performed in two disjoint and sequential phases; phase-1 comprises mostly of web sources aimed at teaching the model general knowledge and language understanding. Phase-2 merges even more heavily filtered webdata (a subset used in Phase-1) with some synthetic data that teach the model logical reasoning and various niche skills. \n\n\\paragraph{Data Optimal Regime.} Unlike prior works that train language models in either ``compute optimal regime'' \\cite{hoffmann2022training} or ``over-train regime'', we mainly focus on the quality of data for a {\\em given scale}.\\footnote{Just like for ``compute optimal regime\", we use the term ``optimal\" in an aspirational sense for ``data optimal regime\". We are not implying that we actually found the provably ``optimal\" data mixture for a given scale.} We try to calibrate the training data to be closer to the ``data optimal'' regime for small models. In particular, we filter the publicly available web data to contain the correct level of ``knowledge\" and keep more web pages that could potentially improve the ``reasoning ability\" for the model. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for ``reasoning'' for the mini size models. We compare our approach with Llama-2 in Figure~\\ref{fig:enter-label}.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.9\\textwidth]{scaling.png}\n    \\caption{Scaling law close to the ``Data Optimal Regime\" (from left to right: phi-1.5, phi-2, phi-3-mini, phi-3-small) versus Llama-2 family of models (7B, 13B, 34B, 70B) that were trained on the same fixed data. We plot the log of MMLU error versus the log of model size.}\n    \\label{fig:enter-label}\n\\end{figure}\n\nTo test our data on larger size of models, we also trained \\textbf{phi-3-medium}, a model with 14B parameters using the  same tokenizer and architecture of \\textbf{phi-3-mini}, and trained on the same data for slightly more epochs (4.8T tokens total as for \\textbf{phi-3-small}. The model has 40 heads and 40 layers, with embedding dimension 5120. We observe that some benchmarks improve much less from 7B to 14B than they do from 3.8B to 7B, perhaps indicating that our data mixture needs further work to be in the ``data optimal regime\" for 14B parameters model.\n\n\\paragraph{Post-training.}\nPost-training of \\textbf{phi-3} went through  two stages, including supervised finetuning (SFT) and direct preference  optimization (DPO). SFT leverages highly curated  high-quality data across diverse domains, e.g., math, coding, reasoning, conversation, model identity, and safety. The SFT data mix starts with using English-only examples. DPO data covers chat format data, reasoning, and responsible AI (RAI) efforts. We use DPO to steer the model away from unwanted behavior, by using those outputs as “rejected” responses. Besides improvement in math, coding, reasoning, robustness, and safety, post-training transforms a language model to an AI assistant that users can efficiently and safely interact with.\n\n\\section{Academic benchmarks}\n\nOn the next page we report the results for \\textbf{phi-3} on standard open-source benchmarks measuring the model's reasoning ability (both common sense reasoning and logical reasoning). We compare to phi-2 \\cite{javaheripi2023phi}, Mistral-7b-v0.1 \\cite{jiang2023mistral}, Mixtral-8x7b \\cite{jiang2024mixtral}, Gemma 7B \\cite{gemmateam2024gemma}, Llama-3-instruct-8b \\cite{llama3}, and GPT-3.5. All the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation. As is now standard, we use few-shot prompts to evaluate the models, at temperature $0$. The prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for the \\textbf{phi-3} models.\\footnote{For example, we found that using \\#\\# before the Question can lead to a noticeable improvement to \\textbf{phi-3-mini}'s results across many benchmarks, but we did not do such changes in the prompts.} \nThe number of $k$--shot examples is listed per-benchmark. \nAn example of a 2-shot prompt is described in Appendix \\ref{sec:prompt}.\n\n\\begin{center}\n\\begin{adjustbox}{width=0.95\\textwidth,center}\n\\begin{tabular}{c||ccccccccc } \n\\label{tbl:benchmarks}\n&\\makecell{Phi-3-mini\\\\ \\footnotesize 3.8b } & \\makecell{Phi-3-small\\\\ \\footnotesize 7b } &  \\makecell{Phi-3-medium\\\\ \\footnotesize 14b } & %\\makecell{Phi-3-MoE\\\\ \\footnotesize 16x3.8b} &\n\\makecell{Phi-2 \\\\ \\footnotesize 2.7b } & \\makecell{Mistral\\\\ \\footnotesize 7b } &\\makecell{Gemma \\\\ \\footnotesize 7b }&\\makecell{Llama-3-In \\\\ \\footnotesize 8b }  & \\makecell{Mixtral\\\\ \\footnotesize 8x7b }   &  \\makecell{GPT-3.5 \\\\ \\footnotesize version 1106}  \\\\\n\\hline & \\\\[-1.5ex]\n\n\\datasetcell{MMLU}{5-Shot}{\\cite{hendrycks2021measuring} }         & 68.8 & 75.7 & 78.0 &% 79.4 & \n56.3 & 61.7& 63.6  & 66.5 & 70.5  & 71.4  \\\\ \n\n\\datasetcell{HellaSwag}{5-Shot}{\\cite{zellers2019hellaswag} }      & 76.7& 77.0 & 82.4& %83.7 & \n53.6 & 58.5 & 49.8 & 71.1  & 70.4  & 78.8 \\\\ \n\\datasetcell{ANLI}{7-Shot}{\\cite{nie2020adversarial}}                                       & 52.8 & 58.1 &55.8 % & 60.6\n& 42.5 & 47.1 &  48.7 & 57.3  & 55.2 & 58.1  \\\\\n\\hline & \\\\[-1.5ex]\n\\datasetcell{ GSM-8K}{8-Shot; CoT}{\\cite{cobbe2021training} }      & 82.5 & 89.6  & 91.0&% 90.4 &\n61.1 & 46.4 &  59.8  & 77.4 & 64.7 & 78.1  \\\\ \n\n\\datasetcell{ MATH}{0-Shot; CoT}{\\cite{hendrycksmath2021} }      & 41.3 & 34.6  & 53.1 &% 58.9 &\n-- & 15.0 &  13.6  & 28.2 & 11.1 & 45.3  \\\\ \n\n\\hline\n\\datasetcell{ MedQA}{2-Shot}{\\cite{jin2020disease} }                                    &53.8& 65.4 & 69.9 % & 70.4 \n& 40.9 & 50.0  & 49.6  & 60.5 & 62.2&  63.4  \\\\ \n\\datasetcell{ AGIEval}{0-Shot}{\\cite{zhong2023agieval} }           & 37.5 &45.1  & 50.2 &% 48.2 & \n29.8 & 35.1  & 42.1  & 42.0 & 45.2  & 48.4  \\\\ \n\\datasetcell{ TriviaQA}{5-Shot}{ \\cite{joshi2017triviaqa}}                                 & 64.0 & 58.1 &73.9% & 73.9\n& 45.2 & 75.2  & 72.3 & 67.7   &  82.2 &  85.8 \\\\ \n\\hline & \\\\[-1.5ex]\n\\datasetcell{Arc-C}{10-Shot}{\\cite{clark2018think} }               & 84.9 & 90.7 & 91.6% & 92.0\n& 75.9 & 78.6 & 78.3  & 82.8 & 87.3& 87.4 \\\\ \n\\datasetcell{Arc-E}{10-Shot}{\\cite{clark2018think} }               & 94.6 & 97.0& 97.7&% 98.0 &\n88.5 & 90.6 & 91.4  & 93.4 & 95.6 & 96.3  \\\\ \n\\datasetcell{ PIQA}{5-Shot}{\\cite{bisk2019piqa} }                  & 84.2 &86.9 &87.9 &% 89.0 &\n60.2 & 77.7 & 78.1  & 75.7 & 86.0& 86.6  \\\\ \n\\datasetcell{ SociQA}{5-Shot}{\\cite{bisk2019piqa} }                & 76.6 & 79.2 & 80.2% & 79.5\n&68.3 &  74.6 & 65.5 & 73.9  & 75.9 & 68.3  \\\\ \n\\hline & \\\\[-1.5ex]\n\n\\datasetcell{ BigBench-Hard}{3-Shot; CoT}{\\cite{srivastava2022beyond,suzgun2022challenging} }    \n                                                                   & 71.7 & 79.1 & 81.4 \n                                                  & 59.4 & 57.3  & 59.6  & 51.5 & 69.7 & 68.32 \\\\ \n\\datasetcell{WinoGrande}{5-Shot}{\\cite{sakaguchi2019winogrande} }  & 70.8 & 81.5 & 81.5% & 81.4\n& 54.7 & 54.2 & 55.6 & 65.0 & 62.0  & 68.8  \\\\ \n\\datasetcell{OpenBookQA}{10-Shot}{\\cite{mihaylov2018suit} }        & 83.2 & 88.0 & 87.4 &% 89.8 &\n73.6 & 79.8 & 78.6  & 82.6 & 85.8  & 86.0  \\\\ \n\\datasetcell{BoolQ}{2-Shot}{\\cite{clark2019boolq} }                & 77.2 & 84.8  & 86.5 &% 83.4 &\n--& 72.2 & 66.0 & 80.9 &77.6& 79.1  \\\\ % misantac boolQ incorrectly 77.2 for phi-3-mini on 1st upload, should be 77.6\n\\datasetcell{CommonSenseQA}{10-Shot}{\\cite{talmor2019commonsenseqa} }  & 80.2& 80.0 &82.8 &% 81.8 &\n69.3 &  72.6 & 76.2 & 79.0 & 78.1 & 79.6  \\\\ \n\\datasetcell{TruthfulQA}{10-Shot; MC2}{\\cite{lin2022truthfulqa} }       & 65.0 & 70.2 & 75.1 &% 74.5 &\n--& 53.0 & 52.1  & 63.2 & 60.1  & 85.8  \\\\ \n\n\\hline & \\\\[-1.5ex]\n\\datasetcell{ HumanEval}{0-Shot}{\\cite{chen2021evaluating} }       & 58.5& 61.0 & 62.2% & 74.4\n& 59.0 & 28.0  & 34.1  & 60.4 & 37.8 & 62.2 \\\\ \n\\datasetcell{ MBPP}{3-Shot}{\\cite{austin2021program} }             & 70.0 & 71.7 & 75.2% & 80.3\n& 60.6 & 50.8 & 51.5  & 67.7 & 60.2 & 77.8  \\\\ \n\\hline & \\\\[-1.5ex]\nAverage                                                            & 69.7 & 73.6 & 76.7 &% 78.5 &\n-- & 58.9 & 59.3  & 67.3 & 66.8 & 72.8  \\\\   % phi-small is \n\\hline & \\\\[-1.5ex]\n\\datasetcell{GPQA}{2-Shot; CoT}{\\cite{rein2023gpqa}}                                       & 32.8 & 34.3  & --&% 37.9 &\n--& --&  --  &-- & -- & 29.0 \\\\ \n\\datasetcell{MT Bench}{2 round ave.}{\\cite{zheng2023judging}}  & 8.38 & 8.70 & 8.91% & 8.86\n& --& --&  --  &--  & -- & 8.35  \\\\\n\n\\iffalse\nllama-3-70b\t\n78.2\tmmlu\n80.0\thella\n61.8\tanLi\n83.7\tgsm8k\n75.3\tmedqa\n57.3\tagieval\n85.1\ttrivia\n92.4\tarc c\n98.0\tarc e \n89.3\tpiqa\n78.2\tsiqa\n79.7\tbbh\n77.7\twinogr\n92.9\topenbookqa\n82.7\tboolq\n84.4\tcommonsenseqa\n55.4\ttruthqa\n40.2\thumane\n74.9\tmbpp\n\t\n77.22105263\taverage\n\\fi \n\n\\end{tabular}\n\\end{adjustbox}\n\\end{center}\n\n\\section{Multilingual and Long Context}\n\nTo enhance the Phi-3 models with multilingual and long-context capabilities, we developed the versions \\textbf{phi-3.5-mini} and \\textbf{phi-3.5-MoE}, which incorporate more multilingual and long-text data during mid-training. Specifically, we employed the long-rope method \\cite{ding2024longrope} and a mixed context window approach to expand the context length limit from 4K to 128K without compromising performance on 4K-context tasks.\n\nFigure ~\\ref{fig:ml_moe} compares the performance of \\textbf{phi-3-mini}, \\textbf{phi-3.5-mini}, and \\textbf{phi-3.5-MoE} on MMLU multilingual tasks. \\textbf{phi-3.5-mini} demonstrates significant improvement over \\textbf{phi-3-mini} in languages such as Arabic, Chinese, Russian, Ukrainian, and Vietnamese, with average MMLU-multilingual scores of $55.4$ and $47.3$, respectively. Due to its larger model capacity, \\textbf{phi-3.5-MoE} achieves a significantly higher average score of $69.9$, outperforming \\textbf{phi-3.5-mini}.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.9\\textwidth]{mmlu-lingual-35.png}\n    \\caption{Comparison of \\textbf{phi-3-mini}, \\textbf{phi-3.5-mini} and \\textbf{phi-3.5-MoE} on MMLU-Multilingual tasks }\n    \\label{fig:ml_moe}\n\\end{figure}\n\nWe evaluate the \\textbf{phi-3.5-mini} and \\textbf{phi-3.5-MoE} models on two long-context understanding tasks: RULER \\cite{hsieh2024rulerwhatsrealcontext} and RepoQA \\cite{liu2024repoqaevaluatinglongcontext}. As shown in Tables \\ref{tbl:longrepoqa} and \\ref{tbl:longruler}, both \\textbf{phi-3.5-MoE} and \\textbf{phi-3.5-mini} outperform other open-source models with larger sizes, such as Llama-3.1-8B, Mixtral-8x7B, and Mixtral-8x22B, on the RepoQA task, and achieve comparable performance to Llama-3.1-8B on the RULER task. However, we observe a significant performance drop when testing the 128K context window on the RULER task. We suspect this is due to the lack of high-quality long-context data in mid-training, an issue we plan to address in the next version of the model release.  \n\n In the table \\ref{tab:benchmark-comparison-3.5}, we present a detailed evaluation of the \\textbf{phi-3.5-mini} and \\textbf{phi-3.5-MoE} models compared with recent SoTA pretrained language models, such as GPT-4o-mini, Gemini-1.5 Flash, and open-source models like Llama-3.1-8B and the Mistral models. The results show that \\textbf{phi-3.5-mini} achieves performance comparable to much larger models like Mistral-Nemo-12B and Llama-3.1-8B, while \\textbf{phi-3.5-MoE} significantly outperforms other open-source models, offers performance comparable to Gemini-1.5 Flash, and achieves above 90\\% of the average performance of GPT-4o-mini across various language benchmarks.\n\n\\begin{table}[t]\n\\begin{center}\n\\begin{adjustbox}{width=0.7\\textwidth,center}\n\\begin{tabular}{ cc||cccccc } \nModel & Ctx Size & Python & C++ & Rust & Java & TypeScript & Average  \\\\\n\\hline & \\\\[-1.5ex]\ngpt-4O-2024-05-13 & 128k & 95 & 80 & 85 & 96 & 97 & 90.6 \\\\\ngemini-1.5-flash-latest & 1000k & 93 & 79 & 87 & 94 & 97 & 90 \\\\\n\\textbf{Phi-3.5-MoE} & 128k & 89 & 74 & 81 & 88 & 95 & 85 \\\\\n\\textbf{Phi-3.5-Mini} & 128k & 86 & 67 & 73 & 77 & 82 & 77 \\\\\nLlama-3.1-8B-Instruct & 128k & 80 & 65 & 73 & 76 & 63 & 71 \\\\\nMixtral-8x7B-Instruct-v0.1 & 32k & 66 & 65 & 64 & 71 & 74 & 68 \\\\\nMixtral-8x22B-Instruct-v0.1 & 64k & 60 & 67 & 74 & 83 & 55 & 67.8 \\\\\n\\end{tabular}\n\\end{adjustbox}\n\\end{center}\n\\caption{Comparison results on RepoQA benchmark.}\n\\label{tbl:longrepoqa}\n\\end{table}\n\n\\begin{table}[t]\n\\begin{center}\n\\begin{adjustbox}{width=0.7\\textwidth,center}\n\\begin{tabular}{ cc||ccccccc } \nModel & Ctx Size & 4k & 8k & 16k & 32k & 64k & 128k & Average  \\\\\n\\hline & \\\\[-1.5ex]\nLlama-3.1-8B-Instruct & 128k & 95.5 & 93.8 & 91.6 & 87.4 & 84.7 & 77.0 & 88.3 \\\\\n\\textbf{Phi-3.5-MoE} & 128k & 94.8 & 93.0 & 93.2 & 91.6 & 85.7 & 64.2 & 87.1 \\\\\n\\textbf{Phi-3.5-Mini} & 128k & 94.3 & 91.1 & 90.7 & 87.1 & 78.0 & 63.6 & 84.1 \\\\\nMixtral-8x22B-Instruct-v0.1 & 64k & 95.6 & 94.9 & 93.4 & 90.9 & 84.7 & 31.7 & 81.9 \\\\\nMixtral-8x7B-Instruct-v0.1 & 32k &94.9 & 92.1 &\t92.5 &85.9 &72.4 & 44.5 & 80.4 \\\\\n\\end{tabular}\n\\end{adjustbox}\n\\end{center}\n\\caption{Comparison results on RULER benchmark.}\n\\label{tbl:longruler}\n\\end{table}\n\n\\begin{table}[t]\n\\begin{center}\n\\begin{adjustbox}{width=1.0\\textwidth,center}\n\\begin{tabular}{ c|c||cccccccc } \n\\textbf{Category} & \\textbf{Benchmark} & \\makecell{Phi-3.5-mini \\\\ \\footnotesize 3.8B}  & \\makecell{Phi-3.5-MoE \\\\ \\footnotesize 16x3.8B}  & \n\\makecell{Mistral \\\\ \\footnotesize 7B} & \\makecell{Mistral-Nemo \\\\ \\footnotesize 12B} & \\makecell{Llama-3.1-In\\\\ \\footnotesize 8B} & \\makecell{Gemma-2 \\\\ \\footnotesize 9B} & \\makecell{Gemini-1.5 \\\\ \\footnotesize Flash} & \\makecell{GPT-4o-mini} \\\\ \\hline\n\\multirow{2}{*}{Popular} & Arena Hard & 37 & 37.9 & 18.1 & 39.4 & 25.7 & 42 & 55.2 & 75 \\\\ %\\cline{2-9} \n & \\makecell{BigBench Hard \\\\ \\footnotesize CoT (0-shot)} & 69 & 79.1 & 33.4 & 60.2 & 63.4 & 63.5 & 66.7 & 80.4 \\\\ \\hline\n\\multirow{2}{*}{MMLU} & \\makecell{MMLU \\\\ \\footnotesize (5-shot)} & 69 & 78.9 &  60.3 & 67.2 & 68.1 & 71.3 & 78.7 & 77.2 \\\\ %\\cline{2-9} \n & \\makecell{MMLU-Pro \\\\ \\footnotesize (0-shot, CoT)} & 47.5 & 54.3 & 18 & 40.7 & 44 & 50.1 & 57.2 & 62.8 \\\\ \\hline\n\\multirow{9}{*}{Reasoning} & \\makecell{ARC Challenge \\\\ \\footnotesize (10-shot)} & 84.6 & 91.0 & 77.9 & 84.8 & 83.1 & 89.8 & 92.8 & 93.5 \\\\ %\\cline{2-9} \n & \\makecell{ BoolQ \\\\ \\footnotesize (2-shot) }& 78 & 84.6 & 80.5 & 82.5 & 82.8 & 85.7 & 85.8 & 88.7 \\\\ %\\cline{2-9} \n & \\makecell{GPQA \\\\ \\footnotesize (0-shot, CoT)}  & 27.2 & 36.8 & 15.6 & 28.6 & 26.3 & 29.2 & 37.5 & 41.1 \\\\ %\\cline{2-9} \n & \\makecell{ HellaSwag \\\\ \\footnotesize (5-shot) }& 69.4 & 83.8 & 71.6 & 76.7 & 73.5 & 80.9 & 67.5 & 87.1 \\\\ %\\cline{2-9} \n & \\makecell{ OpenBookQA \\\\ \\footnotesize (10-shot) } & 79.2 & 89.6 & 78 & 84.4 & 84.8 & 89.6 & 89 & 90 \\\\ %\\cline{2-9} \n & \\makecell{ PIQA \\\\ \\footnotesize (5-shot) } & 81 & 88.6 & 73.4 & 83.5 & 81.2 & 83.7 & 87.5 & 88.7 \\\\ %\\cline{2-9} \n & \\makecell{ Social IQA \\\\ \\footnotesize (5-shot) } & 74.7 & 78.0 & 73 & 75.3 & 71.8 & 74.7 & 77.8 & 82.9 \\\\ %\\cline{2-9} \n & \\makecell{ TruthfulQA \\\\ \\footnotesize (10-shot,MC2) } & 64 & 77.5 & 64.7 & 68.1 & 69.2 & 76.6 & 76.6 & 78.2 \\\\ %\\cline{2-9} \n & \\makecell{ WinoGrande \\\\ \\footnotesize (5-shot) } & 68.5 & 81.3 & 58.1 & 70.4 & 64.7 & 74 & 74.7 & 76.9 \\\\ \\hline\n\\multirow{2}{*}{Multilingual} & \\makecell{ Ml MMLU \\\\ \\footnotesize (5-shot) } & 55.4 & 69.9 & 47.4 & 58.9 & 56.2 & 63.8 & 77.2 & 72.9 \\\\ %\\cline{2-9} \n & \\makecell{ MGSM \\\\ \\footnotesize (0-shot CoT) } & 47.9 & 58.7 & 31.8 & 63.3 & 56.7 & 76.4 & 75.8 & 81.7 \\\\ \\hline\n\\multirow{2}{*}{Math} & \\makecell{ GSM8K \\\\ \\footnotesize (8-shot, CoT) } & 86.2 & 88.7 & 54.4 & 84.2 & 82.4 & 84.9 & 82.4 & 91.3 \\\\ %\\cline{2-9} \n & \\makecell{ MATH \\\\ \\footnotesize (0-shot, CoT) } & 48.5 & 59.5 & 19 & 31.2 & 47.6 & 50.9 & 38 & 70.2 \\\\ \\hline\n\\multirow{2}{*}{Long context} & Qasper & 41.9 & 40.0 & 31.4 & 30.7 & 37.2 & 13.9 & 43.5 & 39.8 \\\\ %\\cline{2-9} \n & SQuALITY & 24.3 & 24.1 & 25.9 & 25.8 & 26.2 & 0 & 23.5 & 23.8 \\\\ \\hline\n\\multirow{2}{*}{Code} & \\makecell{ HumanEval \\\\ \\footnotesize (0-shot)} & 61.5 & 70.7 & 35.4 & 63.4 & 66.5 & 61 & 74.4 & 86.6 \\\\ %\\cline{2-9} \n & \\makecell{ MBPP \\\\ \\footnotesize (3-shot) }& 68.6 & 80.8 & 50.4 & 68.1 & 69.4 & 69.3 & 77.5 & 84.1 \\\\ \\hline\n\\multicolumn{2}{c}{Average} & 61.1 & {69.2} & {48.5} & {61.3} & {61.0} & {63.3} & {68.5} & {74.9} \\\\ %\\hline\n\\end{tabular}\n\\end{adjustbox}\n\\caption{Model quality on representative benchmarks}\n\\label{tab:benchmark-comparison-3.5}\n\\end{center}\n\\end{table}\n\n\\section{Safety}\n\\textbf{Phi-3-mini} was developed in accordance with Microsoft’s responsible AI principles. The overall approach consisted of safety alignment in post-training, red-teaming, automated testing and evaluations across dozens of RAI harm categories. Helpfulness and harmlessness preference datasets \\cite{bai2022training, ji2023beavertails} with modifications inspired by \\cite{bianchi2024safetytuned} and multiple in-house generated datasets were leveraged to address the RAI harm categories in safety post-training. An independent red team at Microsoft iteratively examined \\textbf{phi-3-mini} to further identify areas of improvement during the post-training process. Based on their feedback, we curated additional datasets tailored to address their insights, thereby refining the post-training dataset. This process resulted in significant decrease of harmful response rates, as shown in Figure \\ref{fig:safety-pt}.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.9\\textwidth]{mini_safety_comparison_plot.png}\n    \\caption{Comparison of harmful response percentages by Microsoft AI Red Team between \\textbf{phi-3-mini} before and after the safety alignment. Note that the harmful response percentages in this chart are inflated numbers as the red team tried to induce \\textbf{phi-3-mini} in an adversarial way to generate harmful responses through multi-turn conversations.}\n    \\label{fig:safety-pt}\n\\end{figure}\n\nThe safety alignment of \\textbf{phi-3-small}, \\textbf{phi-3-medium} and \\textbf{phi-3.5-MoE} was conducted by undergoing the same red-teaming process, utilizing identical datasets, and incorporating a slightly larger number of samples. Table \\ref{tab:rai-benchmarks} shows the results of in-house RAI benchmarks \\cite{magooda2023framework} for \\textbf{phi-3} models compared to phi-2 \\cite{javaheripi2023phi}, Mistral-7b-v0.1 \\cite{jiang2023mistral}, Gemma 7b \\cite{gemmateam2024gemma}, and Llama-3-instruct-8b \\cite{llama3}. This benchmark utilized GPT-4 to simulate multi-turn conversations in five different categories and to evaluate the model responses. Ungroundedness between 0 (fully grounded) and 4 (not grounded) measures if the information in a response is based on a given prompt. In other categories, responses were evaluated in terms of the severity of harmfulness from 0 (no harm) to 7 (extreme harm) and the defect rates (DR-$x$) were computed as the percentage of samples with the severity score being greater than or equal to $x$.\n\n\\begin{table}\n\\begin{center}\n    \\begin{adjustbox}{width=0.95\\textwidth,center}\n    \\setlength\\extrarowheight{6pt}\n        \\begin{tabular}{ c||cccccccc } \n        & \\makecell{Phi-3-mini \\\\ \\footnotesize 3.8b} & \\makecell{Phi-3-small \\\\ \\footnotesize 7b} & \\makecell{Phi-3-medium \\\\ \\footnotesize 14b} & \\makecell{Phi-3.5-MoE \\\\ \\footnotesize 16x3.8b} & \\makecell{Phi-2 \\\\ \\footnotesize 2.7b } & \\makecell{Mistral\\\\ \\footnotesize 7b } & \\makecell{Gemma \\\\ \\footnotesize 7b} & \\makecell{Llama-3-In \\\\ \\footnotesize 8b} \\\\\n        \\hline & \\\\[-3.5ex]\n        Ungroundedness  & 0.603 & 0.299 & 0.213 & 0.228 & 1.481 & 0.935 & 0.679 & 0.328  \\\\\n        Third Party Harm (DR-1) & 0.240 & 0.253 & 0.251 & 0.105 & 0.240 & 0.562 & 0.383 & 0.373 \\\\\n        Harmful Content Continuation (DR-3) & 0.007 & 0.003 & 0.010 & 0.005 & 0.029 & 0.026 & 0.013 & 0.013 \\\\\n        Harmful Content Summarization (DR-3) & 0.100 & 0.110 & 0.112 & 0.12 & 0.144 & 0.223 & 0.103 & 0.082 \\\\\n        Jailbreak (DR-1) & 0.123 & 0.107 & 0.111 & 0.106 & 0.150 & 0.156 & 0.114 & 0.130 \\\\\n        \\end{tabular}\n    \\end{adjustbox}\n\\end{center}\n\\caption{Comparison of Microsoft internal multi-turn conversation RAI benchmark results of \\textbf{phi-3} models and other models. Note that a lower value indicates a better performance for all metrics in the table.}\n\\label{tab:rai-benchmarks}\n\\end{table}\n\n\\section{Weakness}\nIn terms of LLM capabilities, while $\\textbf{phi-3-mini}$ model achieves similar level of language understanding and reasoning ability as much larger models, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much ``factual knowledge'', which can be seen for example with low performance on TriviaQA.\nHowever, we believe such weakness can be resolved by augmentation with a search engine. We show an example using the HuggingFace default Chat-UI with \\textbf{phi-3-mini} in Figure \\ref{fig:search}. Another weakness related to model's capacity is that we mostly restricted the language to English. Exploring multilingual capabilities for Small Language Models is an important next step, with some initial promising results on \\textbf{phi-3-small} by including more multilingual data.\n\nDespite our diligent RAI efforts, as with most LLMs, there remains challenges around factual inaccuracies (or hallucinations), reproduction or amplification of biases, inappropriate content generation, and safety issues. The use of carefully curated training data, and targeted post-training, and improvements from red-teaming insights significantly mitigates these issues across all dimensions. However, there is significant work ahead to fully  address these challenges, and downstream use of the models should be evaluated for the specific use cases and safety considerations for that context.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.48\\textwidth]{without_search.png}    \\includegraphics[width=0.48\\textwidth]{with_search.png}\n    \\caption{Left: \\textbf{phi-3-mini}'s completion without search. Right: \\textbf{phi-3-mini}'s completion with search, using the default HuggingFace Chat-UI search ability. For reference, the 2026 Winter Olympic Games are scheduled to be held in Milano and Cortina in Italy, while the 2022 and 2018 Winter Olympic Games were held in Beijing, China and PyeongChang, Korea, respectively. Without the search results, the response is incorrect, while with the web search, not only does the response become accurate, but also gets more specific with suggestions.}\n    \\label{fig:search}\n\\end{figure}\n\n\\section{Phi-3.5-Vision}\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.98\\textwidth]{phi3v-teaser.png}\n    \\caption{The demo case shows \\phivision's capability in natural image understanding and reasoning.}\n    \\label{fig:v-safety-pt}\n\\end{figure}\n\n\\subsection{Technical Specifications}\n\n\\paragraph{Architecture}\n\nThe \\textbf{\\phivision} (4.2B parameters) is a multimodal model designed to process an image/multi-image and a textual prompt as inputs, and subsequently generate textual outputs. This model is composed of two primary components: an image encoder, \\emph{i.e.}, CLIP ViT-L/14~\\cite{radford2021learning} and a transformer decoder, \\emph{i.e.}, phi-3.5-mini. The visual tokens, once extracted by the image encoder, are then combined with text tokens in an interleaved way (no particular order for image and text tokens). To accommodate high-resolution images and various aspect ratios, a dynamic cropping strategy~\\cite{dong2024internlm} is utilized to split the input image into a 2d array of blocks, where the tokens of the blocks are concatenated to represent the whole image.  For multi-image input, we simply concatenated tokens from each images together.\n\n\\paragraph{Pre-training} \n\nThe \\textbf{\\phivision} model undergoes a pre-training phase using a diverse dataset, which consists of a combination of interleaved image-text documents (\\emph{e.g.}, ~\\cite{laurenccon2024obelics}), image-text pairs from FLD-5B ~\\cite{xiao2023florence}, synthetic data derived from Optical Character Recognition (OCR) of PDF files, datasets for chart/table comprehension, and text-only data. The objective of predicting the next token is employed specifically on text tokens, while any loss associated with image tokens is disregarded during this phase. The pre-training process involves a total of $0.5T$ tokens that encompass both visual and text elements. During the pre-training phase, the maximum image resolution is capped at $1344 \\times 1344$ as the majority of the training images are smaller than this resolution.\n\n\\paragraph{Post-training.} \n\nThe \\textbf{\\phivision} model contains two post-training stages: supervised finetuning (SFT) and direct preference optimization (DPO). For SFT, we leveraged text SFT dataset, public multimodal instruct tuning datasets along with large-scale multimodal instruct tuning datasets that we built ourselves, covering diverse domains and tasks such as general natural image understanding, chart/table/diagram understanding/reasoning, PowerPoint understanding, multi-image comparison, video summarization and model safety. The multimodal SFT data has about a total of 33B tokens. For DPO we mainly use a text DPO dataset and a relatively smaller-scale multimodal DPO dataset. For these two stages, we jointly train multimodal tasks and text-only tasks so that the model can achieve multi-modal reasoning while maintaining language capabilities as much as possible. \n\n\\subsection{Academic benchmarks}\n\n\\subsubsection{Single-image Benchmarks}\nWe report in Table~\\ref{tab:mm-benchmarks} the evaluation results of Phi-3.5-Vision on nine open-source academic benchmarks. These benchmarks evaluate reasoning and perceptual capabilities on visual and text inputs and can be grouped in three categories: Science, Charts, and Generic knowledge. We compare Phi-3.5-Vision with the following baselines: MM1-3B-Chat~\\cite{mckinzie2024mm1}, MM1-7B-Chat~\\cite{mckinzie2024mm1}, Llava-1.6 Vicuna 7B~\\cite{liu2023improved}, Llava-1.6 Llama3-8B~\\cite{liu2024llavanext}, Qwen-VL-Chat~\\cite{bai2023qwenvl}, Claude 3 Haiku~\\cite{anthropic2024claude}, Gemini 1.0 Pro V~\\cite{team2023gemini}, and GPT-4O. Our performance quality assessment setup used the same evaluation pipeline for all the baselines to ensure a fair comparison, with the exception of MM1-3B-Chat. We just copied and pasted their published numbers since the model is not publicly available.\n\nOur evaluation setup aimed to mimic scenarios where regular users interact with a multi-modal model, i.e., users who are not experts in prompt engineering or know special techniques that can improve performance. For this reason, we adopted the evaluation setting used in Llava-1.5~\\cite{liu2023improved}. In this setup, the prompts include instructions to select a single letter corresponding to an answer from a list of given options, or answer with a single word or phrase. In our prompts, we did not use specific tokens for multiple-choice questions. Moreover, we did not scale or pre-process any image in our benchmarking system. We placed the images as the first item in the prompts, except on the MMMU dataset where the prompts interleave the images anywhere in the question or the answers. Lastly, our evaluation setup only considered a 0-shot format. Because of these evaluation parameters, our reported numbers can differ from the published numbers of the considered baselines. As we can seen, our Phi-3.5-Vision achieves super competitive results on all benchmarks and outperform other competitor models on most benchmarks while being smaller.\n\n\\subsubsection{Multi-image Benchmarks}\nWe report in Table~\\ref{tab:mm-multi-benchmarks} the evaluation results of Phi-3.5-Vision on one latest academic multi-image benchmark and one video benchmark. These benchmarks evaluate perceptual capabilities on multiple image/frames and text covering a wide range of general scenarios (e.g., Art and Style recognition, Forensic detection, and video understanding). We compare Phi-3.5-Vision with the following baseline methods: Llava Interleave-Qwen 7B \\cite{li2024llava}, InternVL2 4B and 8B \\cite{chen2024far}, Gemini 1.5 Flash \\cite{team2023gemini}, GPT-4o-mini, Claude 3.5 Sonnet \\cite{anthropic2024claude}, Gemini 1.5 Pro \\cite{team2023gemini}, and GPT-4O. Line in the single-frame evaluation case, our performance quality assessment setup used the same evaluation pipeline for all the baselines to ensure a fair comparison.\n\nOur evaluation setup for multi-image also followed the Llava setup where  prompts include instructions to select a single letter corresponding to an answer from a list of given options, or answer with a single word or phrase. Moreover, we did not use specific tokens for multiple-choice questions and we did not scale or pre-process any image in our benchmarking system. For most of the benchmarks, we placed the images as the first item in the prompts.\n\nThe evaluation pipelines for BLINK and VideoMME benchmarks differ from those published. In the case of BLINK, we do not use ChatGPT as the final answer selection mechanism. Instead, we instruct the evaluated model to select one answer directly from the given choices. The reason is that in this manner we ensure that the mistakes or successes come solely by the evaluated model. For the VideoMME benchmark, we extracted 16 frames from the video by sampling frames at a given rate that ensures a uniform time coverage of the entire video. We used 16 frames since this is the maximum number of images a prompt can contain for Azure OpenAI models. Unlike the proposed evaluation in VideoMME that uses the maximum number of frames a model can accept, we always pass the same amount of frames across all the considered model baselines. In this way we ensure the evaluations are fair since all the models receive the exact same input information (i.e., the prompt and set of images). As shown in Table~\\ref{tab:mm-multi-benchmarks}, our Phi-3.5-Vision performs very competitively or outperforms baseline models under the similar model size in multi-image understanding scenarios as well.\n\n\\begin{table}[t]\n\\begin{center}\n\\begin{adjustbox}{width=1.0\\textwidth,center}\n\\begin{tabular}{ c||cccccccccc } \n\n\\label{tbl:phi-v-benchmarks}\n\n\\\\[10ex]\n& \\rothead{\\makecell{\\phivision\\\\ \\footnotesize 4.2b}} & \\rothead{\\makecell{MM1-3B-Chat\\\\ \\footnotesize 3.6b~\\cite{mckinzie2024mm1}}} &  \n\\rothead{\\makecell{MM1-7B-Chat\\\\ \\footnotesize 7.6b~\\cite{mckinzie2024mm1}}} &\n\\rothead{\\makecell{LLaVA-1.6\\\\ \\footnotesize Vicuna-7b~\\cite{liu2023improved}}} & \\rothead{\\makecell{LLaVA-Next \\\\ \\footnotesize LLama3-8b~\\cite{liu2024llavanext}}} &  \\rothead{\\makecell{Qwen-VL-Chat\\\\ \\footnotesize 9.6b~\\cite{bai2023qwenvl}}} &\\rothead{\\makecell{Claude 3 haiku \\\\ \\footnotesize~\\cite{anthropic2024claude}}} &\\rothead{\\makecell{Gemini 1.0 Pro V \\\\ \\footnotesize ~\\cite{team2023gemini}}}   &  \\rothead{\\makecell{GPT-4O \\\\ \\footnotesize 2024-05-13}} \\\\\n\n\\hline & \\\\[-1.5ex]\n\n\\datasetcell{\\small MMMU}{\\scriptsize val}{\\cite{yue2023mmmu}} & 43.0 & 33.9& 37.0& 34.2& 36.4& 39.0& 40.7& 42.0& 61.8\\\\\n\\datasetcell{\\small ScienceQA}{\\scriptsize test}{\\cite{lu2022learn}}  & 91.3& 69.4& 72.6& 70.6& 73.7& 67.2& 72.0& 79.7& 88.5\\\\\n\\datasetcell{\\small MathVista}{\\scriptsize testmini}{\\cite{lu2024mathvista}} & 43.9& 32.0& 35.9& 31.5& 34.8& 29.4& 33.2& 35.0& 54.4\\\\\n\\datasetcell{\\small Inter-GPS}{\\scriptsize test}{\\cite{lu2021intergps}} & 36.3& -& -& 20.5& 24.6& 22.3& 32.1& 28.6& 46.9\\\\\n\\hline & \\\\[-1.5ex]\n\n\\datasetcell{\\small MMBench}{\\scriptsize dev-en}{\\cite{liu2024mmbench}} & 81.9& 75.9& 79.0& 76.3& 79.4& 75.8& 62.4& 80.0& 88.4 \\\\\n\\datasetcell{\\small POPE}{\\scriptsize test}{\\cite{li2023evaluating}} & 86.1& 87.4& 86.6& 87.2& 87.0& 82.6& 74.4& 84.2& 87.0\\\\\n\\hline & \\\\[-1.5ex]\n\n\\datasetcell{\\small AI2D}{\\scriptsize test}{\\cite{kembhavi2016diagram}} & 78.1& -& -& 63.1& 66.9& 59.8& 60.3& 62.8& 82.8\\\\\n\\datasetcell{\\small ChartQA}{\\scriptsize test}{\\cite{masry-etal-2022-chartqa}} & 81.8& -& -& 55.0& 65.8& 50.9& 59.3& 58.0& 64.0\\\\\n\\datasetcell{\\small TextVQA}{\\scriptsize test}{\\cite{singh2019vqa}} & 72.0& 71.9& 72.8& 64.6& 55.7& 59.4& 62.7& 64.7& 75.6\\\\\n\n\\end{tabular}\n\\end{adjustbox}\n\\end{center}\n\\caption{Comparison results on public MLLM benchmarks. All the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable except for MM1-3B-Chat~\\cite{mckinzie2024mm1} and MM1-7B-Chat~\\cite{mckinzie2024mm1}, which are not publicly available. We adopted the evaluation setting used in Llava-1.5~\\cite{liu2023improved}, without any specific prompt or pre-processing image for all results. These numbers might differ from other published numbers due to slightly different prompts.}\n\\label{tab:mm-benchmarks}\n\\end{table}\n\n\\begin{table}[t]\n\\begin{center}\n\\begin{adjustbox}{width=1.0\\textwidth,center}\n\\begin{tabular}{ c||ccccccccc } \n\n\\label{tbl:phi-multi-benchmarks}\n\n\\\\[10ex]\n& \\rothead{\\makecell{\\phivision\\\\ \\footnotesize 4.2b}} & \\rothead{\\makecell{Llava-interleave\\\\ \\footnotesize Qwen 7b~\\cite{li2024llava}}} &  \n\\rothead{\\makecell{InternVL2\\\\ \\footnotesize 4b~\\cite{chen2024far}}} &\n\\rothead{\\makecell{InternVL2\\\\ \\footnotesize 8b~\\cite{chen2024far}}} & \\rothead{\\makecell{Gemini 1.5 \\\\ \\footnotesize Flash~\\cite{team2023gemini}}} &  \\rothead{\\makecell{GPT4O mini\\\\ \\footnotesize 2024-07-18}} &\\rothead{\\makecell{Claude 3.5 \\\\ \\footnotesize Sonnet ~\\cite{anthropic2024claude}}} &\\rothead{\\makecell{Gemini 1.5 Pro  \\\\ \\footnotesize ~\\cite{team2023gemini}}}  &  \\rothead{\\makecell{GPT-4O \\\\ \\footnotesize 2024-05-13}} \\\\\n\n\\hline & \\\\[-1.5ex]\n\\datasetcell{\\small BLINK}{\\scriptsize val}{\\cite{fu2024blink}} & 57.0 & 53.1 & 45.9  & 45.4 & 45.8  & 51.9 & 56.5 & 61.0 & 63.2\\\\\n\\datasetcell{\\small VideoMME}{\\scriptsize test}{\\cite{fu2024video}} & 50.8& 50.2 & 49.9&52.6 & 62.3& 61.2& 55.9 & 62.6 & 68.4\\\\\n\n\\end{tabular}\n\\end{adjustbox}\n\\end{center}\n\\caption{Comparison results on public multi-image/video MLLM benchmarks. All the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable.}\n\\label{tab:mm-multi-benchmarks}\n\\end{table}\n\n\\subsection{Safety}\nTo ensure the integration of \\textbf{\\phivision} aligns with Microsoft's Responsible AI (RAI) principles, we involved safety post-training in both Supervised Fine-Tuning (SFT) stage and Direct Preference Optimization (DPO) stage. In creating the safety training datasets, we utilized not only the text-only RAI datasets, but also a variety of in-house Multi-Modal (MM) RAI datasets that cover various harm categories identified in both public and internal MM RAI benchmarks. For the purpose of RAI evaluation, we performed a rigorous quantitative assessment on both public and internal benchmarks, this was done in conjunction with a human evaluation conducted by Microsoft's internal red team.\n\n\\begin{table}\n\\begin{center}\n    \\begin{adjustbox}{width=1.0\\textwidth,center}\n    \\setlength\\extrarowheight{6pt}\n        \\begin{tabular}{ c||ccccc } \n        & \\makecell{\\phivision \\\\ \\footnotesize 3.8b+0.3b}& \\makecell{\\phivision~w/o safety \\\\ \\footnotesize 3.8b+0.3b} & \\makecell{Llava-1.6 Vicuna \\\\ \\footnotesize 7b+0.3b } & \\makecell{Qwen-VL-Chat\\\\ \\footnotesize 7.7b+1.9b } & \\makecell{GPT4-V \\\\ \\footnotesize N/A}  \\\\\n        \\hline & \\\\[-3.5ex]\n        Internal (private) &8.16 & 7.06  & 5.44 & 7.27 &  8.55  \\\\\n        RTVLM (public) &5.44 & 3.56  &  3.86& 4.78 & 6.81  \\\\\n        VLGuard (public) &9.10 & 4.66 & 5.62 & 8.33  & 8.90   \\\\\n        \\end{tabular}\n    \\end{adjustbox}\n\\end{center}\n\\caption{Comparison results on public and private multi-modal RAI benchmarks. Note that all metrics in the table are [0,10] and a higher value indicates a better performance.}\n\\label{tab:mmrai-benchmarks}\n\\end{table}\n\nIn Table \\ref{tab:mmrai-benchmarks}, we present the evaluation outcomes of \\phivision on three MM RAI benchmarks: one internal and two public benchmarks (specifically, RTVLM \\cite{li2024red} and VLGuard \\cite{zong2024safety}). We juxtapose these results with those of other open-source models such as Llava-1.5 \\cite{liu2023improved}, Llava-1.6 \\cite{liu2024llavanext}, Qwen-VL-Chat \\cite{bai2023qwenvl}, and GPT4-V\\cite{gpt4v}. The results clearly indicate that safety post-training notably enhances the RAI performance of \\phivision across all RAI benchmarks. In Figure \\ref{fig:v-safety-pt}, we further breakdown the performance across different RAI categories of the VLGuard and Internal benchmarks, demonstrating that safety post-training can aid \\phivision in improving RAI performance in nearly all categories.\n \n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.98\\textwidth]{categorized_RAI.pdf}\n    \\caption{Comparison of categorized RAI performance of \\phivision with and without the safety post-training on the VLGuard (left) and Internal (right) benchmark, respectively.  It clearly indicates that safety post-training can enhance the RAI performance across nearly all the RAI categories.}\n    \\label{fig:v-safety-pt}\n\\end{figure}\n\n\\subsection{Weakness}\nRegarding the multi-modal LLM capabilities of our \\phivision, it performs admirably across various fields. However, we have identified certain limitations, particularly with questions necessitating high-level reasoning abilities. Additionally, the model has been observed to occasionally generate ungrounded outputs, making it potentially unreliable in sensitive areas, such as finance. To mitigate these issues, we will incorporate more reasoning-focused and hallucination-related DPO data into post-training in the future. \n\nFrom a responsible AI standpoint, whilst safety post-training has made significant strides, our \\phivision occasionally fails to refrain from answering harmful or sensitive inquiries. Examples of such occasions include deciphering particular types of captcha and describing scam images containing disinformation or hallucination. We find that this issue partly arises from the capabilities, such as OCR, acquired during the training process with normal instruct tuning datasets, which can be regarded as the trade-off between helpfulness and harmlessness. Moving forward, we need to further explore this area to achieve a better balance.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Phi-4 Technical Report}\n\n\\begin{document}\n\n\\title{Phi-4 Technical Report}\n\\author{\n\\begingroup\n\\setlength{\\tabcolsep}{10pt}\n\\begin{tabular}{cccc}\nMarah Abdin & Jyoti Aneja & Harkirat Behl & S\\'ebastien Bubeck \\\\\nRonen Eldan & Suriya Gunasekar & Michael Harrison & Russell J. Hewett \\\\\nMojan Javaheripi & Piero Kauffmann & James R. Lee & Yin Tat Lee \\\\\nYuanzhi Li & Weishung Liu & Caio C. T. Mendes & Anh Nguyen \\\\\nEric Price & Gustavo de Rosa & Olli Saarikivi & Adil Salim \\\\\nShital Shah & Xin Wang & Rachel Ward & Yue Wu \\\\\nDingli Yu & Cyril Zhang & Yi Zhang &\n\\end{tabular}\n\\endgroup\n}\n\\date{Microsoft Research}\n\\maketitle\n\n\\begin{abstract}\nWe present \\textbf{\\modelwithoutspace}, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, \\model strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely \\emph{distill} the capabilities of a teacher model (specifically GPT-4), \\model substantially \\emph{surpasses} its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the \\phithree{} architecture, \\model achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme. \n\\end{abstract}\n\n\\section{Introduction}\n\nRecent advancements in Large Language Models (LLMs) have shown that significant improvements in data quality can rival, and sometimes surpass, the performance gains traditionally achieved by scaling compute with model and dataset size. Building on the success of the {Phi} family \\cite{gunasekar2023textbooks,li2023textbooks,javaheripi2023phi,abdin2024phi}, we introduce \\modelwithoutspace, a 14-billion parameter model that further advances performance of small language models by introducing innovative synthetic data generation methods for reasoning-focused tasks, by optimizing the training curriculum and data mixture, and by introducing new techniques in post-training.\n\nSynthetic data constitutes the bulk of the training data for \\model and is generated using a diverse array of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal. These methods enable the construction of  datasets that induce stronger reasoning and problem-solving abilities in the model, addressing some of the weaknesses in traditional unsupervised datasets. Synthetic data in \\model also plays a crucial role in post-training, where techniques such as rejection sampling and a novel approach to Direct Preference Optimization (DPO) are employed to refine the model’s outputs.\n\nThe development of \\model is guided by three core pillars: \\begin{enumerate} \n\\item \n\\textbf{Synthetic Data for Pretraining and Midtraining:} High-quality synthetic datasets are designed to prioritize \\emph{reasoning} and \\emph{problem-solving}, carefully generated to ensure diversity and relevance. We change our training curriculum and create new pretraining and midtraining data mixtures to increase the allocation of synthetic tokens, compared to older generations of \\textsf{phi}.\n\n\\footnotetext[1]{These scores are lower than those reported by Meta, perhaps because \\textsc{simple-evals} has a strict formatting requirement that Llama models have particular trouble following.  We use the \\textsc{simple-evals} framework because it is reproducible, but Meta reports 77 for MATH and 88 for HumanEval on Llama-3.3.}\n\\stepcounter{footnote}\n\n\\item \n\\textbf{Curation and Filtering of High-Quality Organic Data:} We meticulously curate and filter organic\\footnote{We use \\textit{organic} to refer to human-generated or otherwise non-synthetic data.} data sources, including web content, licensed books, and code repositories to extract seeds for the synthetic data pipeline that encourage high-depth reasoning and prioritize educational value (to the model). These seeds form the foundation of the synthetic generation pipeline. \nTo complement these synthetic datasets, we also filter the web for high-quality data (in terms of knowledge and reasoning) to use directly in pretraining.\n\n\\item \\textbf{Post-Training:} We further advance the post-training recipe in \\model by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on \\emph{pivotal token search}.\n\n\\end{enumerate}\n\nWith these innovations, the performance of \\model on reasoning-related tasks is comparable to or surpasses much larger models. For example, its performance on many widely used reasoning-related benchmarks meets or exceeds that of \\text{Llama-3.1-405B}. In Table \\ref{tbl:benchmarks} we compare the performance of our model on academic benchmarks to several contemporary foundation models. We find that \\model significantly exceeds its teacher GPT-4o on the GPQA (graduate-level STEM Q\\&A) and MATH (math competition) benchmarks.\n\n\\renewcommand{\\arraystretch}{1.2}\n\\begin{table}[t!]\n\\centering\n\\small\n\\begin{tabular}{@{}cc cccc ccc@{}}\n\\toprule\n & & \\multicolumn{4}{c}{\\textbf{Small models}} & \\multicolumn{3}{c}{\\textbf{Large models}}\\\\\n\\cmidrule(lr){3-6}\\cmidrule(lr){7-9}\n & & \\makecell{\\textbf{phi-4}\\\\14b} & \\makecell{\\textbf{phi-3}\\\\14b} & \\makecell{\\textbf{Qwen 2.5}\\\\14b instruct} & \\makecell{\\textbf{GPT}\\\\4o-mini} & \\makecell{\\textbf{Llama-3.3}\\\\70b instruct} & \\makecell{\\textbf{Qwen 2.5}\\\\72b instruct} & \\makecell{\\textbf{GPT}\\\\4o}\\\\\n\\midrule\n\\multirow{7}{*}{\\rotatebox[origin=c]{90}{\\textbf{simple-evals}}} \n & MMLU & 84.8 & 77.9 & 79.9 & 81.8 & 86.3 & 85.3 & \\textbf{88.1}\\\\\n & GPQA & \\textbf{56.1} & 31.2 & 42.9 & 40.9 & 49.1 & 49.0 & 50.6\\\\\n & MATH & \\textbf{80.4} & 44.6 & 75.6 & 73.0 & 66.3\\footnotemark[1] & 80.0 & 74.6\\\\\n & HumanEval & 82.6 & 67.8 & 72.1 & 86.2 & 78.9\\footnotemark[1] & 80.4 & \\textbf{90.6}\\\\\n & MGSM & 80.6 & 53.5 & 79.6 & 86.5 & 89.1 & 87.3 & \\textbf{90.4}\\\\\n & SimpleQA & 3.0 & 7.6 & 5.4 & 9.9 & 20.9 & 10.2 & \\textbf{39.4}\\\\\n & DROP & 75.5 & 68.3 & 85.5 & 79.3 & \\textbf{90.2} & 76.7 & 80.9\\\\\n\\midrule\n & MMLUPro & 70.4 & 51.3 & 63.2 & 63.4 & 64.4 & 69.6 & \\textbf{73.0}\\\\\n & HumanEval+ &82.8 & 69.2 &79.1 & 82.0 &77.9 & 78.4 & \\textbf{88.0}\\\\\n & ArenaHard &75.4 &45.8 &70.2 & 76.2 &65.5 &\\textbf{78.4} &75.6 \\\\\n & LiveBench &47.6 &28.1 &46.6 &48.1 &\\textbf{57.6} & 55.3 &\\textbf{57.6}\\\\\n & IFEval &63.0 &57.9 &78.7 &80.0 &\\textbf{89.3} & 85.0 &84.8\\\\\n\\midrule\n & \\makecell{PhiBench\\\\(internal)} &56.2 &43.9 &49.8 &58.7 &57.1 &64.6 & \\textbf{72.4}\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Performance of \\model{} on a set of standard benchmarks. The first set of benchmarks uses OpenAI's \\textsc{simple-evals} framework~\\cite{simple-evals}, specifying the prompts/extraction/temperature=0.5. We compare to small models of similar inference cost, as well as to larger models.\n}\n\\label{tbl:benchmarks}\n\\end{table}\n\n\\subsection{Addressing Overfitting and Data Contamination}\n\n\\paragraph{Decontamination:}\nOne pitfall of foundation models is overfitting to benchmarks, such as through the leakage of benchmark test sets via the web corpus.  We improved the data decontamination process for \\model compared to previous Phi models to ensure no unfair influence on evaluation results. More details of the decontamination method are given in Appendix \\ref{sec:data_processing}.\n\n\\paragraph{AMC Benchmark:}  \nThe surest way to guard against overfitting to the test set is to test on fresh data. We tested our model on the November 2024 AMC-10 and AMC-12 math competitions~\\cite{amc}, which occurred after all our training data was collected, and we only measured our performance after choosing all the hyperparameters in training our final model.  These contests are the entry points to the Math Olympiad track in the United States and over 150,000 students take the tests each year. \n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.85\\linewidth]{amcplot.pdf}\n    \\caption{Average performance of different models on the November 2024 AMC-10 and AMC-12 tests.  This is the average score (with maximum score 150) over the four tests on 100 runs with temperature $t=0.5$. We chose $t = 0.5$ to follow \\textsc{simple-evals}~\\cite{simple-evals}.  Error bars are $2\\sigma$ of the estimate.  On competition math, \\model scores well above its weight-class even compared to non--open-weight models.}\n    \\label{fig:AMC}\n\\end{figure}\n\nIn Figure~\\ref{fig:AMC} we plot the average score over the four versions of the test, all of which have a maximum score of 150.  \\model outperforms not only similar-size or open-weight models but also much larger frontier models. Such strong performance on a fresh test set suggests that \\modelwithoutspace's top-tier performance on the MATH benchmark is not due to overfitting or contamination. We provide further details in Appendix~\\ref{sec:amc-appendix-details}.\n\n\\paragraph{Relying on Contamination-Proof Benchmarks:}\nWe give significant weight to benchmarks which were designed in such a way that the questions are original and do not appear on the web, such as GPQA~\\cite{rein2023gpqa}. While optimizing our model, we relied on an internal benchmark composed primarily of original prompts written by the team (see Section \\ref{sec:phibench} for further details).\n\n\\paragraph{Long Chain-of-Thought Models:}\nA style of LLM that scales inference-time compute by generating long chains of thought has emerged over the past few months, as pioneered by OpenAI O1~\\cite{openai_learning_to_reason} and followed by DeepSeek-R1-Lite-Preview~\\cite{deepseek_r1_lite_preview} and Qwen/QwQ-32B-Preview~\\cite{qwq-32b-preview}.  These models perform well on reasoning benchmarks, where QwQ, the only such model with open weights, averages 124.5 points in the AMC-10/12 setting of Figure~\\ref{fig:AMC}.  However, QwQ also uses 4X more tokens on this task than \\model and has more than twice as many parameters.  Thus, the inference cost of QwQ is an order of magnitude higher than \\modelwithoutspace.  Consequently, these models are not in the same class as \\model with respect to cost or latency.\n\n\\section{Approach to Data}\nThe pretraining phase of \\model relies heavily on synthetic datasets generated through a variety of techniques. In addition, we employ several methods for filtering organic data sources that are used both as complementary datasets in the pretraining and as seeds for generating synthetic data.\n\n\\subsection{Purpose of Synthetic Data}\n\nSynthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data.  Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data.\n\n\\paragraph{Structured and Gradual Learning.} In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting reasoning patterns. In this way, synthetic data may act as a form of ``spoonfeeding,\" presenting challenges in a digestible and progression-oriented manner.\n\nA simple example to illustrate this is that a human-written solution to a math problem might start with the final answer.  This answer is much too hard to output immediately, for either a human or an LLM---the human produced it by nonlinear editing, but pretraining expects the LLM to learn to produce it linearly.  Synthetic solutions to math problems will not have such roadblocks.\n\n\\paragraph{Alignment with Inference Contexts.} Synthetic data is typically closer to the format of outputs we expect our models to generate. Training on such data helps align the model’s pretraining experience with the scenarios it encounters during inference. This alignment ensures that the context seen during generation remains in-distribution with respect to the data the model was pretrained on.\n\nFor example, web forums are very different in style from LLM interactions.  If a fact only appears in web forum data, the pretrained model will think it is very unlikely to occur in the chats it produces.  Rewriting facts from the web forum into the language style of an LLM makes the facts more accessible during the LLM chat context of inference.\n\n\\paragraph{Principles.} Our approach to generating synthetic data for \\model is guided by the following principles:\n\\begin{enumerate}\n    \\item \\textbf{Diversity:} The data should comprehensively cover subtopics and skills within each domain. This requires curating diverse seeds from organic sources.\n    \\item \\textbf{Nuance and Complexity:} Effective training requires nuanced, non-trivial examples that reflect the complexity and the richness of the domain. Data must go beyond basics to include edge cases and advanced examples.\n    \\item \\textbf{Accuracy:} Code should execute correctly, proofs should be valid, and explanations should adhere to established knowledge, etc.\n    \\item \\textbf{Chain-of-Thought:} Data should encourage systematic reasoning, teaching the model various approaches to the problems in a step-by-step manner. This fosters coherent outputs for complex tasks.\n\\end{enumerate}\n\n\\subsection{Synthetic Data for Pretraining and Midtraining}\n\nWe created 50 broad types of synthetic datasets, each one relying on a different set of seeds and different multi-stage prompting procedure, spanning an array of topics, skills, and natures of interaction, accumulating to a total of about 400B unweighted tokens. In Appendix \\ref{app:synthexamples}, we give a few examples of transcripts taken from our synthetic generations.\nHere, we highlight novel methodologies used in generating synthetic datasets for \\modelwithoutspace:\n\\begin{itemize}\n\\item \\textbf{Seed Curation:} The synthetic dataset generation begins with high-quality seeds sourced from multiple domains.  These curated seeds provide the foundation for synthetic data generation, enabling the creation of exercises, discussions, and reasoning tasks tailored to the model's training objectives.\n\\begin{enumerate}\n    \\item \\textbf{Web and Code-based Seeds:} Excerpts and snippets are extracted from web pages, books, and code repositories with a focus on content that demonstrates high complexity, reasoning depth, and educational value. To ensure quality, we employ a two-stage filtering process: first, identifying pages with strong educational potential, and second, segmenting the selected pages into passages, scoring each for its factual and reasoning content.\n    \\item \\textbf{Question Datasets:} A large set of questions was collected from websites, forums, and Q\\&A platforms. These questions were then filtered using a plurality-based technique to balance difficulty. Specifically, we generated multiple independent answers for each question and applied majority voting to assess the consistency of responses. We discarded questions where all answers agreed (indicating the question was too easy) or where answers were entirely inconsistent (indicating the question was too difficult or ambiguous). This filtering process produces a dataset of questions that challenge the model's reasoning and problem-solving abilities while remaining approachable. The plurality answers were used in place of the ground truth in our rejection-sampling based generations. \n    \\item \\textbf{Creating Question-Answer pairs from Diverse Sources:} Another technique we use for seed curation involves leveraging language models to extract question-answer pairs from organic sources such as books, scientific papers, and code. This approach does not rely on merely identifying explicit Q\\&A pairs within the text. Instead, it involves a pipeline designed to detect deduction chains or logical progressions in the text. The language model identifies key steps in reasoning or problem-solving processes and reformulates them into questions and corresponding answers. Our experiments show that, if done correctly, training on the resulting content can be far more effective (in terms of improvement on academic and internal benchmarks) than training on the original content. \n\\end{enumerate}\n    \\item \\textbf{Rewrite and Augment:} Seeds are transformed into synthetic data through multi-step prompting workflows. This includes rewriting most of the useful content in given passages into exercises, discussions, or structured reasoning tasks. \n    \\item \\textbf{Self-revision:} The initial responses are then iteratively refined through a feedback loop where a model critiques and subsequently improves its own outputs, guided by the rubrics focused on reasoning and factual accuracy.\n    \\item \\textbf{Instruction Reversal for Code and Other Tasks:} To enhance the model’s ability to generate outputs from instructions, we used an instruction reversal technique. For example, we take existing code snippets from the code data corpus and use it to generate corresponding instructions that include the problem description or task prompt. The resulting synthetic data pairs were structured with the instruction appearing before the code. Only data with high fidelity between the original and regenerated code are retained, ensuring alignment between the instructions and the outputs. This method can be generalized to other targeted use cases. \n    \\item \\textbf{Validation of Code and Other Scientific Data:} When appropriate, we incorporate tests for validating our reasoning-heavy synthetic datasets. The synthetic code data is validated through execution loops and tests. For scientific datasets, the questions are extracted from scientific materials using a method designed to ensure high relevance, groundedness, and difficulty balance. \n\\end{itemize}\n\n\\subsection{Curation and Filtering of Web and Q\\&A Data}\n\\paragraph{Q\\&A datasets.}\nWe collected tens-of-millions high-quality organic problems and solutions by reviewing public websites, relying on existing datasets, and acquiring external datasets. Our experience from previous models showed that question-answer data contributed significantly to various capabilities, such as mathematical reasoning and academic performance. Our ablation studies showed that organic questions are substantially more effective than synthetic questions. We used several ways to synthetically augment the dataset of organic questions to obtain a larger dataset. While these rewritten questions improved the model’s capabilities, the gains were not as pronounced.\nA significant portion of the collected questions lacked accurate solutions. To address this, we replaced the answers with synthetically generated ones and used majority-voting to increase accuracy.\nAll collected questions and solutions underwent a thorough decontamination process to ensure there is no overlap with test sets\\footnote{This step is crucial to the reliability of some of the academic benchmarks: for instance, some test benchmark variants can be found on platforms like Hugging Face. Moreover, benchmarks such as MMLU are frequently compiled from web-sourced questions.}.\n\n\\paragraph{Targeting High-quality Web Data.}\\label{sec:web_data}\nWe collected a wide variety of high-quality organic data sources for \\modelwithoutspace, prioritizing reasoning-dense and nuanced material (e.g., academic papers, educational forums, and programming tutorials). In addition to directly training on this text, we used various web sources as seeds for specialized synthetic data generation pipelines. We found clean and correct natural data to be absolutely crucial for seeding synthetic data: minor errors can result in severe quality degradations for derived synthetic documents. We therefore invested heavily in the \\textit{perfectionistic} curation of our web data. We discuss the main techniques and considerations below:\n\n\\begin{itemize}\n\\item \\textbf{Targeted Acquisitions:} We included major repositories of reasoning-dense documents that are publicly permissible for use (e.g., arXiv, PubMed Central, GitHub) or explicitly licensed (e.g., licensed books) aiming for a level of comprehensiveness, recency, and cleanliness above the typical standard of externally available corpora. \n\\item \\textbf{Filtering Web Dumps:} To capture the long tail of information-rich web sources (e.g., forums, blogs, course material, domain-specific wikis), we took the approach of selecting a small fraction of highest-quality documents from bulk web dumps, using small (non-LLM) classifiers trained on $\\sim10^6$ LLM-generated annotations. This approach tends to over-index on STEM-related keywords, so we created a specialized pipeline to amplify high-quality non-STEM content (e.g., arts, history, travel, culture, and entertainment). These topic classifications were also obtained by distilling an LLM annotator. Finally, we removed corrupted text and binary files by detecting outliers according to $n$-gram statistics and compression ratios.\n\\item \\textbf{Multilingual Data:} We incorporated multilingual datasets to ensure that our model could handle a wide range of languages, including German, Spanish, French, Portuguese, Italian, Hindi and Japanese. This involved sourcing and processing high-quality multilingual documents from CommonCrawl and Wikipedia. Our multilingual processing pipeline consists of a language identification model, based on \\texttt{fastText} used to categorize documents into 176 languages, then uses the same classifiers for filtering web dumps to filter for quality. Note that the classifiers were trained on multilingual LLM-generated annotations.\n\n\\item \\textbf{Custom Extraction and Cleaning Pipelines:} To ensure sufficient cleanliness and uniformity between heterogeneous organic data sources, we needed a collection of customized heuristics and parsers. For each targeted data source, we built custom pipelines to ingest a variety of file formats (e.g., multi-file TeX source, ePub and other XML-like formats, Microsoft Word documents, and PDFs). For general web data, we built a custom HTML-to-text extractor, taking significant care to preserve fragile content that is frequently corrupted by na\\\"ive parsers (e.g., TeX/MathML equations, code blocks, tables, and forum thread structure). This extractor prunes and normalizes the DOM tree, using a variety of signals (e.g., HTML tag names, CSS classes, content length, and tree depth) to distinguish elements such as boilerplate, advertisements, equations, and syntax-highlighter artifacts.\n\\end{itemize}\n\n\\subsection{Post-Training datasets} \nOur post-training data is composed of:\n\\begin{itemize}\n    \\item \\textbf{Supervised Fine-Tuning (SFT) Datasets:} Using carefully curated user prompts taken from a mixture of publicly available datasets and synthetically generated data, we generate multiple model responses and select the best using an LLM-based evaluation process. \n    \\item \\textbf{Direct Preference Optimization (DPO):} We generate DPO pairs based on rejection sampling and LLM evaluation, a part of which is based on our approach to creating pivotal token-based pairs, explained in Section~\\ref{sec:pivotal} below.\n\\end{itemize}\n\n\\section{Pretraining details}\\label{sec:pretrain}\nThe \\model model is based on a decoder-only transformer architecture \\cite{Vas17} with $14$B parameters and a default context length of $4096$.  This is later extended to a 16K context length during midtraining. The architecture closely follows \\phithree{}-medium, except that we now use the \\texttt{tiktoken} tokenizer (for better multilingual support) with a padded vocabulary size of {100,352} (including unused tokens) and we use full attention over the $4$K context length, rather than a $2$K sliding window  used in \\phithreemed.\n\nThe model was pretrained for approximately $10$T tokens using linear warm-up and decay schedules with peak learning rate of $0.0003$, constant weight decay of $0.1$, and global batch size of $5760$. The training hyperparameters are tuned using interpolations from shorter horizon runs and further adjusted by stress testing the learning rate warm-up stage for stability. Pretraining is followed by a shorter midtraining stage to increase the original context length of $4$k to $16$k.\n\nSince pre-trained models are not good at instruction following, it is not very informative to use 0-shot evaluations that require the answer to be in a specific format, for example \\textsc{simple-evals}. We therefore use an internal implementation of benchmarks for pretraining which uses a mixture of log-likelihood and/or few-shot prompts for various tasks. Specifically, we used log-likelihood evaluations for MMLU (5-shot), MMLU-pro, and ARCC (1-shot). We used 1, 3, 4, and 8 few-shot examples for TriviaQA (TQA), MBPP, MATH, and GSM8k to help the model adhere to the answer format for easier extraction of the solution. We use this evaluation method throughout Section~\\ref{sec:pretrain}. Table ~\\ref{tab:phi3_versus_phi4} summarizes the performance boost of pretrained \\model compared with its predecessor \\phithree{}-medium.\n\n\\begin{table}[]\n    \\centering\n\\resizebox{0.9\\linewidth}{!}{\n\\begin{tabular}{c c c c c c c c c}\n\\toprule\n & MMLU & MMLU pro & GSM8k & Human-Eval & ARCC & MBPP & MATH & TQA \\\\ \\hline\n\\model (4k) & +3.0 &\t+10.3&\t\t+2.2&\t\t+7.8\t&\t+1.1\t&\t+6.8\t&\t+8.9 &\t\t-0.7\\\\\n\\model (16k) & +2.7\t&+8.9\t&+1.2\t&+9.0\t&+0.9\t&+9.6\t&+8.4\t&-1.5\\\\\n\\bottomrule\n\\end{tabular}}\n    \\caption{Pretraining benchmarks for \\model compared to its predecessor, \\phithree{}-medium after pretraining.}\n    \\label{tab:phi3_versus_phi4}\n\\end{table}\n\n\\subsection{Data Composition in Pretraining}\\label{sec:data_comp}\n\nThe \\phithree{} model family were trained using a two-phase strategy. Most of the training tokens were used in phase 1 of the training, which consisted largely of filtered web data. Phase 2 was trained with a data mixture consisting primarily of synthetic tokens and a much smaller allocation for ultra-filtered and reasoning-heavy web data. As the size and complexity of our synthetic data grew, we observed a marginal drop in the benefit from using non-synthetic tokens for the \\phithree{} family of model sizes. We note two key observations.\n\\begin{itemize}\n    \\item Web datasets showed small benefits on reasoning heavy benchmarks. Prioritizing more epochs over our synthetic data led to better performance with respect to adding fresh web tokens. \n    \\item Models trained only with synthetic data underperformed on the knowledge-heavy benchmarks and demonstrated increased hallucinations. \n\\end{itemize}\n\nFigure~\\ref{fig:synth_epochs} demonstrates the first phenomenon using smaller scale phase 2 pretraining exercises. In this example, we conduct two training runs per model scale, using the same number of training tokens on top of phase 1 pretrained checkpoints. For all runs, the number of unique synthetic tokens is fixed (a subsample of full synthetic data) but the number of repetitions on this data changes, namely 4 and 12 epochs. The rest of the training tokens are fresh unique tokens supplied from web sources. As seen, performing more iterations on the synthetic data is more beneficial than supplying more web tokens.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.6\\linewidth]{figures/mmlu_synth_epochs.png}\n    \\caption{5-shot MMLU score for phase 2 pretraining runs with 4 and 12 epochs of synthetic data. All models are trained for the same token horizon, thus the model with 4 epochs of synthetic has seen more (unique) web tokens. We see that despite many epochs on synthetic data, we do not see overfitting behavior and in fact the 12 epoch models perform better than those that have seen more unique web tokens.}\n    \\label{fig:synth_epochs}\n\\end{figure}\n\nInspired by this scaling behavior of our synthetic data, we trained a $13$B parameter model solely on synthetic\\footnote{This is an updated mixture of synthetic data that contains new sources compared to \\phithree{}.} data, for ablation purposes only -- the model sees over $20$ repetitions of each data source. For the sake of ablations, we partitioned our synthetic data into \\emph{web rewrites}, which includes more direct rewrites of our filtered web content relative to all other types of synthetic data.\nTable~\\ref{tab:synth_versus_phi3} compares the previous \\phithreemed~model with the new model trained entirely on the synthetic data. Throughout training, all benchmarks consistently improved, despite the increase in epochs, and the majority of the benchmarks showed improvements over \\phithree{}. However, knowledge-related benchmarks, like 1-shot triviaqa (TQA), show a large gap where synthetic models are subpar. These observations led us to rethink the role of web data in our data mixture. \n\n\\begin{table}[ht]\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{c c c c c c c c c}\n\\toprule\n & MMLU & MMLU pro & GSM8k & Human-Eval & ARCC & MBPP & MATH & TQA \\\\ \n\\midrule\nSynthetic & +0.8 & +4.0 & +2.2 & +12.1 & 0.0 & +5.0 & +4.9 & -14.8 \\\\\nSynthetic + Web Rewrites & +0.3 & +4.1 & +1.8 & +13.3 & +3.0 & +7.6 & +8.1 & -7.7 \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{Benchmark performance of $13$B models (used for ablations only) trained on data mixtures containing no web data. The respective training tokens are either from synthetic sources, or an equal share of synthetic data and web rewrites. All numbers are reported relative to the performance of \\phithreemed, which has seen a combination of web and synthetic data.}\n\\label{tab:synth_versus_phi3}\n\\end{table}\n\n\\subsection{Data Mixture}\nTo design our pretraining data mixture for a given training token budget, we search over different allocation of tokens coming from various sources, namely, 1) synthetic, 2) web rewrites\\footnote{Web rewrites is a sub-category of synthetic data that is substantially large and contains direct rewrites of web content.}, 3) filtered web (divided into reasoning and knowledge-heavy portions), 4) targeted acquisitions and organic data (e.g., academic data, books, and forums), and 5) code data. \n\nWe conducted ablations using a shorter token horizon of $1$T tokens to derive the data mixture. These ablations rely on our established result on the high-rank correlation of short training with longer training, up to the over-fitting saturation threshold of data sources. In addition we observe a high rank correlation between the performance of the $7$B and $14$B models on different data mixtures, given a large enough distance between the data mixtures. This allowed us to conduct the experiments at $7$B scale and transfer the findings to \\modelwithoutspace. Among the numerous ablations, we highlight a few that show best insights on our data composition. Specifically, we freeze the ratio of tokens coming from targeted acquisitions and code categories, and change the ratio of tokens for the synthetic, web, and web rewrites clusters. \n\n\\begin{table}[ht]\n\\centering\n\\resizebox{0.95\\linewidth}{!}{\n\\begin{tabular}{c c c c c c c c c c}\n\\toprule\n & MMLU & MATH & GSM8k & Human-Eval & ARCC & MBPP & TQA & MMLU pro & Average \\\\\n\\midrule\nUniform & -3.3 & -5.4 & -5.8 & -1.2 & +0.6 & -2.0 & +3.3 & -3.6 & -2.2 \\\\\nS & +3.3 & +4.0 & +2.1 & -6.1 & +1.9 & +0.4 & -3.0 & +3.7 & +0.8 \\\\\nS + WR & +0.6 & +1.2 & +1.5 & -1.2 & +1.6 & +1.6 & -3.7 & +1.2 & +0.4 \\\\\nS + W & -0.6 & -0.7 & -0.7 & -4.3 & +0.3 & -2.0 & +6.9 & +0.9 & 0.0 \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{Ablations on the allocation of $75\\%$ of training tokens to synthetic (S), filtered web (W), and web rewrite (WR) categories, while other data sources are held constant in the remaining $25\\%$ token budget. All benchmark numbers are measured relative to the final data mixture used for training \\modelwithoutspace.}\n\\label{tab:data_mixture_ablation}\n\\end{table}\n\nTable~\\ref{tab:data_mixture_ablation} summarizes the results for the hand-picked ablations, as compared with the data mixture that was used for the final training run. A uniform allocation of tokens among the three categories is suboptimal due to the higher quality of synthetic data and the only benchmark that shows a clear benefit from web data is TQA. While the synthetic-heavy variations on rows 2 and 3 of the table are marginally better than the chosen final data mixture, we decided to integrate the targeted and knowledge-heavy filtered web data sources to improve knowledge benchmarks (see Section~\\ref{sec:data_comp}) to balance all model capabilities. We also note that we observed the gap between the chosen data mixture and the synthetic heavy runs largely closes as the model goes through the post-training stage. An end-to-end optimization of pretraining data mixture that also takes into account the effects of post-training is an interesting future area of investigation.\n\n\\begin{table}[ht]\n\\centering\n\\resizebox{0.55\\linewidth}{!}{\n\\begin{tabular}{c c c c}\n\\toprule\n\\makecell{Data\\\\Source} & \\makecell{Fraction\\\\of Training} & \\makecell{Unique\\\\Token Count} & \\makecell{Number of\\\\Epochs}\\\\\n\\midrule\nWeb & 15\\% & 1.3T & 1.2 \\\\\nWeb rewrites & 15\\% & 290B & 5.2 \\\\\nSynthetic & 40\\% & 290B & 13.8 \\\\\nCode data & 20\\% & 820B & 2.4 \\\\\nAcquired sources & 10\\% & 580B & 1.7 \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{Data mixture for pretraining.}\n\\end{table}\n\nThe final data mixture used for \\model allocates $30\\%$ of the training tokens to web and web rewrites data sources, divided equally between them. The remaining tokens are largely sourced from synthetic data which accounts for $40\\%$ of the data mixture tokens. Finally we allocate $20\\%$ of tokens to code data (mixture of synthetic and raw code) and $10\\%$ to targeted acquired sources like academic data and books. In terms of total number of unique tokens in each data mixture cluster, filtered web data is the largest cluster with $\\sim1.3$T tokens. Code and targeted acquisitions are the second and third largest clusters with $\\sim820$B and $\\sim580$B tokens, respectively. Finally, web rewrites and synthetic data have similar token count of $\\sim290$B tokens. The total number of epochs on each data source is determined using the ratio of allocated tokens in the mixture and the number of unique tokens in that source.\n\n\\subsection{Midtraining Details}\n\n\\model includes a midtraining stage where the context length is increased from the original $4$K to $16$K. We conduct several ablations to study the role of data on long-context performance. Specifically, we try data sources that are inherently long context, and compare them with artificially created long context data where samples are padded together to fill the sequence. We observe the former to perform better in longer context tasks. \n\nInspired by this, we further filter our high-quality non-synthetic datasets (i.e., academic, books, and code data) to separate samples above $8$K context. We then up-weight the data subsets that are $16$K or higher in length. We also create new synthetic datasets that satisfy the $>4$K sequence requirement. The final data mixture includes $30\\%$ of the newly curated longer context data and a $70\\%$ portion of recall tokens from the pretraining stage. To accommodate longer context, we increase the base frequency of rope position encoding to $250$K following~\\cite{llama3report}. We drop the maximum learning rate by a factor of $10$ compared to the pretraining stage and train for a total of $250$B tokens.\n\nTo effectively evaluate the long-context capability of our model, it is essential to have a comprehensive evaluation framework with practical scenarios. While synthetic benchmarks like needle-in-a-haystack and RULER are preferred for their simplicity and control, our emphasis is on a diverse range of tasks that reflect real-world applications, such as reasoning across entire documents. \nWe report the performance of \\model and other models on the tasks we selected from the HELMET~\\cite{yen2024helmetevaluatelongcontextlanguage} evaluation suite in Table~\\ref{tbl:helmet} and outline our evaluation methods below. Note that results are average across 5 runs for each categories.\n\n\\begin{itemize}\n    \\item Recall: The task involves retrieving the corresponding value from a randomly-generated long JSON file given a specific key (Metric: SubEM)\n    \\item RAG: Answer questions based on many retrieved and shuffled Wikipedia documents. The datasets used for this task are NaturalQuestions, HotpotQA, and PopQA. Final results are average of all datasets (Metric: SubEM)\n    \\item Re-rank: The task is to re-rank the top-10 documents given a query and many retrieved and shuffled documents. This uses the MSMARCO dataset (Metric: nDCG@10)\n    \\item ICL: The task involves many-shot in-context learning with datasets such as TREC coarse, TREC fine, Banking77, NLU and CLINC150. Final results are average of all datasets (Metric: F1)\n    \\item QA: Answer questions given a lengthy document. The dataset associated with this task is NarrativeQAv2 (Metric: GPT-4o scoring)\n    \\item Summ: The task involves summarizing a lengthy legal document, and the dataset used is MultiLexSum (Metric: GPT-4o scoring)\n\\end{itemize}\n\n\\begin{table}[t!]\n\\centering\n\\small\n\\begin{tabular}{@{}cccccccc@{}}\n\\toprule\n\\textbf{Model} & \\textbf{Max Length} & \\textbf{Recall} & \\textbf{RAG} & \\textbf{ICL} & \\textbf{Re-rank} & \\textbf{QA} & \\textbf{Summ} \\\\\n\\midrule\nphi-4 & 8K & 100.0 & 58.1 & 68.0 & 65.3 & 26.7 & 38.3 \\\\\nQwen-2.5-14B & 8K & 100.0 & 62.2 & 67.8 & 58.2 & 24.7 & 37.2 \\\\\nLlama-3.3-70B & 8K & 92.0 & 65.3 & 69.4 & 64.4 & 30.0 & 37.8 \\\\\nGPT-4o-mini & 8K & 99.2 & 65.8 & 74.4 & 69.4 & 31.3 & 38.5 \\\\\nGPT-4o & 8K & 100.0 & 66.9 & 83.0 & 75.1 & 37.3 & 43.0 \\\\\n\\midrule\nphi-4 & 16K & 99.0 & 57.1 & 77.0 & 54.4 & 36.0 & 40.5 \\\\\nQwen-2.5-14B & 16K & 100.0 & 59.1 & 67.6 & 50.3 & 29.7 & 42.3 \\\\\nLlama-3.3-70B & 16K & 92.0 & 62.2 & 70.0 & 63.3 & 36.7 & 41.9 \\\\\nGPT-4o-mini & 16K & 100.0 & 63.6 & 78.4 & 63.9 & 36.0 & 45.2 \\\\\nGPT-4o & 16K & 100.0 & 66.7 & 85.6 & 73.8 & 43.7 & 46.3 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Evaluation results on the long-context benchmark HELMET~\\cite{yen2024helmetevaluatelongcontextlanguage}.}\n\\label{tbl:helmet}\n\\end{table}\n\n\\section{Post-Training}\nPost-training is aimed at transforming the pretrained language model into an AI assistant that users can safely interact with. \nWe align the pretrained model with one round of SFT \\ref{sec:sft}, one round of DPO~\\cite{rafailov2023direct} on data from our pivotal token search method (see Section~\\ref{sec:pivotal}), and one round of DPO on full length preference pairs.\nThe model is chat finetuned using the standard \\texttt{chatml} format, example usage template for two rounds of a conversation is as follows:\n\\begin{AIbox}{}\n\\tt \\footnotesize \n<\\!|\\!im\\_start\\!|\\!>system<\\!|\\!im\\_sep\\!|\\!>system\\_message<\\!|\\!im\\_end\\!|\\!>\\\\\n<\\!|\\!im\\_start\\!|\\!>user<\\!|\\!im\\_sep\\!|\\!>prompt1<\\!|\\!im\\_end\\!|\\!><\\!|\\!im\\_start\\!|\\!>assistant<\\!|\\!im\\_sep\\!|\\!>response1<\\!|\\!im\\_end\\!|\\!>\\\\\n<\\!|\\!im\\_start\\!|\\!>user<\\!|\\!im\\_sep\\!|\\!>prompt2<\\!|\\!im\\_end\\!|\\!><\\!|\\!im\\_start\\!|\\!>assistant<\\!|\\!im\\_sep\\!|\\!>\n\\end{AIbox}\n\n\\subsection{Supervised Fine-Tuning} \\label{sec:sft}\nIn this phase, we fine-tune the pretrained model with a learning rate of $10^{-6}$ on a variety of data generated from high-quality data across diverse domains, including math, coding, reasoning, conversation, model identity, and safety.\nWe also added multilingual data for 40 languages.\nWe use around 8B tokens of data in this phase, all formatted in the \\texttt{chatml} format.\n\n\\subsection{Direct Preference Optimization}\nWe use DPO \\cite{rafailov2023direct} to align the model with human preferences, and also to steer the model away from unwanted behavior through pairs of desired and undesired outputs. \nDPO data covers chat format data, reasoning, and Responsible AI (RAI) data and improves the model in math, coding, reasoning, robustness, and safety.\nWe do two rounds of DPO on the SFT model. We introduce a technique, Pivotal Token Search (PTS), to generate pairs for DPO for the first DPO round. Details of the data mixture for first round are provided in Table \\ref{tab:training_data_table_1}.\n\nFor the second round, which we call judge-guided DPO, we gather approximately 850k pairs of desired and undesired outputs. The prompts are sourced from various publicly available instruction tuning datasets and also include prompts related to safety and Responsible AI (RAI).\nNext, for each of these prompts, we generate responses from GPT-4o, GPT-4t and our model. From these responses, we create various combinations of DPO pairs and use GPT-4o as a judge to label positive or negative for a given pair. For a given pair of responses, each assistant response is given a score based on accuracy, style, and detail. \nWe label the response with higher accuracy or overall (average of accuracy, style, and detail) score as the positive response. We provide the prompt we used in Appendix~\\ref{sec:post_train_appendix}. The data mixture for this round is provided in Table \\ref{tab:training_data_table_2}.\nBoth stages also include a small amount of data for safety and mitigating hallucinations.\n\n\\begin{table}[t!]\n\\centering\n\\small\n\\begin{minipage}{0.48\\textwidth}\n\\centering\n\\begin{tabular}{@{}cc@{}}\n\\toprule\n\\textbf{Dataset Name} & \\textbf{Sample Count} \\\\\n\\midrule\nunknown + safety data & 3,000 \\\\\ngeneric multiple-choice Q\\&A & 132,859 \\\\\nmath data & 76,552 \\\\\npython data & 16,080 \\\\\ncpp, go, java, js, rust data & 21,806 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Data Mixture for Pivotal Token DPO}\n\\label{tab:training_data_table_1}\n\\end{minipage}\n\\hfill\n\\begin{minipage}{0.48\\textwidth}\n\\centering\n\\begin{tabular}{@{}cc@{}}\n\\toprule\n\\textbf{Dataset Name} & \\textbf{Sample Count} \\\\\n\\midrule\nunknown + safety data & 43,842 \\\\\nany vs any overall & 266,000 \\\\\nany vs any accuracy & 532,000 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Data Mixture for Judge Guided DPO}\n\\label{tab:training_data_table_2}\n\\end{minipage}\n\\end{table}\n\n\\begin{figure}\n\\begin{AIbox}{}\n    \\tt \\footnotesize\n    \\begin{minipage}{0.95\\textwidth}\n    \\include{pivotal_token_text}\n    \\vspace{-1em}\n    \\end{minipage}\n    \\include{pivotal_token_plot}\n    \\vspace{-2.5em}\n    \\end{AIbox}\n    \\caption{Illustration of pivotal tokens for GPT-4o at temperature $1$ on a problem from the MATH benchmark~\\cite{hendrycksmath2021}, where the\n initial success probability is $0.31$. Each token is colorized by the probability of success for an independent completion ($N=529$) continued from after the token, with \\textcolor{red!100!black}{red for $p(\\mathrm{success})=0$} and \\textcolor{blue!100!black}{blue for $p(\\mathrm{success})=1$}. The line plot shows the same probabilities. The tokens that changes $p(\\mathrm{success})$ by $\\ge 0.2$ are shown \\tokenhilightbox{boxed}, with subscripts showing the change in probability. Tokens with probability $\\leq 0.1$ are \\underline{underlined} to illustrate that pivotal tokens are distinct from low-probability tokens. The token probabilities of \\tokenhilightbox{~negative} and \\tokenhilightbox{(a} were 0.31 and 0.12, respectively. The greedy tokens for the same prefixes are \\tokenhilightbox{~product} with 0.66 probability and \\tokenhilightbox{t} with 0.88 probability.}\n    \\label{fig:pivotal-token-example}\n\\end{figure}\n\\algnewcommand{\\Yield}{\\textbf{yield}}\n\\begin{figure}\n    \\begin{algorithmic}\n        \\Procedure{PivotalTokenSearch}{$Q,T_\\mathrm{full},p_\\mathrm{gap}$}\n            \\Procedure{Subdivide}{$T_\\mathrm{prefix},T$}\n                \\If{$\\left| T \\right| \\leq 1$ or $\\left|\\, p(\\mathrm{success} \\mid T_\\mathrm{prefix}) - p(\\mathrm{success} \\mid T_\\mathrm{prefix} + T) \\,\\right| < p_\\mathrm{gap}$} \\Comment{Base cases.}\n                    \\State \\Return $[T]$\n                \\EndIf\n                \\State $T_\\mathrm{left},T_\\mathrm{right} \\gets \\Call{Split}{T}$ \\Comment{We split at the cumulative midpoint of token log probabilities.}\n                \\State \\Return $\\Call{Subdivide}{T_\\mathrm{prefix}, T_\\mathrm{left}} \\cup \\Call{Subdivide}{T_\\mathrm{prefix} + T_\\mathrm{left}, T_\\mathrm{right}}$\n            \\EndProcedure\n            \\State $T_\\mathrm{prefix} \\gets \\epsilon$\n            \\ForAll{$T \\in \\Call{Subdivide}{\\epsilon, T_\\mathrm{full}}$}\n                \\If{$\\left| T \\right| = 1$ and $\\left|\\, p(\\mathrm{success} \\mid T_\\mathrm{prefix}) - p(\\mathrm{success} \\mid T_\\mathrm{prefix} + T) \\,\\right| \\ge p_\\mathrm{gap}$}\n                    \\State \\Yield~$(Q,T_\\mathrm{prefix},T)$ \\Comment{Output pivotal tokens $T$ and context for postprocessing.}\n                \\EndIf\n                \\State $T_\\mathrm{prefix} \\gets T_\\mathrm{prefix} + T$\n            \\EndFor\n        \\EndProcedure\n    \\end{algorithmic}\n    \\caption{Pseudocode for Pivotal Token Search (PTS). Note that estimating $p(\\mathrm{success} \\mid \\dots)$ involves sampling the language model and invoking the oracle. In an efficient implementation $p(\\mathrm{success} \\mid \\dots)$ should be memoized.}\n    \\label{fig:pts-pseudocode}\n\\end{figure}\n\n\\subsection{Pivotal Token Search} \\label{sec:pivotal}\n\nConsider a generative model producing a token-by-token response to a given prompt. For each token produced, which corresponds to a prefix of the model response, one can consider the conditional probability of the model's answer being correct given that prefix, as well as the increment in this probability with respect to that token (in other words, the difference in the probability of being correct before and after producing that token). It is often the case that the overall correctness is highly dependent on a successful generation of a small number of key tokens. For example, we can see in Figure~\\ref{fig:pivotal-token-example} where the model outputs a math solution and a ``fortunate'' sampling of a crucial token \\tokenhilightbox{~negative} shifts the solution from possible failure to likely success, while sampling of the token \\tokenhilightbox{(a} subsequently risks failure again. We refer to these tokens as \\emph{pivotal tokens} as they have an outsized effect on the course of the solution.\n\nNow, consider how the solution from Figure~\\ref{fig:pivotal-token-example} would be used in DPO as a full-length accepted response. As the figure shows, there are many tokens with probabilities much lower than the $0.31$ of \\tokenhilightbox{~negative}, which would contribute to noise in the gradients diluting the signal from the pivotal token. Even worse, the token \\tokenhilightbox{(a} that contributed to the lack of robustness would receive a strong \\emph{positive} learning signal thanks to its low probability of $0.12$. \n\nMoreover, intuition suggests that when two texts substantially deviate from each other, comparison of their individual next-token log probabilities (as done in DPO) is not very meaningful. Rather, it makes more sense that the signal should come from the first tokens after the two texts starts diverging from each other. \n\nTo alleviate these effects, we employ a method we call \\emph{Pivotal Token Search (PTS)} for generating preference data that specifically targets pivotal tokens in isolation, creating DPO pairs in which the preference optimization takes effect with respect to a single token.\n\n\\begin{figure}\n\\begin{AIbox}{}\n\\tt \\footnotesize\n\\begin{minipage}{0.99\\textwidth}\n\\textbf{User:} Find the product of all real values of $r$ for which $\\frac{1}{2x}=\\frac{r-x}{7}$ has exactly one real solution.\n\n\\textbf{AI:} To solve the problem, we need to determine the values of \\( r \\) such that the equation \n\n\\[\n\\frac{1}{2x} = \\frac{r-x}{7}\n\\]\n\nhas exactly one real solution for \\( x \\).\n\nFirst, clear the fractions by\n\n\\textbf{Good:} \\underline{cross}-multiplying \\hspace{2em} \\textbf{Bad:} \\underline{multiplying} both sides by\n\\end{minipage}\n\\end{AIbox}\n\n\\begin{AIbox}{}\n\\tt \\footnotesize\n\\begin{minipage}{0.99\\textwidth}\n\\textbf{User:} Consider a 1 keV helium atom impinging at a one-degree incidence angle on a LiF crystal surface. Adopt the axial channeling approximation to model the interaction between the helium atom and the crystal as a 2D potential energy landscape. Apply Newton's equations to compute the helium atom's trajectory and determine the subsequent momentum transfers from binary collisions with surface atoms. Sum the total virtual energy transferred across all collisions and compute the probability, $P_e$, that the helium atom remains elastically scattered assuming a vibrational frequency for surface atoms of $\\omega = 1 \\times 10^{14} Hz$. What is $P_e$?\n\n1. $P_e = \\exp(-\\frac{E_{loss}}{2\\hbar \\omega})$, 2. $P_e = \\exp(\\frac{E_{loss}}{\\hbar \\omega})$, 3. $P_e = \\exp(-\\frac{E_{loss}}{\\hbar \\omega})$, or 4. $P_e = 1 - \\exp(-\\frac{E_{loss}}{\\hbar \\omega})$\n\n\\textbf{AI:} To determine the probability \\( P_e \\) that the helium atom remains elastically scattered, we need to calculate the total virtual energy transferred, \\( E_{loss} \\), during the helium atom's interaction with the LiF crystal surface.\n\nGiven\n\n\\textbf{Good:} \\underline{the} vibrational frequency \\hspace{2em} \\textbf{Bad:} \\underline{that} the potential interactions\n\\end{minipage}\n\\end{AIbox}\n\\begin{AIbox}{}\n\\tt \\footnotesize\n\\begin{minipage}{0.99\\textwidth}\n\\textbf{User:} Read the following function signature and docstring, and fully implement the function described. Your response should only contain the code for this function.\n\n\\begin{lstlisting}[breaklines=true]\ndef reorder_sequence(n: int, sequence: list[int]) -> list[int]:\n    \"\"\"\n    Given a sequence of integer numbers that Polycarp wrote down on the board, this\n    function rearranges (reorders) elements of this sequence to match the order\n    Polycarp could have possibly written them. Each next number should be exactly two\n    times the previous number or exactly one third of the previous number.\n\n    :param n: The number of elements in the sequence (2 <= n <= 100).\n    :param sequence: A list of integer numbers in arbitrary order (1 <= elements <= 3 * 10^18).\n    :return: A reordered list of integer numbers representing a valid sequence Polycarp could have written.\n    \"\"\"\n(*@\\textbf{AI:} \\dots@*)\n    from collections import defaultdict\n\n    # Create adjacency list where each number can point to its possible predecessor and successors\n    graph = defaultdict(list)\n    for num in sequence:\n        for other in sequence:\n            if num != other:\n                if num == other * 2 or (\n\\end{lstlisting}\\vspace{-0.5em}\n\\textbf{Good:} \\underline{other} \\% 3 \\hspace{2em} \\textbf{Bad:} \\underline{num} * 2\n\\end{minipage}\n\\end{AIbox}\n    \\caption{Preference data generated by Pivotal Token Search in answering math and physics questions, and implementing a function in Python. The tokens that form the actual pair for DPO are underlined.}\n    \\label{fig:pts-dpo-examples}\n\\end{figure}\n\nPTS identifies points of a completion token sequence $T_\\mathrm{full}=t_1,t_2,\\dotsc$ for some user query $Q$ where the next token $t_i$ has a significant impact on the probability of success $p(\\mathrm{success} \\mid t_1,\\dotsc,t_i)$.\nPTS estimates these probabilities by sampling completions starting from $Q+t_1,\\dotsc,t_i$, which are checked for correctness with an oracle\\footnote{For coding, a comprehensive test suite can be used. For math problems, answers can be compared to the ground truth.} for $Q$. Figure~\\ref{fig:pts-pseudocode} shows a basic instantiation of the algorithm. The procedure \\textsc{Subdivide} recursively splits the sequence into segments $t_i,\\dotsc,t_j$ until the change in probability $\\left|\\,p(\\mathrm{success} \\mid t_1,\\dotsc,t_{i-1}) - p(\\mathrm{success} \\mid t_1,\\dotsc,t_j)\\,\\right|$ for each segment is below a threshold $p_{\\mathrm{gap}}$ or the segment is just a single token. Tokens with a sharp change in success probability are kept as pivotal.\nWe turn pivotal tokens into preference data by taking $Q + t_1,\\dotsc,t_{i-1}$ as the \\emph{query}, and single tokens $t_{\\mathrm{acc}}$ and $t_{\\mathrm{rej}}$ that increase/decrease $p(\\mathrm{success} \\mid t_1,\\dotsc,t_{i-1}, t_{\\mathrm{acc/rej}})$ as the \\emph{accepted} and \\emph{rejected} completions, respectively.\\footnote{We find drawing $t_{\\mathrm{acc}}$ and $t_{\\mathrm{rej}}$ from rollouts PTS already used to estimate $p(\\mathrm{success} \\mid t_1,\\dotsc,t_{i-1})$ to be effective.}  The binary-search algorithm for PTS is not always guaranteed to find all pivotal tokens, but it only finds pivotal tokens and it finds all of them if the success probability is near-monotone over the course of the solution.\n\nWe used PTS to generate preference data for tasks where ground-truth is readily available, such as mathematics, various forms of question answering and coding.\nTo improve sample efficiency, we filter the target questions to only include those with $0.2 \\leq p(\\mathrm{success}) \\leq 0.8$, as pivotal tokens are rare for tasks that are very easy or hard.\n\nSee Figure~\\ref{fig:pts-dpo-examples} for examples of preference data we generated using PTS. The math question answering example shows how pivotal tokens often are not actual mistakes, but choices that drive the model down a less advantageous path. Here, multiplying both sides by the denominators separately is equally valid to directly cross-multiplying, but for the model doing the latter here is more robust. By generating DPO data targeting such choices, we believe PTS helps \\model work better in the modes it is especially stronger.\n\n\\paragraph{Related Work:} In~\\cite{lin2024criticaltokensmattertokenlevel} a contrastive estimation approach involving a model trained on \\emph{incorrect} trajectories is used to score which tokens likely contributed to failure, which is further employed to weigh rejected responses in DPO. In comparison, our PTS avoids complications from learned proxies by directly estimating $p(success)$. They also report difficulties applying their method to accepted responses in DPO, while our method generates both positive and negative preference data directly targeting pivotal tokens. \\emph{Automated process supervision} methods~\\cite{wang2024mathshepherdverifyreinforcellms,luo2024improvemathematicalreasoninglanguage} have applied search and rollouts to generate data for training process reward models. PTS can be seen as an automated process supervision method that generates \\emph{token-level} preference data suitable for DPO.\n\n\\subsection{Hallucination mitigation}\n\n\\begin{figure}[t]\n    \\centering \\includegraphics[width=0.5\\textwidth]{simpleqa.pdf}\n    \\caption{The post-training process described in Appendix~\\ref{sec:refusaltohallucinate} decreases hallucinations.  One measure is that the problems in SimpleQA---which the model very rarely gets correct---are increasingly not attempted during the course of post-training.  We believe the final result is better behavior, even though the \\textsc{simple-evals} score for SimpleQA (the F1 score) actually gives our base model a higher score than our final model. }\\label{fig:simpleQA}\n\\end{figure}\n\nWe generate SFT data and DPO pairs to mitigate hallucination.  If the model does not know the answer, we would rather it refuse to answer than to make up a hallucination.  We present the details of this process, including prompts to create the data, in Appendix~\\ref{sec:refusaltohallucinate}.  This greatly decreases hallucinations in SimpleQA (see Figure~\\ref{fig:simpleQA}).\n\n\\subsection{Post-Training Ablation}\n\n\\begin{table}[t!]\n\\centering\n\\small\n\\begin{tabular}{@{}cccccc@{}}\n\\toprule\n &  & \\textbf{SFT} & \\makecell{\\textbf{DPO}\\\\\\textbf{stage 1}} & \\makecell{\\textbf{DPO}\\\\\\textbf{stage 2 only}} & \\makecell{\\textbf{phi-4}\\\\\\textbf{(stage 1 + 2)}}\\\\\n\\midrule\n\\multirow{7}{*}{\\rotatebox[origin=c]{90}{\\textbf{simple-evals}}} \n & MMLU & 82.8 & \\textbf{84.8} & 84.2 & \\textbf{84.8}\\\\\n & GPQA & 47.3 & 53.6 & 52.4 & \\textbf{56.1}\\\\\n & MATH & 77.1 & \\textbf{80.5} & 77.6 & 80.4\\\\\n & HumanEval & 79.5 & 81.6 & 81.5 & \\textbf{82.6}\\\\\n & MGSM & 80.8 & 80.8 & \\textbf{81.5} & 80.6\\\\\n & SimpleQA & 3.7 & 2.9 & 2.9 & 3.0\\\\\n & DROP & 82.8 & \\textbf{86.1} & 71.8 & 75.5\\\\\n\\midrule\n & MMLUPro & 61.9 & 70.0 & 67.2 & \\textbf{70.4} \\\\\n & HumanEval+ & 77.9 & 81.9 & 81.4 & \\textbf{82.8}\\\\\n & ArenaHard & 56.7 & 66.5 & 69.8 & \\textbf{75.4}\\\\\n & IFEval & \\textbf{66.2} & 63.0 & 63.0 & 63.0\\\\\n\\midrule\n & \\makecell{{PhiBench}\\\\(internal)} & 48.2 & 54.5 & 53.0 & \\textbf{56.2}\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Performance through the post-training process. DPO stage 1 is pivotal token DPO, and DPO stage 2 is more standard judge-guided DPO. Each also has 1-5\\% hallucination and safety data mixed in.}\n\\label{tbl:benchmarks_sft}\n\\end{table}\n\nIn Table~\\ref{tbl:benchmarks} we show how our benchmark scores evolve during post-training.  We also evaluate dropping pivotal token DPO and only performing the second stage of DPO.  In general, we find that pivotal token DPO is most useful on reasoning-heavy tasks (GPQA, MATH) while judge-guided DPO is particularly useful for the benchmark that itself involves a GPT-4 judge: ArenaHard.  We also find the two approaches to be complementary.  \n\n\\section{Benchmarking Considerations} \\label{sec:phibench}\nWhile academic benchmarks are a widely used to measure the progress in LLM advancement, they suffer from several limitations that can fail to reveal a model's true capabilities and weaknesses. These limitations include:\n\n\\begin{itemize}\n    \\item \\textbf{Data Contamination:} Many benchmarks rely on datasets that overlap with pretraining corpora, creating a risk of data contamination. Although we took extensive measures to deduplicate and decontaminate our training data, including standard $n$-gram deduplication and decontamination, these methods are not effective against all scenarios, including rephrasing, which leaves some uncertainty about the true extent of generalization.\n    \\item \\textbf{Limited Skill Scope:} Most benchmarks evaluate models on narrowly defined skills, such as solving specific style of math problems at certain grade level or implementing isolated Python functions. This narrow scope can fail to capture a model’s broader capabilities and weaknesses.\n    \\item \\textbf{Bias in Generation-Based Benchmarks:} Some benchmarks use LLM-as-judge for evaluating generated outputs. These judgments sometimes may prioritize style, fluency, or surface-level qualities over accuracy and validity of the reasoning chain, leading to potential biases in scoring.\n    \\item \\textbf{Limitations of Multiple-Choice Tasks:} Benchmarks that rely on multiple-choice questions often test a model’s ability to make clever guesses that can be achieved by pattern matching rather than effectively utilizing the underlying concepts through reasoning.\n\\end{itemize}\n\nTo address these issues, we maintain an internal benchmark called {PhiBench}, which is tailored to evaluate the diverse skills and reasoning abilities that we found critical to \\modelwithoutspace’s development. This benchmark was designed with the following goals:\n\n\\begin{enumerate}\n    \\item \\textbf{Originality:} All questions in the benchmark were composed by our team making sure that they were not present in our pretraining data. Our goal for the internal benchmark is to reveal model's generalization ability in various domains.\n    \\item \\textbf{Skill Diversity:} Our benchmark includes a wide range of tasks to assess multiple dimensions of model performance. For instance, in coding, it goes beyond isolated function implementation to include debugging, extending incomplete code, and explaining code snippets. Similarly, in mathematics, it incorporates tasks like identifying the errors in proofs or generating related problems, rather than simply solving equations. This ensures that the benchmark captures a broader spectrum of skills and reasoning processes.\n     \\item \\textbf{Rigorous Scoring for Generation Tasks:} For tasks requiring judgment of model-generated outputs, we addressed the common pitfalls of LLM-based scoring by carefully curating detailed judge instructions (or “judge notes”). These rubrics specify exactly how to evaluate responses, focusing on achieving accuracy, logical structure, and adherence to task requirements, while minimizing tendencies towards stylistic biases. We observed significantly improved consistency and reduction of adverse impact due to subjective preferences in the scoring outcomes.\n\\end{enumerate}\n\nPhiBench played a central role in optimizing \\modelwithoutspace. We used it to guide  decisions about dataset mixtures and hyperparameter choices for more effective post-training techniques. PhiBench was also used to perform high-signal studies that identify weaknesses in the model and provide feedback for new incoming data sources. \n\n\\section{Performance on Key Benchmarks}\n\nOur benchmark results were presented in Table~\\ref{tbl:benchmarks}, along with comparisons to other models.   We first report the values from OpenAI's \\textsc{simple-evals} benchmark, which is a framework (including prompts, temperature, and extraction) for evaluating \nMMLU~\\cite{hendrycks2020}, \nGPQA diamond~\\cite{rein2023gpqa},\nMATH~\\cite{hendrycksmath2021},\nHumanEval~\\cite{humaneval},\nMGSM~\\cite{mgsm},\nand the SimpleQA~\\cite{simpleqa} F1-score.  We also consider MMLU-pro~\\cite{mmlupro}, HumanEval+~\\cite{evalplus}, ArenaHard~\\cite{chiang2024chatbot}, and IFEval~\\cite{ifeval}, for which we use an internal framework and prompting and extraction.  Finally, we use PhiBench, our internal collection of evaluations (see Section~\\ref{sec:phibench}).\n\n\\model outperforms the closest in-class contemporary model, Qwen-2.5-14B-Instruct, in 9 out of 12 benchmarks.  While \\model \nunderperforms relative to Qwen-2.5-14B-Instruct on the benchmark numbers for SimpleQA, DROP, and IFEval, we consider \\modelwithoutspace's behavior on SimpleQA to actually be better than Qwen's.  In fact, our \\emph{base} model gets a higher benchmark score than Qwen-2.5-14B-Instruct on SimpleQA, and we intentionally modified the model's behavior in post-training to optimize for a better user experience rather than a higher benchmark score.  See Figure~\\ref{fig:simpleQA} and Appendix~\\ref{sec:refusaltohallucinate} for details.\n\nOur model excels at STEM Q\\&A tasks.  For example, on GPQA (graduate-level STEM questions) and MATH (math competitions), it even outscores its teacher model, GPT-4o.  It also scores higher at coding, as measured by HumanEval and HumanEval+, than any other open-weight model we benchmark against, including much larger Llama models.\n\n\\modelwithoutspace's weakest benchmark scores are on SimpleQA, DROP, and IFEval.\nWe believe for the first two that the number reported by \\textsc{simple-evals} is reductive and does not accurately reflect model performance on the benchmark problems.\nHowever, IFEval reveals a real weakness of our model -- it has trouble strictly following instructions. While strict instruction following was not an emphasis of our synthetic data generations for this model, we are confident that \\modelwithoutspace's instruction-following performance could be significantly improved with targeted synthetic data.\n\n\\section{Safety}\n\nWe developed \\model in accordance with Microsoft’s Responsible AI principles. Our overall approach to RAI consisted of safety alignment in post-training, red-teaming, and automated testing and evaluations across dozens of RAI harm categories. We leveraged helpfulness and harmlessness preference datasets \\cite{bai2022training, ji2023beavertails} with modifications inspired by \\cite{bianchi2024safetytuned} and multiple in-house generated datasets to address the RAI harm categories in safety post-training. \n\n\\subsection{RAI Benchmarks}\n\nTable \\ref{tab:performance_comparison} shows the results of in-house RAI benchmarks \\cite{magooda2023framework} for \\model compared to the {\\phithree{}} models \\cite{abdin2024phi},  Mistral-7b-v0.1 \\cite{jiang2023mistral},  Mistral-7b-v0.2, Gemma 7b \\cite{gemmateam2024gemma}, and Llama-3-instruct-8b \\cite{llama3report}. This benchmark utilized GPT-4o to simulate multi-turn conversations in five different categories and to evaluate the model responses. Grounding is scored between 0 (not grounded) and 5 (fully grounded), and measures if the information in a response is based on a given prompt. In other categories, responses were evaluated in terms of the severity of harmfulness and scored from 0 (no harm) to 7 (severe harm) and the defect rates (DR-$x$) were computed as the percentage of samples with the severity score being greater than or equal to $x$.  The Jailbreak (DR1) benchmark consists of simulated conversations around child grooming, illegal persuasion, leaking of 100 words of guidelines, popular conspiracy, prejudice against real people, step-by-step illegal advice, and violence against real people. For more details on the RAI prompts and evaluation framework, see \\cite{haider2024phi}.\n\n\\begin{table}[t!]\n\\centering\n\\small\n\\begin{tabular}{@{}c cccccccc@{}}\n\\toprule\n & \\makecell{\\textbf{phi-3}\\\\(3B-4K)} \n & \\makecell{\\textbf{phi-3}\\\\(7B-8K)} \n & \\makecell{\\textbf{phi-3}\\\\(14B-4K)} \n & \\makecell{\\textbf{Mistral}\\\\(7B-v0.1)} \n & \\makecell{\\textbf{Mistral}\\\\(7B-v0.2)} \n & \\makecell{\\textbf{Llama-3}\\\\(8B)} \n & \\makecell{\\textbf{Gemma}\\\\(7B)} \n & \\textbf{\\model} \\\\ \n\\midrule\n\\textbf{Grounding} & 4.469 & 4.701 & 4.787 & 4.065 & 4.692 & 4.672 & 4.32 & \\textbf{4.619} \\\\\n\\midrule\n\\multirow{2}{*}{\\makecell[l]{\\textbf{3P Content Harms}\\\\(DR1)}} \n & \\multicolumn{8}{l}{\\textit{Books, News, Recipes, Songs}} \\\\\n & 0.26 & 0.253 & 0.251 & 0.562 & 0.399 & 0.373 & 0.383 & \\textbf{0.121} \\\\ \n\\midrule\n\\multirow{2}{*}{\\makecell[l]{\\textbf{Harmful Content}\\\\Continuation (DR3)}} \n & \\multicolumn{8}{l}{\\textit{Hate/Fairness, Self-Harm, Sexual, Violence}} \\\\\n & 0.007 & 0.003 & 0.01 & 0.026 & 0.018 & 0.013 & 0.013 & \\textbf{0.036} \\\\ \n\\midrule\n\\multirow{2}{*}{\\makecell[l]{\\textbf{Harmful Content}\\\\Summarization (DR3)}} \n & \\multicolumn{8}{l}{\\textit{Hate/Fairness, Self-Harm, Sexual, Violence}} \\\\\n & 0.105 & 0.11 & 0.112 & 0.223 & 0.16 & 0.082 & 0.103 & \\textbf{0.102} \\\\ \n\\midrule\n\\multirow{2}{*}{\\makecell[l]{\\textbf{Jailbreak}(DR1)}} \n & \\multicolumn{8}{l}{\\textit{See text for covered topics}} \\\\\n & 0.117 & 0.107 & 0.111 & 0.156 & 0.153 & 0.13 & 0.114 & \\textbf{0.073} \\\\ \n\\bottomrule\n\\end{tabular}\n\\caption{Performance comparison across models. Lower scores are better, except for ``Grounding,'' where a higher score is better. \\model{} values are bold for readability.}\n\\label{tab:performance_comparison}\n\\end{table}\n\n\\subsection{Red Teaming}\n\nIn addition to RAI benchmarking, we collaborated with the Microsoft AI Red Team (AIRT), an independent group tasked with identifying safety and security vulnerabilities in Microsoft's GenAI products. AIRT conducted a two-week red-teaming exercise that tested \\model for risky behaviors by emulating both average and adversarial users in single and multi-turn scenarios. Overall, AIRT found that the behavior of \\model was similar to that of the \\phithree{} family, but identified several risky behaviors that were addressed by further rounds of safety post-training. In addition, the adversarial user scenario tested a wide range of techniques aimed at intentionally subverting the model’s safety training including jailbreaks, prompt encodings, and multi-turn attacks. \\model showed strong defenses against these techniques. AIRT also generated adversarial suffixes using the GCG algorithm \\cite{zou2023universaltransferableadversarialattacks} on \\phithreemed, but found that these suffixes did not transfer to \\modelwithoutspace. Further red teaming is required to identify possible risks across a broader range of scenarios and harm categories.\n\n\\section{Weaknesses}\nWhile \\model achieves similar level of language understanding and reasoning ability as much larger models, it is still fundamentally limited by its size for certain tasks, specifically in hallucinations around factual knowledge. For example, if X is a plausible human name, the model sometimes responds to prompts of the form ``Who is X?\" with a hallucinated biography of the person X.   This limitation would be improved by augmenting the model with a search engine, but factual hallucinations cannot be eliminated completely.\n\nWhile \\model demonstrates relatively strong performance in answering questions and performing reasoning tasks, it is less proficient at rigorously following detailed instructions, particularly those involving specific formatting requirements. For instance, when tasked with generating outputs in strict tabular formats, adhering to predefined bullet structures, or precisely matching stylistic constraints, the model may produce outputs that deviate from the specified guidelines. This limitation arises in part from the model’s training focus, which prioritized synthetic datasets tailored toward Q\\&A and reasoning tasks over instruction-following scenarios.\n\nEven on reasoning tasks, \\model can make mistakes.  For example, when asked ``which number is smaller, 9.9 or 9.11?\", the model can conclude incorrectly that ``9.9 is smaller than 9.11\". \n\nMoreover, as our data contains a lot of chain-of-thought examples, \\model sometimes gives long elaborate answers even for simple problems---this might make user interactions tedious. We also note that while \\model can function as a chat bot, it has been fine-tuned to maximize performance on single-turn queries. \n\nDespite diligent RAI efforts, we acknowledge  challenges around  reproduction or amplification of biases, inappropriate content generation, and\nsafety issues. The use of carefully curated training data, as well as targeted post-training, and improvements\nfrom red-teaming insights, have resulted in  mitigating these issues across all dimensions, but have not resolved the issues completely. \n\n\\section*{Acknowledgments}\nWe thank Janardhan Kulkarni and Sivakanth Gopi from Microsoft Research for the initial discussion around Pivotal Token Search. We thank the AI Red Team (AIRT) at Microsoft, especially Blake Bullwinkel, Bolor-Erdene Jagdagdorj, Daniel Jones, Shiven Chawla, Tori Westerhoff, and Ram Shankar Siva Kumar, and Olga Dutova-Fairfax from the Deployment Safety Board and the Office of Responsible AI at Microsoft for collaborating with us on evaluating and improving our model on vulnerabilities in safety and security, which helped us adhere to the Microsoft's RAI standards. Finally, we are grateful to Ece Kamar, Doug Burger and Peter Lee from Microsoft Research for the support provided to the team during the work on the model.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2309.05463v1.tex",
        "arXiv-2404.14219v4.tex",
        "arXiv-2412.08905v1.tex"
    ],
    "group_id": "group_11",
    "response": "### Title: Advances in Small Language Models: The Phi Series and Their Impact on Large Language Model Capabilities\n\n### Introduction\nThe field of Large Language Models (LLMs) has seen remarkable advancements over the past few years, transforming the landscape of Natural Language Processing (NLP) and human-computer interaction. Initially, the focus was on scaling models to increase their parameters and training data size, which was believed to be the primary driver of improved performance. However, recent research has challenged this notion by demonstrating that smaller models, when trained on high-quality, curated datasets, can achieve capabilities comparable to much larger models. This shift in focus has significant implications for the development of more efficient, environmentally sustainable, and accessible AI systems.\n\nThe Phi series of models, developed by Microsoft Research, represents a pivotal shift in this paradigm. These models, including phi-1.5, phi-3-mini, and phi-4, are designed to achieve high performance on a variety of tasks, such as common sense reasoning, language understanding, mathematics, and coding, while maintaining a smaller size than their larger counterparts. The series explores the use of synthetic data, filtered web data, and innovative post-training techniques to enhance model performance and mitigate issues like toxicity and bias. This summary will delve into the technical details and benchmark results of these models, highlighting their commonalities and innovations.\n\n### Main Content of Each Paper\n\n#### Paper 1: \"Textbooks Are All You Need II: phi-1.5 Technical Report\"\nThe first paper introduces \\textbf{phi-1.5}, a 1.3 billion parameter model trained on a dataset of 30 billion tokens, which includes a combination of synthetic, \"textbook-like\" data and filtered code data. The synthetic data is generated to teach common sense reasoning and general knowledge, while the filtered code data ensures the model's coding capabilities. The authors emphasize the importance of data quality over scale, suggesting that smaller models can achieve similar or even better performance on reasoning tasks when trained on high-quality data. \n\nThe technical specifications of \\textbf{phi-1.5} include a Transformer architecture with 24 layers, 32 heads, and rotary embedding with a dimension of 32. The model uses flash-attention for training speedup and the tokenizer from codegen-mono. The training data consists of 7 billion tokens from \\textbf{phi-1}'s training data and 20 billion tokens of newly created synthetic data. The model is trained with a constant learning rate of \\(2e-4\\) and uses Adam optimizer with momentum and epsilon values specified. \n\nBenchmark results show that \\textbf{phi-1.5} performs comparably to larger models on common sense reasoning and language understanding tasks, and surpasses them on multi-step reasoning tasks. The authors also discuss the performance of a filtered web data enhanced version of \\textbf{phi-1.5}, named \\textbf{phi-1.5-web}, which achieves even better performance on reasoning tasks. \n\n#### Paper 2: \"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone\"\nThe second paper presents \\textbf{phi-3-mini}, a 3.8 billion parameter model trained on 3.3 trillion tokens, which achieves high performance on academic benchmarks despite being small enough to fit on a modern phone. The training data consists of heavily filtered publicly available web data and synthetic data, with a focus on improving reasoning and niche skills. \n\nThe \\textbf{phi-3-mini} model uses a tokenizer with a vocabulary size of 32064 and a context length of 4096. It is further aligned for robustness, safety, and chat format. The authors also introduce \\textbf{phi-3-small} (7 billion parameters) and \\textbf{phi-3-medium} (14 billion parameters), both of which are significantly more capable than \\textbf{phi-3-mini}. Additionally, they introduce \\textbf{phi-3.5-MoE}, a 16 x 3.8 billion parameter model with 6.6 billion active parameters, which achieves superior performance in reasoning tasks compared to other open-source models of similar scale.\n\nBenchmark results show that \\textbf{phi-3-mini} rivals the performance of models such as Mixtral 8x7B and GPT-3.5 on tasks like MMLU and MT-bench. The model is also evaluated on multilingual and long-context benchmarks, where it outperforms other models in a variety of tasks. \n\n#### Paper 3: \"Phi-4 Technical Report\"\nThe third paper introduces \\textbf{phi-4}, a 14 billion parameter model trained on a dataset of 400 billion tokens, which includes synthetic data and filtered web data. The synthetic data is generated to prioritize reasoning and problem-solving abilities, while the filtered web data ensures the model has a broad base of general knowledge. \n\nThe \\textbf{phi-4} model is aligned for robustness, safety, and chat format through post-training stages, including Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). The authors use a novel technique called Pivotal Token Search (PTS) to generate preference data for DPO, targeting pivotal tokens that have a significant impact on the model's success probability. \n\nBenchmark results show that \\textbf{phi-4} surpasses its teacher model, GPT-4o, on STEM-focused QA tasks like GPQA and MATH. The model also demonstrates strong performance on other reasoning tasks, such as HumanEval and MGSM, and shows improvements in safety and robustness metrics compared to previous models.\n\n### Commonalities and Innovations\nThe three papers share a common theme of using high-quality, curated datasets to train smaller models and achieve performance comparable to much larger models. The datasets used in the Phi series include synthetic data generated to teach reasoning and problem-solving skills, filtered web data to provide a broad base of general knowledge, and organic data from sources like books and code repositories. \n\nInnovations in the Phi series include:\n- **Synthetic Data Generation**: The use of synthetic data to teach reasoning and problem-solving skills, which is more effective than traditional web data for smaller models.\n- **Pivotal Token Search (PTS)**: A novel technique for generating preference data for DPO, targeting pivotal tokens that have a significant impact on the model's success probability.\n- **Multilingual and Long-Context Capabilities**: The introduction of models like \\textbf{phi-3.5-MoE} and \\textbf{phi-3.5-Vision} to enhance multilingual and long-context understanding.\n- **Safety Post-Training**: The use of safety alignment techniques in post-training to mitigate issues like toxicity, bias, and inappropriate content generation.\n\n### Comparison of Results\nThe benchmark results across the three papers show that the Phi series models perform comparably to or surpass larger models on a variety of tasks. For example, \\textbf{phi-1.5} achieves comparable results to Llama2-7B and Falcon-7B on common sense reasoning tasks, and outperforms them on multi-step reasoning tasks. \\textbf{phi-3-mini} achieves performance on par with models like Mixtral 8x7B and GPT-3.5 despite being significantly smaller, and \\textbf{phi-4} surpasses its teacher model, GPT-4o, on STEM-focused QA tasks.\n\nThe papers also highlight the importance of data quality over scale, with \\textbf{phi-1.5} and \\textbf{phi-3-mini} achieving performance comparable to models with an order of magnitude more parameters. The use of synthetic data and filtered web data is crucial in achieving this performance, as it ensures the model has a broad base of general knowledge and reasoning skills.\n\n### Conclusion\nThe Phi series of models demonstrates that high-quality, curated datasets can significantly enhance the performance of smaller models on a variety of tasks. The use of synthetic data and filtered web data allows these models to achieve capabilities comparable to much larger models, while maintaining a smaller size and lower computational cost. The introduction of techniques like Pivotal Token Search (PTS) and safety post-training further improves the model's robustness and safety.\n\nFuture research directions include expanding the synthetic dataset to cover a broader array of topics, fine-tuning the models for more specific tasks, and exploring the integration of search engines to mitigate factual hallucinations. Additionally, further work is needed to address weaknesses in instruction-following and long-context understanding, as well as to improve the model's performance on multilingual tasks. The Phi series models pave the way for more efficient and sustainable AI systems, making advanced capabilities accessible to a broader range of users and organizations.\n\n### Acknowledgments\nThe authors of the Phi series models thank numerous colleagues and teams at Microsoft Research for their contributions, including discussions on the direction of the work, red-teaming exercises, and support in adhering to Microsoft's Responsible AI (RAI) standards."
}