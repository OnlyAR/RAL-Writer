{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Modeling Relational Data with Graph Convolutional Networks}\n\n\\begin{document}\n\n\\title{Modeling Relational Data with Graph Convolutional Networks}\n\n\\author{\n  Michael Schlichtkrull$^*$\\\\\n  University of Amsterdam\\\\\n  \\texttt{m.s.schlichtkrull@uva.nl}\\\\\\And\n  Thomas N.~Kipf\\thanks{Equal contribution.}\\\\\n  University of Amsterdam\\\\\n  \\texttt{t.n.kipf@uva.nl}\\\\ \\And\n   Peter Bloem\\\\\n  VU Amsterdam\\\\\n  \\texttt{p.bloem@vu.nl}\\\\ \\AND\n  Rianne van den Berg\\\\\n  University of Amsterdam\\\\\n  \\texttt{r.vandenberg@uva.nl}\\\\\\And\n  Ivan Titov\\\\\n  University of Amsterdam\\\\\n  \\texttt{titov@uva.nl}\\\\ \\And\n  Max Welling\\\\\n  University of Amsterdam, CIFAR\\thanks{Canadian Institute for Advanced Research}\\\\\n  \\texttt{m.welling@uva.nl}\\\\\n}\n\n\\maketitle\n\n\\begin{abstract}\n\nKnowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e.~subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to deal with the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved by enriching them with an encoder model to accumulate evidence over multiple inference steps in the relational graph, demonstrating a large improvement of 29.8\\% on FB15k-237 over a decoder-only baseline.\n\n\\end{abstract}\n\n\\section{Introduction}\nKnowledge bases organize and store factual knowledge, enabling\na multitude of applications including question answering~\\cite{yao2014information,bao2014knowledge,seyler2015generating,hixon2015learning,bordes2015large,dong2015question} and information retrieval~\\cite{kotov2012tapping,dalton2014entity,xiong2015query,xiong2015esdrank}.\nEven the largest knowledge\nbases (e.g. DBPedia, Wikidata or Yago), despite enormous effort invested in their maintenance, are incomplete, and the lack of coverage\nharms downstream applications. Predicting missing\ninformation in knowledge bases is the main focus of statistical relational learning (SRL).\n\nFollowing previous work on SRL, we assume that knowledge bases store collections of triples of the form (subject, predicate, object). Consider, for example, the triple (\\textit{Mikhail Baryshnikov}, \\textit{educated\\_at}, \\textit{Vaganova Academy}), where we will refer to {\\it Baryshnikov} and {\\it Vaganova Academy} as entities and to {\\it educated\\_at} as a relation. Additionally, we assume that entities are labeled with types (e.g.,  {\\it Vaganova Academy} is marked as a {\\it university}).\nIt is convenient to represent knowledge bases as directed labeled multigraphs with entities corresponding to nodes and triples encoded by labeled edges (see Figure~\\ref{fig:kb}).\n\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=0.95\\linewidth]{figures/kb-example.pdf}\n    \\caption{A knowledge base fragment: The nodes are entities, the edges are relations labeled with their types, the nodes are labeled with entity types (e.g., {\\it university}). The edge and the node label shown in red are the missing information to be inferred.}\n    \\label{fig:kb}\n\\end{figure}\n\nWe consider two fundamental SRL tasks: link prediction (recovery of missing triples) and entity classification (assigning types or categorical properties to entities). In both cases, many missing pieces of information can be expected to reside within the graph encoded through the neighborhood structure -- i.e.  knowing that \\textit{Mikhail Baryshnikov} was educated at the \\textit{Vaganova Academy} implies both that \\textit{Mikhail Baryshnikov} should have the label person, and that the triple (\\textit{Mikhail Baryshnikov}, \\textit{lived\\_in}, \\textit{Russia}) must belong to the knowledge graph. Following this intuition, we develop an encoder model for entities in the relational graph and apply it to both tasks.\n\nOur entity classification model, similarly to \\citet{kipf2016semi}, uses softmax classifiers at each node in the graph. The classifiers take node representations supplied by a relational graph convolutional network (R-GCN) and predict the labels. The model, including R-GCN parameters, is learned by optimizing the cross-entropy loss.\n\nOur link prediction model can be regarded as an autoencoder consisting of (1) an encoder: an R-GCN producing latent feature representations of entities, and (2) a decoder: a tensor factorization model exploiting these representations to predict labeled edges. Though in principle the decoder can rely on any type of factorization (or generally any scoring function), we use one of the simplest and most effective factorization methods: DistMult~\\cite{distmult-embedding_entities_and_relations}. We observe that our method achieves competitive results on standard benchmarks, outperforming, among other baselines, direct optimization of the factorization (i.e.~vanilla DistMult).  This improvement is especially large when we consider the more challenging FB15k-237 dataset~\\cite{toutanova2015observed}. This result demonstrates that explicit modeling of neighborhoods in R-GCNs is beneficial for recovering missing facts in knowledge bases.\n\nOur main contributions are as follows. To the best of our knowledge, we are the first to show that the GCN framework can be applied to modeling relational data, specifically to link prediction and entity classification tasks. Secondly, we introduce techniques for parameter sharing and to enforce sparsity constraints, and use them to apply R-GCNs to multigraphs with large numbers of relations. Lastly, we show that the performance of factorization models, at the example of DistMult, can be significantly improved by enriching them with an encoder model that performs multiple steps of information propagation in the relational graph.\n\n\\section{Neural relational modeling}\nWe introduce the following notation: we denote directed and labeled multi-graphs as $G = (\\mathcal{V}, \\mathcal{E}, \\mathcal{R})$ with nodes (entities) $v_i \\in \\mathcal{V}$ and labeled edges (relations) $(v_i, r, v_j) \\in \\mathcal{E}$, where $r\\in\\mathcal{R}$ is a relation type.\\footnote{$\\mathcal{R}$ contains relations both in canonical direction (e.g. \\textit{born\\_in}) and in inverse direction (e.g. \\textit{born\\_in\\_inv}).}\n\n\\subsection{Relational graph convolutional networks}\nOur model is primarily motivated as an extension of GCNs that operate on local graph neighborhoods \\cite{duvenaud2015convolutional,kipf2016semi} to large-scale relational data. These and related methods such as graph neural networks \\cite{scarselli2009graph} can be understood as special cases of a simple differentiable message-passing framework \\cite{gilmer2017neural}:\n\\begin{equation}\n\\label{eq:message-passing}\nh_i^{(l+1)}= \\sigma \\left( \\sum_{m \\in \\mathcal{M}_i} g_m(h_i^{(l)}, h_j^{(l)}) \\right),\n\\end{equation}\nwhere $h_i^{(l)}\\in\\mathbb{R}^{d^{(l)}}$ is the hidden state of node $v_i$ in the $l$-th layer of the neural network, with $d^{(l)}$ being the dimensionality of this layer's representations. Incoming messages of the form $g_m(\\cdot, \\cdot)$ are accumulated and passed through an element-wise activation function $\\sigma(\\cdot)$, such as the $\\mathrm{ReLU}(\\cdot)=\\max(0,\\cdot)$.\\footnote{Note that this represents a simplification of the message passing neural network proposed in \\cite{gilmer2017neural} that suffices to include the aforementioned models as special cases.} $\\mathcal{M}_i$ denotes the set of incoming messages for node $v_i$ and is often chosen to be identical to the set of incoming edges. $g_m(\\cdot, \\cdot)$ is typically chosen to be a (message-specific) neural network-like function or simply a linear transformation $g_m(h_i, h_j)=W h_j$ with a weight matrix $W$ such as in \\citet{kipf2016semi}.\n\nThis type of transformation has been shown to be very effective at accumulating and encoding features from local, structured neighborhoods, and has led to significant improvements in areas such as graph classification \\cite{duvenaud2015convolutional} and graph-based semi-supervised learning \\cite{kipf2016semi}.\n\nMotivated by these architectures, we define the following simple propagation model for calculating the forward-pass update of an entity or node denoted by $v_i$ in a relational (directed and labeled) multi-graph:\n\\begin{equation}\n\\label{eq:layer}\nh_i^{(l+1)}= \\sigma \\left( \\sum_{r \\in \\mathcal{R}}\\sum_{j \\in \\mathcal{N}^r_i} \\frac{1}{c_{i,r}}W_r^{(l)} h_j^{(l)} + W_0^{(l)}h_i^{(l)} \\right),\n\\end{equation}\nwhere $\\mathcal{N}^r_i$ denotes the set of neighbor indices of node $i$ under relation $r\\in\\mathcal{R}$. $c_{i,r}$ is a problem-specific normalization constant that can either be learned or chosen in advance (such as $c_{i,r}=|\\mathcal{N}^r_i|$).\n\nIntuitively, (\\ref{eq:layer}) accumulates transformed feature vectors of neighboring nodes through a normalized sum. Different from regular GCNs, we introduce relation-specific transformations, i.e. depending on the type and direction of an edge.\nTo ensure that the representation of a node at layer $l+1$ can also be informed by the corresponding representation at layer $l$, we add a single self-connection of a special relation type to each node in the data. Note that instead of simple linear message transformations, one could choose more flexible functions such as multi-layer neural networks (at the expense of computational efficiency). We leave this for future work.\n\nA neural network layer update consists of evaluating (\\ref{eq:layer}) in parallel for every node in the graph. In practice, (\\ref{eq:layer}) can be implemented efficiently using sparse matrix multiplications to avoid explicit summation over neighborhoods. Multiple layers can be stacked to allow for dependencies across several relational steps. We refer to this graph encoder model as a relational graph convolutional network (R-GCN). The computation graph for a single node update in the R-GCN model is depicted in Figure \\ref{fig:model}.\n\n\\begin{figure}[t!]\n        \\centering\n    \\includegraphics[width=0.8\\linewidth]{figures/rgcn-layer.pdf}\n    \\label{fig:model-a}\n    \\caption{Diagram for computing the update of a single graph node/entity (red) in the R-GCN model. Activations ($d$-dimensional  vectors) from neighboring nodes (dark blue) are gathered and then transformed for each relation type individually (for both in- and outgoing edges). The resulting representation (green) is accumulated in a (normalized) sum and passed through an activation function (such as the ReLU). This per-node update can be computed in parallel with shared parameters across the whole graph.}\n    \\label{fig:model}\n\\end{figure}\n\n\\subsection{Regularization}\nA central issue with applying (\\ref{eq:layer}) to highly multi-relational data is the rapid growth in number of parameters with the number of relations in the graph. In practice this can easily lead to overfitting on rare relations and to models of very large size.\n\nTo address this issue, we introduce two separate methods for regularizing the weights of R-GCN-layers: \\textit{basis}- and \\textit{block-diagonal}-decomposition. With the basis decomposition, each $W_r^{(l)}$ is defined as follows:\n\\begin{equation}\n\\label{eq:basis}\nW_r^{(l)} = \\sum_{b=1}^B a_{rb}^{(l)} V_b^{(l)},\n\\end{equation}\ni.e.~as a linear combination of basis transformations $V_b^{(l)}\\in\\mathbb{R}^{d^{(l+1)}\\times d^{(l)}}$ with coefficients $a_{rb}^{(l)}$ such that only the coefficients depend on \\textit{r}. In the block-diagonal decomposition, we let each $W_r^{(l)}$ be defined through the direct sum over a set of low-dimensional matrices:\n\\begin{equation}\n\\label{eq:block}\nW_r^{(l)} = \\bigoplus_{b=1}^B Q^{(l)}_{br}.\n\\end{equation}\nThereby, $W_r^{(l)}$ are block-diagonal matrices:\n$\\mathrm{diag}(Q^{(l)}_{1r}, \\ldots, Q^{(l)}_{Br})$ with $Q^{(l)}_{br} \\in \\mathbb{R}^{(d^{(l+1)}/B)\\times( d^{(l)}/B)}$.\n\nThe basis function decomposition \\eqref{eq:basis} can be seen as a form of effective weight sharing between different relation types, while the block decomposition \\eqref{eq:block} can be seen as a sparsity constraint on the weight matrices for each relation type. The block decomposition structure encodes an intuition that latent features can be grouped into sets of variables which are more tightly coupled within groups than across groups.\nBoth decompositions reduce the number of parameters needed to learn for highly multi-relational data (such as realistic knowledge bases). At the same time, we expect that the basis parameterization can alleviate overfitting on rare relations, as parameter updates are shared between both rare and more frequent relations.\n\nThe overall R-GCN model then takes the following form: We stack $L$ layers as defined in \\eqref{eq:layer} -- the output of the previous layer being the input to the next layer. The input to the first layer can be chosen as a unique one-hot vector for each node in the graph if no other features are present. For the block representation, we map this one-hot vector to a dense representation through a single linear transformation. While we only consider such a featureless approach in this work, we note that it was shown in \\citet{kipf2016semi} that it is possible for this class of models to make use of pre-defined feature vectors (e.g.~a bag-of-words description of a document associated with a specific node).\n\n\\section{Entity classification}\n\\begin{figure}[t!]\n    \\centering\n    \\begin{subfigure}[b]{0.4\\linewidth}\n      \\centering\n      \\includegraphics[width=0.90\\linewidth, trim={0 0.2cm 0.6cm 0}, clip]{figures/entity-classification.pdf}\n      \\caption{Entity classification}        \n      \\label{fig:model-b}\n    \\end{subfigure}%\n    ~\\quad\n    \\begin{subfigure}[b]{0.49\\linewidth}\n        \\centering\n        \\includegraphics[width=\\linewidth, trim={0 0.2cm 0.6cm 0}, clip]{figures/link-prediction.pdf}\n        \\caption{Link prediction}\n        \\label{fig:model-c}\n    \\end{subfigure}\n    \\caption{(a) Depiction of an R-GCN model for entity classification with a per-node loss function. (b) Link prediction model with an R-GCN encoder (interspersed with fully-connected/dense layers) and a DistMult decoder that takes pairs of hidden node representations and produces a score for every (potential) edge in the graph. The loss is evaluated per edge.}\n\\end{figure}\n\nFor (semi-)supervised classification of nodes (entities), we simply stack R-GCN layers of the form \\eqref{eq:layer}, with a $\\mathrm{softmax}(\\cdot)$ activation (per node) on the output of the last layer. We minimize the following cross-entropy loss on all labeled nodes (while ignoring unlabeled nodes):\n\\begin{equation}\n\\mathcal{L}= -\\sum_{i\\in\\mathcal{Y}}\\sum_{k=1}^K t_{ik} \\ln h_{ik}^{(L)}  \\, ,\n\\label{eq:}\n\\end{equation} \nwhere $\\mathcal{Y}$ is the set of node indices that have labels and $h_{ik}^{(L)}$ is the $k$-th entry of the network output for the $i$-th labeled node. $t_{ik}$ denotes its respective ground truth label. In practice, we train the model using (full-batch) gradient descent techniques. A schematic depiction of our entity classification model is given in Figure \\ref{fig:model-b}.\n\\section{Link prediction}\\label{section:link_prediction}\nLink prediction deals with prediction of new facts (i.e. triples \\textit{(subject, relation, object)}). Formally, the knowledge base is represented by a directed, labeled graph $G = (\\mathcal{V},\\mathcal{E},\\mathcal{R})$. Rather than the full set of edges $\\mathcal{E}$, we are given only an incomplete subset $\\hat{\\mathcal{E}}$. The task is to assign scores $f(s,r,o)$ to possible edges $(s,r,o)$ in order to determine how likely those edges are to belong to $\\mathcal{E}$.\n\nIn order to tackle this problem, we introduce a graph auto-encoder model, comprised of an entity encoder and a scoring function (decoder). The encoder maps each entity $v_i \\in \\mathcal{V}$ to a real-valued vector $e_i \\in \\mathbb{R}^d$. The decoder reconstructs edges of the graph relying on the vertex representations; in other words, it scores \\textit{(subject, relation, object)}-triples through a function $s: \\mathbb{R}^d \\times \\mathcal{R} \\times \\mathbb{R}^d \\to \\mathbb{R}$. Most existing approaches to link prediction (for example, tensor and neural factorization methods~\\cite{socher2013reasoning,lin2015modeling,toutanova2016compositional,distmult-embedding_entities_and_relations,complex-complex_embeddings_for_simple_link_prediction}) can be interpreted under this framework. The crucial distinguishing characteristic of our work is the reliance on an encoder. Whereas most previous approaches use a single, real-valued vector $e_i$ for every $v_i \\in \\mathcal{V}$ optimized directly in training, %(i.e.~using the identity function as an encoder model) -- IT I do not think it is clear at that stage\nwe compute representations through an R-GCN encoder with $e_i = h_i^{(L)}$, similar to the graph auto-encoder model introduced in \\citet{kipf2016variational} for unlabeled undirected graphs.\nOur full link prediction model is schematically depicted in Figure~\\ref{fig:model-c}.\n\nIn our experiments, we use the DistMult factorization~\\cite{distmult-embedding_entities_and_relations} as the scoring function, which is known to perform well on standard link prediction benchmarks when used on its own.\nIn DistMult, every relation $r$ is associated with a diagonal matrix $R_r\n\\in \\mathbb{R}^{d \\times d}$ and a triple $(s, r, o)$ is scored as\n\\begin{equation}\nf(s, r, o) = e_s^T R_r e_o \\, .\n\\end{equation}\n\nAs in  previous work on factorization~\\cite{distmult-embedding_entities_and_relations,complex-complex_embeddings_for_simple_link_prediction}, we train the model with negative sampling. For each observed example we sample $\\omega$ negative ones. We sample by randomly corrupting either the subject or the object of each positive example.\nWe optimize for cross-entropy loss to push the model to score observable triples higher than the negative ones:\n\\begin{equation}\n\\begin{split}\n\\mathcal{L} = - \\frac{1}{ (1+\\omega) |\\mathcal{\\hat{E}}|}\\sum\\limits_{(s,r,o,y) \\in \\mathcal{T}} y \\log l\\bigl(f(s,r,o)\\bigr) + \\\\\n(1-y) \\log\\bigl(1-l\\bigl(f(s,r,o)\\bigr)\\bigr) \\, ,\n\\end{split}\n\\end{equation}\nwhere $\\mathcal{T}$ is the total set of real and corrupted triples, $l$ is the logistic sigmoid function, and $y$ is an indicator set to $y=1$ for positive triples and $y=0$ for negative ones.\n\n\\section{Empirical evaluation}\n\n\\subsection{Entity classification experiments}\\label{sec:ent_exp}\nHere, we consider the task of classifying entities in a knowledge base. In order to infer, for example, the type of an entity (e.g. person or company), a successful model needs to reason about the relations with other entities that this entity is involved in.\n\n\\paragraph{Datasets}\nWe evaluate our model on four datasets\\footnote{http://dws.informatik.uni-mannheim.de/en/research/a-collection-of-benchmark-datasets-for-ml} in Resource Description Framework (RDF) format \\cite{ristoski2016collection}: AIFB, MUTAG, BGS, and AM. Relations in these datasets need not necessarily encode directed subject-object relations, but are also used to encode the presence, or absence, of a specific feature for a given entity. In each dataset, the targets to be classified are properties of a group of entities represented as nodes. The exact statistics of the datasets can be found in Table \\ref{table:classification_datasets}. For a more detailed description of the datasets the reader is referred to \\citet{ristoski2016collection}. We remove relations that were used to create entity labels: \\textit{employs} and \\textit{affiliation} for AIFB, \\textit{isMutagenic} for MUTAG, \\textit{hasLithogenesis} for BGS, and \\textit{objectCategory} and \\textit{material} for AM.\n\n\\begin{table}[htp!]\n\\centering\n\\begin{tabular}{lrrrr}\n\\toprule\nDataset & AIFB & MUTAG & BGS & AM  \\\\ \\midrule\nEntities    & 8,285 & 23,644 & 333,845 & 1,666,764 \\\\\nRelations   & 45 & 23 & 103 & 133 \\\\\nEdges   & 29,043 & 74,227 & 916,199& 5,988,321 \\\\\nLabeled  & 176 & 340 & 146 &  1,000 \\\\\nClasses  & 4 & 2 & 2 &  11 \\\\\n\n \\bottomrule\n\\end{tabular}\n\\caption{Number of entities, relations, edges and classes along with the number of labeled entities for each of the datasets. \\textit{Labeled} denotes the subset of entities that have labels and that are to be classified.}\n\\label{table:classification_datasets}\n\\end{table}\n\n\\paragraph{Baselines}\nAs a baseline for our experiments, we compare against recent state-of-the-art classification results from RDF2Vec embeddings \\cite{ristoski2016rdf2vec}, Weisfeiler-Lehman kernels (WL) \\cite{shervashidze2011weisfeiler,de2015substructure}, and hand-designed feature extractors (Feat) \\cite{paulheim2012unsupervised}. Feat assembles a feature vector from the in- and out-degree (per relation) of every labeled entity. RDF2Vec extracts walks on labeled graphs which are then processed using the Skipgram \\cite{mikolov2013distributed} model to generate entity embeddings, used for subsequent classification. See \\citet{ristoski2016rdf2vec} for an in-depth description and discussion of these baseline approaches. All entity classification experiments were run on CPU nodes with 64GB of memory.\n\n\\paragraph{Results}\nAll results in Table \\ref{table:classification_results} are reported on the train/test benchmark splits from \\citet{ristoski2016collection}. We further set aside 20\\% of the training set as a validation set for hyperparameter tuning. For R-GCN, we report performance of a 2-layer model with 16 hidden units (10 for AM), basis function decomposition (Eq. \\ref{eq:basis}), and trained with Adam \\cite{kingma2014adam} for 50 epochs using a learning rate of $0.01$.  The normalization constant is chosen as $c_{i,r}=|\\mathcal{N}_i^r|$. Further details on (baseline) models and hyperparameter choices are provided in the supplementary material.\n\n\\begin{table}[htp!]\n\\centering\n\\begin{tabular}{lcccc}\n\\toprule\nModel & AIFB & MUTAG & BGS & AM  \\\\ \\midrule\nFeat & $55.55$ & $77.94$ & $72.41$ & $66.66$ \\\\\nWL\t\t& $80.55$ & $\\mathbf{80.88}$ & $86.20$ & $87.37$  \\\\\nRDF2Vec  & $88.88$ & $67.20$ & $\\mathbf{87.24}$ & $88.33$ \\\\\nR-GCN\t\t\t& $\\mathbf{95.83}$ & $73.23$ & $83.10$  & $\\mathbf{89.29}$ \\\\\\bottomrule\n\n\\end{tabular}\n\\caption{Entity classification results in accuracy (averaged over 10 runs) for a feature-based baseline (see main text for details), WL \\cite{shervashidze2011weisfeiler,de2015substructure}, RDF2Vec \\cite{ristoski2016rdf2vec}, and R-GCN (this work). Test performance is reported on the train/test set splits provided by \\citet{ristoski2016collection}. \\label{table:classification_results}} %R-GCN: Version of our model that uses a featurized version of the graph ('type' relations are transformed into node feature vectors). -- AIFB and AM are knowledge graphs, MUTAG are molecular graphs in RDF format, BGS is a feature table in RDF format. Note for R-GCN BGS (clean): same hyperparameters as in BGS.\n\\end{table}\n\nOur model achieves state-of-the-art results on AIFB and AM. To explain the gap in performance on MUTAG and BGS it is important to understand the nature of these datasets. MUTAG is a dataset of molecular graphs, which was later converted to RDF format, where relations either indicate atomic bonds or merely the presence of a certain feature. BGS is a dataset of rock types with hierarchical feature descriptions which was similarly converted to RDF format, where relations encode the presence of a certain feature or feature hierarchy. Labeled entities in MUTAG and BGS are only connected via high-degree hub nodes that encode a certain feature.\n\nWe conjecture that the fixed choice of normalization constant for the aggregation of messages from neighboring nodes is partly to blame for this behavior, which can be particularly problematic for nodes of high degree. A potential way to overcome this limitation is to introduce an attention mechanism, i.e. to replace the normalization constant $1/c_{i,r}$ with data-dependent attention weights $a_{ij,r}$, where $\\sum_{j,r}a_{ij,r}=1$. We expect this to be a promising avenue for future research.\n\n\\subsection{Link prediction experiments}\nAs shown in the previous section, R-GCNs serve as an effective encoder for relational data. We now combine our encoder model with a scoring function (which we will refer to as a decoder, see Figure \\ref{fig:model-c}) to score candidate triples for link prediction in knowledge bases.\n\n\\subsubsection{Datasets}\nLink prediction algorithms are commonly evaluated on FB15k, a subset of the relational database Freebase, and WN18, a subset of WordNet containing lexical relations between words. In \\citet{toutanova2015observed}, a serious flaw was observed in both datasets: The presence of inverse triplet pairs $t = (e_1, r, e_2)$ and $t' = (e_2, r^{-1}, e_1)$ with $t$ in the training set and $t'$ in the test set. This reduces a large part of the prediction task to memorization of affected triplet pairs. A simple baseline LinkFeat employing a linear classifier on top of sparse feature vectors of observed training relations was shown to outperform existing systems by a large margin. To address this issue, Toutanova and Chen proposed a reduced dataset FB15k-237 with all such inverse triplet pairs removed. We therefore choose FB15k-237 as our primary evaluation dataset. Since FB15k and WN18 are still widely used, we also include results on these datasets using the splits introduced by \\citet{bordes2013translating}.\n\n\\begin{table}[ht]\n\\centering\n\\begin{tabular}{lrrr}\n\\toprule\nDataset & WN18 & FB15K & FB15k-237\\\\ \\midrule\nEntities    & 40,943  & 14,951 & 14,541\\\\\nRelations   & 18 & 1,345 & 237\\\\\nTrain edges & 141,442 & 483,142 & 272,115\\\\\nVal.~edges & 5,000 & 50,000 & 17,535\\\\\nTest edges & 5,000 & 59,071 & 20,466\\\\ \\bottomrule\n\\end{tabular}\n\\caption{Number of entities and relation types along with the number of edges per split for the three datasets.\\label{table:datasets}}\n\\end{table}\n\n\\paragraph{Baselines}\nA common baseline for both experiments is direct optimization of \\textit{DistMult}~\\cite{distmult-embedding_entities_and_relations}. This factorization strategy is known to perform well on standard datasets, and furthermore corresponds to a version of our model with fixed entity embeddings in place of the R-GCN encoder as described in Section \\ref{section:link_prediction}. As a second baseline, we add the simple neighbor-based LinkFeat algorithm proposed in \\citet{toutanova2015observed}.\n\nWe further compare to ComplEx \\cite{complex-complex_embeddings_for_simple_link_prediction} and HolE \\cite{nickel2015holographic}, two state-of-the-art link prediction models for FB15k and WN18. ComplEx facilitates modeling of asymmetric relations by generalizing DistMult to the complex domain, while HolE replaces the vector-matrix product with circular correlation. Finally, we include comparisons with two classic algorithms -- CP~\\cite{hitchcock1927expression} and TransE~\\cite{bordes2013translating}.\n\n\\begin{table*}[t!]\n\\centering\n\\begin{tabular}{@{}lllllllllll@{}}\n\\toprule\n      & \\multicolumn{5}{c}{FB15k}                             & \\multicolumn{5}{c}{WN18}                            \\\\ \\cmidrule(lr){2-6} \\cmidrule(l){7-11}\n      & \\multicolumn{2}{c}{MRR} & \\multicolumn{3}{c}{Hits @} & \\multicolumn{2}{c}{MRR} & \\multicolumn{3}{c}{Hits @} \\\\ \\cmidrule(lr){2-3} \\cmidrule(lr){4-6} \\cmidrule(lr){7-8} \\cmidrule(l){9-11}\nModel & Raw      & Filtered     & 1       & 3      & 10      & Raw      & Filtered     & 1       & 3      & 10      \\\\ \\midrule\nLinkFeat \t\t&  & 0.779 & & & 0.804 & & 0.938 & & & 0.939 \\\\\n\\midrule\nDistMult \t\t& 0.248 & 0.634 & 0.522 & 0.718 & 0.814 & 0.526 & 0.813 & 0.701 & 0.921 & 0.943 \\\\\nR-GCN  \t\t\t& 0.251 & 0.651 & 0.541 & 0.736 & 0.825 & 0.553 & 0.814 & 0.686 & 0.928 & 0.955 \\\\\nR-GCN+\t\t\t& \\textbf{0.262} & \\textbf{0.696} & \\textbf{0.601} & \\textbf{0.760} & \\textbf{0.842} & 0.561 & 0.819 & 0.697 & 0.929 & \\textbf{0.964}  \\\\ \\midrule\nCP* & 0.152 & 0.326 & 0.219 & 0.376 & 0.532 & 0.075 & 0.058 & 0.049 & 0.080 & 0.125 \\\\\nTransE* \t\t& 0.221 & 0.380 & 0.231 & 0.472 & 0.641 & 0.335 & 0.454 & 0.089 & 0.823 & 0.934 \\\\\nHolE** \t\t\t& 0.232 & 0.524 & 0.402 & 0.613 & 0.739 & \\textbf{0.616} & 0.938 & 0.930 & \\textbf{0.945} & 0.949\\\\\nComplEx* \t\t& 0.242 & 0.692 & 0.599 & 0.759 & 0.840 & 0.587 & \\textbf{0.941} & \\textbf{0.936} & \\textbf{0.945} & 0.947 \\\\ \\bottomrule\n\\end{tabular}\n\\caption{Results on the the Freebase and WordNet datasets. Results marked (*) taken from \\citet{complex-complex_embeddings_for_simple_link_prediction}. Results marks (**) taken from \\citet{nickel2015holographic}. R-GCN+ denotes an ensemble between R-GCN and DistMult -- see main text for details.}\n\\label{table:link-results}\n\\end{table*}\n\n\\paragraph{Results}\nWe provide results using two commonly used evaluation metrics: \\textit{mean reciprocal rank} (MRR) and \\textit{Hits at $n$} (H@n).\nFollowing \\citet{bordes2013translating}, both metrics can be computed in a raw and a filtered setting. We report both filtered and raw MRR (with filtered MRR typically considered more reliable), and filtered Hits at 1, 3, and 10.\n\nWe evaluate hyperparameter choices on the respective validation splits. We found a normalization constant defined as $c_{i,r}=c_{i}=\\sum_{r} |\\mathcal{N}^r_i|$ --- in other words, applied across relation types -- to work best. For FB15k and WN18, we report results using basis decomposition (Eq. \\ref{eq:basis}) with two basis functions, and a single encoding layer with $200$-dimensional embeddings. For FB15k-237, we found block decomposition (Eq. \\ref{eq:block}) to perform best, using two layers with block dimension $5 \\times 5$ and $500$-dimensional embeddings. We regularize the encoder through edge dropout applied before normalization, with dropout rate $0.2$ for self-loops and $0.4$ for other edges. Using edge droupout makes our training objective similar to that of denoising autoencoders~\\cite{vincent2008}. We apply $l2$ regularization to the decoder with a penalty of $0.01$.\n\nWe use the Adam optimizer \\cite{kingma2014adam} with a learning rate of $0.01$. For the baseline and the other factorizations, we found the parameters from \\citet{complex-complex_embeddings_for_simple_link_prediction} -- apart from the dimensionality on FB15k-237 -- to work best, though to make the systems comparable we maintain the same number of negative samples (i.e.~$\\omega=1$). We use full-batch optimization for both the baselines and our model.\n\n\\begin{figure}\n\\centering\n\\begin{tikzpicture}\n\\begin{axis}[xlabel=Average degree,\nylabel=MRR,\nlegend style={font=\\fontsize{9}{11}\\selectfont},\nwidth=1.2\\linewidth,\nheight=0.6\\linewidth,\nscale=.65]\n\\addplot[color=red,mark=triangle*] table[x=Degree,y=GCN] {data/degree.dat};\n\\label{p1}\n\\addplot[color=blue,mark=x] table[x=Degree, y=Distmult] {data/degree.dat};\n\\label{p2}\n\\legend{R-GCN, DistMult}\n\\end{axis}%\n\\end{tikzpicture}\\vspace{-0.5em}\n\\caption{Mean reciprocal rank (MRR) for R-GCN  and DistMult on the FB15k validation data as a function of the node degree (average of subject and object).\\label{figure:degree}}\n\\end{figure}\n\nOn FB15k, local context in the form of inverse relations is expected to dominate the performance of the factorizations, contrasting with the design of the R-GCN model. To better understand the difference, we plot in Figure \\ref{figure:degree} the FB15k performance of the best R-GCN model and the baseline (DistMult) as functions of degree of nodes corresponding to entities in the considered triple\n(namely, the average of degrees for the subject and object entities). It can be seen that our model performs better for nodes with high degree where contextual information is abundant. The observation that the two models are complementary suggests combining the strengths of both into a single model, which we refer to as R-GCN+. On FB15k and WN18 where local and long-distance information can both provide strong solutions, we expect R-GCN+ to outperform each individual model. On FB15k-237 where local information is less salient, we do not expect the combination model to outperform a pure R-GCN model significantly. To test this, we evaluate an ensemble (R-GCN+) with a trained R-GCN model and a separately trained DistMult factorization model: $f(s,r,t)_{\\text{R-GCN+}} = \\alpha f(s,r,t)_{\\text{R-GCN}} + (1- \\alpha) f(s,r,t)_{\\text{DistMult}}$, with $\\alpha=0.4$ selected on FB15k development data.\n\nIn Table~\\ref{table:link-results}, we evaluate the R-GCN model and the combination model (R-GCN+) on FB15k and WN18.\n\nOn the FB15k and WN18 datasets, R-GCN and R-GCN+ both outperform the DistMult baseline, but like all other systems underperform on these two datasets compared to the LinkFeat algorithm. The strong result from this baseline highlights the contribution of inverse relation pairs to high-performance solutions on these datasets. Interestingly, R-GCN+ yields better performance than ComplEx for FB15k, even though the R-GCN decoder (DistMult) does not explicitly model asymmetry in relations, as opposed to ComplEx.\n\nThis suggests that combining the R-GCN encoder with the ComplEx scoring function (decoder) may be a promising direction for future work. The choice of scoring function is orthogonal to the choice of encoder; in principle, any scoring function or factorization model could be incorporated as a decoder in our auto-encoder framework.\n\n\\begin{table}[htp!]\n\\centering\n\\begin{tabular}{llllll}\n\\toprule\n                                  & \\multicolumn{2}{c}{MRR} & \\multicolumn{3}{c}{Hits @} \\\\ \\cmidrule{2-3} \\cmidrule{4-6}\nModel                             & Raw      & Filtered     & 1       & 3       & 10     \\\\ \\midrule\nLinkFeat     &  & 0.063         &    & & 0.079  \\\\ \\midrule\nDistMult     & 0.100         & 0.191         & 0.106   & 0.207   & 0.376  \\\\\nR-GCN               & \\textbf{0.158}       & 0.248 & \\textbf{0.153}     & 0.258     & 0.414     \\\\\nR-GCN+                     & 0.156 & \\textbf{0.249} & 0.151     & \\textbf{0.264}     & \\textbf{0.417} \\\\     \\midrule\nCP     & 0.080 & 0.182 & 0.101 & 0.197 & 0.357 \\\\\nTransE    \t& 0.144\t& 0.233 & 0.147 & 0.263 &\t0.398  \\\\\nHolE    \t& 0.124\t& 0.222 & 0.133 & 0.253 &\t0.391  \\\\\nComplEx    & 0.109 & 0.201 & 0.112 & 0.213   & 0.388 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Results on FB15k-237, a reduced version of FB15k with problematic inverse relation pairs removed. CP, TransE, and ComplEx were evaluated using the code published for \\citet{complex-complex_embeddings_for_simple_link_prediction}, while HolE was evaluated using the code published for \\citet{nickel2015holographic}.}\n\\label{table:fb15k-237}\n\\end{table}\n\nIn Table \\ref{table:fb15k-237}, we show results for FB15k-237 where (as previously discussed) inverse relation pairs have been removed and the LinkFeat baseline fails to generalize\\footnote{Our numbers are not directly comparable to those reported in \\citet{toutanova2015observed}, as they use pruning both for training and testing (see their sections 3.3.1 and 4.2). Since their pruning schema is not fully specified (e.g., values of the relation-specific parameter $t$ are not given) and the code is not available, it is not possible to replicate their set-up.}. Here, our R-GCN model outperforms the DistMult baseline by a large margin of 29.8\\%, highlighting the importance of a separate encoder model. As expected from our earlier analysis, R-GCN and R-GCN+ show similar performance on this dataset. The R-GCN model further compares favorably against other factorization methods, despite relying on a DistMult decoder which shows comparatively weak performance when used without an encoder.\n\n\\section{Related Work}\n\\subsection{Relational modeling}\nOur encoder-decoder approach to link prediction relies on DistMult~\\cite{distmult-embedding_entities_and_relations} in the decoder,  a special and simpler case of the RESCAL factorization~\\cite{nickel2011three}, more effective than the original RESCAL in the context of multi-relational knowledge bases. Numerous alternative factorizations have been proposed and studied in the context of SRL, including both (bi-)linear and nonlinear ones~(e.g., \\cite{bordes2013translating,socher2013reasoning,KaiWei14,nickel2015holographic,complex-complex_embeddings_for_simple_link_prediction}). Many of these approaches can be regarded as modifications or special cases of classic tensor decomposition methods such as CP or Tucker; for a comprehensive overview of tensor decomposition literature we refer the reader to \\citet{kolda2009tensor}.\n\nIncorporation of paths between entities in knowledge\nbases has recently received considerable attention. We can roughly classify previous work into (1) methods creating auxiliary triples, which are then added to the learning objective of a factorization model~\\cite{guu2015traversing,garcia2015composing}; (2) approaches using paths (or walks) as features when predicting edges ~\\cite{lin2015modeling}; or (3) doing both at the same time~\\cite{neelakantan2015compositional,toutanova2016compositional}. The first direction is largely orthogonal to ours, as we would also expect improvements from adding similar terms to our loss (in other words, extending our decoder). The second research line is more comparable; R-GCNs provide a computationally cheaper alternative to these path-based models. %thus, letting us use larger datasets than typically considered with these methods.\nDirect comparison is somewhat complicated as path-based methods used different datasets (e.g., sub-sampled sets of walks from a knowledge base).\n\n\\subsection{Neural networks on graphs}\nOur R-GCN encoder model is closely related to a number of works in the area of neural networks on graphs. It is primarily motivated as an adaption of previous work on GCNs \\cite{bruna2014spectral,duvenaud2015convolutional,defferrard2016convolutional,kipf2016semi} for large-scale and highly multi-relational data, characteristic of realistic knowledge bases.\n\nEarly work in this area includes the graph neural network by \\citet{scarselli2009graph}. A number of extensions to the original graph neural network have been proposed, most notably \\cite{li2015gated} and \\cite{pham2016column}, both of which utilize gating mechanisms to facilitate optimization.\n\nR-GCNs can further be seen as a sub-class of message passing neural networks \\cite{gilmer2017neural}, which encompass a number of previous neural models for graphs, including GCNs, under a differentiable message passing interpretation.\n\n\\section{Conclusions}\n\nWe have introduced relational graph convolutional networks (R-GCNs) and demonstrated their effectiveness in the context of two standard statistical relation modeling problems: link prediction and entity classification. For the entity classification problem, we have demonstrated that the R-GCN model can act as a competitive, end-to-end trainable graph-based encoder. For link prediction, the R-GCN model with DistMult factorization as the decoding component outperformed direct optimization of the factorization model, and achieved competitive results on standard link prediction benchmarks. Enriching the factorization model with an R-GCN encoder proved especially valuable for the challenging FB15k-237 dataset, yielding a 29.8\\% improvement over the decoder-only baseline.\n\nThere are several ways in which our work could be extended. For example, the graph autoencoder model could be considered in combination with other factorization models, such as ComplEx~\\cite{complex-complex_embeddings_for_simple_link_prediction}, which can be better suited for modeling asymmetric relations. It is also straightforward to integrate entity features in R-GCNs, which would be beneficial both for link prediction and entity classification problems. To address the scalability of our method, it would be worthwhile to explore subsampling techniques, such as in \\citet{hamilton2017inductive}. Lastly, it would be promising to replace the current form of summation over neighboring nodes and relation types with a data-dependent attention mechanism. Beyond modeling knowledge bases, R-GCNs can be generalized to other applications where relation factorization models have been shown effective (e.g. relation extraction). \n\n\\subsection*{Acknowledgements}\nWe would like to thank Diego Marcheggiani, Ethan Fetaya, and Christos Louizos for helpful discussions and comments.\nThis project is supported by the European Research Council (ERC StG BroadSem 678254), the SAP Innovation\nCenter Network\nand the Dutch National Science Foundation (NWO VIDI 639.022.518).\n\n\\newpage\n\\onecolumn\n\\setlength{\\parskip}{0.5em}\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Heterogeneous Graph Transformer}\n\n\\begin{document}\n\n\\title{Heterogeneous Graph Transformer}\n\n\\author{Ziniu Hu}\n\\authornote{This work was done when Ziniu was an intern at Microsoft Research.}\n\\affiliation{%\n  \\institution{University of California, Los Angeles}\n}\n\\email{bull@cs.ucla.edu}\n\n\\author{Yuxiao Dong}\n\\affiliation{%\n  \\institution{Microsoft Research, Redmond}\n}\n\\email{yuxdong@microsoft.com}\n\n\\author{Kuansan Wang}\n\\affiliation{%\n  \\institution{Microsoft Research, Redmond}\n}\n\\email{kuansanw@microsoft.com}\n\n\\author{Yizhou Sun}\n\\affiliation{%\n  \\institution{University of California, Los Angeles}\n}\n\\email{yzsun@cs.ucla.edu}\n\n\\begin{abstract}\nRecent years have witnessed the emerging success of graph neural networks (GNNs) for modeling structured data. \nHowever, most GNNs are designed for homogeneous graphs, in which all nodes and edges belong to the same types, making them infeasible to represent  heterogeneous structures. \nIn this paper, we present the \\model\\ (\\short) architecture for modeling Web-scale heterogeneous graphs. \nTo model heterogeneity, we design node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, empowering \\short\\ to maintain dedicated representations for different types of nodes and edges. \nTo handle dynamic heterogeneous graphs, we introduce the relative temporal encoding technique into \\short, which is able to capture the dynamic structural dependency with arbitrary durations. \nTo handle Web-scale graph data, we design the heterogeneous mini-batch graph sampling algorithm---\\sampling---for efficient and scalable training. \nExtensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed \\short\\ model consistently outperforms all the state-of-the-art GNN baselines by 9$\\%$--21$\\%$ on various downstream tasks. The dataset and source code of \\short\\ are publicly available at \\url{https://github.com/acbull/pyHGT}. \n\n\\hide{\nHeterogeneous and dynamic graphs are important abstractions for modeling relational data for many real-world systems, and modelling such graphs have been a challenging research topics. Recently, researchers have adopted deep learning into graph domain and achieve competitive performance in various graph-based applications. However, most existing GNN assume nodes are within same distribution and in static graphs, and thus cannot be directly adapted to heterogeneous and dynamic graphs. In this paper, we propose Heterogeneous Graph Transformer (HGT), for modeling web-scale heterogeneous and dynamic graphs. To model heterogeneity, we leverage the meta relation triplet of heterogeneous graph schema to define interaction and transform matrices, so that the model can capture both the common and specific patterns of different relationships using equal or even smaller parameters. To capture temporal information of graphs, we propose to keep a whole graph with edges happening in different time to avoid structure loss, and adopt relative temporal encoding to make the HGT capture dynamic dependency. We further design a heterogeneous graph sampling algorithm with an inductive timestamp assignment algorithm for training large-scale heterogeneous and dynamic graphs. We conduct experiments on Open Academic Graphs (OAG), which contains 0.1 billion nodes and 2 billion edges. The results show that our proposed HGT outperform all the state-of-the-art baselines on various downstream tasks.\n}%end of hide\\end{abstract}\n\n\\keywords{Graph Neural Networks; Heterogeneous Information Networks; Representation Learning; Graph Embedding; Graph Attention}\n\n\\maketitle\n\n\\section{Introduction}\\label{sec:introduction}\n\nHeterogeneous graphs have been commonly used for abstracting and modeling complex systems, in which objects of different types interact with each other in various ways. \nSome prevalent instances of such systems include academic graphs, Facebook entity graph, LinkedIn economic graph, and broadly the Internet of Things network. \nFor example, the  Open Academic Graph (OAG)~\\cite{DBLP:conf/kdd/ZhangLTDYZGWSLW19} in Figure \\ref{fig:schema} contains five types of nodes: papers, authors, institutions, venues (journal, conference, or preprint), and fields, as well as different types of relationships between them. \n\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=0.48\\textwidth, trim = 5 5 10 0, clip\n    ] \n    {picture/intro-example3.png}\n    \\caption{The schema and meta relations of Open Academic Graph (OAG). \\textmd{Given a Web-scale heterogeneous graph, e.g., an academic network, \\short\\ takes only its one-hop edges as input without manually designing meta paths.}}\n    \\label{fig:schema}\n\\end{figure} \n\nOver the past decade, a significant line of research has been explored for mining heterogeneous graphs~\\cite{Sun:BOOK2012}. \nOne of the classical paradigms is to define and use meta paths to model heterogeneous structures, such as PathSim~\\cite{Sun:VLDB11} and metapath2vec~\\cite{dong2017metapath2vec}. \nRecently, in view of graph neural networks' (GNNs) success~\\cite{gcn, graphsage, gat}, there are several attempts to adopt GNNs to learn with heterogeneous networks~\\cite{DBLP:conf/esws/SchlichtkrullKB18,DBLP:conf/kdd/ZhangSHSC19,DBLP:conf/www/WangJSWYCY19,gt}. \nHowever, these works face several issues: \nFirst, most of them involve the design of meta paths  for each type of heterogeneous graphs, requiring specific domain knowledge; \nSecond, they either simply assume that different types of nodes/edges share the same feature and representation space or keep distinct non-sharing weights for either node type or edge type alone, making them insufficient to capture heterogeneous graphs' properties; \nThird, most of them ignore the dynamic nature of every (heterogeneous) graph; \nFinally, their intrinsic design and implementation make them incapable of modeling Web-scale heterogeneous graphs. \n\nTake OAG for example: First, the nodes and edges in OAG could have different feature distributions, e.g., papers have text features whereas institutions may have features from affiliated scholars, and coauthorships obviously differ from citation links; \nSecond, OAG has been consistently evolving, e.g., 1) the volume of publications doubles every 12 years~\\cite{dong2017century}, and 2) the KDD conference was more related to database in the 1990s whereas more to machine learning in recent years; \nFinally, OAG contains hundreds of millions of nodes and billions of relationships, leaving existing heterogeneous GNNs not scalable for handling it.  \n\nIn light of these limitations and challenges, we propose to study heterogeneous graph neural networks with the goal of maintaining node- and edge-type dependent representations, capturing network dynamics, avoiding customized meta paths, and being scalable to Web-scale graphs. \nIn this work, we present the \\model\\ (\\short) architecture to deal with all these issues. \n\nTo handle graph heterogeneity, we introduce the node- and edge-type dependent attention mechanism.\nInstead of parameterizing each type of edges, the heterogeneous mutual attention in \\short\\ is defined by breaking down each edge $e=(s,t)$ based on its meta relation triplet, i.e., $\\langle$ node type of $s$, edge type of $e$ between $s$ \\& $t$, node type of $t \\rangle$. Figure \\ref{fig:schema} illustrates the meta relations of heterogeneous academic graphs. \nIn specific, we use these meta relations to parameterize the weight matrices for calculating attention over each edge.\nAs a result, nodes and edges of different types are allowed to maintain their specific representation spaces.\nMeanwhile, connected nodes in different types can still interact, pass, and aggregate messages without being restricted by their distribution gaps.\nDue to the nature of its architecture, \\short\\ can incorporate information from high-order neighbors of different types through message passing across layers, which can be regarded as ``soft'' meta paths. \nThat said, even if \\short\\ take only its one-hop edges as input without manually designing meta paths, the proposed attention mechanism can automatically and implicitly learn and extract ``meta paths'' that are important for different downstream tasks. \n\nTo handle graph dynamics, we enhance \\short\\ by proposing the relative temporal encoding (RTE) strategy. \nInstead of slicing the input graph into different timestamps, we propose to maintain all the edges happening in different times as a whole, and design the RTE strategy to model structural temporal dependencies with any duration length, and even with unseen and future timestamps. \nBy end-to-end training, RTE enables \\short\\ to automatically learn the temporal dependency and evolution of heterogeneous graphs. \n\nTo handle Web-scale graph data, we design the first heterogeneous sub-graph sampling algorithm---\\sampling---for mini-batch GNN training. \nIts main idea is to sample heterogeneous sub-graphs in which different types of nodes are with similar proportions, since the direct usage of existing (homogeneous) GNN sampling methods, such as GraphSage~\\cite{graphsage}, FastGCN~\\cite{fastgcn}, and LADIES~\\cite{ladies}, results in highly imbalanced ones regarding to both node and edge types. \nIn addition, it is also designed to keep the sampled sub-graphs dense for minimizing the loss of information. \nWith \\sampling, all the GNN models, including our proposed \\short, can train and infer on arbitrary-size heterogeneous graphs. \n\nWe demonstrate the effectiveness and efficiency of the proposed \\model\\ on the Web-scale Open Academic Graph comprised of 179 million nodes and 2 billion edges spanning from 1900 to 2019, making this the largest-scale\nand longest-spanning representation learning yet performed on heterogeneous graphs. \nAdditionally, we also examine it on domain-specific graphs: the computer science and medicine academic graphs. \nExperimental results suggest that \\short\\ can significantly improve various downstream tasks over state-of-the-art GNNs as well as dedicated heterogeneous models by 9--21$\\%$. \nWe further conduct case studies to show the proposed method can indeed  automatically capture the importance of implicit meta paths for different tasks. \n\n \n\n\\begin{comment}\n\\yd{To Ziniu:  1 how it differs from and better than HAN/RGCN; 2; why better than meta-path based method; 3 why three matrices are better than one}\n\\zn{(1) For heterogeneity, they keep different parameters for each <node, edge, node> pair. However, the number of different relation can vary greatly. For some rare relation, it's not sufficient to update the parameters. On the other hand, there exist some similarity between different relationship. For example, for the first-author link and last-author link obviously share some common pattern. Therefore, is both necessary and possible to decompose the pairwise weights and have parameter sharing. In view of this, we propose to model the interaction by three matrix, source node weight, edge weight, target node weight. The edge weight is relatively smaller compare to node weights. Thus, we can capture both the common and specific patterns of different relationships, using less or equal memory consumption; Also, using three matrices to define interaction and message passing is more natural with heterogeneous graph structure. We use target node's corresponding type matrix to calculate key, use source node's type matrix to calculate query, and edge type matrix serves as an interaction component. Such model better fits the graph structure. For time, these two methods don't capture temporal dependency. \n(2) GNN approaches by its natural can incorporate high-order neighbor information. As long as we can keep the node and edge property, the meta-path information has already been included in through the message passing, and the attention mechanism can even automatically learn the important path for each tasks, without the requirement to explicit defining it.\n(3) [1] Weight sharing. Through decomposition, edge weight is smaller than node weight, and thus even some rare edge can be updated fast and robustly. [2] The original transformer already differentiate the Key matrix, Query matrix and Value matrix. Obviously we cannot keep a brand new matrix for each triplet pair. Such decomposition can also been seen as a trade-off between performance and memory.\n}\n\nMany real-world systems can be modelled as interconnected graphs with multiple node and link types. For example, in a healthcare graph, nodes can be categorized into patients, doctors, diseases, medicines, hospitals, etc. Such kind of graph, known as Heterogeneous Graph has attracted many research interests in recent years. \n\nRecently, researchers have adopted deep learning into graph domain and proposed various Graph Neural Network architectures (GNNs)~\\cite{gcn, graphsage, DBLP:conf/iclr/VelickovicCCRLB18}. to achieve competitive performance in various graph-based applications, such as semi-supervised node classification~\\cite{gcn}, recommendation systems~\\cite{DBLP:conf/kdd/YingHCEHL18} and knowledge graphs~\\cite{DBLP:conf/esws/SchlichtkrullKB18}.\nSeeing the success of GNN for modelling graph in various domains, one natural idea is that whether we can adopt it to model heterogeneous graphs. However, most existing GNN base on the assumption that nodes in the graphs are within same distribution, so that all node pairs can interact with same procedure, while the key property of heterogeneous graphs is that different type of nodes can have totally different semantic meaning, and thus the way to represent them should be different. For example, to represent a patient, we may focus on his/her personal health profile, which is normally a table data, while text description can be significant to represent a medicine. These representations are always not in same space, not necessarily have same dimension, and not generated from same model. In addition, different type of links also have different meaning. For example, the doctor-patient link is obviously different from medicine-disease link. The different semantic meaning of node type and link type hinder the direct usage of GNN into heterogeneous graphs. Although there are several works try to use GNN to model heterogeneous graphs, they simply assume that different nodes have same feature space and within same distribution, which doesn't fully utilize the rich semantic of the heterogeneity. \n\nMoreover, real-world graphs can be dynamic, with new nodes and edges adding or removing. Considering the dynamic of graphs, even a same node will have different semantic meaning in different timestamp. For example, WWW held in 1994 concentrates on the Internet architecture and protocol, while the same conference in 2019 also focus on other interesting topics as graph mining and social science. Therefore, to accurately model real-world graphs, the designed GNNs should also have the capacity to maintain temporal information and dependency.\n\nIn view of the above demands, a desired GNN model should consider both heterogeneity and dynamic of real-world graphs. Therefore, in this paper, we propose a novel GNN architecture, Heterogeneous Graph Transformer (HGT), for modeling web-scale heterogeneous and dynamic graphs. The basic architecture is built based on adopting the transformer into heterogeneous graph, where two connected nodes can interact and propagate information with carefully decoupled weights, so that the model can learn to capture graph patterns for both heterogeneity and dynamic.\n\nFor graph heterogeneity, We use the meta relation triplet of graph schema to define interaction and transform matrices, so that the model can capture both the common and specific patterns of different relationships using equal or even smaller parameters. In this way, the nodes in different types are allowed to maintain in their specific representation distribution, while connected nodes in different types can still interact and transform messages without being restricted by distribution gap.\n\nFor the graph dynamic without losing structure information, we first define a dynamic timestamp schema, under which the graph reserves all the edges happening in different times. Then we propose a relative temporal encoding to incorporate temporal information into HGT using limited computational resources. By end-to-end training, the model can automatically learn the temporal dependency and differences, \n\nIn addition to the model architecture, training GNN for large-scale graph is also a significant challenge. The vanilla GCN has to calculate all the node representation in the graph, which is infeasible for web-scale graph data. To alleviate with it, researchers have proposed different sampling methods to conduct mini-batch training~\\cite{graphsage,fastgcn,DBLP:conf/icml/ChenZS18,ladies}. However, all these sampling methods are designed for homogeneous graphs, and there has not been any work to conduct large-scale training for heterogeneous graphs. We therefore design a heterogeneous layer-dependent sub-graph sampling algorithm, with which the model can train on arbitrary large graphs and can achieve good performance. \n\n\\end{comment}\n\\section{Preliminaries and Related Work}\\label{sec:problem} \\input{section/problem}\n\n\\section{Heterogeneous Graph Transformer}\\label{sec:approach}\n\\begin{figure*}[ht!]\n    \\centering\n    \\includegraphics[width=0.98\\textwidth, trim = 10 0 10 0, clip\n    ]{picture/attention4.png}\n    \\caption{The Overall Architecture of Heterogeneous Graph Transformer. \n    \\textmd{ Given a sampled heterogeneous sub-graph with $t$ as the target node, $s_1$ \\& $s_2$ as source nodes, the \\short\\ model takes its edges $e_1=(s_1, t)$ \\& $e_2=(s_2, t)$ and their corresponding meta relations $<\\tau(s_1), \\phi(e_1), \\tau(t)>$ \\& $<\\tau(s_2), \\phi(e_2), \\tau(t)>$ as input to learn a  contextualized representation $H^{(L)}$ for each node, which can be used for downstream  tasks. \n    Color decodes the node type. \n    HGT includes three components: (1) meta relation-aware heterogeneous mutual attention,  (2) heterogeneous message passing from source nodes, and (3) target-specific heterogeneous message aggregation.}}\n    \\label{fig:my_label}\n\\end{figure*} \n\nIn this section, we present the \\model\\ (\\short). % for modeling heterogeneous graphs with network dynamics. \nIts  idea is to use the \\textbf{meta relations} of heterogeneous graphs to parameterize weight matrices for the heterogeneous mutual attention, message passing, and propagation steps. \nTo further incorporate network dynamics, we introduce a relative temporal encoding mechanism into the model.\n\n\\subsection{Overall \\short\\ Architecture}\n\nFigure~\\ref{fig:my_label} shows the overall architecture of \\model. Given a sampled heterogeneous sub-graph (Cf. Section \\ref{sec:train}), \\short\\ extracts all linked node pairs, where target node $t$ is linked by source node $s$ via edge $e$. The goal of \\short\\ is to aggregate information from source nodes to get a contextualized representation for target node $t$. Such process can be decomposed into three components: \\textit{Heterogeneous Mutual Attention}, \\textit{Heterogeneous Message Passing} and \\textit{Target-Specific Aggregation}. \n\nWe denote the output of the $(l)$-th \\short\\ layer as $H^{(l)}$, which is also the input of the $(l$+$1)$-th layer. \nBy stacking $L$ layers, we can get the node representations of the whole graph $H^{(L)}$, which can be used for end-to-end training or fed into downstream tasks.  \n\n\\hide{\n\\subsection{Relative Temporal Encoding}\n\nTo incorporate temporal information into the model, one naive way is to construct a separate graph for each time slot. However, such a procedure may lose a large portion of structural information across different time slots. \nMeanwhile, the representation of a node at time $t$ might rely on edges that happened at other time slots. \nTherefore, %we claim that \na proper way to model dynamic graphs is to maintain all the edges happening at different times and allow nodes and edges with different timestamps to interact with each other.\n\nIn light of this, we present the Relative Temporal Encoding (RTE) mechanism to model the dynamic dependencies in heterogeneous graphs. \nRTE is inspired by Transformer's positional encoding method~\\cite{DBLP:conf/nips/VaswaniSPUJGKP17, DBLP:conf/naacl/ShawUV18}, which has been shown successful to capture the sequential dependency of words in long texts. \n\nGiven a source node $s$ and a target node $t$, along with their corresponding timestamps $T(s)$ and $T(t)$, we denote the relative time gap $\\Delta T(t,s) = T(t) - T(s)$ as an index to get a relative temporal encoding $RTE(\\Delta T(t,s))$. Noted that the training dataset will not cover all possible time gaps, and thus  $RTE$ should be capable of generalizing to unseen time. We thus adopt a fixed set of sinusoid functions as basis, with a tunable linear projection T-Linear\\footnote{For simplicity, we denote a linear projection L $:\\RR^{a}\\rightarrow \\RR^{b}$ as a function to conduct linear transformation to vector $x\\in\\RR^{a}$ as: L$(x)=Wx+b$, where matrix $W\\in\\RR^{a+b}$ and bias $b\\in\\RR^{b}$. $W$ and $b$ are learnable parameters for L.}$: \\RR^{d} \\rightarrow \\RR^{d}$ as $RTE$:\n\\begin{align}\n   Base\\big(\\Delta T(t,s), 2i\\big) & = sin\\Big(\\Delta T_{t,s} / 10000^{\\frac{2i}{d}}\\Big)\\\\ \n   Base\\big(\\Delta T(t,s), 2i+1\\big) & = cos\\Big(\\Delta T_{t,s} / 10000^{\\frac{2i+1}{d}}\\Big)\\\\ \n   RTE\\big(\\Delta T(t,s)\\big) & = \\text{T-Linear}\\Big( Base(\\Delta T_{t,s}) \\Big)\n\\end{align}\nFinally, the temporal encoding relative to the target node $t$ is added to the source node $s$' representation as follows:\n\\begin{align}\n    \\widehat{H}^{(l-1)}[s] = H^{(l-1)}[s] + RTE\\big(\\Delta T(t,s)\\big)\n\\end{align}\nIn this way, the temporal augmented representation $\\widehat{H}^{(l-1)}$ will capture the relative temporal information of source node $s$ and target node $t$. The whole procedure is illustrated in the Figure \\ref{fig:my_label} (1). \n\n}% end of hide\n\n\\subsection{Heterogeneous Mutual Attention}\n\nThe first step is to calculate the mutual attention between source node $s$ and target node $t$. We first give a brief introduction to the general attention-based GNNs as follows: \n\\begin{align}\nH^{l}[t] \\gets \\underset{\\forall s \\in N(t), \\forall e \\in E(s,t)}{\\textbf{Aggregate}}\\Big(  \\textbf{Attention}(s, t) \\cdot \\textbf{Message}(s)\\Big)\n\\end{align}\nwhere there are three basic operators: \\textbf{Attention}, which estimates the importance of each source node; \\textbf{Message}, which extracts the message by using only the source node $s$; and \\textbf{Aggregate}, which aggregates the neighborhood message by the attention weight. \n\nFor example, the Graph Attention Network (GAT)~\\cite{gat} adopts an additive mechanism as \\textbf{Attention}, uses the same weight for calculating \\textbf{Message}, and leverages the simple average followed by a nonlinear activation for the \\textbf{Aggregate} step. \nFormally, GAT has\n\\begin{align}\n    \\textbf{Attention}_{GAT}(s, t) & = \\underset{\\forall s \\in N(t)}{\\text{Softmax}} \\bigg(\\Vec{a} \\Big(WH^{l-1}[t] \\mathbin\\Vert WH^{l-1}[s]\\Big)\\bigg) \\nonumber\\\\\n    \\textbf{Message}_{GAT}(s)  & = WH^{l-1}[s] \\nonumber\\\\ \n    \\textbf{Aggregate}_{GAT}(\\cdot) & = \\sigma \\Big(\\text{Mean}(\\cdot)\\Big) \\nonumber\n\\end{align}\nThough GAT is effective to give high attention values to important nodes, it assumes that $s$ and $t$ have the same feature distributions by using one weight matrix $W$. \nSuch an assumption, as we've discussed in Section~\\ref{sec:introduction}, \nis usually incorrect for heterogeneous graphs, where each type of nodes can have its own feature distribution. \n\nIn view of this limitation, we design the \\textbf{Heterogeneous Mutual Attention} mechanism. \nGiven a target node $t$, and all its neighbors $s \\in N(t)$, which might belong to different distributions, we want to calculate their mutual attention grounded by their \\textbf{meta relations}, i.e., the $\\langle \\tau(s), \\phi(e), \\tau(t) \\rangle$ triplets. \n\nInspired by the architecture design of Transformer~\\cite{DBLP:conf/nips/VaswaniSPUJGKP17}, we  map target node $t$ into a Query vector, and source node $s$ into a Key vector, and calculate their dot product as attention. The key difference is that the vanilla Transformer uses a single set of projections for all words, while in our case each meta relation should have a distinct set of projection weights. To maximize parameter sharing while still maintaining the specific characteristics of different relations, we propose to parameterize the weight matrices of the interaction operators into a source node projection, an edge projection, and a target node projection. Specifically, we calculate the $h$-head {attention} for each edge $e=(s,t)$ (See Figure \\ref{fig:my_label} (1)) by:\n\\begin{align}\n\\label{eq:hgt-att}\n\\textbf{Attention}_{HGT}(s,e,t)&  = \\underset{\\forall s \\in N(t)}{\\text{Softmax}}\\Big(\\underset{i \\in [1,h]}{\\mathlarger{\\mathbin\\Vert}}ATT\\text{-}head^{i}(s,e,t)\\Big)\\\\\nATT\\text{-}head^{i}(s,e,t)&  = \\Big(K^i(s)\\ W^{ATT}_{\\phi(e)}\\ Q^i(t)^T\\Big) \\cdot \\frac{{\\mu}_{\\langle \\tau(s), \\phi(e), \\tau(t) \\rangle}}{\\sqrt{d}} \\nonumber\\\\\nK^i(s)&  = \\text{K-Linear}^i_{\\tau(s)}\\Big({H}^{(l-1)}[s]\\Big) \\nonumber\\\\\nQ^i(t)&  = \\text{Q-Linear}^i_{\\tau(t)}\\Big(H^{(l-1)}[t]\\Big) \\nonumber\n\\end{align}\nFirst, for the $i$-th attention head $ATT\\text{-}head^{i}(s,e,t)$, we project the $\\tau(s)$-type source node $s$ into the $i$-th \\textit{Key} vector $K^i(s)$ with a linear projection K-Linear$^i_{\\tau(s)}: \\RR^{d} \\rightarrow \\RR^{\\frac{d}{h}}$, where $h$ is the number of attention heads and $\\frac{d}{h}$ is the vector dimension per head. \nNote that K-Linear$^i_{\\tau(s)}$ is indexed by the source node $s$'s type $\\tau(s)$, meaning that each type of nodes has a unique linear projection to maximally model the distribution differences. \nSimilarly, we also project the target node $t$ with a linear projection Q-Linear$^i_{\\tau(t)}$ into the $i-$th Query vector. \n\nNext, we need to calculate the similarity between the Query vector $Q^i(t)$ and Key vector $K^i(s)$. \nOne unique characteristic of heterogeneous graphs is that there may exist different edge types (relations) between a node type pair, e.g., $\\tau(s)$ and $\\tau(t)$. \nTherefore, unlike the vanilla Transformer that directly calculates the dot product between the Query and Key vectors, we keep a distinct edge-based matrix $W^{ATT}_{\\phi(e)}\\in\\RR^{\\frac{d}{h}\\times\\frac{d}{h}}$ for each edge type $\\phi(e)$. In doing so, the model can capture different semantic relations even between the same node type pairs. \nMoreover, since not all the relationships contribute equally to the target nodes, \nwe add a prior tensor $\\mu \\in \\RR^{|\\cA|\\times|\\cR|\\times|\\cA|}$ to denote the general significance of each meta relation triplet, serving as an adaptive scaling to the attention. \n\nFinally, we concatenate $h$ attention heads together to get the attention vector for each node pair. \nThen, for each target node $t$, we gather all attention vectors from its neighbors $N(t)$ and conduct softmax, making it fulfill $\\sum_{\\forall s \\in N(t)}\\textbf{Attention}_{HGT}(s,e,t) = \\mathbf{1}_{h \\times 1}$.\n\n\\subsection{Heterogeneous Message Passing}\nParallel to the calculation of mutual attention, we pass information from source nodes to target nodes (See Figure \\ref{fig:my_label} (2)). \nSimilar to the attention process, we would like to incorporate the meta relations of edges into the message passing process to alleviate the distribution differences of nodes and edges of different types. For a pair of nodes $e =(s,t)$, we calculate its multi-head \\textbf{Message} by:\n\\begin{align}\n\\textbf{Message}_{HGT}(s,e,t)&  = \\underset{i \\in [1,h]}{\\mathlarger{\\mathbin\\Vert}}MSG\\text{-}head^{i}(s,e, t)\\\\\nMSG\\text{-}head^{i}(s,e, t)&  = \\text{M-Linear}^i_{\\tau(s)}\\Big({H}^{(l-1)}[s]\\Big) \\ W^{MSG}_{\\phi(e)} \\nonumber\n\\end{align}\nTo get the $i$-th message head $MSG\\text{-}head^{i}(s,e,t)$, we first project the $\\tau(s)$-type source node $s$ into the $i$-th message vector with a linear projection M-Linear$^i_{\\tau(s)}: \\RR^{d} \\rightarrow \\RR^{\\frac{d}{h}}$. \nIt is then followed by a matrix $W^{MSG}_{\\phi(e)}\\in\\RR^{\\frac{d}{h}\\times\\frac{d}{h}}$ for incorporating the edge dependency. \nThe final step is to concat all $h$ message heads to get the  $\\textbf{Message}_{HGT}(s,e, t)$ for each node pair.\n\n\\subsection{Target-Specific Aggregation}\nWith the heterogeneous multi-head attention and message calculated, we need to aggregate them from the source nodes to the target node (See Figure \\ref{fig:my_label} (3)). \nNote that the softmax procedure in Eq. \\ref{eq:hgt-att} has made the sum of each target node $t$'s attention vectors to one, we can thus simply use the attention vector as the weight to average the corresponding messages from the source nodes and get the updated vector $\\widetilde{H}^{(l)}[t]$ as:\n\\begin{align}\n\\widetilde{H}^{(l)}[t] &= \\underset{\\forall s \\in N(t)}{\\mathlarger{\\oplus}}\\Big(\\textbf{Attention}_{HGT}(s, e, t) \\cdot \\textbf{Message}_{HGT}(s, e, t)\\Big). \\nonumber\n\\end{align}\nThis aggregates information to the target node $t$ from all its neighbors (source nodes) of different feature distributions. \n\nThe final step is to map target node $t$'s vector back to its type-specific distribution, indexed by its node type $\\tau(t)$. \nTo do so, we apply a linear projection A-Linear$_{\\tau(t)}$ to the updated vector $\\widetilde{H}^{(l)}[t]$, followed by residual connection~\\cite{DBLP:conf/cvpr/HeZRS16} as:\n\\begin{align}\n&H^{(l)}[t] = % \\underset{\\forall v: \\tau(v) = \\tau(t)}{\\text{BatchNorm}}\\bigg(\n\\text{A-Linear}_{\\tau(t)}\\Big(\\sigma\\big(\\widetilde{H}^{(l)}[t]\\big)\\Big) + H^{(l-1)}[t]. \\label{eq:output} \n\\end{align}\nIn this way, we get the $l$-th \\short\\ layer's output $H^{(l)}[t]$ for the target node $t$. \nDue to the ``small-world'' property of real-world graphs, stacking the \\short\\ blocks for $L$ layers ($L$ being a small value) can enable each node reaching  a large proportion of nodes---with different types and relations---in the full graph. \nThat is, \\short\\ generates a highly contextualized representation $H^{(L)}$ for each node, which can be fed into any models to conduct downstream heterogeneous network tasks, such as node classification and link prediction. \n\nThrough the whole model architecture, we highly rely on using the \\textbf{meta relation}---$\\langle \\tau(s), \\phi(e), \\tau(t) \\rangle$---to parameterize the weight matrices separately. \nThis can be interpreted as a trade-off between the model capacity and efficiency. Compared with the vanilla Transformer, our model distinguishes the operators for different relations and thus is more capable to handle the distribution differences in heterogeneous graphs. \nCompared with existing models that keep a distinct matrix for each meta relation as a whole, \\short's triplet parameterization can better leverage the heterogeneous graph schema to achieve parameter sharing. \nOn one hand, relations with few occurrences can benefit from such parameter sharing for fast adaptation and generalization. \nOn the other hand, different relationships' operators can still maintain their specific characteristics by using a much smaller parameter set. \n\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=0.47\\textwidth, trim = 10 0 10 0, clip\n    ]{picture/rte.png}\n    \\caption{Relative Temporal Encoding (RTE) to model graph dynamic. \\textmd{Nodes  are associated with timestamps $T(\\cdot)$. After the RTE process, the temporal augmented representations are fed to the \\short\\ model.}}\n    \\label{fig:rte}\n\\end{figure} \n\n\\subsection{Relative Temporal Encoding}\nBy far, we present \\short---a graph neural network for modeling heterogeneous graphs. \nNext, we introduce the Relative Temporal Encoding (RTE) technique for \\short\\ to handle graph dynamic. \n\nThe traditional way to incorporate temporal information is to construct a separate graph for each time slot. \nHowever, such a procedure may lose a large portion of structural dependencies across different time slots. \nMeanwhile, the representation of a node at time $t$ might rely on edges that happen at other time slots. \nTherefore, %we claim that \na proper way to model dynamic graphs is to maintain all the edges happening at different times and allow nodes and edges with different timestamps to interact with each other.\n\nIn light of this, we propose the Relative Temporal Encoding (RTE) mechanism to model the dynamic dependencies in heterogeneous graphs. \nRTE is inspired by Transformer's positional encoding method~\\cite{DBLP:conf/nips/VaswaniSPUJGKP17, DBLP:conf/naacl/ShawUV18}, which has been shown successful to capture the sequential dependencies of words in long texts. \n\nSpecifically, given a source node $s$ and a target node $t$, along with their corresponding timestamps $T(s)$ and $T(t)$, we denote the relative time gap $\\Delta T(t,s) = T(t) - T(s)$ as an index to get a relative temporal encoding $RTE(\\Delta T(t,s))$. \nNoted that the training dataset will not cover all possible time gaps, and thus  $RTE$ should be capable of generalizing to unseen times and time gaps. \nTherefore, we adopt a fixed set of sinusoid functions as basis, with a tunable linear projection T-Linear\\footnote{For simplicity, we denote a linear projection L $:\\RR^{a}\\rightarrow \\RR^{b}$ as a function to conduct linear transformation to vector $x\\in\\RR^{a}$ as: L$(x)=Wx+b$, where matrix $W\\in\\RR^{a+b}$ and bias $b\\in\\RR^{b}$. $W$ and $b$ are learnable parameters for L.}$: \\RR^{d} \\rightarrow \\RR^{d}$ as $RTE$:\n\\begin{align}\n   Base\\big(\\Delta T(t,s), 2i\\big) & = sin\\Big(\\Delta T_{t,s} / 10000^{\\frac{2i}{d}}\\Big)\\\\ \n   Base\\big(\\Delta T(t,s), 2i+1\\big) & = cos\\Big(\\Delta T_{t,s} / 10000^{\\frac{2i+1}{d}}\\Big)\\\\ \n   RTE\\big(\\Delta T(t,s)\\big) & = \\text{T-Linear}\\Big( Base(\\Delta T_{t,s}) \\Big)\n\\end{align}\nFinally, the temporal encoding relative to the target node $t$ is added to the source node $s$' representation as follows:\n\\begin{align}\n    \\widehat{H}^{(l-1)}[s] = H^{(l-1)}[s] + RTE\\big(\\Delta T(t,s)\\big)\n\\end{align}\nIn this way, the temporal augmented representation $\\widehat{H}^{(l-1)}$ will capture the relative temporal information of source node $s$ and target node $t$. The RTE procedure is illustrated in the Figure \\ref{fig:rte}. \n\n\\hide{\n\\begin{figure*}[ht!]\n    \\centering\n    \\includegraphics[width=1.02\\textwidth, trim = 10 0 150 0, clip]{picture/attention.png}\n    \\caption{Overall Architecture of Heterogeneous Graph Transformer. Color denotes the representation or weights from a specific type of node, where blue means Paper, red means Author and orange means Venue. Given a pair of nodes, HGT (1) incorporates temporal information via relative temporal encoding; (2) calculates their mutual attention via meta relation defined weights; (3) calculates message from source side; (4) aggregates heterogeneous messages at target side.}\n    \\label{fig:my_label}\n\\end{figure*} \n\nIn this section, we present the \\model\\ (\\short) architecture for modeling heterogeneous graphs with network dynamics. Its core idea is to use \\textbf{meta relation} to parametrize weight matrices for the heterogeneous mutual attention, message passing, and propagation. \nTo incorporate network dynamics, we introduce a relative temporal encoding mechanism into the model. \n\n\\subsection{Overall \\short\\ Architecture}\n\nFigure~\\ref{fig:my_label} shows the overall architecture of \\model. Given a sampled heterogeneous graph (Cf. Section \\ref{sec:train}), \\short\\ extracts all linked node pairs, where target node $t$ is linked by source node $s$ via edge $e$. The goal of \\short\\ is to aggregates information from $s$ to get a contextualized representation for target node $t$. Such process can be decomposed into four components: \\textit{Relative Temporal Encoding, Heterogeneous Mutual Attention}, \\textit{Heterogeneous Message Passing} and \\textit{Target-Specific Aggregation}. \n\nWe denote the output of the $(l)$-th \\short\\ layer is $H^{(l)}$, which is also the input of the $(l$+$1)$-th layer. \nBy stacking $L$ layers, we can get the node representations of the whole graph $H^{(L)}$, which can be used for end-to-end training or fed into downstream tasks.  \n\n\\subsection{Relative Temporal Encoding}\n\nTo incorporate temporal information into the model, one naive way is to construct a separate graph for each time slot. However, such a procedure may lose a large portion of structural information across different time slots. \nMeanwhile, the representation of a node at time $t$ might rely on edges that happened at other time slots. \nTherefore, %we claim that \na proper way to model dynamic graphs is to maintain all the edges happening at different times and allow nodes and edges with different timestamps to interact with each other.\n\nIn light of this, we present the Relative Temporal Encoding (RTE) mechanism to model the dynamic dependencies in heterogeneous graphs. \nRTE is inspired by Transformer's positional encoding method~\\cite{DBLP:conf/nips/VaswaniSPUJGKP17, DBLP:conf/naacl/ShawUV18}, which has been shown successful to capture the sequential dependency of words in long texts. \n\nGiven a source node $s$ and a target node $t$, along with their corresponding timestamps $T(s)$ and $T(t)$, we denote the relative time gap $\\Delta T(t,s) = T(t) - T(s)$ as an index to get a relative temporal encoding $RTE(\\Delta T(t,s))$. Noted that the training dataset will not cover all possible time gaps, and thus  $RTE$ should be capable of generalizing to unseen time. We thus adopt a fixed set of sinusoid functions as basis, with a tunable linear projection T-Linear\\footnote{For simplicity, we denote a linear projection L $:\\RR^{a}\\rightarrow \\RR^{b}$ as a function to conduct linear transformation to vector $x\\in\\RR^{a}$ as: L$(x)=Wx+b$, where matrix $W\\in\\RR^{a+b}$ and bias $b\\in\\RR^{b}$. $W$ and $b$ are learnable parameters for L.}$: \\RR^{d} \\rightarrow \\RR^{d}$ as $RTE$:\n\\begin{align}\n   Base\\big(\\Delta T(t,s), 2i\\big) & = sin\\Big(\\Delta T_{t,s} / 10000^{\\frac{2i}{d}}\\Big)\\\\ \n   Base\\big(\\Delta T(t,s), 2i+1\\big) & = cos\\Big(\\Delta T_{t,s} / 10000^{\\frac{2i+1}{d}}\\Big)\\\\ \n   RTE\\big(\\Delta T(t,s)\\big) & = \\text{T-Linear}\\Big( Base(\\Delta T_{t,s}) \\Big)\n\\end{align}\nFinally, the temporal encoding relative to the target node $t$ is added to the source node $s$' representation as follows:\n\\begin{align}\n    \\widehat{H}^{(l-1)}[s] = H^{(l-1)}[s] + RTE\\big(\\Delta T(t,s)\\big)\n\\end{align}\nIn this way, the temporal augmented representation $\\widehat{H}^{(l-1)}$ will capture the relative temporal information of source node $s$ and target node $t$. The whole procedure is illustrated in the Figure \\ref{fig:my_label} (1). \n\n\\subsection{Heterogeneous Mutual Attention}\n\nThe second step is to calculate mutual attention between source node $s$ and target node $t$. We first give a brief introduction to the general attention-based GNNs as follow: \n\\begin{align}\nH^{l}[t] \\gets \\underset{\\forall s \\in N(t), \\forall e \\in E(s,t)}{\\textbf{Aggregate}}\\Big(  \\textbf{Attention}(s, t) \\cdot \\textbf{Message}(s)\\Big)\n\\end{align}\nCompared to the general GNN Framework, it implements the neighbor information extractor \\textbf{Extract} by two major components: \\textbf{Attention}, which estimates the importance of each source node, and \\textbf{Message}, which extracts the message by using only the source node $s$.\n\nFor example, the Graph Attention Network (GAT)~\\cite{DBLP:conf/iclr/VelickovicCCRLB18} adopts an additive mechanism as \\textbf{Attention}, uses the same weight for calculating \\textbf{Message}, and leverages the simple average followed by a nonlinear activation for the \\textbf{Aggregate} step. \nFormally, GAT has\n\\begin{align}\n    \\textbf{Attention}_{GAT}(s, t) & = \\underset{\\forall s \\in N(t)}{\\text{Softmax}} \\bigg(A \\Big(WH^{l-1}[t] \\mathbin\\Vert WH^{l-1}[s]\\Big)\\bigg) \\nonumber\\\\\n    \\textbf{Message}_{GAT}(s)  & = WH^{l-1}[s] \\nonumber\\\\ \n    \\textbf{Aggregate}_{GAT}(\\cdot) & = \\sigma \\Big(\\text{Mean}(\\cdot)\\Big) \\nonumber\n\\end{align}\nThough GAT is effective to give high attention values to important nodes, it assumes that $s$ and $t$ have the same feature distributions by using one weight matrix $W$. \nSuch an assumption, as we've discussed in Section~\\ref{sec:introduction}, \nis usually incorrect for heterogeneous graphs, where each type of nodes can have its own feature distribution. \n\nIn view of this limitation, we design a \\textbf{Heterogeneous Mutual Attention} mechanism. \nGiven a target node $t$, and all its neighbors $s \\in N(t)$, which might belong to different distributions, we want to calculate their mutual attention grounded by \\textbf{meta relation}, i.e., the $\\langle \\tau(s), \\phi(e), \\tau(t) \\rangle$ triplet. \n\nInspired by the architecture design of Transformer, we also map target node $t$ into a Query vector, and source node $s$ into a Key vector, and calculate their dot product as attention. The key difference is that the vanilla Transformer uses a single set of projection for all words, while in our case each meta relation should have a distinct set of projection weights. To maximize parameter sharing while still maintaining the specific characteristics of different relations, we propose to parametrize the weight matrices of interaction operator into a source node projection, an edge projection and a target node projection. Specifically, we calculate the $h$-head {Attention} from each source node $s$ to $t$ (See Figure \\ref{fig:my_label} (2)) by:\n\\begin{align}\n\\label{eq:hgt-att}\n\\textbf{Attention}_{HGT}(s,e,t)&  = \\underset{\\forall s \\in N(t)}{\\text{Softmax}}\\Big(\\underset{i \\in [1,h]}{\\mathlarger{\\mathbin\\Vert}}ATT\\text{-}head^{i}(s,e,t)\\Big)\\\\\nATT\\text{-}head^{i}(s,e,t)&  = \\Big(K^i(s)\\ W^{ATT}_{\\phi(e)}\\ Q^i(t)^T\\Big) \\cdot \\frac{{\\mu}_{\\langle \\tau(s), \\phi(e), \\tau(t) \\rangle}}{\\sqrt{d}} \\nonumber\\\\\nK^i(s)&  = \\text{K-Linear}^i_{\\tau(s)}\\Big(\\widehat{H}^{(l-1)}[s]\\Big) \\nonumber\\\\\nQ^i(t)&  = \\text{Q-Linear}^i_{\\tau(t)}\\Big(H^{(l-1)}[t]\\Big) \\nonumber\n\\end{align}\nFirst, for the $i$-th attention head $ATT\\text{-}head^{i}(s,e,t)$, we project the source node $s$ (node type $\\tau(s)$) into the $i$-th \\textit{Key} vector $K^i(s)$ with a linear projection K-Linear$^i_{\\tau(s)}: \\RR^{d} \\rightarrow \\RR^{\\frac{d}{h}}$ , where $h$ is the number of attention heads and $\\frac{d}{h}$ is the vector dimension per head. \nNote that K-Linear$^i_{\\tau(s)}$ is indexed by the source node $s$'s type $\\tau(s)$, meaning that each type of nodes has a unique linear projection to maximally model the distribution differences. \nSimilarly, we also project the target node $t$ with a linear projection Q-Linear$^i_{\\tau(t)}$ into the $i-$th Query vector. \n\nNext, we need to calculate the similarity between the Query vector $Q^i(t)$ and Key vector $K^i(s)$. \nOne unique characteristic of heterogeneous graphs is that there may have different edge types (relations) between a node type pair (e.g., $\\tau(s)$ and $\\tau(t)$). \nTherefore, unlike the vanilla Transformer that directly calculates the dot product between the Query and Key vectors, we keep a distinct edge-based matrix $W^{ATT}_{\\phi(e)}\\in\\RR^{\\frac{d}{h}\\times\\frac{d}{h}}$ for each edge type $\\phi(e)$. In doing so, the model can capture different semantic relations even between the same node type pairs. \nMoreover, since not all the relationships contribute equally to the target nodes, \nwe add a prior tensor $\\mu \\in \\RR^{|\\cA|\\times|\\cR|\\times|\\cA|}$ to denote the general significance of each meta relation triplet, serving as an adaptive scaling to the attention. \n\nFinally, we concatenate $h$ attention heads together to get the attention vector for each node pair. \nThen, for each target node $t$, we gather all attention vectors from its neighbors $N(t)$ and conduct softmax, making it fulfill $\\sum_{\\forall s \\in N(t)}\\textbf{Attention}_{HGT}(s,e,t) = \\mathbf{1}_{h \\times 1}$.\n\n\\subsection{Heterogeneous Message Passing}\nParallel to the calculation of mutual attention, we pass information from source nodes to target nodes (See Figure \\ref{fig:my_label} (3)). \nSimilar to the attention process, we would like to incorporate meta relation into the message passing process to alleviate the distribution differences of nodes and edges of different types. For a pair of nodes $e =(s,t)$, we calculate its multi-head \\textbf{Message} by:\n\\begin{align}\n\\textbf{Message}_{HGT}(s,e,t)&  = \\underset{i \\in [1,h]}{\\mathlarger{\\mathbin\\Vert}}MSG\\text{-}head^{i}(s,e, t)\\\\\nMSG\\text{-}head^{i}(s,e, t)&  = \\text{M-Linear}^i_{\\tau(s)}\\Big(\\widehat{H}^{(l-1)}[s]\\Big) \\ W^{MSG}_{\\phi(e)} \\nonumber\n\\end{align}\nTo get the $i$-th message head $MSG\\text{-}head^{i}(s,e,t)$, we first project the source node $s$ of the node type $\\tau(s)$ into the $i$-th message vector with a linear projection M-Linear$^i_{\\tau(s)}: \\RR^{d} \\rightarrow \\RR^{\\frac{d}{h}}$. \nIt is then followed by a matrix $W^{MSG}_{\\phi(e)}\\in\\RR^{\\frac{d}{h}\\times\\frac{d}{h}}$ for incorporating the edge dependency. \nThe final step is to concat all $h$ message heads to get the  $\\textbf{Message}_{HGT}(s,e, t)$ for each node pair.\n\n\\subsection{Target-Specific Aggregation}\nWith the heterogeneous multi-head attention and message calculated, we need to aggregate them from the source nodes to the target node (See Figure \\ref{fig:my_label} (4)). \nNote that the softmax procedure in Eq. \\ref{eq:hgt-att} has made the sum of each target node $t$'s attention vectors to one, we can thus simply use the attention vector as weight to average the corresponding messages from the source nodes and get the updated vector $\\widetilde{H}^{(l)}[t]$ as:\n\\begin{align}\n\\label{eq:agg}\n\\widetilde{H}^{(l)}[t] &= \\underset{\\forall s \\in N(t)}{\\mathlarger{\\oplus}}\\Big(\\textbf{Attention}_{HGT}(s, e, t) \\cdot \\textbf{Message}_{HGT}(s, e, t)\\Big)\n\\end{align}\n\nEq. \\ref{eq:agg} aggregates information to the target node $t$ from all its neighbors (source nodes) of different feature distributions. \n\nThe final step is to map $t$'s vector back to its type-specific distribution, indexed by its node type $\\tau(t)$. \nTo do so, we apply a linear projection A-Linear$_{\\tau(t)}$ to the updated vector $\\widetilde{H}^{(l)}[t]$, followed by a non-linear activation (Eq. \\ref{eq:output}). Specifically, we have: \n\\begin{align}\n&H^{(l)}[t] =  \\sigma\\Big(\\text{A-Linear}_{\\tau(t)}\\widetilde{H}^{(l)}[t]\\Big)\n + H^{(l-1)}[t] \\label{eq:output}\n\\end{align}\n\nIn this way, we get the $l$-th \\short\\ layer's output $H^{(l)}[t]$ for the target node $t$. \nDue to the ``small-world'' property of real-world graphs, stacking the \\short\\ blocks for multiple layers can enable each node reaching to a large proportion of nodes---with different types and relations---in the full graph. \nIn other words, \\short\\ generates a highly contextualized representation $H^{(L)}$ for each node, which can be fed into any models to conduct downstream heterogeneous network tasks, such as node classification and link prediction. \n\nThrough the whole model architecture, we highly rely on using the \\textbf{meta relation}---$\\langle \\tau(s), \\phi(e), \\tau(t) \\rangle$---to parametrize weight matrices. This can be interpreted as a tradeoff between model capacity and efficiency. Compared with the vanilla Transformer, our model distinguishes the operators for different relations and thus is more capable to handle the distribution differences in heterogeneous graphs. \nCompared with existing models that keep a distinct matrix for each relation, \\short's triple parameterization can better leverage the heterogeneous graph schema to achieve parameter sharing. On one hand, relations with few occurrences can benefit from such parameter sharing for fast adaptation and generalization. One the other hand, different relationships' operators can still maintain their specific characteristics, using much smaller parameters. \n\n}%end of hide (www submission original)\n\n \n\n\\section{Web-scale \\short\\ Training}\\label{sec:train}\n\n \\begin{figure*}[ht!]\n    \\centering\n    \\includegraphics[width=1.0\\textwidth]{picture/sample.png}\n    \\caption{\\sampling\\ with Inductive Timestamp Assignment.}\n    \\label{fig:sample}\n\\end{figure*} \n\nIn this section, we present \\short's strategies for training Web-scale heterogeneous graphs with dynamic information, including an efficient Heterogeneous Mini-Batch Graph Sampling algorithm---\\sampling---and an inductive timestamp assignment method. % for keeping heterogeneous graphs' temporal information. \n\n\\subsection{\\sampling}\n\nThe full-batch GNN~\\cite{gcn} training requires the calculation of all node representations per layer, \nmaking it not scalable for Web-scale graphs. \nTo address this issue, different sampling-based methods~\\cite{graphsage,fastgcn,DBLP:conf/icml/ChenZS18,ladies} have been proposed to train GNNs on a subset of nodes. \nHowever, directly using them for heterogeneous graphs is prone to get sub-graphs that are extremely imbalanced regarding different node types, due to that the degree distribution and the total number of nodes for each type can vary dramatically. \n\nTo address this issue, we propose an efficient Heterogeneous Mini-Batch Graph Sampling algorithm---\\sampling---to enable both \\short\\ and traditional GNNs to handle Web-scale heterogeneous graphs. \n\\sampling\\ is able to 1) keep a similar number of nodes and edges for each type and 2) keep the sampled sub-graph dense to minimize the information loss and reduce the sample variance. \n\nAlgorithm \\ref{alg:sample} outlines the \\sampling\\ algorithm. \nIts basic idea is to keep a separate node budget $B[\\tau]$ for each node type $\\tau$ and to sample an equal number of nodes per type with an importance sampling strategy to reduce variance. \nGiven node $t$ already sampled, we add all its direct neighbors into the corresponding budget with Algorithm~\\ref{alg:budget}, and add $t$'s normalized degree to these neighbors in line~\\ref{line:update}, which will then be used to calculate the sampling probability. \nSuch normalization is equivalent to accumulate the random walk probability of each sampled node to its neighborhood, avoiding the sampling being dominated by high-degree nodes. \nIntuitively, the higher such value is, the more a candidate node is correlated with the currently sampled nodes, and thus should be given a higher probability to be sampled.\n\nAfter the budget is updated, we then calculate the sampling probability in Algorithm~\\ref{alg:sample} line~\\ref{line:prob}, where we calculate the square of the cumulative normalized degree of each node $s$ in each budget. \nAs proved in~\\cite{ladies}, using such sampling probability can reduce the sampling variance. Then, we sample $n$ nodes in  type $\\tau$ by using the calculated probability, add them into the output node set, update its neighborhood to the budget, and remove it out of the budget in lines~\\ref{line:samp}--\\ref{line:end}. \nRepeating such procedure for $L$ times, we get a sampled sub-graph with $L$ depth from the initial nodes. Finally, we reconstruct the adjacency matrix among the sampled nodes. \nBy using the above algorithm, the sampled sub-graph contains a similar number of nodes per  type (based on the separate node budget), and is sufficiently dense to reduce the sampling variance (based on the normalized degree and importance sampling), making it suitable for training GNNs on Web-scale heterogeneous graphs.\n\n\\begin{algorithm}[tb] \n\\caption{Heterogeneous Mini-Batch Graph Sampling} \n\\label{alg:sample} \n\\begin{algorithmic}[1] \n\\REQUIRE\nAdjacency matrix $A$ for each $\\langle \\tau(s), \\phi(e), \\tau(t) \\rangle$ relation pair; Output node Set $OS$; Sample number $n$ per node type; Sample depth $L$.\\\\\n\\ENSURE\nSampled node set $NS$; Sampled adjacency matrix $\\hat{A}$.\n\\STATE  $NS \\gets OS$ // Initialize sampled node set as output node set.\n\\STATE  Initialize an empty Budget $B$ storing nodes for each node type with normalized degree. \n\\FOR {$t \\in NS$}\n    \\STATE  Add-In-Budget($B$, $t$, $A$, $NS$) // Add neighbors of $t$ to $B$.\n\\ENDFOR\n\\FOR {$l \\gets 1$ to $L$}\n    \\FOR {source node type $\\tau \\in B$}\n        \\FOR {source node $s \\in B[\\tau]$}\n            \\STATE  $\\ prob^{(l-1)}[\\tau][s] \\gets \\frac{B[\\tau][s]^2}{\\|B[\\tau]\\|_2^2}$ // Calculate sampling probability for each source node $s$ of node type $\\tau$. \\label{line:prob}\n        \\ENDFOR\n        \\STATE  Sample $n$ nodes ${\\{t_i\\}}_{i=1}^n$ from $B[\\tau]$ using $prob^{(l-1)}[\\tau]$.\n        \\FOR {$t \\in {\\{t_i\\}}_{i=1}^n$} \\label{line:samp}\n            \\STATE  $OS[\\tau].add(t)$ // Add node $t$ into Output node set.\n            \\STATE  Add-In-Budget($B$, $t$, $A$, $NS$) // Add neighbors of $t$ to $B$.\n            \\STATE  $B[\\tau].pop(t)$ // Remove sampled node $t$ from Budget. \\label{line:end}\n        \\ENDFOR\n    \\ENDFOR\n\\ENDFOR\n\\STATE  Reconstruct the sampled adjacency matrix $\\hat{A}$ among the sampled nodes $OS$ from $A$.\n\\RETURN $OS$ and $\\hat{A}$; \n\\end{algorithmic} \n\\end{algorithm}\n\n\\begin{algorithm}[tb] \n\\caption{Add-In-Budget} \n\\label{alg:budget} \n\\begin{algorithmic}[1] \n\\REQUIRE\nBudget $B$ storing nodes for each type with normalized degree; Added node $t$; Adjacency matrix $A$ for each $\\langle \\tau(s), \\phi(e), \\tau(t) \\rangle$ relation pair; Sampled node set $NS$.\\\\\n\\ENSURE\nUpdated Budget $B$.\n     \\FOR {each possible source node type $\\tau$ and edge type $\\phi$}\n        \\STATE  $\\hat{D}_t \\gets 1 \\ /\\ len\\Big(A_{\\langle \\tau, \\phi, \\tau(t) \\rangle}[t]\\Big)$ // get normalized degree of added node $t$ regarding to $\\langle \\tau, \\phi, \\tau(t) \\rangle$.\n        \\FOR {source node $s$ in $A_{\\langle \\tau, \\phi, \\tau(t) \\rangle}[t]$}\n            \\IF {$s$ has not been sampled ($s \\not\\in NS$)} \\label{line:check}\n                 \\IF {$s$ has no timestamp}\n                     \\STATE  $s.time = t.time$ // Inductively inherit timestamp. \\label{line:time}\n                 \\ENDIF\n                \\STATE  $B[\\tau][s] \\gets B[\\tau][s] + \\hat{D}_t$\\ \\ \\ // Add candidate node $s$ to budget $B$ with target node $t$'s normalized degree. \\label{line:update}\n            \\ENDIF\n        \\ENDFOR  \n    \\ENDFOR\n\\RETURN Updated Budget $B$\n\\end{algorithmic} \n\\end{algorithm}\n\n \n\n\\subsection{ Inductive Timestamp Assignment}\n\nTill now we have assumed that each node $t$ is assigned with a timestamp $T(t)$. However, in real-world heterogeneous graphs, many nodes are not associated with a fixed time. Therefore, we need to assign different timestamps to it. We denote these nodes as {plain nodes}. For example, the WWW conference is held in both 1974 and 2019, and the WWW node in these two years has dramatically different research topics. Consequently, we need to decide which timestamp(s) to attach to the WWW node. \n\nThere also exist {event nodes} in heterogeneous graphs that have an explicit timestamp associated with them. \nFor example, the paper node should be associated with its publication behavior and therefore attached to its publication date. \n\nWe propose an inductive timestamp assignment algorithm to assign plain nodes timestamps based on event nodes that they are linked with. \nThe algorithm is shown in Algorithm~\\ref{alg:budget} line~\\ref{line:time}. \nThe  idea is that plan nodes inherit the timestamps from event nodes. \nWe examine whether the candidate source node is an event node. If yes, like a paper published at a specific year, we keep its timestamp for capturing temporal dependency. \nIf no, like a conference that can be associated with any timestamp, we inductively assign the associated node's timestamp, such as the published year of its paper, to this plain node. In this way, we can adaptively assign timestamps during the sub-graph sampling procedure.\n\n \n\n\\hide{\n\nIn this section, we discuss how we train the proposed relational graph transformer for web-scale graphs. We firstly describe an efficient sub-graph sampling mechanism, which is designed explicitly for large-scale heterogeneous graph, and can get an informative subgraph for calculating accurate representation. Then we describe how we assign timestamps inductively for keeping temporal information.\n\n\\subsection{Graph Sampling for Heterogeneous Graph}\n\none major challenge of training deep GNN for large-scale graphs remains a big challenge. Original full-batch GNN~\\cite{gcn} training requires calculating the representation of all the nodes in the graph per GNN layer, which brings in high computation and memory costs. To alleviate this issue, several sampling-based methods~\\cite{graphsage, fastgcn, ladies} have been proposed to train GNNs on a subset of nodes. However, in heterogeneous graphs, nodes an edges within different types cannot be treated equally, as the number and degree distribution of nodes in different types can be significantly different. For example, the number of venue nodes is far less than the number of paper nodes. Therefore, directly using these sampling algorithms to heterogeneous graphs are prone to get sub-graphs that are extremely imbalanced regarding node and edge types. Obviously, such imbalanced sub-graphs are not appropriate to train GNNs. Therefore, we should design a new sampling algorithm specifically for such heterogeneous graphs.\n\nBased on the previous discussion, a promising sampling algorithm for heterogeneous graphs should: (1) Keep a similar number of nodes and edges for each type; (2) Keep the sampled graph dense, to minimize information loss and reduce sample variance. Following these two requirements, we design a mini-batch Heterogeneous Graph Sampling algorithm. The basic idea is that we keep a separate node budget $B[ST]$ for each node type $ST$, and sample an equal number of nodes per type. Given a set of nodes already sampled, we add all their direct neighbors into the corresponding budget with Algorithm~\\ref{alg:budget}, and calculate their normalized degree in line~\\ref{line:update}, which will then be used to calculate sampling probability. Such normalization is equivalent to accumulate the random walk probability of each sampled node to its neighborhood, which can avoid the sampling being dominated by high-degree nodes. After the budget is updated, we then calculate the sampling probability in Algorithm~\\ref{alg:sample} line~\\ref{line:prob}, where we calculate the square of normalized degree for each budget. As is proved in~\\cite{fastgcn, ladies}, using such sampling probability can reduce the sampled variance. We sample $n$ nodes in node type $ST$ using the calculated probability, add each node into output set, update its neighborhood to the budget and remove it out of budget in line~\\ref{line:samp}-\\ref{line:end}. Repeating such procedure for $L$ times, and we get a sampled sub-graph with $L-$th depth from initial nodes. Finally, we reconstruct the adjacency matrix among the sampled nodes and return both as the mini-batch sub-graph. Using the above sampling algorithm, the sampled sub-graph contains an equal number of nodes per node type (based on the separate node budget), and sufficiently dense to reduce sampling variance (based on the normalized degree and importance sampling), and thus is suitable for training GNNs on web-scale heterogeneous graphs.\n\n\\begin{algorithm}[tb] \n\\caption{Mini-Batch Heterogeneous Graph Sampling} \n\\label{alg:sample} \n\\begin{algorithmic}[1] \n\\REQUIRE\nAdjacency Matrix $A$ for each $\\langle \\tau(s), \\phi(e), \\tau(t) \\rangle$ relation pair; Output node Set $OS$; Sample Number $n$ per node type; Sample Depth $L$\\\\\n\\ENSURE\nSampled Node Set $NS$; Sampled adjacency matrix $\\hat{A}$.\n\\STATE  $NS \\gets OS$ // Initialize Sampled Node Set as Output Node Set.\n\\STATE  Initialize an empty Budget $B$ storing nodes for each node type with normalized degree. \n\\FOR {$t \\in NS$}\n    \\STATE  Add-In-Budget($B$, $t$, $A$, $NS$) // Add neighbors of $t$ to $B$.\n\\ENDFOR\n\\FOR {$l \\gets 1$ to $L$}\n    \\FOR {Source Node Type $ST \\in B$}\n        \\STATE  $\\ prob^{(l-1)}[ST][s] \\gets \\frac{B[ST][s]^2}{\\|B[ST]\\|_2^2}$ // Calculate sampling probability for each source node $s$ within node type $ST$. \\label{line:prob}\n        \\FOR {Repeat $n$ times} \\label{line:samp}\n            \\STATE  Sample node $t$ from $B[ST]$ using $prob^{(l-1)}[ST]$.\n            \\STATE  $OS[ST].add(t)$ // Add node $t$ into Output Node Set.\n            \\STATE  Add-In-Budget($B$, $t$, $A$, $NS$) // Add neighbors of $t$ to $B$.\n            \\STATE  $B[ST].pop(t)$ // Remove sampled node $t$ from budget. \\label{line:end}\n        \\ENDFOR\n    \\ENDFOR\n\\ENDFOR\n\\STATE  Reconstruct the sampled adjacency matrix $\\hat{A}$ among the sampled node $OS$ from $A$.\n\\RETURN $OS$ and $\\hat{A}$; \n\\end{algorithmic} \n\\end{algorithm}\n\n \n\n\\begin{algorithm}[tb] \n\\caption{Add-In-Budget} \n\\label{alg:budget} \n\\begin{algorithmic}[1] \n\\REQUIRE\nBudget $B$ storing nodes for each type with normalized degree; Added node $t$; Adjacency matrix $A$ for each $\\langle \\tau(s), \\phi(e), \\tau(t) \\rangle$ relation pair; Sampled node set $NS$.\\\\\n\\ENSURE\nUpdated Budget $B$.\n     \\FOR {each possible source node type $\\tau$ and edge type $\\phi$}\n        \\STATE  $\\hat{D}_t \\gets 1 \\ /\\ len\\Big(A_{\\langle \\tau, \\phi, \\tau(t) \\rangle}[t]\\Big)$ // get normalized degree of added node $t$ regarding to $\\langle \\tau, \\phi, \\tau(t) \\rangle$.\n        \\FOR {source node $s$ in $A_{\\langle \\tau, \\phi, \\tau(t) \\rangle}[t]$}\n            \\IF {$s$ has not been sampled ($s \\not\\in NS$)} \\label{line:check}\n                 \\IF {$s$ has no timestamp}\n                     \\STATE  $s.time = t.time$ // Inductively inherit timestamp. \\label{line:time}\n                 \\ENDIF\n                \\STATE  $B[\\tau][s] \\gets B[\\tau][s] + \\hat{D}_t$\\ \\ \\ // Add candidate node $s$ to budget $B$ with target node $t$'s normalized degree. \\label{line:update}\n            \\ENDIF\n        \\ENDFOR  \n    \\ENDFOR\n\\RETURN Updated Budget $B$\n\\end{algorithmic} \n\\end{algorithm}\n\n \n\n\\subsection{Inductive Timestamp Assignment}\nTill now we've assumed that each node $t$ is assigned with a timestamp $T(t)$. However, in real-world heterogeneous graphs, many nodes are not associated with a fixed time. Instead, we can assign different timestamps to it. We denote these nodes as `plain' nodes. For example, the WWW conference is held in both 1974 and 2019, and the same WWW in these two years has a dramatically different research topic. Therefore, we should decide which timestamp to attach to this WWW node. \n\nNote that there also exist some `event' nodes that have explicit time meaning associated with it. For example, the paper node should be associated with its publication behavior and attached to its publication date. We thus propose an inductive timestamp assignment algorithm that gives `plain' nodes based on the `event' nodes it's associated with. For example, papers are `event' nodes. When we consider a paper node published in 2010, we should use 2010 as its timestamp. Also, when we consider its published venue, say WWW, it's reasonable to also use the publication year 2010 as its timestamp to calculate representation and then pass it to this paper node. Moreover, when we consider the representation of WWW@2010, we use all the papers published in WWW as its neighbors, each associated with their published time. Therefore, the relative temporal encoding used in RGT can capture the temporal dependency of this WWW@2010 node with papers at different times. Thus, we propose an inductive timestamp assignment algorithm, which is shown in Algorithm~\\ref{alg:budget} line~\\ref{line:time}. The basic idea is that we inherit the timestamp from `event' nodes that have fixed timestamp to `plain' nodes that can be associated with any time. We judge whether the candidate source node is an `event' node. If yes, like a paper published at a specific year, we keep its timestamp for capturing temporal dependency. If no, like a conference that can be associated with any timestamp, we inductively assign the target node's timestamp, such as the published year of a paper, to this `plain' node. In this way, we can adaptively assign timestamps during the sub-graph sampling procedure.\n\nNoted that during the sampling, we can have multiple `event' nodes linked to the same `plain' node. For example, multiple papers published at different times, but both on WWW. In this case, we treat the same node with different timestamps as different, which means that in Algorithm~\\ref{alg:budget} line~\\ref{line:check}, we also use the associated timestamp as a judgment indicator. In this way, WWW@1974 and WWW@2019 can both occur in our sampled subgraph, linked to the same neighborhoods. However, due to the existence of relative temporal encoding, the RGT model should learn to attend differently for these two WWW nodes towards all the papers published on it in different years. \n\n}%end of hide\n\n    \n\\section{Evaluation}\\label{sec:evaluation}\n\\hide{\n\n\\begin{table}[!tp]\n\\centering\n\\small\n\\begin{tabular}{ccc} \n\\toprule\nSource & Target & Edge Relation Type \\\\\n\\midrule\n\\multirow{4}{*}{Paper} & Paper & $\\{$Self, Cite, Cite$^{-1}\\}$\\\\\n~& Author & $\\{$Write$_\\text{first}^{-1}$, Write$_\\text{last}^{-1}$, Write$_\\text{other}^{-1}\\}$\\\\\n~& Field & $\\{$In$_{L0}$, In$_{L1}$, In$_{L2}$, In$_{L3}$, In$_{L4}$, In$_{L5}\\}$\\\\\n~& Venue & $\\{$Pub$_{\\text{conf}}$, Pub$_{\\text{journal}}$, Pub$_{\\text{preprint}}\\}$\\\\\n\\midrule\n\\multirow{5}{*}{Author} & Author & $\\{$Self, CoAuthor$\\}$\\\\\n~& Paper & $\\{$Write$_{\\text{first}}$, Write$_{\\text{last}}$, Write$_{\\text{other}}\\}$\\\\\n~& Field & $\\{$In$_{L0}$, In$_{L1}$, In$_{L2}$, In$_{L3}$, In$_{L4}$, In$_{L5}\\}$\\\\\n~& Venue & $\\{$Pub$_{\\text{conf}}$, Pub$_{\\text{journal}}$, Pub$_{\\text{preprint}}\\}$\\\\\n~& Institute & $\\{$Affiliate$\\}$\\\\\n\\midrule\n\\multirow{3}{*}{Field} & Field & $\\{$Self, Within, Within$^{-1}\\}$\\\\\n~& Paper & $\\{$In$_{L0}^{-1}$, In$_{L1}^{-1}$, In$_{L2}^{-1}$, In$_{L3}^{-1}$, In$_{L4}^{-1}$, In$_{L5}^{-1}\\}$\\\\\n~& Author & $\\{$In$_{L0}^{-1}$, In$_{L1}^{-1}$, In$_{L2}^{-1}$, In$_{L3}^{-1}$, In$_{L4}^{-1}$, In$_{L5}^{-1}\\}$\\\\\n\\midrule\n\\multirow{3}{*}{Venue} & Venue & $\\{$Self$\\}$\\\\\n~& Paper & $\\{$Pub$_{\\text{conf}}^{-1}$, Pub$_{\\text{journal}}^{-1}$, Pub$_{\\text{preprint}}^{-1}\\}$\\\\\n~& Author & $\\{$Pub$_{\\text{conf}}^{-1}$, Pub$_{\\text{journal}}^{-1}$, Pub$_{\\text{preprint}}^{-1}\\}$\\\\\n\\midrule\n\\multirow{2}{*}{Institute} & Institute & $\\{$Self, CoAuthor$\\}$\\\\\n~& Author & $\\{$Affiliate$^{-1}\\}$\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Open Academic Graph (OAG) Schema.} \n\\label{tab:schema} \n\\end{table}\n\n}%end of hide\n\n\\begin{table*}[th]\n\\centering\n\\footnotesize\n\\begin{tabular}{c|rr|rrrrr|rrrrr} \n\\toprule\nDataset & $\\#$nodes & $\\#$edges & $\\#$papers & $\\#$authors & $\\#$fields & $\\#$venues & $\\#$institutes & $\\#$P-A & $\\#$P-F & $\\#$P-V & $\\#$A-I & $\\#$P-P \\\\ \n\\midrule\nCS & 11,732,027 & 107,263,811 & 5,597,605 & 5,985,759 &  119,537&  27,433 & 16,931   & 15,571,614 & 47,462,559 & 5,597,606 & 7,190,480 & 31,441,552\\\\ \n\\midrule\nMed & 51,044,324 & 451,468,375 & 21,931,587 & 28,779,507&  289,930 &  25,044&  18,256  &85,620,479 & 149,728,483&21,931,588 & 28,779,507& 165,408,318\\\\ \n\\midrule\nOAG & 178,663,927 & 2,236,196,802  & 89,606,257 & 88,364,081 &  615,228&  53,073&  25,288 & 300,853,688&  657,049,405&  89,606,258&  167,449,933 & 1,021,237,518\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Open Academic Graph (OAG) Statistics.} \n\\label{tab:stat} \n\\end{table*}\n\nIn this section, we evaluate the proposed \\model\\ on three heterogeneous academic graph datasets. \nWe conduct the Paper-Field prediction, Paper-Venue prediction, and Author Disambiguation tasks. \nWe also take case studies to demonstrate how \\short\\ can automatically learn and extract meta paths that are important for downstream tasks\\footnote{The dataset and code are publicly available at \\url{https://github.com/acbull/pyHGT}.}. \n\n\\subsection{Web-Scale Datasets}\n\nTo examine the performance of the proposed model and its real-world applications, we use the Open Academic Graph (OAG)~\\cite{DBLP:conf/www/SinhaSSMEHW15,tang2008arnetminer,DBLP:conf/kdd/ZhangLTDYZGWSLW19} as our experimental basis. \nOAG consists of more than 178 million nodes and 2.236 billion edges---the largest publicly available heterogeneous academic dataset. \nIn addition, all papers in OAG are associated with their publication dates, spanning from 1900 to 2019. \n\nTo test the generalization of the proposed model, we also construct two domain-specific subgraphs from OAG: the Computer Science (CS) and Medicine (Med) academic graphs. \nThe graph statistics are listed in Table \\ref{tab:stat}, in which P--A, P--F, P--V, A--I, and P--P denote the edges between paper and author, paper and field, paper and venue, author and institute, and the citation links between two papers. \n\nBoth the CS and Med graphs contain tens of millions of nodes and hundreds of millions of edges, making them at least one magnitude larger than the other CS (e.g., DBLP) and Med (e.g., Pubmed) academic datasets that are commonly used in existing heterogeneous GNN and heterogeneous graph mining studies. \nMoreover, the three datasets used are far more distinguishable than previously wide-adopted small citation graphs used in GNN studies, such as Cora, Citeseer, and Pubmed~\\cite{gcn,gat}, which only contain thousands of nodes. \n\nThere are totally five node types: `Paper', `Author', `Field', `Venue', and `Institute'. \nThe `Field' nodes in OAG are categorized into six levels from $L_0$ to $L_5$, which are organized with a hierarchical tree. % (We use `Within' and `Within$^{-1}$' to represent this hierarchy). \nTherefore, we differentiate the `Paper--Field' edges corresponding to the field level. \n\nIn addition, we differentiate the different author orders (i.e., the first author, the last one, and others) and venue types (i.e., journal, conference, and preprint) as well. \nFinally, the `Self' type corresponds to the self-loop connection, which is widely added in GNN architectures. \nExcept the `Self' relationship, which are symmetric, all other relation types $\\phi$ have a reverse relation type $\\phi^{-1}$. \n\n\\subsection{Experimental Setup}\n\n\\vpara{Tasks and Evaluation.}\nWe evaluate the \\short\\ model on four different real-world downstream tasks: the prediction of Paper--Field ($L_1$), Paper--Field ($L_2$), and Paper--Venue, and Author Disambiguation. \nThe goal of the first three node classification tasks is to predict the correct $L_1$ and $L_2$ fields that each paper belongs to or the venue it is published at, respectively. \nWe use different GNNs to get the contextual node representation of the paper and use a softmax output layer to get its classification label. \nFor author disambiguation, we select all the authors with the same name and their associated papers. \nThe task is to conduct link prediction between these papers and candidate authors. \nAfter getting the paper and author node representations from GNNs, we use a Neural Tensor Network to get the probability of each author-paper pair to be linked. \n\nFor all tasks, we use papers published before the year 2015 as the training set, papers between 2015 and 2016 for validation, and papers between 2016 and 2019 as testing. \nWe choose NDCG and MRR, which are two widely adopted ranking metrics~\\cite{DBLP:books/daglib/0027504, DBLP:series/synthesis/2014Li}, as the evaluation metrics. \nAll models are trained for 5 times and, the mean and standard variance of test performance are reported. \n\n\\vpara{Baselines.}We compare \\short\\ with two classes of state-of-art graph neural networks. \nAll baselines as well as our own model, \nare implemented via the PyTorch Geometric (PyG) package~\\cite{pyG}. \n\n The first class of GNN baselines is designed for homogeneous graphs, including:\n\\begin{itemize}\n    \\item Graph Convolutional Networks (GCN)~\\cite{gcn}, which simply averages the neighbor's embedding followed by linear projection. We use the implementation provided in PyG.\n    \\item Graph Attention Networks (GAT)~\\cite{gat}, which adopts multi-head additive attention on neighbors. We use the implementation provided in PyG. \n\\end{itemize}\n\nThe second class considered is several dedicated heterogeneous GNNs as baselines,  including: %that are dedicatedly designed for heterogeneous graphs, including:\n\\begin{itemize}\n    \\item Relational Graph Convolutional Networks (RGCN)~\\cite{DBLP:conf/esws/SchlichtkrullKB18}, which keeps a different weight for each relationship, i.e., a relation triplet. We use the implementation provided in PyG.\n    \\item Heterogeneous Graph Neural Networks (HetGNN)~\\cite{DBLP:conf/kdd/ZhangSHSC19}, which adopts different Bi-LSTMs for different node type for aggregating neighbor information. We re-implement this model in PyG following the authors' official code.\n    \\item Heterogeneous Graph Attention Networks (HAN)~\\cite{DBLP:conf/www/WangJSWYCY19} design hierarchical attentions to aggregate neighbor information via different meta paths. We re-implement this model in PyG following the authors' official code.\n\\end{itemize}\n\nIn addition, to systematically analyze the effectiveness of the two major components of \\short, i.e., Heterogeneous weight parameterization (Heter) and Relative Temporal Encoding (RTE), we conduct an ablation study, but comparing with models that remove these components. Specifically, we use $-Heter$ to denote models that uses the same set of weights for all meta relations, and use $-RTE$ to denote models that doesn't include relative temporal encoding. By considering all the permutations, we have: \\short$_{-Heter}^{-RTE}$, \\short$_{-Heter}^{+RTE}$, \\short$_{+Heter}^{-RTE}$ and \\short$_{+Heter}^{+RTE}$\\footnote{Unless other stated, \\short\\ refers to \\short$_{+Heter}^{+RTE}$.}.\n\nWe use our \\sampling\\ algorithm proposed in Section~\\ref{sec:train} for all baseline GNNs to handle the large-scale OAG graph. To avoid data leakage, we remove out the links we aim to predict (e.g., the Paper-Field link as the label) from the sub-graph.\n\n\\begin{table*}[!tp]\n\\centering\n\\small\n\\renewcommand\\arraystretch{1.3}\n\\setlength{\\tabcolsep}{3pt}\n\\begin{tabular}{c|c|c|ccccc|cccc} \n\\toprule\n\\multicolumn{3}{c|}{GNN Models} & GCN~\\cite{gcn}  & RGCN~\\cite{DBLP:conf/esws/SchlichtkrullKB18}& GAT~\\cite{gat}& HetGNN~\\cite{DBLP:conf/kdd/ZhangSHSC19} & HAN~\\cite{DBLP:conf/www/WangJSWYCY19} &{ \\short$_{-Heter}^{-RTE}$} & \\short$_{-Heter}^{+RTE}$ & \\short$_{+Heter}^{-RTE}$ & \\short$_{+Heter}^{+RTE}$ \\\\ \\midrule\n\n\\multicolumn{3}{c|}{$\\#$ of Parameters} &1.69M   & 8.80M   & 1.69M   & 8.41M   & 9.45M   &3.12M   &3.88M & 7.44M   & 8.20M \\\\\n\\midrule\n\\multicolumn{3}{c|}{Batch Time} &  0.46s  &   1.24s &   0.97s &   1.35s &   2.27s &  1.11s &  1.14s &   1.48s &  1.50s \\\\\n\\midrule\n    \\multirow{10}{*}{\\tabincell{c}{CS}} \n        & \\multirow{2}{*}{Paper--Field ($L_1$)} & NDCG \n         &.608$\\pm$.062 & .603$\\pm$.065 & .622$\\pm$.071 & .612$\\pm$.063 &.618$\\pm$.058 & .662$\\pm$.051 & .689$\\pm$.042 &.705$\\pm$.036 & \\textbf{.718$\\pm$.014}\\\\ \n         ~&~& MRR \n         &.679$\\pm$.069 & .683$\\pm$.056 & .694$\\pm$.065 & .689$\\pm$.060 &.691$\\pm$.051 & .751$\\pm$.036& .779$\\pm$.027 &.799$\\pm$.023 & \\textbf{.823$\\pm$.019}\\\\\n    \\cmidrule{2-12}\n        ~ & \\multirow{2}{*}{Paper--Field ($L_2$)} & NDCG & .344$\\pm$.021 & .322$\\pm$.053& .357$\\pm$.058& .346$\\pm$.071 &.352$\\pm$.051& .362$\\pm$.048& .371$\\pm$.043 & .379$\\pm$.047&\\textbf{.403$\\pm$.041}  \\\\\n        ~&~& MRR &.353$\\pm$.053 &.340$\\pm$.061 &.382$\\pm$.057 & .373$\\pm$.051 &.388$\\pm$.065 & .394$\\pm$.072&.397$\\pm$.064&\n        .414$\\pm$.076& \\textbf{.439$\\pm$.078}\\\\\n    \\cmidrule{2-12}\n        ~ & \\multirow{2}{*}{\\tabincell{c}{Paper--Venue}} & NDCG  &.406$\\pm$.081 & .412$\\pm$.076  & .437$\\pm$.082 & .431$\\pm$.074&  .449$\\pm$.072 & .456$\\pm$.069&.461$\\pm$.066 &.468$\\pm$.074&  \\textbf{.473$\\pm$.054}  \\\\  ~&~& MRR & .215$\\pm$.066 &.216$\\pm$.105&.239$\\pm$.089 & .245$\\pm$.069& .254$\\pm$.074 & .258$\\pm$.085& .265$\\pm$.090 &.275$\\pm$.089 &\\textbf{.288$\\pm$.088}\\\\\n    \\cmidrule{2-12}\n        ~ & \\multirow{2}{*}{\\tabincell{c}{Author\\\\Disambiguation}} & NDCG &.826$\\pm$.039 & .835$\\pm$.042&.864$\\pm$.051 & .850$\\pm$.056& .859$\\pm$.053 & .867$\\pm$.048 &.875$\\pm$.046 & .886$\\pm$.048& \\textbf{.894$\\pm$.034}\\\\  ~&~& MRR &.661$\\pm$.045&.665$\\pm$.054 &.694$\\pm$.052 & .668$\\pm$.061 &.688$\\pm$.049 & .703$\\pm$.036& .712$\\pm$.032 &.727$\\pm$.038 & \\textbf{.732$\\pm$.038}  \\\\\n\\midrule\n\\multirow{10}{*}{\\tabincell{c}{Med}} \n          & \\multirow{2}{*}{Paper--Field ($L_1$)} \n          & NDCG &.560$\\pm$.056 & .571$\\pm$.061 & .584$\\pm$.076 & .598$\\pm$.068 & .607$\\pm$.054&.654$\\pm$.048 & .667$\\pm$.045 &.683$\\pm$.037 & \\textbf{.709$\\pm$.029}  \\\\  \n          ~&~& MRR &.465$\\pm$.055 &.470$\\pm$.082 &.493$\\pm$.069 & .509$\\pm$.054& .575$\\pm$.057 &.620$\\pm$.066 &.642$\\pm$.062 &.659$\\pm$.055 & \\textbf{.688$\\pm$.048}\\\\\n    \\cmidrule{2-12}\n        ~ & \\multirow{2}{*}{Paper--Field ($L_2$)} & NDCG& .334$\\pm$.035 & .337$\\pm$.051& .344$\\pm$.063&.342$\\pm$.048  &.350$\\pm$.059 & .359$\\pm$.053& .365$\\pm$.047 & .374$\\pm$.050&\\textbf{.384$\\pm$.046}\\\\\n        ~&~& MRR &.337$\\pm$.061 &.343$\\pm$.063 &.370$\\pm$.058 &.373$\\pm$.061 &.379$\\pm$.052 & .385$\\pm$.071& .397$\\pm$.069&\n        .408$\\pm$.071& \\textbf{.417$\\pm$.074}\\\\\n    \\cmidrule{2-12}\n        ~ & \\multirow{2}{*}{\\tabincell{c}{Paper--Venue }} & NDCG  &.377$\\pm$.059\n         & .383$\\pm$.062 &.388$\\pm$.065&  .412$\\pm$.057& .416$\\pm$.068& .421$\\pm$.083 & .432$\\pm$.078  &\\textbf{.446$\\pm$.083} &.445$\\pm$.085  \\\\  ~&~& MRR &.211$\\pm$.045 & .217$\\pm$.058 &.244$\\pm$.091 & .259$\\pm$.072& .271$\\pm$.056 & .277$\\pm$.081&.282$\\pm$.085 &.288$\\pm$.074 & \\textbf{.291$\\pm$.062}\\\\\n    \\cmidrule{2-12}\n        ~ & \\multirow{2}{*}{\\tabincell{c}{Author\\\\Disambiguation}}&MRR &.776$\\pm$.042 & .779$\\pm$.048&.828$\\pm$.044 &.824$\\pm$.058 & .834$\\pm$.056 & .838$\\pm$.047&.844$\\pm$.041 & .864$\\pm$.043& \\textbf{.871$\\pm$.040}\\\\  ~&~& NDCG &.614$\\pm$.051&.625$\\pm$.049 &.663$\\pm$.046 & .659$\\pm$.061 &.667$\\pm$.053 &.683$\\pm$.055 &.691$\\pm$.046 &.708$\\pm$.041 & \\textbf{.718$\\pm$.043}  \\\\ \n\\midrule\n\\multirow{10}{*}{\\tabincell{c}{OAG}} \n          & \\multirow{2}{*}{Paper--Field ($L_1$)} & NDCG&.508$\\pm$.141  & .511$\\pm$.128 & .534$\\pm$.103 & .543$\\pm$.084 & .544$\\pm$.096 &.571$\\pm$.089 & .578$\\pm$.086 & .595$\\pm$.089 & \\textbf{.615$\\pm$.084}  \\\\  \n          ~&~& MRR &.556$\\pm$.136  & .565$\\pm$.105 & .610$\\pm$.096 & .616$\\pm$.076 & .622$\\pm$.092&.649$\\pm$.081 & .657$\\pm$.078 & .675$\\pm$.082 & \\textbf{.702$\\pm$.081}\\\\\n    \\cmidrule{2-12}\n        ~ & \\multirow{2}{*}{Paper--Field ($L_2$)} & NDCG &.318$\\pm$.074 & .328$\\pm$.046& .339$\\pm$.049& .336$\\pm$.062  & .342$\\pm$.051&.350$\\pm$.045 & .354$\\pm$.046 & .358$\\pm$.052&\\textbf{.367$\\pm$.048}\\\\\n        ~&~& MRR &.322$\\pm$.067 &.332$\\pm$.052 &.348$\\pm$.045 &.350$\\pm$.053 &.358$\\pm$.049 &.362$\\pm$.057 &.369$\\pm$.058&\n        .371$\\pm$.064& \\textbf{.378$\\pm$.071}\\\\\n    \\cmidrule{2-12}\n        ~ & \\multirow{2}{*}{\\tabincell{c}{Paper--Venue }} & NDCG  &.302$\\pm$.066\n         & .313$\\pm$.051 &.317$\\pm$.057& .309$\\pm$.071& .327$\\pm$.062& .334$\\pm$.058 &.341$\\pm$.059  &.353$\\pm$.064 &\\textbf{.355$\\pm$.062}  \\\\  ~&~& MRR &.194$\\pm$.070 & .193$\\pm$.047 &.196$\\pm$.052 & .192$\\pm$.059& .214$\\pm$.067&.229$\\pm$.061 &.233$\\pm$.060 &.243$\\pm$.048 & \\textbf{.247$\\pm$.061}\\\\\n    \\cmidrule{2-12}\n        ~ & \\multirow{2}{*}{\\tabincell{c}{Author\\\\Disambiguation}} & NDCG &.738$\\pm$.042 & .755$\\pm$.048&.797$\\pm$.044 &.803$\\pm$.058 & .821$\\pm$.056 & .835$\\pm$.043&.841$\\pm$.041 & .847$\\pm$.043& \\textbf{.852$\\pm$.048}\\\\   ~&~& MRR &.612$\\pm$.064&.619$\\pm$.057 &.645$\\pm$.063 & .649$\\pm$.052 &.660$\\pm$.049 &.668$\\pm$.059 &.674$\\pm$.058 &.683$\\pm$.066 & \\textbf{.688$\\pm$.054}  \\\\\n\\bottomrule\n\n\\end{tabular}\n\n\\caption{Experimental results of different methods over the three datasets.} \n\\label{tab:result} \n\\end{table*}\n\n\\hide{\nWe compare \\short\\ with several state-of-art graph neural networks. \nAll these baselines as well as our model\\footnote{The dataset is publicly available at \\url{https://www.openacademic.ai/oag/}, and the code and trained-models will be open-sourced upon publication.} are implemented via the PyTorch Geometric (PyG) package~\\cite{pyG}, a GNN framework that supports fast training via graph gather/scatter operation. The first class of GNN baselines is designed for homogeneous graphs, including:\n\\begin{itemize}\n    \\item Graph Convolutional Networks (GCN)~\\cite{gcn}, which simply averages the neighbor's embedding followed by linear projection. We use the implementation provided in PyG~\\footnote{\\url{https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/graph_conv.html}}.\n    \\item Graph Attention Networks (GAT)~\\cite{gat}, which adopts multi-head additive attention on neighbors. We use the implementation provided in PyG~\\footnote{\\url{https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/gat_conv.html}}. \n\\end{itemize}\n\nAlso, we also compared with GNNs that is dedicatedly designed for heterogeneous graphs, including:\n\\begin{itemize}\n    \\item Relational Graph Convolutional Networks (RGCN)~\\cite{DBLP:conf/esws/SchlichtkrullKB18}, which keeps a different weight for each relationship, i.e., a relation triplet. We use the implementation provided in PyG~\\footnote{\\url{https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/rgcn_conv.html}}.\n    \\item Heterogeneous Graph Neural Networks (HetGNN)~\\cite{DBLP:conf/kdd/ZhangSHSC19}, which adopts different Bi-LSTMs for different node type for aggregating neighbor information. We re-implement this model in PyG following the authors' official code~\\footnote{\\url{https://github.com/chuxuzhang/KDD2019_HetGNN}}.\n    \\item Heterogeneous Graph Attention Networks (HAN)~\\cite{DBLP:conf/www/WangJSWYCY19}, which adopts two layers of attentions to aggregate neighbor information via different meta paths. We re-implement this model in PyG following the authors' official code~\\footnote{\\url{https://github.com/Jhy1993/HAN}}.\n\\end{itemize}\n\n\\yd{consider to remove all baselines' github links}\n\nTo further examine whether the components in our model can indeed exploit heterogeneity and temporal dependency, and eventually benefit downstream performance, we also propose two baselines as ablation study:  HGT$_{\\text{noHeter}}$, which uses a same set of weight for all meta relation, and HGT$_{\\text{noTime}}$, which removes the relative temporal encoding component. \n\nAs all of the baseline GNNs cannot handle the large-scale input graphs, we use \nthe heterogeneous mini-batch graph sampling algorithm proposed in Section~\\ref{sec:train} for all of them to get a sub-graph for each interested node or node pair. \nTo avoid data leakage, we remove out the link we aim to predict (e.g. the Paper-Field link as the label) from this sub-graph.\n\n\\vpara{Input Features.}As we don't assume the feature of each data type belongs to the same distribution, we are free to use the most appropriate features to represent each type of node. \nFor paper and author nodes, the node numbers are extremely large. Therefore, traditional node embedding algorithms are not suitable for extracting features for them. \nWe, therefore, resort to the paper titles for extracting features. For each paper, we get its title text and use a pre-trained XLNet~\\cite{xlnet, wolf2019transformers} to get the representation of each word in the title. We then average them weighted by each word's attention to get the title representation for each paper. The initial feature of each author is simply an average of his/her published papers' embeddings. For field, venue and institute nodes, the node numbers are small and we use the metapath2vec model~\\cite{dong2017metapath2vec} to train their node embeddings by reflecting the heterogeneous network structures. \n\n\\vpara{Implementation Details.}\nThe homogeneous graph neural network baselines (e.g., GCN and GAT) assume the node features belong to the same distribution, while our feature extraction doesn't fulfill this assumption. \nIf we directly feed the feature into these different baselines, they are unlikely to achieve good performance. \nTo make a fair comparison, for all the models, we add an adaptation layer between the input feature and the GNNs. This module simply conducts different linear projection for nodes in different node types. Such a procedure can be regarded to map heterogeneous data into the same distribution, which is also adopted in~\\cite{DBLP:conf/kdd/ZhangSHSC19, DBLP:conf/www/WangJSWYCY19}. \n\nWe set the output dimension of such module as 256, and use it as the hidden dimension throughout the networks for all baselines. For all multi-head attention-based methods, we choose the head number as 8. All the GNNs keep 3 layers so that the receptive fields of each network is exactly the same. All the GNNs are optimized via AdamW optimizer~\\cite{DBLP:conf/iclr/LoshchilovH19} with Cosine Annealing Learning Rate Scheduler~\\cite{DBLP:conf/iclr/LoshchilovH17}. For each model, we train it for 200 epochs, select the one with the lowest validation loss as the best model. \n} %end of hide ======================================\n\n\\vpara{Input Features.}As we don't assume the feature of each node type belongs to the same distribution, we are free to use the most appropriate features to represent each type of nodes. \nFor each paper, we use a pre-trained XLNet~\\cite{xlnet, wolf2019transformers} to get the representation of each word in its title. \nWe then average them weighted by each word's attention to get the title representation for each paper. \nThe initial feature of each author is then simply an average of his/her published papers' representations. \nFor the field, venue, and institute nodes, we use the metapath2vec model~\\cite{dong2017metapath2vec} to train their node embeddings by reflecting the heterogeneous network structures. \n\nThe homogeneous GNN baselines assume the node features belong to the same distribution, while our feature extraction doesn't fulfill this assumption. \nTo make a fair comparison, we add an adaptation layer between the input features and all used GNNs. \nThis module simply conducts different linear projections for nodes of different  types. \nSuch a procedure can be regarded to map heterogeneous data into the same distribution, which is also adopted in literature~\\cite{DBLP:conf/kdd/ZhangSHSC19, DBLP:conf/www/WangJSWYCY19}. \n\n\\vpara{Implementation Details.}\nWe use 256 as the hidden dimension throughout the neural networks for all baselines. For all multi-head attention-based methods, we set the head number as 8. \nAll GNNs keep 3 layers so that the receptive fields of each network are exactly the same. \nAll baselines are optimized via the AdamW optimizer~\\cite{DBLP:conf/iclr/LoshchilovH19} with the Cosine Annealing Learning Rate Scheduler~\\cite{DBLP:conf/iclr/LoshchilovH17}. For each model, we train it for 200 epochs and select the one with the lowest validation loss as the reported model. We use the default parameters used in GNN literature and donot tune hyper-parameters. \n\n\\subsection{Experimental Results}\nWe summarize the experimental results of the proposed model and baselines on three datasets in  Table ~\\ref{tab:result}. \nAll experiments for the four tasks are evaluated in terms of NDCG and MRR. \n\nThe results show that in terms of both metrics, the proposed \\short\\ significantly and consistently outperforms all baselines for all tasks on all datasets. \nTake, for example, the Paper--Field ($L_1$) classification task on OAG, \\short\\ achieves relative performance gains over baselines by 15--19\\% in terms of NDCG and 18--21\\% in terms of MRR (i.e., the performance gap divided by the baseline performance). \nWhen compared to HAN---the best baseline for most of the cases, the average relative NDCG improvements of \\short\\ on the CS, Med and OAG datasets are 11$\\%$, 10$\\%$ and 8$\\%$, respectively. \n\nOverall, we observe that on average, \\short\\ outperforms GCN, GAT, RGCN, HetGNN, and HAN by 20\\% for the four tasks on all three large-scale datasets.  \nMoreover, \\short\\ has fewer parameters and comparable batch time than all the heterogeneous graph neural network baselines, including RGCN, HetGNN, and HAN. \nThis suggests that by modeling heterogeneous edges according to their meta relation schema, we are able to have better generalization with fewer resource consumption. \n\n\\vpara{Ablation Study.}The core component in \\short\\ are the heterogeneous weight parameterization (Heter) and Relative Temporal Encoding (RTE).\nTo further analyze their effects, we conduct an ablation study by removing them from \\short. \nSpecifically, the model that removes heterogeneous weight parameterization, i.e., \\short$_{-Heter}^{+RTE}$, drops 4\\% of performance compared with the full model \\short$_{+Heter}^{+RTE}$. \nBy removing RTE (i.e., \\short$_{+Heter}^{-RTE}$), the performance has a 2\\% drop. \nThe ablation study shows the significance of parameterizing with meta relations and using Relative Temporal Encoding.\n\nIn addition, we also try to implement a baseline that keeps a unique weight matrix for each relation. However, such a baseline contains too many parameters so that our experimental setting doesn't have enough GPU memory to optimize it. This also indicates that using the meta relation to parameterize weight matrices can achieve competitive performance with fewer resources.\n\n\\subsection{Case Study}\n\nTo further evaluate how  Relative Temporal Encoding (RTE) can help \\short\\ to capture graph dynamics, we conduct a case study showing the evolution of conference topic. \nWe select 100 conferences in computer science with the highest citations, assign them three different timestamps, i.e., 2000, 2010 and 2020, and construct sub-graphs initialized by them. \nUsing a trained HGT, we can get the representations for these conferences, with which we can calculate the euclidean distances between them. \nWe select WWW, KDD, and NeurIPS as illustration. \nFor each of them, we pick the top-5 most similar conferences (i.e., the one with the smallest euclidean distance) to show how the conference's topics evolve over time. \n\nAs shown in Table~\\ref{tab:case}, these venues' relationships have changed from 2000 to 2020. \nFor example, WWW in 2000 was more related to some database conferences, i.e., SIGMOD and VLDB, and some networking conferences, i.e., NSDI and GLOBECOM. \nHowever,  WWW in 2020 would become more related to some data mining and information retrieval conferences (KDD, SIGIR, and WSDM), in addition to SIGMOD and GLOBECOM. \nAlso, KDD in 2000 was more related to traditional database and data mining venues, while in 2020 it will tend to correlate with a variety of topics, i.e. machine learning (NeurIPS), database (SIGMOD), Web (WWW), AI (AAAI), and NLP (EMNLP). \nAdditionally, our \\short\\ model can capture the difference brought by new conferences. \nFor example, NeurIPS in 2020 would relate with ICLR, which is a newly organized deep learning conference. \nThis case study shows that the relative temporal encoding can help capture the temporal evolution of the heterogeneous academic graphs.\n\n\\begin{table}[t!]\n\\centering\n\\renewcommand\\arraystretch{1.3}\n\\begin{tabular}{ccc} \n\\toprule\nVenue & Time & Top$-$5 Most Similar Venues \\\\\n\\midrule\n\\multirow{3}{*}{WWW} & 2000 & SIGMOD, VLDB, NSDI, GLOBECOM, SIGIR\\\\\n~& 2010 & GLOBECOM, KDD, CIKM, SIGIR, SIGMOD\\\\\n~& 2020 & KDD, GLOBECOM, SIGIR, WSDM, SIGMOD\\\\\n\\midrule\n\\multirow{3}{*}{KDD} & 2000 & SIGMOD, ICDE, ICDM, CIKM, VLDB\\\\\n~& 2010 & ICDE, WWW, NeurIPS, SIGMOD, ICML\\\\\n~& 2020 & NeurIPS, SIGMOD, WWW, AAAI, EMNLP\\\\\n\\midrule\n\\multirow{3}{*}{NeurIPS} & 2000 & ICCV, ICML, ECCV, AAAI, CVPR\\\\\n~& 2010 & ICML, CVPR, ACL, KDD, AAAI\\\\\n~& 2020 & ICML, CVPR, ICLR, ICCV, ACL\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Temporal Evolution of Conference Similarity.} \n\\label{tab:case} \n\\end{table}\n\n\\subsection{Visualize Meta Relation Attention}\nTo illustrate how the incorporated meta relation schema can benefit the heterogeneous message passing process, we pick the schema that has the largest attention value in each of the first two \\short\\ layers and plot the meta relation attention hierarchy tree in Figure~\\ref{fig:meta}. \nFor example, to calculate a paper's representation, \n$\\langle$Paper, $is\\_published\\_at$, Venue, $is\\_published\\_at^{-1}$, Paper$\\rangle$, \n$\\langle$Paper, $has\\_L_2\\_field\\_of$, Field, $has\\_L_5\\_field\\_of^{-1}$, Paper$\\rangle$, \nand $\\langle$Institute, $is\\_affiliated\\_with^{-1}$, Author, $is\\_first\\_author\\_of$, Paper$\\rangle$\nare the three most important meta relation sequences, which can be regarded as meta paths \\textit{PVP, PFP,} and \\textit{IAP}, respectively. \nNote that these meta paths and their importance are automatically learned from the data without manual design.  \nAnother example of calculating an author node's representation is shown on the right. \nSuch visualization demonstrates that \\model\\ is capable of implicitly learning to construct important meta paths for specific downstream tasks, without manual customization.\n\n\\hide{\n \\begin{figure}[t!]\n    \\centering\n        \\includegraphics[width=0.5\\textwidth, trim = 30 0 50 0,clip]{picture/meta.png}\n    \\caption{Hierarchy tree of learned meta relation attention.}\n    \\label{fig:meta}\n\\end{figure} \n\n}\n\n \\begin{figure}[t!]\n    \\centering\n        \\includegraphics[width=0.46\\textwidth, trim = 30 50 40 35,clip]{picture/meta2.png}\n    \\caption{Hierarchy of the learned meta relation attention.}\n    \\label{fig:meta}\n    \\vspace{-0.3cm}\n\\end{figure} \n\n\\hide{\n\\subsection{Dataset Description}\n\nTo test our proposed model's performance on real-world heterogeneous and dynamic web-scale graph, we choose Open Academic Graph (OAG)~\\cite{DBLP:conf/www/SinhaSSMEHW15, DBLP:conf/kdd/ZhangLTDYZGWSLW19}, the largest public heterogeneous academic graph, as our experimental datasets. \nWe define the heterogeneous graph schema of OAG as in table~\\ref{tab:schema}. There are totally 5 node types: `Paper', `Author', `Field', `Venue' and `Institute'. For edges, unlike many previous works on heterogeneous academic graphs that only has a small number of edge relations, in this paper we define totally 32 edge types. For example, the `Fields' in the OAG are categorized into six levels from $L_0$ to $L_5$, which is organized with a hierarchical tree (We use `Within' and `Within$^{-1}$' to represent such hierarchy). Therefore, we differentiate the `Paper-Field' edges in the corresponding field levels. We also differentiate the different author order and venue type as well. In addition to direct edge, we also add some higher-order edges to enrich the graph. For example, a paper can have multiple authors, and thus the `A-P-A' path can represent co-authorship. We also use `A-P-F', `A-P-V', `I-A-P-A-I' paths to augment the graph. The `Self' type corresponds to self-loop connection, which is widely added in GNN architecture. Despite `Self' and `CoAuthor' edge relationship, which are symmetric, all other edge type will have a reverse edge type. \n\nTo test the generalization of our proposed model, we construct two main fields of OAG, the Computer Science (CS) and Medicine (Med). We select the papers with $L_0$ field as CS or Med, and reconstruct two domain-specific datasets CS-OAG and Med-OAG associated with these papers. We also test the result for the whole graph, as All-OAG. The statistics of these three datasets are shown in table~\\ref{tab:stat}. The All-OAG contains 0.178 billion nodes and 3.01 billion edges, spanning papers from 1900 to 2019. Till now, this is the largest-scale graph data to evaluate performance of GNNs, which is far more distinguishable than previously wide-adopted small citation graph, such as Cora, Citeseer and Pubmed~\\cite{gcn}, which only contain thousands of nodes.\n\n\\subsection{Implementation and Baselines}\n\nTo test whether our proposed method can learn good representation for each type of nodes in web-scale heterogeneous graphs, we choose four different real-world downstream tasks as evaluation: Paper-Field (L1), Paper-Field (L2), Paper-Venue, and Author Disambiguation. In the first three tasks, we give a model a paper and wants it to predict the correct L1, L2 field it belongs to or the venue it's published on. We model such problem as a node classification problem, where we use GNN to get the contextual node representation of the paper, and use a softmax output layer to get its classification label. For author disambiguation, we pick all the authors with same name, and the papers that link to one of these same-name author nodes. The task is to conduct link prediction between paper and candidate authors. After getting the paper and author node representation from GNN, we use a DistMult operator~\\cite{DBLP:journals/corr/YangYHGD14a} to get the probability of each author-paper pair to be linked. We choose NDCG and MRR, which are two widely adopted ranking metric~\\cite{DBLP:books/daglib/0027504, DBLP:series/synthesis/2014Li}, as the evaluation measures.\n\nFor all these tasks and all datasets, we use papers published before year 2015 as training, between 2015 and 2016 for validation, and paper after 2016 as testing. For each node or pair of nodes we are considering, we use the sampling algorithm discussed in Section~\\ref{sec:train} to get a sub-graph, and remove out the link we want to predict (e.g. Paper-Field link) from this graph to avoid data leakage.\n\nAs we don't assume the feature of each data type belongs to the same distribution, we are free to use the most appropriate feature to represent each type of nodes. For paper and author nodes, the node number is extremely large. Therefore, traditional node embedding algorithm is not suitable for extracting features for them. We therefore resort to the paper title as feature. For each paper, we get their title text, and use a pre-trained XLNet~\\cite{xlnet, wolf2019transformers} to get representation of each word in the title. We then average them weighted by each word attention to get title representation for each paper. The initial feature of each author is simply an average of his/her published paper's embedding. For field, venue and institute, the node number is small and we can train a node embedding by reflect sub-network structure. \\zn{which graph to train on, which network embedding algorithm? }\n\\zn{Add a table to describe dataset and task statistics if there is some space left}\n\nWe compare HGT with some state-of-art graph neural networks. All these baselines as well as our own model\\footnote{The dataset is publicly available at \\url{https://www.openacademic.ai/oag/}, and the code and trained-models will be open-sourced upon publication.} are implemented via PyTorch Geometric (PyG) package~\\cite{pyG}, a GNN framework that supports fast training via graph gather/scatter operation. The first class of GNNs are designed for homogeneous graphs, including:\n\\begin{itemize}\n    \\item Graph Convolutional Networks (GCN)~\\cite{gcn}, which simply average the neighbor's embedding followed by linear projection. We use the implementation provided in PyG~\\footnote{\\url{https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/graph_conv.html}}.\n    \\item Graph Attention Networks (GAT)~\\cite{gat}, which adopts multi-head additive attention on neighbors. We use the implementation provided in PyG~\\footnote{\\url{https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/gat_conv.html}}. \n\\end{itemize}\nWe also compared with GNNs that dedicatedly designed for heterogeneous graphs, including:\n\\begin{itemize}\n    \\item Relational Graph Convolutional Networks (RGCN)~\\cite{DBLP:conf/esws/SchlichtkrullKB18}, which keeps a different weight for each relationship, i.e., a relation triplet. We use the implementation provided in PyG~\\footnote{\\url{https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/rgcn_conv.html}}.\n    \\item Heterogeneous Graph Neural Networks (HetGNN)~\\cite{DBLP:conf/kdd/ZhangSHSC19}, which adopts different Bi-LSTMs for different node type for aggregating neighbor information. We re-implement this model in PyG following the authors' official code~\\footnote{\\url{https://github.com/chuxuzhang/KDD2019_HetGNN}}.\n    \\item Heterogeneous Graph Attention Networks (HAN)~\\cite{DBLP:conf/www/WangJSWYCY19}, which adopts two layers of attentions to aggregate neighbor information via different meta paths. We re-implement this model in PyG following the authors' official code~\\footnote{\\url{https://github.com/Jhy1993/HAN}}.\n\\end{itemize}\n\nTo further examine whether the components in our model can indeed exploit heterogeneity and temporal dependency, and eventually benefit downstream performance, we also propose two baselines as ablation study:  HGT$_{\\text{noHeter}}$, which uses a same set of weight for all meta relation, and HGT$_{\\text{noTime}}$, which removes the relative temporal encoding component. \n\nAs most of these graph neural network baselines assume the node features belong to the same distribution, while our feature extraction doesn't fulfill this assumption, if we directly feed the feature into these different baselines, they are unlikely to achieve good performance. To make fair comparison, for all the models, we add an adaptation layer between the input feature and the GNNs. This module simply conducts different linear projection for nodes in different node types. Such procedure can be regarded to map heterogeneous data into same distribution, which is also adopted in~\\cite{DBLP:conf/kdd/ZhangSHSC19, DBLP:conf/www/WangJSWYCY19}. We set the output dimension of such module as 256, and use as the hidden dimension throughout the networks for all baselines. For all multi-head attention-based methods, we choose head number as 8. All the GNNs keep 3 layers, so that the receptive fields of each network is exactly the same. All the GNNs are optimized via AdamW optimizer~\\cite{DBLP:conf/iclr/LoshchilovH19} with Cosine Annealing Learning Rate Scheduler~\\cite{DBLP:conf/iclr/LoshchilovH17}. For each model, we train it for 200 epochs, select the one with lowest validation loss as the best model. We train the model for 5 times and calculate the mean and standard variance of test performance. \n\n\\subsection{Performance Analysis}\n\nTable ~\\ref{tab:result} summarizes the performance of HGT and baselines for different datasets and tasks. The results show that our proposed HGT significantly enhance the performance for all tasks. Compared to the current state-of-the-art method, HAN~\\cite{DBLP:conf/www/WangJSWYCY19}, the average relative NDCG improvements (i.e., the performance difference  divided  by the baseline  performance) of HGT on CS-OAG, Med-OAG and All-OAG datasets are 24.0$\\%$, 17.5$\\%$ and 14.6$\\%$ respectively. Moreover, HGT has less parameters than all the heterogeneous graph neural network baselines, including RGCN, HetGNN and HAN. This shows that by modelling relationship by its meta schema, it's possible to have better generalization with few resource consumption. \n\nThe two most important components of HGT is the meta relation parametrization and temporal encoding. To further analyze their effect, we conduct ablation study by removing each component: HGT$_{\\text{noHeter}}$ only maintains a single set of parameter for all relations, which is equivalent to vanilla Transformer applied on graph, while HGT$_{\\text{noTime}}$ discards the relative temporal encoding. We can see that after removing these two components, the NDCG performance drop 7.4$\\%$ and 3.8$\\%$ respectively, indicating the importance of both heterogeneous weight parametrization and temporal encoding. Among these two components, heterogeneity is more informative for the academic graph modelling. Also, we try to implement a baseline that keeps a different weight matrix for each relation. However, such a baseline contains too many parameters so that our experimental setting doesn't have enough GPU memory to optimize it. This also indicates that using heterogeneous schema to parametrize weight matrics can achieve good performance with fewer resources.\n\n\\subsection{Case Study: Venue Similarity Evolution}\n\nTo further evaluate how our proposed model can capture graph dynamics, we conduct a case study showing the evolution of conference similarity. We select 100 conferences in computer science with highest citation, assign them three different timestamps, i.e., 2000, 2010 and 2020, and construct sub-graphs initialized by them. Using a trained HGT, we can get venue representations for these conferences, with which we can calculate their mutual distance with euclidean distance. We select WWW, KDD, NeurIPS and ACL as illustration. For each of these conference, we pick top-5 most similar venues (i.e., with smallest euclidean distance) with it, showing how the conference topic might change. \n\nAs is shown in table~\\ref{tab:case}, these venues's relationship has changed from 2000 to 2020. For example, in 2000, WWW is more related to some database conferences, such as SIGMOD and VLDB, and some network conferences, such as NSDI and GLOBECOM. While WWW in 2020 becomes more related to some data mining and information retrieval conferences as KDD, SIGIR and WSDM. Also, KDD in 2000 is \nmore related with some traditional database and data engineering conferences, including SIGMOD and ICDE. While in 2020, it has a trend to correlates with some machine learning conferences such as NeurIPS, AAAI. Also, our model can capture the difference brought by new conference. For example, NeurIPS in 2020 is relates with ICLR, which is a newly organized conference. This case study shows that our proposed relative temporal encoding mechanism can help capture the temporal differences and dependency.\n\n\\begin{table}[th]\n\\centering\n\\begin{tabular}{ccc} \n\\toprule\nVenue & Time & Top$-$5 Most Similar Venues \\\\\n\\midrule\n\\multirow{3}{*}{WWW} & 2000 & SIGMOD, VLDB, NSDI, GLOBECOM, SIGIR\\\\\n~& 2010 & GLOBECOM, KDD, CIKM, SIGIR, SIGMOD\\\\\n~& 2020 & KDD, GLOBECOM, SIGIR, WSDM, SIGMOD\\\\\n\\midrule\n\\multirow{3}{*}{KDD} & 2000 & SIGMOD, ICDE, ICDM, CIKM, VLDB\\\\\n~& 2010 & ICDE, WWW, NeurIPS, SIGMOD, ICML\\\\\n~& 2020 & NeurIPS, SIGMOD, WWW, AAAI, EMNLP\\\\\n\\midrule\n\\multirow{3}{*}{NeurIPS} & 2000 & ICCV, ICML, ECCV, AAAI, CVPR\\\\\n~& 2010 & ICML, CVPR, ACL, KDD, AAAI\\\\\n~& 2020 & ICML, CVPR, ICLR, ICCV, ACL\\\\\n\\midrule\n\\multirow{3}{*}{ACL} & 2000 & EMNLP, NAACL, COLING, AAAI, ICASSP\\\\\n~& 2010 & EMNLP, NeurIPS, AAAI, NAACL, CVPR\\\\\n~& 2020 & AAAI, ICML, EMNLP, IJCAI, ICASSP\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Temporal Evolution of Conference Similarity.} \n\\label{tab:case} \n\\end{table}\n\n\\subsection{Visualize Meta Relation Attention}\nTo illustrate how the incorporated meta relation schema can benefit the GNN message passing, we pick out the largest attention value associated with each relation schema in first two layers, and plot the meta relation attention hierarchy tree. As is shown in figure~\\ref{fig:meta}, to calculate a paper's representation, $\\langle$Paper, Pub$_{\\text{conf}}$, Venue, Pub$_{\\text{conf}}^{-1}$, Paper$\\rangle$, $\\langle$Paper, In$_{L_2}$, Field, In$_{L_5}^{-1}$, Paper$\\rangle$ and $\\langle$Field, In$_{L_1}$, Author, Write$_{\\text{lastLast}}$, Paper$\\rangle$ are the three most important meta relation sequence, or can be regarded as meta path. Similar result is shown for author. Such visualization indicates that HGT can automatically learn to construct important meta paths for a specific downstream task, without specific domain knowledge.\n\n \\begin{figure}[ht!]\n    \\centering\n    \\includegraphics[width=0.5\\textwidth, trim = 30 0 50 0,clip]{picture/meta.png}\n    \\caption{Hierarchy tree of learned meta relation attention.}\n    \\label{fig:meta}\n\\end{figure} \n\n}%end of hide\n\n\\section{Conclusion}\\label{sec:conclusion}\n\nIn this paper, we propose the \\model\\ (\\short) architecture for modeling Web-scale heterogeneous and dynamic graphs. \nTo model heterogeneity, we use the meta relation $\\langle \\tau(s), \\phi(e), \\tau(t) \\rangle$ to decompose the interaction and transform matrices, enabling the model to have the similar modeling capacity with fewer resources.\nTo capture graph dynamics, we present %the \\dynamic\\ model. \nthe relative temporal encoding  (RTE) technique to incorporate temporal information using limited computational resources. \nTo conduct efficient and scalable training of \\short\\ on Web-scale data, we design the heterogeneous Mini-Batch graph sampling algorithm---\\sampling. \nWe conduct comprehensive experiments on the Open Academic Graph, and show that the proposed \\short\\ model can capture both heterogeneity and outperforms all the state-of-the-art GNN baselines on various downstream tasks.\n\nIn the future, we will explore whether \\short\\ is able to generate heterogeneous graphs, e.g., predict new papers and their titles, and whether we can pre-train \\short\\ to benefit tasks with scarce labels.\n\n\\vpara{Acknowledgements.} We would like to thank Xiaodong Liu for helpful discussions. This work is partially supported by NSF III-1705169, NSF CAREER Award 1741634, NSF\\#1937599, Okawa Foundation Grant, and Amazon Research Award.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{GFS: Graph-based Feature Synthesis for Prediction over Relational Databases}\n\n\\begin{document}\n\n\\title{GFS: Graph-based Feature Synthesis for Prediction over Relational Databases}\n\n\\author{Han Zhang$^{1}$, Quan Gan$^{2}$, David Wipf$^{2}$, Weinan Zhang$^{1}$}\n\\affiliation{\n  \\institution{$^{1}$Shanghai Jiao Tong University, $^{2}$AWS Shanghai AI Laboratory}\n}\n\\email{han.harry.zhang@gmail.com, {quagan, daviwipf}@amazon.com, wnzhang@sjtu.edu.cn}\n\\newcommand{\\david}[1]{{\\color{red} [David: ``#1'']}}\n\\newcommand{\\han}[1]{{\\color{blue} [Han: ``#1'']}}\n\\newcommand{\\quan}[1]{{\\color{olive} [Quan: ``#1'']}}\n\\newcommand{\\weinan}[1]{{\\color{magenta} [Weinan: ``#1'']}}\n\\newcommand{\\minjie}[1]{\\color{yellow} [Minjie: ``#1'']}\n\n\\newcommand{\\model}{{GFS}}\n\n\\newtheorem{remark}{Remark}\n\n\\def\\header{\\vspace{2.5mm} \\noindent}\n\n\\begin{abstract}\nRelational databases are extensively utilized in a variety of modern information system applications, and they always carry valuable data patterns. There are a huge number of data mining or machine learning tasks conducted on relational databases. However, it is worth noting that there are limited machine learning models specifically designed for relational databases, as most models are primarily tailored for single table settings. Consequently, the prevalent approach for training machine learning models on data stored in relational databases involves performing feature engineering to merge the data from multiple tables into a single table and subsequently applying single table models. This approach not only requires significant effort in feature engineering but also destroys the inherent relational structure present in the data.  To address these challenges, we propose a novel framework called Graph-based Feature Synthesis (\\model{}). \\model{} formulates the relational database as a heterogeneous graph, thereby preserving the relational structure within the data. By leveraging the inductive bias from single table models, \\model{} effectively captures the intricate relationships inherent in each table. Additionally, the whole framework eliminates the need for manual feature engineering. In the extensive experiment over four real-world multi-table relational databases, \\model{} outperforms previous methods designed for relational databases, demonstrating its superior performance.\n\\end{abstract}\n\n\\maketitle\n\\pagestyle{plain}\n\\begingroup\n\\renewcommand{\\thefootnote}{}\n\\footnote{\\noindent \\emph{Preprint}, under review}\n\\addtocounter{footnote}{-1}\n\\endgroup\n\n\\section{Introduction}\n\nData mining originates from the tasks of mining useful patterns from databases. \\emph{Column prediction}, where a model is trained to predict the values in a certain \\emph{target column} in a single \\emph{target table} of the database, is an important task. A wide range of applications, such as click-through-rate prediction\\cite{guo2017deepfm, cheng2016wide, zhou2018deep, guo2022miss}, anomaly detection\\cite{han2022adbench, liu2008isolation, zhao2018xgbod, tang2022rethinking}, frequent pattern mining \\cite{han2007frequent, agrawal1994fast,han2000mining}, etc., can all be formulated as column prediction.\n\nHowever, almost all previous data mining works focus the scope of column prediction on a single table. If there are multiple tables and a schema, a common approach is to first merge these tables into a single one, and then perform machine learning methods given the merged single table \\cite{park2022end, rendle2010factorization,qu2018product}.\nThese approaches rely on manually joining these tables into a single table as a feature engineering step. Executing manual feature engineering necessitates a significant amount of effort and substantial domain expertise. An often-cited statistic is that data scientists spend 80\\% of their time integrating and curating their data \\cite{create2020}. Moreover, it can destroy the inherent relational structure embedded within the data, potentially leading to a loss of valuable information. The need and interest in mining information from various domains of relational databases have been growing, with a significant emphasis on reducing human effort and minimizing information loss.\n\nTo this end, we propose a general novel framework called Graph-based Feature Synthesis (\\model{}).\n\\model{} is an embedding update and prediction framework that can plug in any differentiable model designed for single table settings \nas base model or embedding function to finalize the specific model.\nBy leveraging the inductive bias from single table models, \\model{} effectively captures the intricate relationships inherent in each table. \\model{} formulates the relational database as a heterogeneous graph, thereby preserving the relational structure within the data. Additionally, the whole framework eliminates the need for manual feature engineering.\n\nSome existing solutions address a similar issue, yet they possess certain shortcomings that can be remedied by the design of the \\model{} framework.\nAutomated methods represent a category of techniques that consolidate multiple tables into a single table using predefined rules \\cite{kanter2015deep, lam2017one, arda, liu2022feature, galhotra2023metam}. DFS (Deep Feature Synthesis) \\cite{kanter2015deep}, officially implemented by Featuretools \\cite{featuretools}, is the most remarkable representative of the offline methods. %\nIt utilizes depth-first search to aggregate or join columns from other tables to the target table. %\nDespite its conciseness, DFS has several inherent problems to be addressed.\n(1) DFS only offers rule-based aggregation functions such as \\textit{mean}, \\textit{max}, and \\textit{min} for continuous features, or \\textit{mode} for categorical features. Such a limited set of aggregation functions reduces the overall effectiveness of the method and results in the loss of information during the aggregation process.\n(2) The search process employed by DFS relies on depth-first search, making the results sensitive to the order in which the children tables are traversed for a given parent table.  As we show in Section~\\ref{sec:property}, it may ignore some join paths that could be crucial for the downstream task, even if the search depth is increased.\n(3) DFS aggregates every column of the children table using several aggregation functions and appends it to the parent table. Consequently, the number of columns appended to the parent table is multiplied by the number of aggregation functions used. This results in an exponential increase in the number of columns when the depth of the search is deep or when the children table contains a large number of columns.\n\nRecently, methods that try to convert the database into a graph and apply Graph Neural Networks (GNNs) are also developed. When making prediction of a single data instance, GNNs are able to use information of other related data instances.  RDB2Graph (Relational Database to Graph) \\cite{cvitkovic2020supervised} is one of the most representative approaches, which constructs a graph by %\ntreating each row as a node, each foreign key reference as an edge, and running a GNN on top of the raw input features.  %\nHowever, RDB2Graph has several drawbacks:\n(1) RDB2Graph solely relies on an MLP to form the initial node embeddings for each row, which limits its ability to explore the interaction patterns among columns effectively. In contrast, there exist a number of single table models that excel in capturing such column interactions \\cite{rendle2010factorization,guo2017deepfm,qu2018product}.\n(2) The GNN model utilized by RDB2Graph is not specifically designed for tabular data, and its simplicity may result in suboptimal performance when handling relational databases.\n(3) GNN models are notoriously susceptible to an over-smoothing problem when the number of layers becomes too large. However, in complex relational databases, capturing information from all tables necessitates constructing a substantial number of layers, exacerbating this problem.\n\nIn this paper, we propose a novel framework called Graph-based Feature Synthesis (\\model{}) which offers several advantages over existing approaches.  For example, in comparison to DFS, \\model{}  introduces several key improvements:\n(1) Learnable aggregation functions: Instead of being limited to rule-based aggregation functions, \\model{} enhances the capability by incorporating parameterized learnable functions, which provide increased flexibility and power in capturing complex relationships.\n(2) Removing sensitivity to traversal order: \\model{} is not affected by the order in which tables are traversed, which ensures more consistent and reliable results regardless of the sequence in which traversal occurs.  In contrast, we prove that DFS is \\textit{not} similarly invariant.\n(3) Complete coverage of all join paths: Unlike DFS, \\model{} is capable of covering all join paths within a specified number of hops, which means that no important traversal path will be missed, enabling a comprehensive modeling of the relational structure. \n(4) Controlled column growth: Rather than appending every column from child tables, \\model{} simply appends the row embedding of each row to the parent table. This significantly alleviates the issue of exponential column growth, yielding a more manageable and efficient representation.\n\nTurning to RDB2Graph, \\model{} also exhibits several notable improvements:\n(1) Enhanced row embeddings using powerful single table models: instead of relying solely on MLPs, \\model{} incorporates well-designed and powerful single table models to generate row embeddings. %\n(2) Improved prediction using advanced single table models: we utilize powerful single table models for the final prediction step in \\model{}. %\n(3) Input residual connections to mitigate over-smoothing \\cite{alon2020oversquashing,chen2020oversmoothing} to address the over-smoothing problem commonly associated with GNNs, we incorporate residual connections between the raw node features and every layer. %\n\nTo sum up, the main contributions of this paper are threefold:\n\\begin{itemize}[leftmargin=*]\n    \\item After illustrating the need for models specifically designed to handle \\emph{column prediction tasks} within relational databases, we propose the novel and generic \\model{} framework. Based on modular components, \\model{} can incorporate any\n    differentiable model designed for single table settings as an embedding function and/or prediction head.  In so doing we can equip \\model{} with the strong inductive biases of existing single table models, and benefit from future advances in this field.\n    \\item Relative to alternative representative paradigms in the mold of DFS and RDB2Graph, \\model{} offers targeted improvements, such as invariance to traversal order, greater expressiveness, and oversmoothing mitigation.\n    \\item Experiments conducted on four real-world relational datasets demonstrate that \\model{} outperforms existing methods, while ablation studies validate the importance of each \\model{} component.\n\\end{itemize}\n\nThe remainder of the paper is organized as follows. Section \\ref{sec:related_works} first introduces more related work, and Section \\ref{sec:problem_definition} then introduces the formal task definition along with background terminology and notation.  Next, Section \\ref{sec:model} presents our detailed \\model{} framework, followed by Section \\ref{sec:property} which provides theoretical comparisons between \\model{} and two key baseline alternatives, DFS and RDB2Graph. Finally, in-depth experiments and conclusions are presented in Sections \\ref{sec:experiments} and \\ref{sec:conclusion} respectively.\n\n\\section{Related Works}\n\\label{sec:related_works}\nThere is a significant demand in the industry for prediction tasks on relational databases \\cite{netz2000integration, park2022end, aggarwal2012analysis, shahbaz2010data, fernandez2018aurum}.\nHowever, it is true that there has been relatively limited research specifically focusing on prediction in the context of relational databases.\n\n\\header\n\\textbf{Feature Engineering.}\nIn industry, the prevailing approach for prediction tasks involves manual feature engineering on the relational database, followed by the application of a single table model on the resulting single table\\cite{park2022end, rendle2010factorization,qu2018product}.\nThis method requires significant effort in data extraction and feature engineering. Additionally, it heavily relies on the specific task and dataset at hand, necessitating a deep understanding of domain knowledge. However, these efforts are costly and have the drawback of potentially undermining the crucial relational structure inherent in the data.\n\n\\header\n\\textbf{Automatic Feature Engineering.}\nIn the field of automating feature engineering in relational databases, several notable works have been introduced, including DFS \\cite{kanter2015deep}, OneBM (One Button Machine) \\cite{lam2017one}, R2N \\cite{lam2018neural}, ARDA \\cite{arda}, METAM \\cite{galhotra2023metam} and AutoFeature \\cite{liu2022feature}. DFS \\cite{kanter2015deep}, being the most widely used and open-source method, employs depth-first search to aggregate or join columns from other tables to the target table using rule-based aggregation functions. However, as mentioned in the introduction, DFS has certain limitations that we have addressed. \nOneBM shares similarities with DFS as it also utilizes rule-based aggregation functions. However, it improves upon DFS by enumerating the traversal paths, thereby reducing the variance issue to some extent. On the other hand, R2N extends the rule-based aggregation function to LSTM \\cite{hochreiter1997long}. ARDA is another system that automates the data augmentation workflow and uses feature selection. AutoFeature arguments feature from candidate tables to the target table following an exploration-exploitation strategy with a reinforcement learning-based framework. METAM is a novel goal-oriented framework that queries the downstream task with a candidate dataset, forming a feedback loop that automatically steers the discovery and augmentation process. \n\n\\header\n\\textbf{Graph-based Methods.}\nIn the introduction, we analyze RDB2Graph, which represents a preliminary attempt at utilizing GNNs for prediction tasks in relational databases. GNNs are neural networks specifically designed for data structured as graphs. Typically, GNN architectures involve three main steps: node embedding initialization, message passing, and readout.\nIn the node embedding initialization step, each node is assigned an initial node embedding vector. These initial embeddings capture the initial representations of the nodes.\nThe message-passing step involves nodes utilizing their own node embedding vectors, as well as those from their neighboring nodes, to update their own node embeddings. This allows information to be propagated and aggregated across the graph.\nThe readout part employs the final node embeddings to perform predictions, with the specific approach depending on the downstream task.\nIn the context of a relational database, it is natural and intuitive to consider the database as a heterogeneous graph. This perspective allows for the application of GNN architectures designed specifically for heterogeneous graphs. Several popular examples of such architectures are the RGCN (Relational Graph Convolution Network) \\cite{schlichtkrull2018modeling}, attention-based RGAT (Relational Graph Attention Network) \\cite{busbridge2019relational}, transformer-based HGT (Heterogeneous Graph Transformer) \\cite{hu2020heterogeneous}. \nFurthermore, Cvetkov et al. \\cite{cvetkov2023relational} propose a knowledge graph method that utilizes tables alongside the target table to enhance the features of the target table. However, the results presented in that paper demonstrate that DFS performs best in most datasets, remaining a highly effective baseline. Additionally, ATJ-Net\\cite{bai2021atj} constructs hypergraphs to fuse related tables and use architecture search to prone the model.\n\n\\header\n\\textbf{Single table models.}\nWhile models specifically designed for relational databases are comparatively rare, numerous models have been proposed for single-table settings. Among these, tree-based models such as Gradient Boosting Decision Trees (GBDT) \\cite{friedman2001greedy} and Factorization Machines (FM) \\cite{rendle2010factorization} have gained significant attention. Variants of FM, including DeepFM \\cite{guo2017deepfm} and Wide \\& Deep \\cite{cheng2016wide}, are particularly popular in industrial applications and other feature interaction model such as ARM-Net\\cite{cai2021arm}, DCN-V2\\cite{wang2021dcn}. Graph-based models such as EmbDI\\cite{create2020}, TabularNet\\cite{du2021tabularnet}. Furthermore, transformer-based single table models have also recently emerged, such as FT-Transformer \\cite{gorishniy2021revisiting}, TabTransformer \\cite{huang2020tabtransformer}, TURL \\cite{deng2022turl} and RCI \\cite{glass2021capturing}.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/example_figure1.pdf}\n    \\caption{Example schema from AVS dataset. PK stands for primary key, FK for foreign key, rest are other columns.}\n    \\label{fig:AVS}\n\\end{figure}\n\n\\section{Problem Definition}\\label{sec:problem_definition}\n\n\\begin{table*}\n    \\caption{The mapping from a relational database to a graph.}\n    \\centering\n    \\begin{tabularx}{0.9\\textwidth}{p{8cm}l}\n        \\toprule\n        \\textbf{Relational Database} & \\textbf{Graph} \\\\\n        \\midrule\n        Row & Node \\\\\n        Foreign key reference from $X^{A}_{u,i}$ to $X^{B}_{v,j}$ & Directed edge from node $u$ of type $A$ to node $v$ of type $B$ \\\\\n        Table & Node type \\\\\n        Foreign key column & Edge type \\\\\n        Non-Foreign key or Primary key column & Node raw feature \\\\\n        \\bottomrule\n    \\end{tabularx}\n    \\label{tab:rel2graph}\n\\end{table*}\n\nLet $D=\\{X^1, X^2, \\ldots , X^N\\}$ denote a relational database, where each $X^n$ represents the $n$-th consituent table. An example is shown in Fig. \\ref{fig:AVS}.\nThe $i$-th row and $j$-th column of table $X^n$ are denoted as $X^{n}_{i,:}$ and $X^{n}_{:, j}$ respectively. In this way, $X^{n}_{i,j}$ represents the entry value of row $i$ and column $j$ of table $X^n$. We use $C_n$ and $R_n$ to denote the number of columns and rows in $X^n$, respectively. Proceeding further, two tables relate to each other in one or two ways: \\textit{forward} or \\textit{backward}, similar to \\cite{kanter2015deep}, as described next.\n\n\\header\n\\textbf{Forward}: Suppose that there is a foreign key reference from $X^{A}_{:,i}$ to $X^{B}_{:,j}$ and $X^{B}_{:,j}$ is the primary key in $X^B$. Then a \\textit{forward} relationship is from one row $X^{A}_{m, :}$ to another row $X^{B}_{l, :}$ since $X^{A}_{m, i}$ has foreign key reference to $X^{B}_{l, j}$, i.e., $X^{A}_{m, i} = X^{B}_{l, j}$  \nWe can state that $X^A_{m,:}$ has forward relationship to $X^B_{l,:}$ or table $X^{A}$ has a forward relationship to $X^{B}$, and actually each foreign column in $X^{A}$ represents one type of forward relationship. In the example shown in Fig.~\\ref{fig:AVS}, the table \\textit{History} has a forward relationship to table \\textit{Offer}, where each row in \\textit{History} represents a customer, and one customer only refers to one offer.\n\n\\header\n\\textbf{Backward}: A \\textit{backward} case refers to the relationship from one row $X^{B}_{l, :}$ to all the rows $\\{X^{A}_{m_1, :}, X^{A}_{m_2, :}, \\ldots ,X^{A}_{m_n, :}\\}$ that have forward relationship to $X^{B}_{l, :}$ due to the foreign key reference from $X^{A}_{:,i}$ to $X^{B}_{:,j}$. In the same example as above, the table \\textit{Offer} has a backward relationship to \\textit{History}, and many customers can have the same offer.\n\nWith all the necessary terminology and notations now clearly defined, we proceed to introduce the specific task that this paper addresses. A wide range of data mining problems over a relational database $D$ can be formulated as a \\emph{column prediction} task, which will be our focus. We formulate such problems as follows: \n\\begin{definition}[Column Prediction Task on Relational Database] \\label{def:basic_problem}\n    Given a relational database $D$ and all the relationships between tables, predict the values in a target column $X^{T}_{:, \\text{target}}$ of a \\emph{target table} $X^{T} \\in D$ of interest using relevant information available in the database.%\n\\end{definition}\n\n\\section{The \\model{} Framework}\\label{sec:model}\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/GFS1.pdf}\n    \\caption{Overview of the \\model{} framework. We use the node highlighted in red as an example to demonstrate how the node in the graph updates its node embedding and how predictions are made if the node is the target node. Some notations are abbreviated and some columns in tables are not shown for simplicity.}\n    \\label{fig:GFS}\n\\end{figure*}\nWe now present the technical details of the proposed \\model{} framework designed to tackle the column prediction problem specified by Definition \\ref{def:basic_problem}.  To this end, we first describe the process of converting a relational database to a graph, the nodes of which have learnable embeddings.  After discussing the initialization of these embeddings, we then proceed to the core training and inference steps, namely, message passing and label prediction.  The overview of \\model{} is shown in Fig.~\\ref{fig:GFS}, with the associated pseudo-code in Algorithm~\\ref{algo:GFS} below. \n\n\\subsection{Interpreting a Relational Database as a Graph}\n\nA relational database can be interpreted as a heterogeneous directed graph, with the correspondences depicted in Table \\ref{tab:rel2graph}.  We consider each row of a table as a separate node in the entire graph, and all rows within a table are treated as nodes of the same type. If there is a foreign key reference from $X^{A}_{u,i}$ to $X^{B}_{v,j}$, it implies the existence of a directed edge from node $u$ of type $A$ to node $v$ of type $B$. \nAdditionally, foreign key references within the same foreign key column are considered as edges of the same type. For instance, in the schema example of Fig.~\\ref{fig:AVS}, all foreign key references from the table \\textit{Transaction} to the table \\textit{History} will be categorized as the same edge type.\n\nAfter the heterogeneous directed graph is constructed, we then add reverse edges for each edge in order to let the whole graph become a heterogeneous undirected graph which is more suitable for aggregating the information from other tables to the target table.\n\n\\header\n\\textbf{Node (Row) Raw Features.} The values of a row in columns other than primary and foreign keys are regarded as node(row) raw features. Examples include the values in the \\textit{quantity, offervalue, company\\_id} columns from table \\textit{Offer} in Fig. \\ref{fig:AVS}. We denote the raw node features associated with example row $X_{i,:}^n$ as \n\\begin{equation}\n    x=[X_{i,1}^n, X_{i,2}^n, \\ldots, X_{i,C_n}^n]~.\n\\end{equation}\n\n\\header\n\\textbf{Node (Row) Embedding.} The low-dimensional vector which encodes node-wise information is defined as a node embedding. We denote the node embedding associated with $X_{i,:}^n$ as $h(i, n)\\in \\mathbb{R}^d$, \nwhere $n$ references the table $X^n$ and $i$ represents the corresponding intra-table row.\n\n\\subsection{Embedding Initialization}\nWe allow for three types of data in our framework, namely, \\textit{categorical}, \\textit{continuous}, and \\textit{date}. To facilitate follow-up calculations, raw node features of each type are first transformed into dense real-valued embedding vectors.\n\n\\header\n\\textbf{Categorical Values.} \nCategorical values $c_i\\in \\mathbb{Z}$ are embedded into $e_i\\in \\mathbb{R}^{d}$ via a column-wise look-up table, mapping each categorical value to a vector. And different columns that share the same semantic meaning will use the same look-up table. In this way, in Fig.~\\ref{fig:AVS} the same categorical value $c_i$ from the \\textit{chain\\_id} column in table \\textit{Transaction} and from the table \\textit{History} will share the same embedding vector $e_i$.\n\n\\header\n\\textbf{Continuous Values.} \nFor a continuous value $c_i\\in \\mathbb{R}$ within a given column, we first normalize to the interval $[0, 1]$.  We then apply the transform \n\\begin{equation}\n    e_i = \\text{ReLU}(c_i\\times w_f+b_f) \\in \\mathbb{R}^{d}~,\n\\end{equation}\nwhere $w_f, b_f\\in \\mathbb{R}^{d}$ are the weight and bias of a linear transformation layer in column $f$, and all the continuous values in one column share the same weights and biases.\n\n\\header\n\\textbf{Dates.}\nWe encode one date column into four columns, namely \\textit{year, month, day of month, day of week}, and treat the values in each column as categorical.\n\n\\header\nIn aggregate across feature types, we convert the example row $X^{n}_{i,:}$'s raw features $x$ into dense real-valued embedding vectors as \n\\begin{equation}\n    E_{\\theta}(x)\\in \\mathbb{R}^{C_n\\times d}~,\n\\end{equation}\nwhere $\\theta$ represents the learnable parameters of the embedding process.\n\n\\subsection{Message Passing}\\label{sec:message_passing}\nWe introduce message passing steps by taking the node corresponding to row $i$ in table $X^n$ as an example and use $x$ to denote the row raw feature as above. We define $F=\\{X^{f_1}, X^{f_2},\\ldots, X^{f_{|F|}}\\}$ for all the tables $X^{n}$ has forward relationship to. Analogously we define $B=\\{X^{b_1}, X^{b_2},\\ldots, X^{b_{|B|}}\\}$ for backward relationships. Note that both sets can have repetitive elements since $X^n$ can have multiple types of forward or backward relationships with one table.\n\n\\header\n\\textbf{Node (Row) Embedding Function.}\nAfter transforming the node raw feature $x$ to the dense real-valued embedding vectors and gathering information from other node(will explain later), we adopt the node embedding function\n\\begin{equation}\n    N_{\\phi}: \\mathbb{R}^{(C_n+|F|+|B|)\\times d}\\rightarrow \\mathbb{R}^{d}\n\\end{equation}\nto obtain the node embedding, where \n$\\phi$ represents trainable parameters and the input dimension will be \n further explained in the Message Passing Function part. The node embedding function can be similar to any single table model such as an $\\text{MLP}$, $\\text{FM}$~\\cite{rendle2010factorization}, or $\\text{FT-Transformer}$~\\cite{gorishniy2021revisiting}. Additionally, nodes of the same type (rows in the same table) will share the same node (row) embedding function.\n\n\\header\n\\textbf{Aggregation Function. }\nFor the table that $X^{n}$ has a backward relationship to, we need to aggregate the node embedding of the nodes which relate to node $X^{n}_{i,:}$. This function takes a set of node embedding $M$, and the output is a vector of dimension $d$:\n\\begin{equation}\n    \\text{Agg}_{\\psi}: \\mathbb{M}\\rightarrow\\mathbb{R}^{d}~,\n\\end{equation}\nwhere $\\psi$ is the learnable parameter of the aggregation function $\\text{Agg}_{\\psi}$, $\\mathbb{M}$ is the set satisfies that any set of node embedding $M\\in\\mathbb{M}$. Note that the aggregation function is independent of the number of node embedding in $M$ and the same aggregation function will be used for the same edge type (foreign key column).\n\nWe use an aggregation method that is similar to PNA \\cite{corso2020principal}. Specifically, we calculate the \\textit{mean, max, min} of the overall embedding in $M$ and multiply these three vectors three times with three different scalars: (1) \\textit{identity}: $S=1$ (2) \\textit{amplification}: $S\\propto \\log(degree)$ (3) \\textit{attenuation}: $S\\propto \\log(degree)^{-1}$. Subsequently, we obtain nine vectors and ultimately map all these vectors into a single vector. The diagram of this aggregation process is in Fig.~\\ref{fig:GFS}.\n\n\\header\n\\textbf{Message Passing Function.}\nWe will explain how the node in the whole graph gets the information from other nodes here.\n\nFirst, we initialize every embedding $h$ for all the nodes as $\\Vec{0}\\in \\mathbb{R}^d$, then we will update embedding $h$ for each node as \\eqref{eq:message} and \\eqref{eq:update}, the notation here is for the example node corresponding to row $i$ in table $X^n$:\n\\begin{align}\n\\text{Mes}(i, n, l):=& N_\\phi\\big(\\text{concat}[E_\\theta(x), \\tilde{h}^{l}(i, f_1), \\ldots , \\tilde{h}^{l}(i, f_{|F|}),\\label{eq:message}\\\\\n&\\text{Agg}_{\\psi_1}(M^{l}(i, b_1)), \\ldots,\\text{Agg}_{\\psi_{|B|}}(M^{l}(i, b_{|B|}))]\\big)~\\nonumber\\\\\nh^{l+1}(i, n) =& \\text{Mes}(i, n, l)~. \\label{eq:update}\n\\end{align}\n\nRepeating the above step $k$ times, we can get at most $(k-1)$-hops information. And we also define the search depth $K$ as the repeating times of above step.\n\\begin{itemize}[leftmargin=*]\n\\item We define $\\tilde{h}^{l}(i, t)\\triangleq h^l(i_t, t)$, and $i_t$ stands for the row number in table $X^t$ that $X^{n}_i$ has forward relationship to. $\\tilde{h}^{l}(i, f_1), \\ldots, \\tilde{h}^{l}(i, f_{|F|})$ are $l$-step results node embedding of the rows in $X^{f_1},\\ldots,X^{f_{|F|}}$ that $X^{n}_i$ has forward relationship to. \n\\item $M^{l}(i, b_1),\\ldots,M^{l}(i, b_{|B|})$ are $l$-step results node embedding sets of the rows in $X^{b_1},\\ldots,X^{b_{|B|}}$that $X^{n}_i$ has backward relationship to. For example, $X^{b_1}_{j_1, :}, X^{b_1}_{j_2, :},\\ldots,X^{b_1}_{j_m, :}$ are all the rows in $X^{b_1}$ that $X^{n}_i$ has backward relationship to, so $$M^{l}(i, b_1)=\\{h^{l}(j_1, b_1), h^{l}(j_2, b_1), \\ldots , h^{l}(j_m, b_1)\\}$$\n\\item $N_\\phi$ represents the node embedding function for each node, and you can also regard the node embedding function as the row embedding function for each row. Note that the dimension of $E_\\theta(x)$ is $C_n\\times d$, dimension of $[\\tilde{h}^{l}(i, f_1), \\ldots, \\tilde{h}^{l}(i, f_{|F|})]$ is $|F|\\times d$, dimension of $[\\text{Agg}_{\\psi_1}(M^{l}(i, b_1)), \\ldots,\\text{Agg}_{\\psi_{|B|}}(M^{l}(i, b_{|B|}))]$ is $|B|\\times d$ so after concatenating all these vectors, the input dimension of this function is $(C_n+|F|+|B|)\\times d$.\n\\end{itemize}\n\n\\subsection{Label Prediction}\\label{sec:label_prediction}\n\\header\n\\textbf{Base Model.}\nBase Model is the model that \\model{} uses for the final label prediction. Since \\model{} has skip connection from the Node Raw Feature, we can use any model in the single table setting as the base model. Specifically, a base model $\\pi_{\\omega}$ can be written as\n\\begin{equation}\n    \\pi_{\\omega}: \\mathbb{R}^{(C_n+|F|+|B|)\\times d} \\rightarrow \\mathbb{R}^{p}~,\n\\end{equation}\nwhere $\\omega$ is the learnable parameter of the base model, and $p$ is the dimension of the output vector.\n\nWe use the target table's node to do prediction as \\eqref{eq:prediction} and \\eqref{output}. The notation here is for the example node corresponding to row $i$ in table $X^n$. \n\\begin{align}\n\\text{Pred}(i, n, l) :=& \\pi_{\\omega}\\big(\\text{concat}[E_\\theta(x),\\tilde{h}^{l}(i, f_1), \\ldots , \\tilde{h}^{l}(i, f_{|F|}),\\label{eq:prediction}\\\\ \n&\\text{Agg}_{\\psi_1}(M^{l}(i, b_1)), \\ldots, \\text{Agg}_{\\psi_{|B|}}(M^{l}(i, b_{|B|}))]\\big)\\nonumber \\\\\n\\tilde{Y}^n_i =& \\text{Pred}(i, n, l)~. \\label{output}\n\\end{align}\nwhere $\\tilde{Y}^n_i$ represents the output prediction vector of \\model{} by the example node.\n\n\\begin{algorithm}\n\\caption{Graph-based Feature Synthesis}\\label{algo:GFS}\n\\SetKwInOut{Ip}{Input}\n\\SetKwInOut{Op}{Output}\n\\Ip{All tables $X^{1:N}$; search depth $K$}\n\\Op{Prediction $\\tilde{Y}$}\n\\tcp{message passing}\n$h^0(i,n)\\leftarrow\\Vec{0}\\in \\mathbb{R}^d\\,\\,\\forall n\\in[1, N], i\\in[0, R_n]$\\\\\n\\For{$l=1\\ldots K$ \n}{\n    \\For{$n=1\\ldots N$}{\n        \\For{$i=1\\ldots R_n$}{\n            $h^l(i, n)\\leftarrow \\text{Mes}(i, n, l-1)$  \n        }\n    }\n}\n\\tcp{label prediction}\n\\For{$i=1\\ldots R_{T}$}{\n    $\\tilde{Y}^{T}_i=\\text{Pred}(i, T, K)$\n}\n\\end{algorithm}\n\n\\subsection{\\model{} Time Complexity}\nIn the actual computation, we will use the $K$-hop neighbor subgraph induced by the target node with the search depth $K$ to compute the prediction on the target node. \n\nSuppose the subgraph $\\mathcal{G}$ has $N$ nodes, the maximum degree of the nodes in $\\mathcal{G}$ is $\\Delta$, the dimension of node embedding is $d$ as defined before, and the maximum number of columns among each table is $c$.\nConsidering the message passing step for each node, the aggregators(mean, max, min) will cost $O(\\Delta d)$, and the MLP transformation will cost $O(d^2)$. Assume that the computation cost of node embedding function is $O(P)$ \n. Then the time complexity of updating one node's embedding is $O(P+d^2+\\Delta d)$ and we need to update $K$ times.\n\nIn the label prediction step, the computational cost of aggregation is $O(\\Delta d+d^2)$, and assume the computation cost of base model is also $O(P)$.Then the time complexity of doing the label prediction on the target node is $O(P+d^2+\\Delta d)$.\nFinally, the total time complexity is $O(KN(P+d^2+\\Delta d))$ for the target node. \n\nTo specify, if the node embedding function and base model is FT-transformer, $O(P)=O(cd^2+c^d)$, the total time complexity is $O(KN(cd^2+c^2d+\\Delta d))$. And actually the order of magnitude of the computation cost for FT-transformer is representative of higher than many single table models.\n\nIf we delve more into the time complexity, suppose the original whole graph $\\tilde{\\mathcal{G}}$ has $\\tilde{N}$ nodes, and the maximum degree of the nodes in $\\tilde{\\mathcal{G}}$ is $\\tilde{\\Delta}$, we can have at most $\\tilde{\\Delta}^K$ nodes in subgraph $\\mathcal{G}$, so the total complexity is $min(O(K\\tilde{\\Delta}^K(P+d^2+\\tilde{\\Delta}d)), O(K\\tilde{N}(P+d^2+\\tilde{\\Delta}d)))$, and it is $min(O(K\\tilde{\\Delta}^K(cd^2+c^2d+\\tilde{\\Delta}d)), O(K\\tilde{N}(cd^2+c^2d+\\tilde{\\Delta}d)))$,\n\n\\section{Comparative Analysis}\\label{sec:property}\nAs mentioned previously, two strong candidates for column prediction tasks on relational databases are DFS-based and RDB2Graph-based solutions.  To better contextualized GFS relative to these alternatives, this section presents the following comparative analysis.  First, we prove that DFS is not invariant to traversal order, and hence may produce different results leading to undesirable instabilities.  In contrast, GFS outputs are uniquely determined with no analogous ambiguity.  We then prove that GFS generalizes DFS in the sense that there will exist GFS parameterizations that can match any DFS output.  And lastly, we closely examine key factors that differentiate GFS and RDB2Graph, including key design choices unique to the former.\n\n\\subsection{DFS sensitivity to traversal order}\n\nWe compare the capability of \\model{} versus DFS by first noting an important limitation of DFS:\n\n\\begin{theorem}\\label{thm:DFS variant}\n    Given a fixed set of input tables, one of which serves as the target, the output of DFS need not be invariant to the traversal order.\n\\end{theorem}\n\n\\begin{algorithm}\n\\SetKwFunction{MF}{Make\\_Features}\n\\SetKwFunction{Ba}{Backward}\n\\SetKwFunction{Fo}{Forward}\n\\SetKwFunction{Rf}{Rfeat}\n\\SetKwFunction{Df}{Dfeat}\n\\SetKwFunction{Ef}{Efeat}\n\\SetKwProg{Fn}{Function}{}{end}\n\\caption{Deep Feature Synthesis}\\label{alg:DFS}\n\\SetKwInOut{Ip}{Input}\n\\SetKwInOut{Op}{Output}\n\\Ip{Target table $X^i$, set of visited tables $X_V$, search depth $K$}\n\\Op{Augmented target table $X^i$}\n    \\Fn{\\MF{$X^i$, $X_V$, $K$}}{\n    $X_V=X_V\\cup X^i$\\\\\n    $X_B=$\\Ba{$X^i$}\\\\\n    $X_F=$\\Fo{$X^i$}\\\\\n    \\For{$X^j\\in X_B$}{\n        \\If{$X^j$ not in $X_V$}{\n            $X^j=$\\MF{$X^j$, $X_V$, $K-1$}\n        }\n        $F^i=F^i\\cup$ \\Rf($X^i$, $X^j$)\n    }\n    \\For{$X^j\\in X_F$}{\n        \\If{$X^j$ not in $X_V$}{\n            $X^j=$\\MF{$X^j$, $X_V$, $K-1$}\n        }\n        $F^i=F^i\\cup$ \\Df($X^i$, $X^j$)\n    }\n    \\Return{$X^i=F^i\\cup$ \\Ef{$X^i$}}\n    }\n\\end{algorithm}\n\n\\begin{proof}\n    We present the pseudocode of DFS in Algorithm~\\ref{alg:DFS}.  The core of DFS is the recursion of the \\texttt{Make\\_Features} function, which augments the target table $X^i$ given all the tables $X^1, \\ldots, X^n$.  DFS traverses the tables along both backward relationships and forward relationships, denoted by \\texttt{Backward} and \\texttt{Forward}.  During traversal, it either aggregates the information from tables with backward relationships via the function \\texttt{Rfeat} (e.g. SUM), or directly appends the information from tables with forward relationships via the function \\texttt{Dfeat}.  We refer to \\cite{kanter2015deep} for details of how DFS works.\n\n    From the pseudocode we can see that DFS traverses the undirected schema graph of the tables in a depth-first order.  Therefore, when there is a loop in the underlying undirected schema graph, we can expect that the output of DFS will be different.  We therefore present a counterexample and demonstrate how DFS would process it.\n\n    \\begin{figure}\n        \\centering\n        \\includegraphics[width=0.8\\linewidth]{figures/synthetic1.pdf}\n        \\caption{Synthetic Dataset 1's schema (Schema 1) and the traversal path of two traversal types.}\n        \\label{fig:syn-schema-1}\n    \\end{figure}\n\n    \\header\n    As shown in Figure~\\ref{fig:syn-schema-1}, we create three tables $A$, $B$, and $C$, each with a single feature column named $x$, $y$ and $z$ respectively.  We set Table $A$ as the target table, assigning it with an additional $label$ column.\n    Table A has a foreign key reference to both Table B and C, denoted as $A.bID$ and $A.cID$ \\footnote{For the sake of conciseness, we use dot notations to refer to a column or a cell in a table, e.g. $A.x$ refers to the column $x$ in table $A$, and $a.x$ refers to the cell value of row $a$ and column $x$.}, while Table B itself has a foreign key reference to C, denoted as $B.cID$.  Given a depth of 2, there are four possible ways to augment any row $a$ in Table $A$ each corresponding to a join path of length 2 or fewer:\n    \\begin{enumerate}[leftmargin=*]\n        \\item Find the row $c$ in $C$ such that $a.cID = c.cID$ and augment $a$ with $c.z$.  This follows the join path $C \\rightarrow A$.\n        \\item Find the row $b$ in $B$ such that $a.bID = b.bID$ and augment $a$ with $b.y$.  This follows the join path $B \\rightarrow A$.\n        \\item Find the row $b$ in $B$ such that $a.bID = b.bID$, then find the row $c$ in $C$ such that $b.cID = c.cID$ and augment $a$ with $c.z$.  This follows the join path $C \\rightarrow B \\rightarrow A$.\n        \\item Find the row $c$ in $C$ such that $a.cID = c.cID$, then find all row $b$ in $B$ such that $b.cID = c.cID$, and augment $a$ with the aggregation of all $b.y$.  This follows the join path $B \\rightarrow C \\rightarrow A$.\n    \\end{enumerate}\n    DFS can, however,  never capture all four of them because of the depth-first nature.  For the schema above, there are two ways for DFS to traverse and perform feature augmentations, which are also shown in Figure~\\ref{fig:syn-schema-1}.  In Traversal 1, DFS handles the join path $B \\rightarrow C \\rightarrow A$ first, so Table $B$ will already be visited when DFS attempts to traverse further down from $A \\rightarrow B$, therefore ignoring the possible join path $C \\rightarrow B \\rightarrow A$.  The same goes for Traversal 2 where join path $B \\rightarrow C \\rightarrow A$ is ignored.  Therefore, DFS can at most capture three of the four join paths, hence feature augmentations, given the algorithm above.  It can also be seen that the explored join path differs with traversal.\n\\end{proof}\n\n\\begin{remark}\n    \\model{} does not involve any operations analogous to traversal order, and hence its output is uniquely determined without ambiguity. Moreover,  \\model{} can capture all the join paths due to full neighbor sampling.\n\\end{remark}\n\nWe note that the counterexample schema constructed in Figure~\\ref{fig:syn-schema-1} may appear in real-world applications.  For example, some music streaming platforms like Soundcloud allow users to upload music, so it could have a user table(Table C), a music table(Table B), and an interaction table(Table A) referencing both the user and the music tables.  The music table there would have a foreign key reference to the user table as well, and so traversal order could subsequently impact DFS outputs.\n\nWe further empirically verify the impact of DFS's sensitivity as well as \\model{}'s immunity in Section~\\ref{sec:synthetic-experiments}, where we construct synthetic datasets based on the schema shown in Figure~\\ref{fig:syn-schema-1}, as well as a similarly-structured schema shown in Figure~\\ref{fig:syn-schema-2}.\n\n\\subsection{\\model{} generalizes DFS}\n\nThis section will show that \\model{} generalizes DFS, meaning that the output of a certain parametrization of \\model{} will be a superset of DFS's output.  The intuition is to show that \\model{} generalizes DFS under the most simple case of two related tables, where one table has a foreign key reference to the other.  Generalization on more complicated cases can then be established by induction. We will use the notation defined in Section \\ref{sec:model}\n\nWe denote $\\hat{X}^{n,l}_{i,:}$ be the $i$-th row of table $X^n$ augmented by depth-$l$ DFS.  Moreover, we relax the definition of the functions  defined in Section~\\ref{sec:model} (e.g. $E_\\theta$, $Agg_\\psi$, etc.), such that their input and output dimensions can be arbitrary rather than a fixed $d$.\n\nWe first show that given two tables $X_1$ and $X_2$, if $X_1$ has a forward relationship to $X_2$, then \\model{} generalizes DFS.\n\\begin{definition}[Subset and superset vectors]\n    We define that $A=[a_1, \\ldots, a_m]$ is a \\emph{subset vector} of $B=[b_1, \\ldots, b_n]$ if there exists a one-to-one mapping $\\mu: \\{1, \\cdots, m\\} \\mapsto \\{1, \\cdots, n\\}$, such that for all $i \\in \\{1, \\cdots, m\\}$, $a_i = b_{\\mu(i)}$.  We denote it as $A \\subseteq^v B$.  Conversely, we say that $B$ is a superset vector of $A$ and denote it as $B \\supseteq^v A$.\n\\end{definition}\n\n\\begin{lemma}\n    \\label{lem:forward}\n    Suppose we have two tables $X^1$ and $X^2$ with $C_1$ and $C_2$ columns respectively, and $X^1$ has a forward relationship to $X^2$.  Assume that the columns other than foreign keys and primary keys only have continuous values. There exists a two-layer \\model{} parametrization such that for any $i$, $h^{2}(i, 1) \\supseteq^v \\hat{X}^{1,1}_{i,:}=[\\texttt{Make\\_Features}(X^1, \\varnothing, 1)]_{i,:}$, meaning that the second-layer row embedding from \\model{} on $X^1$ is a superset vector of the output from depth-one DFS on $X^1$.\n\\end{lemma}\n\n\\begin{proof}\n    Without loss of generality, we assume that $X^1_{:, 1}$ is a foreign key reference to $X^2_{:, 1}$, which is the primary key column of $X^2$. All other columns just have continuous values. In this case, DFS will simply append the corresponding continuous values from $X^2$ to $X^1$.\n    Let $j$ be the row number such that $X^2_{j, 1} = X^1_{i, 1}$,\n    meaning that $j$ indexes the row of $X^2$ that is referenced by the foreign key in the $i$-th row of $X^1$.  Then, DFS yields\n    $$\n    \\hat{X}^{1,1}_{i,:} = [X^1_{i, 2}, X^1_{i, 3}, \\ldots, X^1_{i,C_1}, X^2_{j, 2}, X^2_{j, 3}, \\ldots, X^2_{j, C_2}]~.\n    $$\n\n    If we set \\model{}'s node embedding function $N_\\phi(x) = x$ and feature embedding function $E_\\theta(x) = x$ both as the identity function, then we can see that\n    \\begin{gather*}\n        h^1(j, 2)=\\text{concat}[E_\\theta(X^2_{j, :}), h^0(i, 1)]=[X^2_{j, 2}, \\ldots, X^2_{j, C_2}, \\Vec{0}] \\\\\n        \\tilde{h}^1(i, 1) = h^1(j, 2), \\tilde{h} \\text{ is defined in }Section \\ref{sec:message_passing}  \\\\\n        E_\\theta(X^1_{i, :}) = [X^1_{i, 2}, X^1_{i, 3}, \\ldots, X^1_{i,C_1}]\n    \\end{gather*}\n    Therefore, for all $i$,\n    $$\n    \\begin{aligned}\n    h^2(i, 1) &= N_\\phi\\big(\\text{concat}(E_\\theta(X^1_{i, :}), \\tilde{h}^1(i, 1))\\big) \\\\\n    &= [X^1_{i, 2}, X^1_{i, 3}, \\ldots, X^1_{i,C_1}, X^2_{j, 2}, X^2_{j, 3}, \\ldots, X^2_{j, C_2}, \\Vec{0}] \\\\\n    &\\supseteq^v \\hat{X}^{1,1}_{i,:}\n    \\end{aligned}\n    $$\n\\end{proof}\n\nWe now prove that the same holds for backward relationship between two tables.\n\n\\begin{lemma}\n    \\label{lem:backward}\n    Suppose we have two tables $X^1$ and $X^2$ with $C_1$ and $C_2$ columns respectively, and $X^1$ has a backward relationship to $X^2$.  Assume that the columns other than foreign keys and primary keys only have continuous values \n    There exists a two-layer \\model{} parametrization such that for any $i$, $h^{2}(i, 1) \\supseteq^v \\hat{X}^{1,1}_{i,:}=[\\texttt{Make\\_Features}(X^1, \\varnothing, 1)]_{i,:}$, meaning that the second-layer row embedding from \\model{} on $X^1$ is a superset vector of the output from depth-one DFS on $X^1$. \n\\end{lemma}\n\n\\begin{proof}\n    Without loss of generality, assume that $X^2_{:, 1}$ is a foreign key reference to $X^1_{:, 1}$, which is the primary key column of $X^1$. All other columns just have continuous values. Let $j_1, j_2, \\ldots, j_m \\in \\{j : X^2_{j, 1} = X^1_{i, 1}\\}$ be all the rows in $X_2$ that has foreign key reference to the $i$-th row in $X_1$.  DFS's output in this case becomes:\n    $$\n    \\hat{X}^{1,1}_{i,:} = [X^1_{i, 2}, X^1_{i, 3}, \\ldots, X^1_{i,C_1}, \\text{Agg}(\\hat{M}_{i})]\n    $$\n    where\n    $$\n    \\begin{gathered}\n    \\hat{M}_{i}=\\{[X^2_{j_1,2},\\ldots,X^2_{j_1,C_2}], \\ldots, [X^2_{j_m, 2}, \\ldots, X^2_{j_m, C_2}]\\}\\\\\n    \\text{Agg}(\\hat{M}_{i}) = [\\text{Agg}(\\hat{M}_{i})_2, \\ldots, \\text{Agg}(\\hat{M}_{i})_{C_2}] \\\\\n    \\text{Agg}(\\hat{M}_{i})_k = [\\text{Agg}(\\{X^2_{j_1, k}, X^2_{j_2, k}, \\ldots, X^2_{j_m, k}\\})]\n    \\end{gathered}\n    $$\n    So function \\text{Agg} is doing dimension-wise aggregation for a set of vectors $\\hat{M}_{i}$.\n    \n    Similar to the proof in Lemma~\\ref{lem:forward}, we can set $N_\\phi(x) = x$, $E_\\theta(x) = x$, $M^1(i, 2) = \\{h^1(j_1, 2), \\ldots, h^1(j_m, 2)\\}$ and $\\text{Agg}_\\psi(\\cdot) = \\text{Agg}(\\cdot)$.  Then    \n    $$\n    \\begin{aligned}\n    h^2(i, 1) &= \\text{concat}(E_\\theta(X^1_{i,:}), \\text{Agg}_\\psi(M^1(i, 2))) \\\\\n    &\\supseteq^v \\hat{X}^{1,1}_{i,:}\n    \\end{aligned}\n    $$\n\\end{proof}\n\n\\begin{theorem}\n    Suppose we have a database with $N$ tables \\\\$\\{X^1, X^2, \\ldots, X^N\\}$, each with $C_1, \\ldots, C_n$ columns respectively. Assume that the columns other than primary key and foreign key just have continuous values.  Then for any $n \\in\\{1, \\ldots, N\\}$ \n    and $l \\in \\mathbb{N}^+$, there exists a $l+1$-layer \\model{} parametrization such that for any $i$, $h^{l+1}(i, n) \\supseteq^v \\hat{X}^{n,l}=[\\texttt{Make\\_Features}(X^n, \\varnothing, l)]_{i,:}$, meaning that the $l+1$-the layer row embedding from \\model{} on $X^n$ is a superset vector of the output of $i$-th row from depth-$l$ DFS on $X^n$.\n\\end{theorem}\n\n\\begin{proof}\n    This can be proven by induction. Without loss of generality, we focus on the case where $n=1$.\n    \n     When $l=1$, only $X^1$ and all the tables that $X^1$ have backward and forward relationships with will matter the result, so this case is a simple extension of Lemma~\\ref{lem:forward} and Lemma~\\ref{lem:backward} to multiple tables. It is obvious that $h^2(i, 1)\\supseteq^v \\hat{X}^{1,1}_{i,:}$.\n     \n     Assume that $h^l(i, 1)\\supseteq^v \\hat{X}^{1,l-1}_{i,:}$, We need to prove that $h^{l+1}(i, 1)\\supseteq^v\\hat{X}^{1,l}_{i,:}$.\n     \n    It is obvious that \\begin{align}\\label{eq:th4tmp}\n        h^l(j, m)\\supseteq^v \\hat{X}^{m,l-1}_{j,:}~,\n    \\end{align}\n    where $\\hat{X}^{m,l-1}=$\\texttt{Make\\_Features}($X^m, \\varnothing, l-1$) for any table $X^m$ and any row number $j$ in this table due to the inductive hypothesis, and then we can follow the process of DFS in Algorithm~\\ref{alg:DFS} in line 5-10 of \\texttt{Make\\_Features}($X^1, \\varnothing, l$).\n\n    We denote $X^{'j}\\triangleq\\texttt{Make\\_Features}(X^j,\\varnothing, l-1)$, and $F^{'i}$ to be the set that follow the same process of $F^{i}$ in Algorithm~\\ref{alg:DFS} but replace \\texttt{Rfeat}($X^i$, $X^j$) to \\texttt{Rfeat}($X^i$, $X^{'j}$) and same with \\texttt{Dfeat}.\n    \n    If $X^j \\notin X_V$, then for any $i_j$\n    \\begin{gather*}\n        X^j=\\texttt{Make\\_Features}(X^j, \\{X^1\\}, l-1)~,\\\\\n        \\texttt{Rfeat}(X^1, X^j)\\subseteq \\texttt{Rfeat}(X^1, X^{'j})~.\n    \\end{gather*}\n    If $X^j \\in X_V$, then $X^j$ must have been visited in \\texttt{Make\\_Features}($X^j, X_V, k$) with some $X_V\\neq\\varnothing$ and $k\\leq l-1$. It is obvious that \n    $$\n        \\texttt{Rfeat}(X^1, X^j)\\subseteq \\texttt{Rfeat}(X^1, X^{'j})~.\n    $$\n    We can repeat a similar process in line 11-16, then we can get\n    $$\n    F^i\\subseteq F^{'i}\n    $$\n    Therefore,  $\\hat{X}^{1, l}\\subseteq\\hat{X}^{'1, l}\\triangleq F^{'i}\\cup$\\texttt{Efeat}($X^{i}$). Actually we do identity mapping for \\texttt{Efeat} in our setting.\n    \\begin{align*}\n        h^{l+1}(i,1)=&\\text{concat}[E_\\theta(X^1_{i,:}), \\tilde{h}^{l}(i, f_1), \\ldots , \\tilde{h}^{l}(i, f_{|F|}),\\\\\n        &\\text{Agg}_{\\psi_1}(M^{l}(i, b_1)), \\ldots,\\text{Agg}_{\\psi_{|B|}}(M^{l}(i, b_{|B|}))]    \n    \\end{align*}\n    According to \\eqref{eq:th4tmp} and the statement we get, we can find that $h^{l}(i_n, n)\\supseteq^v X^{'n}_{i_n,:}$($i_n$ is defined in Section \\ref{sec:message_passing}) for any $n\\in\\{f_1,\\ldots,f_{|F|}, b_1,\\ldots b_{|B|}\\}$ and the same for the aggregation part.\n    Therefore, follow similar prove in Lemma~\\ref{lem:forward} and Lemma~\\ref{lem:backward}, we can show that \n    $$\n    h^{l+1}(i,1)\\supseteq^v\\hat{X}^{'1,l}_{i, :} \\supseteq^v \\hat{X}^{1, l}_{i, :}~.\n    $$\n    \n\n    \n\n    \n    \n\n\\end{proof}\n\n\\subsection{Differences between \\model{} and RDB2Graph}\n\nThe major differences between \\model{} and RDB2Graph are as follows: (1) our message functions $N_\\phi$ in \\eqref{eq:message} and predictor function $\\pi_\\omega$ \\eqref{eq:prediction} are strong differentiable single-table tabular models like DeepFM \\cite{guo2017deepfm} and FT-Transformer \\cite{gorishniy2021revisiting}, (2) all message functions $N_\\phi$ and the downstream predictor function $\\pi_\\omega$ has skip connections to the raw features of the tables, (3) we use PNA-style aggregations while RDB2Graph only use simple aggregations from e.g. RGCN \\cite{schlichtkrull2018modeling} and RGAT \\cite{busbridge2019relational}. (4) we use the node embedding function with same parameter for each node types in every layer while RDB2Graph uses functions with different parameter in different layer.  \n\nDifferentiable tabular models like DeepFM and FT-Transformer express each column value with a latent embedding vector for single table prediction.  \\model{} follows that tradition, \\emph{appending the information of each auxiliary table as another latent vector as if there is a new column to the raw features which are regarded as the original columns in the table}, and then feed them all to the downstream predictor.  Such latent vectors of auxiliary tables are in turn computed by another DeepFM or FT-Transformer running on the auxiliary tables themselves, which in turn has more auxiliary tables' information appended.  The end result is that we have multiple DeepFMs/FT-Transformers stacked together, each modeling the feature interaction within its responsible table, and passes the information to other DeepFMs or FT-Transformers.\n\nMoreover, such DeepFMs or FT-Transformers directly takes the raw features from the tables, meaning that each \\model{} layer intrinsically has a skip connection to the input features.  Such skip connections are common tricks to mitigate oversmoothing for GNNs (e.g. \\cite{chen2020simple}). RDB2Graph in contrast has its predictive capability heavily dependent on the downstream MLP, which is known to be less powerful than DeepFM and FT-Transformer, therefore lacking the ability to model feature interactions within tables and across tables.  They also do not have the skip connections like \\model{} does.\n\nAdditionally, our choice of using PNA-style aggregations, which consists of \\textit{mean}, \\textit{min}, \\textit{max} at the same time, closely mimic what DFS (specifically Featuretools) does by default: DFS aggregates the same column in multiple different ways, generating new columns for each aggregation. Lastly, to enhance parameter efficiency in our framework, especially considering the use of complex node embedding functions like DeepFM and FT-transformer, we implement a shared parameter approach across each layer. \n\n\\section{Experiments}\\label{sec:experiments}\n\n\\subsection{Synthetic Datasets}\n\\label{sec:synthetic-experiments}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{figures/synthetic2.pdf}\n    \\caption{Synthetic Dataset 2's schema (Schema 2) and the traversal path of two traversal types.}\n    \\label{fig:syn-schema-2}\n\\end{figure}\n\nWe have shown in Section~\\ref{sec:property} that DFS is sensitive to traversal order while \\model{} is immune to that.  But can \\model{} indeed capture the necessary join path among all the paths better than DFS?  We empirically verify \\model{}'s capability by actually constructing the two counterexample datasets.\n\\begin{itemize}[leftmargin=*]\n\\item \\textbf{Schema 1}: The schema corresponding to Fig.~\\ref{fig:syn-schema-1} has three tables $A$, $B$ and $C$. Table $A$ is the target table. We set $A.x = 1$ and $C.z = 1$ for all rows in $A$ and $C$, while setting $B.y$ to uniformly random values between [0, 10].  We designate the label column $A.label$ as follows: each row $a$ in $A$ gets a label of 1 iff $\\sum_{b: b \\in B, b.cID=a.CID} b.y \\geq \\tau$, and 0 otherwise.  This ensures that the label column in $A$ is only related to the ``golden\" join path $B \\rightarrow C \\rightarrow A$.\n\n\\item \\textbf{Schema 2}: The schema corresponding to Fig.~\\ref{fig:syn-schema-2} has four tables $A, B, C$ and $D$. Table $A$ is the target table. We set $A.x = 1$ and $C.y = 1$ for all rows in $A$ and $C$, while setting $D.z$ to uniformly random values between $[0, 10]$. We designate the label column $A.label$ as follows: each row $a$ in $A$ gets a label of 1 iff $\\sum_{b, d: b \\in B, d \\in D, b.cID=a.CID, b.dID=d.dID} d.z \\geq \\tau$, and 0 otherwise. This ensures that the label is only related to the ``golden\" join path $D\\rightarrow B\\rightarrow C\\rightarrow A$.\n\\end{itemize}\n\nIn our experiments, $\\tau$ is set to 25 to ensure that the number of rows with label 1 roughly equals that with label 0. We make both datasets' target tables 1000 rows, and we split train/valid/test sets with the ratio 0.6/0.2/0.2. We set the depth of both DFS and \\model{} to 2 for Schema 1 and 3 for Schema 2, as they are the length of golden join paths. The test results are presented in Table \\ref{tab:syn}.\n\n\\begin{table}\n    \\caption{AUC results of synthetic datasets.}\n    \\centering\n    \\renewcommand\\arraystretch{1.25}\n    \\begin{tabular}{l|l|l}\n        \\toprule \n        \\textbf{Schema1} & depth = 2 & depth = 3 \\\\\n        \\midrule\n        DFS (Traversal 1) & 0.9983 & - \\\\\n        DFS (Traversal 2) & 0.4686 & 0.9964 \\\\\n        \\model{}          & 0.9984 & - \\\\\n        \\midrule\n        \\textbf{Schema 2} & depth = 3 & depth = 4 (max)\\\\\n        \\midrule\n        DFS (Traversal 1) & 0.9941 & - \\\\\n        DFS (Traversal 2) & 0.9263 & 0.9303 \\\\\n        \\model{}          & 0.9986 & - \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\label{tab:syn}\n\\end{table}\n\nFrom these results, we can see that the sensitivity of DFS, as demonstrated in Schema 1 and Schema 2, can indeed result in detrimental effects on the final prediction. In both schemas, Traversal 2 of DFS fails to capture the golden join path, resulting in limited learning capability and a significant drop in performance. In contrast, \\model{}'s ability to cover all possible join paths proves advantageous in both Schema 1 and Schema 2 when the number of search depth is right.\n\nOne might wonder if increasing the depth of DFS could eventually allow it to discover the join paths ignored in smaller depths.  To verify that, we also tried increasing the depth until either DFS stops augmenting with more features, or reaches 0.99 AUC result, which we consider as successfully capturing the golden join path.  We can see that DFS failed in Schema 2 even if we increase the depth to the maximum of 4, beyond which it stops augmenting.  Even if increasing the depth did help in Schema 1, it will cost much more time and memory due to the exponential growth of augmented features.  These observations further underscore the need for alternative approaches, such as \\model{}, which is immune to the traversal order sensitivity like DFS. %\n\n\\subsection{Real-World Datasets Experiment Setup}\n\\subsubsection{Datasets Discription}\n\\begin{table*}\n    \\caption{Dataset statistics.}\n    \\renewcommand\\arraystretch{1.25}\n    \\centering\n    \\begin{tabular}{l|c|c|c|c|c|c|c}\n        \\toprule\n        Dataset & \\# Train & \\# Valid & \\# Test & \\# Tables & \\# Total Rows & \\# Foreign Keys & \\# Total Columns \\\\\n        \\midrule\n        AVS         & 96035  & 32011 & 32011 & 3 & 8.2M & 2  & 23  \\\\\n        Outbrain    & 828251 & 21796 & 21796 & 9 & 28M  & 10 & 23  \\\\\n        Diginetica  & 882415 & 20521 & 20521 & 6 & 2.7M & 7  & 20  \\\\\n        KDD15       & 72325  & 24108 & 24108 & 4 & 8.3M & 3  & 19  \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\label{Dataset Information}\n\\end{table*}\n\nWe evaluate the effectiveness of our proposed model on four real-world datasets: \n\\textit{Acquire-valued-shoppers, Outbrain, Diginetica, KDD2015}, They span across different domains, including customer retention prediction, click-through rate prediction, recommendation, fraud detection, etc.\n\\begin{itemize}[leftmargin=*]\n\\item \\textbf{AVS (acquire-valued-shoppers)} \\cite{acquire-valued-shoppers-challenge}: The Acquire-valued-shoppers dataset is to predict which shoppers are most likely to repeat purchases. It has provided complete, basket-level, pre-offer shopping transaction history for a large set of shoppers who were targeted for an acquisition campaign. The incentive offered to that shopper and their post-incentive behavior is also provided.\n\\item \\textbf{Outbrain} \\cite{outbrain-click-prediction}: Outbrain is a platform that pairs relevant advertisements to curious readers on the web pages they would visit. In this dataset, the goal is to predict which advertisement a user will click on, given that the user is browsing a web page (called a \\emph{display}). The user behaviors and the details of the advertisements are also provided. %\n\\item \\textbf{Diginetica} \\cite{diginetica}: This dataset is provided by DIGINETICA and its partners containing anonymized search and browsing logs, product data, and anonymized transactions. The goal is to predict which products will be clicked among all the return results of one search query according to the personal shopping, search, and browsing preferences of the users. \n\\item \\textbf{KDD15} \\cite{KDD15}: This dataset is collected by XuetangX, one of the largest MOOC platforms in China. The goal of this dataset is to predict whether a user will drop a course within the next 10 days based on his or her prior activities.\n\\end{itemize}\n\n\\subsubsection{Data Preprocessing}\n\nFor all datasets, we preprocess datetime columns to four categorical columns: year, month, day, and day of week.  During prediction, we also drop the primary key and foreign key columns to ensure that the model does not memorize the labels based on IDs.  The statistics of the resulting datasets are shown in Table~\\ref{Dataset Information}. The preprocessing code will be released together with our model implementation.\n\n\\subsubsection{Evaluation Metrics}\nTo quantitatively evaluate the model performances, we use the area under ROC curve (\\textit{AUC}), which is widely used for binary classification tasks.\n\n\\subsubsection{Baselines}\nWe compare our method against two categories of approaches.\n\nThe first category involves offline methods that operate on relational databases by consolidating the data into a single table, followed by the application of single-table models. Other than the most important open-source method DFS, we compared our method against the following:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Target-table Only (TT)}.  We only use the target table for prediction.\n    \\item \\textbf{Simple join (SJ)}.  If table $X^A$ has a forward relationship to table $X^B$, it is possible to join the two tables by simply appending the columns from $X^B$ to $X^A$. For instance, in Fig. \\ref{fig:AVS}, the History table has a forward relationship to the Offer table, so we can join the two tables by appending the columns from the Offer table to the History table. This merging process is equivalent to performing a left merge on the Offer table with the History table based on the offer ID. This can be extended to recursive joining of further forward relationships with multiple levels. For example, if target table $X^A$ has a forward relationship to table $X^B$, and table $X^B$ has a forward relationship to table $X^C$, then we can join table $X^C$ to $X^B$ and subsequently join table $X^B$ to $X^A$. This recursive join method gathers information from as many tables as possible without aggregation.  We refer to this as simple join.\n\\end{itemize}\n\nThe second category involves RDB2Graph, which embeds each row into a node embedding vector and applies Graph Neural Network (GNN) models directly on the top. We evaluated our method against RDB2Graph using popular GNN models as the backbone, including RGCN \\cite{schlichtkrull2018modeling}, Relational GAT \\cite{busbridge2019relational}, and HGT \\cite{hu2020heterogeneous}, the last considered state-of-the-art for heterogeneous graphs.\n\nAlthough in Section~\\ref{sec:related_works} we mentioned \nmore methods such as OneBM\\cite{lam2017one}, R2N\\cite{lam2018neural}, ARDA\\cite{arda}, AutoFeature\\cite{liu2022feature}, METAM \\cite{galhotra2023metam} and ATJ-Net\\cite{bai2021atj},  they are either proprietary or not having their source code released, so we cannot compare with them in the experiments.\n\nIn both \\model{} and the first category of methods, we evaluate DeepFM \\cite{guo2017deepfm} and FT-transformer \\cite{gorishniy2021revisiting} as the base model and node embedding function choices \nSince our framework can plug in any differentiable model designed for single table settings, we just use these two powerful and representative single table models as evaluation, and any other differentiable mentioned in Section~\\ref{sec:related_works} can be simply merged into our framework in practical.\n\n\\subsubsection{Parameter Settings}\n\n\\begin{table*}[ht]\n    \\caption{AUC results of real-world datasets.\n    }\n    \\centering\n    \\renewcommand\\arraystretch{1.25}\n    \\begin{tabular}{l|c|c|c|c}\n        \\toprule\n        Model & \\textbf{AVS} & \\textbf{Outbrain} & \\textbf{Diginetica} & \\textbf{KDD15} \\\\\n        \\midrule\n        Search Depth or \\#GNN layers & 2 & 4 & 3 & 2 \\\\\n        \\midrule\n        TT + DeepFM & 0.6737 $\\pm$ 0.0003 & -                   & -                   & -            \\\\\n        TT + FT-Transformer   & 0.6720 $\\pm$ 0.0007 & -                   & -                   & -  \\\\\n        SJ + DeepFM & 0.6902 $\\pm$ 0.0003 & 0.7223 $\\pm$ 0.0008 & 0.6278 $\\pm$ 0.0074 & 0.6297 $\\pm$ 0.0028 \\\\\n        SJ + FT-Transformer   & 0.6894 $\\pm$ 0.0005 & 0.7188 $\\pm$ 0.0014 & 0.6100 $\\pm$ 0.0089 & 0.6061 $\\pm$ 0.0065\\\\\n        \\midrule\n        RDB2Graph + RGCN & 0.6956 $\\pm$ 0.0007 & 0.7420 $\\pm$ 0.0002 & 0.7420 $\\pm$ 0.0184 & 0.8557 $\\pm$ 0.0025\\\\\n        RDB2Graph + GAT  & 0.6978 $\\pm$ 0.0006 & 0.7440 $\\pm$ 0.0003 & 0.7565 $\\pm$ 0.0189 & 0.8629 $\\pm$ 0.0026\\\\\n        RDB2Graph + HGT  & 0.6957 $\\pm$ 0.0006 & 0.7549 $\\pm$ 0.0002 \\textsuperscript{*} & 0.8070 $\\pm$ 0.0005 & 0.8719 $\\pm$ 0.0010\\\\\n        \\midrule\n        DFS + DeepFM & 0.6974 $\\pm$ 0.0006 & 0.7337 $\\pm$ 0.0008 & 0.7963 $\\pm$ 0.0055 & 0.8717 $\\pm$ 0.0011\\\\\n        DFS + FT-Transformer   & 0.6916 $\\pm$ 0.0007 & 0.7303 $\\pm$ 0.0012 & 0.8024 $\\pm$ 0.0018 & 0.8626 $\\pm$ 0.0022\\\\\n        \\midrule\n        \\model{} (ours) & \\textbf{0.7001} $\\pm$ 0.0002 & \\textbf{0.7556} $\\pm$ 0.0008 & \\textbf{0.8106} $\\pm$ 0.0023 & \\textbf{0.8781} $\\pm$ 0.0011\\\\\n        \\bottomrule\n        \\multicolumn{5}{l}{\\textsuperscript{*} RDB2Graph + HGT can only run up to 3 layers due to GPU memory constraints.} %\n    \\end{tabular}\n    \\label{tab:real_dataset}\n\\end{table*}\n\n\\begin{table}[ht]\n    \\caption{Training time (in hours) of different models. The result marked as * is run with search depth = 3 due to OOM problem; however, the other results in the same column are run using search depth = 4. And the results from \\model{} are \\model{} + DeepFM.}\n    \\centering\n    \\renewcommand\\arraystretch{1.25}\n    \\begin{tabular}{l|c|c|c|c}\n        \\toprule %\n        \\multirow{1}{*}{Model} & \\textbf{AVS} & \\textbf{Outbrain} & \\textbf{Diginetica} & \\textbf{KDD15}\\\\\n        \\midrule\n        RGCN & 0.25 & 4.36 & 2.37 & 0.56 \\\\\n        GAT  & 0.33 & 6.00 & 1.76 & 0.51 \\\\\n        HGT  & 1.17 & *8.31 & 4.83 & 2.45 \\\\\n        \\model{} & 0.60 & 8.60 & 3.16 & 0.39 \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\label{tab:time_compare}\n\\end{table}\n\nIn our experiments, we use Weight \\& Bias \\cite{wandb} to automatically search for the best hyperparameter combination. %\nWe run 50 hyperparameter trials for all our experiments, with distribution $LogUniform(10^{-7}, 10^{-3})$ for learning rate and weight decay. For models that use FT-Transformer as the base model, we set the distribution for dropout probability to $Uniform(0, 0.3)$, while for other models, we set it to $Uniform(0, 0.2)$.  We set the hidden dimension $d$ to 16, and the node embedding size of RDB2Graph to 64.  After hyperparameter optimization, we select the one with the best performance on the validation set, and then we rerun the model five times using this hyperparameter combination to reduce the impact of randomness. We record the mean performance and the standard deviation on the test set based on these reruns. We set the search depth of DFS and \\model{} as well as the number of layers in RDB2Graph to the largest value possible to fit computation into GPU memory.\n\n\\subsection{Performance Comparison}\n\\label{sec:performance-comparison}\n\\begin{table*}[ht]\n    \\caption{AUC results of \\model{} with different row embedding functions.}\n    \\centering\n    \\renewcommand\\arraystretch{1.25}\n    \\begin{tabular}{l|c|c|c|c}\n        \\toprule\n        Model & \\textbf{AVS} & \\textbf{Outbrain} & \\textbf{Diginetica} & \\textbf{KDD15}\\\\\n        \\midrule\n        \\model{} ($N_\\phi=\\text{MLP},\\pi_\\omega=\\text{DeepFM}$) & 0.6949 $\\pm$ 0.0026 & 0.7540 $\\pm$ 0.0005 & 0.8095 $\\pm$ 0.0024 & 0.8660 $\\pm$ 0.0029\\\\\n        \\model{} ($N_\\phi=\\text{DeepFM},\\pi_\\omega=\\text{DeepFM}$)     & \\textbf{0.7001} $\\pm$ 0.0002 & \\textbf{0.7556} $\\pm$ 0.0008 & \\textbf{0.8106} $\\pm$ 0.0023 & \\textbf{0.8781} $\\pm$ 0.0011\\\\\n        \\bottomrule\n    \\end{tabular}\n    \\label{tab:row_embedding_ablation}\n\\end{table*}\n\n\\begin{table*}[ht]\n    \\caption{AUC results of \\model{} with different base model.}\n    \\centering\n    \\renewcommand\\arraystretch{1.25}\n    \\begin{tabular}{l|c|c|c|c}\n        \\toprule\n        Model & \\textbf{AVS} & \\textbf{Outbrain} & \\textbf{Diginetica} & \\textbf{KDD15}\\\\\n        \\midrule\n        \\model{} with DeepFM & \\textbf{0.7001} $\\pm$ 0.0002 & \\textbf{0.7556} $\\pm$ 0.0008 & \\textbf{0.8106} $\\pm$ 0.0023 & \\textbf{0.8781} $\\pm$ 0.0011\\\\\n        \\model{} with FT-Transformer & 0.6936 $\\pm$ 0.0010 & 0.7543 $\\pm$ 0.0006 & 0.8082 $\\pm$ 0.0012 & 0.8703 $\\pm$ 0.0011 \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\label{tab:base_model_ablation}\n\\end{table*}\n\nThe results are in Table~\\ref{tab:real_dataset}.  We note that\nTT results are not applicable on most datasets, as the target table in Outbrain, Diginetica and KDD15 contains only foreign keys and the label column; it is therefore meaningless for single-table models since the foreign key columns are dropped to avoid memorization of labels.\n\nWe also compare the training time for each model and all the experiments are run on g4dn.metal. We focused on comparing the \\model{} framework with other GNN models, as these models employ online sampling, which involves aggregating information from other tables during the training process. On the other hand, baselines such as DFS and SJ adopt offline sampling, where information from other tables is aggregated to form a single target table prior to model training, so these methods cannot be fairly compared. The training time results, specifically for \\model{} combined with DeepFM, are presented in Table \\ref{tab:time_compare}.\n\nFrom the real-world dataset results, we have the following observation and analysis.\n\\begin{itemize}[leftmargin=*]\n    \\item \\model{} consistently outperforms other baselines or achieves similar results to the best models across all four datasets. In contrast, other models may fail to perform well in certain datasets. Specifically, HGT performs well in Outbrain but fails in AVS. DFS, RGCN and GAT do not perform well across all datasets.\n    \\item While HGT may appear to be the most powerful baseline, it is important to consider the training time when comparing with \\model{}. When comparing the training time of HGT and \\model{}, we find that \\model{} combined with DeepFM is more efficient than HGT throughout the entire training process. In fact, \\model{} requires only half to one-third of the training time per epoch compared to HGT in most datasets. The similar training time observed in the Outbrain dataset is due to the fact that HGT utilizes a search depth of 3, while other models employ a search depth of 4.\n\\end{itemize}\n\n\\subsection{Ablation Study}\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/option.pdf}\n    \\caption{The performance, training time, GPU memory used, and number of tables searched of \\model{} at different search depths in three different datasets.}\n    \\label{fig:search depth}\n\\end{figure*}\n\nTo gain a deeper understanding of the design rationale behind our proposed framework, \\model{}, we conducted several ablation experiments.\n\\subsubsection{Row Embedding Function}\nIn our experiments we use the same architecture for base model $\\pi_\\omega$ and row embedding function $N_\\phi$.  While the choice of base model $\\pi_\\omega$ is straightforward as \\model{} can enjoy the capacity of a strong single-table model, using DeepFM and FT-Transformer for $N_\\phi$ seems redundant at first glance.  So we compared our model against an alternative where we use an MLP for $N_\\phi$ while still using DeepFM as base model.  The results are in Table~\\ref{tab:row_embedding_ablation}.\n\nWe can have several observations and analyses from the results.\n\\begin{itemize}[leftmargin=*]\n    \\item The importance of a powerful row embedding function becomes evident in the AVS and KDD15 datasets, as there is a significant drop in performance when we replace the previous row embedding function with a simple MLP.\n    This indicates that the feature interactions from DeepFM and FT-Transformer are necessary for the tables other than the target table as well.\n    \\item In the Outbrain and Diginetica datasets, the row embedding function appears to have less impact on performance. This observation can be attributed to the fact that the prediction of the target column relies more on the relationships between rows in different tables rather than the interaction of row features. This hypothesis aligns well with the results obtained from GNN models, where MLP is used as the embedding function to form node embedding vectors. Notably, the performance of the most powerful graph model, HGT, is similar to that of \\model{}, supporting the hypothesis.\n\\end{itemize}\n\n\\subsubsection{Base Model}\nWe experimented with different base models for \\model{}, and it is essential to note that the row embedding function is also adjusted accordingly based on the chosen base model. The results can be seen in Table \\ref{tab:base_model_ablation}, where we find that DeepFM demonstrates superior performance across all the datasets.  This is perhaps unsurprising given that DFS+DeepFM is better than DFS+FT-Transformer on 3 of 4 datasets in Table \\ref{tab:real_dataset}.  We also remark that while DeepFM may perform best for now, any strong differentiable tabular model introduced in the future could potentially serve as an effective replacement. \n\n\\subsection{Effect of Search Depth}\nSearch depth is an important architectural parameter of \\model{} when applied to real-world datasets. In Fig. \\ref{fig:search depth}, we present the \\textit{Performance}, \\textit{Training Time}, \\textit{GPU Memory Used}, and \\textit{Number of Tables Searched} by \\model{} at different search depths in the \\textit{AVS}, \\textit{Outbrain}, and \\textit{Diginetica} datasets for larger search depth range. We gradually increase the search depth until we exhaust the GPU memory. We do not report the results for \\textit{Outbrain} and \\textit{Diginetica} at search depth 0 as it is meaningless, similar to why TT is meaningless as explained in Section~\\ref{sec:performance-comparison}. It is observed that the Training Time and GPU Memory Used tend to increase rapidly, while the performance of \\model{} may reach a bottleneck. Therefore, in real-world applications of \\model{}, selecting an appropriate search depth becomes an important tradeoff between performance and resource utilization (i.e., training time and GPU memory used).\n\n\\section{Conclusion}\\label{sec:conclusion}\nIn this paper, we introduce \\model{}, a novel framework specifically designed for general \\emph{column prediction} tasks on relational databases. \\model{} is an embedding update and prediction framework that can plug in any differentiable model designed for single table settings as base model or embedding function to finalize the specific model. \\model{} also improves upon the previous methods DFS and RDB2Graph and and tackles the inherent problems in previous methods. We conduct comprehensive experiments on real-world datasets to evaluate the performance of \\model{} against other baseline methods. The results demonstrate that \\model{} consistently outperforms the baselines and exhibits superior efficiency compared to the most powerful and complex GNN baseline, HGT. The successful implementation and evaluation of \\model{} highlight its potential as an effective and efficient solution for machine learning tasks on relational databases. \n\n\\begin{acks}\nWe thank the support from Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102) and National Natural Science Foundation of China (62177033), as well as computation resource support from AWS. \n\\end{acks}\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-1703.06103v4.tex",
        "arXiv-2003.01332v1.tex",
        "arXiv-2312.02037v1.tex"
    ],
    "group_id": "group_16",
    "response": "### Title: Modeling Relational Data with Graph Neural Networks: An Exploration of R-GCNs, HGT, and GFS\n\n### Introduction\nThe field of graph neural networks (GNNs) has seen significant advancements in recent years, particularly in the context of modeling structured data such as knowledge bases and relational databases. Knowledge bases and relational databases are complex, multi-relational structures that store a variety of factual and relational information, enabling applications ranging from question answering and information retrieval to recommendation systems and fraud detection. Despite the extensive effort invested in their creation and maintenance, these databases often remain incomplete, necessitating the development of robust models capable of inferring missing information. The primary challenge lies in effectively modeling the heterogeneous and dynamic nature of these databases, which includes capturing the intricate relationships between different types of nodes and edges, as well as the temporal dependencies that evolve over time.\n\nHistorically, statistical relational learning (SRL) has been a key approach to addressing the issue of incomplete knowledge bases. SRL tasks such as link prediction and entity classification have been tackled using various techniques, including tensor factorization methods and path-based models. However, these methods often struggle with scalability and the need for manual feature engineering, which can be time-consuming and prone to errors. Recent advancements in GNNs have provided a promising avenue for addressing these challenges by leveraging the inherent structural information within the graph. However, most existing GNNs are designed for homogeneous graphs, where all nodes and edges belong to the same types, making them less suitable for modeling the complex, heterogeneous structures found in real-world systems.\n\nThis summary explores three recent research papers that introduce novel approaches to modeling relational data using GNNs: Relational Graph Convolutional Networks (R-GCNs), Heterogeneous Graph Transformer (HGT), and Graph-based Feature Synthesis (GFS). These papers aim to address the limitations of existing GNNs by incorporating mechanisms to handle heterogeneity and dynamic information, thereby improving the accuracy and efficiency of downstream tasks such as link prediction and entity classification.\n\n### Main Content of Each Paper\n\n#### Paper 1: Relational Graph Convolutional Networks (R-GCNs)\nThe first paper introduces R-GCNs, a specialized version of GCNs designed to handle multi-relational data in knowledge bases. The authors propose an encoder-decoder architecture where the encoder is an R-GCN that captures neighborhood information through multiple layers of message passing, and the decoder is a tensor factorization model, specifically DistMult, which predicts the existence of labeled edges. The R-GCN model is characterized by its ability to handle different types of relations by introducing relation-specific transformations. This is achieved through a basis function decomposition or a block-diagonal decomposition, which allows for effective weight sharing and sparsity constraints, reducing the number of parameters and mitigating overfitting on rare relations.\n\nThe paper evaluates R-GCNs on four datasets: AIFB, MUTAG, BGS, and AM, for entity classification tasks, and on three datasets: WN18, FB15k, and FB15k-237, for link prediction tasks. The authors demonstrate that R-GCNs outperform traditional factorization methods like DistMult and other baselines such as RDF2Vec and Weisfeiler-Lehman kernels (WL) on entity classification. For link prediction, R-GCNs show significant improvements over DistMult, especially on the FB15k-237 dataset, where the performance gain is 29.8\\%. The authors also introduce an ensemble model (R-GCN+) that combines the strengths of R-GCNs and DistMult, achieving better performance on certain datasets.\n\n#### Paper 2: Heterogeneous Graph Transformer (HGT)\nThe second paper presents HGT, an architecture designed to model web-scale heterogeneous and dynamic graphs. HGT introduces a meta relation-aware heterogeneous mutual attention mechanism, which allows nodes and edges of different types to maintain dedicated representations while still interacting effectively. To handle temporal information, HGT incorporates a relative temporal encoding (RTE) mechanism, which captures the dynamic dependencies between nodes without slicing the graph into different timestamps. This approach ensures that the model can learn the temporal evolution of the graph structure.\n\nThe authors also propose a heterogeneous mini-batch graph sampling algorithm called \\sampling, which maintains a similar number of nodes and edges for each type and ensures that the sampled sub-graphs are dense to minimize information loss. The \\sampling algorithm is crucial for enabling efficient and scalable training on large-scale graphs. HGT is evaluated on the Open Academic Graph (OAG) and two domain-specific subgraphs (CS and Med), demonstrating superior performance on various downstream tasks such as paper-field prediction, paper-venue prediction, and author disambiguation. The model outperforms existing state-of-the-art GNN baselines by 9\\% to 21\\% on average, while also being more parameter-efficient and having comparable training times.\n\n#### Paper 3: Graph-based Feature Synthesis (GFS)\nThe third paper introduces GFS, a framework designed for prediction tasks over relational databases. GFS formulates the relational database as a heterogeneous graph and leverages the inductive bias from single-table models to capture intricate relationships within and across tables. Unlike traditional methods that require manual feature engineering to merge multiple tables into a single table, GFS eliminates the need for such engineering by directly using the relational structure of the database. The framework can plug in any differentiable single-table model as a base model or embedding function, thereby benefiting from the strong inductive biases of these models.\n\nGFS is evaluated on four real-world datasets: AVS, Outbrain, Diginetica, and KDD15, demonstrating superior performance compared to both offline and online methods. The paper also includes an ablation study to validate the importance of each component in GFS, such as the use of differentiable single-table models and PNA-style aggregations. The results show that GFS consistently outperforms existing methods, while also being more efficient in terms of training time and GPU memory usage.\n\n### Commonalities and Innovations\nAll three papers address the challenge of modeling relational data using GNNs, but they each introduce unique innovations to tackle specific issues. R-GCNs focus on handling multi-relational data by introducing relation-specific transformations, which are crucial for capturing the diverse nature of relationships in knowledge bases. HGT extends this idea by incorporating a meta relation-aware attention mechanism and a relative temporal encoding, making it suitable for modeling web-scale heterogeneous and dynamic graphs. GFS, on the other hand, addresses the challenge of prediction tasks over relational databases by formulating the database as a heterogeneous graph and leveraging single-table models to capture feature interactions within and across tables.\n\nThe common thread among these papers is the use of GNNs to capture neighborhood information and propagate it through multiple layers. However, each paper introduces distinct mechanisms to handle heterogeneity and dynamic information. R-GCNs use basis function decomposition and block-diagonal decomposition to manage the large number of parameters associated with multi-relational data. HGT introduces a relative temporal encoding mechanism to capture the temporal dependencies in dynamic graphs, while GFS uses PNA-style aggregations and skip connections to mitigate the oversmoothing problem common in GNNs.\n\n### Comparison of Results and Discussion\nThe experimental results of the three papers highlight the effectiveness and efficiency of their proposed models. R-GCNs achieve state-of-the-art results on entity classification tasks and significant improvements on link prediction tasks, especially on the FB15k-237 dataset. HGT outperforms existing GNN baselines by 9\\% to 21\\% on various downstream tasks on the OAG dataset, demonstrating its capability to capture both heterogeneity and temporal dependencies. GFS consistently outperforms existing methods on real-world datasets, showing its potential as an efficient and effective solution for prediction tasks over relational databases.\n\nThe performance gains of R-GCNs and HGT over traditional factorization methods and GNN baselines are substantial, particularly on more challenging datasets. For instance, R-GCNs achieve a 29.8\\% improvement over DistMult on FB15k-237, while HGT outperforms the best baseline (HAN) by 11\\% on the CS dataset and 10\\% on the Med dataset. GFS also shows significant improvements over existing methods, with the best performance achieved by using DeepFM as the base model.\n\nHowever, the results also highlight some limitations. R-GCNs struggle with datasets like MUTAG and BGS, where high-degree hub nodes dominate the performance. HGT faces challenges in handling the large-scale OAG dataset due to the complexity of the graph structure and the need for efficient sampling algorithms. GFS requires careful selection of the search depth to balance performance and resource utilization, as increasing the depth can lead to exponential growth in the number of columns and computational costs.\n\n### Conclusion\nThe three papers collectively demonstrate the potential of GNNs in modeling relational data, each addressing specific challenges and introducing innovative solutions. R-GCNs effectively handle multi-relational data by introducing relation-specific transformations, while HGT extends this capability to dynamic graphs through relative temporal encoding. GFS leverages the relational structure of databases to perform feature synthesis and prediction tasks without manual feature engineering.\n\nThe main findings from these papers include:\n- R-GCNs achieve state-of-the-art results in entity classification and significant improvements in link prediction.\n- HGT outperforms existing GNN baselines on various downstream tasks, while also being more parameter-efficient and scalable.\n- GFS provides a framework for prediction tasks over relational databases, eliminating the need for manual feature engineering and leveraging the relational structure.\n\nFuture research directions could include:\n- Combining the strengths of R-GCNs and HGT to handle both heterogeneity and temporal dependencies in a unified framework.\n- Exploring the use of attention mechanisms in R-GCNs to better handle high-degree nodes.\n- Investigating the impact of different base models in GFS on various prediction tasks.\n- Developing more efficient sampling algorithms for HGT to handle even larger-scale graphs.\n- Extending GFS to support more complex relational databases with additional constraints and relationships.\n\nThese advancements could further enhance the modeling capabilities of GNNs in handling complex, multi-relational data, thereby improving the performance and scalability of downstream tasks."
}