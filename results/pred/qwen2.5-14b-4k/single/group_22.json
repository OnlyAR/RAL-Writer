{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility}\n\n\\begin{document}\n\n\\pagestyle{headings}\n\\mainmatter\n\\def\\ECCVSubNumber{5327}  % Insert your submission number here\n\n\\title{A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility} % Replace with your title\n\n\\begin{comment}\n\\titlerunning{ECCV-22 submission ID \\ECCVSubNumber} \n\\authorrunning{ECCV-22 submission ID \\ECCVSubNumber} \n\\author{Anonymous ECCV submission}\n\\institute{Paper ID \\ECCVSubNumber}\n\\end{comment}\n\n\\titlerunning{Interactive Vision-Language Navigation with Unknown Command Feasibility}\n\\author{Andrea Burns\\inst{1} \\and\nDeniz Arsan\\inst{2} \\and Sanjna Agrawal\\inst{1} \\and Ranjitha Kumar\\inst{2} \\and Kate Saenko\\inst{1,3} \\and Bryan A. Plummer\\inst{1}}\n\\authorrunning{Burns et al.}\n\\institute{Boston University, Boston MA 02215, USA  \\email{\\{aburns4,sanjna,saenko,bplum\\}@bu.edu} \\\\\n\\and\nUniversity of Illinois Urbana-Champaign, Champaign IL 61820, USA \\\\\n\\email{\\{darsan2,ranjitha\\}@illinois.edu}\\\\\n\\and\nMIT-IBM Watson AI Lab, Cambridge MA 02142, USA\\\\\n}\n\\maketitle\n\\addtocontents{toc}{\\protect\\setcounter{tocdepth}{-10}}\n\n\\begin{abstract}\nVision-language navigation (VLN), in which an agent follows language instruction in a visual environment, has been studied under the premise that the input command is fully feasible in the environment. Yet in practice, a request may not be possible due to language ambiguity or environment changes. To study VLN with unknown command feasibility, we introduce a new dataset Mobile app Tasks with Iterative Feedback (MoTIF), where the goal is to complete a natural language command in a mobile app. Mobile apps provide a scalable domain to study real downstream uses of VLN methods. Moreover, mobile app commands provide instruction for interactive navigation, as they result in action sequences with state changes via clicking, typing, or swiping. MoTIF is the first to include feasibility annotations, containing both binary feasibility labels and fine-grained labels for why tasks are unsatisfiable. We further collect follow-up questions for ambiguous queries to enable research on task uncertainty resolution. Equipped with our dataset, we propose the new problem of feasibility prediction, in which a natural language instruction and multimodal app environment are used to predict command feasibility. MoTIF provides a more realistic app dataset as it contains many diverse environments, high-level goals, and longer action sequences than prior work.\nWe evaluate interactive VLN methods using MoTIF, quantify the generalization ability of current approaches to new app environments, and measure the effect of task feasibility on navigation performance.\n\n\\keywords{Vision-language navigation, task feasibility, mobile apps}\n\\end{abstract}\n\n\\section{Introduction}\n\\begin{figure}[t]\n\\centering\n    \\includegraphics[scale=0.1615]{latest_motivation2.jpg}\n    \\caption{MoTIF natural language commands which may not be possible. At each time step, action coordinates (\\ie, where clicking, typing, or scrolling occurs), the app screen, and view hierarchy (\\ie, the app backend, illustrated behind it) are captured}\n    \\label{fig:motivation}\n\\end{figure}\n\nVision-language navigation (VLN) has made notable progress toward natural language instruction following~\\cite{blukis2021persistent,irshad2021hierarchical,min2021film,nguyen2019hanna,ALFRED20,singh2020moca,vln_ssa}. While navigation datasets exist for home environments~\\cite{vln,embodiedqa,ku-etal-2020-room,ALFRED20} and digital environments like mobile apps and websites~\\cite{li-etal-2020-mapping,vut,langtoelem,wob}, none capture the possibility that the language request may not be feasible in the given environment. When high-level natural language goals are requested, they may not be feasible for various reasons: the request may be ambiguous or state dependent, refer to functionality that is no longer available, or is reasonable in a similar environment but not satisfiable in the current. Task feasibility has been studied to determine question relevance for text-only~\\cite{Gardner2020DeterminingQP} and visual question answering~\\cite{vizwiz,massiceti,ray2016question}, but it has not been explored in interactive multimodal environments.\n\nTo study interactive task feasibility, we propose Mobile app Tasks with Iterative Feedback (MoTIF)\\footnote{\\url{https://github.com/aburns4/MoTIF}}, the largest dataset designed to support interactive methods for completing natural language tasks in mobile apps. As illustrated in Figure~\\ref{fig:motivation}, a sample includes the natural language command (\\ie, task), app view hierarchy, app screen image, and action coordinates for each time step. MoTIF contains both feasible and infeasible requests, unlike any VLN dataset to date. \nIn addition to these binary feasibility labels for each task, we collect subclass annotations for why tasks are infeasible and natural language follow-up questions. \nOur dataset provides a domain with practical downstream applications to study vision-language navigation, as well as data for investigating app design~\\cite{rico,erica,designsemantics}, human-computer interfaces~\\cite{sugilite,convobreak,demoplusLi2021}, and document understanding~\\cite{appalaraju2021docformer,Li_2021_CVPR,canvasvae}. \n\nWe propose a baseline model for task feasibility prediction and confirm app exploration is necessary, with visual inputs key to accuracy. Surprisingly, prior representation learning approaches specific to the mobile app domain (\\eg, app icon features) do not result in the best performance. \nWe then evaluate methods for automating MoTIF's commands and find MoTIF's diverse test set are challenging for prior work. Performance trends between seen and unseen app environments point to the need for more in-app exploration during training and qualitative failures in the best baseline model demonstrate the importance of visual understanding for MoTIF.\n\nWe summarize our contributions below:\n\\begin{itemize}\n    \\item A new vision-language navigation dataset, Mobile app Tasks with Iterative Feedback (MoTIF). MoTIF has free form natural language commands for interactive goals in mobile apps, a subset of which are infeasible. It contains natural language tasks for the most app environments to date. MoTIF also captures multiple interactions including clicking, swiping and typing actions.\n    \\item A new vision-language task: interactive task feasibility classification, along with subclass annotations on why tasks are infeasible and follow-up questions for research toward resolving task uncertainty via dialogue. \n    \\item Benchmarks for feasibility classification and task automation with MoTIF. A thorough feature exploration is performed to evaluate the role of vision and language in task feasibility. We compare several methods on mobile app task automation, analyze generalization, and examine the effects of feasibility.\n\\end{itemize}\n\\section{Related Work}\nWe now discuss the key differences between MoTIF and existing datasets; we provide a side-by-side comparison in Table~\\ref{tab:compare}.\n\\smallskip \n\n\\noindent\\textbf{Task Feasibility}\nVision-language research has recently begun to study task feasibility. Gurari \\etal\\hspace{0.25mm} introduced VizWiz~\\cite{vizwiz}, a visual question answering dataset for images taken by people that are blind, resulting in questions which may not be answerable. To the best of our knowledge, VizWiz is the only vision-language dataset with annotations for task feasibility, but it only addresses question answering over static images. Additionally, images that cannot be used to answer visual questions are easily classified, as they often contain blurred or random scenes (\\eg, the floor). \nGardner \\etal ~\\cite{Gardner2020DeterminingQP} explored question-answer plausibility prediction, but the questions used were generated from a bot, which could result in extraneous questions also easy to classify as implausible. Both are significantly different from the nuanced tasks of MoTIF with human generated queries, for which exploration is necessary to determine feasibility. MoTIF's infeasible tasks are always relevant to the Android app category, making it more challenging to discern feasibility compared to the distinct visual failures present in VizWiz. % having an inherent relevance to the visual environment.\n\\smallskip\n\\begin{table}[t]\n  \\renewcommand\\arraystretch{0.95}\n    \\centering\n        \\caption{Comparison of MoTIF to existing datasets. We consider the number of natural language commands, command granularity, existence of feasibility annotations, the number of environments and whether the visual state is included in annotations}\n    \\begin{tabular}{|l|c|c|c|c|c|c|}\n    \\hline\n       \\multirow{3}{*}{Dataset} & \\multicolumn{3}{c|}{Language Annotations} & \\multicolumn{2}{c|}{Dataset Environment} \\\\ \n    \\cline{2-6}\n         & \\# Human & Task & \\multirow{2}{*}{Feasibility} & \\# & Visual \\\\ % & \\# & Actions  \\\\\n                  & Annotations & Granularity & & Environments & State \\\\ % & views & included \\\\\n                                              \\hline\n\n                            \\textbf{(a) House} & & & & & \\\\\n                          \\hspace{1mm} R2R~\\cite{vln} & 21,567 & Low & \\color{red}{\\ding{55}} & 90 &\\color{green}{\\ding{51}} \\\\ % -- & -- & -- \\\\\n        \\hspace{1mm} IQA~\\cite{IQA} & \\color{red}{\\ding{55}} & High & \\color{red}{\\ding{55}} & 30 &\\color{green}{\\ding{51}} \\\\ % -- & -- & -- \\\\\n         \\hspace{1mm} ALFRED~\\cite{ALFRED20} & 25,743 & High \\& Low &  \\color{red}{\\ding{55}}& 120 & \\color{green}{\\ding{51}} \\\\ %-- & -- & -- \\\\\n         \\hline\n          \\textbf{(b) Webpage} & & & & & \\\\\n       \\hspace{1mm} MiniWoB~\\cite{wob} &  \\color{red}{\\ding{55}}  & High & \\color{red}{\\ding{55}} & 100  & \\color{red}{\\ding{55}} \\\\\n        \\hspace{1mm} PhraseNode~\\cite{langtoelem} & 50,000 & Low & \\color{red}{\\ding{55}} & 1,800 & \\color{red}{\\ding{55}} \\\\ % &  1 & \\\\\n         \\hline\n         \\textbf{(c) Mobile App} & & & & & \\\\ \n         \\hspace{1mm} RicoSCA~\\cite{li-etal-2020-mapping} & \\color{red}{\\ding{55}} & Low & \\color{red}{\\ding{55}} & 9,700 &  \\color{red}{\\ding{55}} \\\\ %&  1 & \\includegraphics[scale=0.06]{click-tight.png} \\includegraphics[scale=0.095]{swipe_2.jpg}\\includegraphics[scale=0.045]{keyboard.jpg} \\\\\n        \\hspace{1mm} PIXELHELP~\\cite{li-etal-2020-mapping} & 187 &  Low & \\color{red}{\\ding{55}} & 4 & \\color{red}{\\ding{55}}\\\\ %& 4 &\\color{red}{\\ding{55}}& 4  &\\includegraphics[scale=0.06]{click-tight.png} \\\\\n        \\hspace{1mm} MoTIF (Ours) & 6,100 & High \\& Low & \\color{green}{\\ding{51}} & 125  & \\color{green}{\\ding{51}} \\\\ % & 8 &  \\includegraphics[scale=0.06]{click-tight.png} \\includegraphics[scale=0.095]{swipe_2.jpg}\\includegraphics[scale=0.045]{keyboard.jpg} \\\\\n         \\hline\n    \\end{tabular}\n    \\label{tab:compare}\n\\end{table}\n\n\\noindent\\textbf{Vision-Language Navigation}\nThere are datasets that strictly navigate to locations like Room-to-Room~\\cite{vln} and Room-Across-Room~\\cite{ku-etal-2020-room}, as well as interactive datasets where agents perform actions in the environment to complete a goal like ALFRED~\\cite{ALFRED20}. MoTIF is most similar to interactive VLN, as the natural language instructions are intended to complete a goal for the user, which requires clicking, typing, or swiping actions in the environment. However, an advantage of MoTIF is that it is a real, non-simulated domain to study interactive navigation, unlike all VLN prior work which uses simulated data~\\cite{IQA,virtualhome8578984,ALFRED20,Zhu2017VisualSP}.\n\\smallskip\n\n\\noindent\\textbf{Digital Task Automation}\nPrior work has not studied web task automation in a multimodal setting, ignoring the rendered website image~\\cite{langtoelem,wob}. The existing datasets MiniWoB~\\cite{wob} and PhraseNode~\\cite{langtoelem} also lack realism, as MiniWoB consists of handcrafted HTML and PhraseNode only captures single action commands on the home screen of websites. Unlike these datasets which limit interaction to a single screen, MoTIF contains action sequences with many different states (as shown in Figure~\\ref{fig:motivation}), with a median of eight visited screens. \n\nRicoSCA and PIXELHELP were introduced for mobile app task automation by Li \\etal~\\cite{li-etal-2020-mapping}. RicoSCA makes use of the mobile app dataset Rico~\\cite{rico}, which captures random exploration in Android apps. Li \\etal\\hspace{0.25mm} synthetically generate random commands with templates like ``\\textit{click on} \\textbf{x}'' and stitch multiple together to any prescribed length. \nThese generated step-by-step instructions do not reflect downstream use, where users ask for a high-level goal. For MoTIF, we instead collect free form high-level goals, and then post-process our data to automatically generate the low level subgoal instructions.\nPIXELHELP is a small mobile app dataset, but most commands are device specific. \\Ie, the tasks refer to the phone itself, such as ``\\textit{in the top control menu click the battery saver},'' and are not in-app tasks like those in Figure~\\ref{fig:motivation}. \nPIXELHELP also only contains clicking, while MoTIF has clicking, typing and swiping actions.\n\n\\section{MoTIF Dataset}\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[scale=0.17275]{modalities.jpg}\n    \\caption{We illustrate captured app modalities: the rendered screen and view hierarchy, which contains element metadata such as the Android class, resource ID, and text}\n    \\label{fig:modes}\n\\end{figure}\n\nFor a mobile app task dataset, we need natural language tasks for apps and their resulting action sequence. Figure~\\ref{fig:motivation} illustrates MoTIF tasks like ``\\textit{open settings and change temperature unit to C}.'' For each command, we collect expert demonstrations of attempts to complete the request. At each time step we capture the app screen, the app backend view hierarchy, what type of action is taken, and where the action occurred. We show the modalities captured at each time step in greater detail in Figure~\\ref{fig:modes}. The Android app backend, \\ie, view hierarchy, is a tree-like structure akin to the Document Object Model (DOM) used for HTML. It organizes each screen element hierarchically, and contains additional metadata like the Android class of an element (\\eg, a text view or image view), its resource identifier, the text it contains, whether it is clickable, and other attributes.\n\n\\subsection{Data Collection}\n\\label{sec:data}\n\nWe provide a general framework for others to collect natural language data with unknown feasibility; Figure~\\ref{fig:collection} illustrates the collection pipeline. We select 125 apps for MoTIF over 15 app categories (the complete app list can be found in the Supplementary). \nTen apps with (1) at least 50k downloads and (2) a rating higher than or equal to 4/5 were chosen for each category.\nNext, a first set of annotators writes commands. A list of (app, task) pairs are then provided to a second set of annotators in an interactive session,\nwhere they attempt the task, specify if it is not feasible, and can ask a clarifying question if not. \nThe Supplementary includes annotator demographics, payment, and collection interface details. \n\n\\smallskip\n \n\\noindent\\textbf{Natural Language Commands}\n\\label{nlanns}\nTo collect natural language tasks, we instruct workers to write commands as if they are asking the app to perform the task for them.\nAnnotators can explore the app before deciding on their list of tasks. We ask them to write functional or navigational tasks, and not commands requiring text comprehension like summarizing an article.\nWe neither structure the written tasks nor prescribe a specific number of tasks to be written for each app.\n\\smallskip\n\n\\noindent\\textbf{Task-Application Pairing}\n\\label{pair}\nWhen collecting natural language tasks, annotators can first explore the app. Once we have tasks for every app, we introduce additional feasibility uncertainty for the demonstration stage by collecting demos for both the original (app, task) list, as well as tasks paired with apps they were not originally written for.  \nWe create these additional (app, task) pairs by clustering tasks within each Android category (for example, clustering all tasks for Music and Audio Android apps) and selecting representatives from each cluster. These representative tasks are then collected for all apps of that category, which we coin ``\\textit{category-clustered}.'' Specifically, we cluster the mean FastText embedding~\\cite{conneau2017word} of the language commands using K-Means~\\cite{kmeans}.\n\nClusters are visualized with T-SNE~\\cite{vanDerMaaten2008} (see Supplementary). If a particular app's tasks are isolated from other clusters, we retain ``\\textit{app-specific}'' pairings, \\ie, the (app, task) pairs for tasks specifically written for the given app. This resulted in 40 apps having only app-specific tasks. If two apps' tasks are closely clustered, we group them; \n 17 apps' tasks were gathered this way. \nFigure~\\ref{fig:motivation} (bottom) shows a category-clustered task which was deemed infeasible by annotators. The command ``\\textit{open settings and clear search history}'' was paired with the music app Spotify even though it was not written for it. This is a sensible request given that Spotify is a music streaming app. Yet, no search history is found under settings, only the option to ``delete cache,'' and follow-up questions are asked.\n\n\\begin{figure}[t]\n    \\centering\n    {\\includegraphics[scale=0.2]{collection.jpg}\n        \\caption{The data collection pipeline (see Section~\\ref{sec:data}). Colored boxes (app, task, demonstration, and feasibility collection) are stages of curating the dataset}\n    \\label{fig:collection}\n    }\n\\end{figure}\n\\label{sec:analysis}\n\n\\smallskip\n\n\\noindent\\textbf{Task Demonstration and Feasibility Annotations}\n\\label{demofeasanns}\nOnce the language commands are paired with apps, we instruct new annotators to demonstrate the task in the given app. We provide a website interface connected to physical Android phones for crowd workers to interact with, as well as anonymized login credentials so that no personally identifiable information is collected. They are instructed to record their demonstration after they have logged in (we consider logging in to be a separate task). After attempting to complete the task, they are brought to a post-survey where they provide details on whether or not the task was successfully completed. We therefore have demonstrations of actions taken both in successful and unsuccessful episodes, which may provide interesting insight toward how to reason about whether a task is or is not feasible, and why.\n\n\\smallskip\n\n\\noindent\\textbf{Natural Language Commands}\nWe collected over 6.1k natural language tasks across 125 Android apps. \nThe vocabulary size was 3,763 after removing non-alphanumeric characters. %, or 3,658 after stop words are removed. \nThe average number of tasks submitted per app is 56, with average length being 5.6 words. The minimum task length is one, consisting of single action tasks like `refresh' or `login,' with the longest at 44 words. \nWord cloud visualizations, additional examples and statistics are in the Supplementary. % \\ab{need more here, check stats and add in examples of tasks how they're different from prior work}\n\\begin{table}[t]\n    \\centering\n      \\renewcommand\\arraystretch{0.95}\n        \\caption{Task feasibility and follow-up question breakdown. Annotators can state the action: can't be completed (impossible), is under-specified (unclear), may be possible, but are unsure how or other tasks need to be completed first (premature)}\n    \\begin{tabular}{|c|c|c|c|c|c|}\n    \\hline\n      \\multirow{2}{*}{\\#} & \\multirow{2}{*}{Feasible}  &  \\multicolumn{3}{c|}{Infeasible} & \\multirow{2}{*}{Total} \\\\\n      \\cline{3-5}\n      & & Impossible & Unclear & Premature & \\\\\n      \\hline\n     Task Demonstrations & 3,337 & 911 & 159 & 300 & 4,707 \\\\ % Demos & 3,323 & 894 & 155 & 295 & 4,667 \\\\\n      \\hline\n      Follow-Up Questions & 93 & 253 & 136 & 164 & 646 \\\\ % F/U Qs & 229 & 372 & 154 & 236 & 991 \\\\\n      \\hline\n    \\end{tabular}\n    \\label{tab:possible}\n\\end{table}\n\\smallskip\n\n\\noindent\\textbf{Feasibility Annotations}\nWe collect at least five expert demonstrations per (app, task) pair for two purposes: to reach a majority feasibility label and to capture different attempts of the same task, as some tasks can be completed in multiple ways. See the Supplementary for an annotator agreement histogram.\n\nOf the resulting tasks, 29.2\\% are deemed infeasible by at least five crowd workers. \nHowever, the tasks considered infeasible do not always correlate to mismatched (app, task) pairs, \\ie, some \n\\textit{app-specific} tasks are deemed infeasible during demonstration. This confirms the need to study commands with unknown feasibility, as someone familiar with an app can still pose requests that are either not possible, ambiguous, or state dependent. Of the infeasible tasks, 16.8\\% are from app-specific pairs. %23\n\\Eg, the request ``\\textit{click shuttle and station}'' originally written for the NASA app was labeled infeasible because the app has changing interactive features. Thus app changes and dynamic features also motivate studying infeasible requests, as a task that was once feasible may not always be.\n\nTable~\\ref{tab:possible} provides statistics on the number of task demonstrations and follow-up questions per feasibility category. There are three options for annotators to choose from: (1) the action cannot be completed in the app, (2) the action is unclear or under-specified, or (3) the task seems to be possible, but they cannot figure out how or other tasks need to be completed first. These map to Table~\\ref{tab:possible}'s impossible, unclear, and premature columns. \nIf a crowd worker cannot complete the task, they are prompted to ask a follow-up question. We instruct them to write the question(s) such that if they had the answer, they may now be able to complete the original action or perform an alternative task for the user. \n\n\\section{Task Feasibility Experiments}\nWe first perform experiments with MoTIF for task feasibility. \nGiven a natural language command and the app states visited during its demonstration, the purpose of task feasibility prediction is to classify if the command can be completed. To determine feasibility, we expect a model to learn the most relevant state for the requested task and if the functionality needed to complete it is present. \nOur results provide an initial upper bound on performance, as the input action sequences can be considered the ground truth exploration needed to determine feasibility, as opposed to a learned agent's exploration.  \nMoTIF has 4.7k demonstrations and we reserve 10\\% for testing. Note that our test set only includes (app, task) pairs for which all annotators agreed on their feasibility annotation.\n\n\\subsection{Models} We propose a Multi-Layer Perceptron (MLP) baseline with two hidden layers that outputs a binary feasibility prediction. Each MLP is trained for 50 epochs with cross entropy using Stochastic Gradient Descent with a learning rate of 1e-2. The natural language command is always input to the classifier, and we ablate which app environment features are additional input. In addition to the feature ablations, we ablate how the demonstration sequence is aggregated (averaging or concatenating over time steps or using the last hidden state of an LSTM~\\cite{lstm}). \n\\smallskip\n\n\\noindent\\textbf{Features}\nWe encode the task command and view hierarchy elements per step with mean pooled features. Specifically, we try both FastText~\\cite{bojanowski-etal-2017-enriching} and CLIP~\\cite{clip} (trained with a Transformer backbone for its image and text encoders~\\cite{dosovitskiy2021image,transformer}). \nAs seen in Figure~\\ref{fig:modes}, the view hierarchy captures all rendered app elements and their attributes: the element's text (ET), resource-identifier (ID) and class labels (CLS) which provide content and type information. We use the best combination of these attributes in Table~\\ref{tab:feas} and have more ablations in the Supplementary. We also include Screen2Vec~\\cite{screen2vec} in our view hierarchy representations. Screen2Vec is a semantic embedding of the view hierarchy, representing the view hierarchy with a GUI, text, and layout embedder. The GUI and text encoders make use of BERT features while the layout features are learned with an autoencoder. Thus, it tries to encode both textual and structural features, but no visual information.\n\nFor visual features, we extract ResNet152~\\cite{resnet} features for ten crops of each app image and CLIP features of each whole app image. We also include icon features by cropping all icon images per screen (\\eg, the menu and umbrella icons shown in Figure~\\ref{fig:modes}). We embed each icon image using the embedding layer of a CNN trained for the downstream task of icon classification by Liu \\etal~\\cite{designsemantics}.\n\\smallskip\n\n\\noindent\\textbf{Metrics}\nWe report the average F1 score over ten runs with different random initialization. ``Infeasible'' is defined as the positive class, as we care more about correctly classifying tasks that are infeasible, than misclassifying feasible tasks.\n\n\\subsection{Results}\n\\begin{table}[t]\n        \\caption{Task feasibility F1 score using our MLP. We ablate input features and action sequence aggregation. The random baseline predicts a feasibility label given the train distribution. On the right is a confusion matrix for the predictions of our best classifier}\n\\begin{minipage}{0.6\\textwidth}\n    \\centering\n          \\renewcommand\\arraystretch{0.95}\n\n    \\begin{tabular}{|l|c|c|c|}\n    \\hline\n       \\multirow{2}{*}{\\textbf{C}$_{feas}$ Input Features} & \\multicolumn{3}{c|}{Demo Aggregation}\\\\\n       \\cline{2-4}\n       & Avg & Cat & LSTM \\\\\n       \\hline\n       \\textbf{Random} & \\multicolumn{3}{c|}{20.1} \\\\\n       \\hline\n     \\textbf{(a) View Hierarchy} & & & \\\\ \n     FastText~\\cite{bojanowski-etal-2017-enriching} (ET, ID) & 16.7 & 43.6 & 34.1 \\\\ \n     CLIP~\\cite{clip} (ET, ID) & 28.0 & 50.9 & 36.2 \\\\\n     Screen2Vec~\\cite{screen2vec} & 25.9 & 33.7 & 36.0 \\\\ \n      \\hline\n      \\textbf{(b) App Screen Image} & & & \\\\\n     ResNet~\\cite{resnet} & 31.3 & 41.9 & 35.9 \\\\ \n    Icons~\\cite{designsemantics} & 0.4 & 40.0 & 15.2 \\\\\n    CLIP~\\cite{clip} & 44.7 & 58.2 & \\underline{42.8} \\\\\n      \\hline\n      \\textbf{(c) Best Combination} & & & \\\\\n      CLIP~\\cite{clip} (Screen, ET, ID) & \\underline{44.8} & \\underline{61.1} & 40.9 \\\\\n      \\hline\n    \\end{tabular}\n\\end{minipage}\n\\begin{minipage}{0.4\\textwidth}\n\\centering\n\n\\begin{tabular}{c >{}r @{\\hspace{0.7em}}c @{\\hspace{0.4em}}c @{\\hspace{0.7em}}l}\n\\hline\n\\multicolumn{4}{c}{\\multirow{2}{*}{\\textbf{{(c) Best Combination}}}} \\\\  \\\\\n\\hline\n\\\\\n  \\multirow{12}{*}{{\\rotatebox{90}{\\textbf{Prediction}}}} & \n    & \\multicolumn{2}{c}{\\textbf{Ground Truth}} & \\\\\n  & & Feasible & Infeasible \\\\\n  & \\rotatebox{90}{\\hspace{-7.5mm}Feasible} & \\MyBox{\\hspace{2.25mm}76.4\\%} & \\MyBox{\\hspace{3mm}8.6\\%} \\\\[2.4em]\n  & \\rotatebox{90}{\\hspace{-8.5mm}Infeasible} & \\MyBox{\\hspace{2.5mm}4.0\\%} & \\MyBox{\\hspace{2.5mm}11.0\\%} \\\\\n\\end{tabular}\n\\end{minipage}\n\\label{tab:feas}\n\\end{table} \n\n\\label{feasperf}\nOur best task feasibility classifier (Table~\\ref{tab:feas}(c) left) achieves an F1 score of 61.1 when CLIP embeds the task, view hierarchy, and app screen image. This is still fairly low, and feature ablations demonstrate room to improve both the language and visual representations. While CLIP has shown significant performance gains in other vision-language tasks, it is somewhat surprising that domain-specific embeddings (\\eg, Screen2Vec, Icons) are not as competitive. The combination of view hierarchy and app screen features does not largely outperform the app screen image CLIP results (and does worse with LSTM aggregation), suggesting a need for better vision-language encodings which can pull features together from different modalities such as the view hierarchy. \n\nWe include the confusion matrix on the right of Table~\\ref{tab:feas} for our best model. In downstream use, the classifier would result in 5\\% of tasks being missed out on; \\ie, 5\\% of tasks were incorrectly classified as infeasible. This reduces the utility of assistive applications, where we'd like all possible commands to correctly be completed. However, the 44\\% of infeasible tasks that were incorrectly classified as feasible can have more negative consequences. In application, this means a vision-language model would attempt to complete an unsatisfiable request, resulting in unknown behavior. We need downstream models to behave in reliable ways, especially for users that cannot verify the task was reasonably completed.\n\nTable~\\ref{tab:feas}(a) left compares methods of encoding the view hierarchy. Using CLIP for view hierarchy elements results in notably better performance than FastText, albeit less significant when input demos are aggregated with an LSTM. Our final view hierarchy embedding is Screen2Vec which performs worse than CLIP and on par with FastText, despite being trained on mobile app data. Screen2Vec may not capture enough low level view hierarchy information to predict feasibility, and methods trained on huge data, even if from another domain, are more powerful.\n\nIn Table~\\ref{tab:feas}(b) left we ablate over the visual representations of the app screen.\nWhile icon representations are trained on images from the same domain as MoTIF, they are significantly less effective than ResNet and CLIP. The F1 score nearly drops to zero when the average icon feature is used, illustrating that the average icon does not carry useful information for feasibility classification. Icon features may be too low-level or require improved aggregation methods.\n\nComparing demonstration aggregation methods (averaging, concatenating, or LSTM), there is a trend that concatenating time steps is the best method, suggesting a sequential representation of the action sequence is needed.\nHowever, when the best representations for the view hierarchy and app screen are combined in Table~\\ref{tab:feas}(c), averaging manages to outperform the LSTM performance. \n\nIn future work we hope to learn hierarchical representations in order to encode global information such as that of Screen2Vec as well as local information from icon embeddings. Taking advantage of the tree structure from the view hierarchy via Transformers or Graph Neural Networks may help learn structured app features. Additionally, all current approaches do not take into account any notion of app ``affordance,'' \\ie, which app elements are most actionable.\n\\section{Task Automation Experiments}\nIn app task automation, we are given an app environment (with all of its modalities) and a language command. The goal is to interact with the app and output a sequence of app actions that complete the task, akin to interactive VLN. At each time step there are two predictions: an action (clicking, typing, or swiping) and a localization (grounding visually on the app screen or classifying over the app elements). We benchmark several methods and analyze performance below. % quantitatively and qualitatively.\n\\subsection{Models} \nWe adapt three models for the mobile app domain with as few changes as possible. The VLN approaches described below (Seq2Seq and MOCA) take both the high-level goal and low level instructions as input while Seq2Act only supports low level instruction. In the supplementary we include input language ablations to consider what performance with real downstream use would look like.\n\\smallskip\n\n\\noindent\\textbf{Seq2Seq} is a VLN model for the ALFRED, a dataset of actionable commands for tasks in household environments. \nIt originally predicts an action and binary mask at each time step. The mask isolates the household object on which the action is performed. The features at each time step include the attended language instruction, the current step's visual features, the last predicted action, and the hidden state of a BiLSTM which takes the former as input. The previous step's BiLSTM hidden state attends to the language input. %of this LSTM from the previous time step.\nThese features are passed to a fully connected layer with Softmax for action prediction and a deconvolutional network for mask prediction. \nWe replace the mask prediction network for three fully connected layers that predict a point in the app screen and minimize the mean squared error. Action prediction is trained via cross entropy.\n\\smallskip\n\n\\noindent\\textbf{MOCA}~\\cite{singh2020moca}, also proposed for ALFRED, decouples the action and grounding predictions of each step in a VLN sequence. One model stream is for the action prediction policy, and another for interaction grounding. Both streams first use a BiLSTM language encoder, which take the high-level goal or low level instruction as input, respectively. The encoded tokens are attended to using ResNet visual features via dynamic attention filters. Then, two LSTM decoders are used: one for the action policy stream and another for the interaction grounding.\n\nAt test time MOCA makes use of an off-the-shelf object segmentation model to perform grounding given the predicted object class. To adapt the object class prediction to mobile apps, we instead perform app element type prediction (prediction is over twelve classes, including button, checkbox, edit text, image view, and more). As no such segmentation model exists for mobile apps yet, we also predict bounding box localization directly using the LSTM decoder output, but use the app element type prediction to narrow grounding options at evaluation. \n\n\\smallskip\n\n\\noindent\\textbf{Seq2Act}~\\cite{li-etal-2020-mapping} %is designed specifically for modeling mobile app tasks.\nmodels mobile app task automation in two stages: action phrase extraction and action grounding. Both stages are modeled with Transformers. \nThe first model predicts a span (\\ie, substring) of the original input command that corresponds to the action type, action location, and action input. \nIt has an encoder-decoder architecture: the encoder embeds the instruction's text tokens and the decoder computes a query vector for the action type, location, and input phrases \ngiven the previously decoded spans. A text span is selected for each decoder query (action type, action location, action input) via cosine similarity. \n\nThe action grounding model takes each extracted phrase as input to predict an action type and location (which app element it is performed on). Actions are predicted given the encoder embedding of the predicted action type span via a Multi-Layer Perception. To localize the action, a Transformer is trained to embed app elements using the view hierarchy attributes as shown in Figure~\\ref{fig:modes}.\nA Softmax is applied to the similarities of the predicted app location span embeddings and the latent app element representations. \nThe max scoring app element becomes the grounding prediction for that time step.\n\\smallskip\n\n\\noindent\\textbf{Datasets} We evaluate task automation on two MoTIF test splits: an app seen and an app unseen split to study generalization to new environments; generalization of tasks across apps is provided in the Supplementary. We jointly train VLN models on MoTIF and RicoSCA for additional data (see the Supplementary for additional experiments trained solely on MoTIF). Seq2Act was originally trained on RicoSCA and we adapt its training data split to be able to evaluate seen versus unseen apps at test time.\n\\smallskip\n\n\\noindent\\textbf{Features} Visual features for Seq2Seq and MOCA are from the last convolutional layer of a ResNet18, as done for the original models; these features are needed for meaningful localization on the mobile app screen.\nWe also include CLIP features of the screen at each time step. Note that VLN methods require a test-time environment; we build an offline version of each Android app to approximate a complete state-action space graph. Details on the creation of these graphs can be found in the Supplementary. Seq2Act does not use off-the-shelf features as input; all text and app element embeddings are learned from scratch. \n\n\\begin{table}[t]\n    \\centering\n          \\renewcommand\\arraystretch{0.95}\n\n        \\caption{Mobile app task accuracy on MoTIF. We evaluate the Seq2Seq and MOCA navigation models and the Transformer grounding model Seq2Act}\n    \\begin{tabular}{|l|c|c|c|c|c|c|}\n    \\hline\n    \\multirow{3}{*}{Model} & \\multicolumn{3}{c|}{App Seen} & \\multicolumn{3}{c|}{App Unseen}\\\\\n    \\cline{2-7}\n   & \\multirow{2}{*}{Action} & \\multirow{2}{*}{Ground} & Action + &  \\multirow{2}{*}{Action} & \\multirow{2}{*}{Ground} & Action +\\\\\n   & & & Ground& & & Ground\\\\\n    \\hline\n    \\textbf{(a) Seq2Seq}~\\cite{ALFRED20} & & & & & &  \\\\\n    \\hspace{5mm} Complete Sequence & 68.5 & 22.5 & 22.5 & 54.3 & 18.0 & 17.7 \\\\\n    \\hspace{5mm} Partial Sequence & 89.5 & 40.4 & 40.1 & 81.7 & 31.3 & 30.6 \\\\\n    \\hline\n    \\textbf{(b) MOCA}~\\cite{singh2020moca} & & & & & &\\\\ \n    \\hspace{5mm} Complete Sequence & 51.1 & 21.3 & 20.7 & 44.8 & 17.0 & 15.1 \\\\\n    \\hspace{5mm} Partial Sequence & 78.5 & 40.0 & 38.6 & 72.2 & 32.7 & 30.0 \\\\\n    \\hline\n    \\textbf{(c) Seq2Act}~\\cite{li-etal-2020-mapping} & & & & & &\\\\\n    \\hspace{5mm} Complete Sequence & \\underline{97.3} & \\underline{32.4} & \\underline{32.4} & \\underline{96.8} & \\underline{28.3} & \\underline{28.3} \\\\\n   \\hspace{5mm} Partial Sequence  & \\underline{99.2} & \\underline{66.4} & \\underline{66.3} & \\underline{99.6} & \\underline{67.7} & \\underline{67.6}  \\\\\n    \\hline\n    \\end{tabular}\n    \\label{tab:automate_full}\n\\end{table}\n\\smallskip\n\n\\noindent\\textbf{Metrics}\nWe report complete and partial sequence accuracy per~\\cite{li-etal-2020-mapping}: the complete score for an action sequence is 1 if the predicted and ground truth sequences have the same length and the same predictions at each step, else 0. The partial sequence score is the fraction of predicted steps that match the ground-truth. These are reported for action prediction (Action), action grounding (Ground), or both jointly. \nSeq2Seq localization is correct if the predicted point falls within the bounding box of the ground truth app element. MOCA localization is correct if the predicted bounding box and ground truth have an IoU greater than 0.5.\n\\subsection{Results}\nDespite MOCA being a more recent model for interactive vision-language navigation, it generally does not outperform Seq2Seq.\nThe app element type prediction MOCA uses may be responsible for the similar or lower accuracy, as the original intention of object class prediction was to narrow down grounding interaction to very few options. \\Eg, in the home environments of ALFRED, which MOCA evaluated on, the object class predicted may be apple. If there is only a single apple in the scene, the object segmentation model would be highly effective for grounding. The mobile app domain differs in that there are many app elements per time step of the same type, \\eg, there are many app icons or app buttons, and this prediction may not significantly reduce the grounding prediction space.\n\nThe Seq2Seq and MOCA models perform worse than Seq2Act. While additional model ablations may improve performance, it is clear that action localization on the continuous app screen is more challenging.\nSeq2Act achieves the highest performance for all metrics. Seq2Act was originally evaluated with PIXELHELP~\\cite{li-etal-2020-mapping} and achieved 70.6\\% complete Action + Ground accuracy on it, much higher than the accuracy reported on MoTIF. This may be due to PIXELHELP containing click-only tasks for four test environments, which does not reflect the model's performance on a greater variety of apps or tasks. MoTIF's step-by-step instructions also contain location descriptions for app elements which don't contain text, differing from the Seq2Act training data distribution.\n\nQualitatively inspecting misclassifications, we find one culprit to be Seq2Act overly relying on matching input task text to view hierarchy text. In Figure~\\ref{fig:qual}, we show Seq2Act's text matching tendency, which can result in failure. For example, Seq2Act predicts the app element with the word ``my'' in it for the input command ``\\textit{go to my profile}.'' These results, in addition to the high visual performance from the feasibility classifier, verifies the need for visual input to correct model bias to match input task text directly to the view hierarchy.  \n\nPerformance is unsurprisingly worse for unseen app environments. \nWe suspect that current model formulations do not learn enough about app elements outside of the ground truth action sequences during training. None of the benchmark models include exploration, and as a result, may be biased to the small subset of elements seen in expert demonstration. In future work, using pretrained generic app features or incorporating exploration into the training process through reinforcement learning approaches may alleviate this.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[scale=0.15]{qual_s2a_frl.jpg}\n    \\caption{Seq2Act text matching. Green and red boxes are valid and invalid predictions, respectively; black are additional valid ground truth. The left shows valid text matching, identifying ``notifications'' in the app Pinterest. The right shows Seq2Act incorrectly matching ``my'' in the input task to the app element ``My favorite'' in the Opera news app}\n    \\label{fig:qual}\n\\end{figure}\n\\section{Discussion}\nWe find our best task feasibility prediction results to be low at a 61.1 F1 score, given that the input demonstrations serve as the oracle exploration needed to determine feasibility. In addition to improving vision-language feasibility reasoning, a necessary next step is to instead use learned explorations during training. Our ablations demonstrate that visual inputs are useful for feasibility prediction, and research toward better mobile app features that actually use the rendered screen could increase performance. \nBuilding hierarchical visual and textual features may provide stronger context clues for determining command feasibility in the app environment. We also hope to perform experiments classifying why tasks are not feasible and automating question generation in response, making use of MoTIF's subclass annotations for infeasible tasks and follow up questions.\n\nBy evaluating action and grounding performance independently, we found that models for completing mobile app tasks can have more difficulty grounding and consistently perform more poorly in new app environments. \nBetter representations of app elements are needed;\nspecifically, incorporating pretraining tasks for improved app features or allowing for exploration outside of ground truth action sequences may be necessary to diversify test time predictions.\n\n\\smallskip\n\\noindent\\textbf{Limitations}\nThe MoTIF dataset is not on the scale of pretraining datasets used in other VL tasks (\\eg, Alt-Text~\\cite{jia2021scaling}, JFT-300M~\\cite{jft300}), as it is very expensive and time costly to collect natural language commands and feasibility labels. MoTIF is nonetheless useful to the research community as it can be used to evaluate how existing methods would solve language-based digital tasks in realistic settings.\n\n\\smallskip\n\\noindent\\textbf{Societal Impact}\nMethods for automating language commands and predicting command feasibility can be used to assist people who are not able to interact with apps, either situationally (\\eg, while driving) or physically (\\eg, users who are low-vision or blind). Improving mobile app task automation could better balance the capabilities of current assistive technologies, which typically lack agency or flexibility~\\cite{screenreader}. \\Eg, screen readers are primarily used for web browsing and information consumption (lacking agency), while interactive virtual assistants (\\eg, Siri, Alexa) have limited, structured commands (lacking flexibility). \n\nMoTIF's collection was designed to ensure no personally identifiable information is captured. But, in downstream use of app task automation, user privacy is of concern. People who use assistive tools (\\eg, people who are blind) already expose sensitive information to other humans to receive help~\\cite{visprivacy,Akter2020IAU}. To mitigate potential harm, deployment of our research can be limited to apps which do not require log in information; these are less likely to include name, address, or payment data. MoTIF does not have tasks which require payment, and we can deny payment related tasks to prevent fraud and other undesired outcomes. \n\n\\section{Conclusion}\n\\label{sec:conc}\nWe introduced Mobile app Tasks with Iterative Feedback (MoTIF), a new VLN dataset that contains natural language commands for tasks in mobile apps which may not be feasible. MoTIF is the first dataset to capture task uncertainty for interactive visual environments and contains greater linguistic and visual diversity than prior work, allowing for more research toward robust vision-language methods. We introduced the task of feasibility prediction and evaluate prior methods for automating mobile app tasks. Results verify that MoTIF poses new vision-language challenges, and that the vision-language community can make use of more realistic data to evaluate and improve upon current methods.\n\n\\smallskip\n\\noindent\\textbf{Acknowledgements} This work is funded in part by Boston University, the Google Ph.D. Fellowship program, the MIT-IBM Watson AI Lab, the Google Faculty Research Award and NSF Grant IIS-1750563.\n\n\\section*{\\Large Supplementary}\n\n\\section{MoTIF Collection}\n\\label{sec:collection}\n\nFor data collection, we use UpWork\\footnote{\\url{https://www.upwork.com/}} as our crowd sourcing platform and hired 34 people to collect our dataset. Of the annotators, 21 identified as female and 13 identified as male. The median age of the annotators was 23.5 years old. Annotators were from 18 different states in the U.S. and had a range of education from a high school diploma to a master's degree (2 have high school degrees, 24 have bachelor's degrees, and 8 have master's degrees).\n\nAnnotators were selected on UpWork if their profile skills listed data entry. As the initial iteration of MoTIF is in English, we also required annotators be fluent in English, but did not require them to be native speakers. We posted separate job listings for the task writing (base rate \\$15/hr) and task demonstration (base rate \\$10/hr) portions of the data collection, having independent annotators for the two stages. Annotators hired for the task writing portion were not informed of our interest in potentially ambiguous or infeasible tasks.\n\nFor the annotators hired for task demonstration, we additionally required them to have personal experience with Android devices so that there was no additional noise introduced from people unfamiliar with Android apps. We created anonymized login information for annotators so that no personally identifiable information was collected. Additional interface details and an example of the interface used by the workers (Figure~\\ref{fig:website}) is provided in Section~\\ref{interface}. \n\\subsection{Data Collection Interface}\n\\label{interface}\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[scale=0.3]{collectioninterface.PNG}\n    \\caption{The website interface annotators use to interact with an Android app and record their task demonstration. We provide anonymized information if needed for logging in or for forms at any point so that no personal identifying information is collected}\n    \\label{fig:website}\n\\end{figure*}\nWe provide an example of what our data collection interface looks like for annotators while they explore an Android app and perform a task demonstration in Figure~\\ref{fig:website}. Annotators are given the natural language task to attempt within the Android app in the `Your Task' section on the right side of the interface. Below, we provide anonymized email login and password credentials for them to use if needed. The left hand side of the collection interface displays the phone screen from a physical Android device which is remotely connected to our collection website, from which we record all actions taken on the phone and the app modalities as described in the main text.\n\n\\subsection{Application List}\nWe include lists of all Android apps we collect demonstrations for in Tables~\\ref{tab:apps1}-\\ref{tab:apps3}. In addition to listing the app package name, we provide the corresponding Google Play Store Category and how that particular app's tasks were paired (app-specific, paired, or category-clustered). The apps selected for MoTIF were across fifteen app categories: lifestyle, communication, dating, food and drink, maps and navigation, news and magazines, productivity, shopping, social, travel, weather, tools, music and audio, entertainment, and education. For privacy, we do not intend to collect any demonstrations of natural language commands within dating apps, and will not be releasing any of the raw data collected when annotators decided on a list of natural language tasks for dating apps in the first stage of collection. We simply include dating apps as one Android category to see what kinds of tasks people would consider being automated in this setting. We will share the resulting natural language tasks, but no captured screen or view hierarchy data. The dating apps included com.wildec.dating.meet4u, com.once.android, emotion.onekm, ru.fotostrana.sweetmeet, com.mason.wooplus, and com.hitwe.android.\n\n\\begin{table}\n    \\centering\n        \\caption{A list of applications used in MoTIF, their Google Play Store Category, and how their submitted natural language tasks were grouped with applications in the (app, task) pairing stage. N/A refers to apps which has technical difficulties during the demonstration stage and we are working to resolve}\n    \\begin{tabular}{|l|l|l|}\n    \\hline\n         Google Play & \\multirow{2}{*}{App Name} & (app, task) \\\\\n         Store Category &  & Pairing Method \\\\\n         \\hline\n         \\multirow{9}{*}{Education} & com.ted.android & \\textit{app-specific}\\\\\n        \\cline{2-3}\n      &  gov.nasa & \\textit{app-specific}\\\\ \\cline{2-3}\n       & example.matharithmetics & \\textit{paired}\\\\ \\cline{2-3}\n       & org.khanacademy.android  & \\textit{app-specific}\\\\ \\cline{2-3}\n      & com.duolingo  & \\textit{app-specific} \\\\ \\cline{2-3}\n       & com.quizlet.quizletandroid & \\textit{app-specific}\\\\ \\cline{2-3}\n       & com.remind101 & N/A \\\\ \\cline{2-3}\n       & org.coursera.android& N/A \\\\ \\cline{2-3}\n       & com.microblink.photomath & \\textit{paired} \\\\ \\hline\n        \\multirow{8}{*}{Entertainment} & com.megogo.application & \\textit{app-specific} \\\\ \\cline{2-3}\n      &  com.app.emotes.dances.fortnite & \\textit{app-specific}\\\\ \\cline{2-3}\n      &  com.scannerradio & \\textit{app-specific}\\\\ \\cline{2-3}\n       & com.google.android.youtube & \\textit{app-specific} \\\\ \\cline{2-3}\n       & com.zombodroid.MemeGenerator &\\textit{app-specific} \\\\ \\cline{2-3}\n       & tv.pluto.android &\\textit{app-specific} \\\\\\cline{2-3}\n       & com.tubitv & \\textit{app-specific}\\\\\\cline{2-3}\n        & com.imdb.mobile & \\textit{app-specific}\\\\\\cline{2-3}\n        & com.eventbrite.attendee & \\textit{app-specific}\\\\ \n         \\hline\n         \\multirow{6}{*}{Communication} & com.google.android.gm & \\textit{app-specific} \\\\ \\cline{2-3}\n        & com.sec.android.app.sbrowser & \\textit{paired}\\\\\\cline{2-3}\n        & com.facebook.orca & N/A \\\\ \\cline{2-3}\n        & com.whatsapp & N/A \\\\ \\cline{2-3}\n        & org.mozilla.firefox & \\textit{paired} \\\\ \\cline{2-3}\n        & com.skype.raider & N/A \\\\ \\hline\n        \\multirow{8}{*}{Food \\& Drinks} & com.joelapenna.foursquared & \\textit{app-specific} \\\\ \\cline{2-3}\n        & com.yum.pizzahut & \\textit{app-specific} \\\\ \\cline{2-3}\n        & com.chickfila.cfaflagship&\\textit{app-specific}  \\\\ \\cline{2-3}\n        & com.dominospizza & \\textit{paired} \\\\ \\cline{2-3}\n        & in.swiggy.android &\\textit{app-specific}  \\\\ \\cline{2-3}\n        & com.opentable & \\textit{app-specific} \\\\ \\cline{2-3}\n        & com.starbucks.mobilecard & \\textit{app-specific} \\\\ \\cline{2-3}\n       & vivino.web.app& \\textit{app-specific} \\\\ \\hline\n         \\multirow{7}{*}{Lifestyle} & com.hm.goe  & \\textit{app-specific} \\\\ \\cline{2-3}\n        & com.adpog.diary  & \\textit{app-specific} \\\\ \\cline{2-3}\n       &  com.aboutjsp.thedaybefore  & \\textit{app-specific} \\\\ \\cline{2-3}\n       &  info.androidz.horoscope  & N/A \\\\ \\cline{2-3}\n        & ru.mail.horo.android & \\textit{paired} \\\\ \\cline{2-3}\n       &  com.urbandroid.sleep & \\textit{app-specific} \\\\ \\cline{2-3}\n        & com.hundred.qibla & \\textit{app-specific}\\\\\n         \\hline\n    \\end{tabular}\n    \\label{tab:apps1}\n\\end{table}\n\n\\begin{table}\n    \\centering\n        \\caption{A list of applications used in MoTIF, their Google Play Store Category, and how their submitted natural language tasks were grouped with applications in the (app, task) pairing stage. N/A refers to apps which has technical difficulties during the demonstration stage and we are working to resolve}\n    \\begin{tabular}{|l|l|l|}\n    \\hline\n         Google  & \\multirow{3}{*}{App Name} & \\multirow{2}{*}{(app, task)}  \\\\\n         Play Store & & \\multirow{2}{*}{Pairing Method} \\\\\n         Category &  &  \\\\\n         \\hline\n        & com.tranzmate & \\textit{category-clustered} \\\\ \\cline{2-3}\n         & com.mapfactor.navigator& \\textit{category-clustered}\\\\ \\cline{2-3}\n        Maps & com.thetrainline&\\textit{category-clustered} \\\\ \\cline{2-3}\n       \\& Navigation  & com.citymapper.app.release& \\textit{app-specific}\\\\ \\cline{2-3} &  com.prime.studio.apps.route.finder.map& \\textit{category-clustered}\\\\ \\cline{2-3}\n         & com.waze& \\textit{category-clustered}\\\\ \\cline{2-3}\n        & com.nyctrans.it &\\textit{category-clustered} \\\\ \\hline\n          & com.radio.fmradio & \\textit{app-specific}\\\\ \\cline{2-3}\n        & deezer.android.app&  \\textit{app-specific}\\\\ \\cline{2-3}\n         & com.spotify.music& \\textit{category-clustered}\\\\ \\cline{2-3}\n        Music & com.pandora.android& \\textit{category-clustered}\\\\ \\cline{2-3}\n       \\& Audio & com.springwalk.mediaconverter& \\textit{category-clustered}\\\\ \\cline{2-3} \n         & com.google.android.music& \\textit{category-clustered}\\\\ \\cline{2-3}\n         & com.clearchannel.iheartradio.controller& \\textit{category-clustered}\\\\ \\cline{2-3}\n        & com.melodis.midimiMusicIdentifier.freemium & \\textit{category-clustered}\\\\\n         \\hline\n         &  fm.castbox.audiobook.radio.podcast& \\textit{category-clustered} \\\\ \\cline{2-3}\n         & com.ss.android.article.master& N/A \\\\ \\cline{2-3}\n         & com.opera.app.news& \\textit{category-clustered}\\\\ \\cline{2-3}\n        News  & bbc.mobile.news.ww& \\textit{category-clustered}\\\\\\cline{2-3}\n       \\& Magazines  & com.quora.android&  N/A \\\\ \\cline{2-3}\n         & com.google.android.apps.magazines& \\textit{category-clustered} \\\\ \\cline{2-3}\n         & com.reddit.frontpage& \\textit{app-specific}\\\\ \\cline{2-3}\n         & com.sony.nfx.app.sfrc & \\textit{category-clustered} \\\\\n         \\hline \n        \\multirow{7}{*}{Shopping} & com.amazon.mShop.android.shopping& \\textit{app-specific}\\\\ \\cline{2-3}\n         & com.abtnprojects.ambatana&  \\textit{category-clustered}\\\\ \\cline{2-3}\n         & com.contextlogic.wish& \\textit{category-clustered}\\\\ \\cline{2-3}\n         & com.joom& \\textit{category-clustered} \\\\ \\cline{2-3}\n          & com.ebay.mobile& \\textit{category-clustered}\\\\ \\cline{2-3}\n         & com.walmart.android& \\textit{category-clustered} \\\\ \\cline{2-3}\n         & club.fromfactory& \\textit{app-specific}\\\\ \\cline{2-3}\n         & com.zzkko& \\textit{app-specific}\\\\ \\cline{2-3}\n         & com.groupon&  \\textit{category-clustered}\\\\\n         \\hline\n         \\multirow{7}{*}{Productivity} & cn.wps.moffice\\_eng& \\textit{category-clustered}\\\\ \\cline{2-3}\n         & com.google.android.apps.docs.editors.sheets& \\textit{category-clustered}\\\\ \\cline{2-3}\n         & com.google.android.apps.docs& N/A \\\\ \\cline{2-3}\n         & com.microsoft.office.outlook& \\textit{category-clustered}\\\\ \\cline{2-3}\n         & com.google.android.calendar &\\textit{category-clustered} \\\\ \\cline{2-3}\n          & com.google.android.apps.docs.editors.slides& \\textit{category-clustered}\\\\ \\cline{2-3}\n          & com.dropbox.android & N/A \\\\ \\hline\n    \\end{tabular}\n    \\label{tab:apps2}\n\\end{table}\n\n\\begin{table}\n    \\centering\n        \\caption{A list of applications used in MoTIF, their Google Play Store Category, and how their submitted natural language tasks were grouped with applications in the (app, task) pairing stage. N/A refers to apps which has technical difficulties during the demonstration stage and we are working to resolve}\n    \\begin{tabular}{|l|l|l|}\n    \\hline\n         Google  & \\multirow{3}{*}{App Name} & \\multirow{2}{*}{(app, task)}  \\\\\n         Play Store & & \\multirow{2}{*}{Pairing Method} \\\\\n         Category &  &  \\\\\n         \\hline\n          \\multirow{7}{*}{Tools}&  com.lenovo.anyshare.gps& \\textit{app-specific}\\\\ \\cline{2-3}\n         & com.antivirus& \\textit{paired} \\\\ \\cline{2-3}\n       &  com.google.android.calculator& \\textit{paired}\\\\ \\cline{2-3}\n         & com.miui.calculator& \\textit{paired}\\\\ \\cline{2-3}\n        & com.google.android.apps.translate& \\textit{app-specific}\\\\ \\cline{2-3}\n       & com.avast.android.mobilesecurity & \\textit{paired}\\\\\n       \\hline\n       \\multirow{9}{*}{Travel}&  com.kayak.android& \\textit{paired} \\\\ \\cline{2-3}\n         & com.tripadvisor.tripadvisor& \\textit{paired}\\\\ \\cline{2-3}\n         & com.trivago& \\textit{paired}\\\\ \\cline{2-3}\n         & com.google.android.apps.maps& \\textit{paired}\\\\ \\cline{2-3}\n         & com.yelp.android& \\textit{app-specific}\\\\ \\cline{2-3}\n         & com.booking& N/A \\\\ \\cline{2-3}\n         & com.google.earth& \\textit{paired} \\\\ \\cline{2-3}\n         & com.mapswithme.maps.pro& \\textit{app-specific}\\\\ \\cline{2-3}\n         & com.google.android.street& \\textit{paired}\\\\ \\cline{2-3}\n        & com.yellowpages.android.ypmobile & \\textit{app-specific}\\\\\n       \\hline\n      \\multirow{9}{*}{Weather} &  com.gau.go.launcherex.gowidget.weatherwidget& N/A \\\\ \\cline{2-3}\n          & com.devexpert.weather& \\textit{category-clustered} \\\\ \\cline{2-3}\n          & com.chanel.weather.forecast.accu & \\textit{category-clustered}\\\\ \\cline{2-3}\n         & com.weather.Weather& \\textit{category-clustered}\\\\ \\cline{2-3}\n          & com.droid27.transparentclockweather& \\textit{app-specific}\\\\ \\cline{2-3}\n         & aplicacion.tiempo & \\textit{category-clustered}\\\\ \\cline{2-3}\n         & com.accuweather.android & \\textit{category-clustered}\\\\ \\cline{2-3}\n         & com.windyty.android & \\textit{category-clustered}\\\\ \\cline{2-3}\n         & com.handmark.expressweather & \\textit{category-clustered}\\\\\n       \\hline\n       \\multirow{7}{*}{Social}&  com.zhiliaoapp.musically & \\textit{category-clustered} \\\\ \\cline{2-3}\n          & com.pinterest & \\textit{category-clustered} \\\\ \\cline{2-3}\n           & com.instagram.android & \\textit{category-clustered}\\\\ \\cline{2-3}\n          & com.facebook.katana & \\textit{category-clustered} \\\\ \\cline{2-3}\n           & com.sgiggle.production & \\textit{app-specific}\\\\ \\cline{2-3}\n           & com.snapchat.android & \\textit{app-specific}\\\\ \\cline{2-3}\n           & com.ss.android.ugc.boom & \\textit{category-clustered} \\\\ \\cline{2-3}\n           & com.lazygeniouz.saveit & \\textit{category-clustered}\\\\ \\hline\n    \\end{tabular}\n    \\label{tab:apps3}\n\\end{table}\n\n\\subsection{Dataset Examples}\nWe include more example (app, task) pairs and their resulting action sequences from MoTIF. Figure~\\ref{fig:infeas_motif_ex} and~\\ref{fig:feas_motif_ex} show samples for infeasible and feasible commands, respectively.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[scale=0.2]{infeas_supp.jpg}\n            \\caption{Example tasks from MoTIF deemed infeasible by annotators. We show the input (app, task) pair for task demonstration, the resulting task demo (which captures the rendered screen, app view hierarchy, and action localization), and the feasibility annotations and follow up questions posed by annotators}\n                \\label{fig:infeas_motif_ex}\n\\end{figure}\n\n\\begin{figure}[t]\n    \\centering\n\n    \\includegraphics[scale=0.2]{feas_supp.jpg}\n            \\caption{Example tasks from MoTIF deemed feasible by annotators. We show the input (app, task) pair for task demonstration, the resulting task demo (which captures the rendered screen, app view hierarchy, and action localization), and the feasibility annotations and follow up questions posed by annotators}\n    \\label{fig:feas_motif_ex}\n\\end{figure}\n\n\\clearpage\n\n\\section{MoTIF Statistics}\nWe include statistics over the high-level goals collected for MoTIF in Section~\\ref{sec:nl_stats} and word cloud visualizations over all commands and per category in Section~\\ref{sec:wordcloud}. We discuss annotator agreement when determining command feasibility in Section~\\ref{sec:agree}. Lastly, the cluster visualizations used to define (app, task) pairs in MoTIF are illustrated in Section~\\ref{sec:tsne}.\n\n\\label{sec:stats}\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.48\\textwidth}\n  \\centering\n  \\includegraphics[width=1.0\\linewidth]{frequency-histogram.png}\n  \\caption{The word frequency distribution of MoTIF's vocabulary}\n  \\label{fig:wordfreq}\n\\end{subfigure}\n\\begin{subfigure}{0.48\\textwidth}\n  \\centering\n  \\includegraphics[width=1.0\\linewidth]{task-len-histogram.png}\n  \\caption{The length (number of words per task) distribution of MoTIF's tasks}\n  \\label{fig:tasklen}\n\\end{subfigure}\n\\caption{Additional statistics on MoTIF's language tasks}\n\\label{fig:langanalysis}\n\\end{figure}\n\n\\subsection{Natural Language Command Statistics}\n\\label{sec:nl_stats}\nWe provide additional statistics on the natural language high-level goals in MoTIF in Figure~\\ref{fig:langanalysis}. In Figure~\\ref{fig:wordfreq} we plot a histogram over the word frequency of the command vocabulary and Figure~\\ref{fig:tasklen} shows a histogram over the task length (\\ie, how many words a task consists of) across all collected natural language tasks. Both reflect a long tail distribution, which is common for word frequency, and follows Zipf's Law. For task length, the distribution is skewed towards shorter length tasks (nearly all collected tasks have fewer than ten words), which aligns with MoTIF's natural language commands mostly capturing high-level goals.\n\\subsection{Word Cloud Visualizations}\n\\label{sec:wordcloud}\nWe include a word cloud illustration over all high-level commands in MoTIF in Figure~\\ref{fig:word1}. The larger the word in the word cloud, the more often it occurs in MoTIF's collected tasks. As we compute the word cloud over all tasks (which span fifteen different Google Play Store app categories) we can see the largest words are those that are action or instruction oriented words, like `click,' `search,' or `show.' In Figure~\\ref{fig:category_wordcloud}, we show word clouds for tasks per app category. \n\nWhile there are some common words with high frequency across all app categories (like the action oriented words largest in Figure~\\ref{fig:word1}), there are other words illustrated that reflect each app category and functionality specific to that topic. For example, in the Education word cloud in the top left of Figure~\\ref{fig:category_wordcloud}, we see words `lesson,' `math,' and `history.' In contrast, the Shopping category in Figure~\\ref{fig:category_wordcloud} shows words like `deal,' `search,' and `cart' with high frequency.\n\nThe word cloud visualizations also show the density of words for each Android app category's collected tasks. The Food \\& Drink, Productivity, and Music \\& Audio app categories have the smallest vocabularies, with less densely populated word clouds. This reflects there being lower diversity in the kinds of requests asked by people for these app categories. On the other hand, Maps \\& Navigation, Weather, and Travel are examples of Android app categories with larger task vocabularies. This can reflect greater diversity in app requests collected, which may be due to the diversity of functionality in these app categories, or the fact that these apps can have highly specific, \\ie, very fine-grained, requests (like searching for one location's weather out of the nearly unlimited locations one could request). \n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[scale=0.05]{all-tasks-vis.png}\n    \\caption{Word cloud visualization over all MoTIF high-level language commands. The larger the word is illustrated, the more often it occurs}\n    \\label{fig:word1}\n\\end{figure}\n\\begin{figure}\n    \\centering\n    \\includegraphics[scale=0.209]{wordclouds.jpg}\n    \\caption{Word cloud visualization of MoTIF high-level language tasks per Android app category. There are fifteen total categories: Education, Dating, Communication, Food \\& Drink, Entertainment, Lifestyle, Maps \\& Navigation, News \\& Magazine, Music \\& Audio, Shopping, Productivity, Social, Tools, Weather, and Travel. The larger the word is illustrated, the more often it occurs}\n    \\label{fig:category_wordcloud}\n\\end{figure}\n\n\\subsection{Annotator Feasibility Agreement}\n\\label{sec:agree}\nWe define annotator feasibility labeling agreement as the fraction of the number of votes for the majority voted label ($max(C_{yes}, C_{no})$) over all votes ($C_{yes} + C_{no}$) for an (app, task) pair in MoTIF, where $C_{yes}$ is the count of votes for feasible and $C_{no}$ is the count of votes for infeasible. In Figure~\\ref{fig:agree}, we bin different degrees of annotator agreement and plot each bin's counts over all (app, task) pairs with demonstrations in MoTIF. The minimum agreement is 50\\% and maximum agreement is 100\\%. The majority of our (app, task) pairs have annotation agreement between 90-100\\%, with 296 (app, task) pairs falling in this maximal bin.\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[scale=0.55]{annotator_agreement.png}\n    \\caption{The annotator feasibility labeling agreement for (app, task) pairs with demonstrations in MoTIF}\n    \\label{fig:agree}\n\\end{figure}\n\n\\subsection{App Category Clustering Visualizations}\n\\label{sec:tsne}\nWe provide the K-Means T-SNE cluster visualizations used in the (app, task) pairing process for each category of apps in Figure~\\ref{fig:all_app_tsne}. These clusters decide whether an app's tasks are kept app-specific, paired to one or two other apps, or are category clustered. We zoom into the cluster visualization for the Weather Android app category in Figure~\\ref{fig:tsne1}. On the left, we see the cluster output for K-Means on the average task embedding (using FastText representations) for the commands written for weather apps. On the right we show the exact same clustering, but now color the points (\\ie, the written tasks) by which app they were originally written for. In the lower left corner of the cluster visualization is an isolated cluster for the com.droid27.transparentclockweather app. As its tasks form an isolated cluster, they are kept app-specific, while all other apps have (app, task) pairs obtained from the category clustering. %-\\ref{fig:tsne15}.\n\nTo actually select the category clustered tasks, we select natural language commands near each cluster's centroid. These serve as cluster representatives for our task demonstration data collection. So, for every Google Play Store app category, we perform K-Means with K=5, as we start by collecting demonstrations for five commands per app. Then, for apps that are chosen to be category clustered, we select the cluster representatives and collect demonstrations of these representatives for each weather app. For additional clarity, see Tables~\\ref{tab:apps1}-\\ref{tab:apps3} for the (app, task) pairing method per app. Eventually, the goal is to collect all possible combinations of (app, task) pairs within a category.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[scale=0.085]{all_tsne_app.jpg}\n    \\caption{T-SNE visualization of K-Means clusters for each Android Google Play Store Category. The visualizations are colored with the originating app label (and not the K-Means cluster label). These visualizations are used to inspect which apps should retain\n    their app-specific tasks during the action sequence demonstration stage}\n    \\label{fig:all_app_tsne}\n\\end{figure}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[scale=0.1325]{app_tsne.jpg}\n    \\caption{T-SNE visualization of K-Means clusters on MoTIF commands from the Weather Google Play Store app category. Points represent MoTIF commands (represented by their mean FastText embedding). The left plot colors points by the clusters output by K-Means, while the right plot colors points by their originating app. In the lower left corner of both plots is a cluster (the green cluster on the left hand side), which when colored by the app the command was originally written for (on the right hand side), we see primarily comes from a single app, com.droid27.transparentclockweather. As a result, this app's commands will not be category clustered, and will stay paired with com.droid27.transparentclockweather}\n    \\label{fig:tsne1}\n\\end{figure}\n\n\\begin{table}[t]\n    \\centering\n        \\caption{Task feasibility F1 score using our MLP. We ablate input features and how action demonstration sequences are aggregated. The random baseline predicts a feasibility label given the train set distribution}\n    \\begin{tabular}{|l|c|c|c|}\n    \\hline\n      \\multirow{2}{*}{\\textbf{C}$_{feas}$ Input Features} & \\multicolumn{3}{c|}{Demo Aggregation}\\\\\n      \\cline{2-4}\n      & Avg & Cat & LSTM \\\\ %& Mean \\\\\n      \\hline\n      \\textbf{Random} & \\multicolumn{3}{c|}{20.1}\\\\\n      \\hline\n     \\textbf{(a) View Hierarchy} & & & \\\\ %& \\\\\n     FastText & & &  \\\\\n    \\hspace{4mm} ET & 22.8 & 44.3 & 37.0 \\\\\n    \\hspace{4mm} ET + ID & 16.7 & 43.6 & 34.1 \\\\ %& 31.5\n     \n    \\hspace{4mm} ET + ID + CLS & 19.7 & 39.6 & 36.2 \\\\\n    CLIP & &  & \\\\\n    \\hspace{4mm} ET & 27.0 & 48.4 & 35.9 \\\\\n     \\hspace{4mm} ET + ID & 28.0 & 50.9 & 36.2 %& 38.4\n     \\\\\n    \\hspace{4mm} ET + ID + CLS & 29.6 & 49.2 & 35.2  \\\\\n      Screen2Vec & 25.9 & 33.7 & 36.0 \\\\ %& 31.9 \\\\\n      \\hline\n      \\textbf{(b) App Screen Image} & & & \\\\ % & \\\\\n      ResNet & 31.3 & 41.9 & 35.9 \\\\ %& 36.4 \\\\\n     Icons & 0.4 & 40.0 & 15.2 \\\\% & 18.5 \\\\\n     CLIP & 44.7 & 58.2 & \\underline{42.8} \\\\% & 48.6 \\\\\n      \\hline\n      \\textbf{(c) Best Combination} & & & \\\\% & \\\\\n      CLIP (Screen + ET + ID) & \\underline{44.8} & \\underline{61.1} & 40.9 \\\\ \n      \\hline\n    \\end{tabular}\n\n    \\label{tab:full_feas}\n\\end{table} \n\n\\section{Task Feasibility Experiments}\n\\label{sec:feas}\nIn Table~\\ref{tab:full_feas}(a), we have additional rows for which view hierarchy element attributes are included as input features to our feasibility classifier. The view hierarchy of an Android app contains several element attributes, including text (ET), resource-identifier (ID), and class (CLS) attributes. We ablate using one or multiple of these attributes and find that on average across demonstration aggregation type, the (ET + ID) input combination results in the best performance. Consequently, we keep it for our best results in the main text.\n\n\\section{Task Automation Experiments}\nWe further detail how task automation experiments are performed in a vision-language navigation paradigm in Section~\\ref{sec:app_graph}, where we describe the test-time environment. Then, we report performance when training VLN methods only on our data in Section~\\ref{sec:auto_data}. In Section~\\ref{sec:auto_language}, we evaluate our models from the main paper on different language inputs (high-level goal, low-level instruction, or both) at test-time and describe performance trends. Lastly, in Section~\\ref{sec:task_gen} we include some additional results on generalization of tasks across apps for a subset of our baselines. \n\n\\subsection{Test-time Evaluation of Seq2Seq and MOCA}\n\\label{sec:app_graph}\nWe build an offline version of each Android app environment to approximate a complete state-action space graph at test time. We merge demonstrations we've collected across all samples.\nThe nodes in this state-action space graph are unique `views' of an application, \\ie, a particular screen within an action demonstration sequence. Nodes are connected by edges which represent the transition between any pair of screens. This transition is defined by the action class (clicking, typing, or swiping) and the location of the action taken at the current screen state (point or bounding box coordinates in the rendered app screen image).\n\\begin{table}[t]\n    \\centering\n          \\renewcommand\\arraystretch{0.95}\n\n        \\caption{Mobile app task complete and partial sequence accuracy on MoTIF when trained on MoTIF alone, or MoTIF and RicoSCA data for the Seq2Seq model. The training and testing language input are kept the same; input contains the high-level goal and low level step by step instructions }\n    \\begin{tabular}{|l|c|c|c|c|c|c|c|}\n    \\hline\n    & & \\multicolumn{6}{c|}{MoTIF Test Split}\\\\\n    \\cline{3-8}\n      Model & Train & \\multicolumn{3}{c|}{App Seen} & \\multicolumn{3}{c|}{App Unseen}\\\\\n    \\cline{3-8}\n \\textbf{Seq2Seq} & Data & \\multirow{2}{*}{Action} & \\multirow{2}{*}{Ground} & Action + &  \\multirow{2}{*}{Action} & \\multirow{2}{*}{Ground} & Action +\\\\\n   & & & & Ground& & & Ground\\\\\n    \\hline\n    Complete & \\multirow{2}{*}{MoTIF} & 45.0 & 17.1 & 15.9 & 33.8 & 13.6 & 11.7 \\\\\n    Partial & & 79.4 & 37.7 & 35.5 & 66.8 & 27.8 & 25.0 \\\\\n    \\hline\n    Complete & MoTIF + & 68.5 & 22.5 & 22.5 & 54.3 & 18.0 & 17.7 \\\\\n    Partial & RicoSCA & 89.5 & 40.4 & 40.1 & 81.7 & 31.3 & 30.6 \\\\\n    \\hline\n    \\end{tabular}\n    \\label{tab:automate_full1}\n\\end{table}\n\n\\begin{table}[t]\n    \\centering\n          \\renewcommand\\arraystretch{0.95}\n\n        \\caption{Mobile app task complete and partial sequence accuracy on MoTIF when trained on MoTIF alone, or MoTIF and RicoSCA data for the MOCA model. The training and testing language input are kept the same; input contains the high-level goal and low level step by step instructions }\n    \\begin{tabular}{|l|c|c|c|c|c|c|c|}\n    \\hline\n    & & \\multicolumn{6}{c|}{MoTIF Test Split}\\\\\n    \\cline{3-8}\n      Model & Train & \\multicolumn{3}{c|}{App Seen} & \\multicolumn{3}{c|}{App Unseen}\\\\\n    \\cline{3-8}\n   \\textbf{MOCA} & Data & \\multirow{2}{*}{Action} & \\multirow{2}{*}{Ground} & Action + &  \\multirow{2}{*}{Action} & \\multirow{2}{*}{Ground} & Action +\\\\\n   & & & & Ground& & & Ground\\\\\n    \\hline\n    Complete & \\multirow{2}{*}{MoTIF} & 37.8 & 16.2 & 12.3 & 24.6 & 17.0 & 13.2 \\\\\n    Partial & & 66.0 & 34.9 & 29.9 & 60.4 & 32.0 & 27.7 \\\\\n    \\hline\n    Complete & MoTIF +  & 51.1 & 21.3 & 20.7 & 44.8 & 17.0 & 15.1 \\\\\n    Partial & RicoSCA & 78.5 & 40.0 & 38.6 & 72.2 & 32.7 & 30.0 \\\\\n    \\hline\n    \\end{tabular}\n    \\label{tab:automate_full2}\n\\end{table}\n\\subsection{Training Data Ablations}\n\\label{sec:auto_data}\n\nWe also ran experiments with Seq2Seq and MOCA when trained only on MoTIF data instead of both MoTIF and RicoSCA. We include these comparisons for Seq2Seq and MOCA in Tables~\\ref{tab:automate_full1} and~\\ref{tab:automate_full2}, respectively. Jointly training on both datasets consistently performs better across all metrics. Additionally, performance trends generally remain the same when comparing the app seen versus app unseen test split: regardless of training data, accuracy is higher on the app seen test split.\nWe report the joint training performance for these methods in the main text for a closer apples-to-apples comparison with Seq2Act.\n\n\\begin{table}[t]\n    \\centering\n          \\renewcommand\\arraystretch{0.95}\n\n        \\caption{Mobile app task complete and partial sequence accuracy on MoTIF with various language inputs at test time for the Seq2Seq model. The training input contains the high level goal and low level step by step instructions }\n    \\begin{tabular}{|l|c|c|c|c|c|c|c|}\n    \\hline\n    & & \\multicolumn{6}{c|}{MoTIF Test Split}\\\\\n    \\cline{3-8}\n      Model & Test & \\multicolumn{3}{c|}{App Seen} & \\multicolumn{3}{c|}{App Unseen}\\\\\n    \\cline{3-8}\n  \\textbf{Seq2Seq} & Input & \\multirow{2}{*}{Action} & \\multirow{2}{*}{Ground} & Action + &  \\multirow{2}{*}{Action} & \\multirow{2}{*}{Ground} & Action +\\\\\n   & & & & Ground& & & Ground\\\\\n    \\hline\n    Complete & High + & 68.5 & 22.5 & 22.5 & 54.3 & 18.0 & 17.7 \\\\\n    Partial & Low & 89.5 & 40.4 & 40.1 & 81.7 & 31.3 & 30.6 \\\\\n    \\hline\n    Complete &\\multirow{2}{*}{Low} & 47.1 & 18.6 & 18.0 & 27.1 & 13.9 & 13.9 \\\\\n    Partial & & 73.7 & 36.6 & 33.9 & 43.6 & 22.6 & 21.2 \\\\\n    \\hline\n    Complete & \\multirow{2}{*}{High} & 30.9 & 15.3 & 14.7 & 18.9 & 11.7 & 8.8 \\\\\n    Partial & & 68.1 & 31.6 & 29.5 & 59.1 & 24.0 & 19.8 \\\\\n    \\hline\n    \\end{tabular}\n    \\label{tab:automate_eval1}\n\\end{table}\n\\subsection{Test-time Language Input Ablations}\n\\label{sec:auto_language}\nWe include ablations for the trained models in the main text for all possible language inputs at test time. Seq2Seq and MOCA were trained on both high-level goal and low-level instructions, as their original models supported both inputs and obtained best performance with them in prior work. Seq2Act does not currently support high-level goal language input, so we cannot jointly evaluate both in a meaningful way. We benchmark models as close to their original architecture as possible, and leave adaptations to future work.\n\nIn the main text, all task automation results were reported on the same language input as was used during training to avoid confounding factors when analyzing generalization to new app environments. Thus, Seq2Seq and MOCA took both high-level and low-level command as input while Seq2Act took only low-level instruction. We now evaluate all possible input language ablations at test time. Evaluating the high-level goal input alone replicates what these models would be provided in practical application, as users would request high-level goals (and not provide step by step instruction). Our high-level input results are useful to evaluate generalization to downstream settings, but we also include results for low-level input alone or both high-level and low-level language instruction (where applicable, as Seq2Act cannot support both) in Tables~\\ref{tab:automate_eval1},~\\ref{tab:automate_eval2}, and~\\ref{tab:automate_eval3}.\n\nThe Seq2Seq partial and complete sequence accuracy for action prediction show that having both high-level goal and low-level instruction inputs result in the best performance, followed by low-level instruction, and then high-level goal. On the other hand, MOCA performs quite similarly when both high-level goal and low-level instruction are input versus low-level instruction alone on action prediction. Additionally, there is less grounding performance degradation over the ablations, which may be a result of MOCA's more constrained test-time environment (which uses app type prediction to narrow the grounding prediction space).\n\nSeq2Act performs best across all metrics when provided the low-level instruction at test time. This is expected, given that Seq2Act was trained on step by step instructions. For both test splits, the action and grounding accuracy is significantly higher with low-level input. As the VLN methods showed having both high-level and low-level inputs can improve performance, adapting Seq2Act to take both as input would be important in future work.\n\n\\begin{table}[t]\n    \\centering\n          \\renewcommand\\arraystretch{0.95}\n\n        \\caption{Mobile app task complete and partial sequence accuracy on MoTIF with various language inputs at test time for the MOCA model. The training input contains the high level goal and low level step by step instructions }\n    \\begin{tabular}{|l|c|c|c|c|c|c|c|}\n    \\hline\n    & & \\multicolumn{6}{c|}{MoTIF Test Split}\\\\\n    \\cline{3-8}\n      Model & Test & \\multicolumn{3}{c|}{App Seen} & \\multicolumn{3}{c|}{App Unseen}\\\\\n    \\cline{3-8}\n  \\textbf{MOCA} & Input & \\multirow{2}{*}{Action} & \\multirow{2}{*}{Ground} & Action + &  \\multirow{2}{*}{Action} & \\multirow{2}{*}{Ground} & Action +\\\\\n   & & & & Ground& & & Ground\\\\\n    \\hline\n    Complete & High + & 51.1 & 21.3 & 20.7 & 44.8 & 17.0 & 15.1 \\\\ \n    Partial & Low & 78.5 & 40.0 & 38.6 & 72.2 & 32.7 & 30.0 \\\\\n    \\hline\n    Complete &\\multirow{2}{*}{Low} & 48.6 & 19.5 & 19.2 & 45.4 & 17.0 & 15.8\\\\\n    Partial & & 77.3 & 36.5 & 36.5 & 74.1 & 32.4 & 30.8 \\\\ \n    \\hline\n    Complete & \\multirow{2}{*}{High}&  13.5 & 19.5 & 8.4 & 11.4 & 18.6 & 6.9\\\\ \n    Partial & & 43.6 & 38.8 & 26.1 & 41.1 & 33.5 &21.2 \\\\\n    \\hline\n    \\end{tabular}\n    \\label{tab:automate_eval2}\n\\end{table}\n\n\\begin{table}[t]\n    \\centering\n          \\renewcommand\\arraystretch{0.95}\n\n        \\caption{Mobile app task complete and partial sequence accuracy on MoTIF with various language inputs at test time for the Seq2Act model. The training input contains the low level step by step instructions }\n    \\begin{tabular}{|l|c|c|c|c|c|c|c|}\n    \\hline\n    & & \\multicolumn{6}{c|}{MoTIF Test Split}\\\\\n    \\cline{3-8}\n      Model & Test & \\multicolumn{3}{c|}{App Seen} & \\multicolumn{3}{c|}{App Unseen}\\\\\n    \\cline{3-8}\n  \\textbf{Seq2Act} & Input & \\multirow{2}{*}{Action} & \\multirow{2}{*}{Ground} & Action + &  \\multirow{2}{*}{Action} & \\multirow{2}{*}{Ground} & Action +\\\\\n   & & & & Ground& & & Ground\\\\\n    \\hline\n    Complete &\\multirow{2}{*}{Low} & 97.3 & 32.4 & 32.4 & 96.8 & 28.3 & 28.3 \\\\\n    Partial & & 99.2 & 66.4 & 66.3 & 99.6 & 67.7 & 67.6 \\\\\n    \\hline\n    Complete & \\multirow{2}{*}{High}&  10.6 & 7.6 & 7.6 & 8.5 &1.9&1.9\\\\\n    Partial & & 28.1 & 13.0  & 10.8 &31.3 & 7.0 & 5.4\\\\\n    \\hline\n    \\end{tabular}\n    \\label{tab:automate_eval3}\n\\end{table}\n\n\\subsection{Generalization of Natural Language Commands across Apps}\n\\label{sec:task_gen}\nWe lastly evaluate generalization of our task automation methods to natural language tasks. Specifically, we present results on two additional test splits: an app seen and task unseen app split (where the task was seen in other apps, but not the current) and an app unseen and task seen split. The former shows the easier setting of having seen the app environment with other tasks during training and the task with other apps during training, whereas the app unseen test split means the task was seen during training with other apps but the model has never seen any task in this particular app.\n\nIntuitively, performance is consistently higher on the easier setting of app seen and task unseen (current app), as the model has had the chance to learn about both the app environment and task instruction, albeit independently. Comparing these task generalization results to the app generalization results in the main text (can also be found in Tables~\\ref{tab:automate_eval1}-\\ref{tab:automate_eval3}), the models can consistently generalize tasks across applications better than they can generalize to new environments. \n\n\\begin{table}[t]\n    \\centering\n          \\renewcommand\\arraystretch{0.95}\n\n        \\caption{Mobile app task complete and partial sequence accuracy on MoTIF with various test splits for evaluating task generalization. The training and test-time input contains the high level goal and low level step by step instructions }\n    \\begin{tabular}{|l|c|c|c|c|c|c|}\n    \\hline\n   \\multirow{5}{*}{Model} & \\multicolumn{6}{c|}{MoTIF Test Split}\\\\\n    \\cline{2-7}\n      & \\multicolumn{3}{c|}{App Seen Task Unseen} & \\multicolumn{3}{c|}{\\multirow{2}{*}{App Unseen Task Seen}}\\\\\n      & \\multicolumn{3}{c|}{(Current App)} & \\multicolumn{3}{c|}{}\\\\\n    \\cline{2-7}\n  & \\multirow{2}{*}{Action} & \\multirow{2}{*}{Ground} & Action + &  \\multirow{2}{*}{Action} & \\multirow{2}{*}{Ground} & Action +\\\\\n   & & & Ground& & & Ground\\\\\n    \\hline\n    \\textbf{Seq2Seq} & & & & & & \\\\\n    Complete & 75.4 & 31.0 & 31.0 & 70.9 & 25.8 & 25.8\\\\\n    Partial & 92.7 & 46.6 & 46.6 & 91.5 & 41.4 & 41.2 \\\\ \n    \\hline\n    \\textbf{MOCA} & & & & & & \\\\\n    Complete & 66.5 & 34.3 & 33.1 & 57.9 & 29.5 & 28.1 \\\\ \n    Partial & 87.8 & 47.7 & 46.2 & 77.8 & 44.7 & 42.7 \\\\\n    \\hline\n    \\end{tabular}\n    \\label{tab:task_gen}\n\\end{table}% \\clearpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{\\ours: A Realistic Web Environment for Building Autonomous Agents}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nWith advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios.\nIn this paper, we build an environment for language-guided agents that is \\emph{highly realistic} and \\emph{reproducible}.\nSpecifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.\nOur environment is enriched with tools~(\\eg a map) and external knowledge bases~(\\eg user manuals) to encourage human-like task-solving.\nBuilding upon our environment, we release a set of benchmark tasks focusing on evaluating the \\emph{functional correctness} of task completions.\nThe tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet.\nWe experiment with several baseline agents, integrating recent techniques such as reasoning before acting. \nThe results demonstrate that solving complex tasks is challenging: our best \\textsc{GPT-4}-based agent only achieves an end-to-end task success rate of 14.41\\%, significantly lower than the human performance of 78.24\\%.\nThese results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that \\ours can be used to measure such progress.%\\footnote{Code, data, environment setup, and video demonstrations are available in the supplementary material.}\n\nOur code, data, environment reproduction resources, and video demonstrations are publicly available at \\url{https://webarena.dev/}.\n\\end{abstract}\n\n\\section{Introduction}\\label{sec:intro}\nAutonomous agents that  perform everyday tasks via human natural language commands could significantly augment human capabilities, improve efficiency, and increase accessibility.\nNonetheless, to fully leverage the power of autonomous agents, it is crucial to understand their behavior within an environment that is both \\emph{authentic} and \\emph{reproducible}.\nThis will allow measurement of the ability of agents on tasks that human users care about in a fair and consistent manner.\n\nCurrent environments for evaluate agents tend to \\emph{over-simplify} real-world situations. %\nAs a result, the functionality of many environments is a limited version of their real-world counterparts, leading to a lack of task diversity~\\citep{shi2017world,anderson_vision-and-language_2018,gordon2018iqa,misra2016tell,shridhar_alfred:_2019,shridhar_alfworld_2020,yao2022webshop}.\nIn addition, these simplifications often lower the complexity of tasks as compared to their execution in the real world~\\citep{puig_virtualhome:_2018,shridhar_alfred:_2019,yao2022webshop}. \nFinally, some environments are presented as a static resource~\\citep{shi2017world, deng2023mind2web} where agents are confined to accessing only those states that were previously cached during data collection, thus limiting the breadth and diversity of exploration.\nFor evaluation, many environments focus on comparing the textual \\emph{surface form} of the predicted action sequences with reference action sequences, disregarding the \\emph{functional correctness} of the executions and possible alternative solutions~\\citep{puig_virtualhome:_2018,jernite_craftassist_2019, xu2021grounding,li2020mapping, deng2023mind2web}.\nThese limitations often result in a discrepancy between simulated environments and the real world, \nand can potentially impact the generalizability of AI agents to successfully understand, adapt, and operate within complex real-world situations.\n\n\\begin{figure}[t]\n    \\vspace{-10mm}\n    \\centering    \\includegraphics[width=\\linewidth]{figs/webarena.pdf}\n    \\caption{\\ours is a standalone, self-hostable web environment for building autonomous agents. \n    \\ours creates websites from four popular categories with functionality and data mimicking their real-world equivalents. \n    To emulate human problem-solving, \\ours also embeds tools and knowledge resources as independent websites. \\ours introduces a benchmark on interpreting \\emph{high-level} \\emph{realistic} natural language command to concrete web-based interactions. We provide validators to programmatically validate the functional correctness of each task.}\n    \\label{fig:overview}\n    \\vspace{-4mm}\n\\end{figure}\n\nWe introduce \\ours{}, a \\emph{realistic} and \\emph{reproducible} web environment designed to facilitate the development of autonomous agents capable of executing tasks~(\\S\\ref{sec:webarena}). An overview of \\ours is in \\autoref{fig:overview}.\nOur environment comprises four fully operational, self-hosted web applications, each representing a distinct domain prevalent on the internet: online shopping, discussion forums, collaborative development, and business content management. %\nFurthermore, \\ours{} incorporates several utility tools, such as map, calculator, and scratchpad, to best support possible human-like task executions.\nLastly, \\ours{} is complemented by an extensive collection of documentation and knowledge bases that vary from general resources like English Wikipedia to more domain-specific references, such as manuals for using the integrated development tool~\\citep{fan2022minedojo}. \nThe content populating these websites is extracted from their real-world counterparts, preserving the authenticity of the content served on each platform.\nWe deliver the hosting services using Docker containers with \\texttt{gym}-APIs~\\citep{1606.01540}, ensuring both the usability and the reproducibility of \\ours.\n\nAlong with \\ours, we release a ready-to-use benchmark with 812 long-horizon web-based tasks~(\\S\\ref{sec:benchmark}).\nEach task is described as a high-level natural language intent, emulating the abstract language usage patterns typically employed by humans~\\citep{bisk-etal-2019-benchmarking}. Two example intents are shown in the upper left of \\autoref{fig:overview}.\nWe focus on evaluating the \\emph{functional correctness} of these tasks, \\ie does the result of the execution actually achieve the desired goal~(\\S\\ref{sec:eval_annotation}).\nFor instance, to evaluate the example in \\autoref{fig:main_example}, our evaluation method verifies the concrete contents in the designated repository.\nThis evaluation is not only more reliable~\\citep{zhong1709seq2sql,chen2021evaluating,wang2022execution} than comparing the textual surface-form action sequences~\\citep{puig_virtualhome:_2018,deng2023mind2web} but also accommodate a range of potential valid paths to achieve the same goal, which is a ubiquitous phenomenon in sufficiently complex tasks.\n\nWe use this benchmark to evaluate several agents that can follow NL command and perform web-based tasks~(\\S\\ref{sec:baseline_agents}). \nThese agents are implemented in a few-shot in-context learning fashion with powerful large language models (LLMs) such as \\textsc{GPT-4} and \\textsc{PALM-2}. \nExperiment results show that the best \\textsc{GPT-4} agent performance is somewhat limited, with an end-to-end task success rate of only 14.41\\%, while the human performance is 78.24\\%.\nWe hypothesize that the limited performance of current LLMs stems from a lack of crucial capabilities such as active exploration and failure recovery to successfully perform complex tasks ~(\\S\\ref{sec:analysis}).\nThese outcomes underscore the necessity for further development towards robust and effective agents~\\citep{lecun2022path} in \\ours.\n\n\\section{\\ours: Websites as an Environment for Autonomous Agents}\\label{sec:webarena}\nOur goal is to create a \\emph{realistic} and \\emph{reproducible} web environment. We achieve reproducibility by making the environment standalone, without relying on live websites. This circumvents technical challenges such as bots being subject to CAPTCHAs, unpredictable content modifications, and configuration changes, which obstruct a fair comparison across different systems over time. We achieve realism by using open-source libraries that underlie many in-use sites from several popular categories and importing data to our environment from their real-world counterparts.\n\n\\subsection{Controlling Agents through High-level Natural Language}\nThe \\ours environment is denoted as $\\mathcal{E}$\\textcolor{black}{$=\\langle \\mathcal{S},\\mathcal{A},\\mathcal{O},\\mathcal{T} \\rangle$} with state space $\\mathcal{S}$, action space $\\mathcal{A}$~(\\S\\ref{sec:actions}) and observation space $\\mathcal{O}$~(\\S\\ref{sec:observation}).\nThe transition function $\\mathcal{T}: \\mathcal{S} \\times \\mathcal{A}$\\textcolor{black}{$\\longrightarrow \\mathcal{S}$} is deterministic, and it is defined by the underlying implementation of each website in the environment.\n\\rebuttal{Given a task described as a natural language intent $\\mathbf{i}$, an agent issues an action $a_t$\\textcolor{black}{$\\in \\mathcal{A}$} based on intent $\\mathbf{i}$, the current observation $o_t$\\textcolor{black}{$\\in \\mathcal{O}$}, the action history $\\mathbf{a}_{1}^{t-1}$ and the observation history~$\\mathbf{o}_{1}^{t-1}$}.\nConsequently, the action results in a new state $s_{t+1}$\\textcolor{black}{$\\in \\mathcal{S}$} and its corresponding observation $o_{t+1}$\\textcolor{black}{$\\in \\mathcal{O}$}. \nWe propose a reward function $r(\\mathbf{a}_{1}^{T}, \\mathbf{s}_{1}^{T})$ to measure the success of a task execution, where $\\mathbf{a}_{1}^{T}$ represents the sequence of actions from start to the end time step $T$, and $\\mathbf{s}_{1}^{T}$ denotes all intermediate states. \nThis reward function assesses if state transitions align with the expectations of the intents. For example, with an intent to place an order, it verifies whether an order has been placed. Additionally, it evaluates the accuracy of the agent's actions, such as checking the correctness of the predicted answer.\n\n\\begin{figure}\n    \\vspace{-10mm}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/main_example.pdf}\n    \\caption{A high-level task that can be fully executed in \\ours{}. Success requires sophisticated, long-term planning and reasoning. To accomplish the goal (top), an agent needs to (1) find Pittsburgh art museums on Wikipedia, (2) identify their locations on a map (while optimizing the itinerary), %\n    and (3) update the README file in the appropriate repository with the planned route.}\n    \\label{fig:main_example}\n    \\vspace{-10pt}\n\\end{figure}\n\n\\subsection{Website Selection}\\label{sec:website_selection}\nTo decide which categories of websites to use, we first analyzed approximately 200 examples from the authors' actual web browser histories. Each author delved into their browsing histories, summarizing the goal of particular segments of their browser session. Based on this, we classified the visited websites into abstract categories.\nWe then identified the four most salient categories and implemented one instance per category based on this analysis: (1) E-commerce platforms supporting online shopping activities~(\\eg Amazon, eBay), (2) social forum platforms for opinion exchanges~ (\\eg Reddit, StackExchange), (3) collaborative development platforms for software development~(\\eg GitLab), and (4) content management systems (CMS) that manage the creation and revision of the digital content (\\eg online store management).\n\nIn addition to these platforms, we selected three utility-style tools that are frequently used in web-based tasks: (1) a map for navigation and searching for information about points of interest (POIs) such as institutions or locations (2) a calculator, and (3) a scratchpad for taking notes.\nAs information-seeking and knowledge acquisition are critical in web-based tasks, we also incorporated various knowledge resources into \\ours. These resources range from general information hubs, such as the English Wikipedia, to more specialized knowledge bases, such as the website user manuals. \n\n\\paragraph{Implementation} \nWe leveraged open-source libraries relevant to each category to build our own versions of an E-commerce website~(OneStopShop), GitLab, Reddit, an online store content management system (CMS), a map, and an English Wikipedia. \nThen we imported sampled data from their real-world counterparts. %\nAs an example, our version of GitLab was developed based on the actual GitLab project.\\footnote{\\url{https://gitlab.com/gitlab-org/gitlab}} We carefully emulated the features of a typical code repository by including both popular projects with many issues and pull requests and smaller, personal projects.\nDetails of all websites in \\ours can be found in Appendix \\ref{app:site_implementation}.\nWe deliver the environment as dockers and provide scripts to reset the environment to a deterministic initial state (See Appendix \\ref{app:env_reset}).\n\n\\subsection{Observation Space}\\label{sec:observation}\n\\begin{figure}\n    \\vspace{-10mm}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/observation_space.pdf}\n    \\caption{We design the observation to be the URL and the content of a web page, with options to represent the content as a screenshot (left), HTML DOM tree (middle), and accessibility tree (right). The content of the middle and right figures are trimmed to save space.}\n    \\label{fig:observation}\n    \\vspace{-2mm}\n\\end{figure}\nWe design the observation space to roughly mimic the web browser experience: a web page URL, the opened tabs\n, and the web page content of the focused tab. \n\\ours is the first web environment to consider multi-tab web-based tasks to promote tool usage, direct comparisons and references across tabs, and other functionalities. \nThe multi-tab functionality offers a more authentic replication of human web browsing habits compared to maintaining everything in a single tab.\nWe provide flexible configuration to render the page content in many modes: (see \\autoref{fig:observation} for an example):\n(1) the raw web page HTML, composed of a Document Object Model (DOM) tree, as commonly used in past work \\citep{shi2017world,deng2023mind2web,li2020mapping}; (2) a screenshot, a pixel-based representation that represents the current web page as an RGB array and (3) the accessibility tree of the web page.\\footnote{\\url{https://developer.mozilla.org/en-US/docs/Glossary/Accessibility_tree}} \nThe accessibility tree is a subset of the DOM tree with elements that are \\emph{relevant} and \\emph{useful} for displaying the contents of a web page.\nEvery element is represented as its role~(\\eg a link), its text content, and its properties~(\\eg whether it is focusable). \nAccessibility trees largely retain the \\emph{structured} information of a web page while being more compact than the DOM representation.%\n\nWe provide an option to limit the content to the contents within a viewport for all modes. %\nThis ensures that the observation can be input into a text-based model with limited context length or an image-based model with image size or resolution requirements.\n\n\\subsection{Action Space}\\label{sec:actions}\nFollowing previous work on navigation and operation in web and embodied environments \\citep{shi2017world,liu2018reinforcement}, we design a compound action space that emulates the keyboard and mouse operations available on web pages.\n\\autoref{tab:actions} lists all the available actions categorized into three distinct groups. The first group includes element operations such as clicking, hovering, typing, and key combination pressing. The second comprises tab-related actions such as opening, closing, and switching between tabs. The third category consists of URL navigation actions, such as visiting a specific URL or navigating forward and backward in the browsing history.\n\nBuilding on these actions, \\ours provides agents with the flexibility to refer to elements for operation in different ways.\nAn element can be selected by its on-screen coordinates, %\n$(x, y)$, or by a unique element ID that is prepended to each element. \nThis ID is %\ngenerated when traversing the Document Object Model (DOM) or accessibility tree. \nWith element IDs, the element selection is transformed into an $n$-way classification problem, thereby eliminating any disambiguation efforts required from the agent or the underlying implementation. \nFor example, issuing the action \\texttt{click\\!\\! [1582]} clicks the button given the observation of \\texttt{[1582] Add to Cart}.\nThis flexible element selection allows \\ours to support agents designed in various ways (\\eg accepting input from different modalities) without compromising fair comparison metrics such as step count.\n\n\\paragraph{User Role Simulation}\nUsers of the same website often have disparate experiences due to their distinct \\emph{roles}, \\emph{permissions}, and \\emph{interaction histories}. \nWe emulate this scenario by generating unique user profiles on each platform. The details can be found in Appendix \\ref{sec:user_role}.\n\n\\section{Benchmark Suite of Web-based Tasks}\\label{sec:benchmark}\nWe provide a benchmark with 812 test examples on grounding high-level natural language instructions to interactions in \\ours.\nEach example has a metric to evaluate the functional correctness of the task execution. \nIn this section, we first formally define the task of controlling an autonomous agent through natural language. Then we introduce the annotation process of our benchmark.\n\n\\subsection{Intent Collection}\\label{sec:intent_collection}\nWe focus on curating \\emph{realistic} intents to carry out \\emph{complex} and \\emph{creative} tasks within \\ours.\nTo start with, our annotators were guided to spend a few minutes exploring the websites to familiarize themselves with the websites' content and functionalities. As most of our websites are virtually identical to their open-web counterparts, despite having sampled data, most annotators can quickly comprehend the websites.\n\nNext, we instructed the annotators to formulate intents based on the following criteria:\n\\begin{enumerate}[label=(\\arabic*),leftmargin=20pt,noitemsep]\n    \\item The intent should be \\emph{abstract} and \\emph{high-level}, implying that the task cannot be fulfilled with merely one or two actions. As an example, instead of \\sent{click the \\texttt{science} subreddit}, we encouraged annotators to come up with something more complex like \\sent{post a greeting message on \\texttt{science} subreddit}, which involves performing multiple actions.\n    \\item The intent should be \\emph{creative}. Common tasks such as account creation can be easily thought of. We encouraged the annotators to add constraints (\\eg \\sent{create a Reddit account \\textbf{identical to my GitLab one}}) to make the intents more unique.\n    \\item The intent should be formulated as a \\emph{template} by making replaceable elements as variables. The annotators were also responsible for developing several instantiations for each variable. For example, the intent \\sent{create a Reddit account identical to my GitLab one} can be converted into \\sent{create a \\{\\{site1\\}\\} account identical to my \\{\\{site2\\}\\} one}, with an instantiation like \\sent{\\{site1: Reddit, site2: GitLab\\}} and another like \\sent{\\{site1: GitLab, site2: OneStopShopping\\}}. \n    Notably, tasks derived from the same template can have distinct execution traces. The similarity resides primarily in the high-level semantics rather than the specific implementation. %\n\\end{enumerate}\n\nWe also provided a prompt for the annotators to use with ChatGPT\\footnote{\\url{https://chat.openai.com/}} for inspiration,\nthat contains an overview of each website and instructs the model to describe potential tasks to be performed on these sites.\nFurthermore, we offered a curated list of examples for annotators to reference. \n\n\\begin{figure}[t]\n    \\vspace{-10mm}\n    \\begin{minipage}{.4\\textwidth}\n        \\footnotesize\n        \\begin{tabular}{@{}l|l@{}}\n            \\toprule\n            \\textbf{Action Type} & \\textbf{Description} \\\\ \\midrule\n            \\texttt{noop} & Do nothing  \\\\\n            \\texttt{click(elem)} & Click at an element  \\\\\n            \\texttt{hover(elem)} & Hover on an element \\\\\n            \\texttt{type(elem, text)} & Type to an element \\\\\n            \\texttt{press(key\\_comb)} & Press a key comb  \\\\\n            \\texttt{scroll(dir)} & Scroll up and down \\\\\n            \\midrule\n            \\texttt{tab\\_focus(index)} & focus on $i$-th tab\\\\\n            \\texttt{new\\_tab} & Open a new tab  \\\\\n            \\texttt{tab\\_close} & Close current tab \\\\ \\midrule\n            \\texttt{go\\_back} & Visit the last URL  \\\\\n            \\texttt{go\\_forward} & Undo \\texttt{go\\_back} \\\\\n            \\texttt{goto(URL)} & Go to URL \\\\\n            \\bottomrule\n        \\end{tabular}\n        \\vspace{-3mm}\n        \\caption{Action Space of \\ours{}}\n        \\label{tab:actions}\n    \\end{minipage}%\n    \\hfill\n    \\begin{minipage}{.53\\textwidth}\n    \\footnotesize\n        \\begin{tabular}{@{}cc@{}}\n            \\toprule\n            \\textbf{Category} & \\textbf{Example} \\\\\n            \\midrule\n            \\multirow{3}{*}{\\shortstack{Information \\\\ Seeking}} & When was the last time I bought shampoo  \\\\\n            \\cmidrule{2-2}\n            & \\multirow{2}{*}{\\shortstack{Compare walking and driving time \\\\ from AMC Waterfront to Randyland}}\\\\\n            & \\\\\n            \\midrule\n            \\multirow{3}{*}{\\shortstack{Site \\\\ Navigation}} & \\multirow{2}{*}{\\shortstack{Checkout merge requests assigned to me}} \\\\\n            & \\\\\n            \\cmidrule{2-2}\n            &  \\multirow{2}{*}{\\shortstack{Show me the ergonomic chair \\\\ with the best rating}}  \\\\\n            & \\\\\n            \\midrule\n            \\multirow{2}{*}{\\shortstack{Content \\\\ \\& \\\\Config}} \n            & \\shortstack{Post to ask ``whether I need a car in NYC''} \\\\\n            \\cmidrule{2-2} \n            & \\shortstack{Delete the reviews from the scammer Yoke}\\\\\n            \\bottomrule\n        \\end{tabular}\n        \\vspace{-3mm}\n        \\caption{Example intents from three categories.}\n        \\label{tab:intent_examples}\n    \\end{minipage}\n    \\vspace{-4mm}\n\\end{figure}\n\n\\paragraph{Intent Analysis} In total, we curated 241 templates and 812 instantiated intents.\nOn average, each template is instantiated to 3.3 examples. \nThe intent distribution is shown in \\autoref{fig:intent_distribution}. \nFurthermore, we classify the intents into three primary categories with examples shown in \\autoref{tab:intent_examples}:\n\\begin{enumerate}[label=(\\arabic*),leftmargin=20pt,noitemsep]\n    \\item \\textbf{Information-seeking} tasks expect a textual response. Importantly, these tasks in \\ours often require navigation across multiple pages or focus on \\emph{user-centric} content. This makes them distinct from open-domain question-answering ~\\citep{yang2018hotpotqa,kwiatkowski2019natural}, which focuses on querying general knowledge with a simple retrieval step. For instance, to answer \\sent{When was the last time I bought the shampoo}, an agent traverses the user's purchase history, checking order details to identify the most recent shampoo purchase.\n    \\item \\textbf{Site navigation}: This category is composed of tasks that require navigating through web pages using a variety of interactive elements such as search functions and links. The objective is often to locate specific information or navigate to a particular section of a site.\n    \\item \\textbf{Content and configuration operation}: This category encapsulates tasks that require operating in the web environment to create, revise, or configure content or settings. This includes adjusting settings, managing accounts, performing online transactions, generating new web content, and modifying existing content. Examples range from updating a social media status or README file to conducting online purchases and configuring privacy settings.\n\\end{enumerate}\n\n\\subsection{Evaluation Annotation}\\label{sec:eval_annotation}\n\\paragraph{Evaluating Information Seeking Tasks} To measure the correctness of information-seeking tasks where a textual answer is expected, we provide the annotated answer $a^*$ for each intent. \nThe $a^*$ is further compared with the predicted answer $\\hat{a}$ with one of the following scoring functions $r_{\\textrm{info}}(\\hat{a}, a^*)$.\n\nFirst, we define \\texttt{exact\\_match} where only $\\hat{a}$ that is identical with $a^*$ receives a score of one. This function is primarily applicable to intent types whose responses follow a more standardized format, similar to the evaluation on question answering literature~\\citep{rajpurkar2016squad,yang2018hotpotqa}. \n\nSecond, we create \\texttt{must\\_include} where any $\\hat{a}$ containing $a^*$ receives a score of one. This function is primarily used in %\nwhen %\nan unordered list of text is expected or where the emphasis of evaluation is on certain key concepts. In the second example in \\autoref{tab:evaluation_examples}, we expect both the correct name and the email address to be presented, irrespective of the precise wording used to convey the answer.\n\nFinally, we introduce \\texttt{fuzzy\\_match} where we utilize a language model to assess whether $\\hat{a}$ is semantically equivalent to $a^*$. \nSpecifically, in this work, we use \\texttt{gpt-4-0613} to perform this evaluation.\nThe corresponding prompt details are provided in Appendix \\ref{app:prompt_fuzzy_match}. \nThe \\texttt{fuzzy\\_match} function applies to situations where the format of the answer is diverse. For instance, in responding to \\sent{Compare the time for walking and driving route from AMC Waterfront to Randyland}, it is essential to ensure that driving time and walking time are accurately linked with the correct terms. \nThe \\texttt{fuzzy\\_match} function could also flexibly match the time ``2h58min'' with different forms such as ``2 hour 58 minutes'', ``2:58'' and others. \nWe demonstrate a language model can achieve nearly perfect performance on this task in \\S\\ref{app:fuzzy_match_perf}.\n\n\\paragraph{Evaluating Site Navigation and Content \\& Config Tasks} The tasks in these categories require accessing web pages that meet certain conditions or performing operations that modify the underlying data storage of the respective websites. \nTo assess these, we establish reward functions $r_{\\textrm{prog}}(\\mathbf{s})$ that programmatically examine the intermediate states $\\mathbf{s}$ within an execution trajectory to ascertain whether the outcome aligns with the intended result.\nThese intermediate states are often the underlying databases of the websites, the status, and the content of a web page at each step of the execution. \n\nEvaluating each instance involves two components. First, we provide a \\texttt{locator}, tasked with retrieving the critical content pertinent to each intent. \nThe implementation of this locator varies from a database query, a website-supported API call, to a JavaScript element selection on the relevant web page, depending on implementation feasibility.\nFor example, the evaluation process for the intent of the fifth example in \\autoref{tab:evaluation_examples}, first obtains the URL of the latest post by examining the last state in the state sequence $\\mathbf{s}$. Then it navigates to the corresponding post page and obtains the post's content by running the Javascript {\\small ``\\texttt{document.querySelector(`.submission\\_\\_inner').outerText}''}.\n\nSubsequently, we annotate \\texttt{keywords} that need to exist within the located content. For example, the evaluation verifies if the post is correctly posted in the ``nyc'' subreddit by examining the URL of the post and if the post contains the requested content by examining the post content.\nWe reuse the \\texttt{exact\\_match} and \\texttt{must\\_include} functions from information-seeking tasks for this purpose.\n\n\\begin{table}[t]\n    \\vspace{-10mm}\n    \\centering\n    \\resizebox{\\linewidth}{!}{%\n    \\begin{tabular}{@{}cccl@{}}\n    \\toprule\n    \\textbf{Function} & \\textbf{ID} & \\textbf{Intent} & \\multicolumn{1}{c}{\\textbf{Eval Implementation}} \\\\\n    \\midrule\n     \\multirow{6}{*}{\\shortstack{$r_{\\textrm{info}}(a^*, \\hat{a})$}} & \\multirow{2}{*}{\\shortstack{1}} & \\multirow{2}{*}{\\shortstack{Tell me the name of the customer who \\\\ has the most cancellations in the history}} & \\multirow{2}{*}{\\shortstack{\\texttt{\\textcolor{blue}{exact\\_match}}($\\hat{a}$, \\textcolor{red}{``Samantha Jones''})}} \\\\\n      & & \\\\\n      \\cmidrule{2-4}\n      & \\multirow{2}{*}{\\shortstack{2}}  &  \\multirow{2}{*}{\\shortstack{Find the customer name and \\\\ email with phone number 8015551212}} & \\multirow{1}{*}{\\shortstack{\\texttt{\\textcolor{blue}{must\\_include}}($\\hat{a}$, \\textcolor{red}{``Sean Miller''})}} \\\\\n      & & & \\multirow{1}{*}{\\shortstack{\\texttt{\\textcolor{blue}{must\\_include}}($\\hat{a}$, \\textcolor{red}{``sean@gmail.com''})}} \\\\\n      \\cmidrule{2-4}\n      & \\multirow{2}{*}{\\shortstack{3}} &  \\multirow{2}{*}{\\shortstack{Compare walking and driving time \\\\ from AMC Waterfront to Randyland}} & \\multirow{1}{*}{\\shortstack{\\texttt{\\textcolor{blue}{fuzzy\\_match}}($\\hat{a}$, \\textcolor{red}{``walking: 2h58min''})}} \\\\\n      & & & \\multirow{1}{*}{\\shortstack{\\texttt{\\textcolor{blue}{fuzzy\\_match}}($\\hat{a}$, \\textcolor{red}{\\textcolor{red}{``driving: 21min''}})}} \\\\\n      \\midrule \n      \\multirow{9}{*}{\\shortstack{$r_{\\textrm{prog}}(\\mathbf{s})$}} & \\multirow{3}{*}{\\shortstack{4}} & \\multirow{3}{*}{\\shortstack{Checkout merge requests \\\\ assigned to me}} & \\multirow{1}{*}{\\shortstack{\\texttt{url=locate\\_current\\_url($\\mathbf{s}$)}}} \\\\\n      & & & \\multirow{1}{*}{\\shortstack{\\texttt{\\textcolor{blue}{exact\\_match}(URL}, \\textcolor{red}{``gitlab.com/merge\\_}}} \\\\\n      & & & \\multirow{1}{*}{\\shortstack{\\hspace{1em}\\textcolor{red}{requests?assignee\\_username=byteblaze''})}} \\\\\n      \\cmidrule{2-4}\n      & \\multirow{4}{*}{\\shortstack{5}} & \\multirow{4}{*}{\\shortstack{Post to ask ``whether I \\\\ need a car in NYC''}} & \\multirow{1}{*}{\\shortstack{\\texttt{url=locate\\_latest\\_post\\_url($\\mathbf{s})$}}} \\\\\n      & & & \\multirow{1}{*}{\\shortstack{\\texttt{body=locate\\_latest\\_post\\_body($\\mathbf{s})$}}} \\\\\n      & & & \\multirow{1}{*}{\\shortstack{\\texttt{\\textcolor{blue}{must\\_include}(URL}, \\textcolor{red}{``/f/nyc''})}} \\\\\n      & & & \\multirow{1}{*}{\\shortstack{\\texttt{\\textcolor{blue}{must\\_include}(body,}\\textcolor{red}{``a car in NYC''}})} \\\\\n      \\bottomrule\n      \\end{tabular}%\n    }\n    \\caption{We introduce two evaluation approaches. $r_{\\textrm{info}}$ (top) measures the correctness of performing information-seeking tasks. It compares the predicted answer $\\hat{a}$ with the annotated reference $a^*$ with three implementations.\n    $r_{\\textrm{prog}}$ (bottom) programmatically checks whether the intermediate states during the executions possess the anticipated properties specified by the intent. %\n    }\n    \\label{tab:evaluation_examples}\n    \\vspace{-4mm}\n\\end{table}\n\n\\paragraph{Unachievable Tasks} Due to constraints such as inadequate evidence, user permissions (\\S\\ref{sec:user_role}), or the absence of necessary functional support on the website, humans may ask for tasks that are not possible to complete.\nInspired by previous work on evaluating question-answering models on unanswerable questions~\\citep{rajpurkar2018know}, we design unachievable tasks in \\ours. \nFor instance, fulfilling an intent like \\sent{Tell me the contact number of OneStopShop} is impracticable in \\ours, given that the website does not provide such contact information. \nWe label such instances as \"N/A\" and expect an agent to produce an equivalent response. \nThese examples allow us to assess an agent's ability to avoid making unfounded claims and its adherence to factual accuracy.\n\n\\paragraph{Annotation Process} The intents were contributed by the authors following the annotation guideline in \\S\\ref{sec:intent_collection}. Every author has extensive experience with web-based tasks. \nThe reference answers to the information-seeking tasks were curated by the authors and an external annotator. To ensure consistency and accuracy, each question was annotated twice. If the two annotators disagreed, a third annotator finalized the annotation.\nThe programs to evaluate the remaining examples were contributed by three of the authors who are proficient in JavaScript programming. \nDifficult tasks were often discussed collectively to ensure the correctness of the annotation.\nThe annotation required the annotator to undertake the full execution and scrutinize the intermediate states.\n\n\\begin{wraptable}[5]{r}{0.25\\linewidth}\n    \\vspace{-10pt}\n    \\footnotesize\n    \\begin{tabular}{@{}l@{\\hspace{5pt}}c@{}}\n        \\toprule\n        Avg. Time  & 110s\\\\\n        Success Rate$_\\textrm{info}$ & 74.68\\%\\\\\n        Success Rate$_\\textrm{others}$ & 81.32\\% \\\\\n         Success Rate$_\\textrm{all}$ & 78.24\\% \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\label{tab:human_peformance}\n\\end{wraptable}\n\n\\paragraph{Human Performance} We sample one task from each of the 170 templates and ask five computer science graduate students to perform these tasks. The human performance is on the right. \nOverall, the human annotators complete 78.24\\% of the tasks, with lower performance on information-seeking tasks. \nThrough examining the recorded trajectories, we found that 50\\% of the failures are due to misinterpreting the intent (\\eg providing travel distance when asked for travel time), incomplete answers (\\eg providing only name when asked for name and email), and incomplete executions (\\eg partially filling the product information), while the remaining instances have more severe failures, where the executions are off-target. More discussions on human annotations can be found in \\S\\ref{app:human_annotation}.\n\n\\section{Baseline Web Agents}\\label{sec:baseline_agents}\nWe experiment with three LLMs using two prompting strategies, both with two examples in the context. In the first setting, we ask the LLM to directly predict the next action given the current observation, the intent and the previously performed action. \nIn the second setting, with the same information, the model first performs chain-of-thought reasoning steps in the text before the action prediction~(CoT,~\\citet{wei2022chain, yao2022react}). \nBefore the examples, we provide a detailed overview of the browser environment, the allowed actions, and many rules.\nTo make the model aware of the unachievable tasks, the instruction explicitly asks the agent to stop if it believes the task is impossible to perform. We refer to this directive as Unachievable hint, or \\textbf{UA hint}.\nThis introduction is largely identical to the guidelines we presented to human annotators to ensure a fair comparison.\nWe use an accessibility tree with element IDs as the observation space.\nThe agent can identify which element to interact with by the ID of the element. For instance, the agent can issue \\texttt{click [1582]} to click the ``Add to Cart'' button with the ID of 1582.\nThe full prompts can be found in Appendix \\ref{app:agent_prompts}.\nThe detailed configurations of each model can be found in Appendix \\ref{sec:exp_configs}.\n\n\\section{Results}\\label{sec:results}\n\\begin{wraptable}{r}{0.53\\linewidth}\n    \\vspace{-13pt}\n    \\footnotesize\n    \\begin{tabular}{@{}c@{\\hspace{3pt}}c@{}l@{\\hspace{5pt}}ccc@{}}\n        \\toprule\n        \\textbf{CoT} & \\textbf{UA Hint} & \\multicolumn{1}{c}{\\textbf{Model}} & \\textbf{SR} & \\textbf{SR$_\\textrm{AC}$} & \\textbf{SR$_\\textrm{UA}$} \\\\\n        \\midrule\n        \\cmark & \\cmark & \\scriptsize \\textsc{text-bison-001} &  5.05 & 4.00 & 27.78 \\\\\n        \\xmark & \\cmark & \\scriptsize \\textsc{GPT-3.5} & 6.41 & 4.90 & 38.89\\\\\n        \\cmark & \\cmark & \\scriptsize \\textsc{GPT-3.5} & 8.75 & 6.44 & 58.33 \\\\\n        \\cmark & \\cmark & \\scriptsize \\textsc{GPT-4} & 11.70 & 8.63 & \\textbf{77.78} \\\\ \n        \\midrule\n        \\xmark & \\xmark & \\scriptsize\\textsc{GPT-3.5} &   5.10 & 4.90 & 8.33 \\\\\n        \\cmark & \\xmark & \\scriptsize\\textsc{GPT-3.5} &  6.16 & 6.06 & 8.33\\\\\n        \\cmark & \\xmark & \\scriptsize\\textsc{GPT-4} & \\textbf{14.41} & \\textbf{13.02} & 44.44 \\\\ \n        \\midrule\n        \n        - & \\cmark & Human & 78.24  & 77.30 & 100.00 \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{The end-to-end task success rate (SR \\%) on \\ours with different prompting strategies. \\textbf{CoT}: the model performs step-by-step reasoning before issuing the action. \\textbf{UA hint}: ask the model to stop when encountering unachievable questions.}\n    \\label{tab:main_results}\n\\end{wraptable}\n\nThe main results are shown on the top of \\autoref{tab:main_results}. \n\\textsc{GPT-4}~\\citep{openai2023gpt} with CoT prompting achieves a modest end-to-end task success rate of 11.70\\%, which is significantly lower than the human performance of 78.24\\%. \n\\textsc{GPT-3.5}~\\citep{chatgpt} with CoT prompting is only able to successfully perform 8.75\\% of the tasks. \nThe explicit reasoning procedure is somewhat helpful, it brings 2.34\\% improvement over the version without it. \nFurther, \\textsc{text-bison-001}~\\citep{anil2023palm} \n underperforms \\textsc{GPT-3.5}, with a success rate of 5.05\\%. \nThese results underline the inherent challenges and complexities of executing tasks that span long horizons, particularly in realistic environments such as \\ours. %\n\n\\subsection{Analysis}\\label{sec:analysis}\n\\paragraph{Do models know when to stop?}  \nIn our error analysis of the execution trajectories, we observe a prevalent error pattern of early stopping due to the model's conclusion of unachievability. \nFor instance, \\textsc{GPT-4} erroneously identifies 54.9\\% of feasible tasks as impossible. \nThis issue primarily stems from the UA hint in the instruction, while this hint allows models to identify unachievable tasks, it also hinders performance on achievable tasks.\nTo address this, we conduct an ablation study where we remove this hint. We then break down the success rate for both achievable and unachievable tasks.\nAs shown in \\autoref{tab:main_results}, eliminating this instruction led to a performance boost in achievable tasks, enhancing the overall task success rate of \\textsc{GPT-4} to 14.41\\%.\nDespite an overall decline in identifying unachievable tasks, \\textsc{GPT-4} retains the capacity to recognize 44.44\\% of such tasks.\nIt does so by generating \\emph{reasons of non-achievability}, even without explicit instructions.\nOn the other hand, \\textsc{GPT-3.5} rarely exhibits this level of reasoning. \nInstead, it tends to follow problematic patterns such as hallucinating incorrect answers, repeating invalid actions, or exceeding the step limits.\nThis result suggests that even subtle differences in instruction design can significantly influence the behavior of a model in performing interactive tasks in complex environments.\n\n\\begin{wraptable}[11]{r}{0.29\\linewidth}\n    \\vspace{-30pt}\n \\includegraphics[width=\\linewidth]{figs/template_success_rate.pdf}\n    \\vspace{-20pt}\n    \\caption{Distribution of success rates on templates with $ \\geq 1$ successful executions on \\textsc{gpt} models (no UA hint).}\\label{fig:success_template}\n\\end{wraptable}\n\n\\paragraph{Can a model maintain consistent performance across similar tasks?} Tasks that originate from the same template usually follow similar reasoning and planning processes, even though their observations and executions will differ.\nWe plot a histogram of per-template success rates for our models in \\autoref{fig:success_template}. \nOf the 61 templates, \\textsc{GPT-4} manages to achieve a 100\\% task success rate on only four templates, while \\textsc{GPT-3.5} fails to achieve full task completion for any of the templates. In many cases, the models are only able to complete one task variation with a template.  \nThese observations indicate that even when tasks are derived from the same template, they can present distinct challenges. \nFor instance, while \\sent{Fork metaseq} can be a straightforward task, \\sent{Fork all repos from Facebook} derived from the same template requires more repetitive operations, hence increasing its complexity.\nTherefore, \\ours provide a testbed to evaluate more sophisticated methods. In particular, those that incorporate memory components, enabling the \\emph{reuse} of successful strategies from past experiments \\cite{zhou2021hierarchical, wang2023voyager}.\nMore error analysis with examples can be found in Appendix \\ref{sec:additional_error_analysis}.\n\n\\section{Related Work}\n\\begin{table}[t!]\n   \\vspace{-10mm}\n   \\centering\n    \\small\n    \\begin{tabular}{l@{\\hspace{3pt}}lcccc}\n    \\toprule\n    \\multicolumn{2}{c}{\\multirow{2}{*}{\\shortstack{Benchmark}}} & \\multirow{2}{*}{\\shortstack{Dynamic \\\\ Interaction?}} & \\multirow{2}{*}{\\shortstack{Realistic \\\\ Environment?}} & \\multirow{2}{*}{\\shortstack{Diverse \\\\ Human Tasks?}} & \\multirow{2}{*}{\\shortstack{Functional \\\\ Correctness?}}  \\\\\n    & \\\\\n    \\midrule\n    Mind2Web&\\citep{deng2023mind2web} & \\xmark & \\cmark & \\cmark & \\xmark \\\\\n    Form/QAWoB&\\citep{shi2017world} & \\xmark & \\cmark & \\cmark & \\xmark  \\\\\n    MiniWoB++&\\citep{liu2018reinforcement} & \\cmark & \\xmark & \\xmark & \\cmark \\\\\n    Webshop&\\citep{yao2022webshop} & \\cmark & \\xmark & \\xmark & \\cmark \\\\\n    ALFRED&\\citep{shridhar_alfred:_2019} & \\cmark & \\xmark & \\xmark & \\cmark \\\\\n    VirtualHome&\\citep{puig_virtualhome:_2018} & \\xmark & \\xmark & \\cmark & \\xmark \\\\\n    AndroidEnv&\\citep{toyama2021androidenv} & \\cmark & \\cmark & \\xmark & \\xmark \\\\\n    \\midrule\n    \\multicolumn{2}{c}{\\ours} & \\cmark & \\cmark & \\cmark & \\cmark \\\\\n    \\bottomrule    \n    \\end{tabular}\n    \\caption{The comparison between our benchmark and existing benchmarks on grounding natural language instructions to concrete executions. Our benchmark is implemented in our fully interactable highly-realistic \\ environment. It features diverse tasks humans may encounter in their daily routines. We design evaluation metrics to assess the functional correctness of task executions.}\n    \\vspace{-4mm}\n    \\label{tab:comparision}\n\\end{table}\n\\paragraph{Benchmarks for Controlling Agents through Natural Language} \nControlling agents via natural language in the digital world have been studied in the literature~\\citep{branavan2009reinforcement,shi2017world,liu2018reinforcement, toyama2021androidenv,deng2023mind2web,li2020mapping,xu2021grounding}. \nHowever, the balance between \\emph{functionality}, \\emph{authenticity}, and \\emph{support for environmental dynamics} remains a challenge. \nExisting benchmarks often compromise these aspects, as shown in \\autoref{tab:comparision}. \nSome works rely on static states, limiting agents' explorations and functional correctness evaluation~\\citep{shi2017world,deng2023mind2web}, while others simplify real-world complexities, restricting task variety~\\citep{yao2022webshop,liu2018reinforcement}.\nWhile AndroidEnv~\\citep{toyama2021androidenv} replicates an Android setup, \\rebuttal{it does not guarantee the reproducibility since live Android applications are used}. ~\\citep{ai2thor, shridhar_alfred:_2019, puig_virtualhome:_2018} and extends to gaming environments~\\citep{fan2022minedojo, kuttler_nethack_2020}, where the environment mechanisms often diverge from human objectives.\n\n\\paragraph{Interactive Decision-Making Agents} \n\\cite{nakano2021webgpt} introduce WebGPT which searches the web and reads the search results to answer questions. \\cite{gur2023real} propose a web agent that synthesizes Javascript code for the task executions. Adding a multi-modal dimension, \\cite{lee2023pix2struct} and \\cite{shaw2023pixels} develop agents that predict actions based on screenshots of web pages rather than relying on the text-based DOM trees.\nPerforming tasks in interactive environments requires the agents to exhibit several capabilities including hierarchical planning, state tracking, and error recovery.\nExisting works~\\citep{huang2022language,madaan2022language,li2023take} observe LLMs could break a task into more manageable sub-tasks~\\citep{zhou2022show}.\nThis process can be further refined by representing task executions as programs, a technique that aids sub-task management and skill reuse~\\citep{zhou2021hierarchical,liang2023code,wang2023voyager,gao2023pal}. Meanwhile, search and backtracking methods introduce a more structured approach to planning while also allowing for decision reconsideration~\\citep{yao2023tree,long2023large}. \nExisting works also incorporate failure recovery, self-correction~\\citep{shinn2023reflexion,kim2023language}, observation summarization~\\citep{sridhar2023hierarchical} to improve execution robustness.\nThe complexity of \\ours presents a unique challenge and opportunity for further testing and improvement of these methods.\n\n\\section{Conclusion}\nWe present \\ours, a highly-realistic, standalone, and reproducible web environment designed for the development and testing of autonomous agents. \\ours includes fully functional web applications and organic data from popular domains.\nAdditionally, we curate a comprehensive benchmark consisting of 812 examples that focus on mapping high-level natural language intents into concrete web interactions. We also offer outcome-based evaluation that programmatically validate the tasks success.\nOur experiments show that even \\textsc{GPT-4} only achieves a limited end-to-end task success rate of 14.41\\%, significantly lagging behind the human performance of 78.24\\%. These findings underscore the need for future research to focus on enhancing the robustness and efficacy of autonomous agents within \\ours environment. \n\n\\section*{Acknowledgement}\nWe would like to thank Emmy Liu, Zhiruo Wang, Zhitong Guo for examining our annotations, Shunyu Yao for providing the raw Amazon product data in Webshop, Pengfei Liu, Zaid Sheikh and Aman Madaan for the helpful discussions.\nWe are also grateful to the Center for AI Safety for providing computational resources.\nThis material is partly based on research sponsored in part by the Air Force Research Laboratory under agreement number FA8750-19-2-0200. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Air Force Research Laboratory or the U.S. Government. This project was also partially supported by a gift from AWS AI.\n\n\\clearpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web}\n\n\\begin{document}\n\n\\title{\\ModelName{}: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web} \n\n\\titlerunning{OmniACT}\n\n\\author{Raghav Kapoor\\inst{1}* \\and\nYash Parag Butala\\inst{1}* \\and\nMelisa Russak\\inst{2} \\and\nJing Yu Koh\\inst{1} \\and\nKiran Kamble\\inst{2} \\and\nWaseem AlShikh\\inst{2} \\and\nRuslan Salakhutdinov\\inst{1}}\n\n\\authorrunning{R. Kapoor, Y. Butala et al.}\n\n\\institute{Carnegie Mellon University \\and Writer.com\\\\\n\\email{\\{raghavka, ypb\\}@cs.cmu.edu} \\\\\n\\url{https://huggingface.co/datasets/Writer/omniact}\n}\n\n\\maketitle\n\\begin{center}\n    \\centering\n    \\captionsetup{type=figure}\n    \\includegraphics[width=0.9\\linewidth]{figs/teaser.pdf}\n    \\captionof{figure}{\\ModelName{} dataset and benchmark for enabling autonomous human-computer interaction agents. The left shows an image paired with a natural language task description as the input, the right shows the resulting action script to be executed on the screen. Examples are presented from Stocks, Apartments, and Weather application. }\n    \\label{fig:teaser}\n\\end{center}%\n\n\\begin{abstract}\n\\freefootnote{* These authors contributed equally. The order is determined by dice rolling.}\nFor decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce \\textit{\\ModelName{}}, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as ``Play the next song\", as well as longer horizon tasks such as ``Send an email to John Doe mentioning the time and place to meet\". Specifically, given a pair of screen image and a visually-grounded natural language task, the goal is to generate a script capable of fully executing the task. We run several strong baseline language model agents on our benchmark. The strongest baseline, GPT-4, performs the best on our benchmark However, its performance level still reaches only 15\\% of the human proficiency in generating executable scripts capable of completing the task, demonstrating the challenge of our task for conventional web agents. Our benchmark provides a platform to measure and evaluate the progress of language model agents in automating computer tasks and motivates future work towards building multimodal models that bridge large language models and the visual grounding of computer screens.\n  \\keywords{Generalist Agent \\and Multimodal Machine Learning \\and Dataset and Benchmark \\and Vision Language Understanding \\and UI grounding \\and Human-computer interaction}\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\n\nPerforming computer tasks based on natural language instructions has been a long-standing goal of artificial intelligence \\cite{wang2023survey}. One concrete objective in the line of research is to develop generalist agents that can assist humans in doing computer tasks \\cite{lecun2022path}, such as \\textit{``Order a pizza from Domino's\"} or \\textit{``Write a message to John.\"} The agent should be able to open the application and perform the task. Executing these actions on a personal computer involves a sequence of interactions with a mouse and keyboard. For example, the simple task of writing an email involves hovering over the application icon, clicking it, clicking the \\textit{`New Email'} button, writing the content of the email, and clicking send. Successfully sending an email requires accurately predicting the correct action at each step and accurately executing it, which is a herculean task even for the best agents today \\cite{gur2018learning}. % Libraries such as (Robotic Process Automation) RPA and \\textit{\\pyautogui} can execute kernel operations for controlling the mouse and keyboard.\n\nA generalist agent for computer tasks must understand natural language instructions, process visual screenshots, and produce the correct sequence of actions to be performed to achieve the intended task. %While it is possible to have agents that work with trial and error on the UI screens, there are challenges that come with that. These challenges can be related to 1) risks associated such as clicking on actions that can't be reversed, eg: purchasing a product. 2) the overhead time that arises due to wrong actions taken.  \nSeveral existing approaches focus on building agents based on the HTML model~\\cite{shi2017world,deng2023mind2web,zhou2023webarena}. However, this approach introduces several challenges and constraints. These agents are limited to web applications and often struggle with complex or long-context HTML code. They cannot interact with native desktop applications or perform tasks that span multiple applications, like drafting an email using text from a code editor, without significant alterations. Furthermore, HTML-based agents, which are inherently powered by text-only language models, typically underperform in tasks requiring visual cues, such as identifying and clicking a blue button on a desktop's top-right corner. In contrast, humans can easily understand UI elements like dropdown menus, typable areas, redirections, and options with just a glance.\n\nTowards the goal of developing a generalist autonomous agent with robust visual and user interface (UI) understanding capabilities, we introduce a new task and dataset, \\ModelName{}, containing over 9.8K pairs of images and instructions (Figure~\\ref{fig:teaser}) across different operating systems and the web. This dataset includes screenshots of various UI screens and corresponding natural language instructions. The objective of these instructions is to generate executable commands using the \\textit{\\pyautogui} Python library~\\cite{PyAutoGUI}. \\textit{\\pyautogui} enables the automation of the mouse and keyboard operations, which helps to facilitate interactions with various native applications across macOS, Windows, and Linux. This simplifies completing specified tasks across different web domains and native desktop applications.\n\nWe evaluate several language model-based agent baselines on this dataset, including LLaMA~\\cite{touvron2023llama}, Vicuna~\\cite{vicuna2023}, Palmyra-X (43B)~\\cite{palmyra-x}, InstructPalmyra-30B~\\cite{InstructPalmyra}, GPT 3.5, and GPT-4~\\cite{openai2023gpt4}. We experiment with fine-tuning Vicuna-13B and LLaMA-13B models using QLoRA~\\cite{dettmers2023qlora}. We also benchmark multimodal baseline LLaVa-v1.5-7B, LLaVa-v1.5-13B~\\cite{touvron2023llama}, Gemini-Pro~\\cite{team2023gemini} and GPT-4-vision-preview~\\cite{yang2023dawn} for the task. Our findings highlight the necessity for a multimodal model capable of executing these tasks, and our analysis provides insights into promising future work in the space. \nOur key contributions are outlined as follows:\n\\begin{enumerate}\n    \\item We release a novel dataset of desktop and website applications consisting of over 9.8K natural language tasks, UI screens, and corresponding code snippets collected through human annotation. We introduce custom performance metrics tailored for computer tasks.\n    \\item We propose DetACT, a module for creating textual representations of the screen using signals from OCR, color, and icon-template matching.\n    \\item We conduct a comprehensive benchmark and analysis of state-of-the-art LLMs and multimodal models on our benchmark. Our results show that \\ModelName{} is a challenging task for even the best LLM agents today, and existing models are far below human performance.\n\\end{enumerate}\n\n\\section{Related Work}\n\\label{sec:related_work}\n\n\\begin{table*}[]\n\\centering\n\\caption{Comparison of OmniACT with other related benchmarks.}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{@{}ccccccccc@{}}\n\\toprule\n\\textbf{Datasets} &\n  \\textbf{Size} &\n  \\textbf{Env Type} &\n  \\textbf{\\begin{tabular}[c]{@{}c@{}}Task \\\\ Heterogeneity\\end{tabular}} &\n  \\textbf{\\begin{tabular}[c]{@{}c@{}}Real-World \\\\ Portayal\\end{tabular}} &\n  \\textbf{\\begin{tabular}[c]{@{}c@{}}Executional \\\\ Correctness\\end{tabular}} &\n  \\textbf{\\begin{tabular}[c]{@{}c@{}}Supports \\\\ Desktop \\\\ Apps\\end{tabular}} &\n  \\textbf{\\begin{tabular}[c]{@{}c@{}}Continuous Scale \\\\ Adaptive \\\\ Evaluation\\end{tabular}} &\n  \\textbf{Task} \\\\ \\midrule\nVisualWebArena~\\cite{koh2024visualwebarena} &\n  910 &\n  Web &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{FE0000} No} &\n  {\\color[HTML]{FE0000} No} &\n  Web Navigation \\\\\nWebArena~\\cite{zhou2023webarena} &\n  812 &\n  Web &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{FE0000} No} &\n  {\\color[HTML]{FE0000} No} &\n  Web Navigation \\\\\nMind2Web~\\cite{deng2023mind2web}&\n  2350 &\n  Web &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{FE0000} No} &\n  {\\color[HTML]{FE0000} No} &\n  {\\color[HTML]{FE0000} No} &\n  Web Navigation \\\\\nWebShop~\\cite{yao2022webshop} &\n  12000 Products &\n  Web &\n  {\\color[HTML]{FE0000} No} &\n  {\\color[HTML]{FE0000} No} &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{FE0000} No} &\n  {\\color[HTML]{FE0000} No} &\n  Web Navigation \\\\\nRUSS~\\cite{xu2021grounding}&\n  80 &\n  Web &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{FE0000} No} &\n  {\\color[HTML]{FE0000} No} &\n  {\\color[HTML]{FE0000} No} &\n  Web Navigation \\\\\nWebSRC~\\cite{chen2021websrc} & \n  2735 &\n  Web &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{32CB00} Yes} &\n  - &\n  {\\color[HTML]{FE0000} No} &\n  {\\color[HTML]{FE0000} No} &\n  QA \\\\ \\midrule\nMiniWoB++ \\cite{humphreys2022data} &\n  100 &\n  \\begin{tabular}[c]{@{}c@{}}Mobile \\\\ Websites\\end{tabular} &\n  {\\color[HTML]{FE0000} No} &\n  {\\color[HTML]{FE0000} No} &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{FE0000} No} &\n  {\\color[HTML]{FE0000} No} &\n  Web Navigation \\\\\nPixelHelp~\\cite{li2020mapping} &\n  187 &\n  Mobile &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{FE0000} No} &\n  {\\color[HTML]{FE0000} No} &\n  {\\color[HTML]{FE0000} No} &\n  UI Grounding \\\\\nMetaGUI ~\\cite{sun2022meta} &\n  1125 &\n  Mobile &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{FE0000} No} &\n  {\\color[HTML]{FE0000} No} &\n  Mobile Navigation \\\\\nMoTIF~\\cite{burns2021mobile}&\n  756 &\n  Mobile &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{FE0000} No} &\n  {\\color[HTML]{FE0000} No} &\n  Mobile Navigation \\\\\nAITW~\\cite{rawles2023android}&\n  715142 &\n  Mobile and Web &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{32CB00} Yes} &\n  {\\color[HTML]{FE0000} No} &\n  {\\color[HTML]{FE0000} No} &\n  \\begin{tabular}[c]{@{}c@{}}Mobile/Web \\\\ Navigation\\end{tabular} \\\\ %\\toprule\n  \\midrule\n\\textbf{OmniACT} (Ours)&\n  \\textbf{9802} &\n  \\textbf{Desktop and Web} &\n  {\\color[HTML]{32CB00} \\textbf{Yes}} &\n  {\\color[HTML]{32CB00} \\textbf{Yes}} &\n  {\\color[HTML]{32CB00} \\textbf{Yes}} &\n  {\\color[HTML]{32CB00} \\textbf{Yes}} &\n  {\\color[HTML]{32CB00} \\textbf{Yes}} &\n  \\textbf{Code Generation} \\\\ \\bottomrule\n\\end{tabular}%\n}\n\n\\label{tab:comparison}\n\\end{table*}\n\n\\subsection{UI Understanding}\n\nUser interface (UI) understanding has garnered interest from researchers in the machine learning and human-computer interaction communities, evolving with various models focusing on understanding the semantics of mobile and web user interfaces. UIBert~\\cite{bai2021uibert}, PixelBERT~\\cite{huang2020pixelbert}, ActionBert~\\cite{he2021actionbert}, VUT~\\cite{li2021vut}, Screen2Words~\\cite{wang2021screen2words}, WidgetCaptioning~\\cite{li2020widget} and Pix2Act~\\cite{shaw2023pixels} are  notable models in this area. They propose approaches for learning the user-interface semantics of the mobile screen using the image and view hierarchy. These models have demonstrated effectiveness in tasks like capability prediction, screen segmentation and understanding, and screen caption generation. Lexi~\\cite{banerjee2023lexi} and Spotlight~\\cite{li2023spotlight} propose models that use vision-only inputs to minimize the reliance on metadata such as view hierarchy. Furata et al. \\cite{furuta2023multimodal} demonstrates the use of fine-tuning for multimodal web navigation. The majority of machine learning models trained for UI understanding leverage the Rico dataset~\\cite{rico} and its extensions, which contain 64,462 unique Android screens and metadata. In addition, \\cite{banerjee2023lexi} released the UICaptions dataset, which consists of diverse image-captions pairs across a wide range of applications. PixelHelp~\\cite{li2020mapping} also released a corpus to train models that can interpret natural language instructions and map them to mobile UI actions.\n\n\\subsection{Autonomous Computer Agents}\nThe advent of large language models (LLMs) has been pivotal in the rapid advancement of agents that operate on web pages. Recent research such as ViperGPT~\\cite{surís2023vipergpt} Chameleon~\\cite{lu2023chameleon}, RCI Agent~\\cite{kim2023language}, VisProg~\\cite{gupta2023visual}, and \\cite{nakano2021webgpt} employ LLMs for planning or action prediction in developing autonomous agents. Benchmark datasets, such as MiniWoB~\\cite{shi2017world}, WebShop~\\cite{yao2022webshop}, \nMacaw-LLM~\\cite{lyu2023macaw},\nASH-Prompting~\\cite{sridhar2023hierarchical}\nMind2Web~\\cite{deng2023mind2web}, WebArena~\\cite{zhou2023webarena}, AgentBench~\\cite{liu2023agentbench}  and VisualWebArena \\cite{koh2024visualwebarena} \nhave also been proposed to measure the ability of LLM-based agents to automate web tasks. These methods mainly involve agents that operate on a text-based Document Object Model (DOM) of HTML scripts. This limits their understanding of screen context, which is crucial for the model's decision-making and action-taking processes. To address this limitation, \\cite{rawles2023android} released Android in the Wild, a dataset comprising screens, natural language instructions, and corresponding actions. Following this, ~\\cite{autoui} proposed a multimodal model, AutoUI, which is designed to build an agent on the Android in the Wild dataset confined to the Android ecosystem. WebAgent \\cite{gur2024realworld-WebAgent} utilized Flan-U-PaLM, for grounded code generation, and HTML-T5 and showed improvement on real-world websites.\n\nCurrent benchmarks for autonomous agents focus mainly on the Web or Android environments, posing challenges for tasks involving desktop applications or spanning multiple applications beyond the web domain. The absence of established benchmarks and datasets in this area, coupled with basic methods for extracting user interface (UI) elements, underscores the need for significant progress in developing more versatile autonomous agents capable of handling diverse tasks beyond the current scope. To highlight the unique features that \\ModelName{} introduces in the assessment of capable autonomous agents, we provide a comparison between the existing benchmarks and our proposed benchmark, \\ModelName{}, in Table \\ref{tab:comparison}.\n\n\\begin{figure*}[!ht]\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{figs/dataset.pdf}\n    \\caption{\\textbf{Data Collection Pipeline.} (1) We select over 60 applications and websites to ensure diversity, (2) segment the screen through human-annotated bounding boxes, (3) label the bounding boxes based on functionality, (4) ask student volunteers to come up with tasks, given a screen image, and (5) reverse map the textual labels to coordinates and filter the scripts based on execution and syntax.}\n    \\label{fig:dataset}\n\\end{figure*}\n\\section{\\ModelName{}}\n\nWe introduce a novel dataset and benchmark, \\ModelName{}, which measures the performance of autonomous agents on both web and desktop applications. Compared to previous benchmarks which focus on text-based reasoning~\\cite{shi2017world,zhou2023webarena,deng2023mind2web, yao2022webshop, humphreys2022data}, our benchmark aims to measure multimodal agents that bridge large language model planners and UI understanding vision models. \\ModelName{} can be accomplished as a standalone task as it is not under a mock environment. \n\nAll actions that a human can execute on the computer can be encoded in the \\textit{\\pyautogui}~\\cite{PyAutoGUI} Python framework. This framework allows a user to execute keyboard and mouse operations by running Python code. The \\textit{\\pyautogui} code to execute these tasks is shown in the third column of Figure~\\ref{fig:teaser}. For other computer tasks, the \\textit{\\pyautogui} library provides functions such as `press', `write', and `scroll' which can be used to execute the task. Our dataset consists of parallel data of natural language tasks, UI screenshots, and ground truth \\textit{\\pyautogui} scripts that achieve successful execution.\n\n\\subsection{Task Formulation}\n\nGiven an input state of a computer defined by the screen $S$ and the task description $T$ in natural language, the goal of the task is to output a sequence of actions $A$ that can successfully accomplish the task $T$ within a screenshot $S$ $\\in \\{\\text{Linux, Windows, MacOS, Webpage}\\}$. Formally, the task can be defined as learning the transition function $f: T \\times S \\rightarrow A$. During dataset collection, we ensure that all task descriptions $T$ are feasible and can be accomplished in the current screenshot $S$. To reduce ambiguity and facilitate better evaluation, we ensure that task descriptions are detailed and unambiguous. Tasks can also be visually grounded (e.g., `Click the red button to start recording') or natural language based (e.g., `Click the My Account button'). We define the action space using the functionalities in the \\textit{\\pyautogui} library: $A \\in \\{\\text{`click', `dragTo', `scroll', `write'}, \\ \\ldots \\}$. The exhaustive list of actions is provided in Table~\\ref{tab:action}. Our action space is much larger than other benchmarks ~\\cite{shi2017world,deng2023mind2web,zhou2023webarena} that resort to two or three interaction options. Mouse actions such as `moveTo', `click', `rightClick', `doubleClick', and `dragTo', additionally require screen coordinates as arguments, which indicate the pixel location of the action. \n\nFigure~\\ref{fig:teaser} illustrates sample tasks and corresponding outputs for three applications within \\ModelName: (1) Stocks (MacOS), (2) Apartments.com (web page), and (3) Weather (MacOS). The first column depicts the input image, and the second column shows the natural language task that is to be executed on the current screen. To execute these tasks, a user must accurately perform a series of operations using the mouse and keyboard. Eg: to check the rate of change in Google's stock price over the last month, the mouse has to be moved to the last month and dragged while holding the left-click button to the current month.\n\n\\subsection{Dataset Preparation}\nTo prepare our dataset, we followed a pipelined approach, as summarized in Figure~\\ref{fig:dataset}. We first selected a variety of applications and websites. For each application or website, we created bounding boxes around key UI elements and labeled them according to their functionality, which is crucial for assisting human annotators in writing accurate \\textit{\\pyautogui} scripts. After each script is written, we converted the labels back into numeric coordinates, allowing us to align the scripts precisely with the locations of the UI elements. Finally, we thoroughly reviewed each script, focusing on its executability and adherence to syntax standards. This ensured the high quality and functionality of our dataset, making it a valuable resource for training and evaluating autonomous agents.\n\n\\begin{wraptable}{R}{5.5cm}\n\\centering\n\\caption{Action types supported by \\ModelName\\ and the number of instances for each action in the dataset.}\n\\resizebox{0.25\\textwidth}{!}{%\n\\begin{tabular}{@{}ccr@{}}\n\\toprule\n\\multicolumn{1}{l}{\\textbf{Type}}                                               & \\textbf{Action}            & \\multicolumn{1}{c}{\\textbf{\\%}} \\\\ \\midrule\n\\rowcolor[HTML]{FFFFC7} \n\\multicolumn{1}{c}{} & Click             & \\multicolumn{1}{c}{63.73}                  \\\\\n\\rowcolor[HTML]{FFFFC7} \n\\multicolumn{1}{c}{} & Double Click      & \\multicolumn{1}{c}{0.58}                   \\\\\n\\rowcolor[HTML]{FFFFC7} \n\\multicolumn{1}{c}{} & Right Click       & \\multicolumn{1}{c}{0.77}                   \\\\\n\\rowcolor[HTML]{FFFFC7} \n\\multicolumn{1}{c}{} & Move/Hover        & \\multicolumn{1}{c}{1.85}                   \\\\\n\\rowcolor[HTML]{FFFFC7} \n\\multicolumn{1}{c}{} & Drag              & \\multicolumn{1}{c}{0.29}                   \\\\\n\\rowcolor[HTML]{FFFFC7} \n\\multicolumn{1}{c}{} & Scroll            & \\multicolumn{1}{c}{1.68}                   \\\\\n\\rowcolor[HTML]{FFFFC7} \n\\multicolumn{1}{l}{\\multirow{-7}{*}{\\textbf{Mouse}}}    & Horizontal Scroll & \\multicolumn{1}{c}{0.17}                   \\\\ \\midrule\n\\rowcolor[HTML]{C3EFAF} \n\\multicolumn{1}{c}{} & Press             & \\multicolumn{1}{c}{16.28}                  \\\\\n\\rowcolor[HTML]{C3EFAF} \n\\multicolumn{1}{c}{} & Hotkey            & \\multicolumn{1}{c}{3.00}                      \\\\\n\\rowcolor[HTML]{C3EFAF} \n\\multicolumn{1}{l}{\\multirow{-3}{*}{\\textbf{Keyboard}}}  & Write             & \\multicolumn{1}{c}{11.65}                  \\\\ \\bottomrule\n\\end{tabular}%\n}\n\n\\label{tab:action}\n\\end{wraptable}\n\n\\subsubsection{Application/Website Selection}\nTo test the computer agents' generalization ability across different tasks, we collect tasks across multiple domains on both desktop and web applications. In total, we collect and annotate 9802 data points (Table~\\ref{tab:dataset-2}), with the split between desktop and web applications approximately 3:1. The emphasis on desktop applications, which do not contain Document Object Model (DOM) hierarchies unlike HTML-based web pages, presents a more complex multimodal challenge where visual cues are crucial. We collect tasks from applications within the three most popular operating systems. We select 22 native applications from MacOS, and 8 each from Linux and Windows. We annotate roughly 3 to 4 screens for every application. The full list of applications is provided in the Appendix. \n\nMany common computer tasks today are still performed through web applications, so we also collect 3-4 screenshots from 27 different web applications. To ensure diversity in task intents, we categorize these tasks into one of the following 6 categories: (1) Shopping, (2) Entertainment, (3) Service, (4) Government, (5) Travel, (6) Health. Inspired by the methodology of~\\cite{deng2023mind2web}, these categories were selected to cover a wide range of user intents and functionalities.\n\n\\subsubsection{UI Screen Segmentation}\nTo collect gold-standard data, we first annotate and segment the screen by identifying the bounding boxes present on the screen. We employ slightly different techniques for web and desktop applications to create the bounding boxes:\n\n\\begin{enumerate}\n    \\item \\textbf{Desktop Applications:} We build a custom annotation interface based on PyQt5\\footnote{\\url{https://pypi.org/project/PyQt5/}} to create bounding boxes manually over a screen image using a simple drag-and-click mechanism. This custom interface expedites the process and allows us to get highly accurate gold-label data for desktop images.\n    \\item \\textbf{Websites:} For webpages, we write JavaScript code to extract all interactable (click,  type, etc.) regions from HTML source code. We also extract banners, dropdowns, submit, and radio buttons from the screen. We filter the elements to retain only those that are visible and interactable within the screen.\n\\end{enumerate}\n\n\\subsubsection{Functionality Tagging}\n\nTo map each bounding box to its correct functional description, we leverage Amazon MTurk workers (see details in Appendix), who are given an image with a bounding box and are required to write the correct description or label of the bounding box's function. For example, given an image of an Amazon webpage with a \\textit{search bar}, the annotator labels it as \\textit{``find-product-search-bar\"}. The logical descriptions are used to create tasks in a structured manner without the need to identify individual bounding box coordinates.\n\n\\subsubsection{Task Creation}\n\n\\begin{figure*}[!ht]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/detact_page-0001.jpg}\n    \\caption{\\textbf{DetACT Module.} Given an initial image and a natural language task description, we use a pipelined approach to run OCR and SAM on the screen. The outputs from SAM are then used by icon and color-matching modules to obtain an exhaustive set of useful UI elements. The list of elements is passed through LLM based filter to select only the elements related to the given task.}\n    \\label{fig:detact}\n\\end{figure*}\n\nOur approach for each screen involves utilizing all human-annotated bounding boxes and their labels to create tasks that can be executed within the confines of a single screen. These tasks are designed to be visually grounded in order to measure the capabilities of multimodal agents. We plan to release the bounding box and their corresponding labels as the metadata for evaluation purposes.\n\nFor dataset compilation, college students with basic Python programming skills served as annotators, accessing API references for \\textit{\\pyautogui} and examples of potential tasks. Each student generated multiple tasks, each accompanied by three alternative natural language reformulations. For instance, \\textit{``What is 3+2?\"} might be reformulated as \\textit{``Calculate the sum of 2 and 3\"} or \\textit{``Add two to three\"}. To avoid train-test leakage, rephrased tasks were consistently placed in the same dataset split. Further details on the annotation process are available in the Appendix.\n\n\\subsubsection{Reverse Mapping and Filtering}\n\n\\begin{wraptable}{R}{5.5cm}\n\\centering\n\\caption{Dataset distribution across splits and platforms.}\n\\resizebox{0.4\\columnwidth}{!}{%\n\\begin{tabular}{@{}cccccc@{}}\n\\toprule\n\\multicolumn{2}{c}{\\textbf{Domain}}                                         & \\textbf{Train} & \\textbf{Validation} & \\textbf{Test}  & \\multicolumn{1}{c}{\\textbf{Total}} \\\\ \\midrule\n\\rowcolor[HTML]{FFFFC7} \n\\multicolumn{1}{c}{\\cellcolor[HTML]{FFFFC7}} & Mac OS               & 3028  & 444        & 786   & \\multicolumn{1}{c}{\\cellcolor[HTML]{FFFFC7}4258}  \\\\\n\\rowcolor[HTML]{FFFFC7} \n\\multicolumn{1}{c}{\\cellcolor[HTML]{FFFFC7}} & Linux                & 761   & 126        & 247   & \\multicolumn{1}{c}{\\cellcolor[HTML]{FFFFC7}1134}  \\\\\n\\rowcolor[HTML]{FFFFC7} \n\\multicolumn{1}{c}{\\multirow{-3}{*}{\\cellcolor[HTML]{FFFFC7}Desktop}} & Windows & 1573 & 216 & 458 & \\multicolumn{1}{c}{\\cellcolor[HTML]{FFFFC7} 2247} \\\\\n\\rowcolor[HTML]{C3EFAF} \n\\multicolumn{1}{c}{\\cellcolor[HTML]{C3EFAF}Web}                                           &          -            & 1427  & 206        & 530   & \\multicolumn{1}{c}{\\cellcolor[HTML]{C3EFAF} 2163}  \\\\ \\midrule\n\\textbf{Total}                                         &                      & 6789  & 992        & 2,021 & 9802  \\\\ \\bottomrule\n\\end{tabular}%\n}\n\\label{tab:dataset-2}\n\\end{wraptable}\n\nTo ensure high-quality data, we incorporate an additional step into the data collection pipeline. We build scripts to map the text-based labels of each bounding box back to their numeric coordinates, and then match the syntax and verify if the task will be executed on the screen. Using this filter, we remove all the non-working or syntactically incorrect data points and finally manually review the set of tasks.\n\nAfter filtering, we obtain 9802 human-annotated, gold-label data points across more than 200 desktop and web screens (Table~\\ref{tab:dataset-2}), split into train, validation, and test sets in a 7:1:2 ratio. All collected data will be publicly released to encourage future work on multimodal agents.\n\n\\section{Evaluation Metrics}\n\nIn this section, we detail various evaluation metrics for benchmarking model performance on the \\ModelName\\ dataset. UI screens have additional constraints such as spatial relevance which are not factored in most conventional similarity-based metrics such as BLEU~\\cite{papineni2002bleu}, CodeBLEU~\\cite{ren2020codebleu}, BERTScore \\cite{zhang2020bertscore} and CodeBERTScore~\\cite{zhou2023codebertscore}. For example, a valid click action is usually not constrained to a single coordinate but can be any coordinate within a specified region. In the event of invalid coordinate predictions, an agent that predicts coordinates further away from the valid region should invoke a higher penalty compared to an agent that predicted coordinates close to the region. We propose two new metrics adapted: Sequence Score (Section~\\ref{sec:sequence_score}) and Action Score (Section~\\ref{sec:action_score}) aimed at utilizing UI information. \n\n\\subsection{Sequence Score}  \\label{sec:sequence_score}\nThe sequence score measures whether the predicted action sequence (e.g., `click', `write', `press') exactly matches the gold sequence. Since predicting the first action in the sequence is relatively straightforward and later actions are more difficult, we define sequence score as follows: \\setlength{\\abovedisplayskip}{3pt}\n\\setlength{\\belowdisplayskip}{3pt}\n\\begin{equation}\nSeqScore_i = \n    \\begin{cases}\n        \\beta_1 + \\beta_2 * (s-1) & \\text{if all actions match} \\\\\n        0 & \\text{otherwise}\n    \\end{cases}\n\\end{equation}\nwhere $s$ is the action sequence length, $\\beta_1$ is set to 0.1 and $\\beta_2$ is set to 1.\n\\subsection{Action Score}  \\label{sec:action_score}\nThe action score measures how well a code snippet containing the correct action sequence can perform the task. Specifically, for a script with a correct action sequence, we introduce penalties for inaccurate behavior. The penalties are described below:\n\n\\begin{enumerate}\n    \\item \\textbf{Click penalty ($M$)}: For actions  `click', `rightClick', `doubleClick', `moveTo', and `dragTo', we penalize code snippets where predicted coordinates lie outside of the bounding box of the UI element. The click penalty for the $j^{th}$ action of the $i^{th}$ example is defined as:\n\\begin{equation}\n\\textbf{$M_i^j$} =  \\alpha_i \\times \n\\begin{dcases}\n    1 - \\frac{\\mu}{\\mu+L_2} & \\text{if $SeqScore_i$} > 0\\\\\n    1 & \\text{otherwise}\n\\end{dcases}\n\\end{equation}\n\nHere $L_2$ corresponds to the smallest Euclidean distance between the predicted coordinate and bounding box. $ L_2$ is zero when the predicted coordinate lies within the target bounding box. $\\mu$ is the Dirichlet smoothing coefficient which we dynamically set to the inverse of the length of the diagonal of the bounding box. This ensures that the penalty for points outside the bounding box varies based on the size of the bounding box. For two predicted points with the same $L_2$, the metric penalizes more heavily if the box is larger. This is sound with the intuition that the chances of clicking on a larger box are higher and should be penalized more in case of a mistake. \n    \\item \\textbf{Key penalty ($K$)}: For actions `press' and `hotkey', we check whether the set of keys in the target code (represented as $GK_i^j$) and predicted code (represented as $PK_i^j$) are the same. It is formally defined as:\n\\setlength{\\abovedisplayskip}{3pt}\n\\setlength{\\belowdisplayskip}{3pt}\n\\begin{equation}\n\\textbf{$K_i^j$ = }\n\\alpha_i \\times\n  \\begin{cases}\n    0 & \\text{if  }GK_i^j = PK_i^j \\text{ and } SeqScore_i > 0\\\\\n    1 & \\text{otherwise}\n  \\end{cases}\n\\end{equation}\n    \\item \\textbf{Write penalty ($W_p$)}: For action type `write', we penalize the output for the sentence to be typed. Specifically, we the employ BLEU score~\\cite{papineni2002bleu}, and compute:\n\\setlength{\\abovedisplayskip}{3pt}\n\\setlength{\\belowdisplayskip}{3pt}\n\\begin{equation}\n\\textbf{$W_i^j$ = }\n\\alpha_i \\times\n\\begin{cases}\n    \\text 1- {BLEU}(GS_i^j, PS_i^j) & \\text{if } SeqScore_i>1 \\\\\n    1 & \\text{otherwise}    \n\\end{cases} \n\\end{equation}\n\nHere, $GS_i^j$ represents the actual sentence to be typed, and $PS_i^j$ represents the sentence predicted by the model in the $j^{th}$ action of example $i$. \n\\end{enumerate}\nIn the above equations, ($\\alpha_i$) is the weighting factor:\n\\setlength{\\abovedisplayskip}{3pt}\n\\setlength{\\belowdisplayskip}{3pt}\n\\begin{equation}\n    \\alpha_i = SeqScore_i / \\text{ length of sequence $i$}\n\\end{equation}\n \n\nThis ensures that the action score $\\in [0,1]$. The mean action score is calculated as follows:\n\\setlength{\\abovedisplayskip}{3pt}\n\\setlength{\\belowdisplayskip}{3pt}\n\\begin{equation}\n\\textbf{Action Score = } \n\\frac{ \\sum_{i} max  \\left (SeqScore_i -  \\sum_j  (M_i^j + K_i^j+ W_i^j) , 0\\right )}{\\sum_{i} SeqScore_i}\n\\end{equation}\n\n\\section{ DetACT: DETecting ACTions from UI}\n\n\\begin{figure}[!ht]\n    \\centering\n    \\includegraphics[width=0.6\\linewidth]{figs/architecture2.pdf}\n\n    \\caption{\\textbf{Baseline Model Architecture.} Image and task descriptions are sent to DetACT module, which gives a filtered list of UI elements relevant to feed into the prompt along with the task. We also show the prompt structure used for action script generation. This structure is passed through the LLM (along with the image for multimodal LLM) to generate the automation script.}\n\n    \\label{fig:model}\n    \n\\end{figure}\n\nUnderstanding UI screens is crucial for multimodal computer tasks. Web-based agents typically use language-only inputs from the HTML DOM. This is insufficient for comprehending the full extent of an application UI, as many components may not be easily described with HTML code. To address this, we propose DetACT, which allows us to convert images of UI layouts into structured code and text outputs for a downstream LLM. DetACT is a system comprised of three distinct modules: the text module, the icon module, and the color module.\n\n\\begin{enumerate}\n    \\item \\textbf{Text Extraction:} We use the EasyOCR model\\footnote{\\url{https://github.com/JaidedAI/EasyOCR}} to parse over the UI screens and collect all text-based elements. Along with the text, we also note the locations of each of these elements. This is depicted in Figure \\ref{fig:detact}, along with a list of text elements found on the screen using the OCR Module. We segment and classify the different regions within the screenshot using the Segment Anything Model (SAM)~\\cite{kirillov2023segment}. From the outputs, we filter out the non-textual segments for our icon and color detection.%, as described below.\n    \n\\item \\textbf{Icon Module:} For matching with the appropriate icon, we use a pack of 1600 icons\\footnote{\\url{https://icomoon.io/}} as templates. Each of these icons is labeled with their appropriate functionality and is matched with the filtered outputs SAM~\\cite{kirillov2023segment}. For the similarity of the two images, we resize the reference icons and segmented region of interest (ROI) to the same size, and convert both images to grayscale. After this, we use the Structural Similarity Index (SSIM)~\\cite{wang2004image}, to find the closest match of the ROI to the icons in our set, and select the ones above the SSIM threshold of 0.95. As seen in Figure~\\ref{fig:detact}, a few icons matched on the screen are \\textit{Globe} icon, \\textit{Calendar} icon, \\textit{Person} icon, and \\textit{Location} icon; each depicting a different use case. % for the travel application.\n\\item \\textbf{Color Module:} Finally, to place all segments of interest into appropriate buckets of colors, we average the RGB pixel values over the ROI and, based on that value, bucket them into different color categories. We categorize colors differently based on the human perspective of the ranges of each color. To avoid ambiguity, we consider eleven major colors, namely yellow, blue, green, red, pink, violet, white, black, orange, brown, and grey. We record the center of the element along with the color. \n\\end{enumerate}\n\nOnce all the elements of each category are extracted with their coordinates, we then filter these UI elements by prompting GPT-4 \\cite{openai2023gpt4}. We ensure that the elements selected are suited only for our task, for which we also provide the task description in our prompts along with the list of elements. Full details of the prompt are provided in the appendix section of the paper. As we observe in Figure~\\ref{fig:detact}, given an image from the Expedia application, and a task (\\textit{``Click on the Black Location icon and enter the destination as Paris.\"}), the LLM filters out the elements to retain only \\textit{``Going To\"}, \\textit{``Location Icon\"}, and the \\textit{Black} colored elements from the screen. This is passed as input to the LLM or VLM backbone. %, which we discuss in the following section. \n\n\\section{Baselines}\n\nTo evaluate the performance of existing language model-based agents, we conduct experiments with both language-based and multimodal baselines. The DetACT module takes in image and text descriptions of the task and outputs the color, icon, and text-based signals. This is concatenated to the prompt for the LLM prompt-based baselines (see Figure~\\ref{fig:model}). Every prompt starts with a role assignment~\\cite{zhao2023survey}, followed by the detailed API reference of the \\textit{\\pyautogui} function set, along with a textual description of their function. We then add five in-context examples from the training set that most closely match the task (based on the cosine similarity of the MiniLM~\\cite{wang2020minilm} embeddings of the reference task and the train examples). We add a list of UI elements filtered by the DetACT module to the prompt. Finally, we provide the rules with the task description. For multimodal baselines, we also pass the image pixels to the vision encoder. We choose coordinate-based UI elements in the prompt as recent techniques like the Set-of-Mark (SOM)~\\cite{yang2023setofmark} prompting does not work for desktop settings since it is difficult to obtain interactive elements from the desktop screen images. We report the results of several baselines:\n\\begin{itemize}\n    \\item \\textbf{Few-shot Generative LLM:} \n    We experiment with models from LLaMA-2~\\cite{touvron2023llama}, Vicuna-1.5~\\cite{vicuna2023}, CodeLLaMA-34B~\\cite{rozière2023code}, Palmyra~\\cite{Palmyra}, and GPT \\cite{openai2023gpt4} series. We use the prompts structure as shown in Figure~\\ref{fig:model} to prompt the model. For LLaMA and CodeLLaMa, we reduce the prompt length to 2000 tokens by removing outputs from the DetACT module with lower confidence, as we observed poor performance on longer prompts. For the other models, we allow prompts with up to 4000 token sizes.\n    \n    \\item \\textbf{Finetuned Generative LLM:} \n    We fine-tuned the LLaMA-13B model and Vicuna-13B using QLoRa~\\cite{dettmers2023qlora} with rank 64 and scaling factor 16 for 300 steps to generate the code given screen description from the DetACT module and the instruction.% We detail the hyperparameters and training details in the Appendix.\n\n    \\item \\textbf{Few-shot Generative Multimodal Models:}\n    As \\ModelName\\ is predominantly multimodal, with a majority of tasks being visually grounded, we conduct experiments with large multimodal models. Given the limited research in this domain \\cite{mlm_survey, mlm_survey2}, there is a scarcity of available multimodal models with significant size adept for this task. Here, we experiment with \\cite{liu2023llava, liu2023improvedllava}, providing a similar prompt as well as the screen image.% in addition to text. \n\\end{itemize}\n\n\\section{Results and Analysis}\n\\label{sec:results}\n\n\\begin{wraptable}{R}{5.5cm}\n\\centering\n\\caption{Baseline Performance. (A) Prompt-only LLMs, (B) Fine Tuned LLMs, (C) Prompt-only Multimodal Models. The table represents the Sequence score (SS), click penalty ($M_p$), Key penalty ($K_p$), Write Penalty ($W_p$), and Action Score (AS). The best results for the (SS) and (AS) are highlighted.}\n\\resizebox{0.45\\columnwidth}{!}{%\n\\begin{tabular}{@{}lcccccc@{}}\n\\toprule\n\\textbf{Model} &\n  \\textbf{\\begin{tabular}[c]{@{}c@{}}SS($\\uparrow$)\\end{tabular}} &\n  \\textbf{\\begin{tabular}[c]{@{}c@{}}$M_p$\\end{tabular}} &\n  \\textbf{\\begin{tabular}[c]{@{}c@{}}$K_p$\\end{tabular}} &\n  \\textbf{\\begin{tabular}[c]{@{}c@{}}$W_p$\\end{tabular}} &\n  \\textbf{\\begin{tabular}[c]{@{}c@{}}AS($\\uparrow$)\\end{tabular}}  \\\\ \\midrule\n\\textbf{\\small Prompt based LLMs} \\\\\nLLaMA-7B~\\cite{touvron2023llama}              & 4.12  & 1.24  & 1.83 & 0.57 & 0.48  \\\\\nVicuna-7B ~\\cite{vicuna2023}            & 3.88  & 1.17  & 1.51 & 0.43 & 0.77  \\\\\nLLaMA-13B ~\\cite{touvron2023llama}            & 4.80   & 1.32  & 0.93 & 0.93 & 1.62\\\\\nVicuna-13B ~\\cite{vicuna2023}           & 5.44   & 1.65  & 0.94  & 1.06 & 1.78  \\\\\nPalmyra-Instruct-30B \\cite{InstructPalmyra} & 7.51   & 5.68  & 0.12 & 0.40  & 1.31  \\\\\nCodeLLaMA-34B \\cite{rozière2023codellama}        & 10.09 & 2.99  & 2.71 & 0.66 & 3.72 \\\\\nPalmyra-X 43B \\cite{palmyra-x}        & 11.20  & 3.12  & 3.02 & 2.12 & 2.94 \\\\\nGPT-3.5-turbo-0613~\\cite{chatgpt-citation}       & 22.85 & 8.13  & 4.51 & 2.31  & 7.89 \\\\\nGPT-4  \\cite{openai2023gpt4}               & \\textbf{32.75} & 10.27 & 6.99 & 3.89 & \\textbf{11.60} \\\\ \\midrule\n\\textbf{\\small Finetuned LLMs} \\\\\nLLaMA-13B FT          & \\textbf{8.92}  & 4.61  & 1.43 & 0.74 & 2.14  \\\\\nVicuna-13B FT         & 8.78  & 4.12  & 1.31 & 0.63 & \\textbf{2.72}  \\\\ \\midrule\n\\textbf{\\small Multimodal LLMs} \\\\\nLLaVA-v1.5-7B~\\cite{liu2023llava}        & 13.23 & 4.73  & 1.24 & 1.44 & 5.82  \\\\\nLLaVA-v1.5-13B~\\cite{liu2023improvedllava}       & {20.56} & 6.07  & 3.44 & 2.85 & {8.19}\\\\ \n{Gemini-Pro}~\\cite{team2023gemini} & 30.98 & 9.05 & 6.81 & 3.66 & 11.46\\\\\n{GPT-4V}~\\cite{liu2023improvedllava}       & \\textbf{38.72} & 10.53  & 7.14 & 4.03 & \\textbf{17.02}\\\\ \n \\midrule\nHuman Performance & 82.23 & 0.12 & 0.36 & 1.61 & 80.14\\\\\n\\bottomrule\n\\end{tabular}%\n}\n\n\\label{tab:results}\n\\end{wraptable}\n\nAs shown in Table~\\ref{tab:results}, we experiment with three different categories of models, namely Prompt-based LLMs, Fine-tuned LLMs, and Prompt-based Multimodal Models. \nGPT-4 is the best-performing approach, scoring higher on the sequence score and invoking lower penalties on coordinate predicting and text input. \nFor prompt-only LLMs, the GPT-3.5-turbo and GPT-4 models outperform the other LLM baselines, including the LLaMA~\\cite{touvron2023llama} and Vicuna~\\cite{vicuna2023} models. We observe that CodeLLaMA-34B~\\cite{rozière2023codellama}, which is trained for code generation, also achieves a higher performance than other models of the same size at predicting the action sequences. \n\nFine-tuned models also perform much better than their few-shot prompt-only counterparts. Fine-tuning substantially improves LLaMA-13B's sequence score (4.80 to 8.92) and action score (1.62 to 2.14), as well as the other metrics. \nDespite this, we observed that both, prompt-based LLMs and finetuned LLMs face severe mouse penalties, especially on click coordinates. This is because they rely solely on text-based signals. \n\nTo address this, we experiment with multimodal language models (Table~\\ref{tab:results}). We observe that the coordinate prediction improves significantly when we provide the entire image as input to the multimodal LLM, as this enables it to fully utilize the screen representation. In addition to open sourced models, we also experiment with the GPT-4-vision API~\\cite{yang2023dawn} which shows that GPT-4 Vision~\\cite{yang2023dawn} outperforms GPT-4 significantly on the Action Score along with improving the sequence score, which we attribute to the strong reasoning abilities of GPT-4 coupled with the improved visual understanding capabilities of the GPT-4-vision model~\\cite{yang2023dawn}. These findings pave the way towards exciting new research directions on building multimodal models for long-horizon planning and code generation. \\\\\n\n\\textbf{Human performance over the task:} \\ModelName{} consists of visually complicated tasks, and tests various types of computer skills. In order to get a gauge of how well humans perform, we collect evaluation data from human evaluators. We split the test set uniformly amongst 10 human evaluators, and provided them with the screenshot and task instruction. We record the actions taken by the annotators, and measure their performance on our predefined metrics (Table~\\ref{tab:results}). \nWe find that users generally exhibit a high level of proficiency when attempting most tasks for the first time. However, there are instances where users face difficulties in successfully completing certain tasks. These are due to factors including the user's inability to fully comprehend the task, difficulties in grounding the task to the provided screenshot, or a lack of familiarity with the UI.\n\n\\section{Conclusion and Future Work}\n\nAutonomous virtual agents offer the potential to automate routine tasks, benefiting users with limited technical expertise. To solve this task, we introduce \\ModelName{}, a unique dataset of 9.8K human-labeled data points. \\ModelName{} benchmarks autonomous agents across a range of tasks on web and desktop applications. \nLLM-based agents, like GPT-4, achieve a respectable action score of 11.6 on our dataset. However, \\ModelName{} presents a challenge for the current state-of-the-art language and multimodal models. It provides a direction for future research on foundational multimodal models that seamlessly integrate language and visual understanding of computer screens and stands poised to drive the next wave of advancements in generalist autonomous agents offering omnipotent assistance to humans$\\text{.}$\n\n\\section{Limitations}\n\nThis work introduces a valuable dataset, yet we recognize a few limitations that exist. State-of-the-art models like GPT-4, may exhibit susceptibility to hallucinations and bias towards specific data types, hindering broad applicability. Reliance on closed models like GPT-4V poses integration challenges due to high costs and time constraints. Despite efforts for equal representation and data collection without personal information, biases may be introduced as the dataset is exclusively in English, and human-curated content may have temporal biases.\n\n\\section{Ethics Statement}\nAs a part of the dataset creation process, we carefully review the pipeline at every stage, ensuring there is no personally identifiable information or offensive content, either during data collection or through the use of LLMs. For all purposes, we create dummy accounts that mimic real-like user content. To get the gold labels scripts we seek help from well-qualified student workers, approved through the institution, and get the bounding box data annotated through MTurk workers, both of whom are paid $\\$25$ per hour, which is greater than the minimum wage rate (We detail this process in the supplementary material). Human studies are also done with the help of student workers approved by the institution at the above-mentioned payscale. We also ensure that all groups have equitable representation and that no personal opinions are reflected in the dataset, avoiding bias during the collection as well as the annotation process.\n\n\\section*{Acknowledgements} We extend our heartfelt gratitude to Writer.com for their generous and unwavering support throughout this project. Their dedicated team's expertise and collaboration were invaluable in achieving our goals.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2202.02312v3.tex",
        "arXiv-2307.13854v4.tex",
        "arXiv-2402.17553v3.tex"
    ],
    "group_id": "group_22",
    "response": "### Title: Vision-Language Navigation and Autonomous Agents in Realistic Digital Environments\n\n### Introduction\nVision-language navigation (VLN) and autonomous agents that can perform tasks based on natural language instructions have been areas of significant interest in the field of artificial intelligence (AI) and human-computer interaction (HCI). VLN involves an agent following language instructions in a visual environment, which has been primarily studied in home environments and digital spaces like websites and mobile apps. However, existing datasets and benchmarks often assume that the input commands are fully feasible within the environment, which is not always the case in real-world scenarios. This assumption overlooks the practical challenges of task feasibility and the need for agents to understand and interact with diverse and complex environments. \n\nCurrent research in VLN and autonomous agents has made notable progress, but it is still limited in terms of handling tasks with unknown feasibility and in realistic settings. The feasibility of tasks can be influenced by various factors, including the ambiguity of the language command, the state of the environment, and the dynamic nature of the applications. Additionally, the performance of these agents is often evaluated in simplified, synthetic environments, which do not accurately reflect the complexities of real-world tasks. \n\nTo address these limitations, researchers have begun to develop new datasets and benchmarks that capture the nuances of task feasibility and the challenges of performing tasks in realistic environments. This summary will explore three recent papers that introduce new datasets and benchmarks for vision-language navigation and autonomous agents in desktop and web applications. Each paper aims to provide a more comprehensive and realistic evaluation of these agents, highlighting the challenges and potential solutions for improving their capabilities.\n\n### Main Content of Each Paper\n\n#### Paper 1: Mobile app Tasks with Iterative Feedback (MoTIF)\nThe first paper introduces the Mobile app Tasks with Iterative Feedback (MoTIF) dataset, which is designed to study vision-language navigation (VLN) in mobile apps with unknown command feasibility. MoTIF is a scalable dataset that includes natural language commands for tasks in mobile apps, along with their corresponding action sequences, app screen images, and view hierarchy data. The dataset is unique in that it contains both feasible and infeasible tasks, with annotations for why tasks are infeasible and follow-up questions to resolve task uncertainty. \n\nThe authors propose a new problem of feasibility prediction, where the goal is to predict whether a given natural language instruction is feasible in a mobile app environment using multimodal inputs. They also evaluate several existing VLN models on task automation, demonstrating that these models struggle with unseen app environments and require better visual and textual representations to improve performance. The paper highlights the importance of visual understanding in determining task feasibility and suggests that future work should focus on hierarchical representations and better multimodal encodings.\n\n#### Paper 2: WebArena: A Realistic Web Environment for Autonomous Agents\nThe second paper presents WebArena, a realistic and reproducible web environment designed to evaluate the functional correctness of autonomous agents that perform tasks based on natural language instructions. WebArena includes fully functional websites from four common domains: e-commerce, social forums, collaborative software development, and content management. The environment also incorporates utility tools and knowledge bases to support human-like task executions. \n\nWebArena introduces a benchmark of 812 long-horizon web-based tasks, each described as a high-level natural language intent. The tasks are diverse and complex, requiring long-term planning and reasoning. The authors evaluate several baseline agents, including GPT-4 and GPT-3.5, and find that even the best models achieve only a 14.41\\% end-to-end task success rate, significantly lower than human performance. This suggests that current large language models (LLMs) lack the necessary capabilities for robust task automation in realistic web environments. The paper emphasizes the need for agents to have better exploration and failure recovery mechanisms to improve their performance.\n\n#### Paper 3: OmniACT: A Dataset and Benchmark for Multimodal Generalist Autonomous Agents\nThe third paper introduces OmniACT, a multimodal dataset and benchmark for assessing agents' ability to generate executable scripts to accomplish computer tasks on both desktop and web applications. Unlike previous benchmarks that focus on text-based reasoning, OmniACT aims to measure multimodal agents that integrate large language model (LLM) planners and vision models for understanding user interfaces (UIs). The dataset consists of over 9.8K pairs of UI screenshots and natural language tasks, along with the corresponding \\textit{\\pyautogui} scripts that achieve successful task execution.\n\nOmniACT proposes DetACT, a module for detecting actions from UI screenshots, which converts images into structured code and text outputs for downstream LLMs. The authors evaluate several LLM-based and multimodal models on the dataset and find that GPT-4 achieves the highest performance, with a sequence score of 32.75 and an action score of 11.60. However, even GPT-4 faces challenges in predicting correct click coordinates and text inputs, highlighting the need for better multimodal understanding. The paper also introduces new evaluation metrics that consider the spatial relevance of UI elements and the accuracy of action sequences.\n\n### Commonalities and Innovations\nAll three papers aim to develop more realistic and comprehensive datasets and benchmarks for evaluating the performance of autonomous agents in digital environments. They share a common goal of bridging the gap between simulated environments and real-world scenarios, where task feasibility and the diversity of applications are critical factors. \n\nHowever, each paper introduces unique innovations and focuses on different aspects of the problem. MoTIF is the first dataset to capture task uncertainty in mobile apps, providing a platform to study the feasibility of natural language commands. WebArena emphasizes the functional correctness of task executions and includes a diverse range of websites and utility tools to support human-like task-solving. OmniACT extends the scope to desktop applications and introduces DetACT, a module for converting UI screenshots into structured code and text outputs, enabling multimodal models to better understand and interact with the environment.\n\n### Comparison of Results and Discussion\nThe results from the three papers highlight the challenges faced by current autonomous agents in realistic digital environments. MoTIF's task feasibility prediction model achieves an F1 score of 61.1 when using CLIP embeddings for the task, view hierarchy, and app screen image. However, the performance is still relatively low, indicating that there is significant room for improvement in vision-language encodings and multimodal representations.\n\nWebArena's evaluation shows that the best GPT-4 agent achieves an end-to-end task success rate of only 14.41\\%, significantly lower than human performance. This suggests that current LLMs lack the necessary capabilities for robust task automation in complex web environments. The authors observe that the performance gap is primarily due to the models' inability to handle task feasibility and perform long-term planning and reasoning.\n\nOmniACT's results indicate that GPT-4 performs the best among the evaluated models, achieving a sequence score of 32.75 and an action score of 11.60. However, even GPT-4 faces challenges in predicting correct click coordinates and text inputs, demonstrating the need for better multimodal understanding. The authors introduce new evaluation metrics that consider the spatial relevance of UI elements and the accuracy of action sequences, providing a more comprehensive assessment of agent performance.\n\n### Conclusion\nThe three papers collectively demonstrate the need for more realistic and comprehensive datasets and benchmarks for evaluating autonomous agents in digital environments. MoTIF introduces a scalable dataset for studying task feasibility in mobile apps, while WebArena focuses on functional correctness in web-based tasks. OmniACT extends the scope to desktop applications and introduces a multimodal approach for understanding and interacting with UIs. \n\nThe results from these papers highlight the limitations of current LLMs and multimodal models in handling task feasibility, long-term planning, and reasoning in complex environments. Future research should focus on developing better multimodal models that can integrate language and visual understanding, as well as incorporating exploration and failure recovery mechanisms to improve agent performance. The datasets and benchmarks introduced in these papers provide valuable resources for advancing the field of vision-language navigation and autonomous agents, enabling researchers to measure progress and identify areas for improvement."
}