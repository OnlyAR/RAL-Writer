{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{QA-MDT: Quality-aware Masked Diffusion Transformer for \\\\ Enhanced Music Generation}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nIn recent years, diffusion-based text-to-music (TTM) generation has gained prominence, offering an innovative approach to synthesizing musical content from textual descriptions. Achieving high accuracy and diversity in this generation process requires extensive, high-quality data, including both high-fidelity audio waveforms and detailed text descriptions, which often constitute only a small portion of available datasets. In open-source datasets, issues such as low-quality music waveforms, mislabeling, weak labeling, and unlabeled data significantly hinder the development of music generation models.\nTo address these challenges, we propose a novel paradigm for high-quality music generation that incorporates a quality-aware training strategy, enabling generative models to discern the quality of input music waveforms during training. Leveraging the unique properties of musical signals, we first adapted and implemented a masked diffusion transformer (MDT) model for the TTM task, demonstrating its distinct capacity for quality control and enhanced musicality. Additionally, we address the issue of low-quality captions in TTM with a caption refinement data processing approach. Experiments demonstrate our state-of-the-art (SOTA) performance on MusicCaps and the Song-Describer Dataset.\nOur demo page can be accessed at https://qa-mdt.github.io/.\n\\end{abstract}\n\n\\section{Introduction}\n\\renewcommand{\\thefootnote}{}\n\\footnotetext{* Equal Contribution \\\\ code and checkpoints on https://github.com/ivcylc/qa-mdt}\nText-to-music (TTM) generation aims to transform textual descriptions of emotions, style, instruments, rhythm, and other aspects into corresponding music segments, providing new expressive forms and innovative tools for multimedia creation. According to scaling law principles~\\citep{peebles2023scalable, li2024scalability}, effective generative models require a large volume of training data. However, unlike image generation tasks~\\citep{chen2024pixart, rombach2021highresolution}, acquiring high-quality music data often presents greater challenges, primarily due to copyright issues and the need for professional hardware to capture high-quality music. These factors make building a high-performance TTM model particularly difficult.\n\nIn the TTM field, high-quality paired data of text and music signals is scarce. This prevalent issue of low-quality data, highlighted in Figure~\\ref{fig:enter-label}, manifests in two primary challenges. Firstly, most available music signals often suffer from distortion due to noise, low recording quality, or outdated recordings, resulting in diminished generated quality, as measured by pseudo-MOS scores from quality assessment models~\\citep{ragano2023audio}. Secondly, there is a weak correlation between music signals and captions, characterized by missing, weak, or incorrect captions, leading to low text-audio similarity, which can be indicated by CLAP scores~\\citep{laionclap2023}. These challenges significantly hinder the training of high-performance music generation models, resulting in poor rhythm, noise, and inconsistencies with textual control conditions in the generated audio. Therefore, effectively training on large-scale datasets with label mismatches, missing labels, or low-quality waveforms has become an urgent issue to address.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\linewidth]{pic_for_nips/fengmian.pdf}\n    \\caption{The distribution curves of CLAP similarity and pseudo-MOS for large-scale open-source music databases AudioSet~\\citep{defferrard2016fma} and FMA~\\citep{defferrard2016fma}, where darker areas represent higher text-audio alignment or audio quality.}\n    \\label{fig:enter-label}\n\\end{figure}\n\nIn this study, we introduce a novel quality-aware masked diffusion transformer (QA-MDT) to enhance music generation. This model effectively leverages extensive, open-source music databases, often containing data of varying quality, to produce high-quality and diverse music. During training, we inject quantified music pseudo-MOS (p-MOS) scores into the denoising stage at multiple granularities to foster quality awareness, with coarse-level quality information seamlessly integrated into the text encoder and fine-level details embedded into the transformer-based diffusion architecture. A masking strategy is also employed to enhance the spatial correlation of the music spectrum and further accelerate convergence. This innovative approach guides the model during the generation phase to produce high-quality music by leveraging information associated with elevated p-MOS scores.\nAdditionally, we utilize large language models (LLMs) and CLAP model to synchronize music signals with captions, thereby enhancing text-audio correlation in extensive music datasets. Our ablation studies on public datasets confirm the effectiveness of our methodology, with the final model surpassing previous works in both objective and subjective measures.\nThe main contributions of this study are as follows:\n\\begin{itemize}\n    \\item We propose a quality-aware training paradigm that enables the model to perceive the quality of the dataset during training, thereby achieving superior music generation in terms of both musicality and audio quality.\n    \\item We innovatively introduced the Masked Diffusion Transformer to music signals, demonstrating its unique efficacy in modeling music latent space and its exceptional capability in perceiving quality control, thereby further improving both the generated quality and musicality.\n    \\item We address the issue of low text-audio correlation in large-scale music datasets for TTM, effectively improving text alignment and generative diversity.\n\\end{itemize}\n\n\\section{Related Work}\\label{sec:rel}\n\n\\paragraph{Text to music generation.}\nText-to-music generation aims to create music clips that correspond to input descriptive or summary text. Previous efforts have utilized either language models (LMs) or diffusion models (DMs) to model quantized waveform representations or spectral features. Models like MusicLM~\\citep{agostinelli2023musiclm}, MusicGen~\\citep{copet2024simple}, MeLoDy~\\citep{lam2024efficient}, and Jen-1~\\citep{li2024jen} leverage LMs and DMs on residual codebooks obtained via quantization-based codecs~\\citep{zeghidour2021soundstream,defossez2022high}. Mo√ªsai~\\citep{schneider2023mo}, Noise2Music~\\citep{huang2023noise2music}, Riffusion~\\citep{forsgren_martiros_2022}, AudioLDM 2~\\citep{liu2023audioldm2}, and Stable Audio~\\citep{evans2024fast} use U-Net-related diffusion to model mel-spectrograms or latent representations obtained through compression networks. Although some approaches attempt to guide the model towards generating high-quality content by setting negative prompts like ``low quality''~\\citep{liu2023audioldm2, chen2024musicldm}, few explicitly inject quality information during training. This results in the model's inability to effectively perceive and control content quality.\n\n\\paragraph{Transformer based diffusion models.}\nTraditional diffusion models typically use U-Net as the backbone, where the inductive biases of CNNs do not effectively model the spatial correlations of signals and are insensitive to scaling laws~\\citep{li2024scalability}. However, transformer-based diffusion models (DiT) \\citep{peebles2023scalable} have effectively addressed these issues. This advantage is particularly evident in fields such as video generation~\\citep{brooksvideo}, image generation~\\citep{peebles2023scalable,chen2024pixart,bao2022all}, and speech generation~\\citep{liu2023vit}. To expedite training and foster inter-domain learning of correlations, the masking strategy has proven effective, yielding SOTA class-conditioned performances on ImageNet~\\citep{gao2023masked}. Additionally, a simpler architecture~\\citep{zheng2023fast} incorporating reconstruction losses and unmasked fine-tuning further enhances model training speed.\nHowever, these models have not yet been verified for text-controlled music generation on large-scale music datasets, and their adaptability with additional control information remains an open question. Make-an-audio 2~\\citep{huang2023make} and, more recently, Stable Audio 2~\\citep{123evans2024long}, have explored the DiT architecture for audio and sound generation. However, their approach models latent tokens by segmenting only along the time dimension to control and extend generation duration. In contrast, our focus is on finer segmentation within the latent space across both time and frequency, aiming for more precise modeling of music signals.\n\n\\paragraph{Quality enhancement in audio domain.}\nPrevious research has made efforts to improve the quality of generated audio, particularly in two key areas: waveform fidelity and the alignment between input text and generated content. Waveform quality can be compromised by issues like aliasing from low sampling rates and limited expressiveness due to monophonic representations, while models like MusicGen~\\citep{copet2024simple} and Stable Audio~\\citep{evans2024fast, 123evans2024long}, which directly model 32k and 44.1k stereo audio, have significantly enhanced perceptual quality. Despite higher sampling rates and channels, the quality of audio in training datasets remains inconsistent, often suffering from noise, dullness, and a lack of rhythm or structure. These problems, often reflected by the Mean Opinion Score (MOS), are rarely addressed. In terms of text-audio alignment, Make-an-audio 2~\\citep{huang2023make} and WavCaps~\\citep{mei2024wavcaps} have employed ChatGPT-assisted data augmentation to improve temporal relationships and accuracy in audio effect generation. Although studies like Music-llama~\\citep{liu2024music} and LP-musiccaps~\\citep{doh2023lp} have introduced captioning approaches for music, few have explored the augmentation and utilization of synthetic data in large-scale music generation tasks.\n\n\\section{Preliminary}\\label{sec:rel}\n\n\\paragraph{Latent diffusion model.}\nDirect application of DMs to cope with distributions of raw signals incurs significant computational overhead~\\citep{ho2020denoising, song2020denoising}. Conversely, studies~\\citep{liu2023audioldm, liu2023audioldm2} apply them in a latent space with fewer dimensions. %through the application of the Short-Time Fourier Transform (STFT) and a pretrained variational autoencoder (VAE) \\citep{liu2023audioldm} to a music waveform \\( M \\). This results in \\( M_\\text{spec} \\in \\mathbb{R}^{L \\times F} \\), where \\( L \\) and \\( F \\) denote the temporal and frequency bins, respectively.\nThe latent representation \\( z_0 \\) is the ultimate prediction target for DMs, which involve two key processes: diffusion and reverse processes. In the diffusion process, Gaussian noise is incrementally added to the original representation at each time step \\( t \\), described by \\( z_{t+1} = \\sqrt{1-\\beta_t} z_t + \\sqrt{\\beta_t} \\epsilon \\), where \\( \\epsilon \\) is drawn from a standard normal distribution \\( \\mathcal{N}(0, I) \\), and \\( \\beta_t \\) is gradually adapted based on a preset schedule to progressively introduce noise into the state \\( z_t \\).\nThe cost function~\\citep{ho2020denoising,liu2023audioldm} is formalized as\n\\(\n\\arg\\min_{\\theta}  \\mathbb{E}_{{(z_0, y), \\epsilon}} \\left[ \\left\\| \\epsilon - D_\\theta\\left(\\sqrt{\\alpha_t}z_0 + \\sqrt{1-\\alpha_t}\\epsilon, t, y\\right) \\right\\|^2 \\right] \n\\).\nwhere \\( D_\\theta \\), the denoising model, strives to estimate the Gaussian noise \\( \\epsilon \\), conditioned on the latent state \\( z_t \\), the time step \\( t \\), the conditional embedding \\( y \\), and where \\(\\alpha_t\\) represents a predefined monotonically increasing function.\n{In the reverse process, we obtain \\( z_{t-1} \\) via the recursive equation:\n\\(\nz_{t-1} = \\frac{1}{\\sqrt{1- \\beta_t}}\\left( z_{t} - \\frac{\\beta_t}{\\sqrt{1-\\alpha_t}}\\epsilon_\\theta \\right)  + \\sqrt{\\frac{1-\\alpha_{t-1}}{1-\\alpha_t}\\beta_t}  \\epsilon\\),\n where \\(\\epsilon_\\theta\\) represents the estimated Gaussian noise.}\n\n\\paragraph{Classifier-free guidance.}\nClassifier-free guidance (CFG), introduced by~\\citep{ho2020denoising}, increases the versatility of DMs by enabling both conditional and unconditional generation. Typically, a diffusion model generates content based on specific control signals \\( y \\) within its denoising function \\( D_{\\theta}(z_t, t, y) \\). CFG enhances this mechanism by incorporating an unconditional mode \\( D_{\\theta}(z_t, t, \\emptyset) \\), where \\( \\emptyset \\) symbolizes the absence of specific control signals. The CFG-enhanced denoising function is then expressed as \\( D_{\\theta}^{\\text{CFG}}(z_t, t, y) = D_{\\theta}(z_t, t, y) + w(D_{\\theta}(z_t, t, y) - D_{\\theta}(z_t, t, \\emptyset)) \\), where \\( w \\geq 1 \\) denotes the guidance scale.\nDuring training, the model substitutes \\( y \\) with \\( \\emptyset \\) at a constant probability \\( p_{\\text{uncond}} \\). In inference, \\( \\emptyset \\) might be replaced by a negative prompt like ``low quality'' to prevent the model from producing such attributes~\\citep{liu2023audioldm2}.\n\n\\section{Method}\\label{sec:method}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{0520_music.pdf}\n    \\caption{Pipeline of proposed quality-aware masked diffusion transformer for music generation.}\n    \\label{fig:kuangjia}\n\\end{figure*}\n\n\\subsection{Quality Information Injection}\n\\label{quality_informaton_injection}\nAt the heart of our work lies the implementation of a pseudo-MOS scoring model~\\citep{ragano2023audio} to meticulously assign music quality to quality prefixes and quality tokens. \n\nWe define our training set as \\(\\mathcal{D}_o = \\{(M_i, T^{o}_i) \\mid i = 1, 2, \\dots, N_D\\}\\), where each \\(M_i\\) represents a music signal and \\(T^{o}_i\\) is the corresponding original textual description.\nTo optimize model learning from datasets with diverse audio quality and minimize the impact of low-quality audio, we initially assign p-MOS scores to each music track using a model fine-tuned with wav2vec 2.0~\\citep{baevski2020wav2vec} on a dataset of vinyl recordings for audio quality assessment, and achieve the corresponding p-MOS set \\( S = \\{s_1, s_2, \\ldots, s_n\\} \\). These scores facilitate dual-perspective quality control for enhanced granularity and precision. \n\nFirst, We analyze this p-MOS set \\(S\\) to identify a negative skew normal distribution with mean \\( \\mu \\) and variance \\( \\sigma^2 \\). We define text prefixes based on \\( s \\) as follows: prepend ``low quality'' if \\( s < \\mu - 2\\sigma \\), ``medium quality'' if \\( \\mu - \\sigma \\leq m \\leq \\mu + \\sigma \\), and ``high quality'' if \\( s > \\mu + 2\\sigma \\). This information is prepended before processing through the text encoder with cross-attention, enabling the initial separation of quality-related information.\n\nTo achieve a more precise awareness and control of waveform quality, we synergize the role of text control with quality embedding. We observed that the distribution of p-MOS in the dataset is approximately normal, which can be shown in Figure \\ref{fig:enter-label}, allowing us to use the Empirical Rule to segment the data accordingly. Specifically, we define the quantization function \\( Q: [0, 5] \\to \\{1, 2, 3, 4, 5\\} \\) to map the p-MOS scores to discrete levels based on the distance from the mean \\( \\mu \\) in terms of standard deviation \\( \\sigma \\):\n\\begin{equation}\nQ(s) = \\left\\lfloor \\frac{s - (\\mu - 2\\sigma)}{\\sigma} \\right\\rfloor + r\n\\end{equation}\n\nwhere \\(r=2\\) %\\(r=\\mathbf{1}\\in \\mathbb{R}^2\\) \nfor \\(s>\\mu\\), otherwise, \\(r=1\\). \nSubsequently, \\( Q(s) \\) is mapped to a \\( d \\)-dimensional quality vector embedding using the embedding function \\( E \\), such that\n\\begin{equation}\nq_{\\text{vq}}(s) = E(Q(s)) \\in \\mathbb{R}^d,\n\\end{equation}\nThis process provides finer granularity of control within the following model and facilitates the ability of interpolative quality control during inference, enabling precise adjustments in \\( \\mathbb{R}^d \\).\n\n\\subsection{Quality-aware Masked Diffusion Transformer}\n\\label{model_architecture}\nIn a general patchify phrase with patch size \\(p_f \\times p_l\\) and overlap size \\(o_f \\times o_l\\), patchified token sequence \\(X=\\{x_1, x_2, \\dots, x_{P}\\} \\subset \\mathbb{R}^{p_f \\times p_l}\\) are obtained through spliting the music latent space \\(\\mathcal{M}_{spec} \\in \\mathbb{R}^{F \\times L}\\), as described in Section ~\\ref{sec:vae}. The total number of patches \\(P\\) is given by:\n\\begin{equation}\nP = \\left\\lceil \\frac{L - p_l}{p_l - o_l} + 1 \\right\\rceil \\times \\left\\lceil \\frac{F - p_f}{p_f - o_f} + 1 \\right\\rceil\n\\end{equation}\n\nA 2D-Rope position embedding~\\citep{su2024roformer} is added to each patch for better modeling of relative position relationship while a binary mask \\(\\boldsymbol{m} \\in \\{0, 1\\}^{P}\\) is applied during the training stage, with a variable mask ratio \\(\\gamma\\). This results in a subset of \\(\\lfloor \\gamma P \\rfloor\\) patches being masked that \\(\\sum_{i=1}^{P_N} m_i = \\lfloor \\gamma P \\rfloor\\), leaving \\(P - \\lfloor \\gamma P \\rfloor\\) patches unmasked. The subset of masked tokens is invisible in the encoder stage and replaced with trainable mask tokens in the decoder stage following the same strategy utilized in AudioMAE~\\citep{huang2022masked} and MDT~\\citep{gao2023masked}.\n\nThe transformer we use consists of \\(N\\)  encoder blocks, \\(M\\) decoder blocks, and an intermediate layer to replace the masked part with trainable parameters. We treat the embedding of the quantized p-MOS score as a prefix token, concatenated with each stage's music tokens. Let \\({X}^k = [{x}^k_1, {x}^k_2, \\ldots, {x}^k_{P}] \\in \\mathbb{R}^{P \\times d}\\) represent the output of \\(k\\)-th  encoder or decoder block, where the initial input of the encoder \\(X^0=z_t=\\alpha_t{z}_0 + \\sqrt{1-\\alpha_t}\\epsilon\\), and the final decoder block estimate  \\(X^{N+M}=z_0 = [{x}_1, {x}_2, \\ldots, {x}_{P}]\\).\nFor \\( k < N \\), indicating the encoder blocks, the sequence transformation focuses only on unmasked tokens:\n\\begin{equation}\n[q_\\text{vq}^{k+1}; {X}^{k+1}] = \\text{ Encoder}^k\\left(\\left[q_\\text{vq}; {X}^k \\odot (\\mathbf{1} - \\boldsymbol{m})\\right]\\right),\n\\end{equation}\nwhere \\(\\boldsymbol{m} \\in \\{0, 1\\}^P\\) is the mask vector, with \\(1\\) indicating masked positions and \\(0\\) for visible tokens.\n\nFor \\(N < k < N + M \\), indicating the decoder blocks, the full sequence including both unmasked tokens and learnable masked tokens is considered:\n\\begin{equation}\n[q_\\text{vq}^{k+1}; {X}^{k+1}] = \\text{Decoder}^k \\left(\\left[q_\\text{vq}; {X}^k\\right]\\right),\n\\end{equation}\nwhere the previously masked tokens are now subject to prediction and refinement.\nIn the decoding phase, the portions that were masked are gradually predicted, and throughout this entire phase, the quality token \\(q_\\text{vq}(s)\\) is progressively infused and optimized. Subsequently, the split patches are unpatchified while the overlapped area is averaged to reconstruct the output noise and every token contributes to calculating the final loss:\n\\begin{equation}\n\\small\n\\mathcal{L}(\\theta) = \\mathbb{E}_{(z_0,\nq_{vq}, y),\\epsilon} \\left[ \\left\\|\\epsilon - D_{\\theta} \\left( \\sqrt{\\alpha_t} z_0\n+ \\sqrt{1-\\alpha_t} \\epsilon, t, q_{vq}, {y} %\\mathcal{F}_{text}(y);\n \\right) \\right\\|^2 \\right]\n\\end{equation}\nIn the inference stage, the model can be guided to generate high-quality music through CFG: \n\\begin{equation}\n\\begin{aligned}\nD_{\\theta}^{\\text{High}}(z_t, t, q_{vq}^{\\text{high}}, y) &= D_{\\theta}(z_t, t, q_{vq}^{\\text{high}}, y) + \\\\\n& w\\left(D_{\\theta}(z_t, t, q_{vq}^{\\text{high}}, y) - D_{\\theta}(z_t, t, q_{vq}^{\\text{low}}, \\emptyset)\\right)\n\\end{aligned}\n\\end{equation}\nHere \\(q_{vq}^\\text{high}\\) and \\(q_{vq}^\\text{low}\\) indicate quantified p-MOS for guiding the model in a balance between generation quality and diversity.\n\\subsection{Music Caption Refinement}\n\\label{sec:audio_tokenization}\n\\looseness=-1\nWe divided the caption refinement stage into three steps including text information enriching with music caption model \\(\\mathcal{F}_\\text{cap}\\), caption alignment adjustment with CLAP cosine similarity function \\(\\mathcal{S}\\) and caption diversity extension with LLMs which we denoted as \\({\\mathcal{F}}_{\\text{llm}}\\).\n\nInitially, pretrained music caption model~\\citep{doh2023lp} is employed to re-annotate each music signal $M_i$ to $T^{g}_i$,  shown as \n$\n\\mathcal{D}_{g} = \\{(M_i, T^{g}_i) \\mid T^{g}_i = \\mathcal{F}_\\text{cap}(M_i), i = 1, 2, \\dots, N\\}\n$.\nCLAP text-audio similarity is applied to filter \\(\\mathcal{D}_{g}\\) with a threshold of \\(\\rho_1\\), resulting in \n\\begin{equation}\n\\mathcal{D}^{\\text{filter}}_{g} = \\{(M_i, T^{g}_i) \\mid \\mathcal{S}(T^{g}_i, M_i) > \\rho_1\\}\n\\end{equation}\nIn this context, we meticulously filter out generated captions that do not correspond with their respective audio files. This misalignment may be attributed to inaccuracies within the captioner's insufficient training. For the filtered data pairs, we opt to retain the use of the original captions.\n\nTo ensure that valuable information from the original captions is not overlooked when using only the generated captions, we adapt a fusing stage to combine the original caption and generated pseudo prompt. Firstly, we need to filter out original captions that is useless or inaccurate, formulated as:\n\\begin{equation}\n\\mathcal{D}^{\\text{filter}}_{o} = \\{(M_i, T^{o}_i) \\mid \\mathcal{S}(T^{o}_i, M_i) > \\rho_2\\}.\n\\end{equation}\nThe issue can stem from the original data being improperly labeled with terms such as 'speech, car' from datasets like AudioSet~\\citep{gemmeke2017audio} and also may be because of desperately missing of the original labels.\n\nFinally, only the original caption that suffers low CLAP text similarity score should be merged with the generated ones, for redundant, repetitive parts result in long and verbose final captions. Thus, we set the threshold to \\(\\rho_3\\) and merge them by LLMs to \\(T_{\\text{fusion}} = \\mathcal{F}_{\\text{llm}}(T^{o}, T^{g})\\):\n\\begin{equation}\n\\begin{aligned}\n\\mathcal{D}_{\\text{merge}} = \\big\\{ (M_i, T_{\\text{fusion}}) \\mid \\mathcal{S}(T^{o}, T^{g}) < \\rho_3, \\\\\n\\hspace*{2.5em}(M_i, T^{o}) \\in \\mathcal{D}^{\\text{filter}}_{o}, (M_i, T^{g}) \\in \\mathcal{D}^{\\text{filter}}_{g} \\big\\}.\n\\end{aligned}\n\\end{equation}\n\n\\section{Experimental Setup}\\label{sec:exp}\n\n\\subsection{Datasets}\\label{sec:datasets}\n\\paragraph{Train datasets.} We used the following databases for our training: AudioSet Music Subset (ASM)~\\citep{gemmeke2017audio}, MagnaTagTune (MTT)~\\citep{law2009evaluation}, Million Song Dataset (MSD)~\\citep{bertin2011million}, Free Music Archive (FMA)~\\citep{defferrard2016fma}, and an additional dataset\\footnote{We use 55k music tracks from https://pixabay.com, which is a large scale copyright free dataset.}. Each track in these databases was clipped to 10-second segments and sampled at 16kHz to ensure uniformity across the dataset. The final training set was developed through a process of caption refinement, as detailed in Section~\\ref{sec:audio_tokenization}. Finally, we got our training set totaling 12.5k hours of diverse music data. The specific composition of these datasets is further elaborated in the Appendix.\n\n\\paragraph{Evaluation datasets.} For comparison with prior work, we evaluate our model on the widely used MusicCaps benchmark~\\citep{agostinelli2023musiclm} and the Song-Describer-Dataset~\\citep{manco2023thesong}. MusicCaps consists of 5.5K \\(10.24\\)-second clips that include high-quality music descriptions from ten musicians, and the Song-Describer-Dataset is made up of 706 licensed high quality music recordings.\n\n\\subsection{Models and Hyperparameters}\\label{sec:hyperparams}\n\n\\paragraph{Audio compression.} \n\\label{sec:vae}\n\nEach 10.24-second audio clip, sampled at 16 kHz, is initially transformed into a \\(64 \\times 1024\\) mel-spectrogram with mel-bins of 64, hop-length of 160 and window length of 1024. Subsequently, this spectrogram is compressed into a \\(16 \\times 128\\) latent representation \\(\\mathcal{M}_{spec}\\) using a Variational Autoencoder (VAE) pretrained with AudioLDM 2~\\citep{liu2023audioldm2} with series of quantization loss and adversarial loss. This latent representation is utilized for adding noise and training purposes. Finally, we employ a pretrained Hifi-GAN~\\citep{kong2020hifi} to reconstruct the waveform from the generated mel-spectrogram.\n\n\\paragraph{Caption processing and conditioning.} \nWe utilize the LP-MusicCaps~\\citep{doh2023lp} caption model for ASM, FMA, and subsets of MTT and MSD that have weak or no captions.\nWe use the official checkpoint from LAION-CLAP~\\citep{laionclap2023}\\footnote{\\texttt{music\\_speech\\_audioset\\_epoch\\_15\\_esc\\_89.98.pt} } for text-to-text and text-to-audio similarity calculations.\nBased on small scale subjective experientment, thresholds are set at \\(\\rho_1 = \\rho_2 = 0.1 \\) to ensure any generated text or original caption not aligned well with the corresponding waveform is filtered out. Additionally, after filtering, generated text that fall below a threshold of \\(\\rho_3 = 0.25 \\) are merged with original tags with the prompt: \\textit{Merge this music caption ``generated caption'' with the ground truth tags ``original tags''. Do not add any imaginary elements and try to keep the modified caption refined and accurate}. We use FLAN-T5-large~\\citep{peebles2023scalable} as text encoder for all models.\n\n\\paragraph{Diffusion backbone.} We train our diffusion model with three backbones for comparison: U-Net~\\citep{ronneberger2015u} based at 1.0B parameters, code based on AudioLDM~\\citep{liu2023audioldm}; and our proposed Quality-aware Masked Diffusion Transformer (QA-MDT) based at 675M parameters with \\(N=20\\) encoder layers and \\( M=8 \\) decoder layers. Flash attention~\\citep{dao2022flashattention} from the xFormers package~\\citep{xFormers2022} is utilized to improve training speed and memory usage for self-attention and cross-attention stages. We study the impact of the patch size and overlap size in the Appendix\n, and apply a patch size of \\(1 \\times 4\\) without overlap for the training of our final model.\nWe train on 10-second audio crops sampled at random from the full track, maintaining a total batch size of 64 on four NVIDIA A100 GPUs, with gradient clipping of 1.0, learning rate of 8e-5, 2000 steps of linear warmup without weight decay, and a condition drop of 0.1 during training. The final model was trained for a total of seven days (38.5k steps). During inference, we use Denoising Diffusion Implicit Models (DDIM)~\\citep{song2020denoising} with 200 steps and a guidance scale of 3.5, consistent with AudioLDM~\\citep{liu2023audioldm}.\n\\label{sec:res}\nWe begin by presenting our approach to refining captioning, which includes the capability for quality awareness, transitioning from text-level control to token-level control. Finally, we compare proposed model with previous works\nsubjectively and objectively.\n\\subsection{Evaluation Metrics}\nWe evaluate the proposed method using objective metrics, including the Fr√©chet Audio Distance (FAD)~\\citep{kilgour2018fr}, Kullback-Leibler Divergence (KL), Inception Score (IS)\\footnote{All above metrics are computed using the \\texttt{audioldm\\_eval} library~\\citep{liu2023audioldm}, ensuring standardized evaluation.}.\nWe also utilize pseudo-MOS scoring model~\\citep{ragano2023audio} to estimate generation quality, with more accurate assessments derived from subjective metrics.\nFor the human studies, we invite human raters to evaluate two aspects of the audio samples (i) overall quality (\\text{Ovl}), and (ii) relevance to the text input (\\text{Rel}). \nFor the overall quality test, raters were asked to rate the perceptual quality of the provided samples with 1 to 5. For the text relevance test, raters were asked to rate the match between audio and text on a scale from 1 to 5.\nOur evaluators consist of people from various backgrounds, including professional music producers, video editors who need to use soundtracks, staff responsible for the operations of music products and complete beginners who are unfamiliar with related knowledge.\nWe evaluate randomly sampled file, where each sample was evaluated by at least 5 raters. \n\n\\section{Results}\n\n\\subsection{Quality Awareness}\n\\begin{figure*}[h]\n    \\centering\n    \\includegraphics[width=1\\textwidth]{pic_for_nips/quality3.pdf}\n    \\caption{\n\\textbf{(Left)} Five VQ-MOS distribution curves are obtained by concurrently using text quality prefixes and quality tokens as controls on the MTT-FS, with quantized MOS levels ranging from 1 to 5 serving as control constraint inferences. The distribution of the training set is normalized by each sample's duration, colored lines represent thresholds of quantized p-MOS tokens during training. \\textbf{(Right)} The effect of using quality text prefixes during training is shown, showcasing testing results on four metrics: FAD, KL, IS, and p-MOS, while gray lines for quantized p-MOS.}\n    \\label{fig:2g}\n\\end{figure*}\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{pic_for_nips/model_performance_comparison.pdf}\n    \\caption{Comparison of model performance under different quality prefixes on MTT-FS, while the blue dashed line represents the threshold set during training to distinguish the three quality prefix levels, and the red one represents the test set average p-MOS value.}\n    \\label{fig:pattern_abl}\n\\end{figure} \nThis subsection explores the effects and interactions of model control over quality tokens and quality text prefixes during the training phase, as well as their comparative effects across different models. In our previous MTT dataset of 1,000 test pairs, we filtered out pairs labeled with \\textit{low quality} or \\textit{quality is poor} to avoid confusion when applying quality prefixes, resulting in a new subset of 519 entries, which we refer to as the MTT Filter Set (MTT-FS).\nFigure~\\ref{fig:pattern_abl} illustrates the impact of different quality prefixes during inference when quality is used as a text prefix during training for U-Net and MDT-based backbones. It was observed that U-Net, when inferred with different quality prefixes, showed only minor changes in p-MOS scores and did not adhere to the threshold set during training. In contrast, MDT demonstrated better learning of quality information from prefixes, achieving p-MOS scores significantly higher than those of U-Net and the test set.\nAdditionally, by decoupling quality information from the training set, we achieved superior diversity and generalizability compared to training and inference without quality text prefixes. Given that quality tokens are specifically designed for the Transformer architecture, Figure~\\ref{fig:2g} (left) shows the controlled outcomes when different quality tokens are used after integrating quantified quality as a token during training. Remarkably, using quality tokens alone provided more precise and accurate p-MOS score control.\nIn our ablation study, we compared the effects of using only text prefixes against combining both approaches. As shown in Figure~\\ref{fig:2g} (right), as the quantized control level gradually increased, the model steadily improved in p-MOS scores, which represent the quality of generation. Concurrently, FAD and KL also progressively optimized until a turning point at level 4, where a higher average p-MOS was achieved than when solely using prefixes. This turning point may be due to the scarcity of examples with quality level 5 in the dataset. Moreover, by combining two types of quality information injection, the refined decoupling and interaction allowed the model to more accurately perceive audio data quality features during training, leading to significant reductions in FAD and KL compared to using only one of the methods.\n\nWe also compare our approach with the traditional ``negative prompt'' strategy in Appendix, highlighting our approach's significant improvement in quality and reduction in FAD.\n\\subsection{Impact of Music Caption Refinement}\n\\begin{table}[ht]\n  \\centering\n  \\setlength{\\tabcolsep}{4pt} % Ë∞ÉÊï¥ÂàóÈó¥Ë∑ù\n  \\caption{Comparison of model performance training on different textual representations, evaluated by FAD, IS and CLAP score.}\n  \\label{tab:ttm}\n  \n  \n  \\begin{tabular}{lccccccc}\n    \\toprule\n    & \\multicolumn{3}{c}{U-Net based} & \\multicolumn{3}{c}{MDT based} \\\\\n    \\cmidrule(lr){2-4} \\cmidrule(lr){5-7}\n    \\text{Caption} & \\text{Fad} \\(\\downarrow\\)& \\text{Is}\\(\\uparrow\\) & \\text{Clap} \\(\\uparrow\\) & \\text{Fad} \\(\\downarrow\\)& \\text{Is} \\(\\uparrow\\)& \\text{Clap} \\(\\uparrow\\) \\\\\n    \\midrule\n    \\(\\mathcal{D}_o\\) & 7.23 & 1.74 & 0.199 & 7.07 & 2.12 & 0.291 \\\\\n    \\(\\mathcal{D}_g\\) & 5.94 & 2.28 & 0.278 & 5.76 & 2.51 & 0.342 \\\\\n    \\(\\mathcal{D}_\\text{merge}\\) & \\textbf{5.87} & \\textbf{2.29} & \\textbf{0.284} & \\textbf{5.64} & \\textbf{2.63} & \\textbf{0.350} \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\\begin{table*}[ht]\n  \\centering\n  \\caption{Objective evaluation results for music generation with diffusion based models and language model based models. We re-infer AudioLDM 2 and MusicGen with label ‚Ä†.}\n  \\label{tab:main-results}\n  \n  \\vspace{1em} % Adding a line space between caption and table content\n  \n  \\scalebox{1}{ \n  \\begin{tabular}{lcc|cccc|cccc}\n    \\toprule\n    & \\multicolumn{2}{c}{\\text{Details}} & \\multicolumn{4}{c}{\\text{MusicCaps}} & \\multicolumn{4}{c}{\\text{Song Describer Dataset}} \\\\\n    \\cmidrule(lr){2-3} \\cmidrule(lr){4-7} \\cmidrule(lr){8-11}\n    \\text{Model} & \\text{Params} & \\text{Hours} & \\text{Fad} \\(\\downarrow\\) & \\text{Kl} \\(\\downarrow\\) & \\text{Is} \\(\\uparrow\\) & \\text{Clap} \\(\\uparrow\\) & \\text{Fad} \\(\\downarrow\\) & \\text{Kl} \\(\\downarrow\\) & \\text{Is} \\(\\uparrow\\) & \\text{Clap} \\(\\uparrow\\) \\\\\n    \\midrule\n    MusicLM & 1290M & 280k & 4.00 & - & - & - & - & - & - & - \\\\\n    MusicGen ‚Ä† & 1.5B & 20k & 3.80 & 1.22 & - & 0.31 & 5.38 & 1.01 & 1.92 & 0.18 \\\\\n    \\midrule\n    Mousai & 1042M & 2.5k & 7.50 & 1.59 & - & 0.23 & - & - & - & - \\\\\n    Jen-1 & 746M & 5.0k & 2.0 & 1.29 & - & 0.33 & - & - & - & - \\\\\n    AudioLDM 2 - Full & 712M & 17.9k & 3.13 & \\textbf{1.20} & - & - & - & - & - & - \\\\\n    AudioLDM 2 - Music ‚Ä† & 712M & 10.8k & 4.04 & 1.46 & 2.67 & 0.34 & 2.77 & 0.84 & 1.91 & 0.28 \\\\\n    \\midrule\n    Ours (U-Net) & 1.0B & 12.5k & 2.03 & 1.51 & 2.41 & 0.33 & \\textbf{1.01} & \\textbf{0.83} & 1.92 & 0.30 \\\\\n    Ours (QA-MDT) & \\textbf{675M} & 12.5k & \\textbf{1.65} & 1.31 & \\textbf{2.80} & \\textbf{0.35} & 1.04 & \\textbf{0.83} & \\textbf{1.94} & \\textbf{0.32} \\\\\n    \\bottomrule\n  \\end{tabular}\n}\n\\end{table*}\n\\begin{table}[ht]\n  \\centering\n  \\caption{Evaluation of model performances among different groups, rated for text relevance (\\text{Rel}) and overall quality (\\text{Ovl}), with higher scores indicating better performance. The groups included Production Operators (\\text{Po}), Professional Music Producers (\\text{Pmp}), Video Editors (\\text{Ve}) and Beginners(Bg)}\n  \\label{tab:zhuguan}\n  \\vspace{1em} % Adding a line space between caption and table content\n  \n  \\fontsize{9}{11}\\selectfont \n  \\setlength{\\tabcolsep}{2pt} \n  \n  \\begin{tabular}{lcccccccc}\n    \\toprule\n    & \\multicolumn{2}{c}{\\text{Po}} & \\multicolumn{2}{c}{\\text{Pmp}} & \\multicolumn{2}{c}{\\text{Ve}} & \\multicolumn{2}{c}{\\text{Bg}} \\\\\n    \\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-7} \\cmidrule(lr){8-9}\n    \\text{Model} & \\text{Ovl} & \\text{Rel} & \\text{Ovl} & \\text{Rel} & \\text{Ovl} & \\text{Rel} & \\text{Ovl} & \\text{Rel} \\\\\n    \\midrule\n    Ground Truth & 4.00 & 4.00 & 4.47 & 3.60 & 4.10 & 3.80 & 3.87 & 3.87 \\\\\n    \\midrule % Added horizontal line\n    AudioLDM 2 & 2.03 & 2.42 & 3.03 & 3.61 & 3.21 & 3.71 & 3.85 & 3.85 \\\\\n    MusicGen & 2.83 & 3.54 & 2.63 & 2.92 & 3.41 & 3.00 & \\textbf{4.33} & 3.83 \\\\\n    Ours(U-Net) & 2.80 & 3.34 & 3.46 & 4.08 & 3.40 & \\textbf{3.96} & 3.88 & 3.96 \\\\\n    Ours(QA-MDT) & \\textbf{3.27} & \\textbf{3.77} & \\textbf{3.69} & \\textbf{4.19} & \\textbf{3.54} & 3.94 & 4.23 & \\textbf{4.00} \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\nWe conducted our ablation study on a subset of our training set, which includes ASM and FMA, totaling approximately 3,700 hours and 1.1 million clips. For evaluation, we utilized an out-of-domain set with 1,000 samples randomly selected from MTT~\\citep{law2009evaluation}.\nTable ~\\ref{tab:ttm} compares the model's performance using different textual representations: sentences formed by merging original tags with commas (\\(\\mathcal{D}_o\\)), generated captions (\\(\\mathcal{D}_g\\)), and generated captions refined through filtering and fusion (\\(\\mathcal{D}_\\text{merge}\\)). During the filtering and fusion stage, 8.9\\% of the generated captions were filtered out, and 15.1\\% were fused with original tags using ChatGPT. Each model underwent training for 60,000 steps with a batch size of 64.\n\n  \n  \nFrom Table~\\ref{tab:ttm} we can also observe consistent trends: employing a captioner to transform audio annotations from sparse words into detailed sentences significantly improved the models‚Äô generalization and diversity. This indicates that detailed annotations are essential for learning the relationship between the models and spectral features. Moreover, the filter and fusion stages led to enhancements across all metrics, highlighting the significance of precise, comprehensive annotations for generalization ability and control ability. We also found that compared to U-Net, the MDT architecture shows stable improvements in basic modeling metrics, making it a better backbone for music spectral modeling.\n\n\\subsection{Compared with Previous Methods}\nWe compared our proposed method with the following representative previous methods: AudioLDM 2~\\citep{liu2023audioldm2}, Mousai~\\citep{schneider2023mo} and Jen-1~\\citep{li2023jen} which model music using spectral latent spaces, MusicLM~\\citep{agostinelli2023musiclm}, and MusicGen~\\cite{copet2024simple}, which focus on modeling discrete representations.\n\nWe re-inferred AudioLDM2-Music and MusicGen-1.5B using their official checkpoints to compare additional metrics under the same environment. The results are presented in Table~\\ref{tab:main-results}. For Ours (U-Net), we inferred all text with the prefix ``high quality'', while for Ours (QA-MDT), we used the same prefix along with a p-MOS quality token set to level 5. When calculating the CLAP score, we evaluated the generated music with original prompt, which did not include any quality prefix.\nThe experimental results show significant advantages in both subjective and objective metrics for our models. Since KL divergence measures the distance between audio samples, higher quality audio often results in deviations from the original waveform of Musiccaps, which can lead to lower performance. Although Ours (U-Net) showed a slight FAD advantage on the Song-Describer-Dataset, this may be due to instabilities arising from the small dataset, and we further demonstrated the superiority of QA-MDT in subsequent subjective experiments. Additionally, since MusicGen was trained on non-vocal tracks, it may underperform on captions that include vocals.\n\nBased on subjective evaluation shown in Table~\\ref{tab:zhuguan}, our proposed method significantly improves overall audio quality and text alignment, thanks to the label optimization for large music datasets and the quality-aware training strategy. By analyzing the backgrounds of the evaluators and their corresponding results, we can also see that for beginners, the comparison between different systems is not sensitive, which is related to their lack of music background experience and knowledge. However, from the perspective of our method in product operators, video editors, and audio producers, our method offers considerable enhancements, underscoring its potential value to audio industry professionals.\n\\section{Conclusion and Discussion}\\label{sec:dis}\n\nIn this study, we identify the challenges posed by large-scale uneven audio quality and unaligned textual annotations in the music generation domain, which hinder the development of diffusion-based TTM generation. By employing a novel quality awareness learning approach based on p-MOS, along with masked diffusion transformer as the backbone for the diffusion process, we achieve enhanced generation quality and musicality for music generation.\n\n\\clearpage\n\n\\appendix\n\\counterwithin{figure}{section}\n\\counterwithin{table}{section}\n\n\\section{Appendix}\n\n\\subsection{Training Dataset}\n\\label{sec:databaseaaa}\n\\begin{table}[h]\n  \\centering\n  \\caption{Database statistics. It should be noted that MSD is basically a package of commercial music, which is not a copyright-free content.}\n  \\vspace{1em} % Add vertical space between the caption and the table\n  \n  \\fontsize{9}{11}\\selectfont % Adjust the table font size\n  \\setlength{\\tabcolsep}{2pt} % Adjust the column spacing\n  \n  \\begin{tabular}{lccccc}\n    \\toprule\n    \\text{Database} & \\text{Duration (h)} & \\text{Clip Num} & \\text{Sample Rate} & \\text{Released} \\\\\n    \\midrule\n    MTT & 200 & 24K & - & 2009 \\\\\n    FMA & 900 & 10.9K & 44.1kHz & 2016 \\\\\n    MSD & 7333 & 880K & 32kHz & 2011 \\\\\n    ASM & 2777 & 1.0M & mix & 2017 \\\\\n    Pixabay & 1375 & 55K & 44.1kHz & - \\\\\n    \\midrule % Added horizontal line\n    Final & 12.5K & - & 16kHz  & - \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n  \n\n\\subsection{Patchify\\:Strategy}\nIn image generation, for latent spaces with consistent aspect ratios, the patchify approach indicates that smaller square patch sizes (\\(2 \\times 2\\) and \\(1 \\times 1\\)) achieve the best results. However, for our audio latent space, which has a size of \\(16 \\times 256\\) with significant aspect ratio imbalance, the traditional square patch pattern may not be the best choice. Therefore, we chose \\(2 \\times 4\\) as the fundamental patch size and conducted comparisons from three aspects: the decoder layer, patchify strategy, and in contrast to the traditional DiT.\n\nDue to the potential inaccuracies inherent in tags, we evaluate our model on the \\textit{Generated Captions without Filter and Fusion} to assess its robustness against possibly inaccurate labels training with ASM and FMA, and test on MTT Test Set.\nIn this section, we aim to explore the fundamental impact of different basic modeling units on spectral. From the experimental comparison of patch sizes \\(2 \\times 4\\), \\(1 \\times 4\\), and \\(2 \\times 2\\) shown in Table \\ref{tab:patchsize}, it is evident that reducing the patch size consistently leads to performance improvements due to the more detailed spectral modeling. Considering the inherent spatial correlations in the Fourier transform within music spectrum, we analyze the results of experiments that apply spectral overlaps of 2 and 1 in the time and frequency domains separately, which indicates that introducing overlap in the latent space does indeed contribute to improved results. \n\\begin{table}[ht]\n\\centering\n\\caption{Performance comparison between different settings of patchify strategies on MTT Test Set.}\n\\label{tab:patchsize}\n\\begin{tabular}{lccccc}\n\\toprule\n\\text{Model} & \\text{Patch Size} & \\text{Overlap Size} & \\text{FAD} \\(\\downarrow\\) & \\text{KL} \\(\\downarrow\\) & \\\\\n\\midrule\nDiT & \\(2 \\times 4\\) & \\(0 \\times 0\\) & 6.861 & 4.355\\\\\nMDT & \\(2 \\times 4\\) & \\(0 \\times 0\\) & 5.901 & 3.913 \\\\\nMDT & \\(2 \\times 2\\) & \\(0 \\times 0\\) & 5.685 & 3.820  \\\\\nMDT & \\(1 \\times 4\\) & \\(0 \\times 0\\) & 5.768 & 3.837 \\\\\nMDT & \\(2 \\times 4\\) & \\(1 \\times 0\\) & 5.757 & 3.837 \\\\\nMDT & \\(2 \\times 4\\) & \\(0 \\times 2\\) & \\textbf{5.172} & \\textbf{3.737}  \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\nA smaller patch sizes, such as \\(2 \\times 1\\) or even the more extreme \\(1 \\times 1\\), might offer improved performance. However, due to the significantly increased training and inference costs that accompany these sizes, we refrained from conducting further detailed experiments. Additionally, we explored the impact of these settings in comparison with DiT architectures and discovered that the mask strategy, leveraging the spectral correlations in both the frequency and time domains, markedly enhances spectral modeling, which is evident both in terms of the convergence rate and the quality of the final results. However, based on subjective listening on a small test dataset, we find that although the overlap strategy can significantly improve the model‚Äôs objective indicators, it leads to a decline in both melodic generation and aesthetic aspects, which is crucial for music generation. Therefore, we choose \\(1 \\times 4\\) as the base patchify strategy for subsequent experiments and the final model.\n\n\\subsection{Comparing with Using ``Negative Prompt''}\n\\label{sec:negative}\n\nIn this section, we aim to compare and analyze the traditional ``negative prompt'' method with our approach. In the implementation of CFG, we use \n\\begin{equation}\n\\small\nD_{\\theta}^{\\text{CFG}}(x, t, y) = D_{\\theta}(x, t, y) + w \\left(D_{\\theta}(x, t, y) - D_{\\theta}(x, t, y_{neg})\\right),\n\\end{equation}\nwhere \\(y_{neg}\\) is formulated as text embedding of \"low quality.\" \n\n\\begin{table}[h]\n\\centering\n\\caption{Performance comparison between three systems on MTT-FS.}\n\\label{tab:system_comp}\n\\begin{tabular}{lcccc}\n\\toprule\n\\text{System} & \\text{FAD} \\(\\downarrow\\) & \\text{KL} \\(\\downarrow\\) & \\text{p-MOS} \\(\\uparrow\\) \\\\\n\\midrule\nMDT  & 5.757 & 3.837  & 3.796\\\\\nMDT + Negative prompt & 5.641 & 3.461 & 3.832 \\\\\nQA-MDT & \\textbf{5.200} & \\textbf{3.214} & \\textbf{4.051} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\nBased on the Table\\ref{tab:system_comp}, we found that any form of quality guidance improves the model‚Äôs generative performance. However, previous attempts to improve quality relied on the rare instances of \"low quality\" in the dataset. This necessitated the careful design of numerous negative prompts to avoid generating low-quality results. Furthermore, the text embedding of \"low quality\" might not be well disentangled during training, leading to an suboptimal results.\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{\\modelname: Synthesizing \\underline{M}usic from Imag\\underline{e} and \\underline{L}anguage Cues using \\\\ Dif\\underline{fusion} Models}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\n\\label{abstract}\n\nMusic is a universal language that can communicate emotions and feelings. It forms an essential part of the whole spectrum of creative media, ranging from movies to social media posts. Machine learning models that can synthesize music are predominantly conditioned on textual descriptions of it. Inspired by how musicians compose music not just from a movie script, but also through visualizations, we propose \\modelname, a model that can effectively use cues from a textual description and the corresponding image to synthesize music. \\modelname is a text-to-music diffusion model with a novel ``visual synapse\", which effectively infuses the semantics from the visual modality into the generated music. To facilitate research in this area, we introduce a new dataset \\ourdataset, and propose a new evaluation metric \\imagemusicmetric. Our exhaustive experimental evaluation suggests that adding visual information to the music synthesis pipeline significantly improves the quality of generated music, measured both objectively and subjectively, with a relative gain of up to \\textbf{67.98\\%} on the FAD score. We hope that our work will gather attention to this pragmatic, yet relatively under-explored research area.\n\n\\blfootnote{$^*$Equal contribution.}\n\\blfootnote{$^\\dagger$Work done during internship at Adobe Research.}\n\n\\end{abstract}\n\n\\vspace{-4.9mm}\n   \n\\section{Introduction}\n\\label{sec:intro}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.48\\textwidth]{Figures/mainfig.pdf}\n    \\caption{\n    We present \\modelname, a music diffusion model equipped with a novel ``visual synapse\", that can effectively infuse image semantics into a text-to-music diffusion model. This task indeed requires a detailed understanding of the concepts in the image. An alternate approach like using a caption generator to convert image to text space to be further used with existing text-to-music methods leads to a sub-optimal overall audio quality (OVL) score. Our approach can knit together complementary information from both modalities to synthesize high-quality music. \n    }\n    \\label{fig:teaser}\n\\end{figure}\n\nMusic is an essential tool for creative professionals and content creators. It can complement and set the mood for an accompanying still image, animation, video, or even text descriptions while creating a social media post. Finding music that matches a specific setting, can indeed be an arduous task. A conditional music generation approach, that can synthesize a music track by analyzing the visual content and the textual description can find a wide range of practical applications in various fields including social media. \n\nInspired by the progress in generative modeling of images, music generation has also garnered significant attention from the community \\cite{mousai, melody, musiclm}. \nRecently, \\citet{musiclm, musicgen} proposed conditioning in the form of melody or humming.\nWhile \\citet{ihearyourturecolors} pursue image-guided audio generation. Despite these efforts, music generation conditioned on multiple modalities like text and image, is largely uncharted.\n\nImages are more expressive \\cite{imageisworththousand} than text-only information and capture more fine-grained semantic information about various visual aspects. \nFor example, as depicted in Fig \\ref{fig:teaser}, to generate a musical track that goes well with a given image, without indeed using it, one has to make the tedious effort of producing long, descriptive captions (either generated by an image captioning model or human annotators) before employing a typical text-to-music generation model. Moreover, the model has to be supplied with critical attributes like \\textit{`tranquil'}, \\textit{`aliveness'} etc (highlighted in figure) to aptly capture the essence of the image. This poses a major bottleneck in the scalability of such systems especially for social media content creators and necessitates direct image conditioning with textual control in music generation.    \n\nMusic is indeed different from generic audio. Music contains an arrangement of elements structured to form a coherent and complete entity. These musical elements include melody, harmony, rhythm, dynamics, and form \\cite{schmidt2012basic, temperley2013statistical}. Unlike audio, music contains harmonies from different instruments forming intricate structures. Prior studies show \\cite{habibi2014music, wu2013effects, zatorre2003music, norman2019divergence, das2020measurement, fedorenko2012sensitivity} that the human brain is extremely sensitive to disharmony. As a result, the margin of error especially in producing musical pieces is low compared to generic audio tracks. This makes music generation a harder task as the model should be equipped to control the fine-grained nuances of a composition involving melody, the interplay of the instruments, and genre.  \n\nAn alternative to generating music would be to retrieve them. Retrieval-based systems \\cite{mulan, manco2022contrastive} struggle to `match' the right track for a given input prompt thereby limiting their practical applicability in open-world scenarios primarily because (a) they tend to search from a pre-existing collection of tracks and (b) finding the correct association between the input prompt and the audio track can be challenging. \nThe problem is inherently complex due to the multifaceted nature of music and the abstract associations between auditory experiences and other sensory modalities. \n\nTo overcome these shortcomings, we introduce the first music generation model that can be conditioned on image and text instruction. We observe that the features from a pre-trained text-to-image diffusion model that consumes the DDIM-inverted latent of the image can guide a text-to-audio diffusion model. Our key novelty is to facilitate this information exchange by incorporating a ``visual synapse\" to the text-to-music model, which includes a set of parameters that learn to combine the signals from both modalities. \n\n\\noindent{\\textbf{We summarise our main contributions below:}}\n\n\\textbf{(1)} We formalize a novel task of generating music that is consistent with a reference image and an associated text prompt.\n\n\\textbf{(2)} We present \\modelname, a novel diffusion model that can address this pragmatic task.  \n\n\\textbf{(3)} We introduce \\ourdataset dataset comprising \\ourdatasetsize $\\langle \\text{image}, \\text{text}, \\text{music} \\rangle$ triplets. To the best of our knowledge, this is the largest collection of these three modalities. Further, we extend the MusicCaps \\cite{musiclm} dataset by supplementing the text, and music pairs with suitable images extracted from corresponding YouTube videos or the web. \n\n\\textbf{(4)} In order to quantitatively establish the correspondence between the image-music pairs we propose a new metric \\imagemusicmetric. We demonstrate that the score follows human perception closely, through a user study.  \n\n\\textbf{(5)} Finally, our exhaustive experimental results reveal that our approach outperforms existing text-to-music generation pipelines on both subjective as well as objective evaluation with a relative gain of up to \\textbf{67.98\\%} on FAD score, thereby setting a new benchmark for multi-modal music synthesis.\n\n\\section{Related Works}\n\\label{sec:related_works}\n\n\\noindent{\\textbf{Music Generation Approaches:}}\nMusic generation has garnered significant attention for a considerable amount of time. While some approaches \\cite{yang2017midinet, muhamed2021symbolic, dong2018musegan} deploy GANs to tackle this task, \\citet{ycart2017study} introduced recurrent neural networks to model polyphonic music. \\citet{bassan} proposed an unsupervised segmentation using ensemble temporal prediction errors. Jukebox \\cite{dhariwal2020jukebox} tackles the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes, modeling those using autoregressive Transformers. Another stream of work \\cite{gan2020foley, yang2017midinet} that predicts the MIDI notes to produce music has gained popularity in this space. However, the scope of these approaches is relatively limited as they need additional decoders to produce the musical pieces from the notations. \n\nMusicLM \\cite{musiclm} generates high-fidelity music from text descriptions by casting the process of conditional music generation as a hierarchical sequence-to-sequence modeling task. \nMubert \\cite{mubert} is an API-based service that employs a Transformer backbone. The encoded prompt is used to match the music tags and the one with the highest similarity is used to query the audio generation API. \nMusicGen \\cite{musicgen} comprises a single-stage transformer LM together with efficient token interleaving patterns. This eliminates the need for hierarchical upsampling. Despite significant progress, none of these approaches utilize the semantic information of images to condition the audio generation.\n\n\\noindent{\\textbf{Diffusion Models for Music Generation:}}\nWith the prolific success of diffusion models in conditional image generation, there have been recent efforts in music generation using them. Riffusion \\cite{riffusion} base their algorithm on fine-tuning a stable diffusion model \\cite{rombach2022high} on mel-spectrograms of music pieces from a paired\nmusic-text dataset. This is one of the first text-to-music generation methods. \nMo√ªsai \\cite{mousai} is a cascading two-stage latent diffusion model that is equipped to produce long-duration high-quality stereo music. \nNoise2Music \\cite{noise2music} introduced a series of diffusion models, a generator, and a cascade model. The former generates an intermediate representation\nconditioned on text, while the later can produce audio conditioned on the intermediate representation of the text. \nMeLoDy \\cite{melody} pursues an LM-guided diffusion model by reducing the forward pass bottleneck and applies a novel dual-path diffusion mode. We find that the visual guidance that is incorporated into our approach significantly enhances the music generation quality when compared to all these approaches. We elaborate this further in \\cref{sec:main_results}.\n\n\\noindent{\\textbf{Diffusion Models for Audio Generation:}}\nDiffusion-based methods \\cite{huang2022generspeech, huang2022prodiff, huang2022fastdiff, popov2021grad, lam2022bddm, lee2021priorgrad} achieve remarkable results in speech synthesis too. FastDiff \\cite{huang2022fastdiff} deploys time-aware location-variable convolutions of diverse receptive field patterns to efficiently model long-term time dependencies with adaptive conditions. AudioLDM \\cite{audioldm} is a text-to-audio system that is built on a latent space to learn continuous audio representations from contrastive language-audio pretraining (CLAP) embeddings. \n\\citet{tango} simplifies the architecture of AudioLDM, and uses FLAN-T5 \\cite{flant5} as the text encoder.\nAnother line of work \\cite{yang2023diffsound, garcia2023vampnet} involves text-conditional discrete diffusion models to generate discrete tokens as a representation for spectrograms. However, the quality of the sound produced by such methods leaves room for improvements in terms of both subjective and objective qualities, thereby limiting their practical usability. \nIn contrast to these approaches, our method generates music samples conditioned on visual and textual signals. \n\n \n\n\\section{Synthesizing Music from Image and Text}\n\\label{sec:method}\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=\\textwidth]{Figures/MelFusion_Mainfig_Updated.pdf}\n    \\caption{Our approach \\textbf{\\modelname} generates music waveform $\\bm{w}$ conditioned on an image $\\bm{I}$ and a given textual instruction $\\bm{Y}$. Visual semantics from  $\\bm{I}$ is instilled into a text-to-music diffusion model (bottom green box) using a pre-trained and frozen text-to-image diffusion model (top blue box). The image $\\bm{I}$ is first DDIM inverted into a noisy latent $\\bm{z}^I_T$. The self-attention features from the decoder layers of the text-to-image LDM that consumes $\\bm{z}^I_T$ is infused into the cross-attention features of text-to-music LDM decoder layers, modulated by learned $\\alpha$ parameters. \n    This fusion operation that happens in the decoder (green stripes) is detailed on the right side of the figure. The music encoder projects the spectrogram representation of the music to the latent space, and the music decoder retrieves back the spectrograms. Finally, a vocoder generates the waveform $\\bm{w}$ from the spectrograms. Please refer to \\cref{sec:method} for more details. \n    }\n    \\label{fig:main-figure}\n\\end{figure*}\n\nWe propose to learn a conditional distribution $\\mathcal{M}(\\bm{w}|\\bm{I}, \\bm{Y})$, that can generate music waveforms $\\bm{w}$ from an image $\\bm{I}$ and a paired textual description $\\bm{Y}$. \nWe materialize $\\mathcal{M}$ as \\modelname, a diffusion model that can succinctly interleave the semantic cues from the image and textual modality while generating acoustically pleasing music. \n\n\\cref{fig:main-figure} provides an overview of our approach.\nOn a high level, our novel methodology consists of two sub-components: 1) an approach to extract relevant visual information from the image conditioning $\\bm{I}$ and 2) a method to induce this conditioning into the text-to-music generative model, in a parameter efficient way. We describe each of these in the subsequent subsections.\n\n\\subsection{Extracting Visual Guidance} \\label{sec:extracting_guidance}\nLatent diffusion models (LDMs) for text-to-image generation \\cite{rombach2022high} have had phenomenal success in generating high-quality images that are well-grounded in their textual conditioning. We hypothesize that the latent representations and their transformations encode rich semantic knowledge, that can guide our audio diffusion model. In our exploration, we make use of a pre-trained Stable Diffusion model \\cite{rombach2022high}. It contains a VQ-VAE \\cite{van2017neural} for encoding and decoding the image to the latent space, a text encoder, and a UNet \\cite{unet} that carries out the diffusion process on the latent. The UNet contains an encoder, a bottleneck layer, and a decoder. Each encoder and the decoder further contain a set of blocks with cross-attention layers, self-attention layers, and convolutional layers. Given any intermediate latent image feature $\\bm{f} \\in \\mathbb{R}^{(w \\times h) \\times d}$, a single self-attention \\cite{attention} operation consist of $\\bm{Q} = \\bm{W}^q \\bm{f}$, $\\bm{K} = \\bm{W}^k \\bm{f}$, $\\bm{V} = \\bm{W}^v \\bm{f}$:\n\\begin{equation}\n    \\text{Attention}(\\bm{Q}, \\bm{K}, \\bm{V}) = \\text{Softmax}\\left(\\frac{\\bm{Q} \\bm{K}^T}{\\sqrt{d_k}}\\right)\\bm{V},\n    \\label{eqn:attn}\n\\end{equation}\nwhere $d_k$ is the dimension of the query and key features. During cross-attention, the key and value matrices operate on the external text conditioning $\\bm{c} \\in \\mathbb{R}^{s \\times d_k}$: $\\bm{K} = \\bm{W}^k \\bm{c}$, $\\bm{V} = \\bm{W}^v \\bm{c}$. Here, $\\bm{W}^q, \\bm{W}^k $ and $ \\bm{W}^v$ are the attention weight matrices that transform either the image features or text conditions into the output of each block.\n\nWe want to transfer over the semantic information that is present within these attention layers corresponding to the image $\\bm{I}$ into the music LDM. For this, we first invert $\\bm{I}$ into the latent space using DDIM Inversion \\cite{song2020denoising} to get $\\bm{z}^{I}_{T}$. This will guarantee that we will be able to generate $\\bm{I}$ from $\\bm{z}^{I}_{T}$. Next, we do the reverse diffusion steps using a pre-trained text-to-image LDM starting from $\\bm{z}^{I}_{T}$ and save the \\textit{self-attention features} $\\bm{K} = \\bm{W}^k \\bm{f}$, $\\bm{V} = \\bm{W}^v \\bm{f}$, to be injected into the music LDM. The intuition behind leveraging the self-attention features is that they control the feature transformations responsible for generating the visual semantics of the image. This is mathematically evident from \\cref{eqn:attn}. \nIn the subsequent section, we elaborate on how we construct the ``synapse\" that can transfer the guidance information from $\\bm{I}$ to the music-diffusion model.\n\n\\subsection{Text-to-Music LDM with Visual Synapse}\\label{sec:visual_synapse}\nInspired by recent text-to-audio \\cite{audioldm,tango} generation approaches, our text-to-music model is also formulated as a latent diffusion model. During training, the music waveform $\\bm{w}$ is first converted to a spectrogram $\\bm{s} \\in \n\\mathbb{R}^{E \\times F}$, which is a visual representation obtained via Fourier Transformation on $\\bm{w}$. $E$ and $F$ denote the number of time slots and frequency slots respectively. Then we encode $\\bm{S}$ using Audio-VAE \\cite{audioldm} to get a latent representation $\\bm{z}_1^M \\in \\mathbb{R}^{C \\times E/r \\times F/r}$, where $C$ is the number of channels and $r$ is the compression level.\n\nThe forward diffusion process involves corrupting $\\bm{z}_1^M$ using a Markovian noise process $q$, which gradually adds noise to $\\bm{z}_1^M$ through $\\bm{z}_T^M$ over $T$ steps with the following Gaussian function:\n\n\\vspace{-0.11in}\n\n\\begin{equation}\n    q(\\bm{z}_{t}^M | \\bm{z}_{t-1}^M) = \\mathcal{N}(\\bm{z}_t^M; \\sqrt{1 - \\beta_t}\\bm{z}_{t-1}^M, \\beta_t \n\\textbf{I}),\n\\end{equation}\nwhere $\\beta_t$ is a predetermined variance schedule. This iterative sampling process can be approximated by a deterministic non-Markovian process as follows \\cite{song2020denoising}:\n\\begin{align} \nq(\\bm{z}_{t}^{M} | \\bm{z}_{1}^{M}) &=  \\mathcal{N}(\\bm{z}_{t}^{M}; \\sqrt{\\bar{\\gamma}_t}\\bm{z}_{1}^{M}, (1 - \\bar{\\gamma}_t) \\textbf{I}) \\\\ \n &=  \\sqrt{\\bar{\\gamma}_t}\\bm{z}_{1}^{M} + \\epsilon \\sqrt{(1 - \\bar{\\gamma}_t)}, \\epsilon \\sim \\mathcal{N}(\\boldsymbol{0}, \\textbf{I}) \\label{eqn:noise}\n\\end{align}\nwhere $\\gamma_t = 1 - \\beta_t$ and $\\bar{\\gamma}_t = \\prod_{r=0}^t \\gamma_r$.\n\nIn the reverse diffusion process, an LDM  $\\epsilon_\\theta(\\cdot,\\cdot,\\cdot)$ (implemented as a UNet), learns to de-noise $\\bm{z}_T^M \\sim \\mathcal{N}(\\boldsymbol{0}, \\textbf{I})$ to recover $\\bm{z}_1^M$. The architecture of the UNet is kept exactly similar to the text-to-image UNet described in \\cref{sec:extracting_guidance}. To incorporate the additional guidance from image conditioning the \\textit{cross-attention} key and value features $\\bm{K}_l^{M}$ and $\\bm{V}_l^{M}$ in each of the decoder layer $l$ of the UNet is modified as follows:\n\n\\vspace{-0.18in}\n\n\\begin{align} \n\\bm{K}^M_l &=  \\alpha_l \\bm{K}^I_l + (1 - \\alpha_l) \\bm{K}^M_l \\\\ \n\\bm{V}^M_l &=  \\alpha_l \\bm{V}^I_l + (1 - \\alpha_l) \\bm{V}^M_l\\label{eqn:synapse},\n\\end{align}\nwhere $\\bm{K}^I_l$ and $\\bm{V}^I_l$ are the \\textit{self-attention} features for the corresponding layer $l$ of the image conditioning LDM from \\cref{sec:extracting_guidance}. Most importantly, the convex combination between these features is modulated by \\textit{learned layer specific $\\alpha$ parameters}. We find that this simple formulation elegantly incorporates the image guidance into the text-to-music diffusion model without hampering its expressivity. As the $\\alpha$ parameters facilitate the information exchange between the text-to-audio and text-to-image diffusion models, analogous to how a synapse in a nervous system facilitates the transfer of electrical and chemical signals between neurons, we refer to this handshake as the \\textit{visual synapse of a text-to-music LDM}. \n\nFinally, the parameters of the LDM $\\theta$ and the $\\alpha$ parameters are trained end-to-end with the following loss function: \n\n\\vspace{-0.17in}\n\n\\begin{equation}\n    \\mathcal{L} = \\mathbb{E}_{t\\sim[1,T], \\bm{z}_1^M, \\bm{\\epsilon}^M_t \\sim \\mathcal{N}(\\boldsymbol{0}, \\textbf{I})}\\lVert \\bm{\\epsilon}^M_t - \\epsilon_\\theta(\\bm{z}_t^M, \\bm{c}, t) \\rVert^2\n    \\label{eqn:loss}\n\\end{equation} \n\n\\subsection{Overall Framework}\n\n\\begin{algorithm}[!t]\n\\small\n\\caption{\\modelname: Training}\n\\label{algo:training}\n\\begin{algorithmic}[1]\n\\Require{Image: $\\bm{I}$; Text: $\\bm{Y}$; Music: $\\bm{M}$; \nPre-trained Text-to-Image LDM: $\\epsilon_\\psi(\\cdot,\\cdot,\\cdot)$; Image Encoder: $\\mathcal{E}^I(\\cdot)$; Music Encoder: $\\mathcal{E}^M(\\cdot)$; Text Encoder: $\\mathcal{T}^M(\\cdot)$; Text-to-Music LDM: $\\epsilon_\\theta(\\cdot,\\cdot,\\cdot)$; Number of Diffusion Steps: $T$.\n}\n\\Ensure{Trained Text-to-Music LDM: $\\epsilon_\\theta(\\cdot,\\cdot,\\cdot)$, Learned mixing coefficient $\\alpha$, for each decoder layer $l$ of LDM: $\\{\\alpha_l\\}$.}\n\\State $\\bm{z}^{I}_{T} \\leftarrow \\text{DDIM\\_Invert}(\\mathcal{E}^I(\\bm{I}))$ \\Comment{\\textit{Initialize Image Latent.}}\n\\State $\\{\\bm{\\epsilon}_1^M, \\cdots \\bm{\\epsilon}_T^M\\} \\leftarrow \\text{Forward\\_Diffusion}(\\mathcal{E}^M(\\bm{M}))$ \\Comment{\\textit{Targets.}}\n\\State $\\bm{z}^{M}_{T} \\sim \\mathcal{N}(\\bm{0}, \\textbf{I})$ \\Comment{\\textit{Initialize Music Latent.}}\n\\State $\\bm{c} \\leftarrow \\mathcal{T}^M(\\bm{Y})$ \\Comment{\\textit{Encoding Text.}}\n\\For{$t \\in \\{T, \\cdots, 1\\}$} \\Comment{\\textit{For each denoising step.}}\n\\For{each layer $l$ in decoder of LDM} \n\\State $\\bm{K}^I_l$, $\\bm{V}^I_l \\leftarrow$ Self-attention features of $\\epsilon_\\psi(\\bm{z}^{I}_{t},\\emptyset, t)$.\n\\State $\\bm{K}^M_l,\\bm{V}^M_l\\leftarrow$Cross-attention features of $\\epsilon_\\theta(\\bm{z}^{M}_{t},\\bm{c},t)$.\n\\State $\\bm{K}^M_l \\leftarrow \\alpha_l \\bm{K}^I_l + (1 - \\alpha_l) \\bm{K}^M_l$  \\Comment{\\textit{Key update.}}\n\\State $\\bm{V}^M_l \\leftarrow \\alpha_l \\bm{V}^I_l + (1 - \\alpha_l) \\bm{V}^M_l$  \\Comment{\\textit{Value update.}}\n\\EndFor\n\\State $\\mathcal{L} \\leftarrow ||\\bm{\\epsilon}_t^M - \\epsilon_\\theta(\\bm{z}^{M}_{t}, \\bm{c}, t)||^2$ \\Comment{\\textit{\\cref{eqn:loss}}}\n\\State Optimize $\\theta$ and all $\\alpha$ parameters to reduce $\\mathcal{L}$.\n\\EndFor\n\\State \\Return $\\epsilon_\\theta(\\cdot,\\cdot,\\cdot)$, $\\{\\alpha_l\\}$.\n\\end{algorithmic}\n\\end{algorithm}\nWe summarize the overall flow of \\modelname~during training in \\cref{algo:training}. Our key novelty is to introduce a channel through which we can guide the text-to-music diffusion model toward the semantic concepts contained in the corresponding image conditioning. This ``synapse\" is detailed in Line 7 to Line 10. The rest of the algorithm follows the standard LDM training flow.\n\nDuring inference, we make use of the trained text-to-image and text-to-music diffusion models, along with the learned $\\alpha$ parameters. As seen in Lines 8 and 9 in \\cref{algo:sampling}, the cross-attention features of the text-to-music LDM decoder are updated to incorporate the visual conditioning in each denoising step. Once the denoising (Line 10) is complete, the latent representation is projected back into a spectrogram using the decoder of Audio VAE \\cite{audioldm}, and then the waveform is generated using HiFi-GAN vocoder \\cite{hifigan} in Lines 11 and 12 respectively.\n\n\\begin{algorithm}[!t]\n\\small\n\\caption{\\modelname: Sampling}\n\\label{algo:sampling}\n\\begin{algorithmic}[1]\n\\Require{Image: $\\bm{I}$; Text: $\\bm{Y}$;\nPre-trained Text-to-Image LDM: $\\epsilon_\\psi(\\cdot,\\cdot,\\cdot)$; Image Encoder: $\\mathcal{E}^I(\\cdot)$; Text Encoder: $\\mathcal{T}^M(\\cdot)$; Trained Text-to-Music LDM: $\\epsilon_\\theta(\\cdot,\\cdot,\\cdot)$; Learned mixing coefficient $\\alpha$, for each decoder layer $l$ of LDM: $\\{\\alpha_l\\}$; Number of Diffusion Steps: $T$; Music Decoder: $\\mathcal{D}^M(\\cdot)$; Vocoder $\\mathcal{V}(\\cdot)$.\n}\n\\Ensure{Music Waveform: $\\bm{w}$}\n\\State $\\bm{z}^{I}_{T} \\leftarrow \\text{DDIM\\_Invert}(\\mathcal{E}^I(\\bm{I}))$ \\Comment{\\textit{Initialize Image Latent.}}\n\\State $\\bm{z}^{M}_{T} \\sim \\mathcal{N}(\\bm{0}, \\textbf{I})$ \\Comment{\\textit{Initialize Music Latent.}}\n\\State $\\bm{c} \\leftarrow \\mathcal{T}^M(\\bm{Y})$ \\Comment{\\textit{Encoding Text.}}\n\\For{$t \\in \\{T, \\cdots, 1\\}$} \\Comment{\\textit{For each denoising step.}}\n\\For{each layer $l$ in decoder of LDM} \n\\State $\\bm{K}^I_l$, $\\bm{V}^I_l \\leftarrow$ Self-attention features of $\\epsilon_\\psi(\\bm{z}^{I}_{t},\\emptyset, t)$.\n\\State $\\bm{K}^M_l,\\bm{V}^M_l\\leftarrow$Cross-attention features of $\\epsilon_\\theta(\\bm{z}^{M}_{t},\\bm{c},t)$.\n\\State $\\bm{K}^M_l \\leftarrow \\alpha_l \\bm{K}^I_l + (1 - \\alpha_l) \\bm{K}^M_l$  \\Comment{\\textit{Key update.}}\n\\State $\\bm{V}^M_l \\leftarrow \\alpha_l \\bm{V}^I_l + (1 - \\alpha_l) \\bm{V}^M_l$  \\Comment{\\textit{Value update.}}\n\\EndFor\n\\State $\\bm{z}^{M}_{t} \\leftarrow \\bm{z}^{M}_{t} - \\epsilon_\\theta(\\bm{z}^{M}_{t},\\bm{c},t)$ \\Comment{\\textit{Reverse Diffusion Step.}}\n\\EndFor\n\\State $\\bm{s} \\leftarrow \\mathcal{D}^M(\\bm{z}^{M}_{0})$ \\Comment{\\textit{Generate Spectrograms.}}\n\\State $\\bm{w} \\leftarrow \\mathcal{V}(\\bm{s})$ \\Comment{\\textit{Generate Waveform from Spectrograms.}}\n\\State \\Return $\\bm{w}$.\n\\end{algorithmic}\n\\end{algorithm}\n\n \n\n\\section{Experiments and Results}\n\\label{experiments_and_results}\nTo complement our newly introduced problem setting which generates music conditioned on visual and textual modality, we introduce \\underline{a new dataset}, a \\underline{new evaluation metric}, and come up with a \\underline{strong baseline} by extending a state-of-the-art text-to-music method to consume image modality. We explain each of these in the subsequent sections.\n\n\\subsection{Datasets}\n\\label{datasets}\n\nTo the best of our knowledge, there is no publicly available dataset that contains the $\\langle \\text{Image}, \\text{Text}, \\text{Music} \\rangle$ triplets that are required to train and evaluate \\modelname. We collect a new dataset \\ourdataset, which contains \\ourdatasetsize manually annotated triplets of $\\langle \\text{Image}, \\text{Text}, \\text{Music} \\rangle$. Further, we extend the MusicCaps \\cite{musiclm} dataset which contains $\\langle \\text{Text}, \\text{Music} \\rangle$ pairs by adding the corresponding image. \n\n\\noindent{\\textbf{\\ourdataset:}}\nWe hired $18$ professional annotators to find $10$-second snippets of YouTube videos corresponding to $15$ predefined genres. The annotators are trained musicians with at least 5 years of practice. For each of these videos, they were asked to provide (a) a free-form text description for up to three sentences, expressing the composition and (b) any other music-related details such as describing the genre, mood, tempo, singer voices, instrumentation, dissonances, rhythm, etc. A carefully selected frame and music from the snippet along with text description from annotators forms $\\langle \\text{Image}, \\text{Text}, \\text{Music} \\rangle$ triplets. We perform strict sanity checks to ensure the quality of these triplets in \\ourdataset. \\cref{fig:dataset_main_paper} shows some image and text samples from the dataset and \\cref{fig:dataset_pie_chart} shows the distribution of different genres in \\ourdataset.  Before annotating YouTube snippets (containing music-albums, art-performance, ensembles etc.), they were asked to check for complementary relevance between visuals and music. Further, we perform manual validation to filter lower-quality samples. We include more examples and more statistics of the dataset in the Appendix.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{Figures/Dataset_pie_chart.png}\n    \\caption{The distribution of different genres in \\ourdataset.}\n    \\label{fig:dataset_pie_chart}\n\\end{figure}\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\columnwidth]{Figures/Dataset_examples.png}\n    \\caption{Some image and text pairs from \\ourdataset. We include more examples in the Appendix.}\n    \\label{fig:dataset_main_paper}\n\\end{figure}\n\n\\noindent{\\textbf{Extended MusicCaps:}}\nMusicCaps \\cite{musiclm} is a subset of the AudioSet \\cite{gemmeke2017audio} dataset, which contains music and a corresponding textual description of the same. We carefully choose two images from the web or YouTube that can go well with each datapoint in MusicCaps, thereby extending $\\langle \\text{Text}, \\text{Music} \\rangle$ pairs to $\\langle \\text{Image}, \\text{Text}, \\text{Music} \\rangle$ triplets. We defer more details to the Appendix.\n\n\\begin{table*}\n\\centering\n\\resizebox{\\textwidth}{!}\n{\\begin{tabular}{l|cc|cccc|cc|cccc|cc}\n\\toprule\n\\multirow{3}{*}{ \\bf Model } & \\multirow{3}{0.7cm}{ \\bf Txt } & \\multirow{3}{0.7cm}{ \\bf Img} & \\multicolumn{6}{c|}{ \\bf MusicCaps } & \\multicolumn{6}{c}{ \\bf \\ourdataset } \\\\\n\\cmidrule { 4 - 15 }\n &  &  & \\multicolumn{4}{c|}{ \\bf Objective metrics } & \\multicolumn{2}{c|}{ \\bf Subjective metrics } & \\multicolumn{4}{c|}{ \\bf Objective metrics } & \\multicolumn{2}{c}{ \\bf Subjective metrics }\\\\\n& & & \\textbf{FAD}$\\downarrow$ & \\textbf{KL}$\\downarrow$ & \\textbf{FD}$\\downarrow$ & \\textbf{\\imagemusicmetric}$\\uparrow$ & \\textbf{OVL}$\\uparrow$ & \\textbf{REL}$\\uparrow$ & \\textbf{FAD}$\\downarrow$ & \\textbf{KL}$\\downarrow$ & \\textbf{FD}$\\downarrow$ & \n\\textbf{\\imagemusicmetric}$\\uparrow$ & \\textbf{OVL}$\\uparrow$ & \\textbf{REL}$\\uparrow$ \\\\\n\\midrule\nRiffusion \\cite{riffusion} & \\textcolor{ForestGreen}{\\ding{51}} & \\textcolor{OrangeRed}{\\ding{55}} & 13.40 & 1.19 & - & - & 79.48 & 75.60 & 14.06 & 1.42 &  32.64 & - & 80.11 & 76.26 \\\\\nMuBERT \\cite{mubert} & \\textcolor{ForestGreen}{\\ding{51}} & \\textcolor{OrangeRed}{\\ding{55}} & 9.60 & 1.58 & - & - & 77.59 & 77.93 & - & - & - & - & - & - \\\\\nMusicLM \\cite{musiclm} & \\textcolor{ForestGreen}{\\ding{51}} & \\textcolor{OrangeRed}{\\ding{55}} & 4.00 & 1.01 & - & - & 81.51 & 82.65 & 3.62 & 0.93 & 23.44 & - & 83.86 & 84.27 \\\\\nMo√ªsai \\cite{mousai} & \\textcolor{ForestGreen}{\\ding{51}} & \\textcolor{OrangeRed}{\\ding{55}} & 7.50 & 1.59 & - & - & 75.94 & 77.33 & 9.13 & 1.63 & 31.51 & - & 75.11 & 74.32 \\\\\nNoise2Music \\cite{huang2023noise2music} & \\textcolor{ForestGreen}{\\ding{51}} & \\textcolor{OrangeRed}{\\ding{55}} & 2.13 & - & - & - & 81.13 & 79.88 & - & - & - & - & - & - \\\\\nMeLoDy \\cite{melody} & \\textcolor{ForestGreen}{\\ding{51}} & \\textcolor{OrangeRed}{\\ding{55}} & 5.41 & - & - & - & 80.61 & 79.25 & - & - & - & - & - & - \\\\\nMusicGen \\cite{musicgen} & \\textcolor{ForestGreen}{\\ding{51}} & \\textcolor{OrangeRed}{\\ding{55}} & 3.40 & 1.23 & - & - & 83.57 & 83.18 & 3.28 & 1.21 & 23.60 & - & 84.61 & 83.25 \\\\ % [-1.5ex]\n\\midrule\nMusicLM + InstructBLIP \\cite{instructBLIP} & \\textcolor{ForestGreen}{\\ding{51}} & \\textcolor{ForestGreen}{\\ding{51}} & 4.12 & 1.18 & 25.68 & 0.55 & 80.21 & 79.85 & 3.88 & 1.07 & 24.96 & 0.63 & 81.18 & 82.42 \\\\\nTANGO++& \\textcolor{ForestGreen}{\\ding{51}} & \\textcolor{ForestGreen}{\\ding{51}} & 3.05 & 1.17 & 23.91 & 0.68 & 84.62 & 83.96 & 2.93 & 1.14 & 22.16 & 0.71 & 85.52 & 84.81 \\\\\n\\CC{}\\textbf{\\modelname\\ (Ours)} & \\CC{}\\textcolor{ForestGreen}{\\ding{51}} & \\CC{}\\textcolor{ForestGreen}{\\ding{51}} & \\CC{}\\textbf{1.12} & \\CC{}\\textbf{0.89} & \\CC{}\\textbf{22.65} & \\CC{}\\textbf{0.76} & \\CC{}\\textbf{86.78} & \\CC{}\\textbf{85.92} & \\CC{}\\textbf{1.05} & \\CC{}\\textbf{0.72} & \\CC{}\\textbf{20.49} & \\CC{}\\textbf{0.83} & \\CC{}\\textbf{88.45} & \\CC{}\\textbf{87.39} \\\\%[-1.3ex]\n\\midrule\n\\textcolor{blue}{$\\Delta_{\\text{\\modelname - MusicGen}}$} & - & - & \\colorbox{increase}{+67.05\\%} & \\colorbox{increase}{+27.64\\%} & - & - & \\colorbox{increase}{+3.84\\%} & \\colorbox{increase}{+3.29\\%} & \\colorbox{increase}{+67.98\\%} & \\colorbox{increase}{+40.49\\%} & \\colorbox{increase}{+13.17\\%} & - & \\colorbox{increase}{+4.53\\%} & \\colorbox{increase}{+4.97\\%} \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{\nOur proposed approach \\modelname offers significant gains over state-of-the-art text-to-music methods (first section), and our adapted text-and-image conditioned baselines (second section) across multiple objective and subjective metrics on two datasets. \\imagemusicmetric is applicable only when the model is conditioned on visual modality. We skip comparison with MuBERT, Noise2Music, and MeLoDy on \\ourdataset dataset as their codebases are not public. Please refer to \\cref{sec:main_results} for more details. \n}\n\\label{tab:main_table}\n\\end{table*}\n\n\\subsection{Evaluation Metrics}\n\\label{evaluation_metrics}\nWe use objective evaluation and human subjective evaluation metrics to measure the efficacy of \\modelname. \n\n\\customsubsubsection{Objective Evaluation:}\nFollowing previous works \\cite{tango, audioldm, audiogen}, Fr√©chet Audio Distance\n(FAD),  Fr√©chet Distance (FD) and KL Divergence scores are used for objective evaluation. FAD \\cite{audiogen} is a perceptual metric that is adapted from Fr√©chet Inception Distance\n(FID) for the audio domain. It uses a VGG-like backbone \\cite{hershey2017cnn} for feature extraction. FD is similar to FAD but uses PANNs \\cite{kong2020panns} as the feature extractor.\nUnlike reference-based metrics, FAD and FD measure the distance between the\ngenerated audio distribution and the real audio distribution without using any reference audio samples. On the other hand, KL Divergence \\cite{audiogen} is a reference-dependent metric that computes the divergence between\nthe distributions of the original and generated audio samples based on the labels generated by a pre-trained\nclassifier. While FAD is more related to human perception, KL Divergence captures the similarities\nbetween the original and generated audio signals based on broad concepts present in them. \n\nFAD, FD, and KL Divergence score captures the `goodness' of generated music, while it doesn't measure whether the generated music is consistent with the image conditioning. We identify this as a gap and propose \\imagemusicmetric metric.\n\n\\noindent{\\textbf{Image Music Similarity Metric (\\imagemusicmetric):}}\n\\label{cimp_metric}\nCLIP score is one of the widely used metrics for measuring the similarity between an image and a corresponding textual description. \n$N$ pairs of images and texts are passed through respective encoders (pre-trained using CLIP loss \\cite{clip}) to obtain corresponding feature embeddings which are used to compute CLIP score matrix $\\mathcal{A}_{\\text{CLIP}} \\in \\mathbb{R}^{N \\times N}$. In a very similar fashion, CLAP scores are computed amongst $N$ audio-text pairs yielding CLAP score matrix $\\mathcal{A}_{\\text{CLAP}} \\in \\mathbb{R}^{N \\times N}$ \\cite{clap}. It is worth noting that in both the matrices the columns represent text modality. This motivates us to develop a metric \\imagemusicmetric, which is a measure of the perceptual\nsimilarity between given image-music pairs bridged by the text modality. In particular, we use CLIP image and text encoders which are contrastively aligned \\cite{clip} to compute the image and text feature embeddings. As a second step, we leverage language as the bridging modality by freezing the CLIP text encoder and aligning the music (audio) encoder via contrastive training \\cite{clap}. Finally, for $\\langle \\text{Image}, \\text{Text}, \\text{Music} \\rangle$ pairs we obtain \\imagemusicmetric by suitably combining $\\mathcal{A}_{\\text{CLIP}}$ and $\\mathcal{A}_{\\text{CLAP}}$ using the given mathematical expression:\n\n\\begin{equation}\n\\mathcal{A}_{\\text{\\imagemusicmetric}} = \\mathcal{A}_{\\text{CLIP}}\\;\\mathcal{A}_{\\text{CLAP}}^{T}\n\\end{equation}\n\n\\customsubsubsection{Subjective Evaluation:}\nFollowing earlier works in text-to-audio generation \\cite{tango, audioldm, audiogen}, we use overall audio quality (OVL) and relevance to image-text inputs (REL) to analyze the results of our subjective user study involving 75 participants. They were presented with 100 randomly generated samples from \\modelname. Each of the metrics (OVL and REL) is a score between $1$-$100$ with $1$ being the lowest. For the OVL score, the users are asked to assign a score based on how perceptually realistic the generated audio is, while the REL score requires them to carefully examine the image and the text prompts before providing a rating based on their relevance with the generated music. We add more details of the user study in the Appendix.\n\n\\subsection{Baseline Methods}\n\nWe compare \\modelname against strong baselines to test its mettle. To the best of our knowledge, there doesn't exist a music diffusion model that is conditioned on visual and textual modality. Hence, we introduce two baselines: 1) caption the image with Instruct BLIP \\cite{instructBLIP} and then pass it along with the caption to MusicLM \\cite{musiclm}. We call this baseline \\textbf{MusicLM + InstructBLIP}.  2) we adapt a recent text-to-audio diffusion model TANGO \\cite{tango} into our setting, as explained next. We call its modified version TANGO++. Further, we compare ourselves with $7$ other text-to-music approaches. We elaborate on them below:\n\n\\noindent{\\textbf{TANGO++:}}\nTANGO \\cite{tango} is a powerful text-to-audio model based on LDMs. They condition the diffusion model on text embeddings from FLAN-T5 \\cite{flant5} text encoder $\\bm{z}_{text}$. To facilitate joint conditioning from text and image $\\bm{I}$, we embed $\\bm{I}$ to the latent space as $\\bm{z}_{image}$ using a ViT \\cite{vit} based CLIP encoder, and align them together through an Image-Text Contrastive loss. Once they are aligned, both the embeddings are fused and the LDM is jointly conditioned. \n\n\\noindent{\\textbf{Text-to-Music Baselines:}}\nTo bring out the utility of conditioning on both visual and textual modality, we compare \\modelname with seven other text-to-music methods too: Riffusion \\cite{riffusion}, Mubert \\cite{mubert}, MusicLM \\cite{musiclm}, Mo√ªsai \\cite{mousai}, Noise2Music \\cite{noise2music}, MeLoDy \\cite{melody} and MusicGen \\cite{musicgen}. We provide more details of each of these in the Appendix.\n\n\\subsection{Results} \\label{sec:main_results}\nWe present exhaustive objective and subjective comparison of \\modelname with the baseline approaches in \\cref{tab:main_table}.\nWhen compared with text-to-music approaches in the first section of the table, our results show the significant utility of adding extra visual conditioning on the generations. The fine-grained contextual information from visual modality is able to supplement the information from the corresponding text, thereby enhancing the quality of music generation.\nFurther, \\modelname is able to consistently outperform MusicLM + InstructBLIP and TANGO++ (which has similar conditioning as ours). This highlights the efficacy of our visual synapse, which infuses the right amount of visual conditioning to enable the model to synthesize perceptually congruent music tracks. Captions from InstructBLIP are superfluous and vague when compared to expert-annotated, high-quality MusicCaps captions on which MusicLM is trained. This distributional shift leads to a performance drop as shown in the table. TANGO++ uses contrastive loss to align CLIP image features and FLAN-T5 text features, further, we use simple addition for joint conditioning -- these design choices can be sub-optimal. \n\nOur subjective human evaluation in \\cref{tab:main_table} also suggests that conditioning the music generation on both visual and textual modality improves its perceptual quality.\n\n\\section{Discussions and Analysis}\n\n         \n\n\\customsubsection{Analyzing the Design Choice of $\\alpha$ parameters:} \n$\\alpha$ parameters introduced in \\cref{eqn:synapse} controls how the self-attention features from the blocks within the text-to-image diffusion model interact with the cross-attention features from the text-to-music diffusion model. \\modelname has one alpha parameter per block within the decoders of both diffusion models. We vary this design choice in \\cref{tab:role_of_alpha}. Attaching the synapse in the decoder offers better performance. This is because the decoder controls the major transformations that contribute to generating the image. Further, learning different $\\alpha$ per block helps to learn block-specific mixing co-efficient, which slightly improves the performance. \n\nWe also perform a sensitivity analysis on the learning rate (LR) used while learning $\\alpha$ parameters in \\cref{tab:alpha_lr}. Based on this analysis, we use a LR of $1e-5$ in our experiments.\n\n\\begin{table}\n\\centering\n\\resizebox{\\columnwidth}{!}\n{\\begin{tabular}{c c|c c c|c c c}\n\\toprule\n\\multirow{2}{*}{ \\bf Encoder } & \n\\multirow{2}{*}{ \\bf Decoder } & \\multicolumn{3}{|c|}{ \\bf Extended MusicCaps} & \\multicolumn{3}{c}{ \\bf \\ourdataset } \\\\\n\\cmidrule { 3 - 8 }\n& & \\textbf{FAD} $\\downarrow$ & \\textbf{KL} $\\downarrow$ & \\textbf{FD} $\\downarrow$ & \\textbf{FAD} $\\downarrow$ & \\textbf{KL} $\\downarrow$ & \\textbf{FD} $\\downarrow$ \\\\\n\\midrule\n\\multicolumn{8}{c}{\\textit{Same $\\alpha$ for all blocks.}}\\\\\n\\midrule\n\\textcolor{ForestGreen}{\\ding{51}} & \\textcolor{OrangeRed}{\\ding{55}} & 3.22 & 1.23 & 24.01 & 2.01 & 1.01 & 21.96 \\\\\n\\textcolor{ForestGreen}{\\ding{51}} & \\textcolor{ForestGreen}{\\ding{51}} &  2.71 & 1.13 & 23.31 & 1.27 & 0.87 & 21.04 \\\\\n\\textcolor{OrangeRed}{\\ding{55}} & \\textcolor{ForestGreen}{\\ding{51}} &  2.79 & 1.14 & 23.44 & 1.29 & 0.87 & 21.13 \\\\\n\\midrule\n\\multicolumn{8}{c}{\\textit{Different $\\alpha$ for each block.}}\\\\\n\\midrule\n\\textcolor{ForestGreen}{\\ding{51}} & \\textcolor{OrangeRed}{\\ding{55}} & 2.03 & 1.10 & 23.36 & 1.92 & 0.93 & 21.28 \\\\\n\\textcolor{ForestGreen}{\\ding{51}} & \\textcolor{ForestGreen}{\\ding{51}} &  1.13 & 0.94 & 22.81 & 1.07 & 0.76 & 20.53 \\\\\n\\CC{}\\textcolor{OrangeRed}{\\ding{55}} & \\CC{}\\textcolor{ForestGreen}{\\ding{51}} & \\CC{}\\textbf{1.12} & \\CC{}\\textbf{0.89} & \\CC{}\\textbf{22.65} & \\CC{}\\textbf{1.05} & \\CC{}\\textbf{0.72} & \\CC{}\\textbf{20.49} \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{\nWe systematically analyze our design choice of learnable $\\alpha$ parameters. We vary the position of the synapse: encoder or decoder and also study whether we need the same or different $\\alpha$ parameters for each block within them. \n}\n\\label{tab:role_of_alpha}\n\\end{table}\n\n\\begin{table}\n\\centering\n\\resizebox{0.95\\columnwidth}{!}\n{\\begin{tabular}{c|c c c|c c c}\n\\toprule\n\\multirow{2}{*}{ \\bf Learning Rate } & \\multicolumn{3}{|c|}{ \\bf Extended MusicCaps} & \\multicolumn{3}{c}{ \\bf \\ourdataset } \\\\\n\\cmidrule { 2 - 7 }\n& \\textbf{FAD} $\\downarrow$ & \\textbf{KL} $\\downarrow$ & \\textbf{FD} $\\downarrow$ & \\textbf{FAD} $\\downarrow$ & \\textbf{KL} $\\downarrow$ & \\textbf{FD} $\\downarrow$ \\\\\n\\midrule\n0.5e-6 & 3.12 & 1.21 & 23.26 & 2.01 & 1.14 & 21.95 \\\\\n0.5e-4 & 1.38 & 0.95 & 22.81 & 1.39 & 0.88 & 20.86 \\\\\n1e-6 & 2.56 & 1.17 & 23.11 & 1.86 & 1.10 & 21.71 \\\\\n\\CC{}\\textbf{1e-5} & \\CC{}\\textbf{1.12} & \\CC{}\\textbf{0.89} & \\CC{}\\textbf{22.65} & \\CC{}\\textbf{1.05} & \\CC{}\\textbf{0.72} & \\CC{}\\textbf{20.49} \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{\nSensitivity analysis on the learning rate for $\\alpha$ parameters.\n}\n\\label{tab:alpha_lr}\n\\end{table}\n\n\\customsubsection{Efficacy of Conditioning on both Modalities:}\nIn order to study the contribution of each modality on \\modelname, we train three different variations of the model by selectively turning off visual conditioning and textual conditioning. We report the results in \\cref{tab:input_conditioning}. We see significant improvement when conditioning on both modalities. \nThis highlights how complementary semantic information from each modality can compose better music.\n\n\\begin{table}\n\\centering\n\\resizebox{\\columnwidth}{!}\n{\\begin{tabular}{c c|c c c|c c c}\n\\toprule\n\\multirow{2}{*}{ \\bf Text } & \n\\multirow{2}{*}{ \\bf Image } & \\multicolumn{3}{|c|}{ \\bf Extended MusicCaps} & \\multicolumn{3}{c}{ \\bf \\ourdataset } \\\\\n\\cmidrule { 3 - 8 }\n& & \\textbf{FAD} $\\downarrow$ & \\textbf{KL} $\\downarrow$ & \\textbf{\\imagemusicmetric } $\\uparrow$ & \\textbf{FAD} $\\downarrow$ & \\textbf{KL} $\\downarrow$ & \\textbf{\\imagemusicmetric} $\\uparrow$ \\\\\n\\midrule\n\\textcolor{ForestGreen}{\\ding{51}} & \\textcolor{OrangeRed}{\\ding{55}} & 3.07 & 1.21 & - & 3.11 & 1.19 & - \\\\\n\\textcolor{OrangeRed}{\\ding{55}} & \\textcolor{ForestGreen}{\\ding{51}} & 5.62 & 1.54 & - & 4.16 & 1.37 & - \\\\\n\\CC{}\\textcolor{ForestGreen}{\\ding{51}} & \\CC{}\\textcolor{ForestGreen}{\\ding{51}} & \\CC{}\\textbf{1.12} & \\CC{}\\textbf{0.89} & \\CC{}\\textbf{0.76} & \\CC{}\\textbf{1.05} & \\CC{}\\textbf{0.72} & \\CC{}\\textbf{0.83} \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{\nConditioning independently on each of the modalities leads to inferior music generation performance in this experiment. \n}\n\\label{tab:input_conditioning}\n\\end{table}\n\n\\customsubsection{Sensitivity Analysis:}\n\\cref{tab:steps_and_guidance} reports the sensitivity analysis of changing the number of denoising steps $T$ and the strength of classifier-free guidance during inference. Similar to the findings from \\citet{tango}, increasing $T$ helps to generate more pleasing music. This can be attributed to enhanced refinement from more denoising. CFG strength of $7$ gives the best result, and we use it in our experiments. \n\n\\begin{table}\n\\centering\n\\resizebox{\\columnwidth}{!}\n{\\begin{tabular}{c|cccc|c|cccc}\n\\toprule\n\\multicolumn{5}{c|}{\\bf Varying Steps} & \\multicolumn{5}{c}{ \\bf Varying Guidance } \\\\\n\\midrule\n\\textbf{Guidance} & \n\\textbf{Steps} &\n\\textbf{FAD}$\\downarrow$ & \\textbf{KL}$\\downarrow$ & \\textbf{FD}$\\downarrow$ & \\textbf{Steps} & \\textbf{Guidance} & \\textbf{FAD}$\\downarrow$ & \\textbf{KL}$\\downarrow$ & \\textbf{FD}$\\downarrow$  \\\\\n\\midrule\n\\multirow{5}{*}{7}\n& 50 & 2.59 & 1.97 & 27.45 & \\multirow{5}{*}{400}\n& 2 & 1.47 & 1.13 & 24.64 \\\\\n& 200 & 1.35 & 1.12 & 25.22 & & 7 & \\textbf{1.12} & \\textbf{0.89} & \\textbf{22.65} \\\\\n& 400 & \\underline{1.12} & \\underline{0.89} & \\underline{22.65} & & 20 & 1.51 & 1.18 & 24.92 \\\\\n& 600 & 1.09 & 0.88 & 22.57 & & 30 & 1.63 & 1.29 & 25.38 \\\\\n& 800 & 1.07 & 0.77 & 22.48 & & 50 & 1.58 & 1.34 & 24.87 \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{\nSensitivity analysis on the number of denoising steps $T$, and the strength of classifier-free guidance.\n}\n\\label{tab:steps_and_guidance}\n\\end{table}\n\n\\begin{table}\n\\centering\n\\resizebox{\\columnwidth}{!}\n{\\begin{tabular}{c|c|c c c | c c}\n\\toprule\n\\multirow{2}{3.0 cm}{ \\centering \\bf Text prompt length (in words) } & \\multirow{2}{*}{ \\bf Image } & \\multicolumn{3}{|c|}{ \\bf Objective metrics} & \\multicolumn{2}{c}{ \\bf Subjective metrics } \\\\\n\\cmidrule { 3 - 7 }\n& & \\textbf{FAD} $\\downarrow$ & \\textbf{KL} $\\downarrow$ & \\textbf{FD} $\\downarrow$ & \\textbf{OVL} $\\uparrow$ & \\textbf{REL} $\\uparrow$ \\\\\n\\midrule\n$\\ge$ 8 $\\le$ 13 & \\textcolor{OrangeRed}{\\ding{55}} & 5.28 & 1.35 & 25.81 & 82.86 & 82.54 \\\\\n$\\ge$ 14 $\\le$ 19 & \\textcolor{OrangeRed}{\\ding{55}} & 3.13 & 1.20 & 23.11 & 85.25 & 85.16 \\\\\n$\\ge$ 20 & \\textcolor{OrangeRed}{\\ding{55}} & 3.02 & 1.19 & 22.65 & 86.04 & 85.96 \\\\\n\\CC{}\\textbf{$\\le$ 7} & \\CC{}\\textcolor{ForestGreen}{\\ding{51}} & \\CC{}\\textbf{1.86} & \\CC{}\\textbf{0.87} & \\CC{}\\textbf{21.36} & \\CC{}\\textbf{87.13} & \\CC{}\\textbf{86.21} \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{\nPerformance of \\modelname~ with varying verbosity of text prompts collected from \\ourdataset.\n}\n\\label{tab:varying text prompt}\n\\end{table}\n\n\\customsubsection{Verbose Text versus Image Conditioning:}\nThe visual synapse infuses fine-grained semantics from the image into text-to-music diffusion models. Another alternative to infuse such semantics would be to use verbose text descriptions. To study this, we remove the visual synapse from \\modelname and train a music generation model conditioned only on text. Then, we vary the length of text prompts and report results in \\cref{tab:varying text prompt}. Interestingly, we find that using image conditioning with a small text prompt outperforms using lengthier prompts. This underscores the utility of visual conditioning and the ability of visual synapses to modulate the LDM effectively.\n\n\\customsubsection{Effect of visual-cues:} We analyse the effect of using a different image and the same text prompt (please refer to the project page). When we change from the walkway image to the blue forest, the music becomes more calm and distant. We change the image to an abandoned amusement park, carnival orchestra, foggy seaside concert and forest at night. We observe a prevalence of eerie ambient sound echoing through the deserted park, occasional circus-inspired motifs, distant sounds of waves and foghorn-like effects and atmospheric strings imitating rustling leaves respectively.\n\n\\customsubsection{Comparison with Text-to-Audio Methods:} We include a comparison of \\modelname with text-to-audio generation approaches in \\cref{tab:comparison_against_text_to_audio}. We finetune their pre-trained checkpoints on our benchmark datasets for this experiment. The complementary information from both modalities allows our approach to outperform these methods too.\n\n\\begin{table}\n\\centering\n\\resizebox{\\columnwidth}{!}\n{\\begin{tabular}{c|c c c|c c c}\n\\toprule\n\\multirow{2}{*}{ \\bf Method } & \\multicolumn{3}{|c|}{ \\bf MusicCaps} & \\multicolumn{3}{c}{ \\bf \\ourdataset } \\\\\n\\cmidrule { 2 - 7 }\n& \\textbf{FAD} $\\downarrow$ & \\textbf{KL} $\\downarrow$ & \\textbf{FD} $\\downarrow$ & \\textbf{FAD} $\\downarrow$ & \\textbf{KL} $\\downarrow$ & \\textbf{FD} $\\downarrow$ \\\\\n\\midrule\nAudioLDM \\cite{audioldm} & 2.29 & 1.29 & 24.07 & 1.86 & 1.42 & 22.49 \\\\\nTANGO \\cite{tango} & 1.96 & 1.17 & 23.31 & 1.93 & 1.18 & 21.92 \\\\ \\midrule\n\\CC{}\\textbf{\\modelname} & \\CC{}\\textbf{1.12} & \\CC{}\\textbf{0.89} & \\CC{}\\textbf{22.65} & \\CC{}\\textbf{1.05} & \\CC{}\\textbf{0.72} & \\CC{}\\textbf{20.49} \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{\nWhile comparing \\modelname with state-of-the-art text-to-audio approaches, we see significant improvement in quality.\n}\n\\label{tab:comparison_against_text_to_audio}\n\\end{table}\n\n\\customsubsection{Effectiveness of \\imagemusicmetric:} We conduct a user study with $64$ participants to check whether the proposed \\imagemusicmetric metric is indeed capturing the relatedness between generated music and the conditioning image. We randomly choose $300$ samples from Extended MusicCaps and \\ourdataset each. We compute the \\imagemusicmetric score, and also ask users for their image-music similarity on a scale of [0,1], for these samples. The average score from \\imagemusicmetric metric and the users for Extended MusicCaps and \\ourdataset are ($0.76$, $0.71$) and ($0.83$, $0.85$) respectively. The high correlation underscores the validity and usefulness of \\imagemusicmetric metric.\n\n\\customsubsection{Using \\imagemusicmetric to Measure Purity of the Datasets:}\nWe compute the IMSM scores for all image-music pairs present in both Extended MusicCaps and \\ourdataset datasets and obtain a score of $0.91$ and $0.93$ respectively. The purpose of this is to quantitatively establish that the curated samples are perceptually in sync and are meaningful. The high values of the IMSM scores demonstrate that the curated image samples are highly perceptually similar and have ample association with the musical compositions.\n\n\\vspace{10pt}\n\\section{Conclusion and Future Works}\n\\label{Conclusions}\nWe explore the utility of infusing image semantics into a text-to-music diffusion model, enabling us to generate music, consistent with both visual and textual semantics in this work. To the best of our knowledge, ours is the first effort towards such a multi-conditioned music generation. We develop \\modelname with a novel ``visual synapse\" to effectively infuse the image semantics into music generation, introduce a new dataset \\ourdataset, and propose a new evaluation metric.\nWe conduct exhaustive experimental analysis on  \\ourdataset and a modified version of MusicCaps \\cite{musiclm} and compare \\modelname against $7$ text-to-music methods, and a modified baseline. The results suggest: $1$) the extra information from the image conditioning significantly boosts music generation quality $2$) our ``visual synapse\" is effective in modulating and infusing the required semantic information into the generative process.\n\n\\modelname can be an essential tool for a creative professional or a social-media content creator who needs to generate music that can go well with their multi-modal post (consider a user posting about their recent picnic -- their photos can be the image conditioning while a short description of the trip can be the textual input to \\modelname). Creating music with semantic lyrics that can go well with a video can be some interesting open-ended follow-up works.\n\n{\n    \\small\n        }\n\n\\newpage\n\n\\newpage\n\\appendix\n\n\\begin{center}\n\\begin{LARGE}\n\\textbf{\\underline{\\textcolor{blue}{Appendix}}}\n\\end{LARGE}\n\\end{center}\n\n\\noindent In this appendix we provide additional information on the following: \\\\\n\\ref{sec:more on tango++} More Details on TANGO++ \\\\\n\\ref{sec:problem motivation} Problem Motivation Revisited \\\\\n\\ref{sec: other baselines} Other Baseline Approaches\\\\\n\\ref{sec: implementation details} Implementation Details \\\\\n\\ref{sec: more experiments} More Experimental Analysis \\\\\n\\ref{sec: more dataset details} Dataset Details \\\\\n\\ref{sec: more user study} User Study Details \\\\\n\\ref{sec: conditional image gen} Inspiration from Conditional Image Generation \\\\\n\\ref{sec: related audio concepts} Related Audio Concepts \\\\\n\n\\section{More Details on TANGO++}\n\\label{sec:more on tango++}\nOur modified baseline model TANGO++ comprises an early-fusion approach, where we align the visual and the textual\nmodalities through an Image-Text Contrastive (ITC) loss. As the generated music is conditioned on both modalities, bringing them to a common latent space is imperative to the success of the system. The text input is passed through the FLAN-T5 text encoder which we keep as frozen. For image encoding\nwe use ViT \\cite{vit}. We project the visual and the textual inputs to a common embedding space and align them using ITC loss. The diffusion model is conditioned on this hybrid embedding to produce audio signals. It is then converted into spectrograms using the decoder and then passed through a HiFi GAN vocoder to produce the music signal. The expression for ITC loss ($\\mathcal{L}_{\\text{ITC}}$) is as follows:\n\n\\begin{align}\\label{eq:clip}\n\\mathcal{L}_{\\text{ITC}} = -\\frac{1}{2\\mathcal{N}}\\sum_{j = 1}^{\\mathcal{N}}\\log\\underbrace{\\left[\\frac{\\exp\\left({\\langle}z^{I}_{j}, z^{T}_{j}{\\rangle}/\\tau\\right)}{\\sum_{l = 1}^{\\mathcal{N}}{\\exp\\left({\\langle}z^{I}_{j}, z^{T}_{l}{\\rangle}/\\tau\\right)}}\\right]}_{\\text{Contrasting images with the texts}} \\nonumber \\\\ -\\frac{1}{2\\mathcal{N}}\\sum_{l = 1}^{\\mathcal{N}}\\log\\underbrace{\\left[\\frac{\\exp\\left({\\langle}z^{I}_{l}, z^{T}_{l}{\\rangle}/\\tau\\right)}{\\sum_{j = 1}^{\\mathcal{N}}{\\exp\\left({\\langle}z^{I}_{j}, z^{T}_{l}{\\rangle}/\\tau\\right)}}\\right]}_{\\text{Contrasting texts with the images}}\n\\end{align}\n\nwhere $\\langle \\cdot, \\cdot\\rangle$ denotes inner product, and $\\tau$ is the temperature parameter. $z^I$ and $z^T$ refer to the image and text latent representations respectively.\n\n\\section{Problem Motivation Revisited}\n\\label{sec:problem motivation}\n\\begin{figure}[H]\n    \\centering\n\\includegraphics[width=0.29\\textwidth]{Figures/problem-motivation.jpg}\n    \\caption{A mock-up of a  social media post that contains an image and associated textual content. Our approach \\modelname, can consume such image-textual pairs as input and synthesize music that can go well with them.}\n    \\label{fig:problem-motivation}\n\\end{figure}\n\nSocial media platforms have become ubiquitous and provide a channel for everyone to express their creativity and share their happenings with the world. It is very common for users to upload an image, and write an associated text with it (\\cref{fig:problem-motivation}). Adding music to these social media posts enhances its visibility and appeal. Instead of retrieving music from an existing database, our approach \\modelname, will be able to generate music tracks that are custom-made, conditioned on the uploaded image and its description. We note that ours is the first approach that operates in this pragmatic setting, to generate music conditioned on both visual and textual modality. \n\n\\section{Other Baseline Approaches}\n\\label{sec: other baselines}\nIn addition to our proposed baseline approach, we compare \\modelname against the following methods. Note that these are text-to-music generation methods unlike our approach and don't support multi-conditioning in input prompts. Hence a direct comparison might not be entirely fair. In most cases these methods don't support introducing an additional modality conditioning as a result we compare our approach against these baselines directly to study the benefits of \\modelname.\n\nRiffusion \\cite{riffusion} base their algorithm on fine-tuning a Stable Diffusion model \\cite{rombach2022high} on mel spectrograms of music pieces from a paired music-text dataset. This is one of the first text-to-music generation methods. Mubert \\cite{mubert} is an API-based service that employs a Transformer backbone. The encoded prompt is used to match the music tags and the one with the highest similarity is used to query the audio generation API. It operates over a relatively smaller set as it produces a combination of audio from a predefined collection. MusicLM \\cite{musiclm} generates high-fidelity music from text descriptions by casting the process of conditional music generation as a hierarchical sequence-to-sequence modeling task. They leverage the audio-embedding network of MuLan \\cite{mulan} to extract the representation of the target audio sequence. Mo√ªsai \\cite{mousai} is a cascading two-stage latent diffusion model that is equipped to produce long-duration high-quality stereo music. It achieves this by employing a specially designed U-Net facilitating a high compression rate. Noise2Music \\cite{noise2music} introduced a series of diffusion models, a generator, and a cascader model. The former generates an intermediate representation\nconditioned on text, while the later can produce audio conditioned on the intermediate representation of the text. MeLoDy \\cite{melody} pursues an LM-guided diffusion model by reducing the forward pass bottleneck and applies a novel dual-path diffusion mode. MusicGen \\cite{musicgen} comprises a single-stage transformer LM together with efficient token interleaving patterns. This eliminates the need for hierarchical upsampling.\n\n\\section{Implementation Details} \\label{sec: implementation details}\nOur text-to-music LDM contains $3$ encoder blocks and $3$ decoder blocks, similar to \\citet{tango}. Empirically we find that finetuning from its pre-trained checkpoint helps convergence. FLAN-T5 \\cite{flant5} is used as the text encoder. \\modelname is trained for $30$ epochs using AdamW optimizer \\cite{adamw}. We attach our visual synapse only on the decoder layers of the LDM. Similar to earlier works \\cite{audioldm,tango}, we find that using classifier-free guidance improves the result. Our training takes $42$ hours on $4$ NVIDIA A$100$ GPUs.\n\n\\section{More Experimental Analysis}\n\\label{sec: more experiments}\n\n\\subsection{Choice of Text-to-Image Diffusion Model}\n\n\\begin{table}[H]\n\\centering\n\\resizebox{\\columnwidth}{!}\n{\\begin{tabular}{c|c c c|c c c}\n\\toprule\n\\multirow{2}{*}{ \\bf Model } & \\multicolumn{3}{|c|}{ \\bf MusicCaps} & \\multicolumn{3}{c}{ \\bf \\ourdataset } \\\\\n\\cmidrule { 2 - 7 }\n& \\textbf{FD} $\\downarrow$ & \\textbf{KL} $\\downarrow$ & \\textbf{FAD} $\\downarrow$ & \\textbf{FD} $\\downarrow$ & \\textbf{KL} $\\downarrow$ & \\textbf{FAD} $\\downarrow$ \\\\\n\\midrule\nStable Diffusion V1.2 & 1.84 & 1.52 & 22.88 & 1.49 & 1.14 & 21.44 \\\\\nStable Diffusion V1.3 & 1.62 & 1.29 & 22.72 & 1.34 & 1.03 & 21.02 \\\\\nStable Diffusion V1.4 & 1.31 & 1.13 & 22.67 & 1.20 & 0.91 & 20.53 \\\\\n\\CC{}\\textbf{Stable DiffusionV1.5} & \\CC{}\\textbf{1.12} & \\CC{}\\textbf{0.89} & \\CC{}\\textbf{22.65} & \\CC{}\\textbf{1.05} & \\CC{}\\textbf{0.72} & \\CC{}\\textbf{20.49} \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{\\modelname with different versions of Stable Diffusion.}\n\\label{tab:sd_version}\n\\end{table}\nWe study the effect of employing different variants of the text-to-image Stable Diffusion model (V1.2 through V1.5) in \\cref{tab:sd_version}. We note that the best results are obtained with the latest variant. This brings to light that our proposed visual synapse is able to cascade the usage of better text-to-image models into improving the quality of music generation. The Stable Diffusion V1.4 and V1.5 checkpoints were initialized with the weights of the Stable Diffusion V1.2 checkpoint and subsequently fine-tuned on 225k steps at resolution $512 \\times 512$ on the LAION dataset and 10\\% dropping of the text-conditioning to improve classifier-free guidance sampling.\n\n\\subsection{Performance with Different Text Encoders}\n\n\\begin{table}[H]\n\\centering\n\\resizebox{\\columnwidth}{!}\n{\\begin{tabular}{c|c c c|c c c}\n\\toprule\n\\multirow{2}{*}{ \\bf Model } & \\multicolumn{3}{|c|}{ \\bf MusicCaps} & \\multicolumn{3}{c}{ \\bf \\ourdataset } \\\\\n\\cmidrule { 2 - 7 }\n& \\textbf{FAD} $\\downarrow$ & \\textbf{KL} $\\downarrow$ & \\textbf{FD} $\\downarrow$ & \\textbf{FAD} $\\downarrow$ & \\textbf{KL} $\\downarrow$ & \\textbf{FD} $\\downarrow$ \\\\\n\\midrule\nBERT \\cite{bert} & 2.82 & 2.23 & 24.73 & 2.91 & 1.94 & 22.13 \\\\\nRoBERTa \\cite{roberta} & 2.35 & 2.02 & 24.09 & 2.17 & 1.87 & 21.95 \\\\\nT5-Small \\cite{t5}  & 1.98 & 1.79 & 23.68 & 1.89 & 1.66 & 21.23 \\\\\nT0 \\cite{t0} & 1.42 & 1.25 & 22.96 & 1.32 & 1.19 & 20.76 \\\\\nCLIPText & 1.24 & 0.94 & 22.78 & 1.16 & 0.91 & 20.58 \\\\\n\\CC{}\\textbf{FLAN-T5 \\cite{flant5}} & \\CC{}\\textbf{1.12} & \\CC{}\\textbf{0.89} & \\CC{}\\textbf{22.65} & \\CC{}\\textbf{1.05} & \\CC{}\\textbf{0.72} & \\CC{}\\textbf{20.49} \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{Performance of \\modelname with different text encoders}\n\\label{tab:text_encoders}\n\\end{table}\nIn \\cref{tab:text_encoders} we compare the performance of \\modelname under different text encoders. We note that the best results are achieved when an instruction-tuned text encoder is employed (FLAN-T5 \\cite{flant5}) over other non-instruction-based models, which correlates with the findings in \\citet{tango}. This is very closely followed by the ClipText \\cite{clip} encoder. \n\n\\subsection{Variation Across Genres}\n\n\\begin{table}[H]\n\\centering\n\\resizebox{\\columnwidth}{!}\n{\\begin{tabular}{c|c c c c | c c}\n\\toprule\n\\multirow{2}{*}{ \\bf Genre name } & \\multicolumn{4}{|c|}{ \\bf Objective metrics} & \\multicolumn{2}{c}{ \\bf Subjective metrics } \\\\\n\\cmidrule { 2 - 7 }\n& \\textbf{FD} $\\downarrow$ & \\textbf{KL} $\\downarrow$ & \\textbf{FAD$_{\\text {VGG}}$} $\\downarrow$ & \\textbf{\\imagemusicmetric} $\\uparrow$ & \\textbf{OVL} $\\uparrow$ & \\textbf{REL} $\\uparrow$ \\\\\n\\midrule\nPop & 22.47 & 0.78 & 1.21 & 0.95 & 86.31 & 90.10 \\\\\nRock & 21.11 & 0.95 & 0.85 & 0.81 & 88.41 & 84.92 \\\\\nHip-Hop/Rap & 19.73 & 0.65 & 1.24 & 0.69 & 83.05 & 88.78 \\\\\nElectronic Dance Music & 20.03 & 1.06 & 0.93 & 0.72 & 85.39 & 86.18 \\\\\nCountry & 19.56 & 0.89 & 0.88 & 0.98 & 89.94 & 87.22 \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{A study on the diversity analysis of \\modelname. We evaluate the performance of our model on generating musical tracks of five different genres on \\ourdataset. }\n\\label{tab:genre_wise_performance}\n\\end{table}\n\nTab \\ref{tab:genre_wise_performance} reports the performance of \\modelname~ across the $5$ most popular genres (chosen through a study undertaken by \\cite{genre_popularity}) on the genre-wise test set collected from \\ourdataset. We find a steady performance of our approach across different genres substantiating the ability of the model to capture the musical nuances like the composition of the instruments, track progression, sequence of instruments introduced, rhythm, tonality, tempo, and beats. Due to the highly subjective nature of the problem, we also perform a human evaluation by subject matter experts. To this end, we employ $7$ individuals formally trained in music to independently listen and report OVL and REL scores considering the aforementioned aspects to assess the quality of genre-wise samples. We report the mean OVL and REL values from all the evaluators on a subset of the corresponding genre-wise test splits. We find that the overall performance of our method is highly encouraging as reported in Tab \\ref{tab:genre_wise_performance}.  \n\n\\subsection{Ablating choice of layers}\nWhen we fuse subset of Decoder Blocks, we see drop in performance in Tab. \\ref{tab:decoder_block_ablation_rebuttal}, as coupling becomes weak. We also ablate encoder and decoder layer separately (refer to Tab. 2 of main paper). Learned $\\alpha$ values for each blocks ($0.37$, $0.59$ and $0.63$ respectively) improves over $\\alpha\\text{=}0.5$ on all metrics, thus avoiding an extra hyper-parameter to tune. With a few layers to account for dimension mismatch, visual synapse can scale to different architectures and avoid layer-to-layer correspondence. We will explore this in a future work.\n\n\\begin{table}[H]\n\\centering\n\\resizebox{\\columnwidth}{!}\n{\\begin{tabular}{c|c c c|c c c}\n\\toprule\n\\multirow{2}{*}{ \\bf Decoder Block } & \\multicolumn{3}{|c|}{ \\bf Extended MusicCaps} & \\multicolumn{3}{c}{ \\bf \\ourdataset } \\\\\n\\cmidrule { 2 - 7 }\n& \\textbf{FAD} $\\downarrow$ & \\textbf{KL} $\\downarrow$ & \\textbf{FD} $\\downarrow$ & \\textbf{FAD} $\\downarrow$ & \\textbf{KL} $\\downarrow$ & \\textbf{FD} $\\downarrow$ \\\\\n\\midrule\n1 & 1.79 & 1.12 & 22.97 & 1.71 & 1.02 & 21.20 \\\\\n1,2 & 1.53 & 1.05 & 22.76 & 1.27 & 0.86 & 20.93 \\\\\n\\CC{}\\textbf{1,2,3} & \\CC{}\\textbf{1.12} & \\CC{}\\textbf{0.89} & \\CC{}\\textbf{22.65} & \\CC{}\\textbf{1.05} & \\CC{}\\textbf{0.72} & \\CC{}\\textbf{20.49} \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{Ablation of different decoder blocks}\n\\label{tab:decoder_block_ablation_rebuttal}\n\\end{table}\n\n\\subsection{On conditioning image} \n\n\\modelname generates music from \\underline{complementary information} from text and image modalities. While selecting images randomly, we have lower FAD/KL/FD scores of 6.38/1.73/26.45 and 8.33/1.57/28.64 on the extended MusicCaps and MeLBench datasets respectively, as it gets conditioned on random image semantics. We see similar trend in the baselines too, and \\modelname still outperforms them. Retrieving or generating image from conditioning text, will also have similar effect due to semantic similarity in both conditioning domains.\n\n\\subsection{Alternate visual conditioning} \nWe compare alternate conditioning from ViT features and ControlNet here. The semantics contained in these representations are inferior to those from text-to-image models (similar to findings in \\cite{yang2023diffusion}). Further, our visual synapse effectively adapts them by learning to modulate the representations, specific to music synthesis. Moreover compared to the generalist model (that consumes multiple modalities) in AudioLDM2 \\cite{audioldm2}, our specialist synaptic model generates better music. Also, their feature concatenation strategy is inferior to our visual synapse, as evident from Tab. \\ref{tab:comparisons_rebuttal}.\n\n\\begin{table}[H]\n\\centering\n\\resizebox{\\columnwidth}{!}\n{\\begin{tabular}{c|c c c|c c c}\n\\toprule\n\\multirow{2}{*}{ \\bf Model } & \\multicolumn{3}{|c|}{ \\bf Extended MusicCaps} & \\multicolumn{3}{c}{ \\bf \\ourdataset } \\\\\n\\cmidrule { 2 - 7 }\n& \\textbf{FAD} $\\downarrow$ & \\textbf{KL} $\\downarrow$ & \\textbf{FD} $\\downarrow$ & \\textbf{FAD} $\\downarrow$ & \\textbf{KL} $\\downarrow$ & \\textbf{FD} $\\downarrow$ \\\\\n\\midrule\nCLIP ViT Feats \\cite{clip} & 1.83 & 1.15 & 23.03 & 1.77 & 1.04 & 21.48 \\\\\nControl Net \\cite{controlnet} & 1.65 & 1.09 & 22.94 & 1.25 & 0.85 & 20.91 \\\\\nAudioLDM2 \\cite{audioldm2} & 1.77 & 1.13 & 22.96 & 1.74 & 1.02 & 21.42 \\\\\n\\CC{}\\textbf{Ours} & \\CC{}\\textbf{1.12} & \\CC{}\\textbf{0.89} & \\CC{}\\textbf{22.65} & \\CC{}\\textbf{1.05} & \\CC{}\\textbf{0.72} & \\CC{}\\textbf{20.49} \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{Comparison against different visual conditioning}\n\\label{tab:comparisons_rebuttal}\n\\end{table}\n\n\\subsection{Subjective analysis}\n\nWe complement our OVL scores with subjective descriptions, where we ask the evaluators to justify the score, stratify  them based on OVL scores, and report the most frequent reasons in Tab. \\ref{tab:subjective_rebuttal}. \n\n\\begin{table}[]\n\\small\n\\centering\n\\setlength{\\tabcolsep}{5pt}\n\\renewcommand{\\arraystretch}{0.25}\n\\resizebox{1.\\columnwidth}{!}\n{\\begin{tabular}{c | l}\n\\toprule\n\\bf OVL Range & \\bf  Reasons \\\\\n\\midrule\n\\multirow{2}{*}{0-25} & Discordant sound, unpleasant, poor quality, mismatched genre, not cohesive, \\\\\n& repetitive melody, distractive background noise, unpleasant timbre, lack of contrast.  \\\\ \\midrule\n\\multirow{2}{*}{26-50} & Unappealing instrumentation, lack of emotional resonance, unusual degree of dissonance, \\\\\n& complex narrative, unrelatable theme, abrupt transition, unbalanced sound levels. \\\\ \\midrule\n\\multirow{2}{*}{51-75} & Inconsistent mood, uninteresting chord progression, uneven transition between sections, \\\\\n& has a nostalgic appeal, cinematic quality, spirituality. \\\\ \\midrule\n\\multirow{3}{*}{76-100} & Exudes calmness,  cohesive, pleasing sequence of notes, well balanced combinations,  \\\\\n& engaging rhythmic pattern, evoke a sense of groove, nice arrangement of instruments, \\\\\n& strong sense of expression, authentic, vibrant texture, catchy, intuitive and natural flow.   \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{Subjective analysis on generated samples}\n\\label{tab:subjective_rebuttal}\n\\end{table}\n\n\\subsection{Learnable versus Fixed $\\alpha$ Parameters}\n\n\\begin{table}[H]\n\\centering\n\\resizebox{\\columnwidth}{!}\n{\\begin{tabular}{c|c c c|c c c}\n\\toprule \n\\multirow{2}{*}{ \\bf Fusion parameter $\\alpha$} & \\multicolumn{3}{|c|}{ \\bf Extended MusicCaps} & \\multicolumn{3}{c}{ \\bf \\ourdataset } \\\\\n\\cmidrule { 2 - 7 }\n& \\textbf{FAD} $\\downarrow$ & \\textbf{KL} $\\downarrow$ & \\textbf{IMSM} $\\uparrow$ & \\textbf{FAD} $\\downarrow$ & \\textbf{KL} $\\downarrow$ & \\textbf{IMSM} $\\uparrow$ \\\\\n\\midrule\n$\\alpha=0$ & 3.07 & 1.21 & - & 3.11 & 1.19 & - \\\\\n$\\alpha=0.10$ & 2.98 & 1.17 & 0.51 & 3.03 & 1.07 & 0.56 \\\\\n$\\alpha=0.50$ & 1.17 & 0.93 & 0.71 & 1.12 & 0.79 & 0.77\\\\\n$\\alpha=0.90$ & 4.96 & 1.38 & 0.85 & 4.11 & 1.29 & 0.89 \\\\\n$\\alpha=1.0$ & 5.62 & 1.54 & - & 4.16 & 1.37 & - \\\\\n\\CC{}{\\textbf{Learnable $\\alpha$}} & \\CC{}\\textbf{1.12} & \\CC{}\\textbf{0.89} & \\CC{}\\textbf{0.76} & \\CC{}\\textbf{1.05} & \\CC{}\\textbf{0.72} & \\CC{}\\textbf{0.83} \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{\nAnalyzing the effect of having fixed versus learnable $\\alpha$.\n}\n\\label{tab:fixed_vs_learnable_alpha}\n\\end{table}\nWe study the impact when $\\alpha$ is kept frozen as compared to being learnable here. The first five entries in \\cref{tab:fixed_vs_learnable_alpha} denote the cases where the value of $\\alpha$ is unaltered during training and kept constant at $0$, $0.10$, $0.50$, $0.90$, and $1.0$ respectively. Experimental results demonstrate that a learnable value of $\\alpha$ produces significantly better results as compared to the fixed counterpart, as the model has the flexibility to learn them to effectively balance between both the conditioning modalities.\n\n \n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=\\textwidth]{Figures/MelBench_dataset_word_frequency_50.png}\n    \\caption{Frequency of top 90 words from \\ourdataset}\n    \\label{fig:dataset_word_frequency}\n\\end{figure*}\n\n\\begin{table*}[!t]\n\\centering\n\\resizebox{\\textwidth}{!}\n{\\begin{tabular}{l|c}\n\\toprule\n\\textbf{Genre} & \\textbf{Subgenre}\\\\\n\\midrule\n\\multirow{2}{*}{Hip-Hop} & \\CCC{}{Alternative Hip Hop, Rap, Pop Rap, Trap, Melodic Rap, Gangster Rap, Southern Hip Hop, Urban Contemporary, Crunk, German Hip Hop, Rap Conscient, Italian Hip Hop,}\\\\\n& \\CCC{}{ East Coast Hip Hop, Hardcore Hip Hop, Atl Hip Hop, Dirty South Rap, Russian Hip Hop, Polish Trap, Underground Hip Hop, Funk Carioca, West Coast Rap, Cloud Rap}\\\\\n\\midrule\n\\multirow{2}{*}{Pop} & Dance Pop, Pov- Indie, Singer-Songwriter Pop, Mexican Pop, J-Pop, Latin Arena Pop, Indie Pop, Modern Country Pop, Art Pop, Alt Z, Indietronica,\\\\\n&    New Wave Pop, Spanish Pop, Italian Adult Pop, Electropop, Turkish Pop, Reggae Fusion, Post-Teen Pop, Hip Pop, Ccm, Indonesian Pop, Pop Nacional \\\\\n\\midrule\n\\multirow{2}{*}{Latin} & \\CCC{}{ Latin Pop, Trap Latino, Urbano Latino, Reggaeton, Musica Mexicana, Rock En Espanol, Norteno, Sierreno,R\\&B Francais, Reggaeton Colombiano, Sad Sierreno, }\\\\\n& \\CCC{}{ Mpb, Sertanejo, Tropical, Latin Alternative, Banda, Corrido, Grupera, Ranchera, Trap Brasileiro, Rap Conciencia, Urbano Espanol  }\\\\\n\\midrule\n\\multirow{2}{*}{Electronic} & Edm, Pop Dance, Uk Dance, Electronica, Electro House, House, German Dance, Tropical House, Downtempo, Brostep, \\\\\n&  Stutter House, Progressive House, Slap House, Big Room, Chill House, New French Touch, Dancefloor Dnb, Chillhop, Pop Edm, Lo-Fi Beats, Trance, Metropopolis  \\\\\n\\midrule\n\\multirow{2}{*}{R\\&B} & \\CCC{}{Soul, Indie Soul, Quiet Storm, Neo Soul, Funk, Alternative R\\&B, Disco, Pop Soul, Afrobeats, Bedroom R\\&B, Dark R\\&B,}\\\\\n& \\CCC{}{ Reggae, British Soul, Contemporary R\\&B, Hi-Nrg, Classic Soul, Uk Contemporary R\\&B, Motown, New Jack Swing, Gospel, Roots Reggae, Philly Soul} \\\\\n\\midrule\n\\multirow{2}{*}{Easy listening} &  Adult Standards, Chanson, Soundtrack, Show Tunes, Hollywood, Movie Tunes, Cartoon, Japanese Soundtrack, Broadway, Deutsch Disney, Swing, British Soundtrack,  \\\\\n&  Lounge, Preschool Children's Music, Scorecore, Romantico, Classic Girl Group, Children's Music, Electro Swing, French Soundtrack, French Movie Tunes, Classic Soundtrack    \\\\\n\\midrule\n\\multirow{2}{*}{World / traditional} & \\CCC{}{Folkmusik, Modern Bollywood, Filmi, Pop Urbaine, World, Afroswing, Dancehall, World Worship, Entehno, Sufi, Naija Worship, Classic Bollywood,  }\\\\\n& \\CCC{}{ Nouvelle Chanson Francaise, Modern Reggae, Laiko, Classic Opm, Uk Dancehall, South African Pop Dance,  Chutney, Celtic,  Manila Sound, Azontobeats   }\\\\\n\\midrule\n\\multirow{2}{*}{Jazz} &  Vocal Jazz, Bossa Nova, Dinner Jazz, Contemporary Post-Bop, Jazz Fusion, Nu Jazz, Background Jazz, Smooth Jazz, Jazz Funk, Contemporary Vocal Jazz, Jazz Piano, \\\\\n&   Jazztronica, Hard Bop, Smooth Saxophone, Cool Jazz, Nz Reggae, Soul Jazz, Torch Song, Folclore Salteno, Indie Jazz, Contemporary Jazz, Brazilian Jazz \\\\\n\\midrule\n\\multirow{2}{*}{Rock} & \\CCC{}{ Permanent Wave, Modern Rock, Classic Rock, Mellow Gold, Album Rock, Soft Rock, Pop Rock, Alternative Rock, Hard Rock,}\\\\\n& \\CCC{}{Folk Rock, New Wave, New Romantic, Indie Rock, Heartland Rock, Latin Rock, Art Rock, Blues Rock, Dance Rock, Country Rock, Alternative Dance, Pop Punk, Punk  }\\\\\n\\midrule\n\\multirow{2}{*}{Classical} &  Orchestral Soundtrack, Compositional Ambient, Classical Performance, Javanese Dangdut, Italian orchestra, Orchestral Performance, Neo-Classical, Orchestra, Classical Piano, British Orchestra, Choral, \\\\\n& Opera, Indian Classical, Hungarian Classical, Epicore, Impressionism,  Chamber Orchestra,  Historically Informed Performance, Violin, Baroque Ensemble, Symfonicky Orchestra, Japanese Guitar     \\\\\n\\midrule\n\\multirow{2}{*}{Blues} & \\CCC{}{ Electric Blues, Jazz Blues, British Blues, Modern Blues, Malian Blues, Rebel Blues, Acoustic Blues, Rhythm And Blues, Doo-Wop, Traditional Blues, Soul Blues,  Louisiana Blues, }\\\\\n& \\CCC{}{ Garage Rock Revival, Indie Quebecois, New Orleans Blues, Texas Blues, Country Blues, Australian Garage Punk, Chicago Blues, Delta Blues, Memphis Blues, Slack-Key Guitar}\\\\\n\\midrule\n\\multirow{2}{*}{Metal} &  Alternative Metal, Post-Grunge, Nu Metal, Rap Metal,\n Groove Metal,\n Power Metal,\n Melodic Metalcore,\n Metalcore,\n Skate Punk\n \n\\\\\n& Glam Metal,\n Thrash Metal,\n Speed Metal,\n Death Metal,\n Funk Metal,\n Screamo,\n Nerdcore Brasileiro,\n Industrial Metal,\n Comic Metal,\n Symphonic Metal,\n Deathcore,\n Gothic Metal,\n Progressive Metal,\\\\\n\\midrule\n\\multirow{2}{*}{Country} & \\CCC{}{Contemporary Country,\n Agronejo,\n Arrocha,\n Country Road,\n Sertanejo Universitario,\n Outlaw Country,\n Nashville Sound,\n Pop Rap Brasileiro,\n Pagode Novo,\n \n}\\\\\n& \\CCC{}{Arrochadeira,\n Forro,\n Forro De Favela,\n Funk 150 Bpm,\n Progressive Bluegrass,\n Black Americana,\n Axe,\n Bandinhas,\n Funk Ostentacao,\n Alternative Country,\n Piseiro,\n Jam Band,\n Classic Texas Country}\\\\\n\\midrule\n\\multirow{2}{*}{Folk/ acoustic} & Singer-Songwriter,\n Neo Mellow,\n Indie Folk,\n New Americana,\n Stomp And Holler,\n British Singer-Songwriter,\n Melancholia,\n Lilith,\n Turbo Folk,\n Countrygaze,\n Neo-Psychedelic,\n\\\\\n& Pop Folk,\n Turkish Folk,\n Ambient Folk,\n Modern Indie Folk,\n Rune Folk,\n Indian Folk,\n Fantasy,\n Alternative Americana,\n Ska Punk,\n Vbs,\n German Indie\n\\\\\n\\midrule\n\\multirow{2}{*}{New age} & \\CCC{}{Rain,\n Color Noise,\n Sleep,\n Sound,\n Healing Hz,\n Solfeggio Product,\n Indie Game Soundtrack,\n Ocean,\n Environmental,\n Water,\n Piano Cover,\n}\\\\\n& \\CCC{}{Acoustic Guitar Cover,\n Lullaby,\n High Vibe,\n Instrumental Worship,\n Atmosphere,\n Background Music,\n Ambient Worship,\n Binaural,\n Brain Waves,\n Background Piano,\nFourth World\n}\\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{Genre and sub-genre-wise division of the collected samples. Our dataset encompasses samples from 15 different genres each further divided into 22 sub-genres}\n\\label{tab:dataset_genre_division}\n\\end{table*}\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=\\textwidth, height=\\textheight]{Figures/Sanjoy_MMGEN_dataset_Supplementary_part_1_compressed.png}\n    \\label{fig:dataset2_1}\n\\end{figure*}\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=\\textwidth]{Figures/Sanjoy_MMGEN_dataset_Supplementary_part_2_compressed.png}\n    \\caption{Samples from \\ourdataset.}\n    \\label{fig:dataset_examples_supp}\n\\end{figure*}\n\n\\section{Dataset Details}\n\\label{sec: more dataset details}\n\n\\subsection {\\ourdataset Statistics}\n\\begin{table}[H]\n\\centering\n\\resizebox{\\columnwidth}{!}\n{\\begin{tabular}{c | c | c}\n\\toprule\n\\textbf{Type of image} & \\textbf{\\# Pieces} & \\textbf{Percentage (\\%) in Dataset}\\\\\n\\midrule\nNatural image & 3206 & 28 \\\\\nAnimation & 2404 & 21 \\\\\nPoster & 2748 & 24 \\\\\nPainting / Sketch & 3092 & 27 \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{Image categories in \\ourdataset.}\n\\label{tab:dataset}\n\\end{table}\n\\cref{tab:dataset} presents the distribution of the image samples in \\ourdataset. To maintain a fair balance across different distributions we collect samples from 4 different categories: natural images, animations, posters, paintings/sketches. This ensures that \\modelname is trained with ample examples from each of these classes and is equipped to tackle images from any of these very frequent and popular classes better. \\ourdataset comprises \\ourdatasetsize samples which is $\\sim2$x larger than the next largest dataset MusicCaps \\cite{musiclm}.\n\n\\cref{fig:dataset_word_frequency} presents the frequency of the top 90 words in \\ourdataset. The annotators were asked to write free-form text descriptions of the musical pieces with an emphasis on the musicality of the samples. We observe that the annotation contains important cues about the nature of the audio track (e.g., `live performance', `chaotic', `forceful vocals', etc). These can supplement a model with useful pieces of information regarding the aesthetics of the composition.  \n\n\\subsection {Dataset Hierarchy and Samples}\n\\cref{tab:dataset_genre_division} contains the genre and sub-genre-wise division of the samples collected in \\ourdataset. We categorise the collected musical samples into 15 broad categories with each of them having 22 sub-genres to facilitate fine-grained control over the composition through the image (theme) and text-instructions (details on musicality). The samples are divided across different genres roughly equally to maintain a good balance.\n\n\\cref{fig:dataset_examples_supp} presents one sample from each of the remaining 13 categories (Electronic and Folk Acoustic present in the main paper). As can be seen from the examples, the captions are of varied lengths and the images are from different distributions (natural images, animation, paintings, etc.).\n\n\\subsection{Extended MusicCaps Data Collection}\nMusicCaps \\cite{musiclm} is a music caption dataset comprising music clips from AudioSet \\cite{gemmeke2017audio} paired with corresponding text descriptions in English. The collection consists of a total of 5,521 examples, out of which 2,858 are from the AudioSet eval and 2,663 are from the AudioSet train split. The authors further tag 1,000 samples as a balanced subset of the dataset - equally divided across genres. All examples in the balanced subset are from the AudioSet eval split. As our setup is not restricted to text and requires joint conditioning in the form of images as well, we supplement this dataset by collecting 2 carefully chosen image frames for each of the 10-second samples from the corresponding YouTube video or web. As some of the samples are not live anymore, we were able to collect a total of \\musiccapssamples~ samples which we divided into a 60\\%/20\\%/20\\% split for train/validation/test respectively. \n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.47\\textwidth]{Figures/Sanjoy_MMGEN_dataset_Supplementary_OVL_REL.png}\n    \\caption{User study interface to collect OVL and REL scores. }\n    \\label{fig:user-study_ovl_rel}\n\\end{figure}\n\n\\begin{figure}[!t]\n    \\centering\n    \\includegraphics[width=0.47\\textwidth]{Figures/Sanjoy_MMGEN_dataset_Supplementary_user_study_3.png}\n    \\caption{User study interface for comparison against prior text-to-music methods}\n    \\label{fig:user-study-comparison}\n\\end{figure}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.47\\textwidth]{Figures/Sanjoy_MMGEN_dataset_Supplementary_IMSM.png}\n    \\caption{User study interface to obtain IMSM scores}\n    \\label{fig:user-study_imsm}\n\\end{figure}\n\n \n\n\\section{User Study Details}\n\\label{sec: more user study}\n\n\\cref{fig:user-study_ovl_rel} presents the user study interface. To obtain the OVL and REL scores, we provide the participants with an image-text pair and the audio sample generated by \\modelname. For the overall audio quality score (OVL) the participants are instructed to add their score between [1,10] while for the relevance score (REL), they are required to rate the sample based on its similarity with the input image-text pairs. \n\nIn \\cref{fig:user-study-comparison} we compare our method against prior text-to-music methods and report the OVL and REL scores in the main paper (Tab. 1). In this case, the participants were presented with only the text-music pairs.\n\n\\cref{fig:user-study_imsm} shows the user study interface for the IMSM score. For this, the participants were presented with image-music pairs and asked to provide their rating between [1,10], with 1 being the lowest. The higher the score, the more perceptually similar the participant has found the image-music pair to be.\n\n\\section{Inspiration from Conditional Image Generation}\n\\label{sec: conditional image gen}\n\nPowered by architectural improvements and the availability of large-scale, high-quality paired training data, conditional image generation methods have made considerable progress in the generative AI space. Promising results from transformer-based auto-regressive approaches \\cite{yu2022scaling, ramesh2021zero} were boosted by diffusion model-based methods \\cite{rombach2022high, saharia2022photorealistic, nichol2021glide}. These approaches have been naturally extended to generate videos from text prompts too \\cite{singer2022make, wu2023tune, ho2022imagen}. Latent diffusion models \\cite{rombach2022high} do the diffusion process in the latent space of a pre-trained VQ-VAE \\cite{van2017neural}. This significantly reduced the compute requirements when compared with image diffusion methods. \\citet{classifierfree} proposed classifier-free guidance to enhance image quality.\nText-to-music and text-to-audio methods are heavily inspired by the success of text-to-image generative methods, and so are we.\n\n\\section{Related Audio Concepts} \n\\label{sec: related audio concepts}\nThe Multimodal Variational Auto-encoders (MVAEs) are latent variable generative models to learn more generalizable representations from diverse modalities through joint distribution estimation. \\citet{arik2018neural} pioneered a neural audio synthesis model based on VAEs. Their approach demonstrated promising results in generating realistic audio samples by learning a latent representation of the audio data. Inspired by this VAEs have been widely used in the audio processing domain for speech synthesis \\cite{liu2021vara, zhang2019learning, tan2024naturalspeech}, audio generation \\cite{jiang2020transformer, tango, caillon2021rave}, and audio denoising \\cite{sadeghi2020audio, bando2020adaptive}. \n\nVocoders are used for a variety of purposes across different domains due to their ability to manipulate and synthesize audio signals efficiently. Among other prominent applications of vocoder, neural voice cloning \\cite{arik2018neural, jia2018transfer}, voice conversion \\cite{liu2018wavenet}, and speech-to-speech synthesis \\cite{jia1904direct} are very popular. GAN-based vocoders \\cite{kong2020hifi} have been employed to generate high-fidelity raw audio conditioned on mel spectrogram. More recently, WaveRNN \\cite{kalchbrenner2018efficient} has been applied for universal vocoding task \\cite{lorenzo2018towards, paul2020speaker, jiao2021universal}.\n\nSpectrograms are a powerful tool for analyzing time-varying signals such as audio and speech. They provide a visual representation of the frequency content of a signal over time, making them widely used in speech processing \\cite{michelsanti2021overview, chuang2022improved, seo2023avformer}, music analysis \\cite{melody, mousai}, and audio synthesis \\cite{adverb, makeanaudio, tango, audit, audioldm} in general. Audio spectrograms are also massively deployed in different audio visual applications \\cite{adverb, park2024can, sung2023sound}.\n\n\\vspace{0.2in}\n\\noindent{\\textbf{Acknowledgements:}} We would like to sincerely thank the data annotators and the volunteers who took part in the user study. We would also like to extend our gratitude to the anonymous reviewers for their constructive and thoughtful feedbacks.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{FLUX that Plays Music}\n\n\\begin{document}\n\n\\twocolumn[\n\\icmltitle{FLUX that Plays Music}\n\n\\icmlsetsymbol{equal}{*}\n\n\\begin{icmlauthorlist}\n\\icmlauthor{Zhengcong Fei}{}\\;\n\\icmlauthor{Mingyuan Fan}{}\\;\n\\icmlauthor{Changqian Yu}{}\\;\n\\icmlauthor{Junshi Huang}{sch} \\\\\nKunlun Inc.\n\\end{icmlauthorlist}\n\n\\icmlaffiliation{sch}{Corresponding author}\n\\icmlcorrespondingauthor{Junshi Huang}{feizhengcong@gmail.com}\n\\icmlkeywords{Machine Learning, ICML}\n\n\\vskip 0.3in\n]\n\n\\begin{abstract}\n\nThis paper explores a simple extension of diffusion-based rectified flow Transformers for text-to-music generation, termed as FluxMusic.  \nGenerally, along with design in advanced Flux\\footnote{https://github.com/black-forest-labs/flux} model, we transfers it into a latent VAE space of mel-spectrum. It involves first applying a sequence of independent attention to the double text-music stream, followed by a stacked single music stream for denoised patch prediction. We employ multiple pre-trained text encoders to sufficiently capture caption semantic information as well as inference flexibility. In between, coarse textual information, in conjunction with time step embeddings, is utilized in a modulation mechanism, while fine-grained textual details are concatenated with the music patch sequence as inputs. \nThrough an in-depth study, we demonstrate that rectified flow training with an optimized architecture significantly outperforms established diffusion methods for the text-to-music task, as evidenced by various automatic metrics and human preference evaluations. \nOur experimental data, code, and model weights are made publicly available at: \\url{https://github.com/feizc/FluxMusic}. \n\n\\end{abstract}\n\n\\section{Introduction}\n\nMusic, as a form of artistic expression, holds profound cultural importance and resonates deeply with human experiences \\cite{briot2017deep}. The task of text-to-music generation, which involves converting textual descriptions of emotions, styles, instruments, and other musical elements into audio, offers innovative tools and new avenues for multimedia creation \\cite{huang2023noise2music}. Recent advancements in generative models have led to significant progress in this area \\cite{yang2017midinet,dong2018musegan,mittal2021symbolic}. Traditionally, approaches to text-to-music generation have relied on either language models or diffusion models to represent quantized waveforms or spectral features \\cite{agostinelli2023musiclm,lam2024efficient,liu2024audioldm,evans2024stable,schneider2024mousai,fei2024music,fei2023masked,chen2024musicldm}. Among these, diffusion models~\\citep{Song2020ScoreBasedGM}, trained to reverse the process of data transformation from structured states to random noise~\\citep{SohlDickstein2015DeepUL,song2020generative}, have shown exceptional effectiveness in modeling high-dimensional perceptual data, including music~\\citep{ho2020denoising,huang2023make,ho2022video,kong2020diffwave,rombach2022high}.\n\nGiven the iterative nature of diffusion process, coupled with the significant computational costs and extended sampling times during inference, there has been a growing body of research focused on developing more efficient training strategy and accelerating sampling schedule ~\\citep{karras2023analyzing,liu2022flow,lu2022dpm,fei2019fast,lu2022dpm2,kingma2024understanding}, such as distillation \\cite{sauer2024fast,song2023consistency,song2023improved}. A particularly effective approach involves defining a forward path from data to noise, which facilitates more efficient training \\cite{ma2024sit} as well as better generative performance. One effective method among them is the Rectified Flow \\citep{liu2022flow,albergo2022building,lipman2023flow}, where data and noise are connected along a linear trajectory. It offers improved theoretical properties and has shown promising results in image generation\\cite{ma2024sit,esser2024scaling}, however, its application in music creation remains largely unexplored.\n\nIn the design of model architectures, traditional diffusion models frequently employ U-Net \\cite{ronneberger2015u} as the foundational structure. However, the inherent inductive biases of convolutional neural networks inadequately captures the spatial correlations within signals \\cite{esser2021taming} and are insensitive to scaling laws~\\cite{li2024scalability}. Transformer-based diffusion models have effectively addressed these limitations~\\cite{Peebles_2023,bao2023all,fei2024scalable,fei2024diffusion} by treating images as sequences of concatenated patches and utilizing stacked transformer blocks for noise prediction. The incorporation of cross-attention  for integrating textual information has established this approach as the standard for generating high-resolution images and videos from natural language descriptions, demonstrating impressive generalization capabilities~\\cite{chen2023pixart,chen2024pixarts,fei2024dimba,esser2024scaling,fei2023jepa,ma2024latte,ma2024latte,yang2024cogvideox}. Notably, the recently open-sourced FLUX model, with its well-designed structure, exhibits strong semantic understanding and produces high-quality images, positioning it as a promising framework for conditional generation tasks.\n\n\\begin{figure}[t]\n  \\centering\n   \\includegraphics[width=1\\linewidth]{framework.pdf}\n   \\caption{\\textbf{Model architecture of FluxMusic. }  We use frozen CLAP-L and T5-XXL as text encoders for conditioned caption feature extraction. The coarse text information concatenated with timestep embedding $y$ are used to modulation mechanism. The fine-grained text $c$ concatenated with music sequence $x$ are input to a stacked of double stream block and single steam blocks to predict nose in a latent VAE space.\n   }\n   \\label{fig:framework} \n\\end{figure}\n\nIn this work, we explore the application of rectified flow Transformers within noise-predictive diffusion for text-to-music generation, introducing FluxMusic as a unified and scalable generative framework in the latent VAE space of the mel-spectrogram, as illustrated in Figure \\ref{fig:framework}. \nBuilding upon the text-to-image FLUX model, we present a transformer-based architecture that initially integrates learnable double streams attention for the concatenated music-text sequence, facilitating a bidirectional flow of information between the modalities. Subsequently, the text stream is dropped, leaving a stacked single music stream for noised patch prediction. We leverage multiple pre-trained text encoders for extracting conditioned caption features and inference flexibility. Coarse textual information from CLAP-L~\\cite{elizalde2023clap}, combined with time step embeddings, is employed in the modulation mechanism, while fine-grained textual details from T5-XXL~\\cite{raffel2020exploring} are concatenated with music patch sequence as input. We train the model with rectified flow formulation and investigate its scalability. Through a in-depth study, we compare our new formulation to existing diffusion formulations and demonstrate its benefits for training efficiency and performance enhancement.\n\nThe primary contributions of this work are as follows: \n\\begin{itemize}\n    \\item We introduce a Flux-like Transformer architecture for text-to-music generation, equipped with rectified flow training. To the best of our knowledge, this is the first study to apply rectified flow transformers to text-to-music generation;\n    \\item  We perform a comprehensive system analysis, encompassing network design, rectified flow sampling, and parameter scaling, demonstrating the advantages of FluxMusic architecture in text-to-music generation; \n    \\item Extensive experimental results demonstrate that FluxMusic achieves generative performance on par with other recent  models with adequate training on both automatic metrics and human preference ratings. Finally, we make the results, code, and model weights publicly available to support further research.  \n\\end{itemize}\n\n\\section{Related Works}\n\n\\subsection{Text-to-music Generation}\nText-to-music generation seeks to produce music clips that correspond to descriptive or summarized text inputs. Prior approaches have primarily employed language models (LMs) or diffusion models (DMs) to generate quantized waveform representations or spectral features. For generating discrete representation of waveform, models such as MusicLM~\\cite{agostinelli2023musiclm}, MusicGen~\\cite{copet2024simple}, MeLoDy~\\cite{lam2024efficient}, and JEN-1~\\cite{li2024jen} utilize LMs and DMs on residual codebooks derived from quantization-based audio codecs~\\cite{zeghidour2021soundstream,defossez2022high}. Conversely, models like Mo√ªsai~\\cite{schneider2024mousai}, Noise2Music~\\cite{huang2023noise2music}, Riffusion~\\cite{forsgren2022riffusion}, AudioLDM 2~\\cite{liu2024audioldm}, MusicLDM~\\cite{chen2024musicldm}, and StableAudio~\\cite{evans2024stable} employ U-Net-based diffusion techniques to model mel-spectrograms or latent representations obtained through pretrained VAEs, subsequently converting them into audio waveforms using pretrained vocoders~\\cite{kong2020hifi}. Additionally, models such as Mustango~\\cite{melechovsky2023mustango} and Music Controlnet~\\cite{wu2024music} incorporate control signals or personalization~\\cite{plitsis2024investigating,fei2023gradient}, including chords and beats, in a manner similar to ControlNet \\cite{zhang2023adding}. Our method along with this approach by modeling the mel-spectrogram within a latent VAE space.\n\n\\subsection{Diffusion Transformers}\n\nTransformer architecture~\\cite{vaswani2017attention} has achieved remarkable success in language models~\\cite{radford2018improving,radford2019language,raffel2020exploring} and has also demonstrated significant potential across various computer vision tasks, including image classification~\\cite{dosovitskiy2020image,he2022masked,touvron2021training,zhou2021deepvit,yuan2021tokens,han2021transformer}, object detection~\\citep{liu2021swin,wang2021pyramid,wang2022pvt, carion2020end}, and semantic segmentation~\\citep{zheng2021rethinking,xie2021segformer,strudel2021segmenter}, among others~\\citep{sun2020transtrack,li2022panoptic,zhao2021point,liu2022video,he2022masked,li2022bevformer}. Building on this success, the diffusion Transformer~\\cite{Peebles_2023,fei2024moe} and its variants~\\cite{bao2023all,fei2024scalable} have replaced the convolutional-based U-Net backbone~\\cite{ronneberger2015u} with Transformers, resulting in greater scalability and more straightforward parameter expansion compared to U-Net diffusion models. This scalability advantage has been particularly evident in domains such as video generation~\\cite{ma2024latte}, image generation~\\cite{chen2023pixart}, and speech generation~\\cite{liu2023vit}. Notably, recent works such as Make-an-audio 2~\\cite{huang2023make,huang2023make2} and StableAudio 2~\\cite{evans2024stable} also explored the DiT architecture for audio and sound generation. In contrast, our work investigates the effectiveness of new multi-modal diffusion Transformer structure similar to Flux and optimized it with rectified flow.\n\n\\section{Methodology}\n\nFluxMusic is a conceptually simple extension of FLUX, designed to facilitate text-to-music generation within a latent space. An overview of the model structure is illustrated in Figure \\ref{fig:framework}. In the following, we begin with a review of rectified flow as applied to diffusion models, followed by a detailed examination of the architecture for each component. We also discuss considerations regarding model scaling and data quality.\n\n\\subsection{Rectified Flow Trajectories}\n\nIn this work, we explore generative models that estabilish a mapping between samples $x_1$ from a noise distribution $p_1$ to samples $x_0$ from a data distribution $p_0$ through a framework of ordinary differential equation (ODE). \nThe connection can be expressed as $dy_t = v_\\Theta(y_t, t)\\,dt $ where the velocity $v$ is parameterized by the weights $\\Theta$ of a neural network. \n\\cite{Chen2018NeuralOD} proposed directly solving it using differentiable ODE solvers. However, it proves to be computationally intensive, particularly when applied to large neural network architectures that parameterize $v_\\Theta(y_t, t)$. \n\nA more effective strategy involves directly regressing a vector field $u_t$ that defines a probability trajectory between $p_0$ and $p_1$.\nTo construct such a vector field $u_t$, we consider a forward process that corresponds to a probability path $p_t$ transitioning from $p_0$ to $p_1=\\mathcal{N}(0, 1)$. This can be represented as $z_t = a_t x_0 + b_t \\epsilon\\quad\\text{, where}\\;\\epsilon \\sim \\mathcal{N}(0,I)$.\nWith the conditions $a_0 = 1, b_0 = 0, a_1 = 0$ and $b_1 = 1$, the marginals $p_t(z_t) =\n  \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)}\n  p_t(z_t \\vert \\epsilon)\\;$ align with both the data and noise distribution.\n\nReferring to \\cite{lipman2023flow,esser2024scaling}, we can construct a marginal vector field $u_t$ that generates the marginal probability paths $p_t$, using the conditional vector fields as follows:\n\\begin{align}\n    u_t(z) = \\mathbb{E}_{\\epsilon \\sim\n  \\mathcal{N}(0,I)} [u_t(z \\vert \\epsilon) \\frac{p_t(z \\vert\n  \\epsilon)}{p_t(z)}],\n  \\label{eq:marginal_u}\n\\end{align}\nThe conditional flow matching objective can be then formulated as:\n\\begin{align}\n   {L} =  \\mathbb{E}_{t, p_t(z | \\epsilon), p(\\epsilon) }|| v_{\\Theta}(z, t) - u_t(z | \\epsilon)  ||_2^2\\;, \\label{eq:condflowmatch}\n\\end{align}\nwhere the conditional vector fields $u_t(z \\vert \\epsilon)$ provides a tractable and equivalent objective. \nAlthough there exists different variants of the above formalism, we focus on Rectified Flows (RF) \\cite{liu2022flow,albergo2022building,lipman2023flow}, which define the forward process as straight paths between the data distribution and a standard normal distribution: \n\\begin{equation}\nz_t = (1-t) x_0 + t \\epsilon,\n\\end{equation}\nwith the loss function ${L}$ corresponds to $w_t^\\text{RF} = \\frac{t}{1-t}$. The network output directly parameterizes the velocity $v_\\Theta$.\n\n\\begin{table*}[t]\n\\centering\n\\setlength{\\tabcolsep}{2.mm}{\n\\begin{tabular}{lcccccc}\n\\toprule\n& \\#Params & \\#DoubleStream $m$ &\\#SingleStream $n$  & Hidden dim. $d$ & Head number $h$ &Gflops  \\\\ \\midrule\nSmall  & 142.3M &8 & 16 & 512 & 16& 194.5G  \\\\\nBase &473.9M &12 &24 &768 & 16 & 654.4G \\\\\nLarge &840.6M&12 &24 &1024 &16 & 1162.6G \\\\\nGiant &2109.9M&16 &32 &1408 & 16  &2928.0G\n\\\\ \\bottomrule \n\\end{tabular}}\n\\caption{\\textbf{Scaling law of FluxMusic model size.} The model sizes and detailed hyperparameters settings for scaling experiments. \n}\n\\label{tab:scale}\n\\end{table*}\n\n\\subsection{Model Architecture}\n\nTo enable text-conditioned music generation, our FluxMusic model integrate both textual and musical modalities. We leverage pre-trained models to derive appropriate representations and then describe the architecture of our Flux-based model in detail. \n\n\\paragraph{Music compression.}\nTo better represent music, following \\cite{liu2024audioldm}, each 10.24-second audio clip, sampled at 16kHz, is first converted into a $64 \\times 1024$ mel-spectrogram, with 64 mel-bins, a hop length of 160, and a window length of 1024. This spectrogram is then compressed into a $16 \\times 128$ latent representation, denoted as $X_{spec}$, using a Variational Autoencoder (VAE) pretrained on AudioLDM 2\\footnote{https://huggingface.co/cvssp/audioldm2-music/tree/main}. This latent space representation serves as the basis for noise addition and model training. Finally, a pretrained Hifi-GAN \\cite{kong2020hifi} is employed to reconstruct the waveform from the generated mel-spectrogram.\n\n\\paragraph{Rectified flow transformers for music generation.}\n\nOur architecture builds upon the MMDiT \\cite{esser2024scaling} and Flux architecture. Specifically, we first construct an input sequence consisting of embedding of the text and noised music. The noised latent music representation $X_{spec} \\in \\mathbb{R}^{h\\times w \\times c}$ is flatten 2$\\times$2 patches to a sequence of length $\\frac{1}{2}h \\cdot \\frac{1}{2}w$. After aligning the dimensionality of the patch encoding and the fine-grained text encoding $c$, we concatenate the two sequences. \n\nWe then forward with two type layers including double stream block and single stream blocks. In the double stream block, we employ two distinct sets of weights for the text and music modalities, effectively treating them as independent transformers that merge during the attention operation. This allows each modality to maintain its own space while still considering the other. \nIn the single stream block, the text component is dropped, focusing solely on music sequence modeling with modulation.  \nIt is also found in AuraFlow\\footnote{https://blog.fal.ai/auraflow/} that removing some of MMDiT layers to just be single DiT block were much more scalable and compute efficient way to train these models. \n\nWe incorporate embeddings of the timestep $t$ and coarse text $y$ into the modulation mechanism. \nDrawing from \\cite{esser2024scaling}, we employ multiple text encoders to capture various levels of textual information, thereby enhancing overall model performance and increasing flexibility during inference. By applying individual dropout rates during training, our model allows the use of any subset of text encoders during inference. This flexibility extends to the ability to pre-store blank textual representations, bypassing the need for network computation during inference.\n\n\\subsection{Discussion}\n\n\\paragraph{Model at Scale.}\nIn summary, the hyper-parameters of proposed FluxMusic architecture include the following key elements:  \nthe number of double stream blocks $m$, number of single stream blocks $n$, hidden state dimension $d$, and attention head number $h$. \nVarious configurations of FluxMusic are listed in Table \\ref{tab:scale}, span a broad range of model sizes and computational requirements, from 142M to 2.1B parameters and from 194.5G to 2928.0G Flops. This range provides a thorough examination of the model's scalability.  \nAdditionally, the Gflop metric, evaluated for a  16$\\times$128 text-to-music generation with a patch size of $p=2$, i.e., 10s music clips according to blank text, is calculated using the \\texttt{thop} Python package.\n\n\\paragraph{Synthetic data incorporation.}\n\nIt is widely recognized that synthetically generated captions can greatly improve performance of generative model at scale, i.e., text-to-image generation \\cite{fei2024dimba,chen2023pixart,betker2023improving,fei2021partially,fei2022deecap}.  \nWe follow their design and incorporate enriched music captions produced by a fine-tuned large language model.  \nSpecifically, we use the LP-MusicCaps model \\cite{doh2024enriching,doh2023lp}, available on  Huggingface\\footnote{https://huggingface.co/seungheondoh/ttmr-pp}.\nTo mitigate the potential risk of the text-to-music model forgetting certain concepts not covered in the music captioner‚Äôs knowledge base, we maintain a balanced input by using a mixture of 20\\% original captions and 80\\% synthetic captions.\n\n\\section{Experiments}\n\n\\subsection{Experimental settings}\n\\paragraph{Datasets.} \nWe employ several datasets, including AudioSet Music Subset (ASM) \\cite{gemmeke2017audio}, MagnaTagTune, Million Song Dataset (MSD) \\cite{bertin2011million}, MagnaTagTune (MTT) \\cite{law2009evaluation}, Free Music Archive (FMA) \\cite{defferrard2016fma}, Music4All \\cite{santana2020music4all}, and an additional private dataset. \nEach audio track was segmented into 10-second clips and uniformly sampled at 16 kHz to ensure consistency across the datasets. Detailed captions corresponding to these clips were sourced from Hugging Face datasets\\footnote{https://huggingface.co/collections/seungheondoh/enriching-music-descriptions-661e9342edcea210d61e981d}. Additionally, we automatically labeled the remaining music data using LP-MusicCaps models. This preprocessing resulted in a comprehensive training dataset encompassing a total of $\\sim$22K hours of diverse music content. \n\nTo benchmark our MusicFlux model against prior work, we conducted evaluations using the widely recognized MusicCaps dataset \\citep{agostinelli2023musiclm} and the Song-Describer-Dataset \\citep{manco2023song}. The prior dataset comprises 5.5K clips of 10.24 seconds each, accompanied by high-quality music descriptions provided by ten professional musicians. The latter dataset contains 706 licensed high-quality music recordings. \n\n\\paragraph{Implementail details.}\nWe utilize the last hidden state of FLAN-T5-XXL as fine-grained textual information and the pooler output of CLAP-L as coarse textual features. \nReferring to \\cite{liu2024audioldm}, our training process involves 10-second music clips, randomly sampled from full tracks. The training configuration includes a batch size of 128, a gradient clipping threshold of 1.0, and a learning rate of 1e-4.\nDuring inference, we apply a rectified flow with 50 steps and use a guidance scale of 3.5. To ensure model stability and performance, we maintain a secondary copy of the model weights, updated every 100 training batches through an exponential moving average (EMA) with a decay rate of 0.99, following the approach outlined by \\citet{Peebles_2023}. For unconditional diffusion guidance, we independently set the outputs of each of the two text encoders to null with a probability of 10\\%.\n\n\\paragraph{Evaluation metrics.}\n\nThe generated results are assessed using several objective metrics, including the Fr√©chet Audio Distance (FAD)~\\citep{kilgour2018fr}, Kullback-Leibler Divergence (KL), Inception Score (IS). \nTo ensure a standardized and consistent evaluation process, all metrics are calculated utilizing the \\texttt{audioldm\\_eval} library \\cite{liu2024audioldm}.\n\n\\begin{figure}[t]\n  \\centering\n   \\includegraphics[width=0.98\\linewidth]{structure.png}\n   \\caption{\\textbf{The loss curve of different model structure with similar parameters.}  We can see that combine double and single stream block is much more scalable and compute efficient way for music generation model. \n   }\n   \\label{fig:stream} \n\\end{figure} \n\n\\begin{figure}[t]\n  \\centering\n   \\includegraphics[width=0.98\\linewidth]{scale.png}\n   \\caption{\\textbf{The loss curve of different model parameters with same structure. } We can see that increase model parameters consistently improve the generative performance. \n   }\n   \\label{fig:scale} \n\\end{figure} \n\n\\subsection{Model Analysis}\n\nTo assess the effectiveness of the proposed approaches and compare different strategies, we isolate and test only the specific components under consideration while keeping all other parts frozen. Ablation studies are performed on a subset of the training set, specifically utilizing the ASM and FMA datasets. For evaluation purposes, we employ an out-of-domain set comprising 1K samples randomly selected from the MTT dataset.\n\n\\begin{table}[t]\n\t\\begin{center}\n\t\t\\setlength{\\tabcolsep}{4.mm}{\n\t\t\t\\begin{tabular}{lccc}\n\t\t\t\\toprule\n & FAD$\\downarrow$   & IS$\\uparrow$  & CLAP$\\uparrow$ \\\\ \\midrule\nDDIM  & 7.42  & 1.67 & 0.201 \\\\\nRF & 5.89  &2.43 & 0.312  \\\\\n\\bottomrule\n\t\t\t\\end{tabular}}\n\t\\end{center}\n {\\caption{\\textbf{Effect of rectified flow training in text-to-music generation.}  We train small version of FluxMusic with different sampling schedule and results show the superiority of RF training with comparable computation burden. }\n\t\t\t\t\\label{tab:loss}}\n\\end{table} \n\n\\begin{table*}[ht] \n  \\scalebox{1}{ \n  \\begin{tabular}{lcccccccccc}\n    \\toprule\n    & \\multicolumn{2}{c}{\\text{Details}} & \\multicolumn{4}{c}{\\text{MusicCaps}} & \\multicolumn{4}{c}{\\text{Song Describer Dataset}} \\\\\n    \\cmidrule(lr){2-3} \\cmidrule(lr){4-7} \\cmidrule(lr){8-11}\n      & \\text{Params} & \\text{Hours} & \\text{FAD} \\(\\downarrow\\) & \\text{KL} \\(\\downarrow\\) & \\text{IS} \\(\\uparrow\\) & \\text{CLAP} \\(\\uparrow\\) & \\text{FAD} \\(\\downarrow\\) & \\text{KL} \\(\\downarrow\\) & \\text{IS} \\(\\uparrow\\) & \\text{CLAP} \\(\\uparrow\\) \\\\\n    \\midrule\n    MusicLM & 1290M & 280k & 4.00 & - & - & - & - & - & - & - \\\\\n    MusicGen  & 1.5B & 20k & 3.80 & 1.22 & - & 0.31 & 5.38 & 1.01 & 1.92 & 0.18 \\\\\n    Mousai & 1042M & 2.5k & 7.50 & 1.59 & - & 0.23 & - & - & - & - \\\\\n    Jen-1 & 746M & 5.0k & 2.0 & 1.29 & - & 0.33 & - & - & - & - \\\\\n    AudioLDM 2 (Full) & 712M & 17.9k & 3.13 & {1.20} & - & - & - & - & - & - \\\\\n    AudioLDM 2 (Music)  & 712M & 10.8k & 4.04 & 1.46 & 2.67 & 0.34 & 2.77 & 0.84 & 1.91 & 0.28 \\\\\n    QA-MDT (U-Net) & 1.0B & 12.5k & 2.03 & 1.51 & 2.41 & 0.33 &  {1.01} & {0.83} & 1.92 & 0.30 \\\\\n    QA-MDT  (DiT) & {675M} & 12.5k & {1.65} & 1.31 & {2.80} & {0.35} & 1.04 & {0.83} & {1.94} & {0.32} \\\\\\midrule \n    FluxMusic & 2.1B &22K & 1.43 & 1.25 & 2.98 & 0.36 & 1.01 & 0.83 & 2.03 & 0.35\\\\\n    \\bottomrule\n  \\end{tabular}\n}\n \\centering\n  \\caption{\\textbf{Evaluation results for text-to-music generation with diffusion-based models and language-based models.} We can see that with compettive parameters and training data, FluxMusic achieve best results in most metrics, demonstrating the promising of structure. }\n  \\label{tab:main} \n\\end{table*}\n\n\\begin{table}[ht]\n  \\setlength{\\tabcolsep}{8pt} \n  \n  \\begin{tabular}{lcccc}\n    \\toprule\n    & \\multicolumn{2}{c}{\\text{Experts}} & \\multicolumn{2}{c}{\\text{Beginners}} \\\\\n    \\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \n    \\text{Model} & \\text{OVL} & \\text{REL} & \\text{OVL} & \\text{REL} \\\\\n    \\midrule\n    Ground Truth & 4.20 & 4.15 & 4.00 & 3.85 \\\\\n    \\midrule % Added horizontal line\n    AudioLDM 2 & 2.55 & 2.45 & 3.12 & 3.74  \\\\\n    MusicGen & 3.13 & 3.34 & 3.06 & 3.70 \\\\\n    FluxMusic & 3.35 & 3.54 & 3.25 & 3.80  \\\\\n    \\bottomrule\n  \\end{tabular}\n  \\centering\n  \\caption{\\textbf{Evaluation results of text-to-music performances in human evaluation.} We denoted for text relevance (\\text{REL}) and overall quality (\\text{OVL}), with higher scores indicating better performance. }\n  \\label{tab:human}\n\\end{table}\n\n\\paragraph{Advantage of model architecture.}\n\nWe examine the architectural design choices for the diffusion network within FluxMusic, focusing on two specific variants: (1) utilizing double stream blocks exclusively throughout the entire network, and (2) employing a combination of both double and single stream blocks. In particular, we use a small version of the model, with one variant comprising 15 double stream blocks, referred to as 15D\\_0S, and the other combining 8 double stream blocks with 16 single stream blocks, referred to as 8D\\_16S. These configurations result in parameter counts of approximately 145.5M and 142.3M, respectively.\nAs depicted in Figure \\ref{fig:stream}, the combined double and single modality stream block architecture not only accelerates the training process but also enhances generative performance, despite maintaining a comparable parameter scale. Consequently, we designate the mixed structure as the default configuration.\n\n\\paragraph{Effect of rectified flow.}\n\nTable \\ref{tab:loss} presents a comparative analysis of various training strategies employed in FluxMusic, including DDIM and rectified flow, using the small model version. Both strategy training with 128 batch size and 200K training steps to maintain an identical computation cost. As anticipated, and in line with prior research \\cite{esser2024scaling}, rectified flow training demonstrates a positive impact on generative performance within the music domain.\n\n\\paragraph{Effect of model parameter scale.}\n\nWe examine the scaling properties of the FluxMusic framework by analyzing the impact of model depth, defined by the number of double and single stream layers, and model width, characterized by the hidden size dimension. Specifically, we train four variants of FluxMusic using 10-second clips, with model configurations ranging from small to giant, as detailed in Table \\ref{tab:scale}.\nAs the loss cureve depicted in Figure \\ref{fig:scale}, performance improves as the depth of double:single modality block increases from 8:16 to 16:32, and similarly, expanding the width from 512 to 1408 results in further performance gains. It is important to note that the model's performance has not yet fully converged, as training was conducted for only 200K steps. Nonetheless, across all configurations, substantial improvements are observed at all training stages as the depth and width of the FluxMusic architecture are increased.\n\n\\subsection{Compared with Previous Methods}\nWe conducted a comparative analysis of our proposed MusicFlux method against several prominent prior text-to-music approaches, including AudioLDM 2~\\citep{liu2024audioldm}, Mousai~\\citep{schneider2024mousai}, Jen-1~\\citep{li2024jen}, and QA-MDT~\\cite{li2024quality}, which model music using spectral latent spaces, as well as MusicLM~\\citep{agostinelli2023musiclm} and MusicGen~\\cite{copet2024simple}, which employ discrete representations. All the results of these comparisons are summarized in Table~\\ref{tab:main}.\n\nThe experimental outcomes highlight the significant advantages of our FluxMusic models, which achieve state-of-the-art performance across multiple objective metrics. These findings underscore the scalability potential of the FluxMusic framework, particularly as model and dataset sizes consistently increase.\nAlthough FluxMusic exhibited a slight advantage in FAD and KL metrics on the Song-Describer-Dataset, this may be attributed to instabilities stemming from the dataset's limited size. Further, our superiority in text-to-music generation was corroborated through additional subjective evaluations.\n\n\\begin{figure*}[t]\n  \\centering\n   \\includegraphics[width=0.96\\linewidth]{case_step.pdf}\n   \\caption{\\textbf{Generated mel-spectrum cases of different training steps. } We plot small version of MusicFlux at every 50K training steps and we can find that the image becomes orderly and fine-grained instead of random and disorderly with the training continues. \n   }\n   \\label{fig:step} \n\\end{figure*}\n\n\\begin{figure*}[t]\n  \\centering\n   \\includegraphics[width=0.96\\linewidth]{case_scale.pdf}\n   \\caption{\\textbf{Generated mel-spectrum cases of different model parameters. } With model size increase, the resulting mel-spectrum becomes more content rich and rhythmically distinct.\n   }\n   \\label{fig:case_scale} \n\\end{figure*}\n\n\\begin{figure*}[t]\n  \\centering\n   \\includegraphics[width=0.95\\linewidth]{case_cfg.pdf}\n   \\caption{\\textbf{Generated mel-spectrum cases of different classifier-free guidance.} We plot four clips with diverse textual prompts from small version of FluxMusic model and concatenate them in one figure. It can be seen that increasing the CFG number results in a more pronounced contrast in the generated mel-spectrum. To consistent with previous works, we set CFG=3.5 by default. \n   }\n   \\label{fig:cfg} \n\\end{figure*}\n\n\\subsection{Human Evaluation}\nWe laso conducted a human evaluation, following settings outlined in \\cite{li2024quality,liu2024audioldm}, to assess the performance of text-to-music generation. This evaluation focused on two key aspects of the generated audio samples: (i) overall quality (OVL) and (ii) relevance to the textual input (REL).\nFor the overall quality assessment, human raters were asked to evaluate the perceptual quality of the samples on a scale from 1 to 5. Similarly, the text relevance test required raters to score the alignment between the audio and the corresponding text input, also on a 1 to 5 scale.\nOur evaluation team comprised individuals from diverse backgrounds, including professional music producers and novices with little to no prior knowledge in this domain. These groups are categorized as experts and beginners. Each randomly selected audio sample was evaluated by at least ten raters to ensure robust results.\n\nAs reflected in Table~\\ref{tab:human}, our proposed FluxMusic method significantly enhances both the overall quality of the music and its alignment with the text input. These improvements can be attributed to the RF training strategy and the advanced architecture of our model. Notably, the feedback from experts indicates substantial gains, highlighting the model's potential utility for professionals in the audio industry.\n\n\\subsection{Visualization and Music Examples}\n\nFor a more convenient understanding, we visualize some generated music clips towards different prompt, from different perspective. These visualizations encompass: (1) different training step, (2) model parameter at scale, (2) setting of classifier-free guidance (CFG) number, the results are presented in Figure \\ref{fig:step}, \\ref{fig:case_scale}, and \\ref{fig:cfg}, respectively. For more cases and listen intuitively, we recommand to visit the project webpage. \n\n\\section{Conclusion}\n\nIn this paper, we explore an extension of the FLUX framework for text-to-music generation. Our model, FluxMusic, utilizes rectified flow transformers to predict mel-spectra iteratively within a latent VAE space. Experiments demonstrate the advanced performance comparable to existing benchmarks. Moreover, our study yields several notable findings: first, a simple rectified flow transformer performs effectively for audio spectrograms. Then, we identify the optimal strategy and learning approach through an ablation study. Future research will investigate scalability using a mixture-of-experts architecture and distillation techniques to enhance inference efficiency.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2405.15863v2.tex",
        "arXiv-2406.04673v1.tex",
        "arXiv-2409.00587v2.tex"
    ],
    "group_id": "group_109",
    "response": "### Summary of Recent Advances in Text-to-Music Generation\n\n#### Title: Recent Advances in Text-to-Music Generation: Quality-aware Training and Multi-modal Conditioning\n\n#### Introduction\nText-to-music generation is an emerging field within machine learning that aims to synthesize musical content based on textual descriptions. This task is crucial for multimedia creation, offering new expressive forms and innovative tools for content creators. The field has seen significant advancements, with models like MusicLM, MusicGen, and Mo√ªsai leading the way by generating high-fidelity music from text descriptions. However, these models face challenges such as the need for extensive, high-quality datasets, and the difficulty in capturing fine-grained nuances of musical compositions, including melody, harmony, rhythm, and dynamics. Additionally, the integration of visual cues and the refinement of textual descriptions to improve the quality and alignment of generated music remain largely unexplored. This summary will delve into three recent studies that address these challenges, each proposing unique methodologies to enhance the quality and diversity of music generation.\n\n#### Paper 1: QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation\n\n**Main Content:**\nThis study introduces QA-MDT, a quality-aware masked diffusion transformer designed to enhance music generation. The primary challenge addressed is the scarcity of high-quality, paired audio and text data, which often leads to low-quality generated music and poor text-audio alignment. To tackle this, QA-MDT employs a pseudo-MOS (Mean Opinion Score) scoring model to assign quality scores to music waveforms, which are then used as quality prefixes and tokens during training. These scores are integrated into the text encoder and the diffusion architecture to foster quality awareness and control. The model also uses a masking strategy to enhance the spatial correlation of the music spectrum, thereby accelerating convergence and improving the quality of generated music. Additionally, QA-MDT leverages large language models (LLMs) and CLAP (Contrastive Language-Audio Pretraining) models to synchronize music signals with captions, enhancing text-audio correlation.\n\n**Datasets and Model Architecture:**\nThe training dataset includes AudioSet Music Subset (ASM), MagnaTagTune (MTT), Million Song Dataset (MSD), Free Music Archive (FMA), and a large-scale copyright-free dataset. The final dataset consists of 12.5k hours of diverse music data, with each track clipped to 10-second segments and sampled at 16kHz. The diffusion model is trained using a Variational Autoencoder (VAE) to compress the mel-spectrograms into a latent representation. The model architecture includes 20 encoder layers and 8 decoder layers, with a masking strategy applied during training to improve spatial correlations. The training process involves 7 days (38.5k steps) on four NVIDIA A100 GPUs, with a batch size of 64 and a learning rate of 8e-5.\n\n**Evaluation Metrics:**\nThe model's performance is evaluated using objective metrics such as Fr√©chet Audio Distance (FAD), Kullback-Leibler Divergence (KL), and Inception Score (IS), alongside subjective metrics like overall quality (OVL) and relevance to the text input (REL). The evaluation datasets include MusicCaps and the Song-Describer Dataset, both of which contain high-quality music clips and descriptions.\n\n**Results:**\nQA-MDT outperforms previous models on both objective and subjective metrics, demonstrating significant improvements in overall quality and text alignment. The model's ability to discern and control the quality of generated music is highlighted through ablation studies, showing that the quality-aware training paradigm enhances the model's performance.\n\n#### Paper 2: \\modelname: Synthesizing Music from Image and Text Cues using Diffusion Models\n\n**Main Content:**\nThis paper proposes \\modelname, a diffusion model that generates music conditioned on both image and text cues. The main challenge here is the lack of datasets that contain image-text-music triplets, which is addressed by introducing a new dataset, \\ourdataset, comprising 15,000 manually annotated triplets. \\modelname introduces a novel \"visual synapse\" to infuse image semantics into the text-to-music diffusion model, enabling the model to generate music that is consistent with the visual and textual semantics. The visual synapse leverages self-attention features from a pre-trained text-to-image diffusion model, which are then infused into the cross-attention features of the text-to-music model during training and inference.\n\n**Datasets and Model Architecture:**\nThe training dataset includes \\ourdataset, which contains 15,000 image-text-music triplets, and an extended version of MusicCaps, which includes images extracted from YouTube videos or the web. The model architecture consists of a text-to-music diffusion model with a visual synapse that injects image semantics into the model. The model is trained for 30 epochs using AdamW optimizer on four NVIDIA A100 GPUs. The training process involves DDIM inversion to obtain noisy latent representations of images and subsequent fusion of image and text features.\n\n**Evaluation Metrics:**\nThe model's performance is evaluated using objective metrics like FAD, KL, and FD, as well as a new metric, \\imagemusicmetric, which measures the perceptual similarity between generated music and the conditioning image. Subjective evaluation involves human raters assessing the overall quality (OVL) and relevance to the text input (REL) of generated music samples.\n\n**Results:**\n\\modelname significantly outperforms existing text-to-music generation methods on both objective and subjective metrics, achieving up to 67.98% improvement in FAD scores. The study also highlights the importance of conditioning on both visual and textual modalities, demonstrating that this approach leads to better music generation quality and diversity.\n\n#### Paper 3: FLUX that Plays Music\n\n**Main Content:**\nThis paper explores the application of rectified flow transformers for text-to-music generation, introducing FluxMusic as a scalable and efficient framework. The primary challenge is the computational overhead and extended sampling times associated with traditional diffusion models, which are addressed by employing rectified flow training. FluxMusic integrates both textual and musical modalities, using a combination of double stream blocks (text and music) and single stream blocks (music only) to predict noise in the latent VAE space of mel-spectrograms. The model architecture is designed to be scalable, with different configurations tested to assess the impact of model size and parameter scaling.\n\n**Datasets and Model Architecture:**\nThe training dataset includes AudioSet Music Subset (ASM), MagnaTagTune (MTT), Million Song Dataset (MSD), Free Music Archive (FMA), Music4All, and an additional private dataset. Each track is segmented into 10-second clips and sampled at 16kHz. The model architecture is based on the Flux framework, with different configurations tested to assess scalability. The model is trained for 200K steps with a batch size of 128 and a learning rate of 1e-4.\n\n**Evaluation Metrics:**\nThe model's performance is evaluated using objective metrics such as FAD, KL, and IS, as well as subjective metrics like overall quality (OVL) and relevance to the text input (REL). The evaluation datasets include MusicCaps and the Song-Describer Dataset.\n\n**Results:**\nFluxMusic achieves state-of-the-art performance across multiple objective metrics, demonstrating the effectiveness of rectified flow training and the advanced architecture of the model. The study also highlights the scalability of the FluxMusic framework, showing consistent performance improvements as model and dataset sizes increase.\n\n#### Commonalities and Innovations\nAll three papers focus on enhancing the quality and diversity of music generation through innovative training strategies and architectures. QA-MDT introduces a quality-aware training paradigm that incorporates pseudo-MOS scores to control the quality of generated music. \\modelname innovates by integrating image semantics into the text-to-music generation process, using a visual synapse to infuse fine-grained contextual information from images. FluxMusic explores the application of rectified flow transformers, demonstrating the scalability and efficiency of this approach for text-to-music generation.\n\n**Commonalities:**\n- All models utilize diffusion-based architectures to generate music from text descriptions.\n- Each model incorporates pre-trained text encoders to capture detailed textual information.\n- The models are trained on large-scale datasets to enhance generalization and diversity.\n- Evaluation involves both objective metrics (FAD, KL, IS) and subjective metrics (OVL, REL).\n\n**Innovations:**\n- QA-MDT: Introduces quality-aware training by injecting pseudo-MOS scores into the diffusion process.\n- \\modelname: Proposes a visual synapse to infuse image semantics into the text-to-music generation process.\n- FluxMusic: Applies rectified flow transformers to predict mel-spectra within a latent VAE space, demonstrating scalability and efficiency.\n\n#### Comparison of Results\nThe three models exhibit varying degrees of performance improvement across different metrics. QA-MDT shows significant gains in both objective and subjective metrics, particularly in terms of p-MOS scores, which indicate the quality of generated music. \\modelname outperforms existing text-to-music methods on FAD, KL, and \\imagemusicmetric, with a relative gain of up to 67.98% on FAD scores. FluxMusic achieves state-of-the-art performance across multiple objective metrics, with notable improvements in FAD and KL scores.\n\n**Objective Metrics:**\n- **FAD (Fr√©chet Audio Distance):** QA-MDT: 1.65, \\modelname: 1.12, FluxMusic: 1.43.\n- **KL (Kullback-Leibler Divergence):** QA-MDT: 1.31, \\modelname: 0.89, FluxMusic: 1.25.\n- **IS (Inception Score):** QA-MDT: 2.80, \\modelname: 0.76, FluxMusic: 2.98.\n- **CLAP (Contrastive Language-Audio Pretraining) Score:** QA-MDT: 0.35, \\modelname: 0.83, FluxMusic: 0.36.\n\n**Subjective Metrics:**\n- **Overall Quality (OVL):** QA-MDT: 3.27, \\modelname: 86.78, FluxMusic: 3.35.\n- **Relevance to Text Input (REL):** QA-MDT: 3.77, \\modelname: 85.92, FluxMusic: 3.54.\n\n**Discussion:**\nThe differences in performance can be attributed to the unique training strategies and architectures employed by each model. QA-MDT's quality-aware training paradigm and masking strategy contribute to its superior performance in terms of musicality and audio quality. \\modelname's visual synapse and multi-modal conditioning lead to significant improvements in the perceptual quality and alignment of generated music. FluxMusic's rectified flow transformers and scalable architecture enable it to achieve state-of-the-art performance across various metrics.\n\n#### Conclusion\nThe three papers highlight significant advancements in the field of text-to-music generation, each addressing unique challenges and proposing innovative solutions. QA-MDT introduces quality-aware training to enhance the quality and musicality of generated music. \\modelname innovates by integrating image semantics into the generation process, demonstrating the utility of multi-modal conditioning. FluxMusic explores the application of rectified flow transformers, showcasing the scalability and efficiency of this approach. These studies collectively underscore the potential of diffusion-based models in generating high-quality, diverse music from text descriptions, and suggest future research directions in refining these models to better capture the nuances of musical compositions and to integrate additional modalities for enhanced generative performance.\n\n**Future Research Directions:**\n- **Quality-aware Training:** Further research could explore more sophisticated quality control mechanisms, such as incorporating real-time quality feedback during inference.\n- **Multi-modal Conditioning:** Investigate the integration of additional modalities, such as video or other sensory inputs, to condition music generation.\n- **Scalability and Efficiency:** Develop more efficient training and inference strategies, such as distillation techniques and mixed expert architectures, to enhance the scalability of diffusion-based models.\n- **Human-in-the-loop Evaluation:** Incorporate more detailed human-in-the-loop evaluation frameworks to better understand the subjective quality of generated music.\n- **Genre-specific Models:** Explore the development of genre-specific models to better capture the unique characteristics of different musical styles.\n\nThese advancements and future research directions promise to further enhance the capabilities of text-to-music generation models, making them more versatile and effective tools for multimedia creation."
}