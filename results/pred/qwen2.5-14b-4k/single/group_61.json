{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on\\\\ Chat-based Large Language Models}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nAlthough large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning.\nTo improve the reasoning abilities, we propose \\textbf{ChatCoT}, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs (\\eg ChatGPT).\nIn ChatCoT, we model the chain-of-thought~(CoT) reasoning as multi-turn conversations, to utilize tools in a more natural way through chatting.\nAt each turn, LLMs can either interact with tools or perform the reasoning. \nOur approach can effectively leverage the multi-turn conversation ability of chat-based LLMs, and integrate the thought chain following and tools manipulation in a unified way. \nSpecially, we initialize the early turns of the conversation by the knowledge about tools, tasks, and reasoning format, and propose an iterative \\emph{tool-augmented reasoning} step to perform step-by-step tool-augmented reasoning.\nThe experiment results on two complex reasoning datasets (MATH and HotpotQA) have shown the effectiveness of ChatCoT on complex reasoning tasks, achieving a 7.9\\% relative improvement over the state-of-the-art baseline.\nOur code and data are available at:   \\url{https://github.com/RUCAIBOX/ChatCoT}.\n\\end{abstract}\n\n\\section{Introduction}\n\nRecently, large language models~(LLMs)~\\cite{LLMsurvey} have shown great potential as general-purpose task solvers in a variety of real-world applications. \nWith excellent few-shot and zero-shot ability, LLMs, such as  GPT-4~\\cite{GPT-4} and LLaMA~\\cite{LLaMA},  can even outperform full-data supervised-tuned models on many tasks with suitable prompting strategies. \n\nAmong these prompting strategies, chain-of-thought~(CoT)  prompting~\\cite{CoT, zero-shot-cot} has been a prominent approach to eliciting the reasoning abilities of LLMs.  \nIt incorporates the intermediate reasoning steps of exemplars into the input prompt, to instruct LLMs to solve a question step by step.  Despite the remarkable improvement by CoT prompting, LLMs still have difficulties in solving complex reasoning tasks that involve specific functionalities, such as arithmetic calculation and information retrieval~\\cite{Lu-arxiv-2022-A, Qian-arxiv-2022-Limitations}. \nTo address this issue, external tools (\\eg calculator, search engine) have been employed to fulfill the basic  functionalities~\\cite{Toolformer,paranjape2023art}, easing the burden of LLMs. \nWith proper interfaces, LLMs can be guided by prompts to manipulate tools when necessary. \n\nHowever, as tools are not intrinsically integrated with LLMs, incorporating external tools would have to interrupt the CoT reasoning process of LLMs.  \nSuch an issue would become more intractable on complex reasoning tasks that frequently invoke the use of tools.  \nTo address it, existing work either relies on LLMs to prearrange the tool use plan for subsequent execution~\\cite{Least-to-Most, Self-planning}, or needs to design formal actions pertaining to specific tasks~\\cite{Sccessive-Prompting,dsp,jiang2023structgpt}. \nDespite the effectiveness, the two types of methods still suffer from potential issues: the former one cannot interact with tools after generating the plan,  even seeing obvious mistakes; while the latter one has to frequently switch between reasoning with LLMs and taking actions, hurting the continuity of the CoT reasoning process.\n\nTo overcome these disadvantages, we seek a more unified way to integrate CoT reasoning and tool manipulation. \nAs the key idea, we consider tools manipulation by LLMs as the \\emph{interaction} between LLMs and tools, in which LLMs send the use requests and tools respond to support specific functions.  \nFurther, inspired by the recent progress of ChatGPT-like LLMs (called  \\emph{chat-based  LLMs}), we model the interaction process between LLMs and tools as a multi-turn conversation, and leverage the excellent chatting capacities for manipulating tools by LLMs. \nAt each turn, the LLM can freely interact with tools when in need, otherwise perform the reasoning by itself. \nThe conversation continues until the final answer is derived by LLMs. \nIn this process, as chat-based LLMs can well understand the multi-turn context, they can follow the thought chain in the whole conversation and naturally invoke the tools accordingly, thus keeping the continuity of the reasoning process.\n\nTo this end, in this paper, we propose ChatCoT, a tool-augmented chain-of-thought reasoning strategy for chat-based LLMs. As the major merit, \nChatCoT can perform the CoT reasoning across multi-turn conversation, and freely interact with tools at immediate steps.\nConcretely, we first store the useful knowledge at early turns of the conversation, including tools, tasks, and multi-turn reasoning format, to help LLMs utilize task-specific knowledge to perform reasoning or manipulate tools.\nThen, we iterate a specially designed  \\emph{tool-augmented reasoning} step in which LLMs interact with tools, to perform step-by-step tool-augmented reasoning, until obtaining the final answer.\n\nTo evaluate the effectiveness, we implement ChatCoT on ChatGPT, and conduct experiments on two complex reasoning benchmarks, \\ie MATH~\\cite{MATH} and HotpotQA~\\cite{HotpotQA}.\nExperimental results show that ChatCoT achieves very promising performance on MATH with 7.9\\% relative improvement in average over the SOTA baselines (\\ie PHP~\\cite{PHP}).\nBesides, our approach can also be integrated with other strategies, \\eg self-consistency, and ChatCoT can achieve better performance by incorporating these strategies.\n\n\\section{Related Work}\n\n\\paratitle{Tool-Augmented Large Language Models.}\nWith the large-scale parameters and pre-training corpus, large language models (LLMs) (\\eg Flan T5~\\cite{flant5}, ChatGPT~\\cite{ChatGPT} and LLaMA~\\cite{LLaMA}) have demonstrated strong zero-shot and few-shot ability in NLP tasks (\\eg language generation, reasoning).\nHowever, LLMs have still struggled with complex reasoning tasks requiring task-specific knowledge and multi-step reasoning (\\eg mathematical problem solving).\nPrevious work~\\cite{jz1,jz2,wizardmath} has constructed task-specific corpus and utilized continue pre-training and instruction tuning to inject relative knowledge into LLMs and enhance the complex reasoning ability of LLMs.\nIn order to further reduce the mistakes made by LLMs, existing methods have explored to augment LLMs with external tools.\nThey can be roughly categorized into the following two types.\nThe first type of methods~\\cite{Atlas, talm, TRICE} train the model parameters to support the utilization of the external tools, where they collect or synthesize the tool-augmented examples to tune the model parameters~\\cite{Toolformer, gorilla, toolkengpt}.\nAnother type of methods~\\cite{PAL, ReAct, carp} utilize carefully designed prompts to guide LLMs to use external tools. They focus on devising proper prompts or tools manipulation ways to select and use tools when necessary~\\cite{TaskMatrix, hugginggpt, ReAct}.\nIn this work, we follow the second type of methods and propose a tool-augmented chain-of-thought reasoning strategy that can better solve complex reasoning tasks.\n\n\\paratitle{Chain-of-Thought Reasoning.}\nTo further enhance the reasoning capacity of LLMs, Chain-of-Thought (CoT) prompting strategy~\\cite{CoT,zero-shot-cot} has been proposed to guide LLMs to generate intermediate reasoning steps which can boost the performance of LLMs.\nThrough special instructions (\\eg \\emph{``Let us think step by step''}) and in-context exemplars with detailed intermediate reasoning steps, LLMs can perform step-by-step reasoning to reach the final answer.\nBased on CoT, recent work has also proposed several methods to further improve the performance, including problem decomposition~\\cite{Least-to-Most, Sccessive-Prompting}, appropriate exemplars selection~\\cite{ye2022Complementary, Shi2023Large}, results post-processing~\\cite{self-consistency, self-refine, PHP}, and changing the reasoning format~\\cite{tree-of-thought, mathchat}.\nHowever, as the generation process of CoT is one-pass, the utilization of tools in intermediate steps would have to interpret it, hurting the continuity of the generation process.\nIn this work, we propose a unified way to integrate CoT reasoning and tool manipulation, which utilizes the excellent multi-turn chatting capacity of LLMs to perform CoT reasoning across multi-turn conversations.\n\nIn this section, we present the task setting, then introduce the Chain-of-Though prompting strategy and the tool manipulation in reasoning tasks.\n\n\\paratitle{Task Setting.}\nIn this work, we focus on improving the reasoning ability of large language models~(LLMs) on complex tasks, \\eg solving mathematics competition problems.\nUnlike tasks that can be solved by humans via straightforward skills or tools, complex tasks require advanced knowledge (\\eg mathematical theorem) and multi-step reasoning to reach the answer.\nTypically, a complex problem includes three types of texts, namely problem statement, solution text, and answer key, denoted as $Q$, $S$ and $A$, respectively.\nThe problem statement $Q$ introduces the background and description of a complex problem, and the solution text illustrates the detailed solving process to obtain the answer key.\nAll of them are composed of a sequence of tokens, where each token is either a text word or a mathematical symbol.\nFormally, given the problem statement $Q$, we aim to utilize LLMs to perform multi-step reasoning, to finally generate its accurate answer $A$.\n\n\\paratitle{Chain-of-Thought Prompting.}\nTo elicit the powerful reasoning ability of LLMs for complex tasks, Chain-of-Thought (CoT) prompt strategy~\\cite{CoT} has been widely used to guide LLMs for performing step-by-step reasoning.\nGenerally, the CoT prompt consists of few exemplars whose a series of intermediate reasoning steps $\\{I_1, \\cdots, I_n \\}$ are also involved.\nEach exemplar can be denoted as $E=\\langle Q, \\{ I_1, \\cdots, I_n \\}, A \\rangle$.\nFormally, given the question and few exemplars, a CoT prompt is composed by integrating them as a long input of the LLM, which can prompt the LLM to generate a similar thought chain that leads to the final answer.\n\n\\paratitle{Tool Manipulation.}\n\\label{tool_manipulation}\nPrevious work has revealed that LLMs are struggling with basic functionality (\\eg arithmetical calculation~\\cite{Toolformer}), which can be solved by using specific external tools (\\eg calculator), denoted as $\\{T_1, \\dots, T_n\\}$. \nTo manipulate tools, existing work mostly relies on writing a detailed prompt for describing how to use available tools for the LLM, then incorporates it to guide the selection of useful tools and generate the tool arguments, and finally calls the tool API to obtain the result.\nFollowing this way, in this work, we focus on three useful tools that have been widely used by humans to solve complex problems:\n\n$\\bullet$ \\emph{Calculator:} Given a mathematical expression, the calculator can compute the value of it or simplify it according to arithmetic rules (\\eg combining like terms and reduction of fractions). \n\n$\\bullet$ \\emph{Equation Solver:} Given the equations system and unknown variables, the equation solver can automatically calculate the value of the contained unknown variables through relative algorithms.\n\n$\\bullet$ \\emph{Retriever:} Given a query, the retriever aims to extract the most relevant information (\\eg documents) from a number of candidates. According to the types of the retrieved corpus, it can be implemented by specialized models, \\eg dense retrieval model. \n\nWe implement the first two tools by using different functions of \\emph{SymPy}~\\cite{sympy}, \na Python library for mathematical symbolic calculation.\nFor the retriever, we adopt SimCSE~\\cite{SimCSE}, a sentence embedding model to measure the text semantic similarity.\nNote that when the input expression or equation is ill-formed or unsolved, the above tools would return an error.\n\n    \\centering\n    \\includegraphics[width=\\textwidth]{figure/framework.pdf}\n    \\caption{The comparison of vanilla CoT and ChatCoT, illustrated for a mathematical problem. For vanilla CoT, the content underlined are generated by LLMs. For ChatCoT, the conversational knowledge memory is initialized to provide tools, task and reasoning format knowledge. Then, the tool-augmented reasoning step is iterated multiple times to perform step-by-step reasoning, until obtaining the answer.}\n    \\label{framework}\n\\end{figure*}\n\n\\section{Approach}\nIn this section, we present our proposed ChatCoT, a new chain-of-thought~(CoT) prompting framework based on multi-turn conversations, for improving chat-based LLMs on complex reasoning tasks with tools. \nThe overall illustration of our proposed ChatCoT is shown in Figure~\\ref{framework}.\n\n\\subsection{Overview}\nFor complex tasks (\\eg advanced mathematical problems), LLMs require to frequently manipulate the tools when in need, to fulfill the intractable intermediate issues. % and achieve better performance.\nHowever, as tools are not intrinsically integrated with LLMs, previous work mostly relies on the LLM to generate the plan of manipulating tools and then execution~\\cite{PAL,lu2023chameleon}, or immediately call tools by stopping the continuous generation of LLMs~\\cite{ReAct}.\nThe above both ways are not suitable for the frequent interactions between LLMs and tools, due to the error accumulation in planning and frequent interruptions in LLM generation.\n\nIn our approach, we decompose the chain-of-thought reasoning process of LLMs into a multi-round conversation. % where LLMs can interact with tools multiple times in corresponding rounds until finally obtain the answer.\nIn each turn, LLMs just require to concentrate on manipulating tools or accomplishing reasoning in the current step, and the whole reasoning process would keep on pushing without premature planning and sudden interruption.\nIn this way, the whole reasoning process would be converted to a conversation between LLMs and an agent, which follows pre-defined rules to guide LLMs and manipulate the tool.\nBy designing proper chatting strategies, the agent would automatically elicit LLMs to perform reasoning and select a tool, or invoke the tool for execution.\n\nIn our approach, we first initialize the multi-turn conversation by feeding chat-based LLMs with the background knowledge, \\ie the description of tools, relevant task exemplars, and the demonstration of decomposed chain-of-thought in chat, which are the \\emph{conversational knowledge memory} for supporting the following reasoning.\nThen, we propose the \\emph{tool-augmented reasoning} procedure that leverages LLMs to perform reasoning with tools in the current step and iterate it to fulfill all sub-tasks in the whole reasoning process, until reaching the answer.\nWe introduce the details of the two components in the following.\n\n\\subsection{Initializing Conversational Knowledge Memory}\nTo guide chat-based LLMs to follow our proposed ChatCoT using external tools, it is essential to design proper prompts in context.\nIn our approach, as we reformulate the chain-of-thought reasoning into a decomposed multi-turn conversation, we can also feed the essential prompts into LLMs at early turns as the context, to initialize the conversation background knowledge.\nIt can be seen as the in-context \\emph{knowledge memory} in the format of dialogue that stores useful knowledge for helping chat-based LLMs manipulate tools or perform reasoning.\nHere, we consider three types of knowledge about tools, task, and multi-turn reasoning format, respectively.\nThe details of prompts are in Appendix~\\ref{conversation_memory_prompt}.\n\n\\paratitle{Tools Knowledge.} \nAs LLMs have never seen tools during pre-training, %it is necessary to introduce the detailed functionality of all available tools and the way tools are used, to LLMs.\nfor each tool in Section~\\ref{tool_manipulation}, we hand-craft its description in the following pattern: ``\\emph{[$T$] can help you [$Y$]}'', where \\emph{[$T$]} is the tool name and \\emph{[$Y$]} shows its detailed functionality.\nThen, we merge all the descriptions and design the input prompt to tell LLMs about the knowledge of all tools.\nWe also hand-craft the expected response of the LLM. It will be also fed into the LLM, to indicate the LLM that it has accepted our prompt and should follow it.\n\n\\paratitle{Retrieval-Augmented Task Knowledge.}\nSince LLMs can learn the task knowledge from in-context exemplars, we leverage a retriever to select the most relevant instance from the training dataset, to provide more useful knowledge for the given question.\nConcretely, we train SimCSE~\\cite{SimCSE}, a sentence embedding method that can measure the semantic similarity of texts, via the unsupervised training strategy on the training set.\nThen, we leverage it to retrieve the top-$k$ most semantically similar exemplars, and concatenate their problem statement $Q$ and solution $S$ to compose the input prompt.\nSimilarly, we also feed it with our expected response into the LLM. \n\n\\paratitle{Multi-turn Reasoning Format.}\nTo elicit LLMs following multi-turn reasoning format, we manually annotate the whole multi-round dialogue $I_1, \\cdots, I_n$ of randomly sampled five questions from the training set, to create the exemplars.\nThen, we feed the dialogues of all the exemplars into the chat-based LLM round by round, as the context to guide LLMs to follow it for performing reasoning.\n\n\\paratitle{Summary.} The above three types of multi-turn utterances are pre-defined with corresponding contents and formats, which compose the \\emph{conversational knowledge memory} of our approach.\nIt would be leveraged to initialize the conversational context, and support the following step-by-step reasoning for answering the question.\n\n\\ignore{\n\\begin{algorithm}\n  \\caption{RSE procedure}\\label{RSE}\n\\small\n    \\SetKwData{Left}{left}\\SetKwData{This}{this}\\SetKwData{Up}{up}\n    \\SetKwFunction{Union}{Union}\\SetKwFunction{Sample}{Sample}\\SetKwFunction{MaskMLM}{MaskMLM}\\SetKwFunction{MaskDAE}{MaskDAE}\n    \\SetKwInOut{Input}{Input}\\SetKwInOut{Parameter}{Parameter}\n    \\SetKwRepeat{Do}{do}{while}\n\n    \\Input{The history of dialogue $H$}\n    \\BlankLine\n    LLMs generate the reasoning step\\;\n    Ask LLMs about which tools they need\\;\n    \\If {LLMs do not need tools} {\n        Encourage LLMs continue reasoning;\\\n    }\n    \\Else {\n        \\Do {Results are not useful} {\n            Ask LLMs about parameters of tools\\;\n            Obtain the results of tools\\;\n            Ask LLMs whether the results are useful\\;\n        }\n        Encourage LLMs continue reasoning\\;\n    }\n\\end{algorithm}}\n\n\\subsection{Iterative Tool-augmented Reasoning}\nBased on the above conversational knowledge memory, we iterate the tool-augmented reasoning step to perform step-by-step tool-augmented reasoning, until finally obtain the answer.\n\n\\subsubsection{Tool-augmented Reasoning Step}\nThe tool-augmented reasoning step can be iterated in multiple times. \nIn each iteration, based on the current results, we first leverage LLMs to perform reasoning, then select the proper tool by LLMs, and finally execute the selected tool to obtain the intermediate result in the current step.\n\n\\paratitle{LLM for Reasoning.}\nGuided by the exemplars in the conversation history, LLMs are able to decompose the whole reasoning process into multi-turn chat.\nSpecially, LLMs would be elicited by the contextual exemplars to directly perform reasoning in natural language based on the current result, without specialized prompts or instructions.\nConsequently, LLMs can rely on the retrieval-augmented task knowledge in context, to generate the natural language solution till the point that needs the functionality of tools.\n\n\\paratitle{LLM for Tools Selection.}\nAfter reasoning, we utilize the LLM to select a useful tool (\\eg calculator), which will be employed to provide the required functionality for the LLM.\nHere, the input prompt of the LLM is ``\\emph{To solve this sub-problem, which tool can we use?}''\nAfter feeding it into the LLM, if the LLM requires to utilize tools, it will select a suitable one, and then we further ask the LLM to formulate the input arguments of the tool, \\eg mathematical expression.\nOtherwise, it will answer ``\\emph{Do not use tool}'', and the LLM will continue to perform reasoning.\n\n\\paratitle{Tools Execution.}\nGiven the selected tool and formulated arguments by LLMs, we can execute the tool with the arguments to obtain the result in the current iteration.\nHere, we also consider that the results from the tool may be not satisfied by the LLM, \\eg irrelevant retrieved documents.\nIn this case, we can also add several feedback rounds where the LLM judges if the result is useful or expected, and then reuse the tool to acquire a new result.\n\n\\subsubsection{Iteration for Step-by-Step Reasoning}\nWe iterate the above step based on the in-context conversation knowledge memory, to perform step-by-step reasoning on the given question $Q$.\nWe start the whole iteration process using the following prompt:\n``\\emph{You should solve the problem step by step and you should follow the react in the history [$Q$]}''.\nThen, after reaching the answer key, the iteration process will be stopped by LLMs.\nIn practice, we find that chat-based LLMs are prone to continue chatting although the answer key has appeared in the reasoning process.\nThus, we set the maximum chat turns, and devise the following prompt to force LLMs to stop reasoning and conclude the answer:\n``\\emph{Base on the context, what is the answer?}''.\n\nAs our proposed approach only decomposes the one-pass chain-of-thought reasoning into multi-turn chat and adds the utilization of tools, it is agnostic to the task types and tools implementation.\nTherefore, it is a general framework that can be applied to a variety of complex reasoning tasks that require suitable tools.\nBesides, our approach also supports the recently proposed improvement strategies based on the chain-of-thought method, \\eg self-consistency~\\cite{self-consistency}.\nWe conduct corresponding experiments in Section~\\ref{sec-analysis} to validate it.\n\n\\ignore{\n\\subsection{Multi-step reasoning via multi-round dialogue}\n\nThe major advantage of multi-round dialogue is that it can convert a single reasoning process into several steps more natural than traditional CoT.\nBy leveraging the chatting ability of LLMs, we design an assistant based on rule which communicates with LLMs to guide LLMs reasoning step by step and help LLMs interact with external tools.\nIn ChatCot, each reasoning step can be view as one round of dialogue, and the full reasoning path $S$ can be transform into dialogue history $H$.\nThrough multi-round dialogue with rule based assistant, LLMs reason step by step and finally solve the problem.\nFor details, given a reasoning problem $D$, assistant will ask LLMs to decompose this problem and solve the sub-problem.\nAccording to the problem and prompt, LLMs decompose the problem and generate corresponding reasoning step $s_i$.\nThen, assistant utilize template to construct prompt to encourage LLMs to continue reasoning.\nBy analogy, LLMs decompose the complex problem into sub-problems and solve it step by step.\nThe right part of Figure~\\ref{comparison} is an example of problem decomposing and reasoning step by step in ChatCoT.\n\nFormally, the full reasoning process can be represented by follow equations.\n\\begin{align}\n\\text{Assistant}&:~\\texttt{Prompt}(D) \\nonumber \\\\\n\\text{LLMs}&:~s_1 = \\{t_{1,1}, t_{1, 2}, \\dots, t_{1, l_1}\\} \\nonumber \\\\\n\\text{Assistant}&:~\\texttt{Prompt}(s_1) \\nonumber \\\\\n\\dots&:~\\dots \\nonumber \\\\\n\\text{LLMs}&: s_n = \\{t_{n,1}, t_{n, 2}, \\dots, t_{n, l_n}\\},~A \\nonumber\n\\end{align}\nThe speaker is on the left of colon, and the content of dialogue is on the right of colon.\n$\\texttt{Prompt}(x)$ is a function to select template by rule based method and generate prompt according to the input $x$.\nGiven the description of problem $D$, assistant utilize function $\\texttt{Prompt}(D)$ to obtain the prompt of problem and feed it into LLMs.\nThen, LLMs generate reasoning steps and interact with assistant until reach the final answer.\nOnce reaching the final answer, LLMs will generate the answer $A$ in specific template, and assistant will extract the final answer from response of LLMs and finish dialogue process.}\n\n\\ignore{\n\\subsection{Discussion}\n\nIn this part, we have a discussion about detailed implement of ChatCoT including prompt construction and adaption to classical modules.\n\n\\subsubsection{Augment modules adaption}\n\nAlthough ChatCoT guide LLMs reasoning step by step through multi-round dialogue which is different from classical CoT, ChatCoT can also adapt to the augment modules which are designed for CoT, such as self-consistency and self-refine.\nBesides, with decomposing reasoning, the methods about decoding strategy~\\cite{Xie-arxiv-2023-Decomposition} can also be used in ChatCoT to boost the performance.\nIn this section, we introduce the methods to adapt ChatCoT to augment modules, and take self-consistency and self-refine as representative works.\n\n\\paratitle{Consistency}\nThe main idea of self-consistency is generating diversified reasoning path to get several answers and selecting the most confident answer through voting.\nFormally, the score the each answer can be calculated by follow equation, and the answer getting the highest score will be chosen as the final answer.\n$$\n\\text{Score}(A_i) = \\sum_{p=1}^{k}{\\mathbbm{1}[A_p == A_i]}\n$$\nWhere $k$ denotes the number to generated reasoning path, and $A_i$ is the answer of $i$-th reasoning path.\nChatCoT can also generate diversified multi-round dialogue which has similar function with reasoning path in CoT via adjusting the temperature parameters.\nBy leveraging diversified multi-round dialogue, ChatCoT can obtain several answers and get the final answer by ensemble.\nEmpirical study shows that ChatCoT adapts to self-consistency better than classical CoT method.\n\n\\paratitle{Refinement}\nFor refinement, a convenient method is converting mulit-round dialogue into a paragraph of reasoning in natural language.\nFirst, to convert dialogue history into a paragraph of natural language, the prompt and instruction which guide LLMs to decompsing reasoning and manipulating external tools is not necessary.\nTherefore, we delete these content containing useless information and keep the content including reasoning steps and results from external tools.\nBy connecting the remaining content together without the name of speaker, the reasoning path in natural language will be obtained and can be used in refinement.\nBy converting multi-round dialogue into reasoning path in natural language, it can easily adapt to the refinement methods like self-refine.}\\section{Experiment}\n\nIn this section, we conduct experiments to evaluate the effectiveness of ChatCoT.\nThe implementation details can be found in Appendix~\\ref{Implementation}.\n\n\\begin{table}[]\n    \\centering\n    \\begin{tabular}{cccc}\n        \\bottomrule\n        \\textbf{Dataset} & \\textbf{Category} & \\textbf{Train} & \\textbf{Dev}/\\textbf{Test}  \\\\\n        \\hline\n        \\multirow{7}*{MATH} & Algebra & 1744 & 1187 \\\\\n            & CP & 771 & 474 \\\\\n            & Precalculus & 746 & 546 \\\\\n            & Prealgebra & 1205 & 871 \\\\\n            & Geometry & 870 & 479 \\\\\n            & IA & 1295 & 903 \\\\\n            & NT & 869 & 540 \\\\\n        \\hline\n        \\multirow{1}*{HotpotQA} & Distractor & 90477 & 7405 \\\\        \n        \\bottomrule\n    \\end{tabular}\n    \\caption{Statistics of the two complex reasoning datasets. CP, IA, and NT denote \\emph{Counting and Probability}, \\emph{Intermediate Algebra}, and \\emph{Number Theory}, respectively.}\n    \\label{dataset}\n\\end{table}\n\n\\subsection{Experimental settings}\n\n\\paratitle{Datasets.}\nWe consider two complex reasoning datasets for evaluation, \\ie MATH~\\cite{MATH} and HotpotQA~\\cite{HotpotQA}.\nThe details of these two datasets are shown in Table~\\ref{dataset}.\nWe adopt accuracy as the evaluation metric.\n\n$\\bullet$ \\emph{MATH} is composed of challenging competition mathematical problems which require advanced mathematical knowledge. It is divided into seven categories, \\ie Algebra, Counting and Probability, Precalculus, Prealgebra, Geometry, Intermediate Algebra, and Number Theory.\nWe adopt the calculator and an equation solver as external tools to help LLMs.\n\n$\\bullet$ \\emph{HotpotQA} is a multi-hop question answering dataset, where each question is associated with a collection of paragraph candidates containing several golden contents which are useful for reasoning.\nWe use the development set under the distractor setting of HotpotQA for evaluation, where the annotation of golden paragraphs is not aware to LLMs.\nWe employ the retriever as the external tool.\n\n\\paratitle{Baselines.}\nWe mainly compare our approach with the following prompting strategies based on ChatGPT~\\cite{ChatGPT}: \n\n$\\bullet$ \\emph{Chain-of-Thought (CoT)}~\\cite{CoT} is a prominent method to boost the performance of LLMs in reasoning tasks.\nIn CoT, LLMs are prompted to generate the intermediate reasoning path and reasoning step by step to reach the final answer.\nPrevious work has shown that the utilization of external tools and similar exemplars improves the performance of CoT.\nTherefore, we implement external tools to help LLMs reason and retrieve to help LLMs select exemplars, which are named \\emph{CoT w/ Tool}, and \\emph{CoT w/ Retri}, respectively.\n\n$\\bullet$ \\emph{Learning-to-Program (LP)}~\\cite{LP} guides LLMs to program in natural language by learning solutions in the training set, and elicits LLMs to solve tasks following the program.\n\n$\\bullet$ \\emph{Progressive-Hint Prompting (PHP)}~\\cite{PHP} proposes to iteratively refine the solution based on the answer hints from previous trials. The iterative method achieves SOTA on MATH.\n\nTo provide a more complete evaluation, we also report the performance of various LLM backbones with the vanilla CoT prompting, including PaLM~\\cite{PaLM}, PaLM 2~\\cite{PaLM-2}, Minerva~\\cite{Minerva}, Galactica~\\cite{Galactica}, LLaMA~\\cite{LLaMA} and GPT-3~\\cite{GPT-3}.\n\n\\begin{table*}\n    \\centering\n    \\begin{tabular}{cccccccccc}\n        \\bottomrule\n        \\multirow{2.5}*{\\textbf{Models}} & \\multirow{2.5}*{\\begin{tabularx}{0.08\\textwidth}{@{}X@{}}\\textbf{Prompt}\\\\\\textbf{Strategy}\\\\ \\end{tabularx}} & \\multicolumn{8}{c}{\\textbf{MATH}} \\\\\n        \\cmidrule(r){3-10}\n         &  & Algebra & CP & PC & PA & Geometry & IA & NT & Avg.  \\\\\n        \\hline\n        GPT-3 & CoT & 6.0 & 4.7 & 4.0 & 7.7 & 3.1 & 4.4 & 4.4 & 5.2 \\\\\n        PaLM & CoT & 9.7 & 8.4 & 4.4 & 19.2 & 7.3 & 3.5 & 6.0 & 8.8 \\\\\n        LLaMA & CoT & - & - & - & - & - & - & - & 10.6 \\\\\n        Galactica & CoT & 29.0 & 13.9 & 12.8 & 27.2 & 12.3 & 9.6 & 11.7 & 20.4 \\\\\n        Minerva & CoT & 51.3 & 28.0 & 18.0 & 55.0 & 26.8 & 13.7 & 21.2 & 33.6 \\\\\n        PaLM 2 & CoT & - & - & - & - & - & - & - & 34.3 \\\\\n        \\hline\n        \\multirow{6}*{ChatGPT} & CoT & 48.1 & 31.4 & \\underline{21.1} & 56.6 & 22.3 & 18.3 & 29.1 & 35.1 \\\\\n         & CoT w/ Tool & 35.9 & 22.6 & 9.3 & 40.5 & 13.6 & 9.4 & 19.4 & 23.8 \\\\\n         & CoT w/ Retri & \\underline{52.7} & 32.7 & 18.9 & \\underline{58.4} & \\underline{29.2} & \\textbf{19.9} & 31.7 & \\underline{37.7} \\\\\n         & LP & 49.6 & 30.2 & 16.3 & 52.3 & 22.5 & 16.9 & 29.8 & 34.0 \\\\\n         & PHP & 51.1 & \\underline{33.7} & 16.1 & 57.7 & 25.4 & 17.1 & \\textbf{35.1} & 36.5 \\\\\n         & ChatCoT & \\textbf{56.1} & \\textbf{34.2} & \\textbf{23.8} & \\textbf{59.2} & \\textbf{29.9} & \\underline{19.5} & \\underline{32.6} & \\textbf{39.4} \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{Experimental results on MATH dataset. PC and PA denote \\emph{Precalculus} and \\emph{Prealgebra}, respectively. Avg. is the average value of all categories. The best are denoted in bold and the second-best are underlined.}\n    \\label{main_results}\n\\end{table*}\n\n\\begin{table}\n    \\centering\n    \\begin{tabular}{cc}\n        \\bottomrule\n        \\textbf{Methods} & \\textbf{HotpotQA} \\\\\n        \\hline\n        CoT & 38.0 \\\\\n        CoT w/ Tool & 31.4 \\\\\n        ChatCoT w/o Feedback & \\underline{53.8} \\\\\n        ChatCoT & \\textbf{59.2} \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{The results on HotpotQA. We report the results of the development set under the distractor setting.}\n    \\label{hotpotqa}\n\\end{table}\n\n\\subsection{Main Results}\n\nWe present the evaluation results of our approach on MATH and HotpotQA datasets in Table~\\ref{main_results} and Table~\\ref{hotpotqa} respectively.\n\nFirst, for the comparison of backbones for CoT prompting, ChatGPT achieves the best performance, demonstrating its outstanding mathematical reasoning ability.\nOur method elicits the reasoning process by leveraging the strong multi-turn dialogue ability of ChatGPT, thus leading to a better release of the reasoning ability from ChatGPT.\n\nSecond, retrieval-augmented methods (\\eg ChatCoT, CoT w/ Retri) outperform other baselines.\nThe reason is that retrieved exemplars may contain more relevant knowledge and reasoning steps that are beneficial to solve the given problem.\nOn Geometry tasks of MATH, CoT w/ Retri achieves the largest improvement over vanilla CoT than other sub-tasks.\nAnother possible reason is that ChatGPT is more unfamiliar to the knowledge and symbol of geometry than others.\nWithout similar exemplars, it is difficult for LLMs to well understand them.\n\nThird, given the results of CoT and CoT w/ Tool on MATH and HotpotQA, we can find that directly utilizing external tools during reasoning is not a suitable way, which may hurt the performance of LLMs.\nThe reason may be that injecting tool usage into the CoT reasoning process will hurt the continuity of reasoning. %and make LLMs confused about the reasoning step.\n\nFinally, ChatCoT achieves state-of-the-art performance on MATH dataset based on ChatGPT and outperforms other baselines on HotpotQA.\nCompared with the previous SOTA method PHP, ChatCoT outperforms six of seven sub-tasks on MATH dataset and achieves 7.9\\% relative improvement on average accuracy over the PHP method.\nThe experiment results have verified the effectiveness of ChatCoT on complex reasoning tasks.\nBy leveraging conversational knowledge memory and multi-round dialogue to reasoning, ChatCoT has the advantage to utilize plug-and-play tools.\nMoreover, on the Number Theory tasks of MATH, we can find that PHP achieves the best performance.\nThe reason may be that there are fewer equations that need to be computed or simplified.\nThus, the advantage of the utilization of tools becomes less obvious.\n\n\\begin{table}\n    \\centering\n    \\begin{tabular}{cccccc}\n        \\bottomrule\n        \\multicolumn{3}{c}{\\textbf{Methods}} & \\multicolumn{3}{c}{\\textbf{MATH}} \\\\\n        \\cmidrule(r){1-3}\\cmidrule(r){4-6}\n        TK & RATK & MRF & PC & Geo & NT \\\\\n        \\hline\n        \\ding{52} & \\ding{52} & \\ding{52} & \\textbf{23.8} & \\textbf{29.9} & \\textbf{32.6}  \\\\\n        \\ding{55} & \\ding{52} & \\ding{52} & 23.3 & 29.2 & 30.6 \\\\\n        \\ding{52} & \\ding{55} & \\ding{52} & 20.0 & 27.4 & 31.0 \\\\\n        \\ding{52} & \\ding{52} & \\ding{55} & 21.6 & 24.2 & 32.2 \\\\\n        \\ding{55} & \\ding{55} & \\ding{52} & 16.7 & 21.1 & 29.3 \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{The results of ablation study. TK, RATK, and MRF denote if using tool knowledge, retrieval-augmented task knowledge, and multi-turn reasoning format at early turns of the conversation, respectively. Geo is the abbreviation of \\emph{Geometry}.} %\\ding{55} indicates the removing of corresponding memory while \\ding{52} indicates the usage of memory. The best results are in bold.}\n    \\label{ablation}\n\\end{table}\n\n\\subsection{Detailed Analysis}\n\\label{sec-analysis}\n\nIn order to further verify the effectiveness of each component in ChatCoT, we conduct experiments about ablation, adaption, tools utilization and expense.\nWe present the case study in Appendix~\\ref{case_study_illustration}.\n\n\\paratitle{Ablation Study.}\nIn the ablation study, we evaluate the effectiveness of conversational memory, including tool knowledge memory, retrieval-augmented knowledge memory, and multi-turn reasoning format memory.\nAs shown in Table~\\ref{ablation}, removing any type of conversational memory will reduce the performance of ChatCoT, which indicates the effectiveness of these memories in complex reasoning.\nIn particular, removing retrieval-augmented knowledge memory or multi-turn reasoning format memory will lead to a large drop, which shows that mathematical knowledge and reasoning format knowledge is important for LLMs in reasoning tasks, while LLMs can learn the usage of external tools from exemplars without descriptions.\n\n\\begin{table}\n    \\centering\n    \\begin{tabular}{ccc}\n        \\bottomrule\n        \\textbf{Methods} & \\textbf{CP} & \\textbf{NT} \\\\\n        \\hline\n        CoT + SC & $35.2_{+3.8\\%}$ & $34.4_{+5.3\\%}$ \\\\\n        ChatCoT + SC & $40.1_{+5.9\\%}$ & $38.3_{+5.7\\%}$ \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{The evaluated accuracy of combining our approach with self-consistency. SC denotes self-consistency. We also report the absolute improvement compared with vanilla methods on subscripts.}\n    \\label{comb_classical}\n\\end{table}\n\n\\begin{table}\n    \\centering\n    \\begin{tabular}{ccc}\n        \\bottomrule\n        \\textbf{Methods} & \\textbf{Frequency} & \\textbf{Success}  \\\\\n        \\hline\n        CoT w/ Tool & 3.0\\% & 85.7\\% \\\\\n        ChatCoT w/o TK & 56.0\\% & 93.0\\% \\\\\n        ChatCoT w/o MRF & 10.0\\% & 64.2\\% \\\\\n        ChatCoT & 70.0\\% & 92.0\\% \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{Frequency and success rate of tool manipulation on Number Theory task of MATH. TK, MRF denote tool knowledge, multi-turn reasoning format at early turns of the conversation respectively.}\n    \\label{tool_ratio}\n\\end{table}\n\n\\begin{table}[t]\n    \\centering\n    \\begin{tabular}{ccc}\n        \\bottomrule\n        \\textbf{Methods} & \\textbf{Generated Tokens} \\\\\n        \\hline\n        CoT & 224.6 \\\\\n        CoT w/ Tool & 296.2 \\\\\n        Self-Consistency & 1017.4 \\\\\n        ChatCoT & 355.2 \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{The comparison of the number of generated tokens from LLMs among different prompt strategies.}\n    \\label{generated_tokens}\n\\end{table}\n\n\\paratitle{Combination with Improvement Strategies.}\nChatCoT is a general method to enhance the ability of tool manipulation of LLMs.\nIt can be integrated with improvement strategies and further boost the performance of LLMs on reasoning tasks.\nTo evaluate the applicability of ChatCoT to improvement strategies designed for CoT, we compare ChatCoT with CoT on two subtasks of MATH, where both methods are augmented with self-consistency~\\cite{self-consistency}, a representative improvement strategy for CoT prompting. \nConcretely, we sample 5 outputs for majority voting in self-consistency.\nAs shown in Table~\\ref{comb_classical}, self-consistency brings improvement in both CoT and ChatCoT.\nIn particular, the absolute improvement of ChatCoT is slightly higher than CoT, showing that ChatCoT can adapt to self-consistency well.\nThe reason is that, with the decomposing of reasoning procedures, the intermediate steps of ChatCoT are more confident, and small mistakes will be corrected easily.\nMoreover, we construct the case study about the combination with ChatCoT and Self-Refine~\\cite{self-refine} in Appendix~\\ref{case_study_sr}.\n\n\\paratitle{Tools Utilization Analysis.}\nAs we mentioned above, in complex reasoning tasks, infrequently or incorrectly utilizing external tools might lead to wrong answers.\nThus, we conduct the experiment about whether LLMs can frequently or correctly leverage based on different methods.\nTable~\\ref{tool_ratio} expresses the performance of tools utilization in the Number Theory task of MATH of baseline and our approach.\n``Frequency'' denotes the ratio of problems where LLMs correctly leverage tools. \n``Success'' denotes the rate of LLMs utilizing tools successfully among all the times of invoking tools.\nWe can observe that ChatCoT achieves a balance of frequency and ratio of success.\nTool knowledge provides the function of tools for LLMs and improves the frequency that LLMs utilize the tools.\nLLMs can learn how to leverage external tools through the multi-turn reasoning format and boost the ratio of successful utilization of tools. \nWithout any of them, the frequency and ratio of success will drop which might not be conducive to reasoning.\n\n\\paratitle{Number of Generated Tokens Analysis.} \nDespite guiding LLMs to reason through multi-turn dialogue, the computation expense of ChatCoT is not significantly larger than the CoT method.\nIn Table~\\ref{generated_tokens}, we present the average number of generated tokens from LLMs in several methods on MATH dataset, which reflects the computation expense to a certain degree.\nWe can observe that ChatCoT is on the same order of magnitude with other baselines (\\eg CoT and CoT w/ Tool).\nTherefore, ChatCoT does not bring significant expenses compared to existing prompting methods.\n\n\\section{Conclusion}\n\nIn this paper, we have proposed ChatCoT, a new framework to  manipulate the tools  for the CoT reasoning. \nIt naturally integrates the reasoning process  and manipulating tools through a form of  multi-turn conversations. \nAt each turn, LLMs can either interact\n with tools or perform the reasoning by itself. Our approach can effectively leverage the multi-turn\n conversation ability of chat-based LLMs.  \nExperimental results on two complex reasoning tasks including MATH and HotpotQA have verified the effectiveness of ChatCoT.\n\nCurrently, our experiments are mainly conducted on mathematical reasoning tasks, and we will test the effectiveness of the proposed approach to more types of reasoning tasks. Besides, we will also consider extending the number of available tools for solving different tasks. \n\\section*{Limitations}\n\nIn this section, we discuss the limitations of our work.\nFirst, we do not utilize GPT-4 in our experiment or evaluate the performance of GPT-4 in the ChatCoT framework. That is because our application for GPT-4 has not been accepted.\nSecond, ChatCoT is designed for chat-based LLMs and it is hardly compatible with other LLMs. However, most LLMs support multi-turn conversation currently and they perform well on reasoning tasks.\nBesides, although LLMs have achieved strong ability in reasoning tasks, the requirement of computation expense and GPU resource is higher than other pre-trained language models which have millions of parameters. The utilization of LLMs will produce more carbon dioxide and pollute the environment.\n\n\\section*{Acknowledgement}\nThis work was partially supported by National Natural Science Foundation of China under Grant No. 62222215, Beijing Natural Science Foundation under Grant No. L233008 and 4222027, and Beijing Outstanding Young Scientist Program under Grant No. BJJWZYJH012019100020098. And this work is also partially supported by the Outstanding Innovative Talents Cultivation Funded Programs 2021 of Renmin University of China. Xin Zhao is the corresponding author.\n\n\\newpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\n\nLarge language models (LLMs) have achieved impressive performance on various reasoning tasks.\nTo further improve the performance, we propose MultiTool-CoT, a novel framework that leverages chain-of-thought (CoT) prompting to incorporate multiple external tools, such as a calculator and a knowledge retriever, during the reasoning process.\nWe apply MultiTool-CoT to the Task 2 dataset of NumGLUE, which requires both numerical reasoning and domain-specific knowledge.\nThe experiments show that our method significantly outperforms strong baselines and achieves state-of-the-art performance.\n\\footnote{Our code is publicly available at \\url{https://github.com/InabaTatsuro/MultiTool-CoT}.}\n\n\\end{abstract}\n\n\\section{Introduction}\n\nReasoning refers to the logical process of inferring unknown facts from known facts.\nSolving reasoning problems requires language understanding, real-world knowledge, arithmetic calculation, and symbolic processing.\nImproving the reasoning capability of artificial intelligence has been a long-standing challenge and remains an active research topic to this day~\\cite{gordon-etal-2012-semeval,sap-etal-2020-commonsense}.\n\nRecently, large language models (LLMs) have achieved amazing performance on various reasoning tasks~\\cite{GPT-3,lewkowycz2022solving,opt,palm}.\nHowever, the amount of real-world knowledge learned by LLMs is still constrained by the size of model parameters and the training data.\nThis problem could be more severe in the case of sparse domain-specific knowledge.\nFurthermore, LLMs are based on the computation among continuous token representations, which cannot ensure accurate arithmetic calculations.\n\nTo solve these problems, previous studies propose to complement the capabilities of LLMs with an external tool, such as a web browser or a calculator~\\cite{2021-nakano-webgpt,dentaku,ReAct}.\nThis is performed by invoking an external tool during reasoning with LLMs and injecting the results into the reasoning process.\nHowever, previous studies have focused on using a single external tool to solve a single problem with LLMs and have not addressed different problems together.\n\nThis paper proposes MultiTool-CoT, an interactive framework that allows LLMs to use multiple external tools during reasoning.\nFigure~\\ref{fig:overview} provides an overview.\nIn MultiTool-CoT, LLMs solve reasoning problems by generating reasoning processes including tool triggers to invoke external tools.\nWe let LLMs learn to invoke multiple external tools at proper reasoning steps by chain-of-thought (CoT) prompting based on few-shot learning~\\cite{wei2022chain}.\n\nAs a proof of concept, we apply MultiTool-CoT to the Task 2 dataset of NumGLUE~\\cite{mishra-etal-2022-numglue}, which requires both numerical reasoning and domain-specific knowledge.\nExperiments show that MultiTool-CoT significantly outperforms strong baselines and achieves state-of-the-art performance.\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{pic/Overview_en.pdf}\n    \\caption{\n        Overview of the MultiTool-CoT.\n        The output of GPT-3, the calculator, the chemical reaction predictor, and the molar mass list are highlighted in green, yellow, orange, and purple, respectively.\n    }\n    \\label{fig:overview}\n\\end{figure*}\n\n\\section{Related Work}\nLarge language models (LLMs) can perform various tasks by \\textit{prompting}~\\cite{10.1145/3560815}.\nAs for reasoning tasks, chain-of-thought (CoT) prompting~\\cite{wei2022chain,kojima2022large} is known for its effectiveness, which elicits the answer with intermediate reasoning steps from LLMs.\n\nThere is a growing body of work on using an external tool to improve reasoning with LLMs.\n\\citet{dentaku} use a calculator to process mathematical formulas that appear in reasoning processes by fine-tuning LLMs to generate mathematical formulas with a tool trigger to call the calculator.\n\\citet{2021-nakano-webgpt} allow LLMs to use a web browser by fine-tuning LLMs to generate action codes to operate the browser.\nPrevious studies focus on a single problem of LLMs, namely, error-prone arithmetic calculation or incomplete real-world knowledge, and address it by fine-tuning LLMs so that they can call a single external tool.\nIn contrast, this study addresses multiple problems together by allowing LLMs to use multiple external tools.\nBesides, this study presents a few-shot learning-based framework~\\cite{GPT-3} for doing this, which does not require fine-tuning.\n\nA very recent study~\\cite{ReAct} proposes a few-shot learning-based method for invoking a Wikipedia API to perform knowledge-intensive reasoning tasks.\nHowever, this study has not investigated the effectiveness of using multiple external tools.\nA Python library named LangChain\\footnote{\\url{https://langchain.readthedocs.io/en/latest}} implements a framework for allowing LLMs to use multiple external tools based on \\citet{ReAct}, which is similar to ours.\nHowever, its effectiveness has not been investigated in any benchmark datasets as of this submission.\n\n\\section{Method}\n\\label{sec:proposed}\n\nWe propose MultiTool-CoT, an interactive framework that allows LLMs to use multiple external tools during reasoning.\nFigure~\\ref{fig:overview} illustrates an overview.\n\nMultiTool-CoT leverages chain-of-thought (CoT) prompting based on few-shot learning~\\cite{wei2022chain}.\nOur prompt consists of an instruction specifying the available external tools, few-shot examples demonstrating several question-answer pairs with reasoning processes, and a question to be solved.\nWe manually annotate the reasoning processes shown as few-shot examples with tool triggers marked with corresponding input data, adhering to a specific format.\nIn this study, we let the string \\texttt{$<<$External tool name$>>$} be a tool trigger.\nFor example, if we use a calculator as an external tool, we annotate the reasoning processes with the tool trigger \\texttt{$<<$Calculator$>>$} after input formulas like $2 \\times 62$.\n\nWhen reasoning, GPT-3 follows the prompt and generates a reasoning process including tool triggers.\nIf a tool trigger is generated, we stop text generation.\nWe then extract the name of the external tool and the input for the tool from the reasoning process, execute the tool with the input, and append the result to the end of the reasoning process.\nAfter that, we restart text generation.\n\nIf we cannot execute an external tool for some reason (e.g., invalid tool input is generated), we fall back on GPT-3 and let it generate the output of the tool.\n\nWe observe that the final answer value is nearly always contained in the last sentence of the reasoning process. Therefore, we apply an additional GPT-3 few-shot learning process for mapping the last sentence to the answer value by prompting several sentence-answer pairs.\n\n\\section{Experiment}\n\nAs a proof of concept, we applied MultiTool-CoT to solve a knowledge-based numerical reasoning task.\n\n\\subsection{Dataset}\nWe used the Task 2 dataset of NumGLUE ~\\cite{mishra-etal-2022-numglue}, which requires both numerical reasoning and domain-specific knowledge, mainly related to chemistry.\nExample~\\ref{ex:task2} shows a question in the dataset.\n\n\\ex. \\label{ex:task2}\nFind the amount of Calcium hydroxide required to react with 2 moles of Carbon dioxide to form 2 moles of Calcium carbonate along with 2 moles of Water.\n\nAll the answers are given as numbers.\nWe used 325 questions in the test split for evaluation.\nWe evaluated the accuracy.\n\n\\subsection{External Tools}\n\\label{sec:application}\n\nWe implemented the following external tools and used them in the proposed framework.\n\n\\begin{itemize}\n    \\item \\textbf{Calculator (\\textsc{Cal})}:\n        The calculator is given a mathematical formula and outputs the calculation result.\n        The calculator is implemented using Python's \\texttt{eval} function\\footnote{\\url{https://docs.python.org/3/library/functions.html\\#eval}}.\n        Operators in mathematical formulas are replaced according to Python's syntax.\n        We prompt GPT-3 to output the tool trigger, \\texttt{$<<$Calculator$>>$}, with a mathematical formula on the same line.\n        \n    \\item \\textbf{Chemical reaction predictor (\\textsc{Crp})}:\n        The chemical reaction predictor is given the chemical formula of reactants and products and outputs the chemical reaction equation by adjusting the coefficients so that the reactants and products have the same number of each atom.\n        We prompt GPT-3 to output the tool trigger, \\texttt{$<<$Chemical reaction predictor$>>$}, with the reactants and products on the previous two lines.\n        \n    \\item \\textbf{Molar mass list (\\textsc{Mml})}:\n        The molar mass list is given a chemical formula and outputs its molar mass.\n        The molar mass of the chemical formula is calculated from the atoms and their number in the formula.\n        The molar mass of the atoms is obtained from the knowledge base listing the weight of all atoms.\n        We prompt GPT-3 to output the tool trigger, \\texttt{$<<$Molar mass list$>>$}, with a chemical formula on the same line.\n\\end{itemize}\n\n\\subsection{Methods for Comparison}\n\n\\begin{table}[t]\n    \\centering\n    \\begin{tabular}{lD{.}{.}{2}}\n        \\toprule\n        Method & \\\\\n        \\midrule\n        Zero-Shot$^{\\dagger}$ & 1 \\\\\n        Zero-Shot+CoT & 32.62 \\\\\n        \\midrule\n        Few-Shot$^{\\dagger}$  & 42 \\\\\n        Few-Shot+CoT & 57.85 \\\\\n        MultiTool-CoT (\\textsc{Cal} only) & 62.77 \\\\\n        MultiTool-CoT (\\textsc{Crp} only) & 64.31 \\\\\n        MultiTool-CoT (\\textsc{Mml} only) & 69.23 \\\\\n        MultiTool-CoT (\\textbf{Ours}) & \\textbf{85}.\\textbf{85} \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{\n    Performance in the Task 2 dataset of NumGLUE.\n    The best result is shown in \\textbf{bold}.\n    ($\\dagger$) is cited from \\citet{mishra-etal-2022-numglue}.\n    }    \n    \\label{tab:result}\n\\end{table}\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{pic/fig1.png}\n    \\caption{\n    An improved example.\n    The green lines indicate correct reasoning processes.\n    The red lines indicate errors related to knowledge or arithmetic calculation.\n    }\n    \\label{fig:mis}\n\\end{figure*}\n\nWe used GPT-3 (\\texttt{text-davinci-003}; 175B parameters) via OpenAI API\\footnote{\\url{https://openai.com/api/}} and compared the following methods.\n\n\\paragraph{Zero-Shot}\nWe fed only the question into GPT-3 and considered the generated text as the answer.\n\n\\paragraph{Zero-Shot+CoT~\\cite{kojima2022large}}\nWe fed the question with the sentence ``Let's think step by step.'' into GPT-3 and obtained the answer with the intermediate reasoning steps.\nWe then added the sentence fragment ``Therefore, the answer (Arabic numerals) is '' after the generated text and fed it into GPT-3 to get the final answer.\n\n\\paragraph{Few-Shot}\nWe fed the question with few-shot examples of question-answer pairs into GPT-3 and obtained the generated text as the answer.\n\n\\paragraph{Few-Shot+CoT}\nWe performed the proposed method without invoking any external tools.\nIf the tool triggers were generated, we used GPT-3 to output the result.\n\n\\paragraph{MultiTool-CoT ($\\{$\\textsc{Cal}$|$\\textsc{Crp}$|$\\textsc{Mml}$\\}$ only)}\nWe performed the proposed method with one of the external tools introduced in Section~\\ref{sec:application}.\nAs for the other external tools, we let GPT-3 generate the result.\n\n\\paragraph{MultiTool-CoT (Ours)}\nWe performed the proposed method with all the external tools introduced in Section~\\ref{sec:application}.\n\nIn few-shot settings, we used 20 questions in the training split as few-shot examples.\nThe questions were manually selected to avoid bias in the number of external tool calls.\nIn order to annotate the questions with reasoning processes with tool triggers, we followed a two-step process.\nFirst, we employed GPT-3 to generate the reasoning processes for solving these questions using zero-shot chain-of-thought prompting~\\cite{kojima2022large}, aiming to obtain reasoning processes that GPT-3 can easily follow.\nThen, we manually annotated the reasoning processes with tool triggers and the input and output for the corresponding external tools.\n\nWe set the temperature parameter of GPT-3 as 0 to generate constant predictions.\nTherefore, we report the results of single runs of the methods.\n\n\\subsection{Results}\n\nTable~\\ref{tab:result} shows the results.\nThe proposed method achieved an accuracy of 85.85, a state-of-the-art performance.\nWe observed a significant performance improvement compared to methods that did not use external tools and methods that used only one external tool.\nNote that the performance improvement from using multiple external tools is larger than the sum of the performance improvements from using each tool individually.\nThis is because GPT-3 can fail to provide accurate answers due to a combination of different types of errors, such as incorrect arithmetic calculation and knowledge.\nThe use of multiple external tools addressed such cases effectively, thereby improving the overall accuracy.\n\n\\subsection{Case Study}\n\\label{subsection:casestudy}\nFigure~\\ref{fig:mis} shows an improved example.\nZero-Shot and Few-Shot generated wrong answers.\nZero-Shot+CoT and Few-Shot+CoT performed reasoning based on the incorrect molar mass of Al2(CO3)3, resulting in incorrect answers.\nBesides, Few-Shot+CoT failed to calculate $12 \\times 3 / 342 \\times 100$.\nOur method, MultiTool-CoT, was able to answer correctly based on correct knowledge and calculation, relying on external tools.\nMore examples are presented in Figure~\\ref{fig:mis3} and Figure~\\ref{fig:mis4} in Appendix.\n\nDespite the excellent results, there were 46 instances in which the proposed method failed to deliver accurate answers.\nUpon manual investigation of all the errors, we identified that the majority of them were caused by incorrect reasoning processes (39\\%) and invalid tool inputs (35\\%).\nThe remaining errors were categorized into incorrect gold answers (15\\%) and variations in answer formats (11\\%).\nExamples can be found in Appendix~\\ref{sec:error}.\nThese errors are beyond the scope of what external tools can assist with.\n\n\\section{Conclusion}\nWe proposed MultiTool-CoT, a framework that allows LLMs to use multiple external tools, such as a knowledge retriever and a calculator, during reasoning.\nWe applied MultiTool-CoT to a numerical reasoning task that requires knowledge of chemistry and confirmed its effectiveness.\nThe proposed framework is general and can be applied to various tasks by changing and extending external tools.\nWe plan to verify the effectiveness of the proposed method in other tasks in the future.\n\n\\section*{Limitations}\nThe major limitation of the present study is that the effectiveness of the proposed method has been confirmed only for a single task.\nThis is because most existing reasoning tasks are relatively simple that they can be solved by a single external tool at most.\nFor example, most existing numerical reasoning tasks provide self-contained questions; that is, all the required knowledge is included in the questions.\nIn such tasks, a calculator is all that is needed as an external tool.\nHowever, it would be rare for a single external tool to be sufficient in real-world applications such as medical text analysis.\nIt is crucial for future work to validate the effectiveness in such realistic scenarios that necessitate the use of multiple external tools.\n\n\\clearpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Efficient Tool Use with Chain-of-Abstraction Reasoning}\n\n\\begin{document}\n\n\\maketitle\n\\renewcommand{\\thefootnote}{\\fnsymbol{footnote}}\n\\footnotetext[1]{Work done during internship at FAIR.}\n\\footnotetext[2]{Equal Supervision.}\n\\renewcommand{\\thefootnote}{\\arabic{footnote}}\n\\begin{abstract}\nTo achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (\\eg{}, web facts, math and physical rules).\nTools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (\\eg{}, Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning.\n\nIn this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning.\nOur method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge.\nThis planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (\\eg{}, math results) relevant to different reasoning questions.\nIt also allows LLMs to perform decoding and calling of external tools in parallel, which avoids the inference delay caused by waiting for tool responses.\nIn mathematical reasoning and Wiki QA domains, we show that our method consistently outperforms previous chain-of-thought and tool-augmented baselines on both in-distribution and out-of-distribution test sets, with an average $\\sim6\\%$ absolute QA accuracy improvement.\nLLM agents trained with our method also show more efficient tool use, with inference speed being on average $\\sim$$1.4\\times$ faster than baseline tool-augmented LLMs.\n\\end{abstract}\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1.0\\columnwidth]{pics/method-decouple.pdf}\n\\caption{Overview of chain-of-abstraction reasoning with tools. Given a domain question (green scroll), a LLM is fine-tuned to first generate an abstract multi-step reasoning chain (blue bubble), and then call external tools to reify the chain with domain-specific knowledge (orange label). The final answer (yellow bubble) is obtained based on the reified chain of reasoning.}\n\\label{method_overview}\n\\end{figure}\n\n\\section{Introduction}\nRecent large language models (LLMs; \\citealp{touvron2023llama2,anil2023palm2,openai2023gpt4}), have made progress at interpreting and executing instructions \\citep{wei2021finetuned,chung2022scaling}, but still make errors when recalling and composing world knowledge for their responses, \\eg{}, making unfactual statements \\citep{maynez2020faithfulness,ji2023survey}, incorrect calculations \\citep{patel2021nlp}, etc. %Instances of erroneous or implausible responses frequently arise when LLMs are tasked with multi-step reasoning processes.\\footnote{Verified by our analysis in \\S\\ref{math_qa_exp}} \nUsing auxiliary tools (\\eg{}, a search engine to provide credible facts, a calculator for accurate math operations, etc.) at inference time can mitigate some of these errors, motivating tool-augmented language models that integrate external API calls into their output generations \\citep{parisi2022talm,schick2023toolformer,hao2023toolkengpt}.\n\nHowever, we show that current tool-augmented LLMs, \\eg{}, Toolformer \\citep{schick2023toolformer}, struggle to reliably and efficiently leverage tools in multi-step reasoning.\nIn particular, tool calls in multi-step reasoning tasks are often interleaved (\\ie{}, the response of an API call is often part of the query of a subsequent call; as shown in Figure~\\ref{method_overview}).\nWithout explicitly modeling these interconnections in reasoning chains, LLMs do not learn effective planning for tool use, which leads to less accurate reasoning with tools.\\footnote{as verified by our analysis in \\S\\ref{results_analysis}}\nMeanwhile, interleaving text generation with API calls also introduces inefficient inference ``waiting times,'' where the model must wait for the response from the API call before resuming the decoding process. This inefficiency becomes more significant in multi-step reasoning scenarios, when multiple rounds of API calls are typically required for each reasoning process.\n\nIn this work, we propose \\textbf{C}hain-\\textbf{o}f-\\textbf{A}bstraction (\\textbf{CoA}) reasoning, a robust and efficient method for LLMs to perform multi-step reasoning with tools.\nAs shown in Figure~\\ref{method_overview}, LLMs are fine-tuned with a goal of making reasoning chains with abstract placeholders.\nThe placeholders do not affect LLMs' reasoning flow, and are subsequently infilled with specific knowledge retrieved from specialized tools, to ground the final answer generations.\nPlanning abstract chain of reasoning encourages LLMs to inter-connect multiple tool calls and adopt more feasible reasoning strategies, which are robust to the variation of domain knowledge involved in each reasoning process, \\eg{}, specific calculation results.\nUnlike previous methods where LLM decoding and API calls are executed in an interleaved manner, our method leverages tools to infill knowledge \\textbf{once} after the whole chain of reasoning is generated.\nThis enables more efficient decoding across multiple examples (\\eg, as in a stream) because CoA traces for subsequent examples can be decoded while tool calls are made for the preceding ones, amortizing overall inference time.\nWe develop a simple pipeline to build fine-tuning data for models to learn CoA, where we first prompt LLMs to re-write existing responses to instructions as abstract chains, and then use domain tools to check the validity of re-writing, as shown in Figure~\\ref{method_rewrite}.\n\nAfter training LLMs to learn CoA reasoning, we evaluate the finetuned models on two representative multi-step reasoning domains, including mathematical reasoning \\citep{cobbe2021training,miao2020diverse,patel2021nlp,koncel2016mawps}, and Wikipedia (Wiki) QA \\citep{yang2018hotpotqa,berant2013semantic,kwiatkowski2019natural,joshi2017triviaqa} that involves reasoning on factual descriptive knowledge.\nWe show that our method boosts LLMs' performances, with average $\\sim$$7.5\\%$ and $4.5\\%$ absolute accuracy improvements on math and Wiki QA, respectively.\nThese improvements are consistent across both in-distribution\nand (zero-shot) out-of-distribution test sets, and are especially pronounced on questions that require complex chain-of-thought reasoning.\\footnote{\\eg{}, more than 3 steps of math derivations} \nMeanwhile, our method also uses tools more efficiently than previous augmentation methods, with average $\\sim$$1.47\\times$ and $1.33\\times$ faster inference speeds on math and Wiki QA tasks, respectively.\nFinally, extensive human evaluation demonstrates that our method guides LLMs to learn more accurate reasoning, which leads to $\\sim8\\%$ fewer reasoning errors.\n\n\\section{Related Work}\n\n\\paragraph{Tool-Augmented LLMs}\nThere is growing interest in augmenting LLMs using external tools.\nConsiderable work has tried to adapt LLMs as tool-using reasoners through in-context learning, demonstrating promising performance improvements in various applications, \\eg{}, math problem solving \\citep{gao2023pal,chen2022program}, biomedical question answering \\citep{jin2023genegpt} and self-critiquing \\citep{gou2023critic}.\nNevertheless, guiding LLMs to effectively use tools using in-context demonstrations is challenging, which requires elaborate task-specific prompt engineering and is restricted by the model's instruction following ability \\citep{jacovi2023comprehensive}. Noticing the limitations of in-context learning, several works teach LLMs to learn the usage of tools by fine-tuning \\citep{parisi2022talm,schick2023toolformer,hao2023toolkengpt}, which more robustly improves LLMs' performance.\nHowever, all above approaches adopt sequential interactions with tools throughout reasoning, slowing the inference speed as a function of the latency of the tool (or API) and the number of API calls that are made.\n\nSome other prior works focus on using LLMs for multi-step reasoning with other modules.\nIn particular, ReAct \\citep{yao2023react} and FireAct \\citep{chen2023fireact} integrate LLMs with tools into a closed loop of thought, action and observation steps.\nThis verbose reasoning loop slows down the LLM decoding, and still incorporates tools via sequential interactions, resulting in inefficient inference.\nAnother line of work, Program of Thoughts \\citep{chen2022program}, DECLARATIVE \\citep{he2023solving} and PAL \\citep{gao2023pal} prompt LLMs to generate program-based reasoning and interact with code executors, which however heavily rely on closed source coding models, \\ie{}, Codex \\citep{chen2021evaluating}, and are restricted to procedural arithmetic reasoning.\nBuilding on these works, CoA proposes a framework to convert natural language reasoning traces into abstract representations, and uses the abstract reasoning traces as fine-tuning data to improve tool-augmented LLMs.\nCoA also accelerates tool-augmented reasoning, by holistically planning the CoA traces and calling tools only once at inference time.\n\n\\paragraph{Tool Usage Planning}\nSeveral previous works research tool usage planning in LLMs.\nSpecifically, HuggingGPT \\citep{shen2023hugginggpt}, Chameleon \\citep{lu2023chameleon}, OpenAGI \\citep{ge2023openagi} and MetaTool \\citep{huang2023metatool} focus on planning the high-level sequence of using multiple tools to address multi-domain mixed tasks.\nSimilarly, LATM \\citep{cai2023large}, ML-BENCH \\citep{liu2023ml} and Gorilla \\citep{patil2023gorilla} aim at planning program-level integration of multiple APIs for designing scripts of procedural tasks, \\eg{}, a script for training a model described by a GitHub repository.\nToolChain* \\citep{zhuang2023toolchain} combines the planning of tool usage with tree-search-based reasoning \\citep{yao2023tree,hao2023reasoning}, which is especially useful for procedural tasks \\citep{xu2023tool,cobbe2021training}.\nDifferent from above work, we focus on the planning of general chain-of-thought \\citep{wei2022chain} reasoning with awareness of domain specialized tools.\n\n\\section{Method}\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1.0\\columnwidth]{pics/method-rewrite.pdf}\n\\caption{Illustration of gold data re-writing for fine-tuning data construction. Given a pair of domain question (green scroll) and gold answer (yellow scroll), an LLM is prompted to re-write the gold answer as a reasoning chain with abstract variables (purple bubble). Then, domain specialized tools validate the correctness of the re-writing by checking whether the abstract chain can be reified to get the final answer (orange label).\n}\n\\label{method_rewrite}\n\\end{figure}\n\n\\paragraph{Chain-of-Abstraction (CoA) Reasoning}\n\nOur method decouples the general reasoning of LLMs from domain-specific knowledge obtained from external tools.\nFigure~\\ref{method_overview} shows an overview of our method.\nIn particular, we first fine-tune LLMs to generate reasoning chains with abstract placeholders, \\eg{}, $y1$, $y2$ and $y3$,\\footnote{We also test placeholders in single-character format, \\eg{}, $x$, $y$ and $z$, but these led to sub-optimal results.} as shown in Figure~\\ref{method_overview}.\nIn the second stage, we reify each reasoning chain by replacing placeholders with domain-specific knowledge obtained from external tools, \\eg{}, calculation results from a calculator, relevant articles retrieved from web search engine, etc.\nFinally, the question is answered based on the reified reasoning chain.\n\nNote that since the LLMs are trained to generate abstract chain of reasoning instead of regular chain-of-thought (CoT) reasoning with explicit values, this enables LLMs to focus on learning general and holistic reasoning strategies without needing to generate instance-specific knowledge for the model's parameters.  \nMoreover, decoupling general reasoning and domain-specific knowledge enables LLM decoding to proceed and switch between different samples in parallel with API calling (via a pipeline), \\ie{}, LLM can start generating the next abstract chain while the tool fills the current chain, which speeds up the overall inference process.\n\n\\paragraph{Fine-tuning Data Construction}\n\nTo construct chain-of-abstraction (CoA) data for fine-tuning LLMs, we collect question answering (QA) samples from existing open-source QA datasets \\citep{cobbe2021training,miao2020diverse,yang2018hotpotqa}, and prompt LLaMa-70B \\citep{touvron2023llama} to re-write the answer of each sampled question, as shown in Figure~\\ref{method_rewrite}.\nSpecifically, we prompt LLaMa-70B to label the spans in gold answers that correspond to knowledge operations (\\eg{}, math derivations, statements based on Wikipedia references) and then to re-write the sentences with labeled spans as fillable CoA traces, where the operation results are replaced with abstract placeholders.\nFor example, the two derivations in the example in Figure~\\ref{method_rewrite} are re-written as ``[$20 + 35 = y1$]\" and ``[$90 - y1 = y2$]\", respectively.\n\nNote that an intermediate knowledge operation result may appear multiple times in an answer, \\eg{}, in Figure~\\ref{method_rewrite}, the first equation's result $55$ is used in the second equation.\nWe prompt LLaMa-70B to replace all occurrences of the same intermediate result with the same placeholder, thereby explicitly connecting the multiple reasoning steps.\nTo ensure that the re-written data is accurate, we use domain-specialized tools to verify the correctness of each CoA reasoning trace.\\footnote{Detailed implementations of reasoning chain verification are described in Sec.~\\ref{ss:math} and \\ref{ss:wikiqa}.}\nSpecifically, we use the tools to execute the labeled operations in each CoA, and only keep questions whose CoA can be infilled with valid results by the tools.\n\n\\section{Experimental Settings}\nWe conduct our experiments on two representative domains: mathematical reasoning and Wikipedia (Wiki) QA, which involves commonsense and logical reasoning on factual descriptive knowledge.\n\n\\subsection{Mathematical Reasoning}\n\\label{ss:math}\nGiven a math question, the QA system needs to generate a natural language solution to the problem with step-by-step arithmetic derivations (as demonstrated in the left column of Figure~\\ref{method_overview}).\nWe assume that the derivations involved in the solution are the specialized knowledge operations required in this domain, which are labeled in square brackets with derivation results being replaced by abstract placeholders, e.g., ``[$20 + 35 = y1$]\".\n\n\\paragraph{Datasets}\nWe construct most of our fine-tuning CoA data by re-writing the GSM8K \\citep{cobbe2021training} training set, which contains 7473 linguistically diverse grade school math problems.\nAs GSM8K dataset focuses on multi-step reasoning, it lacks coverage of single-step arithmetic problems, so we also re-write an additional set of 691 single-step math problems from the ASDiv \\citep{miao2020diverse} dataset.\nAcross these re-written datasets, we find that $\\sim76.6\\%$ of the CoA reasoning traces generated by LLaMa-70B are verified by our equation solver (described below). \nTable~\\ref{tab:ft-data-math} shows the reasoning step distribution (\\ie{}, number of derivations) of our constructed fine-tuning data.\n\n\\begin{table}[t]\n\\centering\n\\resizebox{1.0\\columnwidth}{!}{\n\\smallskip\\begin{tabular}{lccccccc}\n\\toprule\n\\multirow{2}*{\\textbf{Source}}  &  \\multicolumn{7}{c}{\\textbf{Reasoning Step}}   \\\\\n                                        \\cmidrule(lr){2-8}\n                                        &  1  &   2  &   3  &  4  &  5  &  $>$5 & All \\\\\n\\toprule\nGSM8K               &  8  & 1540 & 1648 & 1164 & 666  & 553 & 5579 \\\\\nASDiv               & 677 &   0  &   0  &   0  &   0  &  0  & 677  \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Reasoning step distribution of correctly re-written reasoning chains in math domain.}\n\\label{tab:ft-data-math}\n\\end{table}\n\nFor an in-distribution evaluation, we test models on GSM8K and ASDiv, containing 1319 and 2305 testing problems.\nTo further test the models' generalization ability, we also conduct zero-shot evaluation on other representative math datasets, including SVAMP \\citep{patel2021nlp} and MAWPS \\citep{koncel2016mawps}, which contain 1000 and 2065 testing samples, respectively.\\footnote{For the MAWPS benchmark, we test on the 395, 508, 562 and 600 math problems from AddSub, SingleEq, SingleOp and MultiArith portions, respectively.}\n\n\\paragraph{Domain Tool}\nWe use an equation solver to perform the arithmetic derivations required in the math domain.\nOur equation solver first extracts the derivations labeled in the CoA reasoning, e.g., ``[$20 + 35 = y1$]\" and ``[$90 - y1 = y2$]\", and combines all derivations into a system of equations.\nThen the system of equations is solved by the SymPy toolkit,\\footnote{\\url{https://www.sympy.org/en/index.html}} to get the true value of each variable (\\ie{}, the value of the abstract placeholder).\nFinally, our equation solver returns the reified chain of reasoning by replacing all the variables with their solved true values (including the final answer).\n\n\\begin{table}[t]\n\\centering\n\\resizebox{1.0\\columnwidth}{!}{\n\\smallskip\\begin{tabular}{ll}\n\\toprule\n\\multirow{2}*{\\textbf{Question}} & The director of the romantic comedy ``Big Stone Gap'' is based in \\\\\n& what New York city? \\\\\n\\midrule\n\\textbf{Answer} & Greenwich Village \\\\\n\\midrule\n\\multirow{3}*{\\textbf{Wikipedia}} & Big Stone Gap (film) > Big Stone Gap is a 2014 American romantic \\\\\n\\multirow{3}*{\\textbf{References}} & comedy film directed by Adriana Trigiani. \\\\\n & Adriana Trigiani > Adriana Trigiani is an Italian American film \\\\\n & director based in Greenwich Village. \\\\\n\\midrule\n\\multirow{3}*{\\textbf{CoA Trace}} & Find the [director of romantic comedy ``Big Stone Gap'' -Wiki-> y1]. \\\\\n& The name of this film's director is [y1 -NER(person)-> y2]. \\\\\n& Then determine [y2 in what New York city -Wiki-> y3]. \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Example of CoA fine-tuning data construction in Wiki QA domain.}\n\\label{tab:rewrite_example_wiki}\n\\end{table}\n\n\\subsection{Wikipedia QA}\n\\label{ss:wikiqa}\nGiven a question based on Wikipedia knowledge, the model needs to first identify Wikipedia articles as references related to the question, and then reason on key knowledge in the reference articles to answer the question (as shown in the right column of Figure~\\ref{method_overview}).\nWe assume that the specialized knowledge operation in this domain is the retrieval of relevant Wikipedia articles and important named-entities, which are re-written as Wikipedia searching (WikiSearch) and named-entity recognition (NER)\\footnote{We use NER to extract entities from the article that bridge the former WikiSearch results to the latter WikiSearch queries.} queries.\nTable~\\ref{tab:rewrite_example_wiki} shows an example of a re-written CoA trace for Wiki QA.\n\n\\paragraph{Datasets}\nWe use the HotpotQA \\citep{yang2018hotpotqa} dataset to construct our fine-tuning CoA data in the Wiki QA domain.\nHotpotQA contains 113K multi-hop QA examples, each labeled with two Wikipedia articles that provide supporting knowledge.\nAmong the 90447 training QA pairs, we identify 72991 as \\textbf{Bridge} QA pairs, where an intermediate entity must be identified to link the answer to the question, as shown in Table~\\ref{tab:rewrite_example_wiki}.\nThe remaining 17456 are \\textbf{Comparison} QA pairs, where the attributes of two entities are compared, \\eg{}, ``Are Randal Kleiser and Kyle Schickner of the same nationality?''.\nWe prompt LLaMa-70B to re-write these training QAs into CoAs with WikiSearch and NER queries, and verify each CoA with our domain tools (described below), by checking whether all the articles returned by the WikiSearch queries match one of the titles in the gold articles.\nFinally, 8956 Bridge QAs and 5405 Comparison QAs are used as fine-tuning data, whose re-written CoAs pass the verification.\\footnote{Compared to mathematical reasoning, generating CoA data for Wiki QA requires more complex tool use that combines WikiSearch and NER models, leading to a lower re-writing success rate ($\\sim15.9\\%$).}\nFor Wiki QA, we note that besides training a LLM to produce CoA data using WikiSearch, we also fine-tune a second LLM to learn to generate the final gold answer based on a correctly reified CoA reasoning trace.\n\nWe evaluate models on the HotpotQA development set, which contains 5918 Bridge QA pairs and 1487 Comparison QA pairs. Similar to the mathematical reasoning domain, we also conduct zero-shot evaluation on other open-domain QA datasets: WebQuestions (WQ; \\citealp{berant2013semantic}), NaturalQuestions (NQ; \\citealp{kwiatkowski2019natural}) and TriviaQA \\citep{joshi2017triviaqa}, which contain 2032, 3610 and 17944 test questions, respectively.\n\n\\paragraph{Domain Tools}\nThe specialized tools required for Wiki QA include a Wikipedia search engine to retrieve reference articles, and a NER toolkit to extract entities that bridge multi-step searching queries.\nWe follow Toolformer \\citep{schick2023toolformer} and implement a Wikipedia search engine as a BM25 retriever \\citep{robertson1995okapi,baeza1999modern} that indexes the Wikipedia dump from the KILT benchmark \\citep{petroni2021kilt}.\nWe use the BM25 retriever to search the top-10 articles relevant to the input query, and then re-rank the articles based on their Sentence-BERT \\citep{reimers2019sentence} embedding cosine similarity with the question.\nAfter re-ranking, the top-$1$ article is selected to be the final search result.\n\nWe use SpaCy\\footnote{\\url{https://spacy.io/models/en}} (\\texttt{en\\_core\\_web\\_sm}) as the NER toolkit to extract named entities.\nTo simplify NER, we aggregate the numerous SpaCy NER types into 6 general classes, as shown in Table~\\ref{tab:ner_aggr}.\nIf multiple named entities are recognized, we input each recognized entity to the subsequent WikiSearch query, and select the entity whose subsequent search result has the highest Sentence-BERT embedding cosine similarity with the question.\n\n\\begin{table}[t]\n\\centering\n\\resizebox{1.0\\columnwidth}{!}{\n\\smallskip\\begin{tabular}{cl}\n\\toprule\n\\textbf{General}  &  \\multirow{2}*{\\textbf{SpaCy NER Types included in each General Class}}   \\\\\n\\textbf{Class}  &  \\\\\n\\toprule\nperson  & PERSON  \\\\\ngroup  & NORP, ORG, LANGUAGE \\\\\nlocation  & GPE, FAC, LOC \\\\\nculture & EVENT, WORK\\_OF\\_ART, LAW, PRODUCT \\\\\ndate  & DATE, TIME \\\\\nnumeral & CARDINAL, PERCENT, MONEY, QUANTITY, ORDINAL \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Aggregation of SpaCy NER types.}\n\\label{tab:ner_aggr}\n\\end{table}\n\n\\begin{table*}[t]\n\\centering\n\\resizebox{1.0\\textwidth}{!}{\n\\smallskip\\begin{tabular}{ll@{~}ccccccccc}\n\\toprule\n\\multirow{2}*{\\textbf{Model}} & \\multirow{2}*{\\textbf{Method}} & \\multirow{1.8}*{\\textbf{Use}} & \\multirow{2}*{\\textbf{GSM8K}} & \\multirow{2}*{\\textbf{ASDiv}} & \\multirow{2}*{\\textbf{SVAMP}} & \\multicolumn{5}{c}{\\textbf{MAWPS}} \\\\\n            \\cmidrule(lr){7-11}\n &  & \\textbf{Tool} &  &  &  & \\textbf{AddSub} & \\textbf{SingleEQ} & \\textbf{SingleOp} & \\textbf{MultiArith} & \\textbf{All} \\\\\n\\toprule\n\\multirow{4}*{\\textbf{LLaMa-2}} & CoT-FSP & \\multirow{2}*{\\xmark} & 16.38 & 47.85 & 38.40 & 52.41 & 63.39 & 82.03 & 43.33 & 60.53 \\\\\n\\multirow{4}*{\\textbf{-7B}} & CoT-FT &  & \\underline{35.33} & \\underline{57.18} & \\underline{48.20} & \\underline{66.08} & \\underline{74.41} & \\underline{85.23} & \\underline{65.00} & \\underline{73.03}  \\\\\n            \\cmidrule(lr){2-11}\n            & Toolformer & \\multirow{2}*{\\cmark} & 17.59 & 48.55 & 37.10 & 47.34 & 58.46 & 79.54 & 50.67 & 59.81 \\\\\n            & CoA  &  & \\textbf{37.83}$^{\\ast}$ & \\textbf{57.61} & \\textbf{51.70}$^{\\ast}$ & \\textbf{72.15}$^{\\ast}$ & \\textbf{82.48}$^{\\ast}$ & \\textbf{86.48}$^{\\ast}$ & \\textbf{73.17}$^{\\ast}$ & \\textbf{78.89}$^{\\ast}$ \\\\\n\\midrule\n\\multirow{5}*{\\textbf{LLaMa-2}} & CoT-FSP  & \\multirow{3}*{\\xmark} & 24.03 & 54.14 & 51.30 & \\underline{71.90} & 72.44 & 85.41 & 74.00 & 76.32 \\\\\n\\multirow{5}*{\\textbf{-Chat-7B}}  & CoT-FT &  & 35.41 & 59.00 & 46.90 & 58.23 & 72.24 & 85.41 & 73.00 & 73.37  \\\\\n  & CoA (no Tool) &  & 35.03 & 58.79 & \\underline{51.50} & 68.10 & \\underline{74.21} & \\underline{86.48} & \\underline{77.67} & \\underline{77.38} \\\\\n  \\cmidrule(lr){2-11}\n  & Toolformer  & \\multirow{3}*{\\cmark} & 23.65 & 50.85 & 48.80 & 61.01 & 69.09 & 81.85 & 68.50 & 70.85  \\\\\n  & Toolformer - Math &  & \\underline{36.01} & \\underline{59.18} & 47.60 & 58.99 & 72.44 & 85.94 & 75.50 & 74.43 \\\\\n & CoA  &  & \\textbf{38.29}$^{\\ast}$ & \\textbf{59.57} & \\textbf{54.20}$^{\\ast}$ & \\textbf{72.41} & \\textbf{81.89}$^{\\ast}$ & \\textbf{88.26}$^{\\ast}$ & \\textbf{83.00}$^{\\ast}$ & \\textbf{82.13}$^{\\ast}$ \\\\\n\\midrule\n\\multirow{4}*{\\textbf{LLaMa-2}} & CoT-FSP  & \\multirow{2}*{\\xmark} & 56.18 & 65.94 & 70.60 & 86.08 & 89.17 & \\underline{92.88} & 84.50 & 88.23 \\\\\n\\multirow{4}*{\\textbf{-Chat-70B}} & CoT-FT  &  & 60.50 & 70.24 & 70.40 & 81.52 & 87.60 & 92.35 & 89.17 & 88.18  \\\\\n  \\cmidrule(lr){2-11}\n                                 & Toolformer & \\multirow{3}*{\\cmark} & 52.54 & 69.07 & \\textbf{73.60} & \\textbf{86.84} & 89.76 & 91.46 & 81.50 & 87.26 \\\\\n             & Toolformer - Math &  & \\underline{61.03} & \\underline{70.59} & 73.20 & 85.57 & \\underline{91.34} & 91.99 & \\underline{92.00} & \\underline{90.60} \\\\\n            & CoA  &  & \\textbf{62.32}$^{\\ast}$ & \\textbf{71.89}$^{\\ast}$ & \\underline{73.40} & \\underline{86.33} & \\textbf{94.49}$^{\\ast}$ & \\textbf{93.06} & \\textbf{92.33} & \\textbf{91.91}$^{\\ast}$ \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Evaluation results on LLaMa-2 and LLaMa-2-Chat for mathematical reasoning. ``All'' denotes the averaged results on four MAWPS portions. Exact match rate to the final gold answer (\\ie{}, accuracy) is reported. \nFor each base model, the best and second-best results are \\textbf{bolded} and \\underline{underlined}, respectively. The best results labeled with $^{\\ast}$ are significantly better than their corresponding second-best results, with the significant test p-value $< 0.05$.\n}\n\\label{tab:math_results}\n\\end{table*}\n\n\\subsection{Baselines}\n\nWe apply our \\textbf{CoA} reasoning method to both 7B and 70B LLaMa models, and test various model versions including the first version of LLaMa \\citep{touvron2023llama} and the more advanced LLaMa-2 and LLaMa-2-Chat \\citep{touvron2023llama2}.\nWe compare our method to several baselines, including: a) few-shot prompting using 8 randomly sampled QA exemplars from the original (\\ie{}, not re-written) chain-of-thought data (\\textbf{CoT-FSP}), b) fine-tuning with original chain-of-thought data (\\textbf{CoT-FT})\\footnote{Note that in Wiki QA domain, the HotpotQA data used for prompting or fine-tuning baselines is pre-processed to contain both gold Wikipedia articles (serving as chain-of-thought explanations) and the final answer.}, and c) \\textbf{Toolformer} \\cite{schick2023toolformer} which fine-tunes LLMs on CCNet \\citep{wenzek2020ccnet} texts augmented with API calls.\nFor evaluation on Wiki QA, we also compared our method with \\textbf{FireAct} \\citep{chen2023fireact}, which fine-tunes LLMs on HotpotQA ReAct \\citep{yao2023react} trajectories distilled from GPT-4 \\citep{openai2023gpt4}.\n\n\\section{Results and Analysis}\n\\label{results_analysis}\n\\subsection{Mathematical Reasoning}\n\\label{math_qa_exp}\nTable~\\ref{tab:math_results} shows the evaluation results for the LLaMa-2 and LLaMa-2-Chat models.\\footnote{We include similar evaluation results for the original LLaMa model (7B) in Appendix \\ref{apdx:full_results}.}\nOn the GSM8K and ASDiv datasets, our CoA method outperforms the few-shot baseline CoT-FSP and the regular fine-tuning baseline CoT-FT, demonstrating that CoA fine-tuning with tool augmentation is more effective in adapting LLMs to multi-step reasoning tasks.\nSimilarly, when evaluated on out-of-distribution datasets, SVAMP and MAWPS, CoA also consistently outperforms the baselines.\nInterestingly, for these out-of-distribution datasets, CoT-FT lags further behind CoA, particularly for 7B models, showing that CoA reasoning yields more distributionally robust reasoning performance.\n\nOur CoA method also surpasses the tool-augmented baseline Toolformer, which implies that planning the abstract variables in CoA can improve the accuracy of reasoning with tools.\nHowever, as Toolformer is not originally trained with in-domain fine-tuning data,\\footnote{Toolformer is fine-tuned on CCNet data, which may not contain rich mathematical reasoning samples.} we also fine-tune a new version of Toolformer with the chain-of-thought data from GSM8K and ASDiv, denoted as \\textbf{Toolformer - Math} in Table~\\ref{tab:math_results}.\nWe also observe that CoA performs better than Toolformer - Math, confirming that the introduction of abstract variables enables more robust tool use compared to direct integration of API calls within chain-of-thought reasoning.\n\n\\paragraph{Ablation Study}\nWe verify that the robust generalization performance of our CoA method does not merely benefit from using additional tools, by fine-tuning another LLM to solve the equation (from the same model backbone), rather than calling the equation solver, denoted as \\textbf{CoA (no Tool)} in Table~\\ref{tab:math_results}. \nWe find that CoA (no Tool) performs consistently worse than CoA across all datasets, confirming that using specialized tools enables LLM agents to conduct more precise operations, rather than directly solving the same operations.\nHowever, CoA (no Tool) still outperforms all baseline methods on zero-shot generalization to SVAMP and MAWPS datasets, implying that learning abstract reasoning chains also contributes to better robustness of CoA, perhaps due to better planning of multiple reasoning steps indexed by abstract variables.\n\n\\paragraph{Reasoning Steps}\nOur findings suggest that the benefits of chain-of-abstraction reasoning are most pronounced when problems require long reasoning chains to be solved. Figure~\\ref{math_heatmap} shows the stratified performance of three models on GSM8K QA, relative to the number of reasoning steps in the predicted and gold reasoning chains. \nCompared to the few-shot CoT-FSP, CoA produces reasoning chains that more often match the length of the gold reasoning chains, as reflected by the heat-map statistics (left column) being more aggregated around the diagonal (comparable to CoT-FT).\nAt the same time, we observe that models achieve better QA accuracy when the number of reasoning steps in their generated answers are aligned with the gold references (\\ie{}, the diagonal of heat-maps in right column).\nAbove results show that fine-tuned models are better at learning to produce reasoning chains that match the true reasoning chain for the problem.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1.0\\columnwidth]{pics/math-heatmap.pdf}\n\\caption{GSM8K evaluation results on LLaMa-2-Chat-7B \\wrt{} the number of reasoning steps in the predicted and gold reasoning chain. (Left) The number of test examples that belong to each stratum. (Right) The corresponding model accuracy (\\%) for those examples. Non-diagonal cells with fewer than 15 examples are ignored.}\n\\label{math_heatmap}\n\\end{figure}\n\n\\begin{table}[t]\n\\centering\n\\resizebox{0.75\\columnwidth}{!}{\n\\begin{tabular}{lcc}\n\\toprule\n\\multirow{2}*{\\textbf{Method}}  &  \\multicolumn{2}{c}{\\textbf{Error Rate}}   \\\\\n                                \\cmidrule(lr){2-3}\n                                &  Arithmetic  &   Reasoning   \\\\\n\\toprule\nCoT-FSP         & 17.3 & 70.3   \\\\\nCoT-FT          & 25.2 & 67.8   \\\\\n\\midrule\nCoA & \\textbf{0.0} & \\textbf{60.4} \\\\ % the illness case of variable misuse is also labeld to have reasoning errors, so 60.4 is not changed.\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Human evaluation results of arithmetic and reasoning error rates on 200 GSM8K test samples. Models developed based on LLaMa-2-Chat-7B are presented.}\n\\label{tab:human_eval}\n\\end{table}\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1.0\\columnwidth]{pics/efficiency_lines.pdf}\n\\caption{Wall-clock inference time on GSM8K (seeded with LLaMa-2-Chat-7B). Average time of answering a question is measured (in seconds) \\wrt{} the number of gold reasoning steps required for the question.}\n\\label{efficiency_lines}\n\\end{figure}\n\n\\begin{table}[t]\n\\centering\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{Method}  &  \\textbf{Accuracy}   \\\\\n\\toprule\nCoT-FSP       & 27.90 \\\\\nCoT-FT        & 39.12 \\\\\nToolformer    & 24.56 \\\\\nToolformer - Math & 35.25 \\\\\nCoA & \\textbf{40.79} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\nEvaluation results on GSM8K with self-consistency decoding (seeded with LLaMa-2-Chat-7B). Each model uses majority voting to aggregate the answers of 16 sampled reasoning chains\n}\n\\label{tab:self_consistency}\n\\end{table}\n\n\\begin{table*}[t]\n\\centering\n\\resizebox{1.0\\textwidth}{!}{\n\\smallskip\\begin{tabular}{llcccccccc}\n\\toprule\n\\multirow{2}*{\\textbf{Model}} & \\multirow{2}*{\\textbf{Method}} & \\multirow{1.8}*{\\textbf{Use}}  & \\multicolumn{4}{c}{\\textbf{HotpotQA}} & \\multirow{2}*{\\textbf{WQ}} & \\multirow{2}*{\\textbf{NQ}} & \\multirow{2}*{\\textbf{TriviaQA}} \\\\\n            \\cmidrule(lr){4-7}\n & & \\textbf{Tool} & \\textbf{Bridge} & \\textbf{Comparison} & \\textbf{Both} & \\textbf{Time} &  &  &   \\\\\n\\toprule\n\\multirow{6}*{\\textbf{LLaMa-2}}  & CoT-FSP  & \\multirow{2}*{\\xmark} & 11.69 & 45.46 & 18.47 & 2.074 & 34.65 & 30.91 & 53.48 \\\\\n\\multirow{6}*{\\textbf{-Chat-7B}} & CoT-FT   &  & 14.24 & \\underline{56.69} & 22.77 & \\underline{1.937} & 33.51 & 25.40 & 51.05 \\\\\n    \\cmidrule(lr){2-10}\n                                  & Toolformer  & \\multirow{4}*{\\cmark} & 12.99 & 44.59 & 20.00 & 2.350 & \\underline{36.22} & 30.22 & 54.15 \\\\\n                                  & Toolformer - Wiki &  & 15.68 & 56.42 & 23.86 & 2.301 & \\textbf{36.61} & 32.96 & \\underline{55.08} \\\\\n                                  & FireAct   &  & \\underline{19.18} & 54.14 & \\underline{26.20} & 2.706 & 36.02 & \\underline{35.87} & 52.96 \\\\\n                                & CoA  &  & \\textbf{21.00}$^{\\ast}$ & \\textbf{56.96} & \\textbf{28.22}$^{\\ast}$ & \\textbf{1.896} & 35.97 & \\textbf{38.67}$^{\\ast}$ & \\textbf{57.90}$^{\\ast}$ \\\\\n\\midrule\n\\multirow{4}*{\\textbf{LLaMa-2}}  & CoT-FSP   & \\multirow{2}*{\\xmark} & 21.39 & 56.62 & 28.47 & 6.668 & 34.89 & 37.42 & 63.61 \\\\\n\\multirow{4}*{\\textbf{-Chat-70B}} & CoT-FT   &  & 23.84 & \\underline{63.95} & 31.90 & \\underline{6.401} & 34.15 & 39.75 & 62.28 \\\\\n    \\cmidrule(lr){2-10}\n                                & Toolformer & \\multirow{3}*{\\cmark} & 22.24 & 56.09 & 29.04 & 6.888 & \\underline{37.16} & 40.42 & 64.31 \\\\\n                                & Toolformer - Wiki &  & \\underline{26.38} & 63.82 & \\underline{33.90} & 6.855 & \\textbf{37.70} & \\underline{41.25} & \\underline{66.64} \\\\\n                                & CoA &  & \\textbf{27.61}$^{\\ast}$ & \\textbf{64.09} & \\textbf{34.94}$^{\\ast}$ & \\textbf{6.369} & 36.37 & \\textbf{43.57}$^{\\ast}$ & \\textbf{69.08}$^{\\ast}$ \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Wiki QA evaluation results on LLaMa-2-Chat-based models. ``Both'' denotes the overall evaluation results on both bridge and comparison portions of HotpotQA. ``Time'' denotes the average seconds that each agent needs to answer a question in HotpotQA. Exact match rate to the final gold answer (\\ie{}, accuracy) is reported.\nFor each base model, the best and second-best results are \\textbf{bolded} and \\underline{underlined}, respectively. The best results labeled with $^{\\ast}$ are significantly better than their corresponding second-best results, with the significant test p-value $< 0.05$.\n}\n\\label{tab:wiki_results}\n\\end{table*}\n\nInterestingly, we find that CoA, compared to CoT-FT, achieves higher performance especially on questions that require more reasoning steps.\nIn the right column of Figure~\\ref{math_heatmap}, CoA's improvement over CoT-FT is more pronounced on questions with more than $3$ steps in the gold reasoning chain (highlighted with red squares). \nThis indicates that the model trained with CoA has more robust long chain-of-thought reasoning capability, which is learned from planning with abstractions.\n\n\\paragraph{Human Evaluation}\nTo more comprehensively verify that CoA improves both knowledge operation (\\ie{}, arithmetic by using tools) and reasoning accuracy, we conduct a human evaluation on different model answers to 200 randomly sampled GSM8K test questions. \nSpecifically, given a GSM8K question and a model's answer to the question, we ask human workers to judge whether the answer contains any arithmetic errors (\\eg{}, wrong calculations, invalid equations) or reasoning errors unrelated to math derivations (\\eg{}, misunderstanding of the question, improper strategy for solving the question), and report how often the model makes these two kinds of errors.\nIn Table~\\ref{tab:human_eval}, we find that CoA effectively reduces arithmetic errors to zero, due to the use of equation solver to perform accurate calculations.\nMore importantly, our method also makes fewer reasoning errors compared to the baselines, verifying that CoA fine-tuning guides the model to learn more accurate reasoning through the holistic planning of abstract reasoning chains.\nBy contrast, ordinary fine-tuning (\\ie, CoT-FT) produces a more limited reasoning improvement compared to the few-shot CoT-FSP, while also failing to suppress arithmetic errors.\n\n\\paragraph{Inference Efficiency}\nImportantly, we find that the performance benefits of CoA reasoning do not come with increased computational costs.\nIn Figure~\\ref{efficiency_lines}, we show the average time (seconds) that CoA and baseline agents (seeded with LLaMa-2-Chat-7B) needs to answer a question \\wrt{} required gold reasoning steps.\nCompared to the CoT baselines, CoA requires less time than the few-shot baseline CoT-FSP, whose generation needs to be conditioned on additional examples.\nHowever, CoA is slightly less inference-efficient compared to CoT-FT, likely due to the decoding of additional tokens (\\eg{}, ``['' and ``]'') for the abstract statements.\n\nCompared to Toolformer, CoA has a lower and flatter inference time curve, indicating better scaling as the number of reasoning steps increases.\nThis difference arises because CoA decouples the generation of (abstract) reasoning chains from the retrieval of knowledge (\\ie{}, tool use), allowing full reasoning chains to be decoded before any tool is called.\nThis procedure amortizes inference costs in two ways. \nFirst, tool calls are made after the CoA trace has been decoded, enabling parallel tool calls for the same trace (\\eg, using an equation solver once rather than multiple calls to a calculator), and avoiding the time delay caused by waiting for external API responses. Consequently, the model fine-tuned with CoA is more efficient at multi-step reasoning, especially when the number of reasoning steps (\\ie{}, tool calls) increases.\nSecond, across multiple examples, the model can generate the CoA trace of the next example while tool calls are made for the preceding one, parallelizing CoA decoding and tools calls across examples.\n\n\\paragraph{Self-Consistency Decoding}\nBesides of greedy decoding, we also test more advanced inference strategy, \\ie{}, self-consistency \\citep{wang2022self} decoding, on our CoA reasoning method.\nWe test all methods on the GSM8K dataset seeded with LLaMa-2-Chat-7B.\nEach method samples 16 reasoning chains and uses majority voting to aggregate the 16 answers derived by the reasoning chains, to get the final answer.\nFor the hyperparameters of sampling, we set the temperature, top-k and top-p as 1.0, 40 and 0.5, respectively.\nTable~\\ref{tab:self_consistency} shows our evaluation results.\nWe find that our CoA method consistently outperforms all baseline methods when shifting from greedy decoding to self-consistency decoding.\nThis shows that our method also has better potential to be generalized to different LLM decoding schemes.\n\n\\subsection{Wiki QA}\n\\label{wiki_qa_exp}\nTable~\\ref{tab:wiki_results} shows our Wiki QA results using LLaMa-2-Chat models.\\footnote{We include similar evaluation results on LLaMa-2-7B in Appendix \\ref{apdx:full_results}.}\nSimilar to mathematical reasoning, we fine-tune a new version of Toolformer with in-domain chain-of-thought data from HotpotQA, denoted as \\textbf{Toolformer - Wiki}.\nOn HotpotQA, CoA achieves higher exact match rates with the gold reference compared to the few-shot or fine-tuning baselines.\nIn particular, CoA outperforms all baselines on the more challenging bridge-type QAs, where two steps of reasoning over Wikipedia knowledge are \\textit{consecutively} entangled, \\ie{}, cannot be performed independently in parallel as in comparison-type QAs. \nCompared to FireAct fine-tuning, CoA also achieves better performance on both bridge and comparison QAs, without requiring data distilled from closed source GPT-4.\n\nAs with mathematical reasoning, CoA agents also perform more efficient inference than Toolformer and FireAct agents when answering HotpotQA questions.\nWe also find that CoA is more efficient (\\textbf{Time} column) than both CoT-FSP and CoT-FT, as CoA does not require few-shot examples as additional inputs and does not need to generate long Wiki articles, which are instead provided by the search engine.\nFinally, CoA improves over the baseline methods in zero-shot generalization experiments on other Wiki QA datasets, outperforming all baselines on NaturalQuestions and TriviaQA, and matching the best baselines on WebQuestions.\n\n\\section{Conclusion}\nIn this work, we propose to decouple the general reasoning of LLM agents from specialized knowledge obtained via external tools.\nOur method, chain-of-abstraction (CoA), encourages LLMs to learn the planning of abstract multi-step reasoning, which are more robust to out-of-distribution knowledge shifts.\nCoA also achieves a more efficient pipeline for tool usage that significantly improves the speed of tool-augmented multi-step reasoning.\nThe simple, yet effective, implementations of our method on two diverse tasks (\\ie{}, math reasoning and open-domain QA) demonstrate its potential for being adapted to new reasoning scenarios.\n\n\\section*{Limitations}\nWe acknowledge a few limitations in our work.\nFirst, datasets used for testing our method cannot have exhaustive coverage of all real-world reasoning scenarios.\nWe instead consider two representative reasoning domains, \\ie{}, mathematical reasoning and general open-domain (Wikipedia) QA, and use English as a primary language in our testing.\nFurthermore, our method is tested on the setting of fine-tuning the full LLMs, which requires considerable computational resources, while more efficient model training schemes, \\eg{}, LoRA \\citep{hu2021lora}, can be applied in future work.\n\n\\section*{Acknowledgements}\nWe thank Beatriz Borges, Gail Weiss, Syrielle Montariol, Li Mi and Zeming Chen for reading and providing comments on drafts of this paper.\nAntoine Bosselut gratefully acknowledges the support of the Swiss National Science Foundation (No. 215390), Innosuisse (PFFS-21-29), the EPFL Science Seed Fund, the EPFL Center for Imaging, Sony Group Corporation, and the Allen Institute for AI.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2305.14323v3.tex",
        "arXiv-2305.16896v1.tex",
        "arXiv-2401.17464v3.tex"
    ],
    "group_id": "group_61",
    "response": "### Title: Enhancing Large Language Models' Reasoning Abilities through Tool-Augmented Chain-of-Thought Prompting\n\n### Introduction\n\nThe field of large language model (LLM) reasoning has seen significant advancements in recent years, with models like GPT-4 and LLaMA demonstrating remarkable capabilities in a variety of tasks. However, despite their impressive few-shot and zero-shot learning abilities, LLMs still face challenges in handling complex reasoning tasks that require specific knowledge and multi-step logical processes. These tasks often involve intricate mathematical problems or multi-hop question answering scenarios, where the models struggle due to limitations in their intrinsic knowledge and reasoning capacities. To address these issues, researchers have explored various strategies, including chain-of-thought (CoT) prompting and the integration of external tools. CoT prompting guides LLMs to generate intermediate reasoning steps, thereby improving their logical reasoning process. However, this approach can be hindered by the need for accurate arithmetic calculations and information retrieval, which are not always within the model's capabilities. Therefore, integrating external tools has emerged as a promising method to enhance the reasoning abilities of LLMs. Yet, existing methods of tool integration often disrupt the continuous reasoning process or require complex planning and execution steps, leading to potential inefficiencies and errors.\n\nThis summary focuses on three recent research papers that propose innovative frameworks to improve the reasoning abilities of LLMs through tool augmentation and CoT prompting. The first paper introduces ChatCoT, a tool-augmented CoT reasoning framework for chat-based LLMs, such as ChatGPT. The second paper presents MultiTool-CoT, a framework that enables LLMs to use multiple external tools during reasoning. The third paper introduces Chain-of-Abstraction (CoA) reasoning, which decouples general reasoning from domain-specific knowledge operations, allowing for more efficient and robust tool usage. Each paper addresses the limitations of traditional CoT prompting and tool integration methods, offering novel solutions to enhance the performance of LLMs in complex reasoning tasks.\n\n### Main Content of Each Paper\n\n#### ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models\n\nChatCoT is designed to improve the reasoning abilities of chat-based LLMs, such as ChatGPT, by integrating tool manipulation into the CoT reasoning process. The framework leverages the multi-turn conversation capabilities of chat-based LLMs, allowing for a more natural and continuous interaction between the model and external tools. ChatCoT initializes the conversation with background knowledge about tools, tasks, and reasoning formats, providing a conversational knowledge memory that supports the subsequent reasoning steps. The model then iterates through a tool-augmented reasoning step, where it can freely interact with tools or continue reasoning based on the context. This iterative process continues until the final answer is derived, ensuring that the reasoning process remains coherent and uninterrupted.\n\nThe authors evaluate ChatCoT on two complex reasoning datasets: MATH and HotpotQA. MATH consists of challenging mathematical problems, while HotpotQA involves multi-hop question answering tasks. ChatCoT demonstrates significant improvements over existing baselines, achieving a 7.9% relative improvement on average accuracy for MATH. The framework also integrates well with other strategies, such as self-consistency, further enhancing its effectiveness. The key innovation of ChatCoT lies in its ability to maintain the continuity of the reasoning process by allowing LLMs to interact with tools at any step without interrupting the overall flow of reasoning.\n\n#### MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting\n\nMultiTool-CoT builds upon the idea of using external tools to enhance the reasoning capabilities of LLMs, specifically focusing on GPT-3. Unlike previous methods that typically involve a single tool, MultiTool-CoT enables LLMs to use multiple tools in a single reasoning process. The framework prompts the LLM to generate reasoning steps and tool triggers, allowing it to select and use appropriate tools as needed. This approach is particularly effective for tasks that require both numerical reasoning and domain-specific knowledge, such as solving complex mathematical problems or answering questions that involve multiple sources of information.\n\nThe authors implement MultiTool-CoT on the Task 2 dataset of NumGLUE, which requires both numerical reasoning and chemical knowledge. They use a calculator, an equation solver, and a retriever to support the reasoning process. The results show that MultiTool-CoT achieves state-of-the-art performance, outperforming baselines by a significant margin. The key advantage of MultiTool-CoT is its ability to handle multiple tools in a unified manner, reducing the likelihood of errors that can arise from sequential tool interactions. This method also addresses the challenge of frequent tool use in complex reasoning tasks, ensuring that the LLM can seamlessly integrate tool outputs into its reasoning process.\n\n#### Efficient Tool Use with Chain-of-Abstraction Reasoning\n\nThe Chain-of-Abstraction (CoA) reasoning method proposed in this paper aims to improve the efficiency and robustness of tool-augmented reasoning in LLMs. CoA decouples the general reasoning process from domain-specific knowledge operations, allowing the LLM to generate abstract reasoning chains that can be later filled with specific knowledge from external tools. This approach enables the LLM to focus on learning general reasoning strategies without being burdened by the need to generate specific knowledge for each instance. The method also speeds up the inference process by allowing the LLM to decode reasoning chains in parallel with tool calls, thereby reducing the waiting time for API responses.\n\nThe authors evaluate CoA on mathematical reasoning tasks and Wikipedia (Wiki) QA tasks. For mathematical reasoning, they use datasets like GSM8K and MAWPS, and for Wiki QA, they employ HotpotQA. The results show that CoA outperforms existing baselines, achieving significant improvements in accuracy and inference efficiency. The method also demonstrates robustness to out-of-distribution knowledge shifts, indicating its potential for generalization across different reasoning scenarios. The key innovation of CoA is its ability to plan abstract reasoning chains, which enables more precise and efficient tool usage compared to direct integration of API calls within the reasoning process.\n\n### Commonalities and Innovations\n\nAll three papers aim to enhance the reasoning abilities of LLMs by integrating external tools into the reasoning process. However, they differ in their approaches and specific implementations:\n\n- **ChatCoT** leverages the multi-turn conversation capabilities of chat-based LLMs to maintain a continuous reasoning process while allowing for tool interactions. The framework initializes the conversation with background knowledge and iterates through a tool-augmented reasoning step, ensuring that the reasoning process remains coherent and uninterrupted.\n- **MultiTool-CoT** focuses on enabling LLMs to use multiple tools in a single reasoning process, addressing the challenge of complex tasks that require both numerical and domain-specific knowledge. The method prompts the LLM to generate reasoning steps and tool triggers, allowing for a more unified and efficient tool usage strategy.\n- **CoA** decouples the general reasoning process from domain-specific knowledge operations, enabling the LLM to generate abstract reasoning chains that can be later filled with specific knowledge from external tools. This approach improves the efficiency and robustness of tool-augmented reasoning by allowing parallel decoding and tool calls.\n\n### Comparison of Results and Discussion\n\nThe experimental results of the three papers demonstrate the effectiveness of their respective approaches in enhancing the reasoning abilities of LLMs:\n\n- **ChatCoT** achieves a 7.9% relative improvement in average accuracy on the MATH dataset compared to the state-of-the-art baseline (PHP). It also performs well on HotpotQA, showing its versatility across different reasoning tasks.\n- **MultiTool-CoT** achieves state-of-the-art performance on the Task 2 dataset of NumGLUE, outperforming baselines by a significant margin. The method's ability to handle multiple tools in a unified manner is particularly noteworthy, as it addresses the challenge of frequent tool use in complex reasoning tasks.\n- **CoA** outperforms existing baselines on both mathematical reasoning and Wiki QA tasks, achieving significant improvements in accuracy and inference efficiency. The method's robustness to out-of-distribution knowledge shifts indicates its potential for generalization across different reasoning scenarios.\n\nDespite these promising results, each method has its limitations. ChatCoT relies on chat-based LLMs and may not be compatible with other models. MultiTool-CoT has been tested on a single task and its effectiveness in real-world applications remains to be verified. CoA requires fine-tuning the LLM, which can be computationally expensive and resource-intensive. Future research could explore integrating these methods with more efficient training schemes, such as LoRA, to address these limitations.\n\n### Conclusion\n\nThe three papers discussed in this summary propose innovative frameworks to enhance the reasoning abilities of LLMs through tool augmentation and CoT prompting. ChatCoT leverages multi-turn conversations to maintain continuous reasoning while allowing for tool interactions. MultiTool-CoT enables LLMs to use multiple tools in a unified manner, addressing the challenge of complex tasks that require both numerical and domain-specific knowledge. CoA decouples general reasoning from domain-specific knowledge operations, improving the efficiency and robustness of tool-augmented reasoning.\n\nThese frameworks demonstrate significant improvements in accuracy and inference efficiency across various reasoning tasks. ChatCoT achieves a 7.9% relative improvement on MATH, MultiTool-CoT achieves state-of-the-art performance on NumGLUE, and CoA outperforms existing baselines on mathematical reasoning and Wiki QA tasks. The common theme across these papers is the need to improve the integration of external tools into the reasoning process of LLMs, ensuring that the models can handle complex tasks more effectively.\n\nFuture research directions could include extending these methods to more types of reasoning tasks, such as medical text analysis, and verifying their effectiveness in real-world applications. Additionally, exploring more efficient training schemes and reducing computational costs could further enhance the practicality of these frameworks. The potential for generalization and the robustness of these methods to out-of-distribution knowledge shifts indicate their promise for advancing the capabilities of LLMs in complex reasoning tasks."
}