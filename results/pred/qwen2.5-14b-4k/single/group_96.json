{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{\\icon Interleaving Retrieval with Chain-of-Thought Reasoning\\\\ for Knowledge-Intensive Multi-Step Questions}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nPrompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, \\textit{what to retrieve} depends on \\textit{what has already been derived}, which in turn may depend on \\textit{what was previously retrieved}. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.\\footnote{Code, data, and prompts are available at \\url{https://github.com/stonybrooknlp/ircot}}.\n\\end{abstract}\n\\section{Introduction}\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.45\\textwidth]{images/intro-figure.pdf}\n\\caption{\n\\iconsys interleaves chain-of-thought (CoT) generation and knowledge retrieval steps in order to guide the retrieval by CoT and vice-versa. This interleaving allows retrieving more relevant information for later reasoning steps, compared to standard retrieval using solely the question as the query.}\n\\label{fig:intro-figure}\n\\end{figure}\n\nLarge language models are capable of answering complex questions by generating step-by-step natural language reasoning steps---so called chains of thoughts (CoT)---when prompted appropriately~\\cite{cot}. This approach has been successful when all information needed to answer the question is either provided as context (e.g., algebra questions) or assumed to be present in the model's parameters (e.g., commonsense reasoning). However, for many open-domain questions, all required knowledge is not always available or up-to-date in models' parameters and it's beneficial to retrieve knowledge from external sources~\\cite{internet-augmented-qa,realtime-qa}.\n\n\\emph{How can we augment chain-of-thought prompting for open-domain, knowledge-intensive tasks that require complex, multi-step reasoning?}\n\nWhile a \\textit{one-shot} retrieval from a knowledge source based solely on the question can successfully augment LMs with relevant knowledge for many factoid-based tasks~\\cite{rag,realm,retro,atlas}, this strategy has clear limitations for more complex multi-step reasoning questions. For such questions, one often must retrieve partial knowledge, perform partial reasoning, retrieve additional information based on the outcome of the partial reasoning done so far, and iterate. \nAs an example, consider the question illustrated in Fig.~\\ref{fig:intro-figure}, \\textit{``In what country was Lost Gravity manufactured?''}. The Wikipedia document retrieved using the question (in particular, the roller coaster Lost Gravity) as the query does not mention where Lost Gravity was manufactured. Instead, one must first infer that it was manufactured by a company called Mack Rides, and then perform further retrieval, guided by the inferred company name, to obtain evidence pointing to the manufacturing country.\n\nThus, the retrieval and reasoning steps must inform each other. Without retrieval, a model is likely to generate an incorrect reasoning step due to hallucination. Additionally, without generating the first reasoning step, the text supporting the second step can't be identified easily given the lack of lexical or even semantic overlap with the question. In other words, we need retrieved facts in order to generate factually correct reasoning steps and the reasoning steps to retrieve relevant facts.\n\nBased on this intuition, we propose an \\emph{interleaving approach} to this problem, where the idea is to use retrieval to guide the chain-of-thought (CoT) reasoning steps and use CoT reasoning to guide the retrieval. Fig.~\\ref{fig:intro-figure} shows an overview of our retrieval method, which we call \\iconsys.\\footnote{\\underline{I}nterleaved \\underline{R}etrieval guided by \\underline{C}hain-\\underline{o}f-\\underline{T}hought.} We begin by retrieving a base set of paragraphs using the question as a query. Subsequently, we alternate between the following two steps: (i) \\textit{extend CoT}: use the question, the paragraphs collected thus far, and the CoT sentences generated thus far to generate the next CoT sentence; (ii) \\textit{expand retrieved information}: use the last CoT sentence as a query to retrieve additional paragraphs to add to the collected set. We repeat these steps till the CoT reports an answer or we reach the maximum allowed number of reasoning steps. Upon termination, all collected paragraphs are returned as the retrieval outcome. Finally, we use these as the context for answering the question via direct QA prompting~\\cite{originalgpt3} or CoT prompting~\\cite{cot}.\n\nWe evaluate the efficacy of our system on 4 multi-step reasoning datasets under an open-domain setting: HotpotQA~\\cite{hotpotqa}, 2WikiMultihopQA~\\cite{xanh2020_2wikimultihop}, MuSiQue~\\cite{musique}, and IIRC~\\cite{iirc}. Our experiments using OpenAI GPT3 (\\texttt{code-davinci-002}) \\cite{originalgpt3,instructgpt3,codex} demonstrate that retrieval using \\iconsys is substantially more effective than the baseline, one-step, question-based retrieval by 11-21 recall points under a fixed-budget optimal recall setup.\\footnote{We explain later (in the Metric section and Footnote~\\ref{footnote:retrieval-metric}) the appropriateness of this metric in our setting as opposed to more mainstream information recall metrics.} When \\iconsys is used in conjunction with a prompting-based reader, it also leads to substantial improvement (up to 15 F1 points) in downstream few-shot QA performance and reduces factual errors in generated CoT by up to 50\\%. Our approach also works on much smaller Flan-T5 models (11B, 3B, and 0.7B) showing similar trends. In particular, we find QA using Flan-T5-XL (3B) with \\iconsys even outperforms the 58X larger GPT3 with a one-step question-based retrieval.\nFurthermore, these improvements also hold up in an out-of-distribution (OOD) setting where the demonstrations from one dataset are used when testing on another dataset.\nLastly, we note that our QA scores exceed those reported by recent works on few-shot prompting for open-domain QA (ODQA)~\\cite{decomp,selfask,react}, although a fair apples-to-apples comparison with them isn't possible (cf.~Appendix~\\ref{sec:sota-differences}).\n\nIn summary, our main \\textbf{contribution} is a novel retrieval method, \\iconsys, that leverages LMs' chain-of-thought generation capabilities to guide retrieval and uses retrieval in turn to improve CoT reasoning. We demonstrate that \\iconsys:\n\\begin{enumerate}[nosep]\n\n    \\item improves both retrieval and few-shot QA performance on several multi-step open-domain QA datasets, in both IID and OOD settings;\n\n    \\item reduces factual errors in generated CoTs; and\n\n    \\item improves performance with both large-scale (175B models) as well as smaller-scale models (Flan-T5-*, $\\le$11B) without any training.\n\n\\end{enumerate}\n\\section{Related Work}\n\\label{sec:related_work}\n\n\\paragraph{Prompting for Open-Domain QA.}\n\nLLMs can learn various tasks by simply using a few examples as prompts~\\cite{originalgpt3}. They've also been shown to answer complex questions by producing step-by-step reasoning (chain-of-thoughts, or CoT) when prompted with a few or zero demonstrations~\\cite{cot, zerocot}. Prompting has been applied to open-domain QA~\\cite{internet-augmented-qa,recitationlm,Yu2022GenerateRT} but its value in improving retrieval and QA for multi-step open-domain questions remains relatively underexplored.\n\nRecently three approaches have been proposed for multi-step open-domain QA. SelfAsk~\\cite{selfask} prompts LLMs to decompose a question into subquestions and answers subquestions by a call to Google Search API. DecomP~\\cite{decomp} is a general framework that decomposes a task and delegates sub-tasks to appropriate sub-models. They also decompose questions but delegate retrieval to a BM25-based retriever. Both of these approaches are not developed for CoT reasoning, do not focus on the retrieval problem, and require a single-hop QA model to answer the decomposed questions. Recently proposed ReAct~\\cite{react} system frames the problem as generating a sequence of reasoning and action steps. These steps are much more complex, rely on much larger models (PaLM-540B), and require fine-tuning to outperform CoT for multi-step ODQA. Furthermore, none of these works have been shown to be effective for smaller models without any training. While a direct comparison with these approaches is not straightforward (difference in knowledge corpus, LLMs, examples), we find that our ODQA performance is much higher than all their reported numbers where available (\\S\\ref{sec:exp-results}).\n\n\\paragraph{Supervised Multi-Step Open-Domain QA.}\nPrior work has explored iterative retrieval for open-domain QA in a fully supervised setting. \\citet{multi-step-retriever-reader} proposes an iterative retrieval model that retrieves using a neural query representation and then updates it based on a reading comprehension model's output. \\citet{multihop-retriever-odqa} apply similar neural query reformulation idea for multihop open-domain QA. \\citet{mpr} extends the widely-used Dense Passage Retrieval (DPR)~\\cite{dpr} to multihop setting, which has since been improved by \\citet{baleen}. \\citet{wikipatretriever} leverages the graph structure induced by the entity links present in Wikipedia paragraphs to perform iterative multi-step retrieval. GoldEn (Gold Entity) retriever~\\cite{golden} iteratively generates text queries based on paragraphs retrieved from an off-the-shelf retriever but requires training data for this next query generator. \\citet{webgpt} used GPT3 to answer long-form questions by interacting with the browser but relied on human annotations of these interactions. All of these methods rely on supervised training on a large-scale dataset and can not be easily extended to a few-shot setting.\n\\section{Chain-of-Thought-Guided Retrieval and Open-Domain QA}\n\\label{sec:method}\n\n\\begin{figure*}[!ht]\n\\centering\n\\includegraphics[width=0.95\\textwidth]{images/main-figure.pdf}\n\\caption{\\iconsys interleaves chain-of-thought (CoT) generation and retrieval steps to guide the retrieval by CoT and vice-versa. We start by retrieving $K$ documents using the question as they query and repeat two steps alternatingly until termination. (i) \\texttt{reason}-step generates next CoT sentence based on the question, so far retrieved paragraphs, and CoT sentences. (ii) \\texttt{retrieve}-step retrieves $K$ more paragraphs based on the last CoT sentence. The process terminates when the generated CoT has ``answer is'' or the number of steps exceeds a threshold. The collection of all paragraphs is returned as the retrieval result on the termination.}\n\\label{fig:main-figure}\n\\end{figure*}\n\nOur goal is to answer a knowledge-intensive multi-step reasoning question $Q$ in a few-shot setting by using a knowledge source containing a large number of documents. To do this we follow a \\texttt{retrieve-and-read} paradigm~\\cite{retrieve-and-read}, where the retriever first retrieves documents from the knowledge source and the QA model reads the retrieved documents and the question to generate the final answer. Our contribution is mainly in the \\texttt{retrieve} step (\\S\\ref{subsec:retriever}), and we use standard prompting strategies for the \\texttt{read} step (\\S\\ref{subsec:reader}).\n\nAs noted earlier, for multi-step reasoning, retrieval can help guide the next reasoning step, which in turn can inform what to retrieve next. This motivates our interleaving strategy, discussed next.\n\n\\subsection{Interleaving Retrieval with Chain-of-Thought Reasoning}\n\\label{subsec:retriever}\n\nOur proposed retriever method, \\iconsys, can be instantiated from the following three ingredients: (i) a base retriever that can take a query and return a given number of paragraphs from a corpus or knowledge source; (ii) a language model with zero/few-shot Chain-of-Thought (CoT) generation capabilities; and (iii) a small number of annotated questions with reasoning steps explaining how to arrive at the answer in natural language (chain of thoughts) and a set of paragraphs from the knowledge source that collectively support the reasoning chain and the answer.\n\nThe overview of \\iconsys is given in Fig.~\\ref{fig:main-figure}. We first gather a base set of paragraphs by retrieving $K$ paragraphs using the question $Q$ as the query. Then, we interleave two steps (\\texttt{reason} and \\texttt{retrieve}) iteratively until the termination criterion is met.\n\nThe \\textbf{retrieval-guided reasoning step (``Reason'')} generates the next CoT sentence using the question, the paragraphs collected thus far, and the CoT sentences generated thus far. The prompt template for the task looks as follows:\n\n\\begin{small}\n\\begin{verbatim}\nWikipedia Title: <Page Title>\n<Paragraph Text>\n...\nWikipedia Title: <Page Title>\n<Paragraph Text>\n\nQ: <Question>\nA: <CoT-Sent-1> ... <CoT-Sent-n>\n\\end{verbatim}\n\\end{small}\n \nFor in-context demonstrations, we use the complete CoT in the above format. For a test instance, we show the model only the CoT sentences generated thus far and let it complete the rest. Even though the model may output multiple sentences, for each \\texttt{reason-step}, we only take the first generated sentence and discard the rest.\n\nFor the paragraphs in the in-context demonstrations, we use ground-truth supporting paragraphs and $M$ randomly sampled paragraphs shuffled and concatenated together in the above format. For a test instance, we show all the paragraphs collected thus far across all the previous \\texttt{retrieve-step}s.\n\nIf the generated CoT sentence has the ``answer is:'' string or the maximum number of steps\\footnote{set to 8 in our experiments.} has been reached, we terminate the process and return all collected paragraphs as the retrieval result.\n\nThe \\textbf{CoT-guided retrieval step (``Retrieve'')} uses the last generated CoT sentence as a query to retrieve more paragraphs and adds them to the collected paragraphs. We cap the total number of collected paragraphs\\footnote{set to 15 in our experiments.} so as to fit in at least a few demonstrations in the model's context limit.\n\n\\subsection{Question Answering Reader}\n\\label{subsec:reader}\n\nThe QA reader answers the question using retrieved paragraphs taken from the retriever. We consider two versions of the QA reader implemented via two prompting strategies: \\texttt{CoT Prompting} as proposed by \\citet{cot}, Direct Prompting as proposed by \\citet{originalgpt3}. For CoT prompting, we use the same template as shown in \\S\\ref{subsec:reader}, but at test time we ask the model to generate the full CoT from scratch. The final sentence of CoT is expected to be of the form ``answer is: ...'', so that the answer can be extracted programmatically. If it's not in that form, the full generation is returned as the answer. For Direct Prompting, we use the same template as CoT Prompting but the answer field (``A: '') contains only the final answer instead of CoT. See App.~\\ref{sec:apndx-prompts} for details.\n\n\\section{Experimental Setup}\n\\label{sec:exp-setup}\n\nWe evaluate our method on 4 multi-step QA datasets in the open-domain setting: \\textbf{HotpotQA}~\\cite{hotpotqa}, \\textbf{2WikiMultihopQA}~\\cite{xanh2020_2wikimultihop}, answerable subset of \\textbf{MuSiQue}~\\cite{musique}, and answerable subset of \\textbf{IIRC}~\\cite{iirc}. For HotpotQA, we use the Wikipedia corpus that comes with it for the open-domain setting. For each of the other three datasets, which originally come in a reading comprehension or mixed setting, we used the associated contexts to construct a corpus for our open-domain setting (see App.~\\ref{sec:apndx-corpora} for details). For each dataset, we use 100 randomly sampled questions from the original development set for tuning hyperparameters, and 500 other randomly sampled questions as our test set.\n\n\\subsection{Models}\n\\label{subsec:exp-models}\n\n\\paragraph{Retriever.} We use BM25~\\cite{bm25} implemented in Elasticsearch\\footnote{\\url{https://www.elastic.co/}} as our base retriever. We compare two retriever systems:\n\n(i) \\textbf{One-step Retriever (OneR)} uses the question as a query to retrieve $K$ paragraphs. We select $K \\in \\{5, 7, 9, 11, 13, 15\\}$ that's best on the dev set.\n\n(ii) \\textbf{\\iconsys Retriever} is our method described in \\S\\ref{sec:method}. We use BM25 as its underlying retriever and experiment with OpenAI GPT3 (\\texttt{code-davinci-002})~\\cite{originalgpt3,instructgpt3,codex} and Flan-T5~\\cite{flan} of different sizes as its CoT generator.\n\nFor demonstrating in-context examples to these LMs, we wrote CoTs for 20 questions for all the datasets (see App. \\S\\ref{sec:apndx-prompts}). We then create 3 demonstration (``training'') sets by sampling 15 questions each for each dataset. For each experiment, we search for the best hyperparameters for the dev set using the first demonstration set and evaluate each demonstration set on the test set using the selected hyperparameters. We report the mean and standard deviation of these 3 results for each experiment.\n\n At test time, we pack as many demonstrations as possible within the model's context length limit. The context limit for GPT3 (\\texttt{code-davinci-002}) is 8K word pieces. Flan-T5-* doesn't have any hard limit as it uses relative position embeddings. But we limit Flan-T5's context to 6K word pieces, which is the maximum we could fit in the memory of our 80G A100 GPUs.\n\n\\iconsys Retriever has one key hyperparameter: $K \\in \\{2, 4, 6, 8\\}$, the number of paragraphs to retrieve at each step. Additionally, when creating ``training'' demonstrations for \\iconsys's Reasoner module, we use gold paragraphs and a smaller number $M \\in \\{1, 2, 3\\}$ of distractor paragraphs (\\S\\ref{subsec:retriever}).\n\n\\textbf{Retrieval Metric:} We allow a maximum of 15 paragraphs for all retriever systems and measure the recall of the gold paragraphs among the retrieved set of paragraphs. We search for the hyperparameter $K$ (and $M$ for \\iconsys) that maximizes the recall on the dev set and use it on the test set. The reported metric can thus be viewed as the \\emph{fixed-budget optimal recall} for each system considered.\\footnote{\\label{footnote:retrieval-metric}Note that our retrieved documents are not ranked, making standard information retrieval metrics such as MAP and DCG inapplicable. Further, we can only limit the number of retrieved paragraphs \\emph{per step} to $K$. Since the total number of reasoning steps varies for questions, and in some cases, we don't even obtain all $K$ paragraphs in a given step, the total number of retrieved paragraphs also varies (even though capped at 15). This makes Recall@k, Precision@k, etc., also not applicable as metrics for any given k.}\n\n\\paragraph{QA Reader.} To implement the reader, we use the same LMs as used in the \\texttt{reason-step} of \\iconsys Retriever. We found that QA readers implemented with Flan-T5-* perform better with the Direct Prompting strategy and GPT3 performs better with CoT Prompting strategy (see App. ~\\ref{sec:apndx-readers-results}). Hence we use Direct prompting strategy for QA with Flan-T5-* and CoT with GPT3 for the experiments.\\footnote{\\iconsys, by construction, produces a CoT as a part of its retrieval process. Thus, instead of having a separate post-hoc reader, one can also just extract the answer from the CoT generated during retrieval. However, we found this to be a suboptimal choice, so we always use a separate reader (see App.~\\ref{sec:qa-reader-ablation}).}\n\nThe QA reader has one hyperparameter $M$: the number of distractor paragraphs in the in-context demonstrations. We search for $M$ in $\\{1, 2, 3\\}$. When used in conjunction with \\iconsys retriever $M$ is tied for the CoT generator and the reader.\n\n\\paragraph{Open-Domain QA (ODQA) Models.} Putting retrievers and readers together, we experiment with ODQA models constructed from the various language models denoted as \\textbf{OneR QA} and \\textbf{\\iconsys QA}. For \\iconsys QA, the choice of LM for the CoT generator and the reader is kept the same. We also experiment with retriever-less QA readers \\textbf{NoR QA} to assess how well LMs can answer the question from their parametric knowledge alone. To select the best hyperparameters for the ODQA model, we search for the hyperparameters $K$ and $M$ that maximize the answer F1 on the development set.\n\nIIRC is structured slightly differently from the other datasets, in that its questions are grounded in a main passage and other supporting paragraphs come from the Wikipedia pages of entities mentioned in this passage. We slightly modify the retrievers and readers to account for this (see App.~\\ref{sec:apndx-iirc-special-handling}).\n\n\\begin{figure*}[ht]\n\\centering\n\\includegraphics[width=0.95\\textwidth]{images/main_retrieval_results.pdf}\n\\caption{Retrieval recall for one-step retriever (OneR) and \\iconsys instantiated from \\texttt{Flan-T5-XXL} (left) and \\texttt{GPT3} (right) models. \\iconsys outperforms OneR for both models and all datasets.}\n\\label{fig:main-retrieval-results}\n\\end{figure*}\n\n\\begin{figure*}[ht]\n\\centering\n\\includegraphics[width=0.95\\textwidth]{images/main_qa_results.pdf}\n\\caption{Answer F1 for ODQA model made using (i) no retriever (NoR QA) (ii) one-step retriever (OneR QA) and (iii) \\iconsys QA instantiated from \\texttt{Flan-T5-XXL} (left) and \\texttt{GPT3} (right) models. \\iconsys QA outperforms OneR QA and NoR QA for both models on all datasets, except for GPT3 on IIRC.}\n\\label{fig:main-qa-results}\n\\end{figure*}\\section{Results}\n\\label{sec:exp-results}\n\n\\begin{figure*}[ht]\n\\centering\n\\includegraphics[width=0.95\\textwidth]{images/ood_retrieval_results.pdf}\n\\caption{Retrieval recall for OneR and IRCoT using Flan-T5-XXL (Left) and GPT3 (Right) in out-of-distribution (OOD) setting. HQ (HotpotQA), 2W (2WikiMultihopQA), MQ (MuSiQue). The result X$\\rightarrow$Y indicates prompt demonstrations are from dataset X and evaluation is on dataset Y. \\iconsys outperforms OneR in such an OOD setting.}\n\\label{fig:ood-retrieval-results}\n\\end{figure*}\n\n\\begin{figure*}[ht]\n\\centering\n\\includegraphics[width=0.95\\textwidth]{images/ood_qa_results.pdf}\n\\caption{Answer F1 for NoR QA, OneR QA and IRCoT QA using Flan-T5-XXL (Left) and GPT3 (Right) in out-of-distribution (OOD) setting. HQ (HotpotQA), 2W (2WikiMultihopQA), MQ (MuSiQue). The result X$\\rightarrow$Y indicates prompt demonstrations are from dataset X and evaluation is on dataset Y. \\iconsys QA outperforms OneR QA and NoR QA in such OOD setting.}\n\\label{fig:ood-qa-results}\n\\end{figure*}\n\n\\paragraph{\\iconsys retrieval is better than one-step. }\n\nFig.~\\ref{fig:main-retrieval-results} compares OneR with \\iconsys retrievers made from \\texttt{Flan-T5-XXL} and \\texttt{GPT3} LMs. For both models, \\iconsys significantly outperforms one-step retrieval across all datasets. For \\texttt{Flan-T5-XXL}, \\iconsys improves our recall metric relative to one-step retrieval, on HotpotQA by 7.9, on 2WikiMultihopQA by 14.3, on MuSiQue by 3.5, and on IIRC by 10.2 points. For \\texttt{GPT3}, this improvement is by 11.3, 22.6, 12.5, and 21.2 points, respectively.\n\n\\paragraph{\\iconsys QA outperforms NoR and OneR QA.}\n\nFig.~\\ref{fig:main-qa-results} compares ODQA performance using NoR, OneR and \\iconsys retriever made from \\texttt{Flan-T5-XXL} and \\texttt{GPT3} LMs. For \\texttt{Flan-T5-XXL}, \\iconsys QA outperforms OneR QA on HotpotQA by 9.4, on 2WikiMultihopQA by 15.3, on MuSiQue by 5.0 and IIRC by 2.5 F1 points. For \\texttt{GPT3}, the corresponding numbers (except for IIRC) are 7.1, 13.2, and 7.1 F1 points. For \\texttt{GPT3}, \\iconsys doesn't improve the QA score on IIRC, despite significantly improved retrieval (21 points as shown in Fig.~\\ref{fig:main-retrieval-results}). This is likely because IIRC relevant knowledge may already be present in GPT3, as also evidenced by its NoR QA score being similar. For other datasets and model combinations, NoR QA is much worse than \\iconsys QA, indicating the limits of the models' parametric knowledge.\n\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[width=0.475\\textwidth]{images/factual_errors.pdf}\n\\caption{Number of questions, out of 40, where CoT generated by GPT3 using different methods has at least 1 factual error. Factual errors: \\iconsys $<$ OneR $<$ NoR.}\n\\label{fig:cot-factual-errors}\n\\end{figure}\n\n\\begin{figure*}[ht]\n\\centering\n\\includegraphics[width=0.95\\textwidth]{images/model_scale_retrieval_results.pdf}\n\\caption{Retrieval recall for OneR (bottom) and \\iconsys (top) for LMs of increasing sizes: Flan-T5 \\{base (0.2B), large (0.7B), XL (3B), XXL (11B)\\} and GPT3 (175B) on HotpotQA, 2WikiMultihopQA, MuSiQue. \\iconsys outperforms OneR for all model sizes, including the 0.3B model, and the difference roughly grows with model size. Note: OneR doesn't use LM in its retrieval and so has a fixed score.}\n\\label{fig:model-scale-retrieval-results}\n\\end{figure*}\n\n\\begin{figure*}[ht]\n\\centering\n\\includegraphics[width=0.95\\textwidth]{images/model_scale_qa_results.pdf}\n\\caption{Answer F1 for ODQA models made using OneR (bottom) and \\iconsys (top) for LMs of increasing sizes: Flan-T5 \\{base (0.2B), large (0.7B), XL (3B), XXL (11B)\\} and GPT3 (175B) on HotpotQA, 2WikiMultihopQA and MuSiQue. \\iconsys QA outperforms OneR QA for all model sizes except for the smallest, 0.3B. \\iconsys with 3B model even outperforms OneR with 58X larger GPT3 model showing the value of improved retrieval.}\n\\label{fig:model-scale-qa-results}\n\\end{figure*}\n\n\\paragraph{\\iconsys is effective in OOD setting. }\n\nSince CoT may not always be easy to write for new datasets, we evaluate NoR, OneR, and IRCoT on generalization to new datasets, i.e. OOD setting. To do so, we use prompt demonstrations from one dataset to evaluate on another dataset.\\footnote{We use the evaluation dataset's corpus for retrieval.} For all pairs of the datasets\\footnote{We skip IIRC in this exploration as the task is structured a bit differently and requires special handling (see App.~\\ref{sec:apndx-iirc-special-handling}).} and for both \\texttt{Flan-T5-XXL} and \\texttt{GPT3}, we find the same trend as in the IID setting: \\iconsys retrieval outperforms OneR (Fig.~\\ref{fig:ood-retrieval-results}), and IRCoT QA outperforms both OneR QA and NoR QA (Fig.~\\ref{fig:ood-qa-results}).\n\n\\paragraph{\\iconsys generates CoT with fewer factual errors.}\n\nTo assess whether our approach also improves the factuality of generated CoTs, we manually annotated CoTs generated by NoR QA, OneR QA, and IRCoT QA using GPT3 for 40 randomly sampled questions from each of the four datasets. We considered CoT to have a factual error if at least one of the facts\\footnote{all sentences before the final ``answer is:'' sentence.} is not true.\\footnote{Note that factual error doesn't necessarily mean the predicted answer is incorrect and vice-versa. This is because the model can generate a wrong answer despite all correct facts, and vice-versa. We also account for the possibility of answer annotation errors in the original datasets.} As Fig.~\\ref{fig:cot-factual-errors} shows, NoR makes the most factual errors, OneR makes fewer, and \\iconsys the least. In particular, \\iconsys reduces the factual errors over OneR by 50\\% on HotpotQA and 40\\% on 2WikiMultihopQA.\n\nTable~\\ref{table:nor-oner-cot-examples} illustrates how the CoT predictions for different methods vary qualitatively. Since NoR relies completely on parametric knowledge, it often makes a factual error in the first sentence, which derails the full CoT. OneR can retrieve relevant information closest to the question and is less likely to make such errors early on, but it still makes errors later in the CoT. IRCoT, on the other hand, is often able to prevent such errors in each step.\n\n\\paragraph{\\iconsys is also effective for smaller models.}\n\nTo see how effective \\iconsys is at different LM sizes, we show the scaling plots in Fig.~\\ref{fig:model-scale-retrieval-results}.\\footnote{We skip IIRC here as the smaller models are not good at identifying Wikipedia titles from a paragraph and a question which is necessary for IIRC (see App.~\\ref{sec:apndx-iirc-special-handling}).} We compare the recall for OneR and \\iconsys using \\texttt{Flan-T5} \\{base (0.2B), large (0.7B), XL (3B), XXL (11B)\\}, and GPT3 \\texttt{code-davinci-002} (175B). \\iconsys with even the smallest model (0.2B) is better than OneR, and the performance roughly improves with the model size. This shows the CoT generation capabilities of even small models can be leveraged for improving retrieval. Furthermore, we show the effect of model size on the QA score in Fig.~\\ref{fig:model-scale-qa-results}. For all sizes except the smallest (0.2B), we see \\iconsys QA is better than OneR QA. Moreover, \\iconsys with a 3B model even outperforms OneR and NoR with a 58X larger 175B GPT3 model in all datasets.\n\n\\paragraph{\\iconsys is SOTA for few-shot multistep ODQA.\\footnote{\\label{footnote:sota}App.~\\S\\ref{sec:sota-differences} reports updated SOTA numbers, including contemporaneous and newer works.}}\n\nWe compare \\iconsys QA with five recent approaches to using LLMs for ODQA: Internet-Augmented QA~\\cite{internet-augmented-qa}, RECITE~\\cite{recitationlm} ReAct~\\cite{react}, SelfAsk~\\cite{selfask}, and DecomP~\\cite{old-decomp}. Although these are not head-to-head comparisons as different methods use different APIs, knowledge sources, and even LLMs (see App.~\\ref{sec:sota-differences} for details), it is still informative to explore, in a leaderboard-style fashion, how \\iconsys performs relative to the best numbers published for these recent systems.\n\n\\vspace{0.1cm}\n\\begin{table}[ht]\n    \\centering\n    \\footnotesize\n    \\setlength{\\tabcolsep}{2.0pt}\n    \\begin{tabular}{ccccc}\\toprule\n        Model &  HpQA\\textsuperscript{Br}  &  HpQA & 2WikiMQA & MQ\\textsuperscript{2H} \\\\\n        \\midrule\n        InterAug      &   $-$ | $-$    &        30.3 | $-$\\p{xx}   &        $-$ | $-$        &        $-$ | $-$            \\\\\n        RECITE        &   $-$ | $-$    &        37.1 | 48.4        &        $-$ | $-$        &        $-$ | $-$            \\\\\n        ReAct         &   $-$ | $-$    &        35.1 | $-$\\p{xx}   &        $-$ | $-$        &        $-$ | $-$            \\\\\n        SelfAsk       &   $-$ | $-$    &         $-$ | $-$         &       40.1 | $-$\\p{xx}   &       15.2 | $-$\\p{xx}     \\\\\n        DecomP        &  \\p{x..}$-$ | 50.0  &         $-$ | $-$         &   \\p{x..}$-$ | 59.3      &       $-$ | $-$       \\\\\n        \\midrule\n        \\sys QA       &   \\textbf{45.8 | 58.5}   &    \\bf{49.3 | 60.7}       &   \\bf{57.7 | 68.0}      &  \\bf{34.2 | 43.8} \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{Comparison with other LLM-based ODQA systems on EM and F1 scores. `$-$': score is unavailable. HpQA\\textsuperscript{Br}: Bridge questions subset of HotpotQA. MQ\\textsuperscript{2H}: MuSiQue 2-hop questions. \\iconsys QA with GPT3 (ours) outperforms other systems by a large margin. Note: Comparisons aren't head-to-head as discussed in the text. App.~\\S\\ref{sec:sota-differences} reports updated SOTA numbers, including contemporaneous and newer works.\n    \\label{table:extrinsic-comparison}\n    }\n\\end{table}\n\nAs shown in Table~\\ref{table:extrinsic-comparison}, \\iconsys QA significantly outperforms all of these recent systems by a large margin, setting a new state of the art in terms of what's achievable via retrieval-augmented LLMs (without supervised training).\n\n\\begin{table*}[t]\n    \\centering\n    \\footnotesize\n    \\setlength{\\tabcolsep}{4.0pt}\n    \\begin{tabular}{llllll}\\toprule\n        & \\textbf{Question} &   \\textbf{Gold Facts} & \\textbf{NoR QA} & \\textbf{OneR QA} & \\textbf{IRCoT QA} \\\\\n        \\midrule\n\n        \\multirow{2}{*}{\\rotatebox[origin=c]{90}{\\parbox[c]{3.0cm}{\\centering HotpotQA}}}\n        & \\begin{minipage}[t]{2cm} The tap dancing soloist in My Feet Keep Dancing also starred in what 1935 MGM musical? \\end{minipage}\n        & \\begin{minipage}[t]{3.0cm} $\\Rightarrow$ The tap dancing soloist in My Feet Keep Dancing is \\textcolor{blue}{Fayard Nicholas}. \\\\ $\\Rightarrow$ Fayard Nicholas starred in the 1935 MGM musical \\textcolor{blue}{An All-Colored Vaudeville Show}. \\end{minipage}\n        & \\begin{minipage}[t]{3.0cm} The tap dancing soloist in My Feet Keep Dancing is \\textcolor{red}{Eleanor Powell}. Eleanor Powell also starred in the 1935 MGM musical \\textcolor{red}{Broadway Melody} of 1936. So the answer is: Broadway Melody of 1936. \\end{minipage}\n        & \\begin{minipage}[t]{3.0cm} The tap dancing soloist in My Feet Keep Dancing is \\textcolor{blue}{Fayard Nicholas}. Fayard Nicholas also starred in the 1935 MGM musical \\textcolor{red}{Top Hat}. So the answer is: Top Hat. \\end{minipage}\n        & \\begin{minipage}[t]{3.0cm} The tap dancing soloist in My Feet Keep Dancing is \\textcolor{blue}{Fayard Nicholas}. Fayard Nicholas starred in the 1935 MGM musical \\textcolor{blue}{An All-Colored Vaudeville Show}. So the answer is: An All-Colored Vaudeville Show. \\end{minipage} \\\\\n\n        \\midrule\n\n        \\multirow{1}{*}{\\rotatebox[origin=c]{90}{\\parbox[c]{4.0cm}{\\centering 2WikiMQA}}}\n        & \\begin{minipage}[t]{2cm} Who is the father-in-law of Queen Hyojeong? \\end{minipage}\n        & \\begin{minipage}[t]{3.0cm} $\\Rightarrow$ Queen Hyojeong is the wife of \\textcolor{blue}{King Heonjong of Joseon}. \\\\ $\\Rightarrow$ King Heonjong of Joseon is the son of \\textcolor{blue}{Crown Prince Hyomyeong}. \\end{minipage}\n        & \\begin{minipage}[t]{3.0cm} Queen Hyojeong has a husband named \\textcolor{red}{King Gojong}. King Gojong has a father named \\textcolor{red}{Heungseon Daewongun}. Thus, Queen Hyojeong has a father-in-law named Heungseon Daewongun. So the answer is: Heungseon Daewongun. \\end{minipage}\n        & \\begin{minipage}[t]{3.0cm} Queen Hyojeong is the wife of \\textcolor{blue}{King Heonjong of Joseon}. King Heonjong of Joseon is the son of \\textcolor{red}{King Sejo of Joseon}. Thus, King Sejo of Joseon is the father-in-law of Queen Hyojeong. So the answer is: King Sejo of Joseon. \\end{minipage}\n        & \\begin{minipage}[t]{3.0cm} Queen Hyojeong is the wife of \\textcolor{blue}{King Heonjong of Joseon}. King Heonjong of Joseon is the son of \\textcolor{blue}{Crown Prince Hyomyeong}. Thus, Crown Prince Hyomyeong is the father-in-law of Queen Hyojeong. So the answer is: Crown Prince Hyomyeong. \\end{minipage} \\\\\n\n        \\midrule\n\n        \\multirow{1}{*}{\\rotatebox[origin=c]{90}{\\parbox[c]{3.0cm}{\\centering MuSiQue}}}\n        & \\begin{minipage}[t]{2cm} What is the name of the castle in the city where the performer of A Collection 1984–1989 was born? \\end{minipage}\n        & \\begin{minipage}[t]{3.0cm} $\\Rightarrow$ A Collection 1984–1989 was performed by \\textcolor{blue}{Jane Siberry}. \\\\ $\\Rightarrow$ Jane Siberry was born in \\textcolor{blue}{Toronto}. \\\\ $\\Rightarrow$ The castle in Toronto is the \\textcolor{blue}{Casa Loma}. \\end{minipage}\n        & \\begin{minipage}[t]{3.0cm} The performer of A Collection 1984–1989 is \\textcolor{red}{The The}. The The was born in \\textcolor{red}{London}. The name of the castle in London is the \\textcolor{red}{Tower of London}. So the answer is: the Tower of London. \\end{minipage}\n        & \\begin{minipage}[t]{3.0cm} A Collection 1984–1989 was performed by \\textcolor{blue}{Jane Siberry}. Jane Siberry was born in \\textcolor{blue}{Toronto}. The castle in Toronto is \\textcolor{red}{Peqin Castle}. So the answer is: Peqin Castle. \\end{minipage}\n        & \\begin{minipage}[t]{3.0cm} A Collection 1984–1989 was performed by \\textcolor{blue}{Jane Siberry}. Jane Siberry was born in \\textcolor{blue}{Toronto}. The castle in Toronto is the \\textcolor{blue}{Casa Loma}. So the answer is: Casa Loma. \\end{minipage} \\\\\n\n        \\bottomrule\n\n    \\end{tabular}\n    \\caption{Example CoTs generated by GPT3 with different methods. Since NoR relies on parametric knowledge, it often makes a factual error in the first sentence derailing the full CoT. OneR can retrieve relevant information closest to the question and is less likely to make such errors early on, but it still makes errors later in the CoT. As \\iconsys performs retrieval after each step, it is often able to prevent such errors in each step. More examples are in App.~\\ref{sec:apdx-nor-oner-cot-examples}.}\n    \\label{table:nor-oner-cot-examples}\n\\end{table*}\n\n\\section{Conclusions}\n\nChain-of-thought prompting has significantly improved LLMs' ability to perform multi-step reasoning. We leveraged this ability to improve retrieval, and in turn, improve QA performance for complex knowledge-intensive open-domain tasks in a few-shot setting. We argued that one-step question-based retrieval is insufficient for such tasks, and introduced \\iconsys, which uses interleaved CoT reasoning and retrieval steps that guide each other step-by-step. On four datasets, \\iconsys significantly improves both retrieval and QA performance when compared to one-step retrieval, for both large and relatively smaller-scale LMs. Additionally, CoTs generated by \\iconsys contain fewer factual errors.\n\\section*{Limitations}\n\\label{sec:limitations}\n\n\\iconsys relies on the base LM to have a zero or few-shot CoT-generation ability.  While this is commonly available in large LMs (over 100B), it's not as common for small LMs (under 20B), which to some extent limits \\iconsys adoptability. Given the recent surge of interest~\\cite{ul2,reasoningdistillation1,reasoningdistillation2}, however, smaller LMs will likely increasingly acquire such ability, making IRCoT compatible with many more LMs.\n\n\\iconsys also relies on the base LM to support long inputs as multiple retrieved paragraphs need to fit in the LM's input, in addition to at least a few demonstrations of QA or CoT with paragraphs. This was supported by the models we used as \\texttt{code-davinci-002} (GPT3) allows 8K tokens and Flan-T5-* uses relative position embeddings making it as extensible as the GPU memory constraints allow. Future work can explore strategies to rerank and select the retrieved paragraphs instead of passing all of them to the LM to alleviate the need for the LM to support long input.\n\nThe performance gain of \\iconsys retriever and QA (over OneR and ZeroR baselines) come with an additional computational cost. This is because \\iconsys makes a separate call to an (L)LM for each sentence of CoT. Future work can focus on, for instance, dynamically deciding when to retrieve more information and when to perform additional reasoning with the current information.\n\nLastly, a portion of our experiments was carried out using a commercial LLM API from OpenAI (\\texttt{code-davinci-002}). This model was deprecated by OpenAI after our submission making the reproduction of these experiments challenging despite our best efforts, just like any other work using such APIs. The trends discussed in the paper (\\iconsys $>$ OneR $>$ NoR), we believe, would still hold. Additionally, all our experiments using Flan-T5-*, which exhibit similar trends as that of GPT3, will remain reproducible, thanks to its publicly available model weights.\n\\section*{Ethical Considerations}\n\nLanguage models are known to hallucinate incorrect and potentially biased information. This is especially problematic when the questions asked to it are of a sensitive nature. While retrieval-augmented approaches such as ours are expected to alleviate this issue to some extent by grounding generation in external text, this by no means solves the problem of generating biased or offensive statements. Appropriate care should thus be taken if deploying such systems in user-facing applications.\n\nAll the datasets and models used in this work are publicly available with permissible licenses. HotpotQA has CC BY-SA 4.0 license\\footnote{\\url{https://creativecommons.org/licenses/by-sa/4.0/}}, 2WikiMultihopQA has Apache-2.0 license\\footnote{\\url{https://www.apache.org/licenses/LICENSE-2.0}}, MuSiQUe and IIRC have CC BY 4.0 license\\footnote{\\url{https://creativecommons.org/licenses/by/4.0}}, and Flan-T5-* models have Apache-2.0 license.\\section*{Acknowledgments}\n\nWe thank the reviewers for their valuable feedback and suggestions. We also thank OpenAI for providing access to the \\texttt{code-davinci-002} API. This material is based on research supported in part by the Air Force Research Laboratory (AFRL), DARPA, for the KAIROS program under agreement number FA8750-19-2-1003, in part by the National Science Foundation under the award IIS \\#2007290, and in part by an award from the Stony Brook Trustees Faculty Awards Program.\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Active Retrieval Augmented Generation}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nDespite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output.\nAugmenting LMs by retrieving information from external knowledge resources is one promising solution.\nMost existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input.\nThis is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential.\nIn this work, we provide a generalized view of \\emph{active retrieval augmented generation}, methods that actively decide when and what to retrieve across the course of the generation.\nWe propose \\textbf{F}orward-\\textbf{L}ooking \\textbf{A}ctive \\textbf{RE}trieval augmented generation~(\\textbf{\\ours}), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.\nWe test \\ours along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets.\n\\ours achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method.%\n\\footnote{Code and datasets are available at \\url{https://github.com/jzbjyb/FLARE}.}\n\\end{abstract}\n\n\\section{Introduction}\n\nGenerative language models (LMs) \\cite{gpt3-brown-2020,instructgpt3-ouyang-2022,gpt4-2023,palm-chowdhery-2022,opt-zhang-2022,llama-touvron-2023,zhao-llm-2023} have become a foundational component in natural language processing (NLP) systems with their remarkable abilities.\nAlthough LMs have memorized some world knowledge during training \\cite{lama-petroni-2019,t5pack-roberts-2020,lpaqa-jiang-2020}, they still tend to hallucinate and create imaginary content~\\cite{maynez-etal-2020-faithfulness,zhou-etal-2021-detecting}.\nAugmenting LMs with retrieval components that look up relevant information from external knowledge resources is a promising direction to address hallucination \\cite{knnlm-2020-khandelwal,atlas-izacard-2022}.\n\n\\begin{figure*}[tb]\n\\includegraphics[width=0.9\\textwidth, clip, keepaspectratio]{illustration.pdf}\n\\centering\n\\caption{An illustration of forward-looking active retrieval augmented generation (\\ours). Starting with the user input $\\bm{x}$ and initial retrieval results $\\mathcal{D}_{\\bm{x}}$, \\ours iteratively generates a temporary next sentence (shown in \\textcolor{gray}{\\textit{gray italic}}) and check whether it contains low-probability tokens (indicated with \\underline{underline}). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence.}\n\\label{fig:illustration}\n\\end{figure*}\n\nRetrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve documents based on the user's input, and then generate a complete answer conditioning on the retrieved documents~\\cite{drqa-chen-2017,realm-guu-2020,rag-lewis-2020,fid-2021-izacard,emdr2-2021-sachan,yono-2021-lee,reatt-jiang-2022,atlas-izacard-2022,webgpt-nakano-2021,webbrain-qin-2023,lazaridou-internet-2022,replug-shi-2023}.\nThese single-time retrieval augmented LMs outperform purely parametric LMs, particularly for short-form knowledge-intensive generation tasks such as factoid question answering (QA) \\cite{nq-kwiatkowski-2019,joshi-2017-triviaqa}, where \\emph{the information needs are clear in the user's input, and it is sufficient to retrieve relevant knowledge once solely based on the input}.\n\nIncreasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long-form QA \\cite{eli5-fan-2019,asqa-stelmakh-2022}, open-domain summarization \\cite{wikisum-cohen-2021,wikiasp-hayashi-2021,openmds-giorgi-2022}, and (chain-of-thought; CoT) reasoning \\cite{cot-wei-2022,2wikimultihopqa-ho-2020,strategyqa-geva-2021,hendrycks-2020-mass}.\nIn contrast to short-form generation, long-form generation presents complex information needs that are \\emph{not always evident from the input alone}. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would \\emph{require gathering multiple pieces of knowledge throughout the generation process}.\nFor example, to generate a summary about a particular topic, the initial retrieval based on the topic name (e.g., Joe Biden) may not cover all aspects and details.\nIt is crucial to retrieve extra information as needed during generation, such as when generating a certain aspect (e.g., Joe Biden's education history) or a specific detail (e.g., the date of Joe Biden's presidential campaign announcement).\n\nSeveral attempts have been made to retrieve multiple times throughout generation.\nThese attempts include methods that passively use the past context to retrieve additional information at a fixed interval \\cite{knnlm-2020-khandelwal,retro-borgeaud-2022,icrlm-ram-2023,ircot-trivedi-2022} which might not accurately reflect what LMs intend to generate in the future or retrieve at inappropriate points.\nSome works in multihop QA decompose the full question into sub-questions, each of which is used to retrieve extra information~\\cite{selfask-press-2022,react-yao-2022,decomp-khot-2022,dsp-khattab-2022}.\n\nWe ask the following question: can we create a simple and generic retrieval augmented LM that \\emph{actively decides when and what to retrieve} throughout the generation process, and are applicable to a variety of long-form generation tasks?\nWe provide a generalized view of active retrieval augmented generation.\nOur hypothesis regarding \\emph{when to retrieve} is that LMs should retrieve information only when they lack the required knowledge to avoid unnecessary or inappropriate retrieval that occurs in passive retrieval augmented LMs \\cite{knnlm-2020-khandelwal,retro-borgeaud-2022,icrlm-ram-2023,ircot-trivedi-2022}.\nGiven the observation that large LMs tend to be well-calibrated and low probability/confidence often indicates a lack of knowledge \\cite{mostlyknow-kadavath-2022}, we adopt an active retrieval strategy that only retrieves when LMs generate low-probability tokens.\nWhen deciding \\emph{what to retrieve}, it is important to consider what LMs intend to generate in the future, as the goal of active retrieval is to benefit future generations.\nTherefore, we propose anticipating the future by generating a temporary next sentence, using it as a query to retrieve relevant documents, and then regenerating the next sentence conditioning on the retrieved documents.\nCombining the two aspects, we propose \\textbf{F}orward-\\textbf{L}ooking \\textbf{A}ctive \\textbf{RE}trieval augmented generation (\\textbf{\\ours}), as illustrated in \\autoref{fig:illustration}.\n\\ours iteratively generates \\emph{a temporary next sentence}, use it as the query to retrieve relevant documents \\emph{if it contains low-probability tokens} and regenerate the next sentence until reaches the end.\n\n\\ours is applicable to any existing LMs at inference time without additional training.\nConsidering the impressive performance achieved by GPT-3.5 \\cite{instructgpt3-ouyang-2022} on a variety of tasks, we examine the effectiveness of our methods on \\texttt{text-davinci-003}.\nWe evaluate \\ours on 4 diverse tasks/datasets involving generating long outputs, including multihop QA (2WikiMultihopQA), commonsense reasoning (StrategyQA), long-form QA (ASQA), and open-domain summarization (WikiAsp) \\cite{2wikimultihopqa-ho-2020,strategyqa-geva-2021,asqa-stelmakh-2022,wikiasp-hayashi-2021}.\nOver all tasks, \\ours achieves superior or competitive performance compared to single-time and multi-time retrieval baselines, demonstrating the effectiveness and generalizability of our method.\n\n\\section{Retrieval Augmented Generation}\nWe formally define single-time retrieval augmented generation and propose the framework of active retrieval augmented generation.\n\n\\subsection{Notations and Definitions}\nGiven a user input $\\bm{x}$ and a document corpus $\\mathcal{D}=\\{\\bm{d}_i\\}_{i=1}^{|\\mathcal{D}|}$ (such as all Wikipedia articles), the goal of retrieval augmented LMs is to generate the answer $\\bm{y}=[\\bm{s}_1,\\bm{s}_2,...,\\bm{s}_m]=[w_1, w_2,...,w_n]$ containing $m$ sentences or $n$ tokens leveraging information retrieved from the corpus.\n\nIn retrieval augmented LM, the LM typically pairs with a retriever that can retrieve a list of documents $\\mathcal{D}_{\\bm{q}}=\\text{ret}(\\bm{q})$ for a query $\\bm{q}$; the LM conditions on both the user input $\\bm{x}$ and retrieved documents $\\mathcal{D}_{\\bm{q}}$ to generate the answer.\nSince we focus on examining various methods of determining when and what to retrieve, we follow existing methods \\cite{icrlm-ram-2023,ircot-trivedi-2022} to prepend the retrieved documents before the user input to aid future generation for both baselines and our method for fair comparisons: $\\bm{y}=\\text{LM}([\\mathcal{D}_{\\bm{q}},\\bm{x}])$, where $[\\cdot,\\cdot]$ is concatenation following the specified order.\n\n\\subsection{Single-time Retrieval Augmented Generation}\\label{sec:baseline_single}\nThe most common choice is to directly use the user input as the query for retrieval and generate the complete answer at once $\\bm{y}=\\text{LM}([\\mathcal{D}_{\\bm{x}},\\bm{x}])$.\n\n\\subsection{Active Retrieval Augmented Generation}\\label{sec:activerag}\nTo aid long-form generation with retrieval, we propose active retrieval augmented generation.\nIt is a generic framework that actively decides when and what to retrieve through the generation process, resulting in the interleaving of retrieval and generation.\nFormally, at step $t (t \\ge 1)$, the retrieval query $\\bm{q}_t$ is formulated based on both the user input $\\bm{x}$ and previously generated output $\\bm{y}_{<t}=[\\bm{y}_0,...,\\bm{y}_{t-1}]$:\n\\begin{equation*}\n\\bm{q}_t=\\text{qry}(\\bm{x}, \\bm{y}_{<t}),\n\\end{equation*}\nwhere $\\text{qry}(\\cdot)$ is the query formulation function.\nAt the beginning ($t=1$), the previous generation is empty ($\\bm{y}_{<1}=\\emptyset$), and the user input is used as the initial query ($\\bm{q}_1=\\bm{x}$).\nGiven retrieved documents $\\mathcal{D}_{\\bm{q}_t}$, LMs continually generate the answer until the next retrieval is triggered or reaches the end:\n\\begin{equation*}\n\\bm{y}_t=\\text{LM}([\\mathcal{D}_{\\bm{q}_t},\\bm{x}, \\bm{y}_{<t}]),\n\\end{equation*}\nwhere $\\bm{y}_t$ represents the generated tokens at the current step $t$, and the input to LMs is the concatenation of the retrieved documents $\\mathcal{D}_{\\bm{q}_t}$, the user input $\\bm{x}$, and the previous generation $\\bm{y}_{<t}$.\nWe discard previously retrieved documents $\\cup_{t'<t}\\mathcal{D}_{\\bm{q}_{t'}}$ and only use the retrieved documents from the current step to condition the next generation to prevent reaching the input length limit of LMs.\n\n\\section{\\ours: Forward-Looking Active REtrieval Augmented Generation}\nOur intuition is that (1) LMs should only retrieve information when they do not have the necessary knowledge to avoid unnecessary or inappropriate retrieval, and (2) the retrieval queries should reflect the intents of future generations.\nWe propose two forward-looking active retrieval augmented generation (\\ours) methods to implement the active retrieval augmented generation framework.\nThe first method prompts the LM to generate retrieval queries when necessary while generating the answer using retrieval-encouraging instructions, denoted as \\oursr.\nThe second method directly uses the LM's generation as search queries, denoted as \\oursd, which iteratively generates the next sentence to gain insight into the future topic, and if uncertain tokens are present, retrieves relevant documents to regenerate the next sentence.\n\n\\begin{figure}[tb]\n\\includegraphics[width=1.0\\columnwidth, clip, keepaspectratio]{illustration_search.pdf}\n\\centering\n\\caption{An illustration of forward-looking active retrieval augmented generation with retrieval instructions (\\oursr). It iteratively generates search queries (shown in \\textcolor{gray}{\\textit{gray italic}}) to retrieve relevant information to aid future generations.}\n\\label{fig:illustration_search}\n\\end{figure}\n\n\\subsection{\\ours with Retrieval Instructions}\nInspired by Toolformer \\cite{toolformer-schick-2023}, a straightforward way of expressing information needs for retrieval is to generate ``[Search(query)]'' when additional information is needed~\\cite{toolformer-schick-2023}, e.g., ``The colors on the flag of Ghana have the following meanings. Red is for [Search(Ghana flag red meaning)] the blood of martyrs, ...''\nWhen working with GPT-3.5 models that offer only API access, we elicit such behavior by few-shot prompting~\\cite{gpt3-brown-2020}.\n\nSpecifically, for a downstream task, we place the search-related instruction and exemplars at the beginning as skill 1, followed by the instruction and exemplars of the downstream task as skill 2.\nGiven a test case, we ask LMs to combine skills 1 and 2 to generate search queries while performing the task.\nThe structure of the prompt is shown in Prompt~\\autoref{prompt:search}, and full details can be found in Prompt~\\autoref{prompt:2wiki_search}.\n\\begin{prompt}[title={Prompt \\thetcbcounter: retrieval instructions}, label=prompt:search]\nSkill 1. An instruction to guide LMs to generate search queries.\\\\\nSeveral search-related exemplars.\\\\\n\\\\\nSkill 2. An instruction to guide LMs to perform a specific downstream task (e.g., multihop QA).\\\\\nSeveral task-related exemplars.\\\\\n\\\\\nAn instruction to guide LMs to combine skills 1 and 2 for the test case.\\\\\nThe input of the test case.\n\\end{prompt}\nAs shown in \\autoref{fig:illustration_search}, when the LM generates ``[Search(query)]'' (shown in \\textcolor{gray}{\\textit{gray italic}}), we stop the generation and use the query terms to retrieve relevant documents, which are prepended before the user input to aid future generation until the next search query is generated or reaches the end.\nAdditional implementation details are included in \\autoref{ap:flare}.\n\n\\subsection{Direct \\ours}\nSince we cannot fine-tune black-box LMs, we found queries generated by \\oursr through retrieval instructions might not be reliable.\nTherefore, we propose a more direct way of forward-looking active retrieval that uses the next sentence to decide when and what to retrieve.\n\n\\subsubsection{Confidence-based Active Retrieval}\nAs shown in \\autoref{fig:illustration}, at step $t$, we first generate a temporary next sentence $\\hat{\\bm{s}}_t=\\text{LM}([\\bm{x}, \\bm{y}_{<t}])$ without conditioning on retrieved documents.\nThen we decide whether to trigger retrieval and formulate queries based on $\\hat{\\bm{s}}_t$.\nIf the LM is confident about $\\hat{\\bm{s}}_t$, we accept it without retrieving additional information; if not, we use $\\hat{\\bm{s}}_t$ to formulate search queries $\\bm{q}_t$ to retrieve relevant documents, and then regenerate the next sentence $\\bm{s}_t$.\nThe reason we utilize sentences as the basis of our iteration is due to their significance as semantic units that are neither too short nor too lengthy like phrases and paragraphs.\nHowever, our approach can also utilize phrases or paragraphs as the basis.\n\nSince LMs tend to be well-calibrated that low probability/confidence often indicates a lack of knowledge \\cite{when-jiang-2021,mostlyknow-kadavath-2022,varshney-oqa-2022}, we actively trigger retrieval if any token of $\\hat{\\bm{s}}_t$ has a probability lower than a threshold $\\theta \\in [0, 1]$.\n$\\theta=0$ means retrieval is never triggered, while $\\theta=1$ triggers retrieval every sentence.\n\\begin{equation*}\n\\bm{y}_t = \n\\begin{cases}\n\\hat{\\bm{s}}_t \\quad\\quad \\text{if all tokens of } \\hat{\\bm{s}}_t \\text{ have probs} \\ge \\theta \\\\\n\\bm{s}_t=\\text{LM}([\\mathcal{D}_{\\bm{q}_t}, \\bm{x}, \\bm{y}_{<t}]) \\quad\\quad \\text{otherwise}\n\\end{cases}\n\\end{equation*}\nwhere the query $\\bm{q}_t$ is formulated based on $\\hat{\\bm{s}}_t$.\n\n\\subsubsection{Confidence-based Query Formulation}\nOne way to perform retrieval is to directly use the next sentence $\\hat{\\bm{s}}_t$ as the query $\\bm{q}_t$.\nThis shares a similar spirit with methods that use generated hypothetical titles or paragraphs from LMs as retrieval queries or evidences \\cite{hyde-gao-2022,recitation-sun-2022,genorret-yu-2022,gar-mao-2021}.\nWe generalize such techniques to long-form generation where active information access is essential.\n\nWe found retrieving with the next sentence achieves significantly better results than with the previous context, as shown later in \\autoref{sec:exp_ablation}.\nHowever, it has a risk of perpetuating errors contained in it.\nFor example, if the LM produces the sentence ``Joe Biden attended the University of Pennsylvania'' instead of the correct fact that he attended the University of Delaware, using this erroneous sentence as a query might retrieve misleading information.\nWe propose two simple methods to overcome this issue as illustrated in \\autoref{fig:queryformulation}.\n\n\\begin{figure}[tb]\n\\includegraphics[width=1.0\\columnwidth, clip, keepaspectratio]{queryformulation.pdf}\n\\centering\n\\caption{Implicit and explicit query formulation. Tokens with low probabilities are marked with \\underline{underlines}.}\n\\label{fig:queryformulation}\n\\end{figure}\n\n\\paragraph{Masked sentences as implicit queries.} The first method masks out low-confidence tokens in $\\hat{\\bm{s}}_t$ with probabilities below a threshold $\\beta \\in [0, 1]$, where a higher $\\beta$ results in more aggressive masking.\nThis removes potential distractions from the sentence to improve retrieval accuracy.\n\n\\paragraph{Generated questions as explicit queries.} Another method is to generate explicit questions that target the low-confident span in $\\hat{\\bm{s}}_t$.\nFor example, if the LM is uncertain about ``the University of Pennsylvania'', a question like ``Which university did Joe Biden attend?'' can help retrieve relevant information.\nSelf-ask \\cite{selfask-press-2022} achieved this by manually inserting follow-up questions into downstream task exemplars as shown later in Prompt~\\autoref{prompt:selfask}, which requires task-specific annotation efforts.\nInstead, we developed a universal approach that generates questions for low-confidence spans without additional annotation.\nSpecifically, We first extract all spans from $\\hat{\\bm{s}}_t$ with probabilities below $\\beta$.\nFor each extracted span $\\bm{z}$, we prompt \\texttt{gpt-3.5-turbo} to generate a question $\\bm{q}_{t,\\bm{z}}$ that can be answered with the span:\n\\begin{prompt}[title={Prompt \\thetcbcounter: zero-shot question generation}]\nUser input $\\bm{x}$.\\\\\nGenerated output so far $\\bm{y}_{\\le t}$.\\\\\n\\\\\nGiven the above passage, ask a question to which the answer is the term/entity/phrase ``$\\bm{z}$''.\n\\end{prompt}\nWe retrieve using each generated question and interleave the returned documents into a single ranking list to aid future generations.\nIn summary, queries $\\bm{q}_t$ are formulated based on $\\hat{\\bm{s}}_t$ as follows:\n\\begin{equation*}\n\\bm{q}_t = \n\\begin{cases}\n\\emptyset \\quad\\quad \\text{if all tokens of } \\hat{\\bm{s}}_t \\text{ have probs} \\ge \\theta \\\\\n\\text{mask}(\\hat{\\bm{s}}_t) \\text{ or } \\text{qgen}(\\hat{\\bm{s}}_t) \\quad\\quad \\text{otherwise}\n\\end{cases}\n\\end{equation*}\n\n\\subsection{Implementation Details}\n\\paragraph{Base LM} We validate our method on one of the most advanced GPT-3.5 LMs \\texttt{text-davinci-003} by iteratively querying their API.\\footnote{\\url{\nhttps://api.openai.com/v1/completions} April 23.}\n\n\\paragraph{Document corpus and retrievers.}\nSince we focus on the integration of retrieval and generation, we use off-the-shelf retrievers that take queries as inputs and return a list of relevant documents.\nFor datasets that mainly rely on knowledge from Wikipedia, we use the Wikipedia dump from \\citet{dpr-2020-karpukhin} and employ BM25 \\cite{bm25-2009-robertson} as the retriever.\nFor datasets that rely on knowledge from the open web, we use the Bing search engine as our retriever.\\footnote{\\url{https://www.microsoft.com/en-us/bing/apis/bing-web-search-api}}\n\n\\paragraph{Retrieved document formatting.}\nMultiple retrieved documents are linearized according to their ranking and then added to the beginning of the user input using Prompt~\\autoref{prompt:searchresults}.\n\nOther implementation details such as sentence tokenization and efficiency are included \\autoref{ap:flare}.\n\n\\section{Multi-time Retrieval Baselines}\\label{sec:baseline_multi}\nExisting passive multi-time retrieval augmented LMs can also be formulated using our framework (\\autoref{sec:activerag}).\nIn this section, we formally introduce three baseline categories based on when and what to retrieve.\nThese baselines are not exact reproductions of the corresponding paper because many design choices differ which makes direct comparisons impossible.\nWe implemented them using the same settings, with the only variation being when and what to retrieve.\n\n\\paragraph{Previous-window} approaches trigger retrieval every $l$ tokens, where $l$ represents the window size. Generated tokens from the previous window are used as the query:\n\\begin{align*}\n\\bm{q}_t &= \\bm{y}_{t-1} \\quad (t \\ge 2), \\\\\n\\bm{y}_t &= [w_{(t-1)l+1},..., w_{tl}].\n\\end{align*}\nSome existing methods in this category are RETRO \\cite{retro-borgeaud-2022}, IC-RALM \\cite{icrlm-ram-2023}, which retrieve every few tokens, and KNN-LM \\cite{knnlm-2020-khandelwal}, which retrieves every token.\\footnote{Since KNN-LM uses the contextualized representation corresponding to the current decoding position to retrieve relevant information which encodes all previous tokens. Strictly speaking, $\\bm{q}_t$ should be $\\bm{y}_{<t}$.}\nWe follow \\citet{icrlm-ram-2023} to use a window size of $l=16$.\n\n\\paragraph{Previous-sentence} approaches trigger retrieval every sentence and use the previous sentence as the query, and IRCoT \\cite{ircot-trivedi-2022} belongs to this category:\n\\begin{align*}\n\\bm{q}_t &= \\bm{y}_{t-1} \\quad (t \\ge 2), \\\\\n\\bm{y}_t &= \\bm{s}_t.\n\\end{align*}\n\n\\paragraph{Question decomposition} approaches manually annotated task-specific exemplars to guide LMs to generate decomposed sub-questions while producing outputs.\nFor example, self-ask \\cite{selfask-press-2022}, a method in this category, manually inserts sub-questions in exemplars using Prompt~\\autoref{prompt:selfask}.\nFor the test case, retrieval is triggered dynamically whenever the model generates a sub-question.\n\nThe aforementioned approaches can retrieve additional information while generating.\nHowever, they have notable drawbacks: (1) Using previously generated tokens as queries might not reflect what LMs intend to generate in the future. (2) Retrieving information at a fixed interval can be inefficient because it might occur at inappropriate points. (3) Question decomposition approaches require task-specific prompt engineering, which restricts their generalizability in new tasks.\n\n\\section{Experimental Setup}\nWe evaluate the effectiveness of \\ours on 4 diverse knowledge-intensive tasks using few-shot in-context learning \\cite{radford-2019-gpt2,gpt3-brown-2020,liu-2023-ppp}.\nWe follow previous works \\cite{ircot-trivedi-2022} to sub-sample at most 500 examples from each dataset due to the cost of running experiments.\nDatasets, metrics, and settings are summarized in \\autoref{tab:setting} of \\autoref{ap:setting}.\nThe hyperparameters of \\ours are selected based on the development set and listed in \\autoref{tab:hyperparam}.\n\\ours refers to \\oursd if not specifically stated.\n\n\\paragraph{Multihop QA}\nThe goal of multihop QA is to answer complex questions through information retrieval and reasoning.\nWe use 2WikiMultihopQA \\cite{2wikimultihopqa-ho-2020} which contains 2-hop complex questions sourced from Wikipedia articles that require composition, comparison, or inference, e.g., ``Why did the founder of Versus die?''\nWe follow \\citet{selfconsist-wang-2022} to generate both the chain-of-thought and the final answer.\nExperimental setting details are included in \\autoref{ap:setting}.\n\nWe use regular expressions to extract the final answer from the output and compare it with the reference answer using exact match (EM), and token-level \\fone, precision, and recall.\n\n\\paragraph{Commonsense reasoning}\nCommonsense reasoning requires world and commonsense knowledge to generate answers.\nWe use StrategyQA \\cite{strategyqa-geva-2021} which is a collection of crowdsourced yes/no questions, e.g., ``Would a pear sink in water?''\nWe follow \\citet{cot-wei-2022} to generate both the chain-of-thought and the final yes/no answer.\nDetails are included in \\autoref{ap:setting}. \n\nWe extract the final answer and match it against the gold answer using exact match.\n\n\\paragraph{Long-form QA}\nLong-form QA aims to generate comprehensive answers to questions seeking complex information \\cite{eli5-fan-2019,asqa-stelmakh-2022}.\nWe use ASQA \\cite{asqa-stelmakh-2022} as our testbed where inputs are ambiguous questions with multiple interpretations, and outputs should cover all of them.\nFor example, ``Where do the Philadelphia Eagles play their home games?'' could be asking about the city, sports complex, or stadium. \nWe found in many cases it is challenging even for humans to identify which aspect of the question is ambiguous.\nTherefore, we created another setting (ASQA-hint) where we provide a brief hint to guide LMs to stay on track when generating answers.\nThe hint for the above case is ``This question is ambiguous in terms of which specific location or venue is being referred to.''\nExperimental setting details are included in \\autoref{ap:setting}.\n\nWe use metrics from \\citet{asqa-stelmakh-2022}, including EM, RoBERTa-based QA score (Disambig-\\fone), ROUGE \\cite{lin-2004-rouge}, and an overall score combining Disambig-\\fone and ROUGE (DR).\n\n\\begin{figure*}[tb]\n\\includegraphics[width=1.0\\textwidth, clip, keepaspectratio]{exp_all6.pdf}\n\\centering\n\\caption{Comparision between \\ours and baselines across all tasks/datasets. We report the primary metric for each dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp.}\n\\label{fig:exp_all}\n\\end{figure*}\n\n\\paragraph{Open-domain summarization}\nThe goal of open-domain summarization is to generate a comprehensive summary about a topic by gathering information from open web \\cite{openmds-giorgi-2022}.\nWe use WikiAsp \\cite{wikiasp-hayashi-2021} which aims to generate aspect-based summaries about entities from 20 domains in Wikipedia, e.g., ``Generate a summary about Echo School (Oregon) including the following aspects: academics, history.''\nExperimental setting details are included in \\autoref{ap:setting}.\n\nMetrics include ROUGE, named entity-based \\fone, and UniEval \\cite{unieval-zhong-2023} which measures factual consistency.\n\n\\section{Experimental Results}\nWe first report overall results across 4 tasks/datasets and compare the performance of \\ours with all the baselines introduced in \\autoref{sec:baseline_multi}.\nWe then run ablation experiments to study the efficacy of various design choices of our method.\n\n\\subsection{Comparison with Baselines}\\label{sec:exp_main}\n\\paragraph{Overall results.}\nThe overall performance of \\ours and baseline across all tasks/datasets are reported in \\autoref{fig:exp_all}.\n\\ours outperforms all baseline on all tasks/datasets, indicating that \\ours is a generic method that can effectively retrieve additional information throughout the generation.\n\nAmong various tasks, multihop QA shows the most significant improvement.\nThis is largely due to the task's clear definition and specific objective of producing the final answer through a 2-hop reasoning process, which makes it easier for LMs to generate on-topic output.\nIn contrast, ASQA and WikiAsp are more open-ended, which increases the difficulty of both generation and evaluation.\nThe improvement on ASQA-hint is larger than that of ASQA because identifying ambiguous aspects is challenging even for humans in many cases, and providing a generic hint helps LMs to stay on topic.\n\n\\begin{table}[tb]\n\\small\n\\centering\n\\begin{tabular}{lcccc}\n\\toprule\n\\textbf{Methods} & \\textbf{EM} & \\textbf{\\fone} & \\textbf{Prec.} & \\textbf{Rec.} \\\\\n\\midrule\nNo retrieval & 28.2 & 36.8 & 36.5 & 38.6 \\\\\nSingle-time retrieval & 39.4 & 48.8 & 48.6 & 51.5 \\\\\n\\midrule\n\\multicolumn{5}{c}{\\emph{Multi-time retrieval}} \\\\\nPrevious-window & 43.2 & 52.3 & 51.7 & 54.5 \\\\\nPrevious-sentence & 39.0 & 49.2 & 48.9 & 51.8 \\\\\nQuestion decomposition & 47.8 & 56.4 & 56.1 & 58.6 \\\\\n\\oursr (ours) & 42.4 & 49.8 & 49.1 & 52.5 \\\\\n\\oursd (ours) & \\textbf{51.0} & \\textbf{59.7} & \\textbf{59.1} & \\textbf{62.6} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\ours and baselines on 2WikiMultihopQA. Previous-window \\cite{retro-borgeaud-2022,icrlm-ram-2023}, previous-sentence \\cite{ircot-trivedi-2022}, and question decomposition \\cite{selfask-press-2022,react-yao-2022} methods are reimplemented for fair comparisons.}\n\\label{tab:2wikihop}\n\\end{table}\n\n\\begin{table*}[tb]\n\\centering\n\\begin{tabular}{@{}l@{\\smallcol}c|c@{\\smallcol}c@{\\smallcol}c@{\\smallcol}c|c@{\\smallcol}c@{\\smallcol}c@{\\smallcol}c|c@{\\smallcol}c@{\\smallcol}c@{}}\n\\toprule\n\\textbf{Datasets} & \\textbf{StrategyQA} & \\multicolumn{4}{c|}{\\textbf{ASQA}} & \\multicolumn{4}{c|}{\\textbf{ASQA-hint}} & \\multicolumn{3}{c}{\\textbf{WikiAsp}}  \\\\\n\\textbf{Metrics} & \\textbf{EM} & \\textbf{EM} & \\textbf{D-\\fone} & \\textbf{R-L} & \\textbf{DR} & \\textbf{EM} & \\textbf{D-\\fone} & \\textbf{R-L} & \\textbf{DR} & \\textbf{UniEval} & \\textbf{E-\\fone} & \\textbf{R-L} \\\\\n\\midrule\nNo retrieval & 72.9 & 33.8 & 24.2 & 33.3 & 28.4 & 40.1 & 32.5 & 36.4 & 34.4 & 47.1 & 14.1 & 26.4 \\\\\nSingle-time retrieval & 68.6 & 40.0 & 27.1 & 34.0 & 30.4 & 43.2 & 34.8 & 37.4 & 36.0 & 52.4 & 17.4 & 26.9 \\\\\n\\midrule\n\\multicolumn{13}{c}{\\emph{Multi-time retrieval}} \\\\\nPrevious-window & 71.2 & 39.9 & 27.0 & \\textbf{34.3} & 30.4 & 43.7 & 35.7 & 37.5 & 36.6 & 51.8 & 18.1 & 27.3 \\\\\nPrevious-sentence & 71.0 & 39.9 & 27.9 & \\textbf{34.3} & 30.9 & 44.7 & 35.9 & 37.5 & 36.7 & 52.6 & 17.8 & 27.2 \\\\\n\\ours (ours) & \\textbf{77.3} & \\textbf{41.3} & \\textbf{28.2} & \\textbf{34.3} & \\textbf{31.1} & \\textbf{46.2} & \\textbf{36.7} & \\textbf{37.7} & \\textbf{37.2} & \\textbf{53.4} & \\textbf{18.9} & \\textbf{27.6} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Comparison between \\ours and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-\\fone is Disambig-\\fone, R-L is ROUGE-L, and E-\\fone is named entity-based \\fone.}\n\\label{tab:other}\n\\end{table*}\n\n\\paragraph{Thorough comparisons with baselines.}\nThe performance of all baselines on 2WikiMultihopQA are reported in \\autoref{tab:2wikihop}.\n\\ours outperforms all baselines by a large margin, which confirms that forward-looking active retrieval is highly effective.\nMost multi-time retrieval augmented approaches outperform single-time retrieval but with different margins.\nThe improvement of retrieving using the previous sentence is relatively small which we hypothesize is mainly because the previous sentence often describes entities or relations different from those in the next sentence in 2WikiMultihopQA.\nWhile the previous-window approach might use the first half of a sentence to retrieve information potentially helpful for generating the second half.\nAmong all baselines, the question decomposition approach \\cite{selfask-press-2022} achieves the best performance. which is not surprising since the in-context exemplars manually annotated with decomposed sub-questions (Prompt~\\autoref{prompt:selfask}) guide LMs to generate sub-questions that align with the topic/intent of future generations.\n\\ours outperforms this baseline, indicating that manual exemplar annotation is not necessary for effective future-aware retrieval.\nThe gap between \\oursr and question decomposition is large, indicating that teaching LMs to generate search queries using task-generic retrieval instructions and exemplars is challenging.\n\nWe report all metrics for the other datasets in \\autoref{tab:other}.\n\\ours outperforms baselines with respect to all metrics.\nRetrieval using the previous window underperforms single-time retrieval on ASQA, which we hypothesize is because the previous window does not accurately reflect future intent.\nSince we focus on evaluating factuality, metrics with an emphasis on factual content (such as EM, Disambig-\\fone, UniEval) are more reliable than metrics computed over all tokens (ROUGE-L).\n\n\\subsection{Ablation Study}\\label{sec:exp_ablation}\n\n\\begin{table}[tb]\n\\small\n\\centering\n\\begin{tabular}{@{}l@{\\smallcol}c@{\\smallcol}c@{\\smallcol}c@{\\smallcol}c|c@{\\smallcol}c@{\\smallcol}c@{\\smallcol}c}\n\\toprule\n& \\multicolumn{4}{c|}{\\textbf{2WikiMultihopQA}} & \\multicolumn{4}{c}{\\textbf{ASQA-hint}} \\\\\n& \\textbf{EM} & \\textbf{\\fone} & \\textbf{Prec.} & \\textbf{Rec.} & \\textbf{EM} & \\textbf{D-\\fone} & \\textbf{R-L} & \\textbf{DR} \\\\\n\\midrule\nPrevious & 39.0 & 49.2 & 48.9 & 51.8 & 42.5 & 34.1 & 36.9 & 35.5 \\\\\nNext & 48.8 & 57.6 & 57.1 & 60.5 & 45.9 & 35.7 & 37.5 & 36.6 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{A head-to-head comparison between using the previous sentence and the next sentence for retrieval.}\n\\label{tab:prev_next}\n\\end{table}\n\n\\begin{table}[tb]\n\\small\n\\centering\n\\begin{tabular}{lcccc}\n\\toprule\n\\textbf{\\#Tokens} & \\textbf{EM} & \\textbf{\\fone} & \\textbf{Prec.} & \\textbf{Rec.} \\\\\n\\midrule\n16 & 43.2 & 52.3 & 51.7 & 54.5 \\\\\n32 & 43.6 & 52.4 & 52.0 & 55.0 \\\\\n48 & 40.0 & 49.3 & 49.0 & 52.0 \\\\\nAll & 39.0 & 48.5 & 48.2 & 51.1 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Previous-window approaches using different numbers of tokens as queries.}\n\\label{tab:prev_tokens}\n\\end{table}\n\n\\paragraph{Importance of forward-looking retrieval.}\nWe first validate that forward-looking retrieval is more effective than past-context-based retrieval.\nWe run ablation experiments on 2WikiMultihopQA and ASQA-hint comparing retrieval using the previous versus the next sentence.\nSpecifically, both methods retrieve every sentence and directly use the complete previous/next sentence as queries.\nAs shown in \\autoref{tab:prev_next}, using the next sentence to retrieve is clearly better than using the previous sentence, confirming our hypothesis.\n\nWe also run previous-window approaches using different numbers of past tokens as queries.\nAs shown in \\autoref{tab:prev_tokens}, using too many tokens ($>32$) in the past hurts the performance, further confirming our hypothesis that previous context might not be relevant to intent of future generations.\n\n\\begin{figure}[tb]\n\\includegraphics[width=1.0\\columnwidth, clip, keepaspectratio]{adaptive.pdf}\n\\centering\n\\caption{Performance (EM) of \\ours with respect to the percentage of steps/sentences with retrieval on 2WikiMultihopQA and StrategyQA.}\n\\label{fig:exp_adaptive}\n\\end{figure}\n\n\\paragraph{Importance of active retrieval.}\nNext, we investigate how active retrieval threshold $\\theta$ affects performance.\nTo alter our method from not retrieving to retrieving every sentence, we adjust the confidence threshold $\\theta$ that determines when to trigger retrieval from 0 to 1.\nWe then calculate the proportion of steps/sentences where retrieval is activated, and present the performance based on it.\nAs shown in \\autoref{fig:exp_adaptive}, on 2WikiMultihopQA, the performance plateaus when the retrieval percentage exceeds 60\\%, indicating that retrieval when LMs are confident is not necessary.\nOn StrategyQA, the performance drops when the retrieval percentage exceeds 50\\%, indicating that unnecessary retrieval can introduce noise and impede the original generation process.\nWe found triggering retrieval for 40\\%-80\\% of sentences usually leads to a good performance across tasks/datasets.\n\n\\begin{table}[tb]\n\\small\n\\centering\n\\begin{tabular}{lcccc}\n\\toprule\n\\textbf{$\\beta$} & \\textbf{EM} & \\textbf{\\fone} & \\textbf{Prec.} & \\textbf{Rec.} \\\\\n\\midrule\n0.0 & 0.488 & 0.576 & 0.571 & 0.605 \\\\\n0.2 & 0.498 & 0.588 & 0.582 & 0.616 \\\\\n0.4 & 0.510 & 0.597 & 0.591 & 0.627 \\\\\n0.6 & 0.506 & 0.593 & 0.586 & 0.622 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Performance of \\ours with respect to the masking threshold $\\beta$ on 2WikiMultihopQA.}\n\\label{tab:exp_mask}\n\\end{table}\n\n\\begin{table}[tb]\n\\small\n\\centering\n\\begin{tabular}{l@{\\smallcol}c@{\\smallcol}c@{\\smallcol}c@{\\smallcol}c|c@{\\smallcol}c@{\\smallcol}c}\n\\toprule\n& \\multicolumn{4}{c|}{\\textbf{ASQA-hint}} & \\multicolumn{3}{c}{\\textbf{WikiAsp}} \\\\\n& \\textbf{EM} & \\textbf{D-\\fone} & \\textbf{R-L} & \\textbf{DR} & \\textbf{UniEval} & \\textbf{E-\\fone} & \\textbf{R-L} \\\\\n\\midrule\nImplicit & 45.7 & 36.9 & 37.7 & 37.3 & 53.4 & 18.8 & 27.7 \\\\\nExplicit & 46.2 & 36.7 & 37.7 & 37.2 & 53.4 & 18.9 & 27.6 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{A comparison between implicit and explicit query formulation methods in \\ours.}\n\\label{tab:implicit_explicit_query}\n\\end{table}\n\n\\paragraph{Effectiveness of different query formulation methods}\nWe study implicit query formation by masking and explicit query formulation through question generation.\nIn \\autoref{tab:exp_mask}, we compare the performance of \\ours with different masking thresholds $\\beta$.\nRetrieving directly with the complete sentence ($\\beta=0$) is worse than masking tokens with low probabilities, confirming our hypothesis that low-confidence erroneous tokens can distract retrievers.\nWe compare implicit and explicit query formulation methods in \\autoref{tab:implicit_explicit_query}.\nPerformances of both methods are similar, indicating that both methods can effectively reflect information needs.\n\n\\section{Related Work}\nWe refer to \\autoref{sec:baseline_single} and \\autoref{sec:baseline_multi} for extensively discussion on single-time and multi-time retrieval augmented LMs, which is the most relevant area to this paper.\n\n\\paragraph{Iterative and adaptive retrieval}\nIterative retrieval and refinement has been studied in both text and code generation tasks \\cite{peng-check-2023,repocoder-jiang-2023,zemlyanskiy-gr-2022,retfeed-yu-2023}.\n\\ours differs from these methods in the granularity of generation and retrieval strategies.\nAdaptive retrieval has been studied in single-time retrieval scenarios based on either question popularity or generation probabilities \\cite{mallen-nottrust-2022,oyster-li-2023}, while we focus on long-form generation requiring active information access.\n\\paragraph{Browser-enhanced LMs} WebGPT \\cite{webgpt-nakano-2021} and WebCPM \\cite{webcpm-qin-2023} train LMs to interact with browser to enhance factuality using reinforcement learning or supervised training where multiple queries can be triggered before generation.\n\\ours is built on text-based retrievers but can be combined with a browser to potentially improve retrieval quality.\n\n\\section{Conclusion}\nTo aid long-form generation with retrieval augmentation, we propose an active retrieval augmented generation framework that decides when and what to retrieve during generation.\nWe implement this framework with forward-looking active retrieval that iteratively uses the upcoming sentence to retrieve relevant information if it contains low-confidence tokens and regenerates the next sentence.\nExperimental results on 4 tasks/datasets demonstrate the effectiveness of our methods.\nFuture directions include better strategies for active retrieval and developing efficient LM architectures for active information integration.\n\n\\section{Limitations}\\label{sec:limit}\nWe also conduct experiments on Wizard of Wikipedia \\cite{wow-dinan-2019} and ELI5 \\cite{eli5-fan-2019}, and found that \\ours did not provide significant gains.\nWizard of Wikipedia is a knowledge-intensive dialogue generation dataset where the output is relatively short ($\\sim$20 tokens on average) so retrieving multiple disparate pieces of information might not be necessary.\nELI5 \\cite{eli5-fan-2019} is a long-form QA dataset requiring in-depth answers to open-ended questions.\nDue to issues mentioned in \\citet{hurdle-krishna-2021} such as difficulties of grounding generation in retrieval and evaluation, both single-time retrieval and \\ours did not provide significant gains over not using retrieval.\nFrom an engineering perspective, interleaving generation and retrieval with a naive implementation increases both overheads and the cost of generation.\nLMs need to be activated multiple times (once for each retrieval) and a caching-free implementation also requires recomputing the previous activation each time after retrieval.\nThis issue can be potentially alleviated with special architectural designs that encode the retrieved documents $\\mathcal{D}_{\\bm{q}_t}$ and the input/generation ($\\bm{x}$/$\\bm{y}_{<t}$) independently.\n\n\\section*{Acknowledgements}\nThis work was supported in part by a grant from the Singapore Defence Science and Technology Agency and the IBM PhD Fellowship.\nWe thank Chunting Zhou, Amanda Bertsch, Uri Alon, Hiroaki Hayashi, Harsh Trivedi, Patrick Lewis, Timo Schick, Kaixin Ma, Shuyan Zhou, and Songwei Ge for their insightful discussions and help with the experiments.\n\n\\newpage\n\n\\newpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n\nDynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs).\nThere are two key elements of this paradigm: identifying the optimal moment to activate the retrieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve).\nHowever, current dynamic RAG methods fall short in both aspects. \nFirstly, the strategies for deciding when to retrieve often rely on static rules. \nMoreover, the strategies for deciding what to retrieve typically limit themselves to the LLM's most recent sentence or the last few tokens, while the LLM's information needs may span across the entire context.\nTo overcome these limitations, we introduce a new framework, DRAGIN, i.e., \\underline{D}ynamic \\underline{R}etrieval \\underline{A}ugmented \\underline{G}eneration based on the \\underline{I}nformation \\underline{N}eeds of LLMs. \nOur framework is specifically designed to make decisions on when and what to retrieve based on the LLM's information needs during the text generation process.\nWe evaluate DRAGIN along with existing methods comprehensively over 4 knowledge-intensive generation datasets. Experimental results show that DRAGIN achieves superior performance on all tasks, demonstrating the effectiveness of our method\\footnote{We have open-sourced all the code, data, and models in GitHub: https://github.com/oneal2000/DRAGIN/tree/main}. \n\n\\end{abstract}\n\n\\section{Introduction}\n\nIn recent years, large language models (LLMs) have made significant advancements across various natural language processing (NLP) tasks, quickly becoming a critical element in numerous AI applications~\\cite{brown2020language,chowdhery2022palm,touvron2023llama,scao2022bloom,zhang2022opt}. Despite their impressive capabilities, these models often produce text that seems coherent and plausible but factually incorrect, a problem commonly known as hallucination~\\cite{maynez2020faithfulness,zhou2020detecting,liu2021token,ji2023survey,su2024unsupervised}.\n\nTo mitigate this issue, Retrieval-Augmented Generation (RAG) has emerged as a prominent solution. \nRAG enhances LLMs by retrieving and incorporating relevant information from external databases into the LLMs' inputs. \nIt has demonstrated superior effectiveness across numerous NLP challenges~\\cite{khandelwal2019generalization,borgeaud2022improving,lewis2020retrieval,guu2020retrieval,izacard2020leveraging,jiang2022retrieval,shi2023replug}. \nTraditional methods of RAG typically rely on single-round retrieval, using the LLM's initial input to retrieve relevant information from external corpora. \nWhile this method is effective for straightforward tasks, it tends to fall short for complex multi-step tasks and long-form generation tasks~\\cite{jiang2023active}. \nIn contrast, dynamic RAG~\\cite{trivedi2022interleaving,borgeaud2022improving,ram2023context,jiang2023active} performs multiple times of retrieval during the generation process of LLMs. \nIt includes two steps: identifying the optimal moment to activate the retrieval module (deciding when to retrieve), and crafting the appropriate query once retrieval is triggered (determining what to retrieve).\nDepending on when and what to retrieve, a variety types of methods have been proposed in this direction.\nFor example, IRCoT~\\cite{trivedi2022interleaving} adopts a global augmentation method where retrieval is conducted for each generated sentence, with the latest generated sentence used as the query. \nRETRO~\\cite{borgeaud2022improving} and IC-RALM~\\cite{ram2023context} define a sliding window and trigger the retrieval module based on a preset number of processed tokens, and the last $n$ tokens are used as the query. \n\nHowever, existing dynamic RAG methods face several critical challenges, primarily in determining the optimal timing for retrieval and formulating effective queries when retrieval is triggered. \nFirst of all, existing approaches often rely on static rules to decide when to retrieve, neglecting the assessment of necessity and potential risks involved.\nOn the one hand, depending on the quality of the input query and retrieval models, unnecessary retrieval augmentation may introduce irrelevant or noisy data to LLMs which could jeopardize the quality of the outputs. \nOn the other hand, conducting retrieval augmentation will inevitably increase the time and computation cost of LLM inference, such cost is unworthy if LLMs can generate correct outputs by themselves.\nAdditionally, the strategies of existing studies in determining what to retrieve often restrict themselves to the LLM's most recent sentence or the last few tokens. \nThis approach may not capture the model's real-time information needs since the LLM's information needs may actually be related to terms that span the entire context. Retrieving documents in this manner is thus suboptimal in many cases.\n\nTo overcome these limitations, we introduce a new framework, \\textbf{DRAGIN}, i.e., \\underline{D}ynamic \\underline{R}etrieval \\underline{A}ugmented \\underline{G}eneration based on the \\underline{I}nformation \\underline{N}eeds of LLMs. Our framework is specifically designed to make decisions on when and what to retrieve, based on the LLM's information needs during the text generation process.\nFor the timing of retrieval, we propose \\textbf{RIND}: \\underline{R}eal-time \\underline{I}nformation \\underline{N}eeds \\underline{D}etection, which considers the LLM's uncertainty about its own generated content, the influence of each token on subsequent tokens, and the semantic significance of each token.\nFor the formulation of retrieval queries, we propose \\textbf{QFS}: \\underline{Q}uery \\underline{F}ormulation based on \\underline{S}elf-attention, which innovates query formulation by leveraging the LLM's self-attention across the entire context.\nDRAGIN is a lightweight RAG framework that can be incorporated into any Transformer-based LLMs without further training, fine-tuning, or prompt engineering.\n\nWe comprehensively evaluate DRAGIN along with existing dynamic RAG frameworks over four knowledge-intensive generation benchmarks. Experimental results show that DRAGIN achieves superior performance on all datasets, demonstrating the effectiveness of our method. \nMoreover, the results of the ablation study indicate that our proposed new strategies for \"when to retrieval\" (i.e., RIND) and \"what to retrieval\" (i.e., QFS) perform uniformly better than other strategies in existing RAG methods despite retrieval models and LLMs. \n\nIn summary, the contributions of our paper are as follows:\n\n\\begin{itemize}[leftmargin=*]\n\\item We propose a novel dynamic RAG framework: DRAGIN. In contrast to previous works, our framework optimizes when and what to retrieve based on the real-time information needs of the LLM.\n\n\\item We evaluate existing dynamic RAG methods and DRAGIN on four knowledge-intensive datasets using three different LLMs. Experimental results indicate that DRAGIN achieves state-of-the-art (SOTA) performance.\n\n\\end{itemize}\n\n\\section{Related Work}\n\\subsection{Single-round Retrieval-augmented LLM}\n\nLLMs have demonstrated significant effectiveness across a wide range of tasks. \nHowever, their built-in knowledge can sometimes fall short when dealing with knowledge-intensive tasks. \nTo address this limitation, Retrieval-Augmented Generation (RAG) strategies are widely employed to enhance the performance of LLMs.\nOne of the most direct methods is single-round retrieval augmentation~\\cite{khandelwal2019generalization,borgeaud2022improving,lewis2020retrieval,guu2020retrieval,izacard2020leveraging,jiang2022retrieval,shi2023replug}, which involves using the initial input as a query to retrieve information from an external corpus. \nThe retrieved external knowledge is then incorporated as part of the input for the model. \nPrevious research has explored single-round retrieval augmentation extensively. \nFor instance, REPLUG~\\cite{shi2023replug} treats LLMs as a black box and leverages them to generate training data for the retrieval model. \nFrom a different perspective, UniWeb~\\cite{li2023web} proposes an adaptive search engine-assisted learning method that can self-assess whether the LLM requires retrieval augmentation. \n\n\\subsection{Multi-round Retrieval-augmented LLM}\n\nSingle-round retrieval can be relatively effective for simple tasks or cases where user information needs are clear-cut. \nHowever, for complex tasks or tasks involving the generation of lengthy text, such as long-form question answering, multi-hop reasoning, chain-of-thought reasoning, etc., relying solely on the user's initial input for retrieval may not adequately cover all the external knowledge that the model requires~\\cite{jiang2023active}. \nTherefore, some researchers have begun to explore multi-round retrieval augmentation. For example, RETRO~\\cite{borgeaud2022improving} and IC-RALM~\\cite{ram2023context} trigger retrieval every 4 to 32 tokens, and IRCot~\\cite{trivedi2022interleaving} triggers retrieval every sentence. \nHowever, solely relying on fixed interval-based retrieval without considering the information needs of the LLM itself could produce suboptimal results. \nInspired by this, FLARE~\\cite{jiang2023active} triggers retrieval when encountering an uncertain token.\nSpecifically, if any token in the generated text has a probability lower than a certain threshold, the retrieval module is triggered. \n\n\\begin{figure}[t]\n\\centering\n    \\includegraphics[width=\\columnwidth]{latex/pics/framework.pdf}\n    \\small{\\caption{An illustration of our DRAGIN framework.} \\label{pic:framework}}\n\\end{figure}\n\n\\section{Methodology}\nIn this section, we introduce the DRAGIN framework in detail. DRAGIN consists of two components: Real-time Information Needs Detection (RIND) and Query Formulation based on Self-attention (QFS), as illustrated in Figure~\\ref{pic:framework}. We introduce RIND in section~\\ref{sec:RIND}, and QFS in section~\\ref{sec:QFS}.\n\n\\subsection{Real-time Information Need Detection}\n\\label{sec:RIND}\n\nAs discussed above, most existing dynamic RAG frameworks trigger the retrieval module based on static, predefined rules. \nTo the best of our knowledge, the only notable exception is FLARE~\\cite{jiang2023active} which triggers retrieval dynamically when the LLM's confidence (i.e., the generation probability) on the next token is lower than certain thresholds. \n\nHowever, the necessity of retrieval augmentation not only depends on the generation confidence, but also depends on the importance of the token, the semantic  of the token, and the influence of each token on subsequent tokens.\nTo address the limitations of the existing approaches, we propose an enhanced approach for triggering retrieval within dynamic RAG frameworks, named Real-time Information Needs Detection (RIND). \nThis method refines the retrieval activation process by evaluating not only the uncertainty of each token, but also its semantic contribution and the impact on the following context.\n\nRIND begins by quantifying the uncertainty of each token generated during the LLM's inference process. This is accomplished by recording the entropy of the token's probability distribution across the vocabulary. \nConsider an output sequence generated by an LLM, denoted as \\( T = \\{t_1, t_2, \\ldots, t_n\\} \\), with each \\( t_i \\) representing an individual token within the sequence at position $i$.\nFor any token \\(t_i\\), the entropy \\(\\mathcal{H}_i\\) is computed as follows:\n\\begin{equation}\n\\small\n    \\mathcal{H}_i = -\\sum_{{v} \\in \\mathcal{V}} p_i({v}) \\log p_i({v}),\n\\end{equation}\nwhere $p_i(v)$ denotes the probability of generating the token $v$ over all tokens in the vocabulary $\\mathcal{V}$ at position $i$.\nThis measurement of uncertainty serves as the first dimension in our multi-faceted evaluation of tokens.\n\nIn addition, RIND leverages the self-attention mechanism inherent in Transformer-based LLMs to allocate weights to tokens, which represent the tokens' impact on the subsequent context.\nSpecifically, for any given token \\(t_i\\), we quantify its influence by recording the maximum attention value \\(a_{\\max}(i)\\), which records the maximum attention from all following tokens~\\footnote{We choose the attention scores of the last Transformer layer of the LLM.}.  \nThe attention matrix \\(A\\) is computed as follows:\n\\begin{equation}\n\\small\nA = \\text{softmax}\\left(\\text{mask}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)\\right),\n\\end{equation}\nwhere \\(Q\\) is the query matrix, \\(K\\) is the key matrix, \\(d_k\\) denotes the dimensionality of a key vector. \nThe mask function is applied to the scaled dot-product matrix \\( M = \\frac{QK^\\top}{\\sqrt{d_k}} \\) to prevent each position from attending to subsequent positions. Specifically, for the square matrix \\( M \\) (with dimensions equal to the sequence length), we set \\( M_{i,j} = -\\infty \\) whenever \\( i < j \\). This effectively masks out the upper triangular part of the matrix. The softmax function is then applied row-wise to produce the attention weights.\n\nFollowing this, the maximum attention value \\(a_{\\max}(i)\\) for token \\(t_i\\) is identified by locating the highest \\(A_{j,i}\\) for all \\(j > i\\):\n\\begin{equation}\n\\small\na_{\\max}(i) = \\max_{j > i} A_{j,i}\n\\end{equation}\n\nConsider the semantic contribution of each token, RIND employs a binary semantic indicator to filter out stopwords, thus concentrating on tokens with significant semantic value:\n\\begin{equation}\n\\small\n s_i = \\begin{cases} 0, & \\text{if } t_i \\in S \\\\ 1, & \\text{otherwise} \\end{cases} ,\n\\end{equation}\nwhere $S$ is the stopwords set, $s_i$ is the semantic contribution score of the token $t_i$. This process ensures that only semantically potent tokens contribute to the retrieval decision-making process.\n\nCombining uncertainty, significance, and semantics, RIND computes a comprehensive score for each token $t_i$:\n\\begin{equation}\n\\small\n\\mathcal{S}_{RIND}(t_i) = \\mathcal{H}_i \\cdot a_{\\max}(i) \\cdot s_i\n\\end{equation}\nLet $T = \\{t_1, t_2, \\ldots, t_n\\}$ represent the set of tokens already generated by the LLM. The retrieval module activates when the score $\\mathcal{S}_{RIND}(t_i)$ for any token exceeds a predefined threshold, \\(\\theta\\).\n\n\\subsection{Query Formulation based on Self-attention}\n\\label{sec:QFS}\n\nOnce the position to conduct retrieval augmentation is determined, the next step in the RAG framework is to formulate a query to retrieve necessary information from external databases for the continued generation of LLMs.\nIn the existing dynamic RAG frameworks, all the query formulation methods limit their focus to the LLM's most recent sentence or the last few tokens. This narrow scope fails to adequately cater to the model's real-time information needs, which may span across the entire context. To overcome the shortcomings of these approaches, we propose a novel strategy that utilizes the self-attention mechanisms inherent in Transformer-based LLMs. Our method, termed \"Query Formulation based on Self-Attention\" (QFS), seeks to ascertain the LLM's information needs more precisely by examining its understanding of the full context.\n\nConsider an output sequence generated by an LLM, denoted as \\( T = \\{t_1, t_2, \\ldots, t_n\\} \\), with each \\( t_i \\) representing an individual token within the sequence. Suppose the RIND module identifies the token at position \\( i \\), which requires external knowledge and triggers the retrieval module. \nThe QFS approach then focuses on this specific position to formulate a query. \nFor the token at position \\( i \\), the QFS method evaluates the attention weights across the preceding token sequence \\( \\{t_{i-1}, t_{i-2}, ..., t_1\\} \\). \nSince the generation of \\( t_i \\) by the LLM is based on its interpretation of the entire preceding context, the attention weights reflect the model's self-assessed importance of each token in generating \\( t_i \\). \nThe QFS method prioritizes these tokens based on their attention scores, selecting the top \\( n \\) tokens to construct the query. \nThe query formulation process includes the following steps:\n(1) Extract the attention scores of the last Transformer layer \\( A_i = \\{a_{i,1}, a_{i,2}, ..., a_{i,i-1}\\} \\) for each token \\( t_i \\) in \\( T \\), where \\( a_{i,j} \\) represents the attention score assigned by \\( t_i \\) to \\( t_j \\); \n(2) Sort \\( A_i \\) in descending order to identify the top \\( n \\) tokens with the highest attention scores;\n(3) Find the words corresponding to these tokens from the vocabulary and arrange them according to their original order in the text;\n(4) Construct the query \\( Q_i \\) using the words from these top \\( n \\) tokens, ensuring the query reflects the most relevant aspects of the context as determined by the LLM's self-attention mechanism.\n\n\\subsection{Continue Generation after Retrieval}\n\nOnce the RIND module detects the position $i$ that needs external knowledge, the QFS module creates the query and utilizes an off-the-shelf retrieval model (e.g. BM25) to retrieve relevant information from external knowledge bases. \nSuppose the retrieved documents are denoted as $Di_1$, $Di_2$, and $Di_3$.\nUpon successful retrieval, the next step of the dynamic RAG framework is to integrate this external knowledge into the LLM's generation process. This integration begins with truncating the LLM's output at the identified position $i$ for retrieval augmentation:\n\\begin{equation}\n\\small\nT' = \\text{truncate}(T, t_{i}),\n\\end{equation}\nwhere \\(T'\\) represents the truncated output, \\(T\\) is the original sequence generated by the LLM, and \\(t_{i}\\) is the token at which the need for external knowledge was identified by RIND. \nTo integrate the retrieved knowledge $Di_1$, $Di_2$, and $Di_3$, we adopt a meticulously designed prompt template~\\footnote{The specific content of the prompt template is presented in Appendix~\\ref{appendix:prompt}.}, which is structured as follows:\n\\begin{tcolorbox}[colback=lightgray!20,colframe=darkgray!80,title=The entire input for LLM:]\nBelow are the external knowledge references:\n\n[1] \\textbf{$Di_1$}\n\n[2] \\textbf{$Di_2$}\n\n[3] \\textbf{$Di_3$}\n\nPlease answer the question based on the external knowledge:\n\nQuestion: xxx\n\nAnswer: \\textbf{T'}\n\\end{tcolorbox}\nAt this point, the LLM continues generating content based on the retrieved external knowledge and the truncated output $T'$.\nFollowing the integration of the retrieved knowledge, the LLM resumes generating content from the truncation point, enhanced with additional information. \nThis procedure allows the LLM to bridge the previously identified knowledge gap, facilitating a more informed and precise continuation of its output.\n\nSuppose at a subsequent position $j$ where RIND detects again that the LLM requires external knowledge. In that case, the QFS module is triggered again at position $j$ to generate a new query, retrieving a new set of documents $Dj_1$, $Dj_2$, and $Dj_3$ to replace $Di_1$, $Di_2$, and $Di_3$.\nThe LLM will then continue generating from position $j$ based on the newly retrieved documents, following the same process.\n\n\\section{Experimental Setup}\n\n\\subsection{Datasets}\n\nWe choose two MultihopQA datasets \\textbf{2WikiMultihopQA}~\\cite{ho2020constructing} and \\textbf{HotpotQA}~\\cite{yang2018hotpotqa} to evaluate the RAG framework's ability to answer complex questions that require multi-hop reasoning. We choose the \\textbf{IIRC}~\\cite{ferguson2020iirc} dataset to evaluate the RAG framework's ability in reading comprehension tasks. Furthermore, we utilize the \\textbf{StrategyQA}~\\cite{geva2021did} dataset to evaluate the commonsense reasoning capabilities of DRAGIN and other baselines. \n\n\\subsection{Settings for each Dataset}\n\\label{sec:setting}\n\n\\begin{itemize}[leftmargin=*]\n\n    \\item \\textbf{2WikiMultihopQA}~\\cite{ho2020constructing}. We follow the setting of~\\cite{wang2022self} to generate both chain-of-thought (CoT) reasoning process as well as the final answer. We follow the prompt template of ~\\cite{trivedi2022interleaving} and~\\cite{jiang2023active}. \n    For the evaluation metrics, we extract the final answer from the generated output using pattern matching techniques. The extracted answer is then compared with the reference answer, utilizing methods such as exact match at the answer level, along with token-level measurements of F1 score and precision.\n\n    \\item \\textbf{HotpotQA}~\\cite{yang2018hotpotqa}. We follow the setting and the prompt template of~\\cite{trivedi2022interleaving} to generate both chain-of-thought (CoT) reasoning process as well as the final answer. Our evaluation metric on this dataset is the same as 2WikiMultihopQA.\n\n    \\item \\textbf{StrategyQA}~\\cite{geva2021did}. We follow the setting of~\\cite{wei2022chain} to generate both the CoT reasoning process as well as the final answer. We follow the prompt template of~\\cite{wei2022chain} and~\\cite{jiang2023active}. \n    For the evaluation metrics, the obtained yes/no response is extracted and compared with the standard correct answer using an exact match approach.\n\n    \\item \\textbf{IIRC}~\\cite{ferguson2020iirc}. We follow the setting and the prompt template of~\\cite{trivedi2022interleaving} to generate the final answer. Our evaluation metric on this dataset is the same as 2WikiMultihopQA.\n\\end{itemize}\n\nBesides the settings introduced in this section, the specific prompt templates corresponding to each dataset are presented in Appendix~\\ref{appendix:prompt}.\nAppendix~\\ref{appendix:datasets} provides more detailed descriptions of each dataset's settings.\n\n    \n\n\\subsection{Baselines}\n\n\\begin{table}[t]\n\\caption{A comparative overview of our selected Retrieval-Augmented Generation baselines.}\n\\label{tab:baselines}\n\\setlength\\tabcolsep{2.5pt} % 减少列间距\n\\footnotesize\n{\n\\begin{tabular}{lcc}\n\\toprule\n              & \\textbf{Timing for Retrieval}     & \\textbf{Query Formulation}                                                                  \\\\\n              \\midrule\n\\textbf{SR-RAG}  & Before Generation             & Initial Input                                                                      \\\\\n\\midrule\n\\textbf{FL-RAG}  & Per $n$ Tokens            & Last Generated Tokens                                                                     \\\\\n\\midrule\n\\textbf{FS-RAG}  & Per Sentence            & Last Generated Sentence                                                                     \\\\\n\\midrule\n\\textbf{FLARE}  & \\scriptsize{\\begin{tabular}[c]{@{}c@{}}Any token's probability\\\\ below the threshold\\end{tabular}} & \\scriptsize{\\begin{tabular}[c]{@{}c@{}}Last generated Sentence\\\\ exclude low-probability tokens\\end{tabular}} \\\\\n\\midrule\n\\textbf{DRAGIN} & \\scriptsize{\\begin{tabular}[c]{@{}c@{}}  Generated token's  \\\\importance and uncertainty \\end{tabular}} & \\scriptsize{\\begin{tabular}[c]{@{}c@{}}LLM's attention over \\\\ the entire context\\end{tabular}}          \\\\\n\\toprule\n\\end{tabular}\n}\n\\end{table}\n\nWe choose the following Text Generation baselines for comparison. \nFollowing the setting of FLARE~\\cite{jiang2023active}, we implemented the existing multi-round RAG frameworks using the same settings, with the only variation being the timing of triggering retrieval (when to retrieve) and the query formulation method when the retrieval is triggered (what to retrieve).\n\n\\begin{itemize}[leftmargin=*]\n\n\\item  \\textbf{wo-RAG}. LLM provides direct answers to questions without RAG.\n\\item  \\textbf{SR-RAG} ({Single-round RAG}). Relevant passages are retrieved from an external corpus based on the initial question. The retrieved passages are then added into the LLM's input.\n\\item  \\textbf{FL-RAG} ({F}ix {L}ength RAG)~~\\cite{khandelwal2019generalization, borgeaud2022improving,ram2023context}. A multi-round retrieval augmentation method that triggers the retrieval module every $n$ tokens. The tokens generated in the previous token window are utilized as the query.\n\\item  \\textbf{FS-RAG} ({F}ix {S}entence RAG)~~\\cite{trivedi2022interleaving}. A multi-round retrieval augmentation method that triggers the retrieval module every sentence. The last generated sentence are utilized as the query.\n\\item \\textbf{FLARE}~\\cite{jiang2023active}. A multi-round retrieval augmentation method that triggers retrieval each time it encounters an uncertain token. When the retrieval module is triggered, the last generated sentence without the uncertain tokens are defines as the query.\n\n\\end{itemize}\n\nTo illustrate the differences between DRAGIN and other dynamic RAG baselines directly, we present a comparison of retrieval timing and query formation methods for each dynamic RAG frameworks in Table~\\ref{tab:baselines}.\n\n\\begin{table*}[t]\n\\caption{The overall experimental results of DRAGIN and other baselines on four benchmarks. The best results are in bold.}\n\\label{tab:overall}\n\\centering\n\\small\n\\begin{tabular}{ccccccccc}\n\\toprule\n                                          &                 & \\multicolumn{2}{c}{\\textbf{2WikiMultihopQA}} & \\multicolumn{2}{c}{\\textbf{HotpotQA}} & \\textbf{StrategyQA} & \\multicolumn{2}{c}{\\textbf{IIRC}} \\\\\n                                          \\midrule\n\\textbf{LLM}                              & \\textbf{RAG Method} & \\textbf{EM}          & \\textbf{F1}           & \\textbf{EM}       & \\textbf{F1}       & \\textbf{Accuracy}         & \\textbf{EM}     & \\textbf{F1}     \\\\\n\\toprule\n\\multirow{6}{*}{\\textbf{Llama2-13b-chat}} & \\textbf{wo-RAG} & 0.187                & 0.2721                & 0.223             & 0.3097            & 0.650                & 0.168& 0.2039          \\\\\n                                          & \\textbf{SR-RAG}  & 0.245                & 0.3364                & 0.263             & 0.3706            & 0.654                & \\textbf{0.196}& \\textbf{0.2303}\\\\\n                                          & \\textbf{FL-RAG} & 0.217                & 0.3054                & 0.177             & 0.2682            & 0.648               & 0.155& 0.1875          \\\\\n                                          & \\textbf{FS-RAG} & 0.270                 & 0.3610                 & 0.267             & 0.3715            & 0.655               & 0.171& 0.2061          \\\\\n                                                                                    & \\textbf{FLARE}  & 0.224                & 0.3076                & 0.180              & 0.2756            & 0.655               & 0.138& 0.1667          \\\\\n\n                                          & \\textbf{DRAGIN (Ours)} & \\textbf{0.304}& \\textbf{0.3931}& \\textbf{0.314}& \\textbf{0.4238}& \\textbf{0.689}& 0.185& 0.2221\\\\\n                                          \\toprule\n\\multirow{6}{*}{\\textbf{Llama2-7b-chat}}  & \\textbf{wo-RAG} & 0.146                & 0.2232                & 0.184             & 0.2745            & \\textbf{0.659}      & 0.139& 0.1731          \\\\\n                                          & \\textbf{SR-RAG}  & 0.169                & 0.2549                & 0.164             & 0.2499            & 0.645               & 0.187& 0.2258          \\\\\n                                          & \\textbf{FL-RAG} & 0.112                & 0.1922                & 0.146             & 0.2107            & 0.635               & 0.172& 0.2023          \\\\\n                                          & \\textbf{FS-RAG} & 0.189                & 0.2652                & 0.214             & 0.3035            & 0.629               & 0.178& 0.2157          \\\\\n                                                                                    & \\textbf{FLARE}  & 0.143                & 0.2134                & 0.149             & 0.2208            & 0.627               & 0.136& 0.1644          \\\\\n                                                                                    \n                                          & \\textbf{DRAGIN (Ours)} & \\textbf{0.220}& \\textbf{0.2926}& \\textbf{0.232}& \\textbf{0.3344}& 0.641               & \\textbf{0.192}& \\textbf{0.2336}\\\\\n                                          \\toprule\n\\multirow{6}{*}{\\textbf{Vicuna-13b-v1.5}} & \\textbf{wo-RAG} & 0.146                & 0.2232                & 0.228             & 0.3256            & 0.682               & 0.175& 0.2149          \\\\\n                                          & \\textbf{SR-RAG}  & 0.170                 & 0.2564                & 0.254             & 0.3531            & {0.686}      & 0.217           & 0.2564          \\\\\n                                          & \\textbf{FL-RAG} & 0.135                & 0.2133                & 0.187             & 0.3039            & 0.645               & 0.0985          & 0.1285          \\\\\n                                          & \\textbf{FS-RAG} & 0.188                & 0.2625                & 0.185             & 0.3216            & 0.622               & 0.1027          & 0.1344          \\\\\n                                                                                    & \\textbf{FLARE}  & 0.157                & 0.2257                & 0.092             & 0.1808            & 0.599               & 0.1174          & 0.1469          \\\\\n\n                                          & \\textbf{DRAGIN (Ours)} & \\textbf{0.252}& \\textbf{0.3516}& \\textbf{0.288}& \\textbf{0.4164}& \\textbf{0.687}               & \\textbf{0.2233}& \\textbf{0.2652}\\\\\n                                          \\toprule\n\\end{tabular}\n\\end{table*}\n\\subsection{Selected LLMs}\nTo validate the effectiveness of DRAGIN and other RAG baselines, we conducted experiments with the following LLMs:\n\n\\begin{itemize}[leftmargin=*]\n\\item \\textbf{LLaMA-2-Chat}~\\cite{touvron2023llama2}. LLaMA2 is a collection of pre-trained and fine-tuned LLMs. This series includes fine-tuned LLMs, known as Llama2-Chat, specifically designed for optimal performance in dialogue-based applications. We choose \\textbf{LLaMA-2-Chat-7B} and \\textbf{LLaMA-2-Chat-13B}.\n\n\\item \\textbf{Vicuna-13B-v1.5}~\\cite{chiang2023vicuna} is a collection of open-source chatbots fine-tuned from LLaMA using user-shared conversations gathered from ShareGPT. We have selected the latest versions of Vicuna, namely \\textbf{Vicuna-13B-v1.5}.\n\n\\end{itemize}\n\n\\subsection{Implementation Details}\\label{sec:implement}\n\n\\noindent \\textbf{Hyperparameter:} The hyperparameters are all presented in Appendix~\\ref{appendix:hyper}.\n\n\\noindent \\textbf{Retriever:} We adopt BM25 as our retrieval model based on findings from~\\cite{ram2023context}, which demonstrated its superior performance in Retrieval-Augmented Generation, even outperforming some dense retrieval models. We also explored the impact of replacing BM25 with a SOTA dense retrieval method SGPT~\\cite{muennighoff2022sgpt}, which is detailed in Section~\\ref{sec:SGPT}.\n\n\\noindent \\textbf{Stopwords:} \nFor the identification of stopwords within the RIND module, we utilized the en\\_core\\_web\\_sm language model from the Spacy library, a tool recognized for its effectiveness and efficiency in Natural Language Processing tasks as evidenced by previous research~\\cite{shelar2020named}.\n\n\\noindent \\textbf{External Knowledge Corpus:}\nWe adopt Wikipedia as our external knowledge corpus. Each article are segmented into 100-token passages. \n\n\\noindent \\textbf{LLM Configuration}: For the selected LLMs, we directly download model parameters from the official Hugging Face repositories for each model, and use the code provided by Hugging Face to conduct text generation. For the generation configuration, we have chosen greedy decoding as the decoding strategy for LLM inference to ensure the reproducibility of our experimental results. However, for practical applications, we recommend using the official default generation configuration provided by each model, as this will yield better performance.\n\n\\section{Experimental Results}\n\n\\subsection{Overall Results of DRAGIN and Baselines}\nOur experiments comprehensively evaluate the performance of DRAGIN against various baselines across four benchmark datasets: 2WikiMultihopQA, HotpotQA, StrategyQA, and IIRC. The results, summarized in Table~\\ref{tab:overall}, underscore several critical insights:\n(1) The integration of single-round retrieval augmentation consistently boosts LLMs' performance across all datasets when compared to direct question answering, confirming the effectiveness of RAG.\n(2) Despite the overall positive impact of retrieval augmentation, we observe that fixed rules-based retrieval methods, e.g. FL-RAG and FS-RAG, do not always outperform single-round retrieval. This observation validates our hypothesis that retrieval augmentation, if conducted at a wrong position, may not be helpful in improving the quality of LLM's outputs.\nThis underscores the significance of timing in the activation of retrieval processes, which should be tailored to the information needs of Large Language Models (LLMs), activating retrieval only when LLMs necessitate external knowledge.\n(3) The performance of FLARE varies significantly among different datasets. Interestingly, as shown in our ablation study (\\S ~\\ref{sec:how}), the query formulation strategies are significantly better than those used by other baselines, but its overall performance is not. This indicates that the timing of retrieval augmentation in FLARE is far from perfect.\n(4) Our proposed DRAGIN method demonstrates superior performance across most LLMs and datasets. This indicates the robustness and effectiveness of DRAGIN in enhancing LLMs' capabilities.\n(5) DRAGIN demonstrates more substantial performance improvements in MultihopQA tasks, such as 2WikiMultihopQA and HotpotQA, than in tasks requiring common sense reasoning, like those in the StrategyQA dataset. This difference highlights DRAGIN's specialized capability in managing complex, multi-step reasoning tasks.\n\n\\begin{table}[t]\n\\caption{Comparison of the frequency of retrieval module activation in dynamic RAG frameworks across all datasets. 2WMQA, HQA, SQA indicates 2WikiMultihopQA, HotpotQA, StrategyQA respectively.}\n\\centering\n\\label{tab:efficiency}\n\\setlength\\tabcolsep{4pt} % 减少列间距\n\\footnotesize\n\\begin{tabular}{lccccc}\n\\toprule\n                                                            && \\textbf{2WMQA} & \\textbf{HQA} & \\textbf{SQA} & \\textbf{IIRC}  \\\\\n                                          \\midrule\n  &\\textbf{} & \\textbf{\\#Num}           & \\textbf{\\#Num}    & \\textbf{\\#Num}      & \\textbf{\\#Num} \\\\\n \\toprule\n                                            \\multirow{4}{*}{\\textbf{L13B}}&\\textbf{FL-RAG} & 3.770                     & 3.194             & 3.626               & 3.426\\\\\n                                            &\\textbf{FS-RAG} & 3.131                    & 4.583             & 4.885               & 4.305\\\\\n                                                                                        &\\textbf{FLARE}  & 1.592                    & 3.378             & 0.625               & 5.521\\\\\n\n                                            &\\textbf{DRAGIN} & 2.631                    & 3.505             & 4.786               & 2.829\\\\\n                                           \\midrule\n                                            \\multirow{4}{*}{\\textbf{L7B}}&\\textbf{FL-RAG} & 3.342                    & 3.809             & 3.757               & 2.839\\\\\n                                            &\\textbf{FS-RAG} & 3.833                    & 4.152             & 4.546               & 4.210\\\\\n                                                                                        &\\textbf{FLARE}  & 0.941                    & 1.064             & 1.271               & 1.095\\\\\n\n                                            &\\textbf{DRAGIN} & 2.836                    & 3.013            & 4.629               & 2.927\\\\\n                                           \\midrule\n                                            \\multirow{4}{*}{\\textbf{V13B}}&\\textbf{FL-RAG} & 4.199                    & 3.564             & 3.591               & 3.189\\\\\n\n                                            &\\textbf{FS-RAG} & 3.720                     & 5.701             & 6.820& 6.032\\\\\n                                            &\\textbf{FLARE}  & 1.093                    & 1.078             & 1.118               & 0.335\\\\\n                                            \n                                            &\\textbf{DRAGIN} & 2.542                    & 3.184             & 3.744               & 3.120\\\\\n                                          \\toprule\n\\end{tabular}\n\\end{table}\n\n\\subsection{Efficiency}\n\\label{sec:efficiency}\n\nIn this section, we investigate the efficiency of various dynamic RAG frameworks across multiple datasets. We measure efficiency based on the number of retrieval calls made, as outlined in Table~\\ref{tab:efficiency}. Due to the special design of FS-RAG, the \\#NUM for FS-RAG also indicates the average number of sentences produced by the LLM in response to queries on this dataset. \nAmong the evaluated frameworks, FLARE stood out for its efficiency, requiring the fewest retrieval calls across all datasets. DRAGIN followed closely, with fewer retrieval calls than FS-RAG and FL-RAG. \n\n\\subsection{Timing of Retrieval}\n\\label{sec:when}\n\nIn this subsection, we investigate the impact of the timing of retrieval on the performance of dynamic RAG frameworks. Specifically, we fixed the method of query formulation to use the last complete sentence generated by the LLM as the query, and varied the timing of retrieval as the only variable. We examined DRAGIN alongside three existing frameworks: FLARE, FL-RAG, and FS-RAG on the IIRC dataset.\nAs shown in Table~\\ref{tab:when}, DRAGIN consistently outperforms all other dynamic RAG methods. This highlights the effectiveness of our novel approach to determining the optimal moment for retrieval. DRAGIN's superior performance suggests that its method for detecting the real-time information needs of LLMs and triggering retrieval accordingly is particularly adept at enhancing the quality of the generated text.\n\n\\begin{table}[]\n\\caption{The influence of the ‘When to Retrieve’ decision on various dynamic RAG frameworks, with the IIRC dataset as the evaluation benchmark. The best results are in bold. L13B indicates LLaMA2-13B-Chat, V13B indicates Vicuna-13b-v1.5. We fix the query formulation method, the last complete sentence generated by the LLM is selected as the query for all the baselines.}\n\\label{tab:when}\n\\centering\n\\small\n\\begin{tabular}{ccccc}\n\\toprule\n                               & \\textbf{}       & \\textbf{EM}    & \\textbf{F1}     & \\textbf{Prec.}  \\\\\n                               \\midrule\n\\multirow{4}{*}{\\textbf{L13B}} & \\textbf{FLARE}  & 0.128          & 0.1599          & 0.1677          \\\\\n                               & \\textbf{FL-RAG} & 0.155          & 0.1875          & 0.1986          \\\\\n                               & \\textbf{FS-RAG} & 0.171          & 0.2061          & 0.2185          \\\\\n                               & \\textbf{DRAGIN} & \\textbf{0.187} & \\textbf{0.2242} & \\textbf{0.2319} \\\\\n                               \\midrule\n\\multirow{4}{*}{\\textbf{V13B}} & \\textbf{FLARE}  & 0.097          & 0.1277          & 0.1324          \\\\\n                               & \\textbf{FL-RAG} & 0.099          & 0.1285          & 0.1324          \\\\\n                               & \\textbf{FS-RAG} & 0.103          & 0.1344          & 0.1358          \\\\\n                               & \\textbf{DRAGIN} & \\textbf{0.196} & \\textbf{0.2367} & \\textbf{0.2476} \\\\\n                               \\toprule\n\\end{tabular}\n\\end{table}\n\nWe also evaluate the impact of varying threshold values within the RIND module on the performance of the DRAGIN framework.\nWe present the experimental results on the HotpotQA dataset in Table~\\ref{tab:threshold}. \nOur experimental results show that DRAGIN's performance remains stable across threshold settings, indicating a low sensitivity to changes in this hyperparameter.\nThe threshold value is pivotal in determining the retrieval module's activation frequency. \nAs the threshold increases, the frequency of the retrieval module's activation decreases, suggesting that adjusting the threshold can strike a balance between the system's efficiency and the accuracy of its outputs in practical applications.\n\n\\begin{table}[t]\n\\caption{Comparasion between different threshold of RIND for LLaMA-13B-Chat model on the HotpotQA dataset. The best results are in bold.}\n\\label{tab:threshold}\n\\centering\n\\small\n\\setlength\\tabcolsep{5pt} % 减少列间距\n\\begin{tabular}{cccc}\n\\toprule\n\\textbf{threshold} & \\textbf{EM}    & \\textbf{F1}     & \\textbf{Prec.}   \\\\\n\\midrule\n\\textbf{0.3}       & 0.295          & 0.3856          & 0.3873          \\\\\n\\textbf{0.4}       & 0.297          & 0.387           & 0.389           \\\\\n\\textbf{0.5}       & 0.299          & 0.3897          & 0.3915          \\\\\n\\textbf{0.6}       & \\textbf{0.304} & 0.3931          & \\textbf{0.3946} \\\\\n\\textbf{0.7}       & \\textbf{0.304} & 0.3927          & 0.3937          \\\\\n\\textbf{0.8}       & 0.301          & 0.392           & 0.3927          \\\\\n\\textbf{0.9}       & 0.301          & \\textbf{0.3944} & 0.3947          \\\\\n\\textbf{1}         & 0.293          & 0.3869          & 0.3875         \\\\\n\\toprule\n\\end{tabular}\n\\end{table}\n\n\\subsection{Query Formulation}\n\\label{sec:how}\n\n\\begin{table}[t]\n\\caption{The influence of the query formulation methods on various dynamic RAG frameworks, with the HotpotQA dataset as the evaluation benchmark. The best results are in bold. L13B indicates LLaMA2-13B-Chat, V13B indicates Vicuna-13b-v1.5.}\n\\label{tab:what}\n\\centering\n\\small\n\\setlength\\tabcolsep{5pt} % 减少列间距\n\\begin{tabular}{ccccc}\n\\toprule\n                               &                       & \\textbf{EM} & \\textbf{F1} & \\textbf{Prec.} \\\\\n                               \\toprule\n\\multirow{5}{*}{\\textbf{L13B}} & \\textbf{FLARE}        & 0.262       & 0.3674      & 0.3792         \\\\\n                               & \\textbf{Full Context} & 0.252       & 0.3584      & 0.3711         \\\\\n                               & \\textbf{FS-RAG}       & 0.255       & 0.3574      & 0.3685         \\\\\n                               & \\textbf{FL-RAG}       & 0.241       & 0.3394      & 0.3495         \\\\\n                               & \\textbf{DRAGIN}       & \\textbf{0.314}       & \\textbf{0.4238}      & \\textbf{0.4401}         \\\\\n                               \\midrule\n\\multirow{5}{*}{\\textbf{V13B}} & \\textbf{FLARE}        & 0.225       & 0.3366      & 0.3420          \\\\\n                               & \\textbf{Full Context} & 0.221       & 0.3402      & 0.3457         \\\\\n                               & \\textbf{FS-RAG}       & 0.216       & 0.3432      & 0.3507         \\\\\n                               & \\textbf{FL-RAG}       & 0.214       & 0.3268      & 0.3264         \\\\\n                               & \\textbf{DRAGIN}       & \\textbf{0.288}       & \\textbf{0.4164}     &\\textbf{0.4226}        \\\\\n                               \\toprule\n\\end{tabular}\n\\end{table}\n\nThis subsection delves into the impact of query formulation techniques on the performance of dynamic RAG frameworks.\nWe standardize the timing of trigger retrieval to RIND, which is proven to be the most effective timing based on the experimental results detailed in section ~\\ref{sec:when}.\nWe focus on the comparison between DRAGIN and three existing frameworks: FLARE, FL-RAG, and FS-RAG.\nThe query formulation method of FLARE is the last generated sentence excludes low-probability tokens.\nFL-RAG selects the closest 25 tokens to this position as the query.\nFS-RAG selects the sentence before this position as the query.\nWe also evaluate the effectiveness of using the full context as the query.\nAs shown in Table~\\ref{tab:what}, DRAGIN's query formulation method performs best among all the dynamic RAG frameworks.\nFLARE emerged as the second most effective query formulation method, outperforming the FS-RAG and FL-RAG methods.\nMoreover, leveraging the entire context as a query did not yield optimal results, indicating potential redundancy within the full context. This finding validates the effectiveness of our proposed QFS method, which aims to select tokens from the context that can represent the real-time information needs of the LLM as the query.\n\n\\subsection{Impact of Retriever}\n\\label{sec:SGPT}\nIn the dynamic RAG paradigm, the choice of the retriever plays an important role in retrieving relevant passages from a corpus based on a given query. \nIn the field of information retrieval, the two popular types of retrieval methods are lexical matching~\\cite{zhai2008statistical,robertson2009probabilistic} and dense retrieval~\\cite{su2023caseformer,gao2021condenser,su2023wikiformer,muennighoff2022sgpt,li2023thuir,ma2023caseencoder,ye2024relevance,su2023thuir2,li2023towards,chen2023thuir,chen2022web,li2023thuir3,fang2024scaling}.\nAmong lexical matching techniques, BM25 stands out for its widespread adoption and effectiveness~\\cite{robertson2009probabilistic}. Conversely, among existing dense retrieval methods, none has achieved the widespread popularity of BM25. We have opted for SGPT, which has recently attained state-of-the-art performance across a variety of datasets.~\\cite{muennighoff2022sgpt}.\n\nIn our experimental analysis presented in Table \\ref{tab:retriever}, we found that BM25 consistently surpasses SGPT in performance across various datasets within the dynamic RAG framework, despite SGPT's generally better performance in numerous information retrieval tasks. This outcome aligns with the findings of prior research, such as the study by~\\cite{ram2023context}, which underscored BM25's effectiveness in RAG tasks. These results indicate that despite progress in dense retrieval technologies like SGPT, the simpler, lexicon-based BM25 algorithm is still a strong baseline for enhancing the performance of LLM in RAG tasks.\n\n\\begin{table}[t]\n\\caption{Comparison of performance between BM25 and SGPT using the LLaMA2-13B-Chat model. The method with better performance is highlighted.}\n\\label{tab:retriever}\n\\centering\n\\small\n\\setlength\\tabcolsep{5pt} % 减少列间距\n\\begin{tabular}{ccccc}\n\\toprule\n                                          & \\textbf{retriever} & \\textbf{EM}     & \\textbf{F1}     & \\textbf{Prec.}  \\\\\n                                          \\midrule\n\\multirow{2}{*}{\\textbf{2WMQA}} & \\textbf{BM25}      & \\textbf{0.304}  & \\textbf{0.393} & \\textbf{0.395} \\\\\n                                          & \\textbf{SGPT}      & 0.273           & 0.356          & 0.357          \\\\\n                                          \\midrule\n\\multirow{2}{*}{\\textbf{HQA}}        & \\textbf{BM25}      & \\textbf{0.314}& \\textbf{0.424}& \\textbf{0.437} \\\\\n                                          & \\textbf{SGPT}      & 0.264           & 0.371          & 0.388          \\\\\n                                          \\midrule\n\\multirow{2}{*}{\\textbf{IIRC}}            & \\textbf{BM25}      & \\textbf{0.185} & \\textbf{0.222} & \\textbf{0.235} \\\\\n                                          & \\textbf{SGPT}      & 0.169          & 0.201          & 0.207          \\\\\n                                          \\toprule\n\\end{tabular}\n\\end{table}\n\n\\section{Conclusions and Future Works}\n\nIn this work, we propose DRAGIN, a dynamic RAG framework tailored to address the real-time information needs of LLMs during text generation. By integrating RIND for timely retrieval activation and QFS for precise query formulation, DRAGIN significantly outperforms existing dynamic RAG methods across various knowledge-intensive benchmarks. \n\n\\section{Limitations}\nWe acknowledge the limitations of this paper. One of the primary limitations is the reliance on the self-attention mechanism of Transformer-based LLMs for both Real-time Information Needs Detection (RIND) and Query Formulation based on Self-attention (QFS). While self-attention scores are accessible for all open-source LLMs, it's important to note that our method is not applicable to certain APIs that do not provide access to the self-attention scores. Thus, our future work aims to develop more methods to overcome this constraint.\n\n\\section{Ethics Statement}\nIn conducting this research, we have prioritized ethical considerations at every stage to ensure the responsible development and application of AI technologies. \nOur research does not rely on personally identifiable information or require manually annotated datasets. \nWe firmly believe in the principles of open research and the scientific value of reproducibility. To this end, we have made all models, data, and code associated with our paper publicly available on GitHub. \nThis transparency not only facilitates the verification of our findings by the community but also encourages the application of our methods in other contexts. \n\n\\section*{Acknowledgments}\nThis work is supported by Quan Cheng Laboratory (Grant No. QCLZD202301).\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2212.10509v2.tex",
        "arXiv-2305.06983v2.tex",
        "arXiv-2403.10081v3.tex"
    ],
    "group_id": "group_96",
    "response": "### Title: Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions: A Comparative Analysis\n\n### Introduction\n\nThe field of large language model (LLM) research has seen significant advancements in recent years, with models demonstrating remarkable capabilities in generating coherent and contextually relevant text. However, these models often struggle with knowledge-intensive tasks that require multi-step reasoning, especially when the necessary information is not directly available within their parameters or is outdated. Traditional approaches to addressing this issue involve a one-step retrieve-and-read strategy, where the model retrieves relevant information based solely on the initial question. This method, while effective for simpler tasks, is insufficient for more complex, multi-step reasoning tasks, where the retrieval of information is contingent upon the model's previous reasoning steps. To tackle this challenge, researchers have developed novel frameworks that integrate retrieval and reasoning in a more dynamic and iterative manner. This summary will explore three recent papers that propose innovative methods to enhance the performance of LLMs in knowledge-intensive, multi-step question answering (QA) tasks. These methods include IRCoT, FLARE, and DRAGIN, each offering unique solutions to improve the accuracy and efficiency of LLMs in generating factually correct answers.\n\n#### Historical Context and Current Challenges\n\nLLMs have shown surprising proficiency in generating step-by-step reasoning, often referred to as Chains-of-Thought (CoT), when prompted appropriately. This capability has been particularly useful in tasks requiring multi-step reasoning, such as multi-hop QA, where the model must infer intermediate facts before reaching the final answer. However, these models frequently hallucinate facts due to the lack of comprehensive or up-to-date knowledge, leading to incorrect reasoning steps and, consequently, inaccurate answers. The challenge lies in dynamically retrieving relevant information throughout the reasoning process to ensure that the model's CoT remains grounded in factual evidence. Traditional one-step retrieval methods are inadequate for such tasks, as they fail to account for the evolving nature of the model's information needs.\n\n#### Current Progress and Challenges\n\nRecent research has focused on developing frameworks that can iteratively retrieve information and guide the model's reasoning process. These frameworks aim to enhance the model's ability to generate accurate CoTs by continuously updating its knowledge base. The key challenge is to determine the optimal timing for retrieval and to formulate effective queries that reflect the model's current information needs. This summary will delve into three such frameworks: IRCoT, FLARE, and DRAGIN, each addressing the aforementioned challenges in distinct ways.\n\n### Main Content of Each Paper\n\n#### IRCoT: Interleaving Retrieval with Chain-of-Thought Reasoning\n\nIRCoT, introduced by Trivedi et al., is a framework designed to improve retrieval and QA performance for knowledge-intensive multi-step questions. The core idea behind IRCoT is to interleave retrieval with CoT reasoning steps, allowing the retrieval process to be guided by the CoT and vice versa. This approach is particularly useful in scenarios where the required knowledge is not directly available or up-to-date within the model's parameters. IRCoT begins by retrieving a base set of paragraphs using the question as a query. Subsequently, it alternates between two steps: (i) extending the CoT by generating the next reasoning step based on the question, previously retrieved paragraphs, and the CoT sentences generated thus far; and (ii) expanding the retrieved information by using the last CoT sentence as a query to retrieve additional paragraphs. This process continues until the CoT reports an answer or the maximum number of reasoning steps is reached. The authors evaluated IRCoT on four multi-step QA datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC, demonstrating substantial improvements in retrieval and QA performance.\n\n#### FLARE: Forward-Looking Active Retrieval Augmented Generation\n\nFLARE, proposed by Jiang et al., is a forward-looking active retrieval augmented generation framework that dynamically decides when and what to retrieve during the text generation process. Unlike static retrieval methods, FLARE triggers retrieval only when the model generates low-probability tokens, indicating a lack of confidence in the generated content. The framework uses the next sentence generated by the model as a query to retrieve relevant documents, which are then used to regenerate the sentence. This ensures that the retrieval is only conducted when necessary, preventing unnecessary or inappropriate retrieval. FLARE also employs a query formulation method that excludes low-probability tokens from the query to improve retrieval accuracy. The authors evaluated FLARE on four diverse tasks/datasets, including multihop QA, commonsense reasoning, long-form QA, and open-domain summarization, demonstrating superior or competitive performance compared to existing methods.\n\n#### DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of LLMs\n\nDRAGIN, introduced by Chiang et al., is a dynamic RAG framework that optimizes the timing and formulation of retrieval queries based on the real-time information needs of LLMs. DRAGIN's RIND module evaluates the uncertainty, significance, and semantic contribution of each token generated by the model to determine the optimal moment for retrieval. The QFS module then formulates queries based on the LLM's self-attention over the entire context, ensuring that the queries reflect the model's current information needs. DRAGIN integrates retrieved information into the LLM's generation process by truncating the output at the identified position and continuing generation based on the retrieved documents. The authors evaluated DRAGIN on four knowledge-intensive generation benchmarks, demonstrating superior performance across most LLMs and datasets.\n\n### Commonalities and Innovations\n\nAll three papers address the challenge of improving LLMs' performance in knowledge-intensive, multi-step reasoning tasks by integrating retrieval and generation processes. However, they differ in their approaches to determining when and what to retrieve. IRCoT uses a fixed number of paragraphs retrieved at each step and relies on the CoT sentences to guide further retrieval. FLARE triggers retrieval based on the generation of low-probability tokens and uses the next sentence as a query. DRAGIN, on the other hand, evaluates the real-time information needs of the model using a combination of uncertainty, significance, and semantic contribution scores, and formulates queries based on the model's self-attention over the entire context.\n\n### Comparative Analysis of Results\n\nThe authors of each paper evaluated their methods on different datasets and tasks, providing a comprehensive comparison with existing approaches. IRCoT demonstrated significant improvements in retrieval and QA performance on HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC, outperforming one-step retrieval methods by up to 21 recall points and up to 15 F1 points in QA performance. FLARE achieved superior or competitive performance on multihop QA, commonsense reasoning, long-form QA, and open-domain summarization, with notable improvements in multihop QA. DRAGIN outperformed existing dynamic RAG methods on all datasets, with the most substantial improvements observed in multihop QA tasks.\n\n### Conclusion\n\nThe three frameworks, IRCoT, FLARE, and DRAGIN, represent significant advancements in the field of retrieval-augmented generation, particularly for knowledge-intensive, multi-step reasoning tasks. Each framework addresses the limitations of traditional one-step retrieval methods by dynamically deciding when and what to retrieve based on the model's current information needs. IRCoT leverages CoT sentences to guide retrieval, FLARE triggers retrieval based on low-probability tokens, and DRAGIN evaluates the real-time information needs of the model using a combination of uncertainty, significance, and semantic contribution scores. These frameworks not only improve the accuracy of the model's CoT but also reduce hallucination, leading to more factually correct answers. Future research could focus on refining these frameworks to further enhance their efficiency and applicability to a wider range of tasks and models. Additionally, developing methods that do not rely on self-attention scores could broaden the scope of these frameworks, making them compatible with a wider range of LLMs and APIs."
}