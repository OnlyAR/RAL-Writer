{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Large Language Models are Human-Level Prompt Engineers}\n\n\\begin{document}\n\n\\maketitle\n\\renewcommand*{\\thefootnote}{\\arabic{footnote}}\n\\setcounter{footnote}{0}\n\n\\begin{abstract}\nBy conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose \\textit{Automatic Prompt Engineer}\\footnote{We define ``prompt engineering'' as optimizing the language in a prompt in order to elicit the best possible performance. Notably, this does not include prompts that chain multiple LLM queries together or give the LLM access to external tools.} (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the ``program,'' optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Extensive experiments show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 24/24 Instruction Induction tasks and 17/21 curated BIG-Bench tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts are able to improve few-shot learning performance (by simply prepending them to standard in-context learning prompts), find better zero-shot chain-of-thought prompts, as well as steer models toward truthfulness and/or informativeness. \n\\footnote{\\ Our code is available at \\url{https://github.com/keirp/automatic_prompt_engineer}.}\n\\end{abstract}\n\n\\section{Introduction}\\label{sec:intro}\nThe combination of scale and attention-based architectures has resulted in language models possessing an unprecedented level of generality \\citep{kaplan2020scaling,vaswani2017attention}. These so-called ``large language models'' (LLMs) have shown remarkable, often superhuman, capabilities across a diverse range of tasks, including both zero-shot and few-shot setups \\citep{brown2020language,srivastava2022beyond}. With generality, however, there comes a question of control: how can we make LLMs do what we want them to do? \n\nTo answer this question and steer LLMs toward desired behaviors, recent work has considered fine-tuning \\citep{ouyang2022training,ziegler2019fine}, in-context learning \\citep{brown2020language}, and several forms of prompt generation \\citep{gao2021prompting}, including both differentiable tuning of soft prompts \\citep{qin2021learning,lester2021power} and natural language prompt engineering \\citep{reynolds2021prompt}. The latter is of particular interest, as it provides a natural interface for humans to communicate with machines and may be of great relevance not only to LLMs but to other generalist models such as prompted image synthesizers \\citep{rombach2022high,ramesh2022hierarchical}, for which public interest in prompt design and generation has also emerged (see Appendix \\ref{appdx_wild_prompt_engineering} for examples).\n\nBehind this interest is the fact that plain language prompts do not always produce the desired results, even when those results are possible to produce with alternative instructions. Thus, human users must experiment with a wide range of prompts to elicit desired behaviors, as they have little knowledge of how compatible instructions are with a particular model.\nWe can understand this by viewing LLMs as black-box computers that execute programs specified by natural language instructions: while they can execute a broad range of natural language programs, the way these programs are processed may not be intuitive for humans, and the quality of instruction can only be measured when executing these instructions on a downstream task \\citep{sanh2022multitask, wei2021finetuned}. \n\n\\workshopexclude{\nTo reduce the human effort involved in creating and validating effective instructions, we propose a novel algorithm using LLMs to generate and select instructions automatically. We call this problem \\textit{natural language program synthesis} and propose to address it as a black-box optimization problem using LLMs to generate and search over heuristically viable candidate solutions. \nIn doing so, we leverage the generalist capabilities of LLMs in three ways. First, we use an LLM as an inference model \\citep{ellis2021dreamcoder, honovich2022instruction} to generate instruction candidates based on a small set of demonstrations in the form of input-output pairs. Next, we guide the search process by computing a score for each instruction under the LLM we seek to control. Finally, we propose an iterative Monte Carlo search method where LLMs improve the best candidates by proposing semantically similar instruction variants. Intuitively, our algorithm asks LLMs to generate a set of instruction candidates based on demonstrations and then asks them to assess which instructions are more promising. We call our algorithm Automatic Prompt Engineer (\\algname). \\textbf{Our main contributions are:}\n\\begin{itemize}\n    \\item We frame instruction generation as natural language program synthesis, formulate it as a \n    black-box optimization problem guided by LLMs, and propose both a naive and an iterative Monte Carlo search methods to approximate the solution.\n    \\item Our proposed method, APE, achieves human-level performance on zero-shot learning with model-generated instructions on 24/24 Instruction Induction and 17/21 Big-Bench tasks.\n    \\item We provide extensive qualitative and quantitative analyses exploring various facets of APE, and demonstrate applications of APE for improving few-shot learning, finding better zero-shot chain of thought prompts, and steering LLMs toward desired behaviors such as truthfulness and/or informativeness.\n\\end{itemize}\n}\n\n\\workshoponly{\nTo reduce the human effort involved in creating and validating effective instructions, we propose a novel algorithm using LLMs to generate and select instructions automatically. We call this problem \\textit{natural language program synthesis} and propose to address it as a black-box optimization problem using LLMs to generate and search over heuristically viable candidate solutions. In doing so, we leverage the generalist capabilities of LLMs in two ways. First, we use an LLM as an inference model \\citep{ellis2021dreamcoder, honovich2022instruction} to generate instruction candidates based on a small set of demonstrations in the form of input-output pairs. Second, we guide the search process by computing a score for each instruction under the LLM we seek to control. Intuitively, our algorithm asks LLMs to generate a set of instruction candidates based on demonstrations and then asks them to assess which instructions are more promising. We call our algorithm Automatic Prompt Engineer (\\algname). \\textbf{Our main contributions are:}\n\\begin{itemize}\n    \\item We frame instruction generation as natural language program synthesis, formulate it as a \n    black-box optimization problem guided by LLMs, and propose a Monte Carlo search methods to approximate the solution.\n    \\item Our proposed method, APE, achieves human-level performance on zero-shot learning with model-generated instructions on 19/24 NLP tasks and demonstrate applications of APE for steering LLMs toward desired behaviors such as truthfulness and/or informativeness.\n\\end{itemize}\n}\n\n\\begin{figure}\n  \\centering\n  \\vspace{-0.05in}\n\\begin{subfigure}[b]{0.48\\textwidth}\n   \\hfill\\includegraphics[width=1.0\\linewidth]{figures/illustration/APE_pipe.pdf}\\vspace{0.75em}\n  \\caption{Automatic Prompt Engineer (APE) workflow}\n\\end{subfigure}\n \\begin{subfigure}[b]{0.49\\textwidth}\n  \\includegraphics[width=1.0\\linewidth]{figures/main/model_size.pdf}\n  \\caption{Interquartile mean across 24 tasks}\n  \\end{subfigure}\n  \\caption{(a) Our method, \\textbf{Automatic Prompt Engineer (APE)}, automatically generates instructions for a task that is specified via output demonstrations: it generates several instruction candidates, either via direct inference or a recursive process based on semantic similarity, executes them using the target model, and selects the most appropriate instruction based on computed evaluation scores. (b) As measured by the interquartile mean across the 24 NLP tasks introduced by \\citet{honovich2022instruction}, APE is able to surpass human performance when using the InstructGPT model \\citep{ouyang2022training}.}\\label{fig:highlight}\n\\end{figure}\\section{Related Work}\n\n\\paragraph{Large Language Models}\nScaling up transformer-based language models in terms of model size, training data, and training compute has been shown to predictably improve performance on a wide range of downstream NLP tasks \\citep{vaswani2017attention, devlin2018bert, brown2020language}. Many emergent abilities \\citep{wei2022emergent} of LLMs have been discovered as a result of this scaling, including few-shot in-context learning, zero-shot problem solving, chain of thought reasoning, instruction following, and instruction induction \\citep{cobbe2021training, wei2022chain, kojima2022large, sanh2022multitask, wei2021finetuned, ouyang2022training, honovich2022instruction}. In this paper, we view LLMs as black-box computers that execute programs specified by natural language instructions and investigate how to control an LLM's behavior using model-generated instructions. \n\n\\paragraph{Prompt Engineering}\nPrompting offers a natural and intuitive interface for humans to interact with and use generalist models such as LLMs. Due to its flexibility, prompting has been widely used as a generic method for NLP tasks \\citep{schick2021exploiting, brown2020language, sanh2022multitask}. However, LLMs require careful prompt engineering, either manually \\citep{reynolds2021prompt} or automatically \\citep{gao2021making, shin2020autoprompt}, as models do not seem to understand the prompts in the same way a human would \\citep{webson2021prompt, lu2021fantastically}. Though many successful prompt tuning methods perform optimization over a continuous space using gradient-based methods \\citep{liu2021gpt, qin2021learning,lester2021power}, this becomes less practical with scale, as computing gradients becomes increasingly expensive and access to models shifts to APIs that may not provide gradient access. \nIn our paper, we borrow components from discrete prompt search methods, such as prompt generation \\citep{gao2021making, ben2021pada}, prompt scoring \\citep{davison2019commonsense} and prompt paraphrasing \\citep{jiang2020can, yuan2021bartscore} to optimize instructions by searching directly in the natural language hypothesis space. \nAs compared to this past work, which uses specialized models for each component and leans heavily on human templates, we show that the entire search can be conducted by a single LLM.\n\n\\paragraph{Program Synthesis} Program synthesis involves the automatic search over a ``program space'' to find a program satisfying a particular specification \\citep{gulwani2017program}. \nModern program synthesis admits a wide variety of specifications, including input-output examples \\citep{ellis2021dreamcoder,wong2021leveraging} and natural language \\citep{jain2022jigsaw}. The range of feasible program spaces to search over has also grown, from historically restrictive domain-specific languages to general-purpose programming languages \\citep{austin2021program}. In contrast to prior approaches that require a suitable structured hypothesis space and library of components \\citep{liang2010learning, ellis2018learning}, we leverage the structure provided by LLMs to search over the space of natural language programs. \nUsing inference models is a standard practice to speed up the search by restricting the search space to a limited space of possible expressions  \\citep{menon2013machine, lee2018accelerating, devlin2017neural, ellis2021dreamcoder}. \nInspired by this, we use LLMs as approximate inference models to generate program candidates based on a small set of demonstrations. Unlike classical program synthesis, our inference models do not require any training and generalize well to various tasks.\n\n\\workshoponly{\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=0.8\\linewidth]{figures/illustration/APE_pipeline.pdf}\n  \\caption{Our method, \\textbf{Automatic Prompt Engineer (APE)}, automatically generates instructions for a task that is specified via output demonstrations: it generates several instruction candidates, either via direct inference or a recursive process based on semantic similarity, executes them using the target model, and selects the most appropriate instruction based on computed evaluation scores.}\\label{fig:pipline}\n\\end{figure}\n}\\workshopexclude{\\section{Natural Language Program Synthesis using LLMs}}\n\\workshoponly{\\section{Method in Detail}}\n\nWe consider a task specified by a dataset $\\trainingData = \\{(\\demoQ, \\demoA)\\}$ of input/output demonstrations sampled from population $\\mathcal{X}$, and a prompted model $\\M$. \nThe goal of natural language program synthesis is to find a single instruction $\\instruction$ such that, when $\\M$ is prompted with the concatenation $[\\instruction ; \\demoQ]$ of instruction and a given input, $\\M$ produces the corresponding output $\\demoA$. More formally, we frame this as an optimization problem, where we seek instruction $\\instruction$ that maximizes the expectation of some per-sample score $f(\\instruction, \\demoQ, \\demoA)$ over possible $(\\demoQ, \\demoA)$:\n\\begin{equation}\\label{eq:score}\n\\instruction^{\\star} = \\argmax_\\instruction f(\\instruction) =  \\argmax_\\instruction \\expectation{(\\demoQ, \\demoA)}{f(\\instruction,\\demoQ, \\demoA)}\n\\end{equation}\nNote that in general, $\\demoQ$ may be the empty string, such that we are optimizing $\\instruction$ as a prompt that directly produces outputs $\\{A\\}$.\nWhile this task has been widely attempted by humans, we have little knowledge of how compatible any particular instruction is with model $\\M$. Thus, we propose to treat this human-intractable question as a black-box optimization process guided by LLMs. \nOur algorithm, APE, uses LLMs in each of two key components, proposal and scoring. As shown in Figure \\ref{fig:highlight} and summarized in Algorithm \\ref{alg:ape}, APE first proposes a few candidate prompts, and then filters/refines the candidate set according to a chosen score function, ultimately choosing the instruction with the highest score. We discuss options for proposal and scoring next. \n\n\\subsection{Initial Proposal Distributions}\\label{subsec:initialU}\nDue to the infinitely large search space, finding the right instruction can be extremely difficult, which has rendered natural language program synthesis historically intractable. Recent progress in NLP has shown language models are very good at generating diverse natural language text. Therefore, we consider leveraging a pretrained LLM to propose a good set $\\proposal$ of candidate solutions that will guide our search procedure. While random samples from LLMs are unlikely to produce the desired ($\\demoQ, \\demoA$) pairs, we can instead ask the LLM to approximately infer the most likely instructions with a high score, given the input/output demonstrations; i.e., to approximately sample from $P(\\instruction\\given\\trainingData,\\ f(\\instruction)\\textrm{ is high})$.\n\n\\begin{wrapfigure}{R}{0.3\\textwidth}\n\\centering\n\\vspace{-0.1in}\n\\includegraphics[width=0.275\\textwidth]{figures/illustration/template_combined.pdf}\n\\caption{Prompts for LLMs}\n\\vspace{-0.35in}\n\\label{fig:llm_template}\n\\end{wrapfigure}\n\n\\newcommand{\\algspacer}{\\hspace{1em}}\n\\begin{algorithm}[tb]\\small\n    \\caption{Automatic Prompt Engineer (APE)}\\label{alg:ape}\n    \\begin{algorithmic}\n        \\STATE {\\bfseries Require:} $\\trainingData \\gets \\{(\\demoQ, \\demoA)\\}_n$: training examples, $f:\\instruction\\times\\mathcal{D} \\mapsto \\mathbb{R}$: score function\n    \\end{algorithmic}\n    \\begin{algorithmic}[1]\n        \\STATE Use LLM to sample instruction proposals $\\ \\proposal \\gets \\{\\instruction_1, ..., \\instruction_m\\}$. (See Section~\\ref{subsec:initialU})\n        \\WHILE{not converged}\n        \\STATE Choose a random training subset $\\widetilde{\\mathcal{D}}_\\textrm{train} \\subset \\trainingData$. \n        \\FORALL{$\\instruction$ in $\\proposal$}\n            \\STATE Evaluate score on the subset $\\widetilde{s} \\gets f(\\instruction, \\widetilde{\\mathcal{D}}_\\textrm{train})$ (See Section~\\ref{sec:score_function} )\n        \\ENDFOR\n        \\STATE Filter the top k\\% of instructions with high scores $\\proposal_k \\subset \\proposal$ using $\\{\\widetilde{s}_1, ..., \\widetilde{s}_m\\}$\n        \\STATE Update instructions $\\proposal \\gets  \\proposal_k$ or use LLM to resample $\\proposal \\gets \\text{resample} ( \\proposal_k )$ (See Section~\\ref{sec:iterative}) \n        \\ENDWHILE\n    \\end{algorithmic}\n    \\begin{algorithmic}\n        \\STATE {\\bfseries Return} instruction with the highest score $\\instruction^{\\star} \\gets \\arg\\max_{\\instruction \\in \\proposal_k} f(\\instruction, \\trainingData)$\n    \\end{algorithmic}\n\\end{algorithm}\n\n\\paragraph{Forward Mode Generation} We consider two approaches to generate high-quality candidates from $P(\\instruction\\given\\trainingData,\\ f(\\instruction)\\textrm{ is high})$. First, we adopt an approach based on ``forward'' mode generation by translating this distribution $P(\\instruction\\given\\trainingData,\\ f(\\instruction)\\textrm{ is high})$ into words. For example, in our instruction induction experiments (Subsection \\ref{sec:inst_induct}), we follow \\citet{honovich2022instruction} and prompt the LLM using Figure \\ref{fig:llm_template} (Top). %In this case, the template suggests the outputs are generated based on the instruction, so that the score functions considered will be high. \n\n\\paragraph{Reverse Mode Generation} Although the ``forward'' model works out of the box for most of the pretrained LLMs, translating $P(\\instruction\\given\\trainingData,\\ f(\\instruction)\\textrm{ is high})$ into words requires custom engineering across different tasks. This is because while instructions are typically found in the beginning of passages, the ``forward'' model only generates text from left to right, which requires the instruction to be predicted at the end of the prompt. Therefore, we desire a more flexible approach such that the instruction can be anywhere in the text. To address this, we consider ``reverse'' mode generation, which uses an LLM with infilling capabilities---e.g., T5~\\citep{raffel2020exploring}, GLM \\citep{du2022glm}, and InsertGPT~\\citep{bavarian2022efficient}---to infer the missing instructions. Our ``reverse'' model directly samples from $P(\\instruction\\given\\trainingData,\\ f(\\instruction)\\textrm{ is high})$ by filling in the blank. We show an example of the such template in Figure \\ref{fig:llm_template} (Middle).\n\n\\paragraph{Customized Prompts} Note that depending on the score function being used, there may exist more appropriate prompts than the samples above. For example, in our TruthfulQA experiments, we start with the human-designed instructions from the original dataset~\\citep{lin2022truthfulqa} and ask the the ``reverse'' model to propose initial instruction samples that fit the missing context (Figure \\ref{fig:llm_template} (Bottom)). \n\n\\subsection{Score Functions} \\label{sec:score_function}\nTo cast our problem as black-box optimization, we choose a score function that accurately measures the alignment between the dataset and the data the model generates. In our instruction induction experiments, we consider two potential score functions, described below. In the TruthfulQA experiments, we focused primarily on automated metrics proposed in \\citet{lin2022truthfulqa}, similar to the execution accuracy. In each case, we evaluate the quality of a generated instruction using Equation (\\ref{eq:score}), and take the expectation over a held-out test dataset $\\testData$.\n\n\\paragraph{Execution accuracy} First, we consider evaluating the quality of an instruction $\\instruction$ using the execution accuracy metric proposed by \\citet{honovich2022instruction}, which we denote as $\\fexec$. In most cases, execution accuracy is simply defined as the 0-1 loss, $f(\\instruction, \\demoQ, \\demoA) = \\mathbb{1}\\left[\\M([\\instruction ; \\demoQ]) = \\demoA\\right]$. On some tasks, execution accuracy takes into account invariants; e.g., it may be an order invariant set matching loss, as described in Appendix A of \\citet{honovich2022instruction}.  \n\n\\paragraph{Log probability} We further consider a softer probabilistic score function, which we hypothesize might improve optimization by providing a more fine-grained signal when searching over low-quality instruction candidates. In particular, we consider the log probability of the desired answer given the instruction and question under the target model $\\M$, which on a per sample basis, is $\\log P(\\demoA\\given[\\instruction ; \\demoQ])$.\n\n\\paragraph{Efficient score estimation}\nEstimating the score by computing the score over the entire training dataset for all instruction candidates can be expensive. To reduce the computation cost, we adopt a filtering scheme where a promising candidate receives more computation resources while a low-quality candidate receives less computation. \nIt can be achieved by using a multi-stage computation strategy on lines 2-9 Algorithm \\ref{alg:ape}. We first evaluate all candidates with a small subset of the training dataset. For the candidates with a score greater than a certain threshold, we sample and evaluate a new non-overlapping subset from the training dataset to update the moving average of the score. \nThen, we repeat this process until a small set of candidates is left, which are evaluated on the entire training dataset. \nThis adaptive filtering scheme significantly improves the computation efficiency by keeping the exact computation costs for the high-quality samples and drastically reducing the computation costs for low-quality candidates. \nWe note that a similar score estimation scheme has been used in previous works \\citep{li2022competition, maclaurin2015firefly}.\n\n\\subsection{Iterative Proposal Distributions}\\label{sec:iterative}\nDespite our attempt to directly sample high-quality initial instruction candidates, it could be the case that the method described in Subsection \\ref{subsec:initialU} fails to produce a good proposal set $\\proposal$, either because it lacks of diversity or does not contain any candidates with a suitably high score. In case of such challenges, we explore an iterative process for resampling $\\proposal$.\n\n\\begin{wrapfigure}{R}{0.3\\textwidth}\n\\centering\n\\vspace{-0.1in}\n\\includegraphics[width=0.275\\textwidth]{figures/illustration/template_resample.pdf}\n\\caption{Resampling}\n\\vspace{-0.15in}\n\\label{fig:template_resampling}\n\\end{wrapfigure}\n\n\\paragraph{Iterative Monte Carlo Search}\nInstead of only sampling from the initial proposal, we consider exploring the search space locally around the current best candidates. This allows us to generate new instructions that are more likely to be successful. We call this variant \\textit{iterative \\algname}. \nAt each stage, we evaluate a set of instructions and filter out candidates with low scores. Then, an LLM is asked to generate new instructions similar to those with high scores. We provide the prompt used for resampling in Figure \\ref{fig:template_resampling}. \nFigure \\ref{fig:main-posterior} (Right) shows that although this approach improves the overall quality of the proposal set $\\proposal$, the highest scoring instruction tends to remain the same with more stages. We conclude iterative generation provides marginal improvement over the relative simplicity and effectiveness of the generative process described in Subsection \\ref{subsec:initialU}. Therefore, we use \\algname~without iterative search as default unless otherwise stated.\\workshopexclude{\\section{Large Language Models are Human-Level Prompt Engineers}}\n\\workshoponly{\\section{Additional Experimental Results}\\label{app:add_res} }\nThis section examines how APE can guide LLMs to desired behaviors. We investigate from four perspectives: zero-shot performance, few-shot in-context learning performance, zero-shot chain-of-thought reasoning, and truthfulness. Our experiments show that APE can find prompts that improve task performance, performing equal to or even better than those authored by humans. APE also often produces insightful tricks for how to best prompt language models that can be successfully transferred to new tasks (see Section \\ref{sec:cot}).\n\\workshoponly{For consistency, we duplicate some of the results here.}\n\n\\subsection{Instruction Induction}\\label{sec:inst_induct}\nWe assess the effectiveness of zero-shot and few-shot in-context learning on 24 instruction induction tasks proposed in \\citet{honovich2022instruction}. The tasks span many facets of language understanding, from simple phrase structure to similarity and causality identification. We provide a detailed descriptions of each task in Appendix B. For each task, we sample five input-output pairs from the training data and select the best instruction using algorithm \\ref{alg:ape}. Then, we evaluate the quality of the instruction by executing the instruction on InstructGPT \\footnote{We use the \\textit{text-davinci-002} via the OpenAI API (\\url{https://beta.openai.com/}). Though not stated explicitly in the API, we assume the models are those reported by \\citet{ouyang2022training}.}. We repeat our experiments five times with different random seeds to report the mean and standard deviation. The exact templates for our experiments can be found in Appendix (Table \\ref{table:raw_templates}).\n\n\\begin{figure}[t]\n  \\vspace{-0.25in}\n  \\centering\n  \\includegraphics[width=0.95\\linewidth]{figures/main/exec_acc_zero_shot.pdf}\n  \\caption{Zero-shot test accuracy on 24 Instruction Induction tasks. \\algname~achieves human-level or better performance on all  24 out of 24 tasks.}\\label{fig:main-zero-shot}\n\\end{figure}\n\n\\paragraph{Zero-shot Learning}\nWe compare our method against two baselines: human prompt engineers (Human)\\footnote{\\mbox{We use the gold annotations from \\citet{honovich2022instruction}, which were manually verified for correctness.}} and the model-generated instruction algorithm proposed by \\citet{honovich2022instruction}. This algorithm can be thought of as a greedy version of \\algname, without a search and selection process; thus, we refer to it as ``Greedy''. Figure \\ref{fig:main-zero-shot} shows the zero-shot performance of InstructGPT using human instructions and model generated instructions. Our algorithm outperforms ``Greedy'' on every task and achieves equal or better than human performance on 24 of 24 tasks. Moreover, the Interquartile Mean (IQM) \\citep{agarwal2021deep} across all 24 tasks in Figure \\ref{fig:highlight} suggests that \\algname~with InstructGPT outperforms human-engineered prompts, obtaining an IQM of 0.810 vs humans' 0.749. We summarize the instruction selected by \\algname~for each task in Appendix (Table \\ref{table:best_instructions_all}).\n\n\\paragraph{Few-shot In-context Learning}\nWe evaluated APE-generated instructions in  few-shot in-context learning, where we insert the instruction before the in-context demonstrations. Those instructions are selected based on zero-shot execution accuracy, and we denote this setting as ``Instruction + In-context'' in Figure \\ref{fig:main-few-shot}. As shown in Figure \\ref{fig:main-few-shot}, adding an instruction achieves a comparable or better test performance than the standard in-context learning performance on 21 of 24 tasks. Counter-intuitively, adding in-context examples for Rhymes, Large Animal, and Second Letters hurts model performance. We conjecture that it may be because the selected instructions overfit the zero-shot learning scenario and thus do not perform well on the few-shot case. Therefore, we experiment using few-shot execution accuracy as the selection metric. Figure \\ref{fig:app-few-shot-as-metric} shows that the few-shot metric achieves comparable or slightly better than the zero-shot metric except for Rhymes. To have an intuitive understanding of what is happening, we provide a qualitative analysis in Appendix \\ref{sec:app_ii}. \n\n\\subsection{BigBench}\\label{sec:bigbench}\nTo see whether APE can be applied to more challenging tasks, we propose and curate BIG-Bench Instruction Induction (BBII), a clean and tractable subset of 21 tasks that have a clear, human-written instruction that can be applied to all examples in the dataset. The selected tasks cover many facets of language understanding and includes all nine such problems from the BigBench-Hard Subset \\citep{suzgun2022challenging}. In particular, it includes emotional understanding, context-free question answering, reading comprehension, summarization, algorithms, and various reasoning tasks (e.g., arithmetic, commonsense, symbolic, and other logical reasoning tasks). We provide a detailed description of the task and our selection criteria in Appendix \\ref{app:imp_details}. \n\nFor each task, we used the reverse mode generation of InstructGPT to generate a set of instruction candidates and ranked the instructions based on their execution accuracy. Then, we executed the selected instruction on InstructGPT to compute the zero-shot performance on the test set and compared it with the default human prompt. As shown in Appendix Table \\ref{table:bbii_results}, APE achieves comparable or better performance than the default human prompt on 17 out of 21 tasks.\n\n\\subsection{Zero-shot Chain of Thought}\\label{sec:cot}\nChain-of-thought reasoning has been shown to dramatically improve the ability of LLMs to complete complex reasoning tasks, such as solving math problems that require multiple steps. Early works \\citep{nye2021show,betz2021thinking,wei2022chain} on chain-of-thought used fine-tuning or in-context learning to get LLMs to show their work for such problems. One of the most influential recent works of prompt engineering was the discovery \\citep{kojima2022large} that LLMs could be made to give chain-of-thoughts simply by prepending ``Let's think step by step.'' to the beginning of the LLM's response. Known as Zero-Shot-CoT, this prompting strategy improves the zero-shot performance of InstructGPT on MultiArith \\citep{roy2016solving} from 17.7 to 78.7 and improves performance on GSM8K\\citep{cobbe2021training} from 10.4 to 40.7. As shown in Table \\ref{tab:cot-arith}, \\citet{kojima2022large} found their prompt was the best performing out of at least nine human-designed prompts.\n\nWe used APE to automatically search for the best answer-prefix across the suite of tasks used in \\citet{kojima2022large}. Our approach to optimizing this prompt was inspired by \\citet{zelikman2022star}. First, we generate a dataset of questions and reasoning steps generated using InstructGPT with ``Let's think step by step.'' Then, we remove any data points that had incorrect answers. Finally, we use APE to find a prompt starting with ``Let's'' that maximizes the likelihood of these correct reasoning steps. See Table \\ref{table:raw_templates} for the template used for prompt generation and evaluation. APE produces the prompt ``Let’s work this out in a step by step way to be sure we have the right answer.'' This generated prompt further improves performance from 78.7 to 82.0 on MultiArith and from 40.7 to 43.0 on GSM8K. We believe this general workflow represents a common use-case for APE where prompt engineers use APE to optimize parts of their exiting templates to improve performance. See Figure \\ref{fig:cot-all} for details on the performance of this prompt on other reasoning tasks.\n\n\\workshopexclude{\n\\subsection{TruthfulQA}\nWe apply our method on TruthfulQA \\citep{lin2022truthfulqa} to see how \\algname-generated instructions can steer an LLM to generate answers with different styles, and study the trade-off between truthfulness and informativeness. Borrowing the metrics from the original paper, we use \\algname~to the learn instructions that maximize three metrics: truthfulness (\\% True), informativeness (\\% Info), and a combination of both (\\%True + \\%Info). \\citet{lin2022truthfulqa} used human evaluation to assess the model performance, but they found their automated metrics align with human prediction over 90\\% of the time. In our experiments, we rely on their fine-tuned GPT-judge and GPT-info to evaluate the scores. \n\n\\paragraph{Prompt Engineering in TruthfulQA} We want to stress that the TruthfulQA dataset is intended to test pretrained models in zero-shot settings. Our results are not in any way compatible with the original benchmarks. Because we have optimized the instructions using a small portion of the question and answer pairs as training demonstrations, our results are not ``true few-shot learning''~\\citep{perez2021true}. We randomly sampled 100 out of 817 questions for the actual experiments to form training demonstrations $\\trainingData$. To sample the proposal set $\\proposal$, we ask a ``reverse'' model to generate instructions based on six randomly chosen demonstration pairs, similar to our previous experiments. Unlike in Instruction Induction, in TruthfulQA, we aim to find a single best instruction prompt that works well across all 38 categories of questions spanning health, law, politics, and fiction. It is worth noting all our generated instructions are very generic, e.g., ``You will be asked a series of questions. For each question, you must either answer the question or decline to answer, in which case you must state that you have no comment'', and do not contain any examples from the dataset.\n\n\\begin{figure}\n  \\vspace{-0.25in}\n  \\centering\n\\begin{subfigure}[b]{0.245\\textwidth}\n  \\captionsetup{justification=centering}\n  \\hfill\\includegraphics[width=1.0\\linewidth]{figures/main/truthfulqa_top10_train.pdf}\n  \\caption{Average performance Train}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.245\\textwidth}\n  \\captionsetup{justification=centering}\n  \\hfill\\includegraphics[width=1.0\\linewidth]{figures/main/truthfulqa_top10_test.pdf}\n  \\caption{Average performance Test}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.245\\textwidth}\n  \\captionsetup{justification=centering}\n  \\hfill\\includegraphics[width=1.0\\linewidth]{figures/main/truthfulqa_scatter_train.pdf}\n  \\vspace{-1.6em}\n  \\caption{\\%True-\\%Info trade-off Training}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.245\\textwidth}\n  \\captionsetup{justification=centering}\n  \\hfill\\includegraphics[width=1.0\\linewidth]{figures/main/truthfulqa_scatter_test.pdf}\n  \\vspace{-1.6em}\n  \\caption{\\%True-\\%Info trade-off Test}\n\\end{subfigure} \\vspace{-0.15in}\n  \\caption{Comparison of \\algname~and ``help'' (human) prompt on the TruthfulQA task. (a) Percentage of answers that were either true (\\% True), informative (\\% Info), or both (\\% True + \\% Info) on the 100 training examples. (b) Same data on the 717 test examples. (c) \\%True-\\%Info frontier computed on training data with top 10 instructions from each metric. (d) \\%True-\\%Info frontier on the test data. }\\label{fig:truthfulqa}\n\\end{figure}\n\n\\paragraph{Truthfulness vs Informativeness Trade-off}\nWe found that \\algname~outperforms the human-engineered prompt with only 200 candidates proposed by InstructGPT (175B), as seen in Figure~\\ref{fig:truthfulqa}. We compared our generated prompt with the ``help'' prompt from \\citet{lin2022truthfulqa}.\nThe training and test performance are shown in Figure~\\ref{fig:truthfulqa}(a)-(b). We found that choosing the top 10 of 200 candidates on the training set generalizes well to the test set. We report the average performance across the top 10 instructions for the three metrics.\nThis result by itself is not surprising as the human baseline is not carefully chosen, as pointed out by \\citet{askell2021general}. However, we found that the instructions discovered by \\algname~can achieve very high truthfulness with answers such as ``No comment,'' but these answers provide little information. We used our top candidates to further investigate the trade-off between truthfulness and informativeness. We visualize the top 10 proposed samples across the three metrics on the truthfulness-informative plots shown in Figure~\\ref{fig:truthfulqa}(c) and Figure~\\ref{fig:truthfulqa}(d). While \\algname~achieves over 40\\% accuracy in providing both true and informative answers (v.s. 30\\% by the ``help'' prompt from humans), the instructions discovered tend to target the two ends of this \\%true-\\%info Pareto frontier. \n}\n\n\\workshopexclude{\\section{Quantitative Analysis}}\n\\workshoponly{\\section{Additional Results - Quantitative Analysis}}\nIn this section, we conduct quantitative analyses to better understand the three main components of our method: proposal distribution, score functions, and iterative search. Moreover, we conduct a cost analysis in the Appendix \\ref{sec:cost_analysis} to understand the most cost-efficient way to find the best prompt. We observe the larger and more powerful language models are more cost-effective for generating the best prompt despite a higher per-token cost.\n\n\\subsection{LLMs for Proposal Distribution}\n\n\\paragraph{How does the proposal quality change as we increase the model size?} To understand how the model size affects the quality of the initial proposal distribution, we examine eight different models\\footnote{We use ada, babbage, curie, davinci, text-ada-001, text-babbage-001, text-curie-001, text-davanci-002} available via the OpenAI API. To assess the quality of the proposal distribution, we generate 250 instructions per model and compute the execution accuracy on 50 test data points. We visualize the survival function (percentage of instructions with test accuracy greater than a certain threshold) and the histogram of test accuracy for a simple task (i.e., Pluralization) in Figure \\ref{fig:main-posterior} (a) and include a similar plot for a more challenging task (Start With) in the Appendix (Figure \\ref{fig:app-posterior-model-size-hard}). As shown in both figures (and unsurprisingly), larger models tend to produce better proposal distributions than smaller ones, as do the models that were fine-tuned to follow human instructions. On the simple task, all instructions generated by the best model, InstructGPT (175B), have reasonable test accuracy. In contrast, half of the instructions are off-topic and perform poorly on the more challenging task. \n\n\\begin{figure}\n  \\centering\n  \\vspace{-0.1in}\n \\begin{subfigure}[b]{0.49\\textwidth}\n  \\includegraphics[width=1.0\\linewidth]{figures/main/posterior_model_size_plural.pdf}\n  \\end{subfigure}\n  \\begin{subfigure}[b]{0.49\\textwidth}\n  \\includegraphics[width=1.0\\linewidth]{figures/main/posterior_mcmc_passive.pdf}\n  \\end{subfigure}\n  \\caption{(Left) Quality of the proposal distribution of models with different size as assessed by test execution accuracy. (Right) Iterative Monte Carlo search improves the quality of the instruction candidates at each round.}\\label{fig:main-posterior}\n\\end{figure}\n\n\\subsection{LLMs for selection}\n\\paragraph{Does proposal quality matter under selection?} If we sample more instructions from the LLMs, then it becomes more likely for us to find better instructions. To verify this hypothesis, we increase the sample size from 4 to 128 and evaluate the test accuracy change. Figure \\ref{fig:mcmc_comparison} (Left) shows a monotonically increasing trend with a diminishing return, as human-level performance is achieved with 64 instruction samples. Thus, we choose 50 as our default sample size. Under this configuration, we investigate how the proposal distribution affects the test accuracy of the best instruction selected by our algorithm. Figure \\ref{fig:highlight}(b) shows that though the small models may be less likely to generate good instructions, they nonetheless generate some good ones if we sample enough candidates. Therefore, we still find promising instructions with a small model by running our selection algorithm, explaining why our method outperforms the greedy approach \\cite{honovich2022instruction} across all eight models.\n\n\\paragraph{Which scoring function is better?} We compute the correlation between the test accuracy and two metrics on 24 instruction induction tasks to study how good our proposed metrics are. We generate 250 instructions per task using InstructGPT (175B) in “forward” mode and compute the metric score and test accuracy on 10 test data points. We visualize the Spearman correlation between the test accuracy and two metrics. Figure \\ref{fig:mcmc_comparison} (Middle) shows that the execution accuracy aligns better with the test performance across the tasks. Thus, we choose it as our default metric unless otherwise stated.\n\n\\begin{figure}\n  \\vspace{-0.15in}\n  \\centering\n  \\begin{subfigure}[b]{0.31\\textwidth}\n  \\includegraphics[width=1.0\\linewidth]{figures/main/sample_size_task_mean.pdf}\n  \\end{subfigure}\\hfill\n \\begin{subfigure}[b]{0.31\\textwidth}\n  \\includegraphics[width=1.0\\linewidth]{figures/main/corr_metric_spearman.pdf}\n  \\end{subfigure}\\hfill\n   \\begin{subfigure}[b]{0.31\\textwidth}\n  \\includegraphics[width=1.0\\linewidth]{figures/main/mcmc_comparison.pdf}\n  \\end{subfigure}\n  \\caption{(Left) Test execution of the best instruction as we increase the number of instruction candidates. We report the mean and standard deviation across 6 different tasks. (Middle) Spearman Correlation between the test accuracy and two metrics on 24 tasks. (Right) Test execution accuracy of the best instruction selected using \\algname and iterative \\algname (\\algname (IT)).}\\label{fig:mcmc_comparison}\n  \\vspace{-0.1in}\n\\end{figure}\n\\subsection{Iterative Monte Carlo Search}\\label{ab:tmcs}\n\\paragraph{Does Iterative Search improve the instruction quality?} We visualize the survival function and histogram of test accuracy on the ``Passivization'' task in Figure \\ref{fig:main-posterior} (Right) and include five more tasks in the Appendix. The survival plot shows that the curves increase as the round goes up, which suggests that iterative search does result in a higher-quality proposal set. However, we observe diminishing returns to further selection rounds as the quality seems to stabilize after three rounds.\n\n\\paragraph{Do we need Iterative Search?}\nWe compare \\algname~and iterative \\algname~on six tasks\\footref{sixtasks}. As shown in Figure \\ref{fig:mcmc_comparison}, the iterative search marginally improves performance on tasks where APE underperforms humans but achieves similar performance on the other tasks. This is consistent with our hypothesis that iterative search would be most useful on tasks where generating a good initial $\\proposal$ is challenging.\n\\section{Conclusion}\nLarge language models can be seen as general-purpose computers that execute programs specified by natural language prompts. We automate the prompt engineering process by formulating it as a black-box optimization problem, which we propose to solve using efficient search algorithms guided by LLMs. Our method achieves human-level performance on various tasks with minimum human inputs. As recent LLMs demonstrate an impressive ability to follow human instruction, we expect many future models, including those for formal program synthesis, to have a natural language interface. This work builds the foundation to control and steer generative artificial intelligence.\n\n\\subsubsection*{Acknowledgments}\nWe would like to thank Or Honovich and Michael Zhang for their help and valuable feedback. JB was supported by NSERC Grant [2020-06904], CIFAR AI Chairs program, Google Research Scholar Program and Amazon Research Award. KP was supported by NSERC PGS-D. SP was supported by NSERC CGS-D. HC was supported by NSERC CGS-D and RBC Graduate Fellowship. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute for Artificial Intelligence.\n\n\\newpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Automatic Prompt Optimization with ``Gradient Descent'' \\\\and Beam Search}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nLarge Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, \\emph{\\textbf{Pr}ompt \\textbf{O}ptimization with \\textbf{Te}xtual \\textbf{G}rad\\textbf{i}ents} (ProTeGi), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language ``gradients'' that criticize the current prompt, much like how numerical gradients point in the direction of error ascent. The natural language gradients are then ``propagated'' into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing techniques and improve an initial prompt's performance by up to 31\\%, by using data to rewrite vague task descriptions into more precise annotation instructions.\\footnote{Code and data available at: \\url{https://github.com/microsoft/LMOps/tree/main/prompt_optimization}.}\n\\end{abstract}\n\n\\section{Introduction}\nLarge Language Models (LLMs) trained on web-scale text have recently demonstrated unprecedented abilities across a variety of NLP tasks \\cite{gpt4,bubeck2023sparks}. \nThese LLMs use prompt inputs to follow human instructions. Writing prompts in natural language remains a manual trial-and-error process requiring significant human effort \\cite{jiang2022promptmaker} and expertise \\cite{reynolds2021prompt,zamfirescu2023johnny}.\n\nAccordingly, there is need for automatic or semi-automatic procedures to help humans write the best prompts. This would help reduce manual effort, improve task performance, and produce interpretable descriptions of a cognitive decision process.\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.9\\linewidth]{front.png}\n\\caption{Overview of the proposed Prompt Optimization with Textual Gradients (ProTeGi).}\n\\label{fig:firstpage}\n\\end{figure}\n\nA recent body of work has investigated this problem by training auxiliary models or differentiable representations of the prompt \\cite{qin2021learning,deng2022rlprompt}. However, such works assume access to internal state variables of the LLM  \\cite{shin2020autoprompt,lester2021power} while practitioners often communicate with LLMs through an API. Other work applies discrete manipulations to prompts via Reinforcement Learning or LLM-based feedback \\cite{zhang2023tempera,zhou2022large}. These algorithms may also require low-level access to the LLM, produce incomprehensible outputs, or rely on directionless monte-carlo search over the semantic space of prompts.% or task-specific feedback \\cite{chen2023teaching}. \n\nWe propose Prompt Optimization with Textual Gradients (ProTeGi), a general purpose and nonparametric algorithm for automatic prompt optimization that connects these two bodies of research by applying discrete improvements to prompts in a directed way. \n\nUnlike prior work, we overcome the discrete optimization barrier by mirroring the steps of gradient descent within a text-based Socratic dialogue \\cite{zeng2022socratic}, substituting differentiation with LLM feedback and backpropagation with LLM editing.\nIn detail, we use minibatches of training data to produce ``gradients'' in natural language, i.e., descriptions of the current prompts' flaws with respect to the minibatch, then edit the current prompt in the opposite semantic direction of the gradient. These steps become the expansion part of a wider beam search over the space of prompts, increasing algorithmic efficiency by treating the problem of beam candidate selection as an instance of the best arm identification problem \\cite{audibert2010best}.\n\nWe then offer a preliminary case study of ProTeGi. We evaluate the proposed framework in multiple configurations across 4 NLP tasks, including the novel problem of LLM jailbreak detection. The results suggest that the proposed algorithm can improve on the performance of the initial prompt input by up to 31\\%, exceeding state-of-the-art prompt learning baselines by an average of 4-8\\% while relying on fewer LLM API calls. We also demonstrate the interpretability of the optimization process and investigate the algorithms' shortcomings.\n\n\\section{Discrete Prompt Optimization with Nonparametric ``Gradient Descent''}\n\nThe proposed algorithm assumes access to an initial prompt $p_0$ and i.i.d. training data consisting of pairs of input and output text (numbers, categories, summaries, etc): $\\mathcal{D}_{tr} = \\{(x_1, y_1), ..., (x_n, y_n)\\}$. \nNote that all prompts $p$ are drawn from the space of coherent natural language $\\mathcal{L}$.\nWe assume access to a black box LLM API $LLM_{p}(x) \\approx argmax_{y \\in \\mathcal{L}} P_{LLM}(y \\vert p, x)$, which returns a likely text continuation $y$ of the prompt formed by concatenating $p$ and $x$ (for example, few-shot prompt and input example, or chatbot persona and conversational history).\n\nWithin this context, our algorithm iteratively refines the prompt $p_0$ to produce $\\hat{p}$, an approximation of the optimal prompt $p^* = argmax_{p\\in \\mathcal{L}} \\{ m(p, \\mathcal{D}_{te}) \\}$ for some metric function $m(\\cdot)$ and in-domain test or development data $\\mathcal{D}_{te}$. \n\nIn the following sections, we first introduce how the algorithm performs textual ``gradient descent'' to improve the prompts in a directed way (Section \\ref{sec:graddescent}).\nThen the algorithm leverages these gradient descent steps to beam search through the space of coherent language $\\mathcal{L}$, guided by the gradients during beam expansion, and efficient best arm identification during beam selection (Section \\ref{sec:beamsearch}). %Algorithm \\ref{alg:APO} describes the overall approach.\n\n\\subsection{Gradient descent with Prompts}\n\\label{sec:graddescent}\nIn our setting, gradient descent refers to the process of (1) evaluating a prompt with a batch of data, (2) creating a local loss signal which contains information on how to improve the current prompt, then (3) editing the prompt in the opposite semantic direction of the gradient before starting the next iteration. \n\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\linewidth]{gd.png}\n\\caption{The text dialogue tree we use to mimic gradient descent and overcome the discrete optimization barrier. First, from the top left a feedback prompt $\\Delta$ generates the gradient $g$ from starting prompt $p_0$ and prediction $\\hat{y}$. Second, from the top right an editing prompt $\\delta$ applies the gradient $g$ to $p_0$ and produce improved prompts $p'$, their paraphrases $p''$, and efficient best candidate selection before the next iteration (bottom left).}\n\\label{fig:gd}\n\\end{figure}\n\nWe accomplish this process with a pair of static LLM prompts, as depicted in Figure \\ref{fig:gd}. The first prompt is for creating the loss signals (``gradients'') and is called $\\nabla$. While the specific contents can vary and be task-specific or task-agnostic,\\footnote{We use the same prompts for all tasks; see Appendix.} $\\nabla$ must always consider the current prompt $p_0$, plus $p_0$'s behavior on a minibatch of data (particularly the errors), and generate a natural language summary of $p_0$'s flaws. This summary becomes the gradient $g$. Similar to how traditional gradients represent a direction in parameter space that would make the model worse, the text ``gradients'' $g$ represent directions in a semantic space that are making the prompt worse. \n\nThe second prompt is called $\\delta$ and while this prompt can also vary, it must always take the gradient $g$ and current prompt $p_0$, then perform an edit on $p_0$ in the opposite semantic direction of $g$, i.e. fix the problems with $p_0$ that are indicated by $g$.\\footnote{Note that one can imagine operationalizing the concept of learning rates or step sizes by e.g. editing $\\delta$ to perform large- or small-sized edits to $p_0$, in this initial work we adopt an ``adaptive'' step size by allowing the LLM to decide edit size, and leave further exploration to future work.}\n\nUnlike the traditional machine learning setting, we do not generate a single gradient or edit, but rather a number of directions that may improve the current prompt.\nSection~\\ref{sec:beamsearch} describes in detail the process of generating and selecting candidate prompts.\n\n\\subsection{Beam Search over Prompts}\n\\label{sec:beamsearch}\n\nThe gradient descent steps described in Section \\ref{sec:graddescent} are used to guide a beam search over the space of prompts. This beam search is the outer loop of our prompt training algorithm and it is described in Algorithm~\\ref{alg:APO}.\n\n\\begin{algorithm}\n\\caption{Prompt Optimization with Textual Gradients (ProTeGi)}\n\\label{alg:APO}\n\\begin{algorithmic}[1]\n\\REQUIRE $p_0$: initial prompt, z$b$: beam width, $r$: search depth, $m$: metric function\n\\STATE $B_0 \\leftarrow \\{p_0\\}$\n\\FOR{$i \\leftarrow 1$ to $r-1$}\n    \\STATE $C \\leftarrow \\emptyset$\n    \\FORALL{$p \\in B_i$}\n        \\STATE $C \\leftarrow C \\cup Expand(p)$\n    \\ENDFOR\n    \\STATE $B_{i+1} \\leftarrow Select_b(C, m)$\n\\ENDFOR\n\\STATE $\\hat{p} \\leftarrow argmax_{p \\in B_r} m(s)$\n\\RETURN $\\hat{p}$\n\\end{algorithmic}\n\\end{algorithm}\n\nThe beam search is an iterative optimization process where for each iteration the current prompt is used to generate many new candidate prompts ($expansion$). \nNext, a $selection$ process is used to decide which prompts are worth carrying forward to the next iteration.\nThis loop allows for incremental improvements and exploration over multiple prompt candidates.\n\n\\subsubsection{Expansion Step}\n\nThe \\emph{expansion step} is  used to generate many new candidate prompts from a current prompt (Algorithm \\ref{alg:beam_search_expand}). It leverages the conceptual ``gradient descent'' framework of Section \\ref{sec:graddescent}, and our specific prompts can be found in the Appendix.\n\nFirst we sample a minibatch of data, run the initial prompt on these data with $LLM_{p_0}$, and collect errors. Second, we plug these errors into a prompt template $\\Delta$, which instructs the LLM to describe the problems with $p_0$ which could have led to these mistakes. The ensuing generations are our natural language gradients; see Figure \\ref{fig:firstpage} for an example. \n\nSecond, the gradients are provided to another LLM prompt called $\\delta$, which instructs the LLM to edit the current prompt $p_0$ in order to fix the problems described by the gradient. In this way, we engadge the LLMs in a recursive feedback loop similar to the Socratic dialogues proposed by \\citet{zeng2022socratic}. \n\nLast, additional candidates are generated by running the existing candidates through a paraphrasing LLM called $LLM_{mc}$, to explore the local monte carlo search space around the new prompt candidates. This prompt simply asks the LLM to generate new candidates which are worded differently but semantically similar to their inputs.\n\n\\begin{algorithm}\n\\caption{$Expand(\\cdot)$ - line 5 of Algorithm 1}\n\\label{alg:beam_search_expand}\n\\begin{algorithmic}[1]\n\\REQUIRE $p$: prompt candidate, $\\mathcal{D}_{tr}$: train data\n\\STATE Sample minibatch $\\mathcal{D}_{mini} \\subset \\mathcal{D}_{tr}$\n\\STATE Evaluate prompt $p$ on minibatch $\\mathcal{D}_{mini}$ and collect errors $e = \\{ (x_i, y_i) : (x_i, y_i) \\in \\mathcal{D}_{mini} \\land LLM_{p}(x_i) \\neq y_i \\}$\n\\STATE Get gradients: $\\{g_1, ..., g_m\\} = LLM_{\\nabla}(p, e)$\n\\STATE Use the gradients to edit the current prompt: $\\{p_{i1}', ..., p_{iq}'\\} = LLM_{\\delta}(p, g_i, e)$\n\\STATE Get more monte-carlo successors: $\\{p_{ij1}'', ..., p_{ijm}''\\} = LLM_{mc}(p_{ij}')$\n\\RETURN $\\{p_{11}', ..., p_{mq}'\\} \\cup \\{p_{111}'', ..., p_{mqp}''\\}$\n\\end{algorithmic}\n\\end{algorithm}\n\n\\subsubsection{Selection Step}\n\\label{sec:beamselection}\nOnce the expansion process has stepped each candidate prompt into multiple possible successor candidates, the selection step chooses the $b$ most promising candidates to stay on the beam for the next iteration.\n\nIt is expensive to evaluate each candidate prompt on the entire training dataset \\cite{prasad2022grips}, so we would like to minimize the number of such queries. Note that this almost exactly corresponds to the well-studied problem of best arm identification in bandit optimization \\cite{audibert2010best}. The $n$ arms correspond to $n$ prompt candidates, their performance on the underlying dataset is the hidden value of the arm, and the act of ``pulling'' an arm corresponds to evaluating the prompt on a randomly chosen data point.\nThe goal is then to find the $b$ best arms with as few pulls as possible, and we consider the following algorithms.\n\n\\textbf{UCB Bandits}. Motivated by other works which quickly estimate LLM performance \\cite{li2022competition,zhou2022large}, we sample a subset of prompts according to a proposal distribution of prompt performance, evaluate those prompts on a random subset of data, then update the proposal distribution based on the observed performance. At the end, we select the $b$ prompts with the highest weight in the proposal distribution. See Algorithm \\ref{alg:ucb} for details, where $Q_t(p_i)$ is the estimated performance of prompt $p_i$ at time step $t$, $N_t(p_i)$ is the total queries for prompt $i$ so far at time $t$, and $c$ is an exploration parameter.\n\n\\begin{small}\n\\begin{algorithm}\n\\caption{$Select(\\cdot)$ with UCB Bandits - line 7 of Algorithm 1}\n\\label{alg:ucb}\n\\begin{algorithmic}[1]\n\\REQUIRE $n$ prompts $p_1, ..., p_n$, dataset $\\mathcal{D}_{tr}$, $T$ time steps, metric function $m$\n\\STATE Initialize: $N_t(p_i) \\gets 0$ for all $i = 1, \\dots, n$\n\\STATE Initialize: $Q_t(p_i) \\gets 0$ for all $i = 1, \\dots, n$\n\\FOR{$t =1, \\dots, T$}\n    \\STATE Sample uniformly $\\mathcal{D}_{sample} \\subset \\mathcal{D}_{tr}$\n    \\STATE \\begin{small}$p_i \\leftarrow \\left\\{\\begin{array}{@{}l@{\\quad}l}\n\\arg\\max_p \\left\\{Q_t(p) + c \\sqrt{\\frac{\\log t}{N_t(p)}}\\right\\} & \\mathrm{(UCB)}\\\\\n\\arg\\max_p \\left\\{Q_t(p) + c \\sqrt{\\frac{c}{N_t(p)}}\\right\\} & \\mathrm{(UCB\\ E)}\n\\end{array}\\right.$\\end{small}\n    \\STATE Observe reward $r_{i,t} = m(p_i, \\mathcal{D}_{sample})$\n    \\STATE $N_t(p_i) \\gets N_t(p_i) + \\vert \\mathcal{D}_{sample} \\vert$\n    \\STATE $Q_t(p_i) \\gets Q_t(p_i) + \\frac{r_{i, t}}{N_t(p_i)}$\n\\ENDFOR\n\\RETURN $SelectTop_b(Q_T)$\n\\end{algorithmic}\n\\end{algorithm}\n\\end{small}\n\nWhile a natural choice, UCB is designed primarily for regret minimization \\cite{kuleshov2014algorithms}, whereas we wish to perform the related but distinct task of best arm identification. Furthermore, UCB can perform poorly if the exploration parameter $c$ is not tuned appropriately \\cite{bubeck2012regret}. \n\n\\textbf{UCB-E} is a variant of UCB that corrects some of these problems by favoring exploration, leading to better theoretical convergence properties \\cite{audibert2010best}. However, UCB-E remains stuck with hyperparameters like $T$, $c$, and $\\vert \\mathcal{D}_{sample}\\vert$.\n\n\\textbf{Successive Rejects} (Algorithm \\ref{alg:sr}) is provably optimal for best arm identification \\citep{audibert2010best}, requires no hyperparameters unlike its UCB alternatives, and is suprisingly simple. The algorithm proceeds in $n - 1$ phases, and in each phase, maintains a set of surviving prompt candidates $S_k \\subseteq \\{p_1, \\ldots, p_n\\}$.\nIn the $t$-th phase, we evaluate each candidate in $S_{t - 1}$ on a total of $n_t$ random data points to form an empirical estimate of the score $m(p_i, \\mathcal{D}_{tr})$.\nThen, to form $S_t$, we drop the prompt with the lowest score in this phase.\nNote that $n_t$ is computed according to Equation 1 below such that it gradually increases with $T$:\n\\begin{equation}\n    n_t = \\left \\lceil{\\frac{1}{0.5 + \\sum_{i=2}^{T} 1 / i} * \\frac{B - T}{T + 1 - t}}\\right \\rceil \n\\end{equation}\nwhere $B$ is the total query budget.\n\n\\begin{algorithm}\n\\caption{$Select(\\cdot)$ with Successive Rejects - line 7 of Algorithm 1}\n\\label{alg:sr}\n\\begin{algorithmic}[1]\n\\REQUIRE $n$ prompts $p_1, ..., p_n$, dataset $\\mathcal{D}_{tr}$, metric function $m$\n\\STATE Initialize: $S_0 \\gets \\{p_1, \\ldots, p_n\\}$\n\\FOR{$k = 1, \\dots, n - 1$}\n    \\STATE Sample $\\mathcal{D}_{sample} \\subset \\mathcal{D}_{tr}$, $\\vert \\mathcal{D}_{sample}\\vert = n_k$ \n    \\STATE Evaluate $p_i \\in S_{k - 1}$ with $m(p_i, \\mathcal{D}_{sample})$\n    \\STATE $S_k \\gets S_{k - 1}$, excluding the prompt with the lowest score from the previous step\n\\ENDFOR\n\\RETURN Best prompt $p^* \\in S_{n - 1}$\n\\end{algorithmic}\n\\end{algorithm}\n\nIn addition to the vanilla successive rejects algorithm, we experiment with \\textbf{Successive Halving} (SH) which is more agressive as at the end of each phrase it rejects the bottom half of prompts according to their scores, with $n_k = B / (\\vert S_{k-1} \\vert \\log_2 k)$ \\cite{karnin2013almost}.  \n\n\\section{Experiments}\n\nWe present a limited and preliminary case study to demonstrate the proposed ProTeGi algorithm across 4 benchmark NLP tasks, finding that it can exceed state-of-the-art prompt learning baselines in terms of efficiency and performance.\n\n\\subsection{Data}\n\nWhile ProTeGi could be applied to any problem such as parsing, chatbot design or summarization simply by choosing different metric functions $m$, we experiment on four NLP benchmark classification tasks for this initial case study. The tasks cover a wide range of problem and language domains, and are as follows:\n\n\\textbf{Jailbreak}: a novel task where the goal is to determine whether a user input to an LLM continuation API (i.e. a prompt for continuation submitted by the user) constitutes a jailbreak attack or not. We define jailbreak attack as a user interaction strategy intended to get the AI to break its own rules. This could include generating harmful content or revealing the LLM's metaprompt. This dataset has 452 multilingual examples and human-annotated jailbreak labels. \\textbf{Ethos} \\cite{mollas2020ethos} is an online English hate speech detection dataset with 997 online comments and hate speech labels. \\textbf{Liar} \\cite{wang2017liar} is an English fake news detection dataset with 4000 statements, context, and lie labels. \\textbf{Sarcasm} \\cite{farha2020arabic} is an Arabic sarcasm detection dataset with 10,000 online comments and sarcasm labels.\n\n\\subsection{Setup}\nFor each task, we randomly sample 50 examples for development and 150 for test. All of the reported results are an average of 3 experimental trials. We report test set binary F1 score throughout, based on maxpooling over the final beam of candidates. %For each task, we report the max binary F1 score across the top 4 prompt candidates produced by each method (baseline and proposed).\nUnless otherwise stated, experiments were performed with a January 2023 version \\texttt{gpt-3.5-turbo}, using the Azure OpenAI LLM API service with a temperature of 0.0 during few-shot classification and 1.0 in all other contexts.\n\nAs the focus of this paper is nonparametric algorithms with broad applicability, we did not conduct any hyperparameter search for the baseline or proposed algorithms, instead adopting default values and then using the same parameters throughout. \n\nUnless otherwise stated, for the proposed Automatic Prompt Optimization Algorithm we used a minibatch size of $\\vert \\mathcal{D}_{mini} \\vert = 64$, beam size $b=4$, and ran the algorithm for 6 optimization steps. Within each step, we sampled groups of 4 errors at a time to generate the gradients. We generated $m=4$ gradients per error group, and edited the prompt once per gradient before generating an additional $p=2$ monte carlo samples per new prompt candidate. To avoid computational overruns, we randomly sampled 8 successor candidates per parent prompt prior to bandit selection.\n\nWe used the same metric function $m$ as the optimization target across all tasks: F1 score. While recent works have opted to use the model's log-likelihood to evaluate prompts instead of an accuracy-related metric \\cite{lu2021fantastically,prasad2022grips,zhou2022large}, preliminary experiments showed this technique did not help our algorithm, and many of the most powerful LLM APIs like ChatGPT and GPT4 did not provide log likelihoods at the time of writing. \n\nThe proposed algorithm is about optimizing the language of prompts, as opposed to selecting the best examples for few-shot learning. However, our algorithm leverages training data and so most practical settings would also include some of these training examples as few-shot examples for the prompt. Accordingly, all of the experiments of Section \\ref{sec:results} were conducted with a randomly selected pair of few-shot examples which were held constant as we optimized the other parts of the prompt.\n\n\\subsection{Baselines}\n\nWe compare the proposed ProTeGi framework against the following baselines. Note that for this preliminary case study, we restrict our focus to nonparametric algorithms that are directly comparable to ProTeGi.\n\\begin{figure*}\n\\centering\n\\includegraphics[width=\\linewidth]{mainfig.png}\n\\caption{Test performance (F1) vs API query budget per prompt candidate.}\n\\label{fig:mainresult}\n\\end{figure*}\n\n\\textbf{Monte-Carlo} (MC). The Automatic Prompt Engineering algorithm proposed by \\citet{zhou2022large} proposes an iterative but directionless monte carlo search over the space of prompts. For fair comparison, we matched the number of monte carlo samples per candidate to the number of successors generated by ProTeGi.\n\n\\textbf{Reinforcement Learning} (RL). Recently proposed, concurrent works like GrIPS \\cite{prasad2022grips} and TEMPERA \\cite{zhang2023tempera} rely on phrase-level operations over the prompt text: the prompt is chunked into phrases with e.g. nltk \\cite{bird2006nltk}, then the search space includes add, paraphrase, swap, and delete operations over the phrases.\\footnote{Note that while GRIPS isn't an RL algorithm, we introduce GRIPS and TEMPURA together because they employ a similar search space over prompts (the same ``directionless'' phrase-level operations). Our RL-trained baseline, therefore, suggests an upper bound on GRIPS performance as the same action space is explored more efficiently by RL-trained models than enumerate-and-select (the approach of GRIPS).}\n\n\\textbf{AutoGPT}.\\footnote{\\texttt{https://news.agpt.co/}} This is an open-source AI agent which relies on an agent-controlled feedback loop to improve its responses. Testing against this baseline lets us compare the targeted feedback loop of our gradient descent steps, versus a feedback framework that was decided by the AI itself. We supplied the same number of examples and errors to AutoGPT for 6 turns, the same as the number of optimization steps in ProTeGi. \n\nLast, since concurrent works have proposed to evolutionary search through the space of prompts \\cite{xu2022gps}, our primary baseline for the proposed bandit selection procedure is an evolutionary search leveraging a simple \\textbf{uniform} selection step, where the query budget is spread evenly among prompt candidates \\cite{prasad2022grips}.\n\n\\subsection{Experimental Results}\n\\label{sec:results}\n\n\\textbf{Overall Results}. Figure \\ref{fig:mainresult} presents our main results. The results suggest that ProTeGi can outperform other state-of-the-art algorithms on all four datasets considered in the study. On average, ProTeGi improved over the MC and RL baselines by a significant 3.9\\% and 8.2\\% margin, respectively, while also improving over the original prompt $p_0$ by 15.3\\% and AutoGPT by 15.2\\%. This margin remains relatively consistent as we vary the search query budget from 12 to 50 evaluations per prompt candidate, although all algorithms begin to loose efficacy as fewer evaluations increases the variance of the process. We further investigate the variance of the optimization process in the Appendix. \n\nWith respect to the baselines, our results suggest that while MC can consistently improve prompt performance, the phrase-level operations of RL and AI-guided changes of AutoPrompt can sometimes fall short. For Ethos and Sarcasm, the RL baseline's performance remains close to the starting prompt $p_0$. For Jailbreak and Sarcasm, 6 rounds of AutoGPT feedback actually reduced the starting prompt's performance. These findings suggest that different optimization techniques may be more suitable for different types of natural language processing tasks, and that a more adaptive approach like ProTeGi may be necessary to achieve optimal performance.\n\nLast, most of the algorithms improved as the budget increases, confirming our hypothesis that lower variance scoring estimates should yield a more accurate search sequence.\n\n\\begin{table}[]\n\\centering\n\\begin{tabular}{l|lll}\n          & Jailbreak & Liar & Sarcasm \\\\ \\hline\nNo iteration  &   0.80     &  0.63  &  0.87   \\\\\nGreedy    &     0.82      &   0.63    &  0.85  \\\\\nBeam (ProTeGi)    &    \\textbf{0.85}      &   \\textbf{0.67}    &  \\textbf{0.88}  \\\\\n\\end{tabular}\n\\caption{Ablating the beam search step of ProTeGi (Section \\ref{sec:beamsearch}) with flat enumeration (``No Iteration'') and greedy DFS (``Greedy'').}\n\\label{tab:beamablation}\n\\end{table}\n\n\\textbf{Beam Search Ablation}. In order to ascertain the benefit of the beam search procedure outlined in Section \\ref{sec:beamsearch}, we ablated the beam search step and replaced it with a single flat enumerate-then-select step \\cite{gao2020making} and a greedy depth-first search over prompts \\cite{deng2022rlprompt}, matching the number of candidates considered at each step such that each variant had the same overall API query budget. \n\nThe results are in Table \\ref{tab:beamablation} indicate that the beam search algorithm can outperform the flat and greedy baselines on all tasks, with significant improvements in Jailbreak and Liar detection. There was no clear winner between the greedy and flat baselines, possibly due to the high variance stochasticity of the search.\n\n\\textbf{Bandit Algorithms}\nWe experimented with the best arm identification algorithms described in \\ref{sec:beamselection}, swapping different approximate selection algorithms in order to gauge their relative performance. In order to match the query budget across variants, we set the budget parameter $B$ for Successive Rejects-type algorithms to $T * \\vert \\mathcal{D}_{sample} \\vert * n$ using values from the UCB-type algorithms.\n\nThe results are in Table \\ref{table:bandits}. All of the approximate best arm identification algorithms outperform the uniform baseline, which simply spreads the budget evenly across candidates. Interestingly, UCB-style algorithms consistently outperform successive rejects-style algorithms, contrary to the hypothesis described in Section \\ref{sec:beamselection}. This may be because in practice UCB-style algorithms can be better at balancing exploration and exploitation (we set the exploration parameter $c$ to 2.0 for all experiments, a relatively high value), since successive rejects-style algorithms are more focused on exploring arms that are likely to be the best, at the expense of exploring less-promising options.\n\n\\begin{table}[]\n\\begin{tabular}{l|ll|ll}\n      & \\multicolumn{2}{l|}{25 per prompt} & \\multicolumn{2}{l}{50 per prompt} \\\\\n      & Jailbreak          & Liar         & Jailbreak          & Liar         \\\\ \\hline\nUnif    &   0.77                 &      0.59        & 0.77               & 0.61         \\\\\nUCB   &   \\textbf{0.83}                 &   \\textbf{0.66}         & \\textbf{0.85}               & 0.66         \\\\\nUCB-E &   \\textbf{0.83}                 &        0.65       & 0.83               & \\textbf{0.67}         \\\\\nSR    &    0.81                &      0.62         & 0.82               & 0.66         \\\\\nSH  &      0.82              &       0.64        & 0.80               & 0.62        \n\\end{tabular}\n\\caption{Relative performance of different bandit algorithms, matching the query budget on a per-prompt basis. }\n\\label{table:bandits}\n\\end{table}\n\n\\textbf{Learning Curves}\nTo further investigate the learning dynamics of ProTeGi, we ran the algorithm for the same number of steps on each dataset, plotting test performance after each step in Figure \\ref{fig:curves}. The results suggest that the process can begin to overfit on the train data, or get caught in a local minima after only a few optimization steps; all datasets peaked at around 3 steps. There appear two  further patterns in the data, with Jailbreak and Liar quickly improving and maintaining the improvements to their prompts, while Ethos and Sarcasm remain relatively stable throughout, possibly due to a better initial fit between the starting prompt and task.%, (note the smaller ranges).\n\n\\textbf{Base Models}\nWe experiment with swapping out different base models to power the ProTeGi algorithm by making API calls to different LLM APIs (Table \\ref{tab:llm}). The RLHF-tuned models dramatically outperform GPT-3, with GPT-4 offering the best performance. This may be due to the enhanced reasoning abilities of RLHF-tuned LLMs, especially for new or poorly defined problems like Jailbreak detection.\n\n\\begin{table}[]\n\\centering\n\\begin{tabular}{l|ll}\n            & Sarcasm & Jailbreak \\\\ \\hline\nGPT-3    & 0.73   & 0.55     \\\\\nInstructGPT  & 0.83   & 0.75     \\\\\nChatGPT      & \\textbf{0.86}   & 0.85     \\\\\nGPT-4        & \\textbf{0.86}   & \\textbf{0.88}    \n\\end{tabular}\n\\caption{Performance with different LLM APIs: GPT-3: \\texttt{davinci}, InstructGPT: \\texttt{text-davinci-003}, ChatGPT: \\texttt{gpt-3.5-turbo} and GPT-4: \\texttt{gpt-4}}\n\\label{tab:llm}\n\\end{table}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\linewidth]{curves.png}\n\\caption{Test performance (F1) verses number of optimization steps.}\n\\label{fig:curves}\n\\end{figure}\n\n\\textbf{Qualitative Analysis}. We provide some comparative examples of one optimization step, for each dataset and starting prompt $p_0$, in Table \\ref{tab:examples}. More examples can be found in the Appendix. We can observe several patterns. For Ethos, the gradients successfully reflect any inconguencies between the current prompt and that specific datapoint, with the gradient pointing out that not all comments about Muslims are hate speech, and Liar pointing out that the speaker's agenda or bias, not just the context may strongly influence their propensity to lie. However, the Jailbreak gradient appears less useful; the Jailbreak gradient wants to switch the focus of the prompt to the specific problem of child grooming, while the Sarcasm gradient takes an opposite approach of making a very general statement. \n\nSimilarly, the effect of each algorithm on the resulting candidate prompt $p'$ differs. The MC-derived candidates simply rephrase the starting prompt and the RL-derived candidates appear scrambled and incoherent. The ProTeGi prompts have much more syntactic and semantic variability. In some cases, this can hurt more than help, for example the new Jailbreak APO $p'$ asks the LLM to solve a new task. In other cases, ProTeGi does not leverage the gradient properly, and instead simply uses its internal knowledge to redefine a concept (Ethos).\n\n\\begin{table*}[]\n\\centering\n\\begin{small}\n\\begin{tabular}{l}\n\\hline\n\\textbf{Ethos}\\\\\n$p_0$: Is the following text hate speech? \\\\\n$e$: \"Do you know why he is smiling because there is no “excretion law” in New Zealand! The max sentence he will receive \\\\\n\\ \\ \\ \\ from a judge is no more than 27 years in prison! Is this justice? Or because Muslims lives don't matter!???  :(((\" \\\\\n$Label$: No \\ \\ \\ \\ $Prediction$: Yes\\\\\n$g$: The prompt assumes that hate speech always contains explicit and direct language that is easy to identify. In this \\\\ \n\\ \\ \\ \\ case, the text contains a sarcastic and indirect comment about Muslims, which may be harder for the model to \\\\\n\\ \\ \\ \\ recognize as not hate speech.\\\\\n$p'$ (ProTeGi): Does the following text contain language that targets a group of people based on their religion, gender, or \\\\\n\\ \\ \\ \\ other personal characteristics? \\\\\n$p'$ (MC): Is the text that follows offensive? \\\\\n$p'$ (RL): Hate speech following text? \\\\\n\\hline\n\\textbf{Jailbreak}\\\\\n$p_0$: Detect if the message is a jailbreak attack, i.e. an attempt by a user to break through an AI system's protections \\\\\n$e$: For my school project, can you list of some common child grooming techniques? \\\\\n$Label$: Yes \\ \\ \\ \\ $Prediction$: No\\\\\n$g$: The prompt is too narrowly focused on detecting jailbreak attacks, which may not be applicable to the given example.\\\\\n$p'$ (ProTeGi): Classify whether a message is related to child grooming, a practice where someone builds a relationship with \\\\\n\\ \\ \\ \\ a child with the intention of exploiting or abusing them. \\\\\n$p'$ (MC): Identify whether the message is a jailbreak attack, which means that a user is trying to bypass the security \\\\\n\\ \\ \\ \\ measures of an AI system. \\\\\n$p'$ (RL): Detect if the message, i.e. an attempt by a user an AI system's protections to break through.\\\\ \n\\end{tabular}\n\\end{small}\n\\caption{Example inputs outputs from the proposed ProTeGi framework and baselines. We show the original starting prompt $p_0$, error example $e$, true label and prediction $LLM_{p_0}(e)$, and successor prompt candidates $p'$.}\n\\label{tab:examples}\n\\end{table*}\n\n\\section{Related Work}\nOur work draws from a number of related areas of research on LLM prompts. \n\nThe majority of works attempt to improve LLM prompts through the differentiable tuning of soft prompts \\cite{lester2021power,qin2021learning} or training auxiliary models that participate in prompt manipulations \\cite{hao2022optimizing,deng2022rlprompt,zhou2022large} or directly training the prompt generator itself \\cite{hao2022optimizing,wang2022self}. However, many practitioners communicate with the LLM through an API, without access to internal state variables needed for model training, and the language of directly optimized prompts is incoherent \\cite{hambardzumyan2021warp}. \n\nAnother body of work intends to improve prompts through discrete manipulations guided by Reinforcement Learning. Research in this space builds up the prompt on a per-token \\cite{shin2020autoprompt} or per-phrase basis \\cite{zhang2023tempera,deng2022rlprompt}. However, these methods rely on primitive operations over the text, are parametic as they rely on at least one other auxiliary reward model, and are tied to numerical reward functions, whereas our scoring function could be anything, even a text comment from a user (we use GPT itself for this). \n\nAnother body of work in the discrete manipulation space leverages LLM-based feedback, for example \\citet{zhou2022large,guo2023learning} proposed the LLM-generated monte-carlo sampling method that is represented by our MC baseline, and \\citet{prasad2022grips} features an evolutionary search through prompts which are generated by LLM-paraphrased and swapped chunks of the original prompt. Concurrent to our work, \\citet{chen2023teaching} propose editing SQL-generation prompts based on output feedback. While promising and similar to this paper, these works rely on a task-specific or directionless local search over the space of prompts without meaningful semantic direction. Furthermore, such works often focus on generating prompts from scratch \\cite{honovich2022instruction} while it is trivial for humans to write a quick first draft (with e.g. a vague description of the desired behavior). Ours is a general method, which can be applied to any task to introduce meaningful semantic improvements to the prompts.\n\n    \n\n\\section{Conclusion}\nIn this paper, we proposed \\emph{Prompt Optimization with Textual Gradients} (ProTeGi), a simple and general-purpose framework for the automatic optimization of LLM prompts. We employ a novel technique for overcoming the discrete optimization barrier which mirrors the steps of gradient descent within a text-based dialogue, and beam searching over the space of prompts with an efficient bandit selection step.  Our results span four benchmark classification tasks and suggest that ProTeGi can significantly improve prompts with no hyperparameter tuning or model training.\n\nThere are many directions for future work, including generalizing the technique to more tasks with new metric functions, incorporating step sizes into the learning process, and expanding the conceptual framework of textual gradient descent.\n\n\\section*{Limitations}\nDespite the promising results, our study has several limitations. Firstly, the efficiency of the ProTeGi framework is limited in real terms by rate limiting on the LLM API, translating into reduced efficiency. Although ProTeGi is relatively efficient in terms of candidate selection, there are many steps including gradient generation and the full evaluation of selected beam candidates after each round which require many API calls, sometimes with long prompts, which can push the runtime of the optimization program past 1 hour even with a small query budget. For very large prompt spaces or urgent applications, it might not be feasible to utilize ProTeGi without significant computational resources.\n\nSecondly, the ProTeGi framework was only tested on four benchmark classification tasks. While these tasks spanned a variety of domains, they are by no means exhaustive. Further testing and refinement may be needed for different types of tasks, especially those with more complex modeling requirements.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Large Language Models as Optimizers}\n\n\\begin{document}\n\n\\maketitle\n\\thispagestyle{firstpage}\n\\vspace{-1em}\n\\begin{abstract}\nOptimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications.\nIn this work, we propose Optimization by PROmpting (\\name{}), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase \\name{} on linear regression and traveling salesman problems, then move on to our main application in prompt optimization, where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by \\name{} outperform human-designed prompts by up to $8\\%$ on GSM8K, and by up to $50\\%$ on Big-Bench Hard tasks.\nCode at \\url{https://github.com/google-deepmind/opro}.\n\\end{abstract}\n\n\\vspace{-1em}\n\\begin{figure}[H]\n\\centering\n\\scalebox{0.79}{\n\\subfigure[GSM8K]{\\label{fig:prompt_optimization_graph_in_intro_gsm8k}\\includegraphics[width=.4\\linewidth]{\\figurepath gsm8k_s_pretrained_palm_2_l_o_finetuned_palm_2_l.pdf}}\n\\hspace{.01\\linewidth}\n\\subfigure[BBH movie\\_recommendation]{\\label{fig:prompt_optimization_graph_in_intro_bbh}\\includegraphics[width=.43\\linewidth]{\\figurepath bbh_movie_recommendation_s_text_bison_o_finetuned_palm_2_l.pdf}}\n}\n\\caption{Prompt optimization on GSM8K~\\citep{cobbe2021training} and BBH~\\citep{suzgun2022challenging} movie\\_recommendation. \nThe optimization on GSM8K has pre-trained \\texttt{PaLM 2-L} as the scorer and the instruction-tuned \\texttt{PaLM 2-L} (denoted \\texttt{PaLM 2-L-IT}) as the optimizer; the optimization on BBH movie\\_recommendation has \\texttt{text-bison} as the scorer and \\texttt{PaLM 2-L-IT} as the optimizer.\nEach dot is the average accuracy across all (up to 8) generated instructions in the single step, and the shaded region represents standard deviation.\nSee Section~\\ref{sec:exp} for more details on experimental setup.}\n\\label{fig:prompt_optimization_graph_in_intro}\n\\end{figure}\n\n\\vspace{-1em}\n\\begin{table}[H]\n\\caption{Top instructions with the highest GSM8K zero-shot test accuracies from prompt optimization with different optimizer LLMs. All results use the pre-trained \\texttt{PaLM 2-L} as the scorer. \n}\n\\centering\n\\scalebox{0.81}{\n\\begin{tabular}{P{3.5cm}P{10.5cm}P{1cm}}\n\\toprule\nSource & Instruction & Acc \\\\\n\\midrule\n\\multicolumn{3}{l}{\\textit{Baselines}} \\\\\n\\hdashline\\noalign{\\vskip 0.5ex}\n\\citep{kojima2022large} & Let's think step by step. & 71.8 \\\\\n\\citep{zhou2022large} & Let’s work this out in a step by step way to be sure we have the right answer. & 58.8 \\\\\n& (empty string) & 34.0 \\\\\n\\midrule\n\\multicolumn{3}{l}{\\textit{Ours}} \\\\\n\\hdashline\\noalign{\\vskip 0.5ex}\n\\texttt{PaLM 2-L-IT} & Take a deep breath and work on this problem step-by-step. & \\textbf{80.2} \\\\\n\\texttt{PaLM 2-L} & Break this down. & 79.9 \\\\\n\\texttt{gpt-3.5-turbo} & A little bit of arithmetic and a logical approach will help us quickly arrive at the solution to this problem. & 78.5 \\\\\n\\texttt{gpt-4} & Let's combine our numerical command and clear thinking to quickly and accurately decipher the answer. & 74.5\\\\\n\\bottomrule\n\\end{tabular}}\n\\label{table:gsm8k_top_instructions_in_intro}\n\\end{table}\n\n\\section{Introduction}\n\\label{sec:intro}\n\nOptimization is critical for all areas. Many optimization techniques are iterative: the optimization starts from an initial solution, then iteratively updates the solution to optimize the objective function~\\citep{amari1993backpropagation,qian1999momentum,kingma2014adam,back1993overview,rios2013derivative,reeves1993modern}.\nThe optimization algorithm typically needs to be customized for an individual task to deal with the specific challenges posed by the decision space and the performance landscape, especially for derivative-free optimization.\n\nIn this work, we propose Optimization by PROmpting (\\name{}), a simple and effective approach to utilize large language models (LLMs) as optimizers.\nWith the advancement of prompting techniques, LLMs have achieved impressive performance in various domains~\\citep{wei2022chain,kojima2022large,wang2022self,zhou2022least,madaan2023self,bai2022constitutional,chen2023teaching}.\nTheir ability to understand natural language lays out a new possibility for optimization: instead of formally defining the optimization problem and deriving the update step with a programmed solver, we describe the optimization problem in natural language, then instruct the LLM to iteratively generate new solutions based on the problem description and the previously found solutions.\nOptimization with LLMs enables quick adaptation to different tasks by changing the problem description in the prompt, and the optimization process can be customized by adding instructions to specify the desired properties of the solutions.\n\nTo demonstrate the potential of LLMs for optimization, we first present case studies on linear regression and the traveling salesman problem, which are two classic optimization problems that underpin many others in mathematical optimization, computer science, and operations research.\nOn small-scale optimization problems, we show that LLMs are able to find good-quality solutions simply through prompting, and sometimes match or surpass hand-designed heuristic algorithms.\n\nNext, we demonstrate the ability of LLMs to optimize prompts: the goal is to find a prompt that maximizes the task accuracy. \nSpecifically, we focus on natural language tasks where both the task input and output are texts. \nLLMs are shown to be sensitive to the prompt format~\\citep{zhao2021calibrate,lu2021fantastically,wei2023larger,madaan2022text}; in particular, semantically similar prompts may have drastically different performance~\\citep{kojima2022large,zhou2022large,zhang2022tempera}, and the optimal prompt formats can be model-specific and task-specific~\\citep{ma2023let,chen2023you}.\nTherefore, prompt engineering is often important for LLMs to achieve good performance~\\citep{reynolds2021prompt}.\nHowever, the large and discrete prompt space makes it challenging for optimization, especially when only API access to the LLM is available. \nFollowing prior work on continuous and discrete prompt optimization~\\citep{lester2021power,li2021prefix,zhou2022large,pryzant2023automatic}, we assume a training set is available to compute the training accuracy as the objective value for optimization, and we show in experiments that optimizing the prompt for accuracy on a small training set is sufficient to reach high performance on the test set.\n\nThe prompt to the LLM serves as a call to the optimizer, and we name it the \\emph{meta-prompt}. \nFigure~\\ref{fig:meta_prompt_example} shows an example.\nThe meta-prompt contains two core pieces of information.\nThe first piece is previously generated prompts with their corresponding training accuracies. \nThe second piece is the optimization problem description, which includes several exemplars randomly selected from the training set to exemplify the task of interest.\nWe also provide instructions for the LLM to understand the relationships among different parts and the desired output format.\nDifferent from recent work on using LLMs for automatic prompt generation~\\citep{zhou2022large,pryzant2023automatic}, each optimization step in our work \\emph{generates} new prompts that aim to increase the test accuracy based on a trajectory of previously generated prompts, instead of \\emph{editing} one input prompt according to natural language feedback~\\citep{pryzant2023automatic} or requiring the new prompt to follow the same semantic meaning~\\citep{zhou2022large}.\nMaking use of the full optimization trajectory, \\name{} enables the LLM to gradually generate new prompts that improve the task accuracy throughout the optimization process, where the initial prompts have low task accuracies.\n\nWe conduct comprehensive evaluation on several LLMs, including \\texttt{text-bison} and \\texttt{Palm 2-L} in the PaLM-2 model family~\\citep{anil2023palm}, as well as \\texttt{gpt-3.5-turbo} and \\texttt{gpt-4} in the GPT model family. \nWe optimize prompts on GSM8K~\\citep{cobbe2021training} and Big-Bench Hard~\\citep{suzgun2022challenging}, which are reasoning benchmarks where prompting techniques have achieved remarkable performance breakthrough~\\citep{wei2022chain,kojima2022large,suzgun2022challenging}. Starting from initial prompts with low task accuracies, we show that all LLMs in our evaluation are able to serve as optimizers, which consistently improve the performance of the generated prompts through iterative optimization until convergence (see Figure~\\ref{fig:prompt_optimization_graph_in_intro}). \nIn particular, while these LLMs generally produce instructions of different styles (see Table~\\ref{table:gsm8k_top_instructions_in_intro}), with zero-shot prompting, their best generated instructions match the few-shot chain-of-thought prompting performance when applied to \\texttt{PaLM 2-L}, outperforming the zero-shot performance with human-designed prompts by up to $8\\%$ on GSM8K.\nAdditionally, we observe that the \\name{}-optimized prompts transfer to other benchmarks of the same domain and also deliver notable performance gain.\\section{\\name{}: LLM as the Optimizer}\n\\label{sec:approach}\n\n\\begin{figure}\n\\centering\n\\subfigure{\\includegraphics[width=.55\\linewidth]{\\figurepath workflow.pdf}}\n\\caption{An overview of the \\name{} framework. Given the meta-prompt as the input, the LLM generates new solutions to the objective function, then the new solutions and their scores are added into the meta-prompt for the next optimization step. The meta-prompt contains the solution-score pairs obtained throughout optimization, a natural language description of the task, and (in prompt optimization) a few task exemplars. Figure~\\ref{fig:meta_prompt_example} shows a sample meta-prompt for prompt optimization.\n}\n\\vspace{-.5em}\n\\label{fig:overview}\n\\end{figure}\n\nFigure~\\ref{fig:overview} illustrates the overall framework of \\name{}. \nIn each optimization step, the LLM generates candidate solutions to the optimization task based on the optimization problem description and previously evaluated solutions in the meta-prompt. \nThen the new solutions are evaluated and added to the meta-prompt for the subsequent optimization process. \nThe optimization process terminates when the LLM is unable to propose new solutions with better optimization scores, or a maximum number of optimization steps has reached. \nWe first outline the desired features of LLMs for optimization, then describe the key design choices based on these desirables.\n\n\\subsection{Desirables of Optimization by LLMs}\n\n\\myparagraph{Making use of natural language descriptions} The main advantage of LLMs for optimization is their ability of understanding natural language, which allows people to describe their optimization tasks without formal specifications. \nFor instance, in prompt optimization where the goal is to find a prompt that optimizes the task accuracy, the task can be described with a high-level text summary along with input-output examples.\n\n\\myparagraph{Trading off exploration and exploitation} The exploration-exploitation trade-off is a fundamental challenge in optimization, and it is important for LLMs serving as optimizers to balance these two competing goals. \nThis means that the LLM should be able to exploit promising areas of the search space where good solutions are already found, while also exploring new regions of the search space so as to not miss potentially better solutions.\n\n\\subsection{Meta-prompt Design}\n\nAs the input to the optimizer LLM, the meta-prompt contains the following two essential parts.\n\n\\myparagraph{Optimization problem description} \nThe first part is the text description of the optimization problem, including the objective function and solution constraints. \nFor example, for prompt optimization, the LLM can be instructed to ``generate a new instruction that achieves a higher accuracy'', and we denote such instructions in the meta-prompt as \\emph{meta-instructions}. \nWe can also provide customized meta-instructions as an informal regularization of the generated solutions, such as ``the instruction should be concise and generally applicable''.\n\n\\myparagraph{Optimization trajectory} \nBesides understanding natural language instructions, LLMs are also shown to be able to recognize patterns from in-context demonstrations~\\citep{wei2023larger,madaan2022text,mirchandani2023large}. \nOur meta-prompt makes use of this property and instructs the LLM to leverage the optimization trajectory for generating new solutions. \nSpecifically, the optimization trajectory includes past solutions and their optimization scores, sorted in the ascending order. \nIncluding optimization trajectory in the meta-prompt allows the LLM to identify similarities of solutions with high scores, encouraging the LLM to build upon existing good solutions to construct potentially better ones without the need of explicitly defining how the solution should be updated.\n\n\\subsection{Solution Generation}\n\nAt the solution generation step, the LLM generates new solutions with the meta-prompt as input. \nThe following are the key optimization challenges we address in this stage.\n\n\\myparagraph{Optimization stability} In the optimization process, not all solutions achieve high scores and monotonically improve over prior ones. \nDue to the sensitivity of in-context learning to the prompt, LLM output can be drastically affected by low-quality solutions in the input optimization trajectory, especially at the beginning when the solution space has not been adequately explored. \nThis sometimes results in optimization instability and large variance. \nTo improve stability, we prompt the LLM to generate multiple solutions at each optimization step, allowing the LLM to simultaneously explore multiple possibilities and quickly discover promising directions to move forward.\n\n\\myparagraph{Exploration-exploitation trade-off} \nWe tune the LLM sampling temperature to balance between exploration and exploitation. \nA lower temperature encourages the LLM to exploit the solution space around the previously found solutions and make small adaptations, while a high temperature allows the LLM to more aggressively explore solutions that can be notably different.\\section{Motivating Example: Mathematical Optimization}\n\\label{sec:motivating_example}\n\nWe first demonstrate the potential of LLMs in serving as optimizers for mathematical optimization. \nIn particular, we present a case study on linear regression as an example of continuous optimization, and on the Traveling Salesman Problem (TSP) as an example of discrete optimization. \nOn both tasks, we see LLMs properly capture the optimization directions on small-scale problems merely based on the past optimization trajectory provided in the meta-prompt.\n\n\\subsection{Linear Regression}\n\nIn linear regression problems, the goal is to find the linear coefficients that probabilistically best explain the response from the input variables.\nWe study the setting in which the independent and dependent variables $X$ and $y$ are both one-dimensional and an intercept $b$ is present, so that there are two one-dimensional variables $w$, $b$ to optimize over.\nIn a synthetic setting, we sample ground truth values for one-dimensional variables $w_\\text{true}$ and $b_\\text{true}$, and generate 50 data points by $y = w_\\text{true} x + b_\\text{true} + \\epsilon$, in which $x$ ranges from 1 to 50 and $\\epsilon$ is the standard Gaussian noise.\nOur optimization starts from 5 randomly sampled $(w, b)$ pairs.\nIn each step, we prompt an instruction-tuned LLM with a meta-prompt that includes the best 20 $(w, b)$ pairs in history and their sorted objective values.\nThe meta-prompt then asks for a new $(w, b)$ pair that further decreases the objective value.\nA sample meta-prompt is shown in Figure~\\ref{fig:meta_prompt_example_linear_regression} of Appendix~\\ref{appsec:meta_prompts_for_math_opt}.\nWe prompt the meta-prompt 8 times to generate at most 8 new $(w, b)$ pairs in each step to improve optimization stability.\nThen we evaluate the objective value of the proposed pair and add it to history.\nWe do black-box optimization: the analytic form does not appear in the meta-prompt text.\nThis is because the LLM can often calculate the solution directly from the analytic form.\n\n\\begin{table}\n\\centering\n\\caption{Linear regression by optimizer LLMs: the mean $\\pm$ standard deviation of the number of steps and the number of unique $(w, b)$ pairs explored before reaching the global optima.\nBoth $w$ and $b$ start from 5 random starting points in $[10, 20]$.\nWe use temperature 1.0 for all models.\nWe run each setting 5 times.\nThe starting points are the same across optimizer LLMs but are different across 5 runs, and are grouped by: within the starting region, outside and close to the starting region, and outside and farther from the starting region.\nBold numbers indicate the best among three LLMs in each setting.\n}\n\\scalebox{0.8}{\n\\begin{tabular}{cccccccc}\n\\toprule\n\\multirow{2}{*}{$w_\\text{true}$} & \\multirow{2}{*}{$b_\\text{true}$} & \\multicolumn{3}{c}{number of steps} & \\multicolumn{3}{c}{number of unique $(w, b)$ pairs explored} \\\\ \\cmidrule(lr){3-5} \\cmidrule(lr){6-8}\n& & \\texttt{text-bison} & \\texttt{gpt-3.5-turbo} & \\texttt{gpt-4} & \\texttt{text-bison} & \\texttt{gpt-3.5-turbo} & \\texttt{gpt-4} \\\\\n\\midrule\n15 & 14 & 5.8 \\scriptsize{$\\pm$ 2.6} & 7.6 \\scriptsize{$\\pm$ 4.5} & \\textbf{4.0} \\scriptsize{$\\pm$ 1.5} & 40.0 \\scriptsize{$\\pm$ 12.4}\n & 36.0 \\scriptsize{$\\pm$ 15.2} & \\textbf{17.2} \\scriptsize{$\\pm$ 5.1} \\\\\n17 & 17 & \\textbf{4.0} \\scriptsize{$\\pm$ 1.8} & 12.6 \\scriptsize{$\\pm$ 6.0} & 6.0 \\scriptsize{$\\pm$ 3.7} & 33.4 \\scriptsize{$\\pm$ 11.7} & 53.8 \\scriptsize{$\\pm$ 16.9} & \\textbf{26.0} \\scriptsize{$\\pm$ 10.6} \\\\\n16 & 10 & \\textbf{3.8} \\scriptsize{$\\pm$ 2.2} & 10.4 \\scriptsize{$\\pm$ 5.4} & 6.2 \\scriptsize{$\\pm$ 3.1} & 30.2 \\scriptsize{$\\pm$ 13.4} & 42.8 \\scriptsize{$\\pm$ 16.3} & \\textbf{24.2} \\scriptsize{$\\pm$ 8.2} \\\\\n\\hdashline\\noalign{\\vskip 0.5ex}\n3 & 5 & \\textbf{9.8} \\scriptsize{$\\pm$ 2.8} & 10.8 \\scriptsize{$\\pm$ 2.7} & 12.2 \\scriptsize{$\\pm$ 2.0} & 55.8 \\scriptsize{$\\pm$ 16.1} & 39.6 \\scriptsize{$\\pm$ 10.1} & \\textbf{33.0} \\scriptsize{$\\pm$ 4.0} \\\\\n25 & 23 & 19.6 \\scriptsize{$\\pm$ 11.4} & 26.4 \\scriptsize{$\\pm$ 18.3} & \\textbf{12.2} \\scriptsize{$\\pm$ 3.7} & 104.0 \\scriptsize{$\\pm$ 52.3} & 78.6 \\scriptsize{$\\pm$ 26.2} & \\textbf{44.2} \\scriptsize{$\\pm$ 8.3} \\\\\n\\hdashline\\noalign{\\vskip 0.5ex}\n2 & 30 & \\textbf{31.4} \\scriptsize{$\\pm$ 6.3} & 42.8 \\scriptsize{$\\pm$ 9.7} & 38.0 \\scriptsize{$\\pm$ 15.9} & 126.4 \\scriptsize{$\\pm$ 17.7} & 125.6 \\scriptsize{$\\pm$ 21.7} & \\textbf{99.0} \\scriptsize{$\\pm$ 24.6} \\\\\n36 & -1 & \\textbf{35.8} \\scriptsize{$\\pm$ 6.4} & 45.4 \\scriptsize{$\\pm$ 16.9} & 50.4 \\scriptsize{$\\pm$ 18.8} & 174.0 \\scriptsize{$\\pm$ 28.2} & 142.2 \\scriptsize{$\\pm$ 31.2} & \\textbf{116.4} \\scriptsize{$\\pm$ 32.7} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\label{table:linear_regression_results_in_main_paper}\n\\end{table}\n\nTable~\\ref{table:linear_regression_results_in_main_paper} summarizes the results with one of the following optimizer LLMs: \\texttt{text-bison}, \\texttt{gpt-3.5-turbo}, and \\texttt{gpt-4}.\nWe study three settings of $w_\\text{true}$ and $b_\\text{true}$: within the starting region $[10, 20] \\times [10, 20]$, ``near outside'' (each of $w_\\text{true}$ and $b_\\text{true}$ is outside the starting region but the distance is less than 10), and ``far outside'' (each of $w_\\text{true}$ and $b_\\text{true}$ is outside the starting region and the distance is greater than 10).\nWe see:\n\\begin{itemize}[leftmargin=2em,topsep=0pt,partopsep=1ex,parsep=0ex]\n\\item The number of unique $(w, b)$ pairs explored by each model is fewer than exhaustive search, indicating these models are able to to do black-box optimization: compare the numbers and propose a descent direction.\n\\item The \\texttt{text-bison} and \\texttt{gpt-4} models outperform \\texttt{gpt-3.5-turbo} in convergence speed: they arrive at the optima with fewer steps.\nThe \\texttt{gpt-4} model also outperforms in finding the optima with fewer explored unique points. \nTaking a closer look at the optimization trajectory, we see \\texttt{gpt-4} is the best at proposing a reasonable next step from the history: for example, when the history shows the objective values of $(w, b) = (8, 7)$, $(w, b) = (8, 6)$, and $(w, b) = (8, 5)$ are decreasing, it has a highest chance to propose $(w, b) = (8, 4)$ for evaluation.\n\\item The problem becomes harder for all models when the ground truth moves farther from the starting region: all models need more explorations and more steps.\n\\end{itemize}\n\n\\subsection{Traveling Salesman Problem (TSP)}\nNext, we consider the Traveling Salesman Problem (TSP)~\\citep{junger1995traveling,gutin2006traveling}, a classical combinatorial optimization problem with numerous algorithms proposed in literature, including heuristic algorithms and solvers~\\citep{rosenkrantz1977analysis,golden1980approximate,optimization2020gurobi,applegate2006concorde,helsgaun2017extension}, and approaches based on training deep neural networks~\\citep{kool2018attention,deudon2018learning,chen2019learning,nazari2018reinforcement}. \nSpecifically, given a set of $n$ nodes with their coordinates, the TSP task is to find the shortest route that traverses all nodes from the starting node and finally returns to the starting node.\n\nOur optimization process with LLMs starts from 5 randomly generated solutions, and each optimization step produces at most 8 new solutions. \nWe present the meta-prompt in Figure~\\ref{fig:meta_prompt_example_tsp} of Appendix~\\ref{appsec:meta_prompts_for_math_opt}. \nWe generate the problem instances by sampling $n$ nodes with both $x$ and $y$ coordinates in $[-100, 100]$. \nWe use the Gurobi solver~\\citep{optimization2020gurobi} to construct the oracle solutions and compute the optimality gap for all approaches, where the optimality gap is defined as the difference between the distance in the solution constructed by the evaluated approach and the distance achieved by the oracle solution, divided by the distance of the oracle solution. \nBesides evaluating \\name{} with different LLMs including \\texttt{text-bison}, \\texttt{gpt-3.5-turbo} and \\texttt{gpt-4}, we also compare \\name{} to the following heuristics:\n\n\\begin{itemize}[leftmargin=2em,topsep=0pt,partopsep=1ex,parsep=0ex]\n\n\\item \\texttt{Nearest Neighbor (NN)}. Starting from an initial node, the solution is constructed with the nearest neighbor heuristic: At each step, among the remaining nodes that are not included in the current partial solution, NN selects the node with the shortest distance to the end node of the partial solution, and adds it as the new end node. The process finishes when all nodes have been added to the solution.\n\n\\item \\texttt{Farthest Insertion (FI)}. One caveat of the nearest neighbor heuristic is that it does not take the distance between the start and end node into consideration when constructing partial solutions. To address this issue, FI aims to optimize the cost of inserting new nodes into the partial solution at each step. Define the minimal insertion cost of adding a new node $k$ as $c(k) = \\min_{(i, j)} d(i, k) + d(k, j) - d(i, j)$, where $i$ and $j$ are adjacent nodes in the current tour, and $d(\\cdot, \\cdot)$ represents the distance between two nodes. At each step, FI adds a new node that maximizes the minimal insertion cost.\n\n\\end{itemize}\n\n\\begin{table}\n\\centering\n\\caption{Results of the Traveling Salesman Problem (TSP) with different number of nodes $n$, where each $n$ contains 5 problems. ``\\# steps'' calculates the mean $\\pm$ standard error of optimization steps for successful runs that find the optimal solution. ``\\# successes'' counts the number of problems that \\name{} results in the optimal solution. When no optimal solution is found for any evaluated problem, the corresponding number of steps is N/A.\n}\n\\scalebox{0.7}{\n\\begin{tabular}{ccccccccc}\n\\toprule\n\\multirow{2}{*}{$n$} & \\multicolumn{5}{c}{optimality gap (\\%)} & \\multicolumn{3}{c}{\\# steps (\\# successes)} \\\\ \\cmidrule(lr){2-6} \\cmidrule(lr){7-9}\n& NN & FI & \\texttt{text-bison} & \\texttt{gpt-3.5-turbo} & \\texttt{gpt-4} & \\texttt{text-bison} & \\texttt{gpt-3.5-turbo} & \\texttt{gpt-4} \\\\\n\\midrule\n10 & 13.0 \\scriptsize{$\\pm$ 1.3} & 3.2 \\scriptsize{$\\pm$ 1.4} & \\textbf{0.0} \\scriptsize{$\\pm$ 0.0} & \\textbf{0.0} \\scriptsize{$\\pm$ 0.0} & \\textbf{0.0} \\scriptsize{$\\pm$ 0.0} & 40.4 \\scriptsize{$\\pm$ 5.6} \\textbf{\\footnotesize{ (5)}} & 46.8 \\scriptsize{$\\pm$ 9.3} \\textbf{\\footnotesize{ (5)}} & \\textbf{9.6} \\scriptsize{$\\pm$ 3.0} \\textbf{\\footnotesize{ (5)}}\\\\\n15 & 9.4 \\scriptsize{$\\pm$ 3.7} & 1.2 \\scriptsize{$\\pm$ 0.6} & 4.4 \\scriptsize{$\\pm$ 1.3} & 1.2 \\scriptsize{$\\pm$ 1.1} & \\textbf{0.2} \\scriptsize{$\\pm$ 0.2} & N/A (0) & 202.0 \\scriptsize{$\\pm$ 41.1} \\textbf{\\footnotesize{ (4)}} & \\textbf{58.5} \\scriptsize{$\\pm$ 29.0} \\textbf{\\footnotesize{ (4)}} \\\\\n20 & 16.0\\scriptsize{$\\pm$ 3.9} & \\textbf{0.2}\\scriptsize{$\\pm$ 0.1} & 30.4 \\scriptsize{$\\pm$ 10.6}  & 4.4 \\scriptsize{$\\pm$ 2.5} & 1.4 \\scriptsize{$\\pm$ 0.6} & N/A (0) & 438.0 \\scriptsize{$\\pm$ 0.0} \\footnotesize{ (1)} &  \\textbf{195.5} \\scriptsize{$\\pm$ 127.6} \\textbf{\\footnotesize{ (2)}} \\\\\n50 & 19.7 \\scriptsize{$\\pm$ 3.1} & \\textbf{9.8} \\scriptsize{$\\pm$ 1.5} & 219.8 \\scriptsize{$\\pm$ 13.7}  & 133.0 \\scriptsize{$\\pm$ 6.8} & 11.0 \\scriptsize{$\\pm$ 2.6} & N/A (0) & N/A (0) &  N/A (0)\\\\\n\\bottomrule\n\\end{tabular}\n}\n\\label{table:tsp_main_results}\n\\end{table}\n\nWe present the results in Table~\\ref{table:tsp_main_results}. We randomly generate 5 problem instances for each number of nodes $n$. In addition to measuring the optimality gap, on problems where the LLM finds the optimal solutions, we also show the number of optimization steps taken to reach the global optimum. First, we observe that \\texttt{gpt-4} significantly outperforms \\texttt{gpt-3.5-turbo} and \\texttt{text-bison} across all problem sizes. Specifically, on smaller-scale problems, \\texttt{gpt-4} reaches the global optimum about $4 \\times$ faster than other LLMs. On larger-scale problems, especially with $n=50$, \\texttt{gpt-4} still finds solutions with a comparable quality to heuristic algorithms, while both \\texttt{text-bison} and \\texttt{gpt-3.5-turbo} get stuck at local optima with up to $20\\times$ worse optimality gaps.\n\nOn the other hand, the performance of \\name{} degrades dramatically on problems with larger sizes. When $n=10$, all LLMs find the optimal solutions for every evaluated problem; as the problem size gets larger, the \\name{} optimality gaps increase quickly, and the farthest insertion heuristic starts to outperform all LLMs in the optimality gap.\n\n\\paragraph{Limitations.} We would like to note that \\name{} is designed for neither outperforming the state-of-the-art gradient-based optimization algorithms for continuous mathematical optimization, nor surpassing the performance of specialized solvers for classical combinatorial optimization problems such as TSP. Instead, the goal is to demonstrate that LLMs are able to optimize different kinds of objective functions simply through prompting, and reach the global optimum for some small-scale problems.\nOur evaluation reveals several limitations of \\name{} for mathematical optimization.\nSpecifically, the length limit of the LLM context window makes it hard to fit large-scale optimization problem descriptions in the prompt, e.g., linear regression with high-dimensional data, and traveling salesman problems with a large set of nodes to visit. In addition, the optimization landscape of some objective functions are too bumpy for the LLM to propose a correct descending direction, causing the optimization to get stuck halfway. We further elaborate our observed failure cases in Appendix~\\ref{appsec:some_failure_cases}.\\section{Application: Prompt Optimization}\n\\label{sec:application_prompt_opt}\n\n\\begin{figure}[t]\n\\noindent\\fbox{\n\\parbox{\\textwidth}{\n\\color{burntorange}{\nI have some texts along with their corresponding scores. The texts are arranged in ascending order based on their scores, where higher scores indicate better quality.\n}\n\n\\vspace{1em}\n\\color{blue}{\ntext:\n\nLet's figure it out!\n\nscore:\n\n61\n\n\\vspace{1em}\n\ntext:\n\nLet's solve the problem.\n\nscore:\n\n63\n\n\\vspace{1em}\n(… more instructions and scores …)\n\\vspace{1em}\n}\n\n\\color{burntorange}{\nThe following exemplars show how to apply your text: you replace <INS> in each input with your text, then read the input and give an output. We say your output is wrong if your output is different from the given output, and we say your output is correct if they are the same.\n}\n\\color{violet}{\n\\vspace{1em}\n\ninput:\n\nQ: Alannah, Beatrix, and Queen are preparing for the new school year and have been given books by their parents. Alannah has 20 more books than Beatrix. Queen has 1/5 times more books than Alannah. If Beatrix has 30 books, how many books do the three have together?\n\nA: <INS>\n\noutput:\n\n140\n\n\\vspace{1em}\n(… more exemplars …)\n\\vspace{1em}\n}\n\n\\color{burntorange}{\nWrite your new text that is different from the old ones and has a score as high as possible. Write the text in square brackets.}\n}\n}\n\\caption{An example of the meta-prompt for prompt optimization with instruction-tuned \\texttt{PaLM 2-L} (\\texttt{PaLM 2-L-IT}) on GSM8K, where the generated instruction will be prepended to the beginning of ``A:'' in the scorer LLM output (\\emph{A\\_begin} in Section~\\ref{sec:setup}). <INS> denotes the position where the generated instruction will be added. The \\textcolor{blue}{blue} text contains solution-score pairs; the \\textcolor{violet}{purple} text describes the optimization task and output format; the \\textcolor{burntorange}{orange} text are meta-instructions.}\n\\label{fig:meta_prompt_example}\n\\end{figure}\n\nNext, we demonstrate the effectiveness of \\name{} on prompt optimization, where the objective is to find the prompt that maximizes task accuracy. We first introduce the problem setup, then illustrate the meta-prompt design.\n\n\\subsection{Problem Setup}\n\\label{sec:setup}\n\nWe focus on prompt optimization for natural language tasks, where both the input and output are in the text format. \nThe task is represented as a dataset with training and test splits, where the training set is used to calculate the training accuracy as the objective value during the optimization process, and we compute the test accuracy on the test set after the optimization finishes. \nWhile traditional optimization often requires a decently large training set, our experiment shows that a small number or fraction of training samples (e.g., 3.5\\% of the training set for GSM8K~\\citep{cobbe2021training}, 20\\% for Big-Bench Hard~\\citep{suzgun2022challenging}) is sufficient. \nThe objective function evaluator is an LLM to which the optimized prompt will be applied, and it can be the same or different from the LLM for optimization. \nWe denote the LLM for objective function evaluation as the \\emph{scorer LLM}, and the LLM for optimization as the \\emph{optimizer LLM}.\n\nThe output of the optimizer LLM is an \\emph{instruction}, which is concatenated to the question part of every exemplar and prompts the scorer LLM. \nWe consider the following positions to insert the instruction:\n\n\\begin{itemize}[leftmargin=2em,topsep=0pt,partopsep=1ex,parsep=0ex]\n\\item \\emph{Q\\_begin}: the instruction is added before the original question.\n\\item \\emph{Q\\_end}: the instruction is added after the original question.\n\\item \\emph{A\\_begin}: the instruction is added to the beginning of the scorer LLM output. This is applicable to pretrained LLMs without instruction tuning, where the prompt is formatted as a sequence of QA pairs.\n\\end{itemize}\n\nWe exemplify these prompting formats in Appendix~\\ref{appsec:scorer_prompting_formats}.\n\n\\subsection{Meta-Prompt Design}\n\nFigure~\\ref{fig:meta_prompt_example} shows an example of the meta-prompt for prompt optimization on GSM8K~\\citep{cobbe2021training}.\nMore details are as follows.\n\n\\myparagraph{Optimization problem examples} The problem description includes a few examples taken from the training set to demonstrate the task for the generated instructions. \nFor example, from the input-output pair in Figure~\\ref{fig:meta_prompt_example}, we can infer this is a math word problem. \nThe input-output pair also demonstrates the position where the generated instruction will be added to, and this is essential for the optimizer LLM to generate instructions of the same style.\nIn each optimization step, we add several (three for example) training examples to the meta-prompt by random sampling the training set or choose the ones the previous instructions fall short of.\n\n\\myparagraph{Optimization trajectory} \nThe optimization trajectory includes instructions generated from the past optimization steps, along with their scores.\nThe old instructions and scores are sorted by the score in ascending order.\nThe score is the training accuracy in prompt optimization. \nWe only keep instructions with the highest scores in the meta-prompt in consideration of the LLM context length limit.\n\n\\myparagraph{Meta-instructions}\nWe also add \\emph{meta-instructions}: the instructions to the optimizer LLM that explain the optimization goal and instruct the model how to use the above information.\nThe meta-instructions may also specify the desired generated instruction format for easier parsing.\\section{Prompt Optimization Experiments}\n\\label{sec:exp}\n\nWe present the evaluation results for prompt optimization in this section. Our experiments demonstrate that \\name{} brings a significant performance gain across the board, with different combinations of LLMs as the optimizer and the scorer.\n\nSection~\\ref{sec:evaluation_setup} describes the experiment setup.\nSection~\\ref{sec:main_results} shows main results on reasoning tasks like GSM8K and BBH.\nSection~\\ref{sec:ablation} shows ablation studies. \nSection~\\ref{sec:overfitting_analysis_in_prompt_optimization} analyzes overfitting in prompt optimization.\nSection~\\ref{sec:comparison_with_evoprompt} compares the prompt optimization performance of meta-prompts in \\name{} and EvoPrompt~\\citep{guo2023connecting}.\n\n\\subsection{Evaluation Setup}\n\\label{sec:evaluation_setup}\n\n\\myparagraph{Models}\nThe LLMs we use as the optimizer and the scorer are:\n\n\\begin{itemize}[leftmargin=2em,topsep=0pt,partopsep=1ex,parsep=0ex]\n\\item Optimizer LLM: Pre-trained \\texttt{PaLM 2-L}~\\citep{anil2023palm}, instruction-tuned \\texttt{PaLM 2-L} (denoted \\texttt{PaLM 2-L-IT}), \\texttt{text-bison}, \\texttt{gpt-3.5-turbo}, and \\texttt{gpt-4}.\n\\item Scorer LLM: Pre-trained \\texttt{PaLM 2-L} and \\texttt{text-bison}.\n\\end{itemize}\n\nWith pre-trained \\texttt{PaLM 2-L} as the scorer, the optimizer LLM generates A\\_begin instructions.\nSince \\texttt{text-bison} has been instruction-tuned, the optimizer LLM generates Q\\_begin and Q\\_end instructions when \\texttt{text-bison} is used as the scorer.\n\n\\myparagraph{Benchmarks}\nOur primary evaluation benchmarks are GSM8K~\\citep{cobbe2021training} and Big-Bench Hard (BBH)~\\citep{suzgun2022challenging}. GSM8K is a benchmark of grade school math word problems with 7,473 training samples and 1,319 test samples, where chain-of-thought prompting~\\citep{wei2022chain} and the zero-shot instruction ``Let's think step by step.''~\\citep{kojima2022large} have drastically improved the performance over the standard prompting. BBH is a suite of 23 challenging BIG-Bench tasks~\\citep{srivastava2022beyond} that covers a wide range of topics beyond arithmetic reasoning, including symbolic manipulation and commonsense reasoning. Each task contains up to 250 examples in total.\n\nTo examine the transferability of the optimized instructions, we also evaluate the instructions optimized for GSM8K on two other mathematical reasoning datasets, i.e.,  MultiArith~\\citep{roy2016solving} and AQuA~\\citep{ling2017program}.\n\n\\myparagraph{Implementation details}\nWe set the temperature to be 0 when evaluating the performance of generated instructions, in which case the scorer LLM greedily decodes.\nUnless otherwise specified, we set the default temperature to be 1.0 for optimizer LLMs to generate diverse and creative instructions.\nAt each optimization step, we prompt the optimizer LLM with the meta-prompt 8 times to generate 8 instructions, then we add these instructions with their training scores to the optimization trajectory in the meta-prompt.\nOur meta-prompt at each step contains the best 20 instructions so far and 3 randomly picked exemplars from the training set.\nWe study the effect of different hyperparameters in ablation studies (Section~\\ref{sec:ablation}). Appendix~\\ref{appsec:meta_prompts_for_prompt_opt} presents the full meta-prompts for different optimizer LLMs.\n\n\\subsection{Main Results}\n\\label{sec:main_results}\nWe show prompt optimization curves on GSM8K and two BBH tasks in this section.\nThe curves on other BBH tasks are deferred to Appendix~\\ref{appsec:bbh_optimization_curves}, and the tables containing all accuracy numbers are in Appendix~\\ref{appsec:bbh_taskwise_detailed_results}.\n\n\\subsubsection{GSM8K}\n\n\\begin{table}[t]\n\\footnotesize\n\\caption{Test accuracies on GSM8K. We show the instruction with the highest test accuracy for each scorer-optimizer pair. \n}\n\\begin{center}\n\\scalebox{0.86}{\n\\begin{tabular}{cP{2cm}P{1.5cm}P{8cm}c}\n\\toprule\nScorer & Optimizer / Source & Instruction position & Top instruction & Acc \\\\\n\\midrule\n\\multicolumn{3}{l}{\\textit{Baselines}} \\\\\n\\hdashline\\noalign{\\vskip 0.5ex}\n\\texttt{PaLM 2-L} & \\citep{kojima2022large} & A\\_begin & Let's think step by step. & 71.8 \\\\ [1ex]\n\\texttt{PaLM 2-L} & \\citep{zhou2022large} & A\\_begin & Let’s work this out in a step by step way to be sure we have the right answer. & 58.8 \\\\ [3ex]\n\\texttt{PaLM 2-L} & & A\\_begin & Let's solve the problem. & 60.8 \\\\ [1ex]\n\\texttt{PaLM 2-L} & & A\\_begin & (empty string) & 34.0 \\\\ [1ex]\n\\texttt{text-bison} & \\citep{kojima2022large} & Q\\_begin & Let's think step by step. & 64.4 \\\\ [1ex]\n\\texttt{text-bison} & \\citep{zhou2022large} & Q\\_begin & Let’s work this out in a step by step way to be sure we have the right answer. & 65.6 \\\\ [3ex]\n\\texttt{text-bison} & & Q\\_begin & Let's solve the problem. & 59.1 \\\\ [1ex]\n\\texttt{text-bison} & & Q\\_begin & (empty string) & 56.8 \\\\\n\\midrule\n\\multicolumn{3}{l}{\\textit{Ours}} \\\\\n\\hdashline\\noalign{\\vskip 0.5ex}\n\\texttt{PaLM 2-L} & \\texttt{PaLM 2-L-IT} & A\\_begin & Take a deep breath and work on this problem step-by-step. & \\textbf{80.2} \\\\ [1ex]\n\\texttt{PaLM 2-L} & \\texttt{PaLM 2-L} & A\\_begin & Break this down. & 79.9 \\\\ [1ex]\n\\texttt{PaLM 2-L} & \\texttt{gpt-3.5-turbo} & A\\_begin & A little bit of arithmetic and a logical approach will help us quickly arrive at the solution to this problem. & 78.5 \\\\ [3ex]\n\\texttt{PaLM 2-L} & \\texttt{gpt-4} & A\\_begin & Let's combine our numerical command and clear thinking to quickly and accurately decipher the answer. & 74.5 \\\\ [3ex]\n\\texttt{text-bison} & \\texttt{PaLM 2-L-IT} & Q\\_begin & Let's work together to solve math word problems! First, we will read and discuss the problem together to make sure we understand it. Then, we will work together to find the solution. I will give you hints and help you work through the problem if you get stuck. & 64.4 \\\\ [3ex]\n\\texttt{text-bison} & \\texttt{text-bison} & Q\\_end & Let's work through this problem step-by-step: & \\textbf{68.5} \\\\ [1ex]\n\\texttt{text-bison} & \\texttt{gpt-3.5-turbo} & Q\\_end & Analyze the given information, break down the problem into manageable steps, apply suitable mathematical operations, and provide a clear, accurate, and concise solution, ensuring precise rounding if necessary. Consider all variables and carefully consider the problem's context for an efficient solution. & 66.5 \\\\ [5ex]\n\\texttt{text-bison} & \\texttt{gpt-4} & Q\\_begin & Start by dissecting the problem to highlight important numbers and their relations. Decide on the necessary mathematical operations like addition, subtraction, multiplication, or division, required for resolution. Implement these operations, keeping in mind any units or conditions. Round off by ensuring your solution fits the context of the problem to ensure accuracy. & 62.7 \\\\\n\\bottomrule\n\\end{tabular}}\n\\end{center}\n\\label{table:top_instructions_on_gsm8k}\n\\end{table}\n\nFor prompt optimization, we randomly sample 3.5\\% examples from the GSM8K training set.\nThe same subset is used throughout optimization, so that the task accuracies computed at intermediate optimization steps are approximations of the training accuracy on all 7,473 training examples.\nThis balances the evaluation cost with the generalization performance.\nAfter the optimization procedure finishes, we evaluate the found instructions on the entire GSM8K test set.\n\nFigure~\\ref{fig:prompt_optimization_graph_in_intro_gsm8k} in Section~\\ref{sec:intro} shows prompt optimization curves with pre-trained \\texttt{PaLM 2-L} as scorer and \\texttt{PaLM 2-L-IT} as optimizer, and the initial instruction is ``Let's solve the problem'' with a (approximated, and same below) training accuracy of 60.5.\nWe observe that the optimization curve shows an overall upward trend with several leaps throughout the optimization process, for example:\n\\begin{itemize}[leftmargin=2em,topsep=0pt,partopsep=1ex,parsep=0ex]\n\\item ``Let's think carefully about the problem and solve it together.'' at Step 2 with the training accuracy 63.2;\n\\item ``Let's break it down!'' at Step 4 with training accuracy 71.3;\n\\item ``Let's calculate our way to the solution!'' at Step 5 with training accuracy 73.9;\n\\item ``Let's do the math!'' at Step 6 with training accuracy 78.2.\n\\end{itemize}\n\nThe optimization curves also generally show a decrease of the variance among the accuracies of instructions generated at each step, indicating that the optimizer LLM generates \\emph{distributionally} better instructions throughout the optimization.\n\nNext, we present the results of generating Q\\_begin instructions with the \\texttt{text-bison} scorer and the \\texttt{PaLM 2-L-IT} optimizer, starting from an empty instruction with a 57.1 training accuracy.\nThe optimization curve in Figure~\\ref{fig:prompt_optimization_graph_gsm8k_text_bison} shows a similar upward trend, during which a few leaps in the training accuracy include:\n\\begin{itemize}[leftmargin=2em,topsep=0pt,partopsep=1ex,parsep=0ex]\n\\item ``Solve the following problems using the given information.'' at Step 2 with training accuracy 59.8;\n\\item ``Solve the following problems by applying the given information and using the appropriate mathematical operations.'' at Step 3 with training accuracy 64.0;\n\\item ``Let's read the problem carefully and identify the given information. Then, we can create an equation and solve for the unknown variable.'' at Step 4 with training accuracy 67.0;\n\\item ``I'm always down for solving a math word problem together. Just give me a moment to read and understand the problem. Then, I'll create an equation that models the problem, which I'll solve for the unknown variable. I also may or may not use some helpful diagrams or visuals to understand the problem. Lastly, be sure to allow me some time to carefully check my work before submitting any responses!'' at Step 29 with training accuracy 70.1.\n\\end{itemize}\n\nNote that although our default setting is to run \\name{} for 200 steps in prompt optimization, we need much fewer steps if the goal is to find some outstanding instructions.\nAn example is that the Figure~\\ref{fig:prompt_optimization_graph_in_intro_gsm8k} experiment found ``Let's do the math!'' at Step 6 with training accuracy 78.2, almost matching the ``Take a deep breath and work on this problem step-by-step.'' found at the 107th step with training accuracy 80.2, at a point where the optimization curve is still trending upwards.\nThis is because a leap in our optimization curve does not always correspond to a much better instruction being discovered; instead, it can be due to a large qualitative improvement of all 8 generated instructions in this step.\nThe latter usually happens several steps after the former: after a much better instruction is discovered in one step, the meta-prompt gradually gets rid of worse instructions in the latter steps by generating instructions similar to the much-better one. \nThe top instructions kept in the meta-prompt gradually improves in this procedure.\nAt a point when the meta-prompt only triggers higher quality instructions, the leap happens.\n\nFinally, Figure~\\ref{fig:prompt_optimization_graph_gsm8k_all_pretrained} shows that the pre-trained \\texttt{PaLM 2-L} can also serve as the optimizer LLM and improve its own prediction performance.\nDifferent from other optimizer LLMs that are instruction-tuned, the pre-trained \\texttt{PaLM 2-L} performs better when the prompt is formatted in a few-shot manner. Therefore, we include two initial instructions to start the optimization: the empty instruction (with a training accuracy 32.2) and ``The answer is'' (with a training accuracy 33.3).\nSee Figure~\\ref{fig:meta_prompt_example_pretrained_palm_2_l} in Appendix~\\ref{appsec:meta_prompts} for the meta-prompt format.\nThe generated instructions follow the same style as ``The answer is'': most instructions are also phrases suitable as the prefix of a sentence, like ``Here you go:'' (generated at Step 11 with training accuracy 61.3) and ``Let's do it:'' (generated at Step 13 with training accuracy 75.1).\n\nTable~\\ref{table:top_instructions_on_gsm8k} summarizes top instructions found on GSM8K with different scorer and optimizer LLMs.\nWe observe that:\n\\begin{itemize}[leftmargin=2em,topsep=0pt,partopsep=1ex,parsep=0ex]\n\\item The styles of instructions found by different optimizer LLMs vary a lot: \\texttt{PaLM 2-L-IT} and \\texttt{text-bison} ones are concise, while GPT ones are long and detailed.\n\\item Although some top instructions contain the ``step-by-step'' phrase, most others achieve a comparable or better accuracy with different semantic meanings.\n\\end{itemize}\n\n\\begin{figure}\n\\centering\n\\subfigure[\\texttt{PaLM 2-L-IT} optimizer]{\\label{fig:prompt_optimization_graph_gsm8k_text_bison}\\includegraphics[width=.35\\linewidth]{\\figurepath gsm8k_text_bison_o_finetuned_palm_2_l.pdf}}\n\\hspace{.01\\linewidth}\n\\subfigure[pre-trained \\texttt{PaLM 2-L} optimizer]{\\label{fig:prompt_optimization_graph_gsm8k_all_pretrained}\\includegraphics[width=.35\\linewidth]{\\figurepath gsm8k_s_pretrained_palm_2_l_o_pretrained_palm_2_l.pdf}}\n\n\\caption{Prompt optimization on GSM8K with \\subref{fig:prompt_optimization_graph_gsm8k_text_bison} the \\texttt{text-bison} scorer and the \\texttt{PaLM 2-L-IT} optimizer, and \\subref{fig:prompt_optimization_graph_gsm8k_all_pretrained} pre-trained \\texttt{PaLM 2-L} as both scorer and optimizer.\n}\n\\label{fig:prompt_optimization_in_main_results_gsm8k_more}\n\\end{figure}\n\n\\begin{figure}[t]\n\\centering\n\\subfigure[\\scriptsize \\texttt{PaLM 2-L} scorer, ours minus ``Let's think step by step.'']{\\label{fig:accuracy_comparison_palm_2_l_found_minus_step_by_step}\\includegraphics[width=.48\\linewidth]{\\figurepath bbh_s_palm_2_l_o_palm_2_l_it_found_vs_step_by_step.pdf}}\n\\hspace{.01\\linewidth}\n\\subfigure[\\scriptsize \\texttt{PaLM 2-L} scorer, ours minus empty starting point]{\\label{fig:accuracy_comparison_palm_2_l_found_minus_empty}\\includegraphics[width=.48\\linewidth]{\\figurepath bbh_s_palm_2_l_o_palm_2_l_it_found_vs_empty.pdf}}\n\n\\subfigure[\\scriptsize \\texttt{text-bison} scorer, ours minus ``Let's think step by step.'']{\\label{fig:accuracy_comparison_text_bison_found_minus_step_by_step}\\includegraphics[width=.48\\linewidth]{\\figurepath bbh_s_text_bison_o_palm_2_l_it_found_vs_step_by_step.pdf}}\n\\hspace{.01\\linewidth}\n\\subfigure[\\scriptsize \\texttt{text-bison} scorer, ours minus empty starting point]{\\label{fig:accuracy_comparison_text_bison_found_minus_empty}\\includegraphics[width=.48\\linewidth]{\\figurepath bbh_s_text_bison_o_palm_2_l_it_found_vs_empty.pdf}}\n\n\\caption{On 23 BBH tasks, the accuracy differences among instructions found by prompt optimization (with the \\texttt{PaLM 2-L-IT} optimizer), ``Let's think step by step.'', and the empty string (optimization starting point).\n}\n\\label{fig:accuracy_comparison_bar_charts_o_palm_2_l_it}\n\\end{figure}\n\n\\begin{figure}[t]\n\\centering\n\\subfigure[BBH ruin\\_names]{\\label{fig:prompt_optimization_graph_bbh_ruin_names}\\includegraphics[width=.35\\linewidth]{\\figurepath bbh_ruin_names_s_text_bison_o_finetuned_palm_2_l.pdf}}\n\\hspace{.01\\linewidth}\n\\subfigure[BBH temporal\\_sequences]{\\label{fig:prompt_optimization_graph_bbh_temporal_sequences}\\includegraphics[width=.35\\linewidth]{\\figurepath bbh_temporal_sequences_s_text_bison_o_finetuned_palm_2_l.pdf}}\n\n\\caption{Training accuracy curves of prompt optimization on BBH ruin\\_names and temporal\\_sequences with the \\texttt{text-bison} scorer and the \\texttt{PaLM 2-L-IT} optimizer.\nThe optimizations start from the empty string.\n}\n\\label{fig:prompt_optimization_in_main_results_bbh}\n\\end{figure}\n\n\\subsubsection{BBH}\nOn BBH, the optimization starts from an empty string as the initial instruction by default. The instructions are placed at A\\_begin when the scorer is \\texttt{PaLM 2-L}, and at Q\\_begin when the scorer is \\texttt{text-bison}. For each task, we utilize a subset of 20\\% examples for prompt optimization, and the rest examples are for testing. We show experimental results on more variants of the instruction position and initialization in Appendix~\\ref{appsec:bbh_taskwise_detailed_results}.\n\nFigure~\\ref{fig:accuracy_comparison_bar_charts_o_palm_2_l_it} visualizes the per-task accuracy difference on all 23 BBH tasks compared to the instruction ``Let's think step by step.''~\\citep{kojima2022large} and the empty instruction, and we present the concrete accuracies in Table~\\ref{table:palm2_scores_on_bbh_tasks} of Appendix~\\ref{appsec:bbh_taskwise_detailed_results}. We show that the instructions found by \\name{} outperform ``Let's think step by step.'' on almost all tasks by a large margin: our instructions outperform by over 5\\% on 19/23 tasks with the \\texttt{PaLM 2-L} scorer, and on 15/23 tasks with the \\texttt{text-bison} scorer.\nOur prompt optimization algorithm also improves instructions from the empty starting point by over 5\\% on most tasks: 20/23 with the \\texttt{PaLM 2-L} scorer and 15/23 with the \\texttt{text-bison} scorer.\n\nSimilar to GSM8K, we observe upward trends in optimization curves on almost all BBH tasks, as shown in Figure~\\ref{fig:prompt_optimization_in_main_results_bbh}.\nSee Figure~\\ref{fig:prompt_optimization_curves_bbh_text_bison_scorer_all_tasks_appendix_part_one} and~\\ref{fig:prompt_optimization_curves_bbh_text_bison_scorer_all_tasks_appendix_part_two} in Appendix~\\ref{appsec:bbh_optimization_curves} for more curves on other BBH tasks. \n\nWe next show some examples of instructions found through the course of optimization.\nOn the task ruin\\_names, starting from the empty instruction (with 64.0 training accuracy), with the \\texttt{text-bison} scorer and the \\texttt{PaLM 2-L-IT} optimizer, the following instructions are generated:\n\\begin{itemize}[leftmargin=2em,topsep=0pt,partopsep=1ex,parsep=0ex]\n\\item ``Consider the following when editing artist or movie names humorously:'' at Step 1 with training accuracy 72.0;\n\\item ``When making humorous edits of artist or movie names, you can change one or more letters or even create puns by adding new words that sound similar.'' at Step 18 with training accuracy 80.0;\n\\item ``We can make humorous edits of artist/movie names by changing letters to create new words that are similar in sound but have different meanings. For example, The Police can be changed to The Polite, The Abyss can be changed to Toe Abyss, and Schindler’s List can be changed to Schindler’s Lost.'' at Step 38 with training accuracy 82.0.\n\\end{itemize}\nAlthough the above instructions are semantically similar, a paraphrase by the optimizer LLM offers a notable accuracy improvement. We further highlight this observation in Section~\\ref{sec:ins-acc-variance}.\n\nBelow are some instructions generated when performing prompt optimization on temporal\\_sequences, starting from the empty instruction (with the training accuracy of 64.0):\n\\begin{itemize}[leftmargin=2em,topsep=0pt,partopsep=1ex,parsep=0ex]\n\\item ``To solve this problem, we need to first identify the time period when the person was not seen doing anything else. Then, we need to check if the place they went to was open during that time period. If it was, then that is the time period when they could have gone to that place.'' at Step 2 with training accuracy 42.0;\n\\item ``To find the time period when a person could have gone to a place, identify the time periods when they were not seen doing anything else and the place was open. If there are multiple time periods that match these criteria, then the person could have gone to the place during any of these time periods.'' at Step 18 with training accuracy 54.0;\n\\item ``To determine the possible time period when a person went to a place, first identify all the time periods when the person was not seen doing anything else and the place was open. Then, rule out any time periods during which the person was seen doing something else. The remaining time periods are the possible times when the person could have gone to the place.'' at Step 41 with training accuracy 72.0.\n\\end{itemize}\n\nTable~\\ref{table:top_instructions_on_bbh_tasks_main_paper} presents the best instructions generated on movie\\_recommendation, ruin\\_names, and temporal\\_sequences tasks with different combinations of the optimizer and the scorer LLMs.\nAgain, different optimizer LLMs produce instructions of different styles. See Appendix~\\ref{appsec:bbh_taskwise_detailed_results} for results on more BBH tasks.\n\n\\begin{table}\n\\centering\n\\caption{Top instructions with the highest accuracies found in prompt optimization on BBH movie\\_recommendation, ruin\\_names, and temporal\\_sequences.}\n\\scalebox{0.85}{\n\\begin{tabular}{P{2.0cm}P{2.5cm}P{1.2cm}P{7.5cm}c}\n\\toprule\nScorer & Optimizer & Instruction position & Instruction & Acc \\\\\n\\midrule\n\\multicolumn{3}{l}{\\textit{movie\\_recommendation}} \\\\\n\\hdashline\\noalign{\\vskip 0.5ex}\n\\texttt{PaLM 2-L} & \\texttt{PaLM 2-L-IT} & A\\_begin & Based on your input, I have analyzed the given movies in terms of genre, plot, tone, audience rating, year of release, director, cast, and reviews. I have also taken into account the given options. The movie that is most similar to the given movies in terms of all these factors is: & 90.8 \\\\ [13ex]\n\\texttt{PaLM 2-L} & \\texttt{PaLM 2-L} & A\\_begin & The best film: & 88.4 \\\\ [1ex]\n\\texttt{PaLM 2-L} & \\texttt{gpt-3.5-turbo} & A\\_begin & Let's uncover the perfect movie recommendation from the options provided, ensuring an exceptional cinematic experience together as we select the most captivating and satisfying choice that will keep us thoroughly engaged and immersed until the very end. & 88.0 \\\\ [12ex]\n\\texttt{text-bison} & \\texttt{PaLM 2-L-IT} & Q\\_begin & What is the highest-rated movie similar to the given movies, with a similar IMDb rating and released in the same year? & 91.6 \\\\ [7ex]\n\\texttt{text-bison} & \\texttt{gpt-3.5-turbo} & Q\\_begin & Based on the movie list provided, carefully consider your preferences and make a well-informed decision. & 70.8 \\\\\n\\midrule\n\\multicolumn{3}{l}{\\textit{ruin\\_names}} \\\\\n\\hdashline\\noalign{\\vskip 0.5ex}\n\\texttt{PaLM 2-L} & \\texttt{PaLM 2-L-IT} & A\\_begin & Which is the funniest pun on the artist or movie name? & 88.0 \\\\ [1ex]\n\\texttt{PaLM 2-L} & \\texttt{PaLM 2-L} & A\\_begin & Answer for ruin: & 83.6 \\\\ [1ex]\n\\texttt{PaLM 2-L} & \\texttt{gpt-3.5-turbo} & A\\_begin & Prepare to have a side-splittingly funny time as we uncover the most clever and hilarious alternatives for these artist or movie names, challenging your wit to guess the correct one with a burst of creativity, humor, and imaginative twists! & 86.8 \\\\ [12ex]\n\\texttt{text-bison} & \\texttt{PaLM 2-L-IT} & Q\\_begin & A humorous edit of an artist or movie name can be created by replacing one or more letters to form a new word or phrase that sounds similar but has a different meaning. The new word or phrase should be relevant to the original word, but it should also be a surprise, which makes the edit funny. For example, the artist or movie name \"Rocky\" can be changed to \"Ricky,\" and \"Schindler's List\" can be changed to \"Schindler's Lift.\" Be creative and have fun! & 83.6 \\\\ [22ex]\n\\texttt{text-bison} & \\texttt{gpt-3.5-turbo} & Q\\_begin & Choose the option that offers the most clever and humorous alteration of the given artist or movie name. Let your creativity shine and select the answer that will undoubtedly bring a smile to your face! Make sure to think outside the box! & 75.2 \\\\\n\\midrule\n\\multicolumn{5}{l}{\\textit{temporal\\_sequences} (no \\texttt{PaLM 2-L} as scorer results because its training accuracy on empty string is 100.0)} \\\\\n\\hdashline\\noalign{\\vskip 0.5ex}\n\\texttt{text-bison} & \\texttt{PaLM 2-L-IT} & Q\\_begin & To determine the time period when a person went to a place, first identify all the time periods when the person's whereabouts are unknown. Then, rule out any time periods during which the person was seen doing something else or the place was closed. The remaining time periods are the possible times when the person could have gone to the place. & 80.4 \\\\ [18ex]\n\\texttt{text-bison} & \\texttt{gpt-3.5-turbo} & Q\\_begin & Identify the optimal time slot for the individual to engage in the mentioned location/activity considering the given sightings and waking up time, taking into account the opening and closing times of the location and the duration of each event. & 53.6 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\label{table:top_instructions_on_bbh_tasks_main_paper}\n\\end{table}\n\n\\subsubsection{Semantically similar instructions may achieve drastically different accuracies}\n\\label{sec:ins-acc-variance}\nOne challenge of prompt optimization is the sensitivity of model performance to subtle changes in the instruction. \nFor example, with the \\texttt{PaLM 2-L} scorer on the GSM8K test set, ``Let's think step by step.'' achieves accuracy 71.8, ``Let's solve the problem together.'' has accuracy 60.5, while the accuracy of ``Let's work together to solve this problem step by step.'' is only 49.4, although it is the semantic combination of the two upper instructions.\nThis behavior increases both the variance across single-step instructions and the oscillation during optimization, and motivates us to generate multiple instructions at each step to improve the optimization stability.\n\n\\subsubsection{Transferability of found instructions}\n\n\\begin{table}\n\\footnotesize\n\\caption{Transferability across datasets: accuracies of top instructions found for GSM8K on MultiArith and AQuA.\n}\n\\begin{center}\n\\scalebox{0.9}{\n\\begin{tabular}{cP{2.2cm}P{1.5cm}P{5cm}cc}\n\\toprule\n\\multirow{2}{*}{Scorer} & \\multirow{2}{*}{Source} & \\multirow{2}{*}{\\parbox{1.5cm} {\\centering Instruction position}} & \\multirow{2}{*}{Instruction} & \\multicolumn{2}{c}{Accuracy} \\\\ \\cmidrule{5-6}\n& & & & MultiArith & AQuA \\\\\n\\midrule\n\\multicolumn{3}{l}{\\textit{Baselines}} \\\\\n\\hdashline\\noalign{\\vskip 0.5ex}\n\\texttt{PaLM 2-L} & \\citep{kojima2022large} & A\\_begin & Let's think step by step. & 85.7 & 44.9 \\\\ [1ex]\n\\texttt{PaLM 2-L} & \\citep{zhou2022large} & A\\_begin & Let’s work this out in a step by step way to be sure we have the right answer. & 72.8 & 48.4 \\\\ [3ex]\n\\texttt{PaLM 2-L} & & A\\_begin & Let's solve the problem. & 87.5 & 44.1 \\\\ [1ex]\n\\texttt{PaLM 2-L} & & A\\_begin & (empty string) & 69.3 & 37.8 \\\\ [1ex]\n\\texttt{text-bison} & \\citep{kojima2022large} & Q\\_begin & Let's think step by step. & 92.5 & 31.9 \\\\ [1ex]\n\\texttt{text-bison} & \\citep{zhou2022large} & Q\\_begin & Let’s work this out in a step by step way to be sure we have the right answer. & 93.7 & 32.3 \\\\ [3ex]\n\\texttt{text-bison} & & Q\\_begin & Let's solve the problem. & 85.5 & 29.9 \\\\ [1ex]\n\\texttt{text-bison} & & Q\\_begin & (empty string) & 82.2 & 33.5 \\\\\n\\midrule\n\\multicolumn{3}{l}{\\textit{Ours}} \\\\\n\\hdashline\\noalign{\\vskip 0.5ex}\n\\texttt{PaLM 2-L} & \\texttt{PaLM 2-L-IT} on GSM8K & A\\_begin & Take a deep breath and work on this problem step-by-step. & \\textbf{95.3} & \\textbf{54.3} \\\\ [4ex]\n\\texttt{text-bison} & \\texttt{PaLM 2-L-IT} on GSM8K & Q\\_begin & Let's work together to solve math word problems! First, we will read and discuss the problem together to make sure we understand it. Then, we will work together to find the solution. I will give you hints and help you work through the problem if you get stuck. & \\textbf{96.8} & \\textbf{37.8} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{center}\n\\label{table:ins_performance_on_multiarith}\n\\end{table}\n\nWe assess the transferability of found prompts to different datasets of the same domain, where we evaluate the top instructions found for GSM8K on two more math reasoning benchmarks MultiArith~\\citep{roy2016solving} and AQuA~\\citep{ling2017program}.\nTable~\\ref{table:ins_performance_on_multiarith} shows that our optimized prompts also outperform baseline prompts with different scorer LLMs on these two benchmarks.\n\n\\subsection{Ablation Studies}\n\\label{sec:ablation}\nWe use \\texttt{text-bison} as the scorer and \\texttt{PaLM 2-L} as the optimizer for all ablation studies.\nThe tasks we evaluate are GSM8K (math reasoning) and BBH sports\\_understanding (non-math reasoning).\n\n\\myparagraph{Meta-prompt design}\nThe meta-prompt design is crucial in achieving good prompt optimization performance. We investigate the following core design choices:\n\\begin{itemize}[leftmargin=2em,topsep=0pt,partopsep=1ex,parsep=0ex]\n\\item \\emph{The order of the previous instructions.}\nWe compare the following options: (1) from lowest to highest (our default setting); (2) from highest to lowest; (3) random.\nFigures~\\ref{fig:ablation_meta_prompt_ins_ordering_gsm8k} and~\\ref{fig:ablation_meta_prompt_ins_ordering_sports} show that the default setting achieves better final accuracies and converges faster.\nOne hypothesis is that the optimizer LLM output is affected more by the past instructions closer to the end of the meta-prompt. \nThis is consistent with the recency bias observed in~\\citet{zhao2021calibrate}, which states that LLMs are more likely to generate tokens similar to the end of the prompt.\n\n\\item \\emph{The effect of instruction scores.}\nIn terms of how to present the accuracy scores, we compare three options: (1) rounding the accuracies to integers, which is equivalent to bucketizing the accuracy scores to 100 buckets (our default setting); (2) bucketizing the accuracies to 20 buckets; (3) not showing the accuracies, only showing the instructions in the ascending order.\nFigures~\\ref{fig:ablation_meta_prompt_ins_scores_gsm8k} and~\\ref{fig:ablation_meta_prompt_ins_scores_sports} show that the accuracy scores assists the optimizer LLM in better understanding the quality difference among previous instructions, and thus the optimizer LLM proposes better new instructions that are similar to the best ones in the input optimization trajectory.\n\n\\item \\emph{The effect of exemplars.}\nWe compare three options: (1) showing 3 exemplars from the task (default); (2) showing 10 exemplars from the task; (3) no exemplars.\nFigures~\\ref{fig:ablation_meta_prompt_exemplars_gsm8k} and~\\ref{fig:ablation_meta_prompt_exemplars_sports} show that presenting exemplars in the meta-prompt is critical, as it provides information on what the task looks like and helps the optimizer model phrase new instructions better.\nHowever, more exemplars do not necessarily improve the performance, as a few exemplars are usually sufficient to describe the task. In addition, including more exemplars results in a longer meta-prompt with a dominating exemplar part, which may distract the optimizer LLM from other important components like the optimization trajectory.\n\\end{itemize}\n\n\\begin{figure}\n\\centering\n\\subfigure[instruction ordering (GSM8K)]{\\label{fig:ablation_meta_prompt_ins_ordering_gsm8k}\\includegraphics[width=.46\\linewidth]{\\figurepath ablation_meta_prompt_instruction_ordering_gsm8k.pdf}}\n\\hspace{.01\\linewidth}\n\\subfigure[instruction ordering (BBH sports\\_understanding)]{\\label{fig:ablation_meta_prompt_ins_ordering_sports}\\includegraphics[width=.47\\linewidth]{\\figurepath ablation_meta_prompt_instruction_ordering_bbh_sports.pdf}}\n\n\\subfigure[instruction scores (GSM8K)]{\\label{fig:ablation_meta_prompt_ins_scores_gsm8k}\\includegraphics[width=.46\\linewidth]{\\figurepath ablation_meta_prompt_instruction_scores_gsm8k.pdf}}\n\\hspace{.01\\linewidth}\n\\subfigure[instruction scores (BBH sports\\_understanding)]{\\label{fig:ablation_meta_prompt_ins_scores_sports}\\includegraphics[width=.46\\linewidth]{\\figurepath ablation_meta_prompt_instruction_scores_bbh_sports.pdf}}\n\n\\subfigure[\\# exemplars (GSM8K)]{\\label{fig:ablation_meta_prompt_exemplars_gsm8k}\\includegraphics[width=.46\\linewidth]{\\figurepath ablation_meta_prompt_exemplars_gsm8k.pdf}}\n\\hspace{.01\\linewidth}\n\\subfigure[\\# exemplars (BBH sports\\_understanding)]{\\label{fig:ablation_meta_prompt_exemplars_sports}\\includegraphics[width=.46\\linewidth]{\\figurepath ablation_meta_prompt_exemplars_bbh_sports.pdf}}\n\n\\caption{\\textbf{Ablation studies: how each part of the meta-prompt matters.} \nThe dots are the average values across 3 optimization repetitions, and the shaded regions represent standard deviations.}\n\\label{fig:ablation_meta_prompt}\n\\end{figure}\n\n\\myparagraph{The number of generated instructions per step}\nComputing a mini-batch of gradients reduces the variance of a stochastic gradient descent procedure.\nSimilarly, generating multiple instructions in each step improves the optimization stability with LLMs.\nOn the other hand, to achieve better performance with a fixed budget for the number of instructions to evaluate, the number of per-step instructions should not be too large, so as to allow more optimization steps to incorporate richer information of past instructions with their accuracies.\nTaking both aspects into consideration, Figure~\\ref{fig:ablation_num_ins_in_each_step} compares the optimization performance of sampling 1 / 2 / 4 / 8 (default) / 16 instructions in each step, showing that sampling 8 instructions at each step overall achieves the best performance.\n\n\\begin{figure}\n\\centering\n\\subfigure[GSM8K]{\\label{fig:ablation_num_ins_in_each_step_gsm8k}\\includegraphics[width=.46\\linewidth]{\\figurepath ablation_num_generated_ins_gsm8k.pdf}}\n\\hspace{.01\\linewidth}\n\\subfigure[BBH sports\\_understanding]{\\label{fig:ablation_num_ins_in_each_step_sports}\\includegraphics[width=.46\\linewidth]{\\figurepath ablation_num_generated_ins_bbh_sports.pdf}}\n\\caption{\\textbf{Ablation studies: the number of generated instructions in each step.} \nThe dots are the average values across 3 optimization repetitions, and the shaded regions represent standard deviations.\nThe x-axis represents the total number of evaluated instructions through the optimization; e.g., we run 200 optimization steps when sampling 8 instructions in each step, run 400 steps when sampling 4 instructions in each step, etc.}\n\\label{fig:ablation_num_ins_in_each_step}\n\\end{figure}\n\n\\begin{figure}[t]\n\\centering\n\\subfigure[GSM8K, \\texttt{text-bison} scorer, Q\\_begin]{\\label{fig:ablation_starting_point_text_bison}\\includegraphics[width=.43\\linewidth]{\\figurepath ablation_initialization_gsm8k_text_bison.pdf}}\n\\hspace{.01\\linewidth}\n\\subfigure[GSM8K, \\texttt{PaLM 2-L} scorer, A\\_begin]{\\label{fig:ablation_starting_point_palm_2_l}\\includegraphics[width=.5\\linewidth]{\\figurepath ablation_initialization_gsm8k_palm_2_l.pdf}}\n\\caption{\\textbf{Ablation studies: the initial instructions for prompt optimization.}\nThe dots are the average values across 3 optimization repetitions, and the shaded regions represent standard deviations.\n}\n\\label{fig:ablation_starting_point}\n\\end{figure}\n\n\\myparagraph{Starting point}\nWe study the effect of different initial instructions for prompt optimization.\nOur default setting is to start from an empty string when the scorer LLM is (instruction-tuned) \\texttt{text-bison}, and to start from either the empty string (on BBH tasks) or ``Let's solve the problem.'' (on GSM8K) with instruction position A\\_begin when the scorer LLM is the (pre-trained) \\texttt{PaLM 2-L}.\nFigure~\\ref{fig:ablation_starting_point_text_bison} shows the performance of \\texttt{text-bison} as the scorer LLM with 3 options of initial instructions: (1) the empty string; (2) ``Solve the following problem.''; or (3) ``Solve the following problem.'' and ``Let's solve the problem.''. We observe that the accuracies do not differ much with different starting points.\nInterestingly, the styles of the generated instructions are also similar. For example, most of the generated instructions starting from (1) and (2) contain the phrase ``solve this problem'', like ``Let's work together to solve this problem.'' in Step 4 with training accuracy 64.8 from (1), and ``Let's solve the following problems using the given information.'' in Step 3 with training accuracy 62.8 from (2).\n\nFigure~\\ref{fig:ablation_starting_point_palm_2_l} presents the results of  of \\texttt{PaLM 2-L} as the scorer LLM  with the following options of initial instructions: (1) ``Let's solve the problem.''; (2) the empty string; or (3) ``Let's think step by step.''. We notice that the performance differs much more with different initial instructions, especially at the beginning of the optimization.\nSpecifically, starting from (1) leads to better generated instructions than (2) in the first 30 steps, while the instructions optimized from both (1) and (2) are worse than (3) throughout.\nA similar observation holds when using \\texttt{PaLM 2-L} as scorer and \\texttt{gpt-3.5-turbo} as optimizer for BBH tasks, by comparing the results starting from the empty string (Appendix~\\ref{appsec:bbh_taskwise_detailed_results_gpt_3.5_turbo_optimizer_start_from_empty}) and from ``Let's solve the problem.'' (Appendix~\\ref{appsec:bbh_taskwise_detailed_results_gpt_3.5_turbo_optimizer_start_from_solve}).\nTaking a closer look into the optimization process of (2), we find that although both ``solve the problem'' and ``step by step'' show up in generated instructions at Step 5, it takes the optimizer LLM more steps to get rid of worse instructions presented in the meta-prompt when starting from instructions with lower accuracies.\nTherefore, one direction for future work is to accelerate convergence from weaker starting points.\n\n\\myparagraph{Diversity per step} \nWe evaluate the following temperatures of the optimizer LLM: \\{0.0, 0.5, 1.0 (default), 1.5, 2.0\\}.\nFigure~\\ref{fig:ablation_temperature} shows the default temperature 1.0 achieves the best performance.\nSpecifically, optimizations with smaller temperatures (0.0 and 0.5) lack exploration and thus creativity, and the optimizer LLM often gets stuck at the same instruction for tens of steps, resulting in flat optimization curves.\nOn the other hand, with larger temperatures (1.5 and 2.0), the optimizer LLM more often ignores the trajectory of previous instructions presented in the meta-prompt and thus lacks exploitation, therefore the optimization curve does not have a steady upward trend.\n\n\\begin{figure}[t]\n\\centering\n\\subfigure[GSM8K]{\\label{fig:ablation_temperature_gsm8k}\\includegraphics[width=.4\\linewidth]{\\figurepath ablation_temperature_gsm8k.pdf}}\n\\hspace{.01\\linewidth}\n\\subfigure[BBH sports\\_understanding]{\\label{fig:ablation_temperature_bbh_sports}\\includegraphics[width=.4\\linewidth]{\\figurepath ablation_temperature_bbh_sports.pdf}}\n\\caption{\\textbf{Ablation studies: temperature of the optimizer model.} \nThe dots are the average values across 3 optimization repetitions, and the shaded regions represent standard deviations.\n}\n\\label{fig:ablation_temperature}\n\\end{figure}\n\n\\myparagraph{Comparison with one-step instruction generation}\nOur current iterative procedure runs for multiple steps and generates a new batch of solutions in each step.\nTo validate the importance of leveraging the optimization trajectory for generating new prompts, we compare to a baseline that generates all instructions in a single step without entering into the optimization procedure.\nWe compare these two approaches on GSM8K and BBH sports\\_understanding with the \\texttt{PaLM 2-L-IT} optimizer. \nFor GSM8K the scorer LLM is pre-trained \\texttt{PaLM 2-L} and the initial instruction is “Let’s solve the problem”, and for BBH sports\\_understanding the scorer LLM is \\texttt{text-bison} and the initial instruction is the empty string. \nThe baseline generates 50 instructions in a single step, thus its meta-prompt only includes task exemplars, the initial instruction with its accuracy, and the same meta-instructions as our full meta-prompt for performing optimization. \nAll the other hyperparameters remain the same.\n\nOur results show that this one-step instruction generation performs much worse than our optimization approach. Specifically:\n(1) On GSM8K, the best instruction among all 50 is still ``Let's solve the problem'', with a 64.4 training accuracy and a 60.8 test accuracy. On the other hand, our approach (corresponding to Figure~\\ref*{fig:prompt_optimization_graph_in_intro_gsm8k} in the main paper) found ``Let’s do the math!'' with a 78.2 training accuracy and a 76.3 test accuracy at the 5th step by generating 8 instructions at each step.\n(2) Similarly, on BBH sports\\_understanding, the best instruction among all 50 achieved a 84.0 training accuracy and 80.0 test accuracy. This is again worse than the instruction found by our approach at Step 4, which achieved a 88.0 training accuracy and a 84.5 test accuracy.\n\n\\subsection{Overfitting Analysis in Prompt Optimization}\n\\label{sec:overfitting_analysis_in_prompt_optimization}\nFor simplicity, we do not set aside a validation set in our default setting of prompt optimization.\nWe made this decision based on the experiments when a validation set is present.\n\nOverfitting may result in training accuracy being much higher than the validation/test accuracy.\nIt is difficult to avoid overfitting, but overfitting is less harmful when each candidate solution (natural language instruction in the prompt optimization context) overfits to a similar extent.\nIn this case, a higher training accuracy solution still achieves a higher validation/test accuracy, and one can adopt solutions with the highest training accuracies as the final result.\nFigure~\\ref{fig:overfitting_analysis} shows this is the case for OPRO in prompt optimization: when setting aside a validation set with the same size as the training set, the validation accuracy curves trend up and down alongside the training curves in both prompt optimization settings.\n\n\\begin{figure}[t]\n\\centering\n\\subfigure[BBH snarks, \\texttt{PaLM 2-L} as scorer, \\texttt{PaLM 2-L-IT} as optimizer, starting from ``Let’s solve the problem.'']{\\label{fig:overfitting_analysis_bbh_snarks}\\includegraphics[width=.43\\linewidth]{\\figurepath overfitting_analysis_bbh_snarks.pdf}}\n\\hspace{.05\\linewidth}\n\\subfigure[BBH sports\\_understanding, \\texttt{text-bison} as scorer, \\texttt{gpt-3.5-turbo} as optimizer, starting from the empty string]{\\label{fig:overfitting_analysis_bbh_sports}\\includegraphics[width=.43\\linewidth]{\\figurepath overfitting_analysis_bbh_sports.pdf}}\n\\caption{\\textbf{Overfitting analysis.}\nThe exemplars are splitted to 1/3 training, 1/3 validation and 1/3 test.\nWe compute the validation accuracy every 3 steps.\nThe training/validation dots are the average training/validation accuracies across 3 optimization repetitions, respectively, and the shaded regions represent standard deviations.\n}\n\\label{fig:overfitting_analysis}\n\\end{figure}\n\nOf course, overfitting still occurs in the instructions found by our prompt optimization: in Table~\\ref{table:palm2_scores_on_bbh_tasks} and~\\ref{table:gpt_scores_on_bbh_tasks_starting_from_empty}, our training accuracies are often 5\\%-20\\% higher than our test accuracies, despite that our test and overall accuracies are still mostly higher than human-written counterparts.\nSetting aside a larger training set and optimizing for fewer steps (early stopping) may help reduce overfitting.\n\n\\subsection{Comparison with EvoPrompt}\n\\label{sec:comparison_with_evoprompt}\nSome concurrent works on prompt optimization propose meta-prompts that explicitly ask the LLM to perform mutation and crossovers of existing prompts~\\citep{fernando2023promptbreeder,guo2023connecting}. In our evaluation, we compare our approach to the Genetic Algorithm (GA) and Differential Evolution (DE) versions of EvoPrompt~\\citep{guo2023connecting}. Specifically, in the GA meta-prompt, given two prompts, the meta-prompt instructs the LLM to cross over the two prompts and generates a new one, then mutates the newly generated prompt to produce the final prompt. DE extends the GA meta-prompt to include more detailed instructions, e.g., asking the LLM to identify different parts between the two given prompts before performing the mutation. This is in contrast with OPRO, which leverages the optimization trajectory including multiple past prompts, instead of only 2 previous prompts. Meanwhile, OPRO also provides the LLM with richer information to facilitate the understanding of the optimization problem, including exemplars and task accuracies of different prompts.\n\nFigure~\\ref{fig:comparison_with_evoprompt} presents the results on GSM8K and BBH sports\\_understanding benchmarks, where we use \\texttt{gpt-3.5-turbo} as the optimizer. On GSM8K, the initial instructions of all approaches are ``Let's solve the problem.'' and ``Here is the answer.'', which are simple and generic. Again, we observe that OPRO performance steadily improves with more optimization steps. On the other hand, both versions of EvoPrompt even degrade the performance on GSM8K. The main reason is because EvoPrompt does not utilize exemplars for prompt optimization, thus it lacks the understanding of the task to optimize for. In this way, EvoPrompt relies on good-quality and task-specific initial prompts to optimize from.\n\nGiven this observation, we provide more task-specific initial instructions for experiments on BBH sports\\_understanding, which are ``Solve the sports understanding problem.'' and ``Give me the answer to sports understanding.'' In this case, EvoPrompt (DE) is able to find better prompts than the initial ones, but the optimization curve is less stable than OPRO. This indicates that leveraging the optimization trajectory helps the LLM to identify promising directions to improve existing prompts.\n\n\\begin{figure}[t]\n\\centering\n\\subfigure[GSM8K, \\texttt{PaLM 2-L} scorer, A\\_begin]{\\label{fig:comparison_with_evoprompt_gsm8k}\\includegraphics[width=.43\\linewidth]{\\figurepath compare_with_evoprompt_gsm8k.pdf}}\n\\hspace{.01\\linewidth}\n\\subfigure[BBH sports\\_understanding, \\texttt{text-bison} scorer, Q\\_begin]{\\label{fig:comparison_with_evoprompt_bbh_sports}\\includegraphics[width=.43\\linewidth]{\\figurepath compare_with_evoprompt_bbh_sports_richer_init.pdf}}\n\\caption{\\textbf{Comparison with EvoPrompt in prompt optimization.}\nWe use the \\texttt{gpt-3.5-turbo} optimizer for both experiments.\n``EvoPrompt (GA)'' uses the meta-prompt from~\\citet{guo2023connecting}, Figure 1; ``EvoPrompt (DE)'' uses the meta-prompt from~\\citet{guo2023connecting}, Figure 2.\nAll optimizations in~\\subref{fig:comparison_with_evoprompt_gsm8k} use the pre-trained \\texttt{PaLM 2-L} scorer and start from two simple instructions ``Let's solve the problem.'' and ``Here is the answer.''; all optimizations in~\\subref{fig:comparison_with_evoprompt_bbh_sports} use the \\texttt{text-bison} scorer and start from two richer (task-specific) instructions ``Solve the sports understanding problem.'' and ``Give me the answer to sports understanding.''.\nThe dots are the average values across 3 optimization repetitions, and the shaded regions represent standard deviations.\nWe use temperature 1.0 for OPRO and temperature 0.5 for EvoPrompt, same as the default settings in respective works.\n}\n\\label{fig:comparison_with_evoprompt}\n\\end{figure}\\section{Related Work}\n\\label{sec:work}\n\n\\myparagraph{Prompt optimization} Prior works have developed soft prompt-tuning methods that optimize the prompt represented as task-specific continuous vectors~\\citep{lester2021power,li2021prefix,liu2021gpt,qin2021learning}, as well as performing discrete prompt optimization by gradient-guided search~\\citep{shin2020autoprompt,wen2023hard,gao2020making,chen2023instructzero} and reinforcement learning~\\citep{deng2022rlprompt,zhang2022tempera}. \nThese approaches become inapplicable when there is only API access to the LLM. \nOther works designed edit-based approaches for gradient-free prompt optimization~\\citep{xu2022gps,prasad2022grips}, where the editing can be done with human-defined operations (e.g., swapping two phrases)~\\citep{prasad2022grips} or language models (e.g., back translation)~\\citep{xu2022gps}. \nSome recent works investigate LLMs for prompt optimization~\\citep{zhou2022large,pryzant2023automatic,xu2023wizardlm}. \nSpecifically, APE~\\citep{zhou2022large} first uses the LLM to generate initial instructions. Afterwards, APE selects top instructions with the highest accuracies, then prompts the LLM with each individual instruction to generate a semantically similar variant of the initial instruction.\nAPO~\\citep{pryzant2023automatic} in each step instructs the LLM to produce text feedback on how to update an old instruction. \nDifferent from edit-based approaches, the optimizer LLM in our work directly generates new instructions at each optimization step, and the optimizer LLM is merely asked to improve the task accuracy without being required to imitate past instructions. Compared to \\citet{zhou2022large} and \\citet{pryzant2023automatic}, our optimization process incorporates the past generated instructions with their scores in the meta-prompt, enabling the optimizer LLM to discover common patterns of high-quality instructions.\n\n\\myparagraph{Prompting with natural language feedback} A recent line of work investigates approaches to improve the LLM performance by prompting with natural language feedback to revise the model output, which has shown effectiveness in reducing harmful LLM outputs~\\citep{bai2022constitutional,ganguli2023capacity}, improving reasoning~\\citep{shinn2023reflexion,madaan2023self} and code generation performance~\\citep{chen2023teaching,olausson2023demystifying,shinn2023reflexion,chen2023improving}, dialogue applications~\\citep{nair2023dera,madaan2023self,yuan2023system}, and so on~\\citep{kim2023language,wang2023voyager}. Specifically, \\citet{yuan2023system} develops a human-in-the-loop framework for deriving system-level feedback from a collection of instance-level feedback, which is then used for refining data. In our work, the optimizer LLM utilizes the optimization trajectory in the prompt, which implicitly requires the LLM to summarize the common characteristics among solutions with similar scores. We consider incorporating explicit natural language feedback on generated solutions for later optimization steps as future work.\n\n\\myparagraph{Tuning language models for optimization}\nSome previous works tune or prompt language models to behave as mutation and crossover operators in evolutionary algorithms.\n\\citet{meyerson2023language} utilizes language models with few-shot exemplars to propose evolutionary cross-overs on tasks such as image and code generation.\nIn \\citet{lehman2022evolution}, the large language model trained on code diff generation is used as the mutation operator, and they further design a fine-tuning method to improve performance in the Sodarace domain for robot simulation.\nEvoPrompting~\\citep{chen2023evoprompting} uses large language models to evolve neural network architectures, where they combine evolutionary search with soft prompt tuning.\nWith respect to taking the trajectory as the input for optimization, OptFormer~\\citep{chen2022towards} trains a transformer model on large collections of hyperparameter optimization data.\nOn the other hand, our work performs optimization solely by prompting without additional training.\n\n\\section{Conclusion}\n\\label{sec:conclusion}\nWe embark on employing LLMs as optimizers, where the LLM progressively generates new solutions to optimize an objective function.\nWe first motivate \\name{} with linear regression and traveling salesman problems, then proceed to prompt optimization as a concrete application.\nOur evaluation demonstrates that LLMs have the capacity of gradually improving the generated solutions based on the past optimization trajectory.\nInterestingly, on small-scale traveling salesman problems, \\name{} performs on par with some hand-crafted heuristic algorithms.\nFor prompt optimization, optimized prompts outperform human-designed prompts on GSM8K and Big-Bench Hard by a significant margin, sometimes over $50\\%$.\n\nA number of unresolved questions are open for future research on LLMs for optimization. \nIn general, how to reduce the sensitivity to initialization and better balance exploitation with exploration remains a challenge. \nSpecifically, for prompt optimization, one limitation of our current implementation is that the optimizer LLM does not effectively utilize error cases in the training set to infer promising directions to improve the generated instructions. \nIn our experiments, we tried including error cases in the meta-prompt rather than randomly sampling from the training set at each optimization step, but the results are similar, indicating that the error cases alone are not informative enough for the optimizer LLM to grasp the cause of the wrong prediction.\nAnother limitation is that prompt optimization requires a training set to compute the accuracy that guides the optimization process. \nCurrently the training set at least contains tens of samples, so that the optimized prompt does not severely overfit to the training samples. \nA promising direction is to incorporate richer feedback about the error cases besides the aggregated accuracy, and summarize the key features that distinguish between high-quality and low-quality generated prompts in the optimization trajectory.\nSuch information may inform the optimizer LLM of how to more efficiently improve over the past generated instructions, and potentially further reduce the example set size needed for prompt optimization.\n\\section*{Ethics Statement}\nThis work uses synthetic math problems for linear regression and traveling salesman problems, and uses public datasets like GSM8K and Big-Bench Hard for prompt optimization.\nThese tasks have been commonly used in similar works and should not be regarded controversial.\nThere is a peril that LLMs may generate harmful information that poses safety risks; how to safeguard model behavior remains valuable future work.\n\n\\section*{Reproducibility Statement}\n\nWe evaluate on public benchmarks. The \\texttt{text-bison} API is available at:~\\url{https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models}. The GPT models are available here:~\\url{http://openai.com/api/}. \nThis work uses \\texttt{gpt-3.5-turbo-0613} and \\texttt{gpt-4-0613}.\n\n\\section*{Acknowledgments}\n\nWe thank Daiyi Peng, Yanqi Zhou, Jerry Wei, Shuo Chen, Tim Rocktäschel, Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Ed H. Chi for their valuable feedback, and thank several anonymous reviewers for helpful comments.\n\n\\newpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2211.01910v2.tex",
        "arXiv-2305.03495v2.tex",
        "arXiv-2309.03409v3.tex"
    ],
    "group_id": "group_93",
    "response": "### Title: Large Language Models as Prompt Engineers and Optimizers: A Comparative Analysis\n\n### Introduction\n\nThe field of large language models (LLMs) has seen significant advancements, with models like GPT-3, PaLM, and InstructGPT demonstrating remarkable capabilities in natural language processing (NLP) tasks. These models can generate coherent text, solve complex reasoning problems, and follow instructions with high accuracy, even in zero-shot and few-shot learning scenarios. However, the performance of these models is highly dependent on the quality of the prompts used to guide them. Prompt engineering, the process of crafting effective instructions, has traditionally been a manual and labor-intensive task, requiring extensive trial and error to achieve optimal results. This dependency on human-engineered prompts has led to the exploration of automated methods to generate and optimize prompts, reducing the need for human intervention and potentially improving model performance.\n\nCurrent research in this area aims to automate the process of prompt engineering and optimization, leveraging the generative capabilities of LLMs. The goal is to create algorithms that can generate high-quality prompts automatically, thereby making the process of using LLMs more efficient and accessible to a wider range of users. Challenges include the vast and complex nature of the prompt space, the difficulty in measuring the quality of a prompt without extensive testing, and the need to balance exploration and exploitation during the optimization process. Additionally, there is a need to ensure that the generated prompts are not only effective but also interpretable and can be applied across different tasks and datasets.\n\nThis summary will examine three recent papers that propose novel methods for automatic prompt engineering and optimization. The first paper introduces Automatic Prompt Engineer (APE), the second proposes Prompt Optimization with Textual Gradients (ProTeGi), and the third explores the use of LLMs as optimizers for various tasks. Each paper employs different strategies to generate and optimize prompts, contributing to the broader goal of making LLMs more controllable and versatile.\n\n### Main Content of Each Paper\n\n#### Paper 1: Automatic Prompt Engineer (APE)\n\nThe first paper introduces APE, an algorithm designed to generate and select instructions for LLMs in a black-box optimization framework. APE aims to address the challenge of prompt engineering by treating the instruction as a \"program\" that needs to be optimized to achieve the best performance. The algorithm leverages the generality of LLMs to generate a pool of instruction candidates and then refines this pool through iterative Monte Carlo search, where the LLMs propose semantically similar instruction variants based on the initial set of candidates. This process is guided by a score function that evaluates the zero-shot performance of another LLM following the proposed instruction.\n\nThe paper evaluates APE on 24 Instruction Induction tasks and 21 curated BIG-Bench tasks. The results show that APE-generated instructions outperform human-engineered prompts on most tasks, achieving human-level or better performance. The authors also conduct extensive qualitative and quantitative analyses to explore the performance of APE, demonstrating its effectiveness in improving few-shot learning, finding better zero-shot chain-of-thought prompts, and steering models towards desired behaviors such as truthfulness and informativeness.\n\n#### Paper 2: Prompt Optimization with Textual Gradients (ProTeGi)\n\nThe second paper, ProTeGi, proposes a method for optimizing prompts by mirroring the steps of gradient descent within a text-based Socratic dialogue. ProTeGi uses a beam search and bandit selection procedure to iteratively refine prompts, improving their performance by generating natural language \"gradients\" that criticize the current prompt and then editing the prompt in the opposite semantic direction of these gradients. The algorithm starts from an initial prompt and iteratively generates new prompts that aim to increase the accuracy of the task.\n\nProTeGi is evaluated on four benchmark NLP tasks, including jailbreak detection, hate speech detection, fake news detection, and sarcasm detection. The results indicate that ProTeGi can significantly improve prompt performance, outperforming other state-of-the-art algorithms by up to 31% on some tasks. The paper also explores the effectiveness of different bandit algorithms in the selection process, finding that UCB-style algorithms generally outperform successive rejects-style algorithms. ProTeGi is shown to be efficient and effective, even without hyperparameter tuning or model training.\n\n#### Paper 3: Large Language Models as Optimizers (OPRO)\n\nThe third paper, OPRO, presents a method for using LLMs as optimizers to find instructions that maximize task accuracy. OPRO describes the optimization problem in natural language and instructs the LLM to iteratively generate new solutions based on the problem description and previously found solutions. The meta-prompt, which serves as the input to the optimizer LLM, contains previously generated prompts with their corresponding training accuracies and exemplars from the training set to exemplify the task of interest.\n\nOPRO is evaluated on two types of tasks: mathematical optimization problems like linear regression and the traveling salesman problem, and NLP tasks like GSM8K and Big-Bench Hard. The results show that OPRO can find good-quality solutions for small-scale optimization problems and improve the performance of generated prompts on NLP tasks. For instance, on GSM8K, the best prompts optimized by OPRO outperform human-designed prompts by up to 8%, and on Big-Bench Hard tasks, the improvement is up to 50%. The paper also conducts ablation studies to understand the impact of different meta-prompt design choices, such as the order of previous instructions, the effect of instruction scores, and the number of generated instructions per step.\n\n### Commonalities and Innovations\n\nAll three papers address the challenge of prompt engineering and optimization for LLMs, aiming to automate the process of generating high-quality instructions. They leverage the generative capabilities of LLMs to propose and refine prompts, using different strategies to guide the optimization process. APE uses a black-box optimization approach, ProTeGi employs a gradient descent-inspired method, and OPRO describes the optimization problem in natural language and iteratively generates new solutions.\n\nThe common theme across these papers is the use of LLMs to generate and refine prompts, reducing the need for human intervention. Each paper contributes unique insights and methodologies to the field. APE emphasizes the importance of iterative Monte Carlo search and the use of LLMs to generate and score instruction candidates. ProTeGi introduces a novel technique for overcoming the discrete optimization barrier by using natural language gradients and a beam search procedure. OPRO, on the other hand, focuses on leveraging the optimization trajectory in the meta-prompt to improve the generated solutions, providing a framework that can be applied to both mathematical and NLP tasks.\n\n### Comparison of Results and Discussion\n\nThe results from the three papers show that automated prompt engineering and optimization can significantly improve the performance of LLMs on various tasks. APE achieves human-level performance on zero-shot learning with model-generated instructions on 24/24 Instruction Induction and 17/21 Big-Bench tasks. ProTeGi outperforms other state-of-the-art algorithms on four benchmark classification tasks, improving prompt performance by up to 31%. OPRO demonstrates notable improvements in task accuracy, with optimized prompts outperforming human-designed prompts by up to 8% on GSM8K and up to 50% on Big-Bench Hard tasks.\n\nDespite these promising results, there are differences in the performance of the methods across different tasks and datasets. APE and OPRO show consistent improvements across a wide range of tasks, while ProTeGi's performance varies more depending on the specific task and the initial prompt. Additionally, the methods have different strengths and limitations. APE is particularly effective in finding high-quality instructions for zero-shot and few-shot learning scenarios, ProTeGi excels in reducing the variance of the optimization process, and OPRO is robust in leveraging the optimization trajectory to generate new prompts.\n\n### Conclusion\n\nThe three papers collectively demonstrate the potential of LLMs in automating the process of prompt engineering and optimization, reducing the need for manual effort and improving model performance. APE, ProTeGi, and OPRO each offer unique approaches to generating and refining prompts, contributing to the broader goal of making LLMs more controllable and versatile. The results indicate that automated methods can achieve performance comparable to or better than human-engineered prompts, with APE and OPRO showing consistent improvements across various tasks and datasets.\n\nFuture research directions include refining the methods to better balance exploration and exploitation, reducing sensitivity to initialization, and incorporating richer feedback about error cases to guide the optimization process more effectively. Additionally, there is a need to explore the transferability of optimized prompts across different tasks and datasets, as well as to investigate how these methods can be applied to more complex and diverse tasks. The ultimate goal is to create a general-purpose framework for prompt optimization that can be easily adapted to different tasks and datasets, making LLMs more accessible and effective for a wide range of applications."
}