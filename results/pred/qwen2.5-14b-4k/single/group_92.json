{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Self-Alignment with Instruction Backtranslation}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nWe present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named {\\em instruction backtranslation}, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents ({\\em self-augmentation}), and then  selecting high quality examples from among these candidates ({\\em self-curation}).  This data is then used to finetune a stronger model.  Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.\n\n\\end{abstract}\n\n\\section{Introduction}\n\nAligning large language models (LLMs) to perform instruction following typically requires finetuning on large amounts of human-annotated instructions or preferences~\\citep{ouyang2022training,touvron2023llama, bai2022training}  or distilling outputs from more powerful models~\\citep{wang2022self,honovich2022unnatural,alpaca,vicuna2023,peng2023instruction,xu2023wizardlm}.\nRecent work highlights the importance of human-annotation data quality~\\citep{zhou2023lima,kopf2023openassistant}. However, annotating instruction following datasets with such quality is hard to scale. \n\n\\if 0\nAligning large language models (LLMs) to perform generic instruction following typically requires finetuning on large amounts of human-annotated instructions or preferences~\\citep{ouyang2022training,touvron2023llama, bai2022training} or using a stronger LLM in data creation (e.g. via knowledge distillation) or curation~\\citep{wang2022self,honovich2022unnatural,alpaca,vicuna2023,peng2023instruction,xu2023wizardlm}.\n \nRecent work on instruction finetuning highlights the importance of data quality~\\cite{zhou2023lima,kopf2023openassistant}. However, handcrafting instruction following datasets is hard to scale. \n\\fi\n\nIn this work, we instead leverage large amounts of \\emph{unlabelled} data to create a high quality instruction tuning dataset by developing an iterative self-training algorithm. The method uses the model itself to both augment  and curate\nhigh quality  training examples to improve its own performance. Our approach, named {\\em instruction backtranslation}, is inspired by the classic {backtranslation} method from machine translation, in which human-written target sentences are automatically annotated with model-generated source sentences in another language \\citep{sennrich2015improving}. \n\nOur method starts with a seed instruction following model and a web corpus. The model is first used to \\textit{self-augment} its training set: for each web document, it creates an instruction following training example by predicting a  prompt (instruction) that would be correctly answered by (a portion of) that document. Directly training on such data (similarly to \\cite{koksal2023longform}) gives poor results in our experiments, \nboth because of the mixed quality of human written web text, and noise in the generated instructions. To remedy this, we show that the same seed model can be used to \\textit{self-curate}\nthe set of newly created augmentation data by predicting their quality, and  can then be  self-trained on only the highest quality (instruction, output) pairs. \nThe procedure is then iterated, using the improved model to better curate the instruction  data, and re-training to produce a better model.\n\nOur resulting model, {\\em Humpback}, outperforms\nall other existing non-distilled models on the Alpaca leaderboard \\citep{alpaca_eval}. \nOverall, instruction backtranslation is a scalable method for enabling language models to improve their own ability to follow instructions.\n\n\\if 0\n\\begin{itemize}\n\\item We propose a scalable approach to improve LLMs to follow instructions. At the core of our approach is to leverage an seed instruction following model to \\textit{self-augment} and \\textit{self-select} training data to perform self-training. Self-augmentation is performed by creating instruction following training examples from unlabeled data source such as a web corpus. The specific data augmentation steps include generating instructions given outputs, selecting high quality (instruction, output) pairs as self-training examples to improve the next iteration of intermediate instruction following models.\n\n\\item Our method demonstrate more efficient data scaling compared to other hand-crafted and distilled instruction following datasets.\n\n\\item Our method achieves high quality instruction following models evaluated on Alpaca leaderboard, outperforming all other models not relying on distillation data, and with the best data efficiency. \n\n\\item We compare to existing LM alignment approach, and discuss the strengths and weakness of our approach.\n\\end{itemize}\n\\fi \\section{Method}\n\\label{methods}\n\nOur self-training approach assumes access to a base language model, a small amount of seed data, and a collection of unlabelled examples, e.g. a web corpus. The unlabelled data is a large, diverse set of human-written documents which includes writing about all manner of topics humans are interested in -- but crucially is not paired with instructions. \nA \\textbf{first key assumption} is that there exists some subset of this very large human-written text that would be suitable as gold generations for some user instructions.\nA \\textbf{second key assumption} is that we can predict  instructions for these candidate gold answers that can be used as high quality example pairs to train an instruction following model.\n\nOur overall process,  which we call instruction backtranslation, \n thus performs two core steps: \n\\begin{enumerate}[leftmargin=*]\n    \\item {\\em Self-augment}: Generate instructions for unlabelled data, i.e. the web corpus, to produce candidate training data of (instruction, output) pairs for instruction tuning. \n    \\item {\\em Self-curate}: Self-select high quality demonstration examples as training data to finetune the base model to follow instructions. This approach is done iteratively where a better intermediate instruction-following model can improve on selecting data for finetuning in the next iteration.\n\\end{enumerate}\n\nWe describe these steps in more details below. An overview of the approach is illustrated in \\autoref{fig:method}.\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=1.0\\columnwidth]{figs/fuzzy_v3.pdf}\n  \\caption{An overview of our {\\bf instruction backtranslation} method. We start from a base language model, e.g. LLaMa, a small amount of seed examples of (instruction, output) pairs, and a collection of unlabelled documents which are considered candidate outputs for unknown instructions. \\textbf{Self-augmentation}: the base model is finetuned with (output, instruction) pairs from the seed examples as an instruction prediction model\n  $M_{yx}$, which is used to generate candidate instructions for outputs from the unlabelled data. \\textbf{Self-curation}: starting from an intermediate instruction-following model $M_0$ finetuned from seed examples only, it selects high-quality (instruction, output) pairs $\\mathcal{A}_k^{(1)}$ from the candidates from the previous step, and uses them as finetuning data for the next intermediate model $M_1$, which is in turn used to select training data for obtaining $M_2$. }\n  \\label{fig:method}\n\\end{figure}\n\\vspace{-3mm}\n\n\\subsection{Initialization}\n\\paragraph{Seed data.} We start with a seed set of human-annotated (instruction, output) examples that will be used to fine-tune language models to give initial predictions in both directions: predicting an output given an instruction, and an instruction given an output. \n\n\\paragraph{Unlabelled data.} We use a web corpus as a source of unlabelled data.\nFor each document, we perform preprocessing to extract self-contained segments $\\{ y_{i}\\}$, which are portions of text following an HTML header. We further run deduplication, length filtering, and remove potential low quality segments with several heuristics such as the proportion of capitalized letters in the header. \n\n\\subsection{Self-Augmentation (generating instructions)}  \\label{sec:self-augment}\n\nWe finetune the base language model with (output, instruction) pairs $\\{(y_{i}, x_{i})\\}$ from the seed data to obtain a backward model $M_{yx}\\coloneqq p(x|y)$. For each unlabelled example $y_i$, we run inference on the backward model to generate a candidate instruction $\\hat{x_{i}}$ from which we  derive the  candidate augmented paired data $\\mathcal{A} \\coloneqq \\{(\\hat{x_{i}}, y_{i})\\}$.\nAs we will see in experiments, not all of these candidate pairs are of high quality, and in that case using them all for self-training may not be beneficial. We thus consider the important next step of curation of a high quality subset.\n\n\\subsection{Self-Curation (selecting high-quality examples)} \n\nWe select high quality examples using the language model itself. \nWe start with a seed instruction model $M_{0}$ finetuned on (instruction, output) seed examples only. We then use $M_{0}$ to score each augmented example $\\{(\\hat{x}_{i}, y_{i})\\}$ to derive a quality score $a_i$.  This is done using prompting, instructing the trained model to rate the quality of a candidate pair on a 5-point scale. The precise prompt we use is given in \\autoref{table:rating_prompt}.\nWe can then select a subset of the augmented examples with score $a_i \\ge k$ to form a curated set $\\mathcal{A}_k^{(1)}$.\n\n\\paragraph{Iterative self-curation} \nWe further propose an iterative training method to produce higher quality predictions.\nOn iteration $t$ we use the curated augmentation data $\\mathcal{A}_k^{(t-1)}$ from the previous iteration, along with the seed data as training data to finetune an improved model $M_t$. This model in turn can be used to rescore the augmented examples for quality, resulting in an augmentation set $\\mathcal{A}_k^{(t)}$. We perform two iterations of data selection and finetuning to get the final model $M_2$. \n\nWhen combining both seed data and augmented data for finetuning, we use tagging to distinguish these two data sources. Specifically, we append an additional sentence to examples (called ``system prompt\"). We use $S_a \\coloneqq$ ``Answer in the style of an AI Assistant.\" for seed data, and $S_w \\coloneqq$ ``Answer with knowledge from web search.\" for augmented data. This approach is similar to methods used to tag synthetic data for backtranslation in machine translation \\citep{caswell2019tagged}.\n\n\\section{Experiments}\n\\label{results}\n\n\\subsection{Experimental Setup}\n\\label{subsec:exp_setup}\n\n\\paragraph{Seed data.}  We use 3200 examples from the Open Assistant dataset~\\citep{kopf2023openassistant} as human-annotated seed data to train our models. Each example is an (instruction, output) pair $\\{(x_{i}, y_{i})\\}$, chosen from the first turn of the conversation tree. We only sample English language responses that are high quality, based on their human annotated rank (rank 0). \n\n\\vspace{-2mm}\n\\paragraph{Base model \\& finetuning.} We use the pretrained LLaMA model \\citep{touvron2023llama} with 7B, 33B and 65B parameters as the base models for finetuning. During training, we only optimize the loss on the output tokens, not the input tokens, thus deviating from the standard language modeling loss. We use the same hyperparameters as existing supervised finetuning (SFT) methods \\citep{zhou2023lima,touvron2023llama} for most models:  learning rate $1e-5$ which linearly decays to $9e-6$ at the end of training, weight decay 0.1, batch size 32 (examples) and dropout 0.1. For finetuning with less than 3000 examples we use batch size 8 (more details in \\autoref{tab:scaling_details}). We refer to our trained Llama-based  instruction backtranslation model as {\\em Humpback}\\footnote{Due to its relation to camel's backs, but also the large scale nature of whales ( \n\\includegraphics[width=3.3mm]{figs/1f40b.pdf}~{\\footnotesize{$>$}}\n\\includegraphics[width=2.7mm]{figs/1f42a.pdf}~).\n}. For generation, we use nucleus sampling \\citep{holtzman2019curious} with temperature $T=0.7$, $p=0.9$.\n\\vspace{-2mm}\n\\paragraph{Unlabelled data.} We use the English portion of  the Clueweb corpus as the source of unlabelled data~\\citep{overwijk2022clueweb22}.  Among those, we sampled 502k segments.\n\n\\vspace{-2mm}\n\n\\paragraph{Baselines.} The main baselines we compare to are the following  approaches: \n\\vspace{-2mm}\n\\begin{itemize}[leftmargin=*]\n    \\item text-davinci-003 \\citep{ouyang2022training}: an instruction following model based on GPT-3 finetuned with instruction data from human-written instructions, human-written outputs, model responses and human preferences using reinforcement learning (RLHF).\n    \\item LIMA~\\citep{zhou2023lima}: LLaMA models finetuned with 1000 manually selected instruction examples from a mixture of community question \\& answering (e.g. StackOverflow, WikiHow, etc.) and human expert-written instruction and responses. \n    \\item Guanaco \\citep{dettmers2023qlora}: LLaMA models finetuned with 9000 examples from the OpenAssistant dataset. The difference from the 3200 seed examples used in this paper is that Guanaco includes (instruction, output) pairs from all turns while we only used the first-turn.\n\\end{itemize}\n\nWe additionally report comparisons to various other models, e.g. which use data distilled from larger and more powerful models such as GPT-4, but do not consider them as directly comparable to our LlaMa-based approach.\n\n\\paragraph{Evaluation.} We evaluate on test prompts from several sources: Vicuna \\citep{vicuna2023} (80 prompts), Self-instruct \\citep{zhang2023self} (252 prompts), Open Assistant \\citep{kopf2023openassistant} (188 prompts), Koala \\citep{koala_blogpost_2023} (156 prompts), HH\\_RLHF \\citep{bai2022training} (129 prompts), LIMA \\citep{zhou2023lima} (300 prompts), crowdsourced from authors (64 prompts). In total there are 1130 unique prompts, providing a good coverage on a variety of task categories, e.g. writing, coding, mathematical reasoning, information seeking, advice, roleplay, safety, etc. We sample 256 prompts from them excluding those in the AlpacaEval test set as a dev set. We ran both automatic evaluation using AlpacaEval \\citep{alpaca_eval}, which computes the win rate against baseline models based on GPT-4 judgements, as well as human preference evaluation. \n\n\\subsection{Seed and Augmentation Data Statistics} \n\n\\paragraph{Data statistics.} In Table \\ref{tab:train_data_stats} we provide the  statistics of the seed data as well as various versions of the augmented data. We can see that augmented data tends to have longer outputs compared to the seed data, and self-curated higher quality training data ($\\mathcal{A}_4^{(2)}$ and $\\mathcal{A}_5^{(2)}$) has both shorter instructions and outputs among all augmented data, closer to the length of the original seed instruction data.\n\n\\begin{table}[t]\n    \\caption{Statistics of seed, self-augmentation and self-curation finetuning data. Instruction and output lengths are given as the number of characters.\n  \\label{tab:train_data_stats}\n    }\n  \\centering\n  \\small\n  \\begin{tabular}{lccc}\n    \\toprule\n        & \\textbf{\\# examples} & \\textbf{Instruction Length}  &  \\textbf{Output Length}   \\\\\n    \\midrule\n\n  Seed data & 3200  &  148 $\\pm$ 322 & 1072  $\\pm$ 818   \\\\ \n    \\vspace{1mm}\n  Augmented data, $\\mathcal{A}_{5}^{(2)}$  & 41821 & 115  $\\pm$ 175 & 1663  $\\pm$ 616  \\\\\n    \\vspace{1mm}\n   Augmented data, \n  $\\mathcal{A}_{4}^{(2)}$  & 195043 & 206  $\\pm$ 298 & 1985  $\\pm$ 649  \\\\ \n  Augmented data, all  & 502133  & 352  $\\pm$ 134 & 1722  $\\pm$ 653  \\\\  \n    \\bottomrule\n  \\end{tabular}\n  \\vspace{1mm}\n\\end{table}\n\\paragraph{Generated Instructions.}  We conduct the task diversity analysis of the seed data and augmented data using the approach from \\cite{wang2022self}. Figure \\ref{fig:verb_noun_pie} visualizes the distribution of the verb-noun structure of instructions in the seed data and augmented data ($\\mathcal{A}_5^{(2)}$ category) respectively. Similar to the seed data, there are a few head tasks related to writing, information seeking and advice, although the type of content from unlabeled data (article, recipe, description, release, etc.) complements those in the seed data (essay, script, code, story, etc.). The augmented data increases the task diversity especially in the long tail. \n\n\\subsection{Scaling Analysis} \\label{sec:scaling_analysis}\n\\paragraph{Data quality vs. data quantity.} In order to understand the importance of data quality vs. data quantity in learning to follow instructions, we compared finetuning on augmented data of different quality. Specifically, we compared finetuning on augmented data without quality-based selection (w/o curation), self-selected data in $\\mathcal{A}_{4}^{(2)}$ (score $\\geq 4$) and $\\mathcal{A}_{5}^{(2)}$ (score $\\geq 4.5$) categories. Results are shown  in Figure \\ref{fig:data_quality_scaling}. We find that training on augmented data without self-curation does not improve instruction following performance despite scaling up data quantity. However,  training on the high quality portion of the augmented data leads to increasing instruction following performance, with steady improvement as we continue to scale up the amount of augmented data. Prior work proposed the ``superficial alignment hypothesis\", that only a few thousands of high-quality instruction following examples are sufficient for aligning a pretrained base model to follow instructions \\cite{zhou2023lima}. Our results provide a contrasting observation that increasing the quantity of high-quality data provides  further gains (whereas increased quantities of low-quality data does not). \n\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=0.55\\columnwidth]{figs/data_scaling_quality.pdf}\n  \\caption{Evaluating self-augmented data of different data size and quality using self-curation. The y-axis is the win rate against text-davinci-003 when finetuning 7B LLaMa with the given data size and quality. We compare three augmentation datasets:  without self-curation,  $\\mathcal{A}_{4}^{(2)}$ and  $\\mathcal{A}_{5}^{(2)}$ that are progressively smaller augmentation sets but of higher data quality \n  (see \\autoref{tab:train_data_stats}\n  for statistics).\n  Similar to observations in LIMA using human-annotated data \\citep{zhou2023lima}, improving the quality of the training data dramatically improves the quality of the model, despite the smaller dataset size. }\n  \\label{fig:data_quality_scaling}\n\\end{figure}\n\n\\paragraph{Data scaling efficiency.} \nWe compare the performance of various instruction-following models as we alter the amount of instruction following finetune data they use. We measure the win rate of each model against text-davinci-003 when finetuning 7B LLaMa with the given finetune dataset.\nWe also report an estimate of this efficiency using the data scaling coefficient $\\alpha$, which is calculated by fitting empirical data with $w = \\alpha \\log N + C$, where $w$ is the win rate measuring generation quality of the model finetuned on $N$ examples.\n\nWe compare our instruction backtranslation method\n(self-augmentation and self-curation with $k=5$, 2 iterations) to methods using instruction datasets created from different sources.\n\n\\begin{table}[h]\n\\caption{Scaling coefficient $\\alpha$ of representive instruction datasets created using differnet methods and data sources.\n      \\label{tab:scaling_alpha}\n    }\n  \\centering\n  \\begin{tabular}{lll}\n    \\toprule\n     & \\textbf{Source}     &  \\textbf{$\\alpha\\uparrow$ } \\\\\n    \\midrule\n\nHumpback (this work) & OA, self-augmented and self-curated & 6.95 \\\\\nWizardLLM\\tablefootnote{The specific version of the data we used is \\url{https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k/tree/main}.} \\citep{xu2023wizardlm} & Distilled from ChatGPT, GPT-4 (June 2023) & 5.69 \\\\\nAlpaca-GPT4 \\citep{peng2023instruction} & Distilled from GPT-4 (April 2023) & 5.40 \\\\\nVicuna \\citep{vicuna2023} & Distilled from ChatGPT, GPT-4 (June 2023) & 4.53 \\\\\nOpen Assistant (OA) \\citep{kopf2023openassistant} & Human Annotation & 4.43 \\\\\nLIMA \\citep{zhou2023lima} & Human Annotation, Community QA & 2.86 \\\\\nAlpaca \\citep{alpaca} & Distilled from ChatGPT (March 2023) & 1.99 \\\\\nFLAN v2 \\citep{chung2022scaling} & Instruction data for NLP tasks & 0.22 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\nResults are  shown in Figure \\ref{fig:data_scaling_all_7b}, with the estimated scaling coefficient $\\alpha$ summarized in Table \\ref{tab:scaling_alpha}. \nWe find that most distilled instruction datasets have better data efficiency than datasets created from other sources, e.g. NLP tasks (FLAN v2) or extracted from community Q\\&A (LIMA). Both improving instruction diversity (e.g. WizardLLM vs. Vicuna) and response quality (e.g. Alpaca-GPT4 vs. Alpaca) seem to yield better data efficiency. Scaling up augmented data using the $\\mathcal{A}_5$  data achieved both higher instruction following performance and more efficient data scaling. We provide further analysis on jointly scaling data and model size in Appendix \\ref{appendix:additional_analysis}. \n\\begin{figure}\n  \\centering\n  \\includegraphics[width=0.75\\columnwidth]{figs/data_scaling_all_7b.pdf}\n  \\caption{Comparing data efficiency of different instruction tuning datasets. The y-axis is the win rate against text-davinci-003 when finetuning 7B LLaMa with the given instruction tuning dataset.\n  Dashed lines depict models that use distillation from more powerful models to construct data, and methods with solid lines do not.\n  }\n  \\label{fig:data_scaling_all_7b}\n\\end{figure}\n\n\\subsection{Model Quality}\n\\paragraph{AlpacaEval.} We use the automatic evaluation (using GPT-4) from AlpacaEval to evaluate generation quality on 805 prompts from the  Alpaca Leaderboard.  AlpacaEval compares the pairwise win rate against the reference model text-davinci-003. We compare our method's performance among three categories of instruction models: \n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Non-distilled}: LLaMa models trained without relying on any external model (e.g. ChatGPT, GPT-4, etc.) for any form of supervision. Most models in this category heavily rely on human annotated data. \n    \\item \\textbf{Distilled}: models trained with a more powerful external model in the loop, e.g. using data distilled from an external model.\n    \\item \\textbf{Proprietary}: models trained with proprietary data and techniques. \n\\end{itemize}\n\nResults are given in Table \\ref{tab:alpaca_leaderb}. Our method is the top-performing model among non-distilled models at both 65B and 33B model scales. We note that Guanaco and OASST are trained on the same data source as our seed data, but with more annotated examples. We also evaluated Humpback based on LLaMa 2 \\citep{touvron2023llama2} 70B to verify its performance further improves with stronger base model.\n\n\\begin{table}[t]\n    \\caption{\n    Results on the Alpaca leaderboard (win rate over text-davinci-003 evaluated by GPT-4). Humpback outperforms other non-distilled models by a wide margin with efficient data scaling beyond human annotated data. \n  \\label{tab:alpaca_leaderb}\n    }\n    \\small\n  \\centering\n  \\begin{tabular}{cllll}\n    \\toprule\n     &   & \\textbf{Annotated Examples} & \\textbf{Total Examples} & \\textbf{Win Rate \\%}  \\\\\n    \n    \\midrule  \n  \n   \\multirow{4}{4em}{Non-distilled} & Humpback 33B & 3k & 45k & \\textbf{79.84} \\\\\n\n     & OASST RLHF 33B & 161k & 161k & 66.52 \\\\\n     & Guanaco 33B & 9k & 9k & 65.96 \\\\\n    & OASST SFT 33B & 161k & 161k & 54.97 \\\\\n     \\midrule\n      \\multirow{3}{4em}{Non-distilled} & Humpback 65B & 3k & 45k & \\bf{83.71} \\\\\n   & Guanaco 65B & 9k & 9k & 71.80 \\\\\n    & LIMA 65B & 1k & 1k & 62.70  \\\\\n     \\midrule  \n     \\multirow{2}{4em}{Non-distilled} & Humpback 70B & 3k & 45k & 87.94 \\\\\n   & LLaMa2 Chat 70B & 1.4m & 5.7m & \\bf{92.66} \\\\\n   \\midrule\n   \\multirow{4}{4em}{Distilled}  & Vicuna 33B & 140k & 140k & \\bf{88.99} \\\\\n    & WizardLLM 13B & 190k & 190k & 86.32 \\\\\n    & airoboros 65B & 17k & 17k & 73.91 \\\\\n     & Falcon Instruct 40B & 100k & 100k & 45.71 \\\\\n   \n     \\midrule\n  \\multirow{3}{4em}{Proprietary} & GPT-4 & & & \\bf{95.28} \\\\\n   & Claude 2 & & & 91.36 \\\\\n   & ChatGPT & &  & 89.37 \\\\ % re-eval in July\n   & Claude & & & 88.39 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\paragraph{Human Evaluation.} We also conduct human evaluation on the general quality of the model responses on the combined test set described in Section \\ref{subsec:exp_setup}, which covers several existing benchmarks. For each prompt, we present outputs from two models side-by-side, comparing our method to a given baseline model, and ask the human evaluator to choose from three options: 1) output from the first model is significantly better than the second model; 2) output from the second model is significantly better than the first model; 3) there is no significant difference between the two outputs. We randomize the order the models are presented in to avoid position bias. Figure \\ref{fig:human_eval_pref} summarizes the comparison with both open source and proprietary models. We can see that the human preference distribution is roughly consistent with the preference distribution using GPT-4 as the judge from AlpacaEval, corroborating observations from \\citet{alpaca_eval}, \\citet{zhou2023lima} and \\citet{zheng2023judging}.  \n\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=0.65\\columnwidth]{figs/human_eval_pref.pdf}\n  \\caption{Humpback is preferred to both open source (e.g. LIMA\\citep{zhou2023lima} (65B), Guanaco \\citep{dettmers2023qlora} (65B),Falcon-Instruct\\citep{falcon40b}) (40B) and proprietary (e.g. davinci-003\\citep{ouyang2022training} and Claude\\citep{bai2022training}) instruction-tuned models in pairwise human preference judgements.}\n  \\label{fig:human_eval_pref}\n  \\vspace{-3mm}\n\\end{figure}\n\n\\paragraph{Commonsense Reasoning and MMLU.} We evaluate on five commonsense reasoning benchmarks, SIQA \n\\citep{sap2019socialiqa}, PIQA \\citep{bisk2020piqa}, Arc-Easy \\citep{clark2018think}, Arc-Challenge \\citep{clark2018think}, and Openbook QA (OBQA) \\citep{mihaylov2018can}, which measures reasoning ranging from social interactions to grade 3 to 9 science questions. We compute zero-shot accuracy based on perplexity of the correct answer following LLaMa\\citep{touvron2023llama}. We also evaluate on the  massive multitask language understanding (MMLU) \\citep{hendrycks2020measuring} benchmark. The results are summarized in \\autoref{tab:commonsense_eval}. We found that compared to the base model, our model has improved zero-shot performance on social reasoning, challenging science problems which require more reasoning (Arc-C),  Openbook QA and MMLU. Detailed results by domains are included in Appendix \\ref{appendix:additional_analysis}.\n\n\\begin{table}[h]\n  \\caption{Comparison on zero-shot commonsense reasoning and MMLU.\n  \\label{tab:commonsense_eval}\n  }\n  \\centering\n  \\small\n  \\begin{tabular}{lllllll}\n    \\toprule\n        & \\textbf{SIQA} & \\textbf{PIQA}  & \\textbf{Arc-E} & \\textbf{Arc-C} & \\textbf{OBQA} & \\textbf{MMLU}  \\\\\n    \\midrule\n    LLaMA 33B & 50.2  & 82.2 & 80.0 & 54.8 & 58.6 & 49.5 \\\\\n    Humpback 33B & 53.4  & 74.5 & 84.4 & 68.5  & 46.4 & 55.4 \\\\\n    LLaMA 65B & 52.3  & 82.8 & 78.9 & 56.0 & 60.2  & 54.8 \\\\\n    Humpback 65B & 60.4  & 78.9 & 88.7 &  73.0 & 64.0 & 59.0 \\\\\n    \\bottomrule\n  \\end{tabular}\n  \\vspace{1mm}\n\\end{table}\n\n\\subsection{Ablations}\nWe perform further ablation studies to understand the effectiveness of self-augmented data in our method. \n\n\\paragraph{Training on self-augmented data only.} As is shown in Figure \\ref{fig:aug_data_only}, when training on self-augmented data alone (without seed data), and without self-curation, the quality of instruction following does not improve, or even  deteriorates with more data. However, training on the higher quality self-curated data brings improvements as training set size increases. While this self-curated data  does not outperform seed training data scaling alone, when joint training with both seed and self-augmented data we observe large improvements. This indicates that seed data and augmented data are complimentary, where the seed data has the same distribution as the target domain (AI assistant response), while the data from web corpus may enlarge the diversity of the instructions and outputs. In Appendix \\ref{appendix:additional_analysis} provides further qualitative analysis to illustrate the improvement over training with seed data alone.\n\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=0.45\\columnwidth]{figs/data_scaling_bt_only.pdf}\n  \\caption{Combining self-curated data with seed data significantly outperforms using seed data alone. Using augmentation without self-curation performs poorly, showing that curation is critical. \n  }\n  \\label{fig:aug_data_only}\n\\end{figure}\n\n\\paragraph{System prompts.}\nIn Table \\ref{tab:abl_system_prompt}, we disentangle the effects of system prompts in joint finetuning and during inference. We found adding system prompts to distinguish augmented data from seed data is helpful. Interestingly, using a combined system prompt \\{$S_a$, $S_w$\\} at inference time, which concatenates the one for the seed data with the one for augmented data, is better than either no system prompt or using the seed data prompt, despite that the concatenation was not seen during training.  \n\\begin{table}[t]\n\\caption{Effect of system prompt. We report mean win rate and its standard error.\n\\label{tab:abl_system_prompt}\n}\n  \\centering\n  \\begin{tabular}{llc}\n    \\toprule\n     \\textbf{Train} & \\textbf{Inference}    &  \\textbf{Win Rate (\\%)}  \\\\\n    \\midrule\n    $S_a$ for seed data, $S_w$ for augmented data  & \\{$S_a$, $S_w$\\}  & \n66.47 $\\pm$3.04 \\\\ \n\n    \\midrule\n   no system prompt  &  no system prompt & 59.96 $\\pm$3.09   \\\\\n    $S_a$ for seed data, $S_w$ for augmented data  & $S_a$ &  62.69 $\\pm$3.06   \\\\\n    $S_a$ for seed data, $S_w$ for augmented data   & no system prompt &   62.70 $\\pm$3.07   \\\\\n    \n    \\bottomrule\n  \\end{tabular}\n\n\\end{table}\n\n\\section{Related Work}\n\n\\paragraph{Instruction tuning for LLMs.} Our work shares the same goal as the broad category of efforts on finetuning large language models to follow instructions. Early work on instruction tuning mainly focused on NLP tasks, with the finding that finetuning with NLP datasets formatted as instruction-output pairs improves cross-task generalization \\citep{wei2021finetuned,mishra2021cross,sanh2021multitask,wang2022super}. Recent work \\citet{ouyang2022training} extends instruction tuning to a broader range of general tasks, especially incorporating instructions from users of language models.\n\n\\vspace{-2mm}\n\\paragraph{Instruction generation and curation.} A key challenge to enable LLMs to perform general instruction-following is gathering demonstration examples for finetuning. Existing high-quality instruction-following LLMs rely on human annotations in various steps, including writing instructions, writing model responses, providing preferences to indicate desired response, etc. Those instruction sets are often proprietary, one exception being the recent OpenAssistant datasets \\citep{kopf2023openassistant}. Overall, the human annotation approach is difficult to scale since collecting annotations on a wide range of tasks is expensive, time consuming and requires expertise in different domains. \n\nSeveral works have explored using LLMs to generate instructions. Unnatural instructions prompts GPT-3 to generate more instructions given a few in-context seed instructions \\citep{honovich2022unnatural}. Self-instruct \\citep{wang2022self} uses the same approach to generate instructions, as well as outputs for those instructions. They further perform manually engineered filtering rules to remove low-quality instruction-output pairs. \\citet{xu2023wizardlm} generates more complex instructions by creating variants of user instructions sent to ChatGPT.  \n\nAll these approaches use model-generated responses for training data. More similar to our method is the concurrent work  of \\citet{koksal2023longform}, which takes human-written text as a natural response, and uses the LLM to generate the corresponding instruction conditioning on the response. A critical difference in our work is that we show that the self-curation step is vital to improve such a procedure.\nA further difference is that they use distillation via an instruction tuned LLM (InstructGPT) to generate instructions, while our approach does not rely on distilling from a more powerful model in the loop, and is instead an instance of self-alignment. \n\\vspace{-2mm}\n\\paragraph{Self-alignment.} Our work is  an instance of the growing body of work on \\textit{self-alignment}, i.e. utilizing the model to improve itself and  align its response with desired behaviors such as model-written feedback, critique, explanations, etc. Differently to our work, many of these works either construct training data in an unsupervised way\n\\citep{sun2023principledriven,bai2022constitutional}, whereas we augment human-written web pages,\nor they use the model to generate additional context to condition on at inference time to improve the output \\citep{saunders2022self, zhang2023self,madaan2023self}.\n\n\\vspace{-2mm}\n\n\\paragraph{Data quality.}\n\nSeveral approaches have shown that curating high-quality human-written data results in strong performance, for example PALMS \\citep{solaiman2021process} and\nLIMA \\citep{zhou2023lima}. Instead of manually curating high-quality data, our work focus on selecting high-quality using the model itself. In concurrent work, \\cite{chen2023alpagasus} also provides an algorithmic approach to select high quality data. They differ from our work in that they prompt a stronger model (ChatGPT) to score the quality of model generated responses from distillation, while this work scores the quality of human-written data as a response to a self-generated instruction. \n\n\\paragraph{Distillation.} Most finetuned LLaMA models are based on knowledge distillation from ChatGPT or GPT-4, such as Alpaca \\citep{alpaca}, Alpaca-GPT 4\\citep{peng2023instruction}, Vicuna \\citep{vicuna2023}, FalconInstruct \\citep{falcon40b}, OpenChat \\citep{openchat}, UltraChat \\citep{ding2023enhancing}. \nHence, these approaches require that you already have a strong model, but do not provide a recipe for building a strong model from scratch.\nDrawbacks of these approaches are also discussed in \\cite{gudibande2023false}.\n\\section{Conclusion}\nWe proposed a scalable approach to finetune large language models to follow instructions. Our method leverages large amounts of unlabeled data by developing an iterative self-training algorithm that we dub instruction backtranslation. Our method uses the model itself to both augment  and curate\nhigh quality training examples to improve its own performance. On the Alpaca leaderboard, our finetuned models outperform all other non-distilled instruction-following models, while using fewer human annotated examples.\nFuture work should scale this method further by considering larger unlabeled corpora, which our analysis suggests should  yield further gains.\\newpage\n\n\\newpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation}\n\n\\begin{document}\n\n\\maketitle\n\n\\vspace{-0.7cm}\n\\begin{center}\n    \\url{https://github.com/Zheng0428/COIG-Kun}\n\\end{center}\n\\vspace{5pt}\n\\begin{abstract}\nIn this paper, we introduce Kun\\footnote{The dataset is named Kun as Chinese pronunciation of Humpback~\\cite{li2023self}.}, a novel approach for creating high-quality instruction-tuning datasets for large language models (LLMs) without relying on manual annotations. \nAdapting a self-training algorithm based on instruction back-translation and answer polishment, Kun leverages unlabelled data from diverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial dataset of over a million Chinese instructional data points. \nThis approach presents a novel departure from traditional methods by using a self-curation process to refine and select the most effective instruction-output pairs. \nOur experiments with the 6B-parameter Yi model across various benchmarks demonstrate Kun's robustness and scalability.\nOur method's core contributions lie in its algorithmic advancement, which enhances data retention and clarity, and its innovative data generation approach that substantially reduces the reliance on costly and time-consuming manual annotations. \nThis methodology presents a scalable and efficient solution for improving the instruction-following capabilities of LLMs, with significant implications for their application across diverse fields.\n\\end{abstract}\n\\begin{figure*}[t]\n  \\centering\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/flowchart.pdf}\n  \\hfill\n    \\centering\n  \\caption{Overview of \\textit{Answer Polishment}.\nInitially, the Yi base model is fine-tuned using quality seed instruction data to create a label and a primary chat model. The label model then annotates a large amount of primary data, turning it into labeled data. This is filtered and refined by rules and the primary chat model, producing the final dataset. This dataset is used to further train the primary chat model, resulting in an highly efficient final chat model.}\n  \\label{fig:flowchart}\n\\end{figure*}\n\n\\section{Introduction}\n\nThe development of large language models (LLMs) relies on human-annotated datasets, yet the creation of such datasets typically faces scalability issues due to the significant resources required. \nOur study introduces Kun, a novel approach leveraging unlabelled data to create a high-quality instruction-tuning dataset. \nThis method diverges from manual annotations, employing a self-training algorithm that includes a unique process called AP (\\textbf{A}nswer \\textbf{P}olishment),\n\nAP is central to Kun's strategy. \nIt addresses a critical challenge in the Humpback~\\citep{li2023self} method, where raw data, once labeled, are directly used in instruction datasets.\nThe unscreened raw data often mismatches between instructions and responses, as raw data may not inherently align with the instructional context. \nAP refines this raw data, ensuring a tighter correlation between the instructions and responses through a back-translation process. This leads to a dataset where each instruction-output pair is more coherent and contextually relevant.\nUnlike methods dependent on LLMs~\\citep{peng2023instruction,taori2023alpaca,zheng2023judging}, Kun offers an independent and scalable approach to instruction-based training.\n\nWe opt for the 6B-parameter Yi model due to its open-source nature and dependable performance\\footnote{\\url{https://github.com/01-ai/Yi}}. \nIts efficacy is tested and proven across diverse dataset sizes, including widely recognized benchmarks like C-EVAL~\\citep{huang2023ceval} and CMMLU~\\citep{li2023cmmlu}.\nTo evaluate the performance of the model, we design a comprehensive human evaluation which contains 500 prompts from ShareGPT-zh, covering various tasks. Responses generated by our model are compared with those from other models, showcasing the superiority of our \\emph{Kun-52k} variant.\nFurther details can be found in \\ref{sec:import area}.\nAdditionally, we evaluate the quality of our dataset, which includes 1,000 instruction-output pairs each from sources like Wudao~\\citep{Wudao}, Wanjuan~\\citep{he2023wanjuan}, and SkyPile~\\citep{wei2023skywork}. This evaluation, focusing on clarity, feasibility, practicality, and alignment, ensures the high quality of our dataset.\nThe key contributions of our work are:\n\n\\begin{itemize}\n\\item \\textit{Algorithmic Advancement}: AP in Kun enhances data retention and resolves ambiguities, leading to an expanded pool of high-quality data for fine-tuning.\n\n\\item \\textit{Large-scale high quality data creation}: Over a million diverse Chinese instructional data points are produced from sources like Wudao, Wanjuan, and SkyPile, surpassing traditional crowdsourced annotations in quality and reducing reliance on manual annotation.\n\\end{itemize}\n\n\\section{Related Work}\n\n\\textbf{Instruction Tuning} \n\nInstruction tuning is widely recognized as a key technique for activating LLMs to adhere to human conversational norms.\n~\\citep{naturalinstructions,supernaturalinstructions,wang2023interactive}. \nInstruction tuning empowers various domain-specific or task-specific LLMs, including natural language generation evaluation~\\citep{jiang2023tigerscore}, math~\\citep{yue2023mammoth,xu2023wizardlm,azerbayev2023llemma}, code~\\cite{luo2023wizardcoder}, music~\\cite{li2023mertech,deng2023musilingo}, and medicine~\\cite{wang2023huatuo}.\nInstruction tuning not only tailors the models' task-specific responsiveness but also bolsters their cross-task generalization capabilities, thus enhancing performance across various dynamic application contexts~\\citep{weifinetuned,sanh2022multitask,naturalinstructions,supernaturalinstructions}.\nRecent studies have broadened the scope of instruction tuning to encompass a wider array of general tasks, notably incorporating input from users of language models~\\citep{ouyang2022training,peng2023instruction}.\n\nHowever, the open-source community is still lacking high-quality Chinese instruction tuning corpora. Current datasets, like COIG~\\citep{zhang2023chinese}, BELLE~\\citep{ji2023exploring}, MOSS~\\citep{sun2023moss}, and OL-CC~\\citep{olcc}, face issues such as limited scope, poor quality, commercial restrictions, or insufficient coverage. This gap hampers the advancement of LLMs in effectively processing and executing complex Chinese instructions, highlighting the critical need for more diverse and superior-quality datasets.\n\n\\textbf{Self-Improvement of LLMs}\n\n\\begin{wrapfigure}{r}{0.5\\textwidth}\n  \\centering\n  \\includegraphics[width=0.5\\textwidth]{images/three_type_pie_v1.pdf}\n  \\caption{The top 10 categories in each of these three areas: Academic Disciplines, Industry Sectors, Text Type}\n  \\label{fig:type}\n\\vspace{-0.5cm}\n\\end{wrapfigure}\n\nIn fine-tuning LLMs, the availability of extensive, high-quality instructional data is crucial. Presently, the generation of such data mainly relies on human manual annotation, a labor-intensive method that lacks scalability for future data augmentation. \n\nAn alternative approach involves deriving instructional data from more advanced LLMs~\\citep{alpaca}, exemplified by extracting authentic instruction-response sets from dialogues within the GPT series models~\\citep{selfinstruct}.\nA more refined technique utilizes the Self-Instruction framework, autonomously generating additional instructional data from initial seed data. Combined with the Ada-Instruct or the Evol-Instruct framework, this approach can transform basic instructions into complex ones, specifically tailored for distinct tasks~\\citep{cui2023ada,luo2023wizardcoder}.\n\nNevertheless, these instruction generation methodologies all require a robust teacher.The ultimate potential of the model is limited by the teacher's expertise or resource expenditure~\\citep{li2023self}. \nTo overcome this limitation, the SPIN~\\citep{chen2024self} framework incorporates a self-play mechanism, It generates training data from previous iterations, refining its strategy by distinguishing between responses generated autonomously and those derived from human-annotated data. \n\nThis gradual process elevates the LLM from a nascent model to a robust one. Considering the vast amount of knowledge present in web text, Humpback~\\cite{li2023self} introduces a technique based on Instruction Backtranslation. \nThis method allows a base model to independently utilize vast amounts of unlabeled data to generate a high-quality instruction tuning dataset. \nHowever, empirical findings indicate that the effectiveness of this method is still constrained by the seed model's performance and its ability to discern truly high-quality data.\n\n\\section{Method}\n\nOur training methodology necessitates a foundational model, high-quality seed instruction data, and a substantial volume of unlabeled data, with the primary source being web text. Given the extensive content diversity inherent in large-scale web documents, which encompass a wide array of topics such as music, art, technology, etc., reflecting the broad spectrum of human interests, certain subsets of these documents may be more apt for generating instructions. Unlike labeled data, these documents lack predetermined outcomes or objectives.  This method involves the refinement and optimization of data selection during the fine-tuning process of Large Language Models (LLMs), as illustrated in Figure \\ref{fig:flowchart}. This approach allows for the collection of a significant volume of instructional data at a low cost, circumventing the exorbitant expenses associated with manual labor, in a manner that is both academically rigorous and professional.\nOur method consists of two main steps:\n\n\\begin{itemize}\n\\item  Supervised Fine-Tuning (\\textbf{SFT}) with High-Quality Seed Data: This involves using SFT on the base model with high-quality seed data to create two models - the label model for annotating primary data and the primary chat for improving data quality.\n\\item  Quality Assessment and Refinement in Primary Chat: The primary chat assesses and refines the label model's output. This repeated process produces a lot of high-quality data, essential for the primary chat's further training. It leads to a high-performance final chat model, trained extensively with superior data.\n\\end{itemize}\nA more comprehensive explanation of each step is provided subsequently.\n\n\\begin{figure}[ht]\n  \\centering\n    \\centering\n    \\includegraphics[width=0.48\\textwidth]{images/Input_length_v1.pdf}\n  \\hfill\n    \\centering\n    \\includegraphics[width=0.48\\textwidth]{images/Output_length_v1.pdf}\n  \\caption{Length distribution of instructions and outputs based on Yi-6B model}\n  \\label{fig:length}\n\\end{figure}\n\n\\subsection{Initialization}\n\n\\textbf{Seed data.} We use 26,000 instructions and their corresponding outputs as our seed data. Each pair is hand-annotated and undergoes strict quality control to ensure high accuracy and consistency.\n\n\\noindent\n\\textbf{Primary Data.} The primary data originates from three significant Chinese datasets: WuDao, Wanjuan, and SkyPile. These datasets are distinguished by their extensive scale, diverse content, and comprehensive nature. Such characteristics make them ideal for mining golden texts aligned with specific user instructions. To facilitate this, we preprocess these datasets to isolate self-contained segments, denoted as ${y}_{i}$.\n\n\\noindent\n\\textbf{Supervised Fine-Tuning.} We utilize high-quality seed data to execute SFT on the foundational model, yielding two distinct models: the label model and the primary chat model.\n\\begin{itemize}\n\n    \\item Primary Chat Model: This model is fine-tuned utilizing the (instruction-output) pairs {(${x}_{i}$, ${y}_{i}$)} from the seed data. This process creates a forward model, $M_{xy} := p(y \\mid x)$, with $x$ and $y$ maintaining their respective meanings as instructions and outputs.\n\n   \\item Label Model: Conversely, this model undergoes fine-tuning with the (output-instruction) pairs {(${y}_{i}$, ${x}_{i}$)} derived from the seed data, leading to the formation of a backward model, denoted as $M_{yx} := p(x \\mid y)$. In this context, $x$ signifies the instruction, while $y$ denotes the corresponding output.\n   \n\\end{itemize}\n\n\\subsection{Generating Candidate Instructions with the Label Model}\n\nFor each example ${y}_{i}$ in the unlabeled set, we utilize the backward model to infer a candidate instruction, denoted as $\\hat{x}_{i}$. This procedure generates a collection of potential enhanced pairing data, represented as {($\\hat{x}_{i}$, ${y}_{i}$)}. During annotation, initial filtering is based on perplexity (\\textbf{ppl}) and length, and excludes any data exceeding 512 tokens. We also discard data unsuitable for instructional use, like purely descriptive statements, while retaining useful data, such as commands and questions. We apply a filter prompt in this selection, keeping only data that triggers a positive response. The final labeled dataset contains instruction and output components, with instructions from the label model and outputs from primary data, known as candidate labeled data. Figure \\ref{appendix:filter} shows the filter prompt used in this stage.\n\\begin{figure*}[ht]\n  \\centering\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/filter.pdf}\n  \\hfill\n    \\centering\n  \\caption{Filter prompt we use to screen out unsuitable content for instructions.}\n  \\label{appendix:filter}\n\\end{figure*}\n\n\\subsection{Refinement of Candidate Labeled Data Using the Primary Chat Model}\n\nGiven the extensive volume of primary data, not all candidate labeled data derived from it are of uniformly high quality. Our objective is to curate high-quality labeled data, necessitating rigorous filtration of the candidate labeled data.\n\nOur experiments tested two different filtering methods, each with its strengths and weaknesses:\n\\begin{itemize}\n\n    \\item Comprehensive Scoring of Labeled Data: This method evaluates the full labeled data set based on a combined score, including instructions and outputs.\n\n    \\item Focused Scoring of Instruction Component: This technique only assesses the instruction part (output from the label model). High-scoring instructions are chosen, and then the output part of these selected data is refined.\n\\end{itemize}\n\nOur analysis shows that the second method is more effective than the first. In the first method, good outputs are often discarded because of poor instructions from the label model, and the reverse is also true, causing unnecessary exclusions. Moreover, this approach occasionally  retains data with one poor quality instruction because the corresponding output is high quality, and vice versa, leading to uneven data quality and negatively impacting further training.\n\nIn contrast, the second method only scores the instruction component,\nas in instruction tuning for LLM, instructions are often considered more important than outputs,Yet, it doesn't assess the output, sometimes leading to suitable instructions paired with unsuitable outputs. To address this, we use the primary chat model to evaluate and refine the instructions and outputs, ensuring they align well. This approach produces high-quality labeled data. The score and refine prompts we used in this process are shown in Figure \\ref{appendix:score_and_refine}.\n\nUtilizing the substantial volume of top-quality labeled data from these procedures, we further train the main chat model, achieving a high-performance final model, as shown in Experiments.\n\n\\section{Experiments}\n\nIn this section, we comprehensively detail the experimental procedures and methodologies employed in our study.\n\n\\subsection{Experimental Setup}\n\nWe first detail the experimental setup used in our study, covering the base model selection, fine-tuning process, baseline comparisons, and the evaluation methods.\n\n\\subsubsection{Base Model \\& Finetuning}\n\nIn our experiments, we utilize the Yi model with 6B parameters\\footnote{\\url{https://huggingface.co/01-ai/Yi-6B}}, developed by 01.AI, as our foundational language model for fine-tuning. Renowned for its proficiency in both English and Chinese, the Yi series has shown impressive results on global benchmarks like the AlpacaEval Leaderboard~\\citep{dubois2023alpacafarm,alpaca_eval} and SuperCLUE~\\citep{xu2023superclue}.\n\nThe fine-tuning process is carried out using varying sizes of our high-quality, instructionally curated dataset. This phase is executed on a computing setup with 32 Nvidia A800 GPUs, amounting to a total of 192 GPU hours. We adopt a learning rate of 2e-5 and a batch size of 16, aiming for an optimal balance between computational efficiency and model performance. All the models have been fine-tuned with the same number of update steps.\n\n    \n\n\\begin{table*}[htbp]\n\\begin{centering}\n\\small\n\\begin{tabular*}{\\textwidth}{@{\\extracolsep{\\fill}}lccccccc}\n\\toprule[1pt] \n\\multirow{2}{*}{ \\textbf{Source} } \n& \\multicolumn{3}{c}{ \\textbf{Instruction Quality} } & \\multicolumn{3}{c}{ \\textbf{Output Quality} }  \\\\\n & Clarity\\% & Feasibility\\% & Practicality\\% & Excellent\\% & Pass\\% & Fail\\%  \\\\\n\\midrule[1pt] \n\\textbf{Wudao}        & 96.67          & 96.40           & 96.87            & 69.50           & 20.03              & 10.47    \n\\\\\n\\textbf{Wanjuan}    & 98.27          & 97.63           & 96.57            & 85.63           & 11.13              & 3.24    \n\\\\\n\\textbf{Skypile}      & 98.90          & 98.37           & 95.40            & 42.73           & 40.43              & 16.84    \n\\\\ \\midrule \n\\textbf{ALL}     & 97.94          & 97.47            & 96.28            & 66.00           & 23.87              & 10.13    \n\\\\\n\\bottomrule[1pt]\n\\end{tabular*}\n\\end{centering}\n\\caption{Manual Quality Analysis of Synthetic Data Generated by Kun.}\\label{tab:statistics}\n\\end{table*}\n\\subsubsection{Baselines}\n\nFor the Kun dataset, we annotated command data from three sources: Wudao, Wanjuan, and Skypile. Quantitative details of this augmented dataset are provided in Figure \\ref{fig:quantity}. In evaluating the performance of Kun, our study contrasts it with data curated from four prominent Chinese open-sourced datasets, including COIG~\\citep{zhang2023chinese,coig-pc,coig-pc-lite}, OL-CC~\\citep{olcc}, and BELLE~\\citep{ji2023exploring}. These datasets are unique in their composition and focus, providing a comprehensive basis for comparison. \n\n\\subsubsection{Evaluation}\n\\label{sec: model evaluation}\n\\noindent\n\\textbf{Human Evaluation.} To assess the general quality of model responses, we conduct human evaluations using a test set of 500 prompts sampled from ShareGPT-zh. These prompts, derived from real world user inquiries, encompass a diverse array of tasks, such as creative writing, information seeking, providing guidance, logical reasoning, storytelling, problem-solving, etc.\n\nFor the evaluation, responses generated by different models for each prompt are presented side-by-side. Human evaluators are asked to choose their preferred answer, providing a direct comparison of model performance. In total, eight models were compared.For this evaluation, we engage a team of experienced crowdsource annotators, ensuring a balanced and unbiased assessment. Detailed examples that show the comparison process can be found in Figure \\ref{appendix:example}.\n\\noindent\n\\textbf{Standard Benchmarks.} In addition to human evaluations, the models are also assessed using two standard benchmarks for Chinese LLM evaluation: C-EVAL ~\\citep{huang2023ceval} and CMMLU ~\\citep{li2023cmmlu}.\n\nThese evaluation methods, comprising both human judgment and standardized benchmarks, offer a multifaceted perspective on the capabilities of the Kun model, enabling a thorough comparison with existing models and datasets.\n\n\\subsection{Augmentation Data Statistics}\n\nIn this section, we delve into the detailed statistical analysis and diversity assessment of our augmented dataset, as well as the rigorous quality evaluation conducted using human annotators. \nBy exploring the comprehensive scale, varied nature, and assessed quality of the instruction-output pairs, we aim to highlight the robustness and reliability of the data curated for our study.\n\n\\subsubsection{Statistics and Diversity}\n\nOur work involve the purification of approximately 377,592 high-quality instruction-output pairs from the Wudao, Wanjuan, and Skypile datasets.We analyze a 10\\% subset of instructions from the past 20 years, revealing significant temporal diversity with 56\\% of instructions from the recent three years (Figure \\ref{fig:year}). The variation in instruction and output lengths, analyzed using the Yi-6B model, is shown in Figure \\ref{fig:length}, reflecting content complexity.\n\nTo assess instruction diversity, we categorize them into 24 academic disciplines, 16 industry sectors, and 15 text types as per Wikipedia\\footnote{\\url{https://www.wikipedia.org}} using the Qwen-72B-Chat\\footnote{\\url{https://huggingface.co/Qwen/Qwen-72B-Chat}}~\\citep{bai2023qwen}. Repeated for accuracy, this categorization highlights the data's range, as shown in Figure \\ref{fig:type}, where the top 10 categories in each area signify its broad scope.\n\n\\begin{figure}[htbp]\n  \\centering\n\n  \\begin{minipage}{0.4\\textwidth}\n    \\centering\n    \\begin{tabular}{lc}\n      \\toprule[1pt] \n      \\textbf{Source} & \\textbf{Consistency\\%} \\\\\n      \\midrule[1pt] \n      Clarity         & 96.87 \\\\\n      Feasibility     & 97.73 \\\\\n      Practicality    & 97.43 \\\\\n      \\midrule \n      ALL             & 92.13 \\\\\n      \\bottomrule[1pt]\n    \\end{tabular}\n    \\captionof{table}{The proportion of identical evaluations from three assessors on a single dimension. \\textbf{All}: The proportion of consistent assessments across all three dimensions within the same item.}\n    \\label{tab:Consistency}\n  \\end{minipage}\n  \\hfill\n  \\begin{minipage}{0.50\\textwidth}\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{images/heatmap.png}\n    \\caption{Heatmap of Model Comparative Win Rates in Human Evaluations. mix-lite:Kun-26k+COIG-lite.mix-one:Kun-26k+COIG-one}\n    \\label{fig:heatmap}\n  \\end{minipage}\n  \\vspace{-0.5cm}\n\\end{figure}\n\n\\subsubsection{Quality Evaluation}\n\\label{sec: data evaluation}\n\nA critical aspect of our dataset curation process is the rigorous data quality assessment. We conduct a comprehensive quality evaluation of the instruction-output pairs to achieve this. For augmented data curated from each source (Wudao, Wanjuan, and Skypile), we randomly select 1,000 instruction-output pairs, resulting in 3,000 pairs subjected to an independent quality assessment. \n\n\\noindent\n\\textbf{Instruction Quality.} For instruction quality, a team of 24 professional annotators with a bachelor's degree or higher evaluates each instruction across three key dimensions: clarity, feasibility, and practicality. Each aspect is assessed with a simple yes/no answer, providing a straightforward yet effective measure of instruction quality. The evaluation criteria are as follows:\n\\begin{itemize}\n    \\item \\textbf{Clarity:} Evaluators determine whether the instruction was unambiguous and coherent, encompassing necessary information without any vague terms or explanations.\n\n    \\item \\textbf{Feasibility:} Evaluators assess whether the instruction was valid and answerable within the context and scope of the model's capabilities.\n\n    \\item \\textbf{Practicality:} Evaluators judge the relevance of the instruction in everyday scenarios.\n\\end{itemize}\n\n\\noindent\n\\textbf{Output Quality.} The quality of the outputs is evaluated based on their alignment with the instructions. Evaluators are asked to rate each output as \\emph{Excellent}, \\emph{Pass}, or \\emph{Fail}, based on how well it met the requirements and intent of the instruction.\n\nTo ensure objectivity and reliability, three different evaluators evaluate each instruction-output pair.  The consistency rates for the evaluation across the three dimensions of the instructions have all exceeded 90\\%, and the evaluation of the instruction-response are also Consistently. This results demonstrate a significant degree of consistency in their judgments. Further details on evaluating identical ratings are presented in Figure \\ref{fig:Consistency} and Table \\ref{tab:Consistency}. Examples that demonstrate the process of assessing can be found in Figure \\ref{appendix:example}\n﻿\n\n\\begin{figure}[ht]\n    \\centering\n    \\begin{minipage}{0.45\\textwidth}\n        \\centering\n        \\vspace{0.2cm}\n        \\includegraphics[width=\\textwidth]{images/Consistency.pdf}\n        \\caption{Distribution of evaluation identical ratings for instruction-response from three evaluators.}\n        \\label{fig:Consistency}\n    \\end{minipage}\n    \\hfill\n    \\begin{minipage}{0.45\\textwidth}\n        \\centering\n        \\vspace{-0.1cm}\n        \\includegraphics[width=\\textwidth]{images/Data_Quantity.pdf}\n        \\vspace{0.03cm}\n        \\caption{Distribution of Data Sources}\n        \\label{fig:quantity}\n    \\end{minipage}\n\\end{figure}\n\nAs indicated in Table \\ref{tab:statistics}, the instruction quality across all sources is consistently high, suggesting effective formulation and clarity in conveying their purposes. However, the output quality varies more noticeably among the sources. While some sources like Wanjuan exhibite a high percentage of \"Excellent\" outputs, others such as Skypile demonstrate a more diverse distribution of output quality. This section presents the analysis of our experiment results, encompassing human evaluation outcomes and performance on standard benchmarks. \n\n\\subsubsection{Human Evaluation}\n\\label{sec:import area}\n\nOur human evaluation results are illustrated through the heatmap in Figure \\ref{fig:heatmap}, which shows the model vs. model win rates, with color variations indicating the relative performance strengths. \nThe heatmap highlights that the \\emph{Kun-52k} model emerges as the most dominant, followed by the mixed model, showcasing their superior ability to handle a wide range of prompts. \nIn contrast, the baseline models \\emph{COIG-39k}, and \\emph{Belle-52k}, garner lower preference percentages. This suggests that despite their strengths, these models may not align as closely with user expectations or prompt requirements as the Kun models.\nFurther analysis is provided in Appendix A.4.\n\n\\subsubsection{Standard Benchmarks}\nThe performance of the models on standard benchmarks, Table \\ref{tab:benchmark_stats} presents the performance statistics of various models on the C-EVAL and CMMLU benchmarks. As shown, we evaluate numerous models, including different sizes of the Kun model, baseline models, and mixed models. Each model's performance is measured in terms of perplexity and generation quality, providing a comprehensive view of its strengths and weaknesses.\n\nFrom the table, we observe that \\emph{Kun-39k} generally exhibits lower perplexity and higher generation quality, confirming its top-tier performance in language understanding and generation. Interestingly, the mixed model display robust performance, with the mixed model often outperforming \\emph{Kun-52k}. The baseline models and smaller Kun variants present mixed results, excelling in some metrics while falling short in others. \nThese highlight potential areas for further improvement in model training and fine-tuning strategies.\n\n\\begin{table}{}\n\\centering\n\\small\n\\begin{tabular}{lcccc}\n\n\\toprule \n\\multirow{2}{*}{\\textbf{Model}} & \\multicolumn{2}{c}{\\textbf{C-EVAL}} & \\multicolumn{2}{c}{\\textbf{CMMLU}} \\\\\n                                & PPL    & GEN   & PPL    & GEN \\\\\n\n\\midrule\n\nKun-9k                          & 72.95               & 72.96             & 73.53               & 24.53    \\\\\nKun-26k         &73.73     &\\underline{73.26}           & \\definecolor{lightblue}{rgb}{0.68, 0.85, 0.9}\\colorbox{lightblue}{\\underline{75.75}}               & 25.23    \\\\\nKun-39k                         & \\definecolor{lightblue}{rgb}{0.68, 0.85, 0.9}\\colorbox{lightblue}{\\underline{73.84}}               & \\definecolor{lightblue}{rgb}{0.68, 0.85, 0.9}\\colorbox{lightblue}{\\textbf{73.38}}             & 75.68               & 25.31    \\\\\nKun-52k                         & 73.36               & 72.72             & 75.48               & 25.50    \\\\\nKun-100k                        & 72.95               & 72.96             & 73.53               & 24.53    \\\\\nKun-200k                        & 73.36               & 72.07             & 74.81               & 31.81    \\\\\nKun-360k                        & 73.25               & 71.88             & 74.14               & \\definecolor{lightblue}{rgb}{0.68, 0.85, 0.9}\\colorbox{lightblue}{\\textbf{46.58}}    \\\\\n\\midrule\nKun-52k + & & & &\\\\\nCOIG-one-38k          & \\multirow{-2}{*}{73.61}               &\\multirow{-2}{*}{\\textbf{73.38}}     & \\multirow{-2}{*}{75.70}     & \\multirow{-2}{*}{34.84}    \\\\\n\\midrule\nCOIG-38k                & \\textbf{74.09}               & 72.95             &\\textbf{75.92}               & 38.40    \\\\\nBelle-52k                       & 72.31               & 72.56             & 74.73               &\\underline{44.56}    \\\\\nOL-CC-10k                       & 72.00               & 71.62             & 74.97               & 34.38    \\\\\n\\bottomrule\n\\end{tabular}\n\n\\caption{Performance Statistics on Standard Benchmarks.The best results in\neach section are \\textbf{Bold} , the second-best results are \\underline{underlined}, while the results of our best model are in \\definecolor{lightblue}{rgb}{0.68, 0.85, 0.9}\\colorbox{lightblue}{Blue}. PPL:Perplexity  GEN:Generation }\n\n\\label{tab:benchmark_stats}\n\\end{table}\n\n\\section{Conclusion}\nOur approach represents a breakthrough in instruction-tuning for LLMs, utilizing a novel self-training algorithm to leverage over a million quality Chinese instructional data points from diverse sources effectively. \nThis strategy, different from manual annotations, \nnot only enhances the instruction-following capabilities of LLMs but also ensures the high quality and diversity of training data. \nEmpirical evaluations using the 6B-parameter Yi model across benchmarks like C-EVAL, CMMLU, and human evaluations, have demonstrated its robustness and scalability. \nInnovations within our approach, such as AP, have notably improved data retention and clarity, offering a scalable, efficient way to augment LLMs' instructional capabilities. This research not only progresses the field but also broadens LLMs' application scope, offering a novel solution to the challenges in developing instruction-tuning datasets.\n\n\\section{Ethics Statement}\nThis study adheres to the highest ethical standards, ensuring all research activities are conducted with a commitment to responsibility and respect for participant rights. Our ethical policy encompasses data usage, intellectual property rights respect, and research transparency. To safeguard data privacy and security, particularly when handling unlabeled data from sources like Wudao, Wanjuan, and SkyPile, we implement stringent measures to comply with data protection laws, especially concerning personal information. This involves anonymizing and desensitizing data prior to utilization. In terms of intellectual property rights, we ensure that all employed data and generated guiding points adhere to applicable copyright laws and intellectual property agreements, thereby avoiding infringement on any third-party intellectual property rights. Moreover, we pledge to provide a comprehensive account of our research methodology, detailing the processes of data generation, model training, and evaluation, to facilitate reproducibility and validation by the academic community.\n\n\\section{Limitations}\nThis study introduces an innovative methodology for generating data to reduce the reliance on costly and time-consuming manual annotations. However, there are limitations impacting the generalizability and scalability of our findings. \nFirstly, the diversity of data: the methodology, while capable of generating instructional data from a vast pool of unlabeled data, may produce data whose quality and diversity are constrained by the original data source's breadth and caliber. This is particularly relevant when generating instructions for niche or specialized domains, where additional methods might be necessary to guarantee comprehensive coverage and precision. Secondly, the generalization capability of the model: although tests on the 6B-parameter Yi model confirm the methodology's efficacy, its performance and applicability could differ among models of various sizes and tasks. Its effectiveness might require further investigation, especially in smaller models or those not designed for Chinese language processing. Thirdly, the assumptions underlying the algorithm: the study's self-training algorithm relies on instruction back-translation and answer embellishment. These premises might not hold across all instruction or answer types, notably for tasks demanding specialized knowledge or creativity, where the generated instructional data may not adequately reflect complex cognitive processes. Future Directions: Subsequent research should examine the methodology's applicability to large-scale language models across different languages and the generation of high-quality instructional data in specialized fields such as medicine or law. Additionally, advancing self-training algorithms to more effectively manage intricate and specialized instructions represents a crucial avenue for exploration.\n\n\\clearpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Better Alignment with Instruction Back-and-Forth Translation}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nWe propose a new method, instruction back-and-forth translation, to construct high-quality synthetic data grounded in world knowledge for aligning large language models (LLMs). Given documents from a web corpus, we generate and curate synthetic instructions using the backtranslation approach proposed by \\citet{li2023self}, and rewrite the responses to improve their quality further based on the initial documents. Fine-tuning with the resulting (backtranslated instruction, rewritten response) pairs yields higher win rates on AlpacaEval than using other common instruction datasets such as Humpback, ShareGPT, Evol-Instruct, Open Orca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the responses with an LLM outperforms direct distillation, and the two generated text distributions exhibit significant distinction in embedding space.\nFurther analysis shows that our backtranslated instructions are of higher quality than other sources of synthetic instructions, while our responses are more diverse and complex than those obtained from distillation. Overall we find that instruction back-and-forth translation combines the best of both worlds---making use of the information diversity and quantity found on the web, while ensuring the quality of the responses which is necessary for effective alignment.\n\\end{abstract}\n\n\\section{Introduction}\\label{intro}\n\\vspace{-0.25em}\n\\begin{figure*}[t]\n\\centering\n\\hspace{-0.1em}\n\\includegraphics[trim=0 0cm 0 0,clip,width=1.01\\linewidth]\n{fig1.png}\n\\vskip -0.5em\n\\caption{\\textbf{Overview of our proposed pipeline: instruction back-and-forth translation.} (1) We first fine-tune a base model, e.g. Llama-2, on some seed examples from Open Assistant, for the task of instruction generation. We then extract initial candidate responses from a web corpus, e.g. Dolma, and use the fine-tuned model to obtain synthetic instructions that would go with the corresponding responses; (2) We separately obtain an instruction-following model by fine-tuning the same base model on the seed examples, and use it to score the quality of the (synthetic instruction, web-scraped response) pairs; (3) With the highest scoring pairs, we ask an existing aligned model (e.g. Llama-2-chat) to improve the responses further, conditioned on the generated instructions and the initial web texts. Steps (1) and (2) follow \\citet{li2023self} with some modifications (i.e. using preprocessed documents from Dolma instead of parsing raw HTMLs from ClueWeb). We provide a specific data example in the bottom row.\n}\n\\label{fig:main_fig}\n\\vspace{-1em}\n\\end{figure*}\nIn recent years, it is increasingly common for large language models (LLMs) to be deployed through a chat interface to interact with users' queries. This capability is achieved by taking models that have been pre-trained on massive amounts of web-crawled text and fine-tuning them on a relatively smaller set of instruction-response pairs or preferences \\cite{ouyang2022training}. Popular\ninstruction-tuning corpora are often constructed by \\textit{(i)} human annotation and curation \\cite{kopf2024openassistant,DatabricksBlog2023DollyV2,zhou2024lima}, \\textit{(ii)} converting existing texts, e.g. from other NLP tasks \\cite{longpre2023flan} or web crawls \\cite{li2023self,koksal2023longform} to instruction-response pairs, and \\textit{(iii)} distilling knowledge from a model \\cite{vicuna2023,mukherjee2023orca}. \n\nThere are benefits and disadvantages to each of these approaches. While \\textit{(i)} can offer strong baselines \\cite{zhou2024lima, kopf2024openassistant}, the reliance on human annotations makes scaling up these datasets difficult. %As a result, datasets from this approach remain limited in quantity.\nThe success of \\textit{(i)} and \\textit{(ii)} is also more dependent on having an effective filtering mechanism to remove noisy data from humans and the web. On the other hand, \\textit{(iii)} is more cost-effective and scalable; many current state-of-the-art instruction datasets employ distillation. However, distillation alone has also been claimed to be a ``false promise''---\\citet{gudibande2023false} shows that specific capability improvements depend on the coverage of the distilled data and that the performance gains from distilling can plateau quickly. These findings suggest that less noisy and more diverse data is crucial for instruction-tuning to close the gap between open and closed LLMs.\n\nGiven the knowledge breadth that can be found on the internet, generating instruction-response data with backtranslation has been shown to be a promising alternative to distillation \\cite{li2023self}. In particular, the authors parse the ClueWeb corpus \\cite{overwijk2022clueweb22} for self-contained text segments, train a model specifically for instruction generation on Open Assistant data \\cite{kopf2024openassistant}, and generate instructions to go with the text segments. However, this approach relies on high-quality text data provided by ClueWeb, a paid-access corpus containing only the most popular web pages from search results, as well as a list of manually specified rules for parsing raw HTML files into structured responses (which are then used directly for fine-tuning). %This poses accessibility and scalability issues respectively.\n\nIn this work, we also adopt the instruction backtranslation technique but make it more accessible and scalable. We make direct use of preprocessed documents from a large-scale \\textit{open-source} corpus like Dolma \\cite{dolma} and generate instructions via backtranslation accordingly. We find that the quality of our instructions are comparable to those backtranslated from ClueWeb. To make up for the lack of manually designed rules for structuring the response, we experiment with using an LLM to rewrite the response based on the generated instruction and the initial web text. This also allows us to avoid directly distilling and overfitting to an LLM's knowledge. An overview of our pipeline, which we call \\emph{back-and-forth translation}, can be found in Figure \\ref{fig:main_fig}. %We seek to leverage the quality of the response annotations produced via distillation, while still incorporating the information diversity of web-scraped texts.\n\nGiven the same data quantity, fine-tuning Llama-2-70B on the instruction-response pairs from our data generation pipeline improves the AlpacaEval win rate by 3.6\\% compared to using the backtranslation data from previous work \\cite{li2023self}, and by at least 3.2\\% compared to using other existing distillation datasets such as OpenOrca \\cite{OpenOrca}, ShareGPT \\cite{vicuna2023}, Evol-Instruct \\cite{xu2023wizardlm}, Alpaca-GPT4 \\cite{peng2023instruction} and Self-instruct \\cite{wang2022self} (Section \\ref{performance}). By asking a model to rewrite responses based on initial texts extracted from Dolma, we obtain a distribution of responses that interpolates between the original web text distribution and the distribution of outputs distilled from the same model. Fine-tuning on the rewritten responses in turn outperforms fine-tuning on the distilled responses corresponding to the same instructions (Section \\ref{rewrite_vs_distill}). In addition, we offer some insights into how instruction backtranslation and response rewriting affect the quality of instructions and responses respectively, especially in comparison with existing data generation methods (Sections \\ref{instruction_quality} and \\ref{response_quality}). Overall our results suggest that back-and-forth translation offers an effective way to generate instruction-tuning data enriched with diverse information found on the web, while ensuring the quality of the response annotations by having aligned LLMs in the loop.\n\\vspace{-0.25em}\n\\section{Method}\\label{method}\nFigure \\ref{fig:main_fig} shows an overview of our pipeline. Here we describe each step in more detail.\n\\vspace{-0.25em}\n\\subsection{Background: instruction backtranslation}\nOur work is inspired by the backtranslation method from \\citet{li2023self}. In this previous work, the authors fine-tune a base language model on some seed instruction-response pairs ${(x_s, y_s)}$ (e.g. from Open Assistant \\cite{kopf2024openassistant}) to obtain a backward model $M_{yx} := p(x|y)$ that learns to generate instructions. The authors then extract candidate responses ${y_i}$ from .warc files of a web corpus, ClueWeb, using carefully constructed HTML-parsing rules, and augment the responses with corresponding instructions output by the backward model. This yields a set of candidate ${(\\hat{x}_i, y_i)}$ pairs. \n\nThe authors also separately fine-tune the same base language model on the same seed data ${(x_s, y_s)}$ to obtain a forward model $M_{xy} := p(y|x)$ that can follow instructions. This model is then prompted to score candidate ${(\\hat{x}_i, y_i)}$ pairs on a 5-point scale. The final instruction-tuning dataset consists of only score-5 examples. The paper provides ablations to show that this curation step is critical to achieving performance gains, especially with increasing instruction data quantity.\n\nThe two steps derived from \\citet{li2023self} are denoted as \\texttt{(1) Backtranslation} and \\texttt{(2) Filtering} in Figure \\ref{fig:main_fig}.\n\\vspace{-0.25em}\n\\subsection{The rewriting process}\nA major limitation of previous work \\cite{li2023self} \nis the limited availability of high-quality candidate responses. The authors rely on Clueweb \\cite{overwijk2022clueweb22} as the source for unlabelled responses $\\{y_i\\}$, using only highly linked websites visited by a search engine (e.g. Wikipedia, popular news sites) as this offers quality control over the extracted texts $\\{y_i\\}$. However, (i) ClueWeb requires paid access, (ii) the text segments were extracted from raw HTML format, which requires specific preprocessing and may still result in segments that are sub-optimal as responses.\n\nOur work removes the data access restrictions and preprocessing steps by using cleaned documents from an open-source corpus, Dolma \\cite{dolma}, for the initial web-scraped responses. Since these documents come pre-extracted (with all HTML structures removed) and were written for various purposes, they may contain redundant information and the content presentation could also be improved (e.g. by being split into paragraphs). Consequently, we use an LLM to improve these documents to better resemble responses from AI Assistants; this step is denoted as \\texttt{(3) Rewriting} in Figure \\ref{fig:main_fig}. Conditioned on initial text $y_i$ from Dolma and the corresponding backtranslated instruction $\\hat{x}_i$, we prompt an aligned LLM, Llama-2-70B-chat, to rewrite the response to improve its quality ($y_i'$). The full prompt can be found in Appendix \\ref{app:prompt}. \n\nBy default, we apply rewriting to ${(\\hat{x}_i, y_i)}$ pairs that have passed the filtering stage. However, we also experiment with skipping the filtering step, i.e. rewrite responses for \\textit{any} pair even if the forward model finds some web responses and corresponding generated instructions not properly aligned. Overall we find that step \\texttt{(3) Rewriting} is more effective compared to \\texttt{(2) Filtering}, though using both offers complementary performance benefits. We will elaborate on this in Section \\ref{performance}.\n\n\\vspace{-0.25em}\n\\section{Experiment setup}\n\\subsection{Training details}\n\\vspace{-0.25em}\n\\paragraph{Data.} To source the initial web-crawled responses, we use the Common Crawl subset of Dolma v1 \\cite{dolma}. This subset has been preprocessed with quality and content filters, in addition to undergoing deduplication. We additionally filter out documents whose lengths are close to exceeding the context length of Llama-2; this removes about 25\\% of the Common Crawl subset. Besides, we only use data from the head split (which consists of documents with the best perplexity scores), as preliminary experiments show that this split offers better candidate responses than the middle split (see Table \\ref{tab:cc_sources} in Appendix).\n\nFor the seed data used to train the forward and backward models, we follow previous work \\cite{li2023self} and use 3200 examples from the Open Assistant dataset \\cite{kopf2024openassistant}, chosen from the first turn of each conversation tree. Note that the seed data only consists of English language responses that are considered high-quality, based on their human annotated rank (rank 0).\n\\vspace{-0.25em}\n\\paragraph{Model.} We fine-tune a Llama-2-70B base model \\cite{touvron2023llama2} on the seed data to obtain the forward and backward models used in steps (1) and (2) of our pipeline. The rewriting step employs Llama-2-70B-chat by default. We also experiment with using a smaller model (Llama-2-7B-chat) as well as the forward model from step (2) for rewriting, but we observe that the output quality is worse (Appendix \\ref{app:rewrite_ablations}).\nFor performance evaluation, we fine-tune both the 7B and 70B scales of the Llama-2 base model on the resulting instruction-response pairs in a supervised manner. Specific hyperparameters can be found in Appendix \\ref{app:more_train_details}.\n\\vspace{-0.25em}\n\\paragraph{Evaluation.} Given a fine-tuned Llama-2 model, we prompt it to respond to 805 questions from the AlpacaEval benchmark \\cite{alpaca_eval} and report the model's win rate over text-davinci-003 as evaluated by GPT-4 model. We also adopt the length-controlled win rate evaluation from AlpacaEval 2.0 \\cite{dubois2024length}, see Appendix \\ref{app:alpaca2} for more details. Performance on other NLP tasks can be found in Appendix \\ref{app:other_nlp_evals}.\n\n\\subsection{Baselines}\\label{baselines}\nThe other fine-tuning data sources we compare to include:\n\\begin{itemize}[topsep=0pt, itemsep=0pt, leftmargin=8pt, parsep=2pt]\n    \\item \\textbf{Open Orca} \\cite{OpenOrca,mukherjee2023orca}: contains GPT-4-distilled outputs to FLAN tasks \\cite{longpre2023flan}, which are converted from existing NLP datasets using manually crafted templates. The tasks have been augmented with prompting to elicit some form of reasoning during distillation.\n    \\item \\textbf{ShareGPT} \\cite{vicuna2023}: the data comes from ShareGPT.com, where users shared their own conversation logs with ChatGPT. We only take the first instruction and first response from each conversation for fine-tuning.\n    \\item \\textbf{ClueWeb + filtering} \\cite{li2023self}: responses are parsed from HTML files in the ClueWeb corpus \\cite{overwijk2022clueweb22} and do not undergo rewriting. Instructions are generated with the backtranslation approach. After preprocessing and two rounds of curation, previous work produces 41.8K instruction-response pairs in total.\n    \\item \\textbf{Self-instruct} \\cite{wang2022self}: the instructions, inputs and outputs are generated by GPT-3, bootstrapped from a small set of seed tasks.\n    \\item \\textbf{Alpaca-GPT4} \\cite{peng2023instruction}: contains GPT-4-distilled responses to instructions from Alpaca dataset \\cite{taori2023alpaca}. Alpaca's instruction generation seeks to improve over the Self-instruct framework by using different prompts and a more advanced model (text-davinci-003).\n    \\item \\textbf{Evol-Instruct} \\cite{xu2023wizardlm}: starting from the Alpaca instruction set, this work uses a set of evolution prompts to rewrite the instructions to improve their complexity. This evolution process is repeated multiple times, using an eliminator in between to filter out the failed instructions. Responses are then distilled from ChatGPT.\n\\end{itemize}\nWe note that except for the first two, the rest of the baselines employ synthetic instructions. Besides, the majority of these datasets (other than ClueWeb) distill responses from different existing LLMs. We use these GPT-distilled datasets for research-only, non-commercial purposes (i.e. to serve as competitive baselines to compare our method against).\n\n\\vspace{-0.25em}\n\\section{Fine-tuning results}\\label{performance}\nWe validate the effectiveness of our data generation method, by examining the AlpacaEval performance of Llama-2 models fine-tuned on our data versus on other datasets, given the same instruction quantity.\n\\vspace{-0.25em}\n\\subsection{7B scale}\nWe fix the number of samples to be 12.8K and compare fine-tuning Llama-2-7B base model on our backtranslation and rewritten data to fine-tuning on other common baselines described in Section \\ref{baselines}. Results are shown in Table \\ref{tab:7b_results}.\nBelow we describe our different data variations in more detail:\n\\begin{itemize}[topsep=0pt, itemsep=0pt, leftmargin=8pt, parsep=2pt]\n\\item \\textit{Dolma + filtering:} Data obtained from steps (1) and (2) of our pipeline (Figure \\ref{fig:main_fig}). We gather initial responses from the Dolma corpus, generate corresponding instructions with backtranslation, and filter out instruction-response pairs that do not get a score 5 from our forward model (i.e. not well-aligned). Responses do not undergo rewriting for this baseline.\n\\item \\textit{Dolma + rewriting:} Data obtained from steps (1) and (3) of our pipeline (Figure \\ref{fig:main_fig}). We source candidate responses from Dolma, generate instructions with backtranslation and rewrite the responses with Llama-2-70B-chat. No intermediate filtering is done in this case.\n\\item \\textit{Dolma + filtering + rewriting:} Data obtained from going through all the steps of our pipeline (Figure \\ref{fig:main_fig}) as described in Section \\ref{method}.\n\\end{itemize}\nWe find that at this scale, our filtered backtranslation data (\\textit{Dolma + filtering}) outperforms similarly constructed data from previous work \\cite{li2023self} (\\textit{ClueWeb + filtering}). Our best dataset that undergoes both filtering and rewriting yields better win rate than all other baselines. It is worth noting that fine-tuning on rewritten responses from unfiltered instruction-response pairs (\\textit{Dolma + rewriting}) outperforms fine-tuning on initial web-scraped responses that have passed the filtering stage but have not been rewritten (\\textit{Dolma + filtering}). This signals that the rewriting step is more effective than filtering at improving the quality of instruction-tuning data.\n\\begin{table}\n\\centering\n\\begin{adjustbox}{max width=\\textwidth}\n\\renewcommand{\\arraystretch}{1.1}\n\\small\n\\hspace{-0.5em}\n\\begin{tabular}{p{3.55cm}p{1.18cm}p{1.735cm}}\n    \\hline\n    \\textbf{Data source} &  \\textbf{Data size} & \\textbf{Win rate (\\%)} \\\\\n    \\hline\n    Dolma + filtering & 12.8K & 71.70 \\\\\n    Dolma + rewriting & 12.8K & 73.44 \\\\\n    Dolma + filtering + rewriting & 12.8K & \\textbf{74.38} \\\\\n    ClueWeb + filtering & 12.8K & 70.77 \\\\\n    Open Orca & 12.8K & 74.20 \\\\\n    ShareGPT & 12.8K & 72.69 \\\\\n    Evol-Instruct & 12.8K & 72.32 \\\\\n    Alpaca-GPT4 & 12.8K & 71.17 \\\\\n    Self-instruct & 12.8K & 65.11 \\\\\n    \\hline\n\\end{tabular}\n\\end{adjustbox}\n\\vskip -0.5em\n\\caption{\\textbf{Performance of fine-tuning Llama-2-7B.} Given the same data quantity (12.8K), fine-tuning on the instruction-response pairs obtained from instruction back-and-forth translation outperforms fine-tuning on the backtranslated ClueWeb data from previous work \\cite{li2023self}, as well as other common instruction datasets. We also find that the rewriting step is more effective than the filtering step at improving the data quality, and subsequently, the model win rate.}\n\\label{tab:7b_results}\n\\vspace{-1em}\n\\end{table}\n\\vspace{-0.25em}\n\\subsection{70B scale}\nWe also experiment with fine-tuning the Llama-2-70B base model on different variants of the backtranslation data described in the previous section. While the instruction-tuning data generated by previous work \\cite{li2023self} is limited by the amount of high-quality text from the initial web corpus (i.e. ClueWeb), our approach overcomes this limitation with response rewriting. We generate 51.2K instruction-response pairs with our pipeline. In Table \\ref{tab:70b_results}, we find that by simply doing backtranslation on Dolma texts and filtering like previous work (\\textit{Dolma + filtering}), the resulting model slightly lags behind the Humpback model from \\cite{li2023self} (\\textit{ClueWeb + filtering}) in terms of win rate. However, after rewriting responses in the filtered subset (\\textit{Dolma + filtering + rewriting}), we manage to outperform previous work by 3.6\\%. Similar to the 7B scale, we also observe at the 70B scale that fine-tuning on rewritten responses from unfiltered instruction-response pairs (\\textit{Dolma + rewriting}) is more effective than fine-tuning on web-scraped responses that have passed the filter but have not been rewritten (\\textit{Dolma + filtering}).\n\\begin{table}\n\\centering\n\\begin{adjustbox}{max width=\\textwidth}\n\\renewcommand{\\arraystretch}{1.1}\n\\small\n\\hspace{-0.5em}\n\\begin{tabular}{p{3.55cm}p{1.18cm}p{1.735cm}}\n    \\hline\n    \\textbf{Data source} &  \\textbf{Data size} & \\textbf{Win rate (\\%)}\\\\\n    \\hline\n    Dolma + filtering & 51.2K & 87.42 \\\\\n    Dolma + rewriting & 51.2K & 90.52 \\\\\n    Dolma + filtering + rewriting & 51.2K & \\textbf{91.74} \\\\\n    ClueWeb + filtering & 41.8K & 88.18 \\\\\n    Open Orca & 51.2K & 87.31 \\\\\n    ShareGPT & 51.2K & 88.56 \\\\\n    Evol-Instruct & 51.2K & 86.05 \\\\\n    Alpaca-GPT4 & 51.2K & 86.18 \\\\\n    Self-instruct & 51.2K & 78.48 \\\\\n    \\rowcolor{gray!10} Dolma + filtering + rewriting & 25.6K & 90.22 \\\\\n    \\rowcolor{gray!10} Dolma + filtering + distilling & 25.6K & 87.58 \\\\\n    \\hline\n\\end{tabular}\n\\end{adjustbox}\n\\vskip -0.5em\n\\caption{\\textbf{Performance of fine-tuning Llama-2-70B.} While backtranslation data from previous work \\cite{li2023self} is limited by the number of high-quality web pages in Clueweb, our approach relies on Dolma texts and thus has access to many more candidate responses. Similar to the 7B scale results, (i) rewriting is more effective at improving data quality than filtering, (ii) filtering backtranslated instructions and then rewriting the responses does the best and outperforms previous work. We also observe that using an aligned LLM for response rewriting yields better data, and subsequently, win rate, than using the same model for distillation.}\n\\vspace{-1em}\n\\label{tab:70b_results}\n\\end{table}\n\\vspace{-0.25em}\n\\section{Understanding rewritten data quality}\\label{understanding}\nGiven the performance benefits of rewritten data, we analyze how the outputs obtained from rewriting are different from those obtained from distillation. We also analyze the characteristics of the instruction-response pairs resulting from our pipeline compared to other existing datasets.\n\\vspace{-0.25em}\n\\subsection{Rewriting versus Distilling}\\label{rewrite_vs_distill}\nAs rewriting involves asking Llama-2-70B-chat to improve the response quality, conditioned on an initial web-crawled response and a backtranslated instruction, a fundamental question arises: \\emph{does the rewriting process leverage information in the raw text or does it simply distill knowledge stored in Llama-2-70B-chat?}\n\nGiven the same set of backtranslated instructions, we use MAUVE score \\cite{pillutla2021mauve} to quantify the distributional differences among three sets of responses: initial web-scraped responses (from Dolma), rewritten responses, and responses distilled from Llama-2-70B-chat. MAUVE was originally designed to measure the gap between machine- and human-generated texts. This metric computes the area under the curve of divergence frontiers in a quantized space, after embedding text samples with a language model (by default, GPT-2). MAUVE score ranges between 0 and 1; the higher it is, the more similar the text distributions are. In the first row of Table \\ref{tab:mauve_score}, we sample two disjoint sets of 10K distilled responses and find that they exhibit high MAUVE score (0.960) as expected, since they are from the same distribution. Comparing 10K initial responses sourced from Dolma to 10K responses distilled from Llama-2-70B-chat, we observe that these two sets of texts differ significantly, even though they are supposedly responses to the same (backtranslated) queries (MAUVE score = 0.0338). Rewritten responses exhibit some similarity with distilled outputs but there still exists a significant gap between them (MAUVE score = 0.340). %The same applies to the gap between rewritten and raw responses. \nThis suggests that the rewriting process is sufficiently distinct from distillation. We provide some examples of rewritten and distilled responses in Appendix \\ref{app:data_examples}.\n\\begin{table}[]\n\\centering\n\\begin{adjustbox}{max width=\\textwidth}\n\\renewcommand{\\arraystretch}{1.1}\n\\small\n\\begin{tabular}{p{4cm}p{2.1cm}}\n    \\hline\n    \\textbf{Text distributions} & \\textbf{MAUVE score} \\\\\n    \\hline\n    Distilled responses vs. \\newline Distilled responses & 0.960 $\\pm$ 0.002 \\\\\n    \\rowcolor{gray!10} \n    Distilled responses vs. \\newline Rewritten responses &\n0.340 $\\pm$ 0.009 \\\\\n    Distilled responses vs. \\newline Initial web responses  & 0.0338 $\\pm$ 0.0007 \\\\\n    \\hline\n    \\end{tabular}\n\\end{adjustbox}\n\\vskip -0.5em\n\\caption{\\textbf{Rewritten responses interpolate between the initial web-scraped responses and the distillation outputs from the aligned LLM used for rewriting.}\nWe use MAUVE score \\cite{pillutla2021mauve} to measure the distances among the three text distributions: initial responses sourced from Dolma, the rewritten responses and the distilled outputs of Llama-2-70B-chat, all in response to the same instruction set. We find that the rewritten responses appear more similar to the distilled outputs compared to the web texts, though there still exists a substantial gap between the first two distributions.\n}\n\\label{tab:mauve_score}\n\\vspace{-1em}\n\\end{table}\nWe also compare empirical performance of fine-tuning on rewritten data versus distilled data. For the latter, we feed 25.6K instructions randomly sampled from our filtered backtranslated dataset to Llama-2-70B-chat and let the model answer directly. For the former, we use the same set of 25.6K instructions and prompt Llama-2-70B-chat to rewrite the corresponding web-scraped responses. Fine-tuning a Llama-2-70B model on the distilled responses yields lower win rate compared to fine-tuning on the rewritten texts (bottom two rows of Table \\ref{tab:70b_results}). This demonstrates that the rewriting process improves the quality of response data in general, beyond just extracting what an LLM already knows, possibly because rewriting incorporates the information diversity found in web-scraped texts.\n\n\\vspace{-0.25em}\n\\subsection{Instruction quality analysis}\\label{instruction_quality}\n\\paragraph{Measuring instruction quality empirically.   }\n\\begin{figure}[t]\n\\includegraphics[trim=0 0 0 0.4cm,clip,width=0.95\\columnwidth]\n{distill_perf.png}\n\\vspace{-1.5em}\n\\caption{\\textbf{Quality of instruction prompts from various datasets, measured by their ability to distill useful information from a fixed model.} We randomly sample 12.8K instructions from each dataset in our experiments and input them to Llama-2-70B-chat to distill its knowledge. The quality of the instructions is then measured by the performance of a model (Llama-2-7B) fine-tuned on the (instruction, distilled response) pairs. We find that backtranslated instructions surpass other synthetic instruction generation methods (e.g. Alpaca), while still underperforming human-written queries (e.g. ShareGPT). This gap is partly, but not entirely, due to ShareGPT having longer instructions.}\n\\label{fig:distill_perf}\n\\vspace{-1.25em}\n\\end{figure}\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[trim=0 0 0 0,clip,width=0.92\\linewidth]\n{trigrams.png}\n\\vskip -1em\n\\caption{\\textbf{Diversity of instructions and responses from our backtranslation data and other common instruction datasets, as measured by the number of unique trigrams.} We find that \\textit{(i)} while instructions generated in this work are more diverse than synthetic instructions from existing baselines including ClueWeb and Alpaca-GPT4, our instruction diversity still lags behind that of manually-crafted prompts, \\textit{(ii)} in terms of responses, using web-crawled texts sourced from Dolma yields the most diverse responses; rewriting reduces the response diversity slightly, but still offers much more text diversity than distillation.\n}\n\\label{fig:trigrams}\n\\vspace{-1em}\n\\end{figure*}\nWe attempt to isolate the quality of instructions from different datasets by unifying the response distribution to be outputs distilled from the same model. More specifically, we randomly sample 12.8K instructions from our backtranslation data (with and without filtering) as well as from each of the baseline datasets described in Section \\ref{baselines}. We discard all existing answers, and feed each set of 12.8K queries to Llama-2-70B-chat to obtain distilled responses. We then fine-tune a Llama-2-7B model on each set of (instruction, new response) pairs and evaluate the AlpacaEval win rate of the resulting model. In Figure \\ref{fig:distill_perf}, we find that \\textit{(i)} filtered backtranslation instructions (i.e. from Dolma or ClueWeb) outperform other synthetic instruction generation methods (i.e. Alpaca \\cite{taori2023alpaca} and Self-instruct \\cite{wang2022self}), \\textit{(ii)} however, synthetic instructions still lag behind human-written ones, obtained via user interactions with a chat interface (ChatGPT), \\textit{(iii)} instructions constructed from transforming existing NLP datasets (i.e. Open Orca \\cite{OpenOrca,mukherjee2023orca}) do not yield high distillation performance, possibly due to limited and repetitive task types.\n\\vspace{-0.25em}\n\\paragraph{Diversity.}\nAs a proxy for diversity, we measure the number of unique trigrams across different instruction sets, given the same data quantity. In Figure \\ref{fig:trigrams} (left), we observe that while our instruction set (backtranslated from Dolma) is more diverse than other synthetic instruction sets---including ClueWeb, Alpaca and Self-instruct, there is still a significant gap in diversity between model-generated instructions and manually-crafted ones (e.g., Open Orca and ShareGPT).\n\nIt is worth noting that the human-written instructions (e.g. during their interactions with ChatGPT) tend to be substantially longer than synthetic ones, see Table \\ref{tab:average_len} in the appendix. We thus conduct an ablation study to study how much the length factor contributes to the quality of instructions. We filter out instructions in ShareGPT that exceed the maximum length of our backtranslated instructions (i.e. 514 tokens) and among the remaining datapoints, randomly sample 12.8K instructions to repeat the distillation experiment described earlier (Figure \\ref{fig:distill_perf}). This yields a shorter version of ShareGPT (\"ShareGPT (short)\") in which the instructions have about the same average length as our backtranslated instructions (i.e. 69 tokens). We observe that when fixing the output distribution to be distilled responses from Llama-2-70B-chat, fine-tuning on \"ShareGPT (short)\" is less effective than using the original ShareGPT instructions, but still outperforms backtranslated instructions (Figure \\ref{fig:distill_perf}). This suggests that independent of length, human-written instructions are still of higher quality overall compared to synthetic instructions.\n\\vspace{-0.25em}\n\\paragraph{Complexity.} \nFollowing the InsTag Complexity metric employed by \\citet{liu2023makes}, we use the InsTag public tagger \\cite{lu2023instag}, which is a Llama-2-7B model fine-tuned on ChatGPT-generated tags, to automatically tag our text data with all detectable semantics and intentions. We use the average number of unique tags uncovered in instructions, and separately, responses, from each dataset as a proxy for complexity. In Table \\ref{tab:average_tags}, we observe that our Dolma-backtranslated instructions offers higher InsTag complexity than most instruction sets from previous work, including ClueWeb-backtranslated data \\cite{li2023self} and Open Orca \\cite{OpenOrca}. Human-written instructions, i.e. from user interactions with ChatGPT, still yield the highest number of tags on average.\n\\begin{table}\n\\centering\n\\begin{adjustbox}{max width=\\textwidth}\n\\renewcommand{\\arraystretch}{1.1}\n\\small\n\\begin{tabular}{p{3.6cm}p{1.4cm}p{1.4cm}}\n    \\hline\n    \\textbf{Data source} &  \\textbf{Instruction} & \\textbf{Response} \\\\\n    \\hline\n    Dolma + filtering & 5.6 & 8.3 \\\\\n    Dolma + rewriting & 4.8 & 6.8 \\\\\n    Dolma + filtering + rewriting & 5.1 & 6.6 \\\\\n    ClueWeb & 3.7 & 6.6 \\\\\n    Open Orca & 3.5 & 4.9 \\\\\n    Alpaca-GPT4 & 3.6 & 4.4 \\\\\n    Self-instruct & 3.1 & 5.1 \\\\\n    ShareGPT & 6.2 & 5.2 \\\\\n    \\hline\n\\end{tabular}\n\\end{adjustbox}\n\\vskip -0.75em\n\\caption{\\textbf{Average number of semantic and intention tags uncovered in different instruction-tuning datasets by the InsTag model \\cite{lu2023instag}.} While our backtranslated instructions yield fewer tags than human-written ones (i.e. ShareGPT), they exhibit higher complexity than synthetic instructions from other datasets. Besides, we find that responses sourced from or are based on web texts generally have higher InsTag complexity than those obtained via distillation.}\n\\vspace{-1em}\n\\label{tab:average_tags}\n\\end{table}\n\\vspace{-0.25em}\n\\subsection{Response quality analysis}\\label{response_quality}\n\\paragraph{Diversity.} We apply the same diversity analysis as in Section \\ref{instruction_quality} to response data. In Figure \\ref{fig:trigrams} (right), we observe that the initial web responses sourced from Dolma are substantially more diverse than outputs distilled from existing models (e.g. ChatGPT, GPT-4, GPT-3), as well as web texts from ClueWeb. The rewriting process reduces information diversity of these web-scraped responses slightly, but still leads to much more diverse responses than distillation. Overall this analysis demonstrates the importance of including web sources in the data construction process, in order to encourage more diverse instructions \\textit{and} responses, compared to just distilling knowledge from existing LLMs. \n\\vspace{-0.25em}\n\\paragraph{Complexity.} \nApplying the same analysis as in Section \\ref{instruction_quality}, using the average number of semantic and intention tags uncovered by InsTag tagger \\cite{lu2023instag} as a proxy for complexity, we compare the responses from our pipeline (with and without rewriting) to responses from other baseline datasets. In Table \\ref{tab:average_tags}, we find that the initial web-scraped responses from Dolma yield the highest complexity. Rewriting generally reduces the InsTag complexity of the response. However, our rewritten responses are still substantially more complex than most existing response data, which is commonly distilled from high-performing LLMs.\n\nWe also provide an analysis of instruction and response lengths as another quality metric%for different instruction-tuning sets used in our experiments\n. Refer to Appendix \\ref{app:data_stats} for more details.\n\\vspace{-0.25em}\n\\section{Related Work}\n\\vspace{-0.25em}\nWe discuss related papers that construct new instruction-tuning datasets or propose methods to improve existing ones. More in-depth review can be found in \\citet{zhang2023instruction}.\n\\vspace{-0.25em}\n\\paragraph{Human-crafted data.} Open Assistant \\cite{kopf2024openassistant}, Dolly \\cite{DatabricksBlog2023DollyV2} and Super-NI \\cite{wang2022super} are some examples of datasets that contain solely human-generated and human-annotated conversations, covering a range of topics and NLP tasks. These datasets tend to be relatively small in scale due to the expensive costs of manual annotation and verification.\n\nOther papers do not explicitly ask humans to create questions and answer them, but instead re-purpose existing datasets. For example, FLAN \\cite{longpre2023flan} and Natural instructions \\cite{mishra2021cross} transform inputs and outputs of more than 60 NLP tasks into instruction-tuning data. This suffers from the same scalability issue as human-annotated datasets.\n\\vspace{-0.25em}\n\\paragraph{Synthetic instruction generation.} In contrast to sourcing manually written instructions, which may be expensive to scale, some papers propose ways to automatically generate large quantities of instructions \\cite{wang2022self, taori2023alpaca}. In particular, our work is inspired by the backtranslation technique proposed in \\citet{li2023self}, which fine-tunes an LLM specifically for the task of instruction generation, and then applies the model to augment text segments extracted from the web with corresponding instructions. The paper suggests that this approach allows the resulting instruction-tuning data to be more diverse especially in the long tail. Another prior work, LongForm \\cite{koksal2023longform}, introduces a similar approach for generating instructions.\n\nMost related to our approach is the work by \\citet{chen2023tegit}, who train an LLM to generate \\textit{both} instructions and responses from web-scraped documents. %\\footnote{The original version of this work, which appeared last year, }. \nIn contrast to their method, we (i) generate instructions separately with backtranslation and then ask an LLM to improve the existing responses, (ii) obtain better performance with much fewer data (Table \\ref{tab:cc_sources}) (iii) generate more data (51.2K compared to 12.4K), (iv) offer more insights into the quality of our instructions and responses in comparison to other existing datasets. In addition, concurrent work by \\citet{zheng2024kun} also proposes more detailed scoring and refinement prompts to improve the instruction curation and response formatting of the backtranslation pipeline from \\citet{li2023self}, applying it to Chinese text data.\n\\vspace{-0.25em}\n\\paragraph{Distillation.} Perhaps the most common approach in instruction-tuning data generation, distillation seeks to mimic the capabilities of powerful LLMs (e.g. GPT-4) by feeding queries to these models and using the outputs to fine-tune subsequent LLMs. Datasets that are built this way include ShareGPT \\cite{vicuna2023}, OpenInstruct \\cite{wang2023far}, Alpaca-GPT4 \\cite{peng2023instruction} and UltraFeedback \\cite{tunstall2023zephyr}.\n\\vspace{-0.25em}\n\\paragraph{Improving instruction-tuning data quality.} Some prior work studies characteristics of high-quality instruction-tuning data \\cite{liu2023makes} and proposes curation techniques accordingly. LIMA \\cite{zhou2024lima} carefully collects 1K fine-tuning samples via both internet sourcing and human annotation, and shows that strong performance can be achieved despite the small data quantity. Similarly, \\citet{chen2023alpagasus} demonstrates that performance gain is possible by fine-tuning on only a small subset of the original dataset (Alpaca), using ChatGPT as the quality evaluator. \\citet{zhao2024long} finds that selecting only the 1K longest responses from existing datasets offers a very strong baseline, independent of GPT-4's preference for longer texts. Evol-Instruct \\cite{xu2023wizardlm} and Orca \\cite{mukherjee2023orca} manually prompt models to enhance the complexity of instructions, and subsequently, data generation (e.g. by asking for justification). \\citet{fan2024reformatted} reformats the responses of existing instruction data to augment them with relevant information and align them with pre-determined criteria set by humans.\n\\vspace{-0.25em}\n\\section{Discussion}\nWe propose instruction back-and-forth translation: combining instruction backtranslation method from \\citet{li2023self} with response rewriting, in order to benefit from both the information diversity found on the internet and the quality of model annotations, while enabling scalability owing to the size of the web corpus where we source initial responses from. \n\\vspace{-1.5em}\n\\paragraph{Future work.} Our findings motivate a number of interesting future directions. One concrete question is whether applying other existing curation techniques---e.g. quality filters proposed by \\citet{liu2023makes}---to our pool of (synthetic instructions, rewritten response) pairs would lead to further performance gains. In addition, we also look forward to scaling up our data generation pipeline and studying the implication of the rewritten data on the pre-training process, given concurrent work \\cite{maini2024rephrasing} that explores paraphrasing pre-training data into the question-answering format.\n\\vspace{-0.25em}\n\\paragraph{Limitations.} Although we try to control for confounding factors (e.g. data quantity)%or model to distill from in our experiments\n, our findings are only obtained from using one model family, i.e. Llama-2 \\cite{touvron2023llama2}. Besides, our %backtranslation and rewritten \npipeline revolves around general-purpose English instructions, with limited coding or science-related tasks. Nevertheless, it is possible to extend our method to more domain-specific data, e.g. by crawling texts from StackOverflow, generating instructions and rewriting the responses with Code Llama \\cite{roziere2023code}. \n\\vspace{-0.25em}\n\\paragraph{Ethical considerations \\& Potential risks.} It is possible that sourcing response data from the web could affect the factuality of the fine-tuned model and/ or make it more prone to hallucination. The same risks apply to the response rewriting process. Future work could include additional steps to verify the quality of the information in the responses, and check whether it overlaps with what the model already knows, before using the data for fine-tuning.\n\n\\section*{Acknowledgments}\nWe are grateful to Weijia Shi and Mike Lewis for helpful discussion as well as feedback on the\nmanuscript. TN is supported by the UW-Meta AI Mentorship Program.\nThis work is supported in part by NSF grant 2019844.\n\n\\newpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2308.06259v3.tex",
        "arXiv-2401.06477v4.tex",
        "arXiv-2408.04614v2.tex"
    ],
    "group_id": "group_92",
    "response": "### Title: Scalable Methods for Instruction-Following in Large Language Models\n\n### Introduction\n\nThe field of large language model (LLM) instruction-following has seen rapid advancements in recent years, driven by the need to align these models with user instructions to perform a wide array of tasks effectively. Historically, aligning LLMs with instructions has relied heavily on human-annotated datasets, which are labor-intensive and expensive to scale. This has led to a focus on developing more efficient and scalable methods for instruction-following, including knowledge distillation from more powerful models and the use of synthetic data generation techniques.\n\nCurrent progress in the field includes the development of various datasets and methods designed to improve the instruction-following capabilities of LLMs. These methods range from human-annotation and curation to the use of synthetic data generated by LLMs themselves. However, the scalability of these methods remains a significant challenge. Human-annotation methods are resource-intensive and difficult to scale, while distillation methods require access to more powerful models, which may not always be available. Synthetic data generation methods, such as instruction backtranslation, offer a promising alternative but often face issues related to data quality and diversity.\n\nThis summary focuses on three recent papers that propose scalable methods for instruction-following in LLMs. The first paper introduces the concept of instruction backtranslation, a method that uses a seed model to generate and curate high-quality training data from unlabeled web corpora. The second paper, Kun, adapts this method to the Chinese language and introduces an answer polishment process to enhance the quality of generated responses. The third paper, Better Alignment with Instruction Back-and-Forth Translation, further refines the backtranslation method by incorporating a rewriting process to improve the quality of responses, thereby enhancing the overall effectiveness of the instruction-following model.\n\n### Main Content of Each Paper\n\n#### Paper 1: Self-Alignment with Instruction Backtranslation\n\nThis paper introduces a method called instruction backtranslation, which aims to build high-quality instruction-following models without relying on extensive human-annotation. The method leverages a small amount of seed data and a large web corpus to generate and curate training examples. The process involves two main steps: self-augmentation and self-curation.\n\n- **Self-Augmentation**: The seed model is fine-tuned to generate instructions for unlabeled web documents. This step produces a large set of candidate (instruction, output) pairs.\n- **Self-Curation**: The generated pairs are scored for quality using the seed model, and only the highest quality pairs are selected for further training. This iterative process is repeated to improve the quality of the instruction-following model.\n\nThe authors use the LLaMa model with 7B, 33B, and 65B parameters as the base model for fine-tuning. They compare their method to other instruction-following models, including those that rely on distillation from more powerful models. The results show that their method, dubbed Humpback, outperforms non-distilled models on the Alpaca leaderboard, demonstrating the effectiveness of instruction backtranslation in improving model performance.\n\n#### Paper 2: Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation\n\nKun is an adaptation of the instruction backtranslation method to the Chinese language. It introduces an answer polishment process to enhance the quality of responses generated from unlabeled web corpora. The method uses a large-scale open-source corpus, Dolma, to source initial web-scraped responses and applies a pre-trained Yi model to generate instructions and score the quality of the instruction-response pairs.\n\n- **Answer Polishment**: After generating instructions, the method uses an aligned LLM to rewrite responses based on the generated instructions and the initial web text. This step ensures that the responses are coherent and contextually relevant.\n- **Data Curation**: The method curates high-quality instruction-response pairs by filtering out low-quality pairs and rewriting the remaining responses. This process is iterative, with each iteration improving the quality of the data and the model.\n\nThe authors evaluate their method using the 6B-parameter Yi model and find that it outperforms other non-distilled models on benchmarks like C-EVAL and CMMLU. The results indicate that the answer polishment process is critical for improving the quality of responses and ensuring the effectiveness of the instruction-following model.\n\n#### Paper 3: Better Alignment with Instruction Back-and-Forth Translation\n\nThis paper proposes an enhanced version of the instruction backtranslation method, called instruction back-and-forth translation. It combines the backtranslation process with a rewriting step to improve the quality of responses. The method uses the Dolma corpus to source initial web-scraped responses and applies a pre-trained Llama-2 model to generate instructions and score the quality of the instruction-response pairs.\n\n- **Rewriting Process**: After generating instructions, the method uses an aligned LLM to rewrite responses based on the generated instructions and the initial web text. This step ensures that the responses are of higher quality and more diverse than those obtained via distillation.\n- **Data Curation**: The method curates high-quality instruction-response pairs by filtering out low-quality pairs and rewriting the remaining responses. This process is iterative, with each iteration improving the quality of the data and the model.\n\nThe authors evaluate their method using the Llama-2 model and find that it outperforms other instruction-following models on the AlpacaEval leaderboard. The results suggest that the rewriting process is more effective than filtering alone in improving the quality of instruction-response pairs.\n\n### Commonalities and Innovations\n\nAll three papers share the goal of developing scalable methods for instruction-following in LLMs, leveraging unlabeled web corpora to generate and curate high-quality training data. They all use a similar pipeline involving the generation of instructions and the scoring of instruction-response pairs. However, each paper introduces unique innovations to improve the effectiveness of the method.\n\n- **Paper 1** introduces the concept of instruction backtranslation, which uses a seed model to generate and curate training data from unlabeled web corpora. The method is iterative, with each iteration improving the quality of the instruction-following model.\n- **Paper 2** adapts the instruction backtranslation method to the Chinese language and introduces an answer polishment process to enhance the quality of responses. This process ensures that the responses are coherent and contextually relevant, making the method more effective for Chinese instruction-following.\n- **Paper 3** further refines the instruction backtranslation method by incorporating a rewriting process to improve the quality of responses. This step ensures that the responses are of higher quality and more diverse than those obtained via distillation, leading to better performance on the AlpacaEval leaderboard.\n\n### Comparison of Results and Discussion\n\nThe results of the three papers demonstrate the effectiveness of their respective methods in improving the instruction-following capabilities of LLMs. \n\n- **Paper 1** shows that Humpback outperforms other non-distilled models on the Alpaca leaderboard, achieving a win rate of 79.84\\% for the 33B model and 83.71\\% for the 65B model. The method is scalable and efficient, achieving performance gains with fewer human-annotated examples.\n- **Paper 2** demonstrates that the Kun model outperforms other non-distilled models on benchmarks like C-EVAL and CMMLU. The model achieves a win rate of 74.38\\% on the 7B scale and 91.74\\% on the 70B scale, indicating the effectiveness of the answer polishment process.\n- **Paper 3** shows that the instruction back-and-forth translation method outperforms other instruction datasets on the AlpacaEval leaderboard. The method achieves a win rate of 74.38\\% on the 7B scale and 91.74\\% on the 70B scale, indicating the effectiveness of the rewriting process.\n\nThe differences in results can be attributed to the specific innovations introduced in each paper. For instance, the answer polishment process in Kun ensures that the responses are coherent and contextually relevant, leading to better performance on Chinese benchmarks. Similarly, the rewriting process in Paper 3 ensures that the responses are of higher quality and more diverse, leading to better performance on the AlpacaEval leaderboard.\n\n### Conclusion\n\nThe three papers introduce scalable methods for instruction-following in LLMs, leveraging unlabeled web corpora to generate and curate high-quality training data. These methods offer a promising alternative to traditional human-annotation methods, which are resource-intensive and difficult to scale. The results demonstrate that these methods can achieve performance gains comparable to or even surpassing those obtained via distillation from more powerful models.\n\n- **Paper 1** introduces the concept of instruction backtranslation, which uses a seed model to generate and curate training data from unlabeled web corpora. The method is scalable and efficient, achieving performance gains with fewer human-annotated examples.\n- **Paper 2** adapts the instruction backtranslation method to the Chinese language and introduces an answer polishment process to enhance the quality of responses. This process ensures that the responses are coherent and contextually relevant, making the method more effective for Chinese instruction-following.\n- **Paper 3** further refines the instruction backtranslation method by incorporating a rewriting process to improve the quality of responses. This step ensures that the responses are of higher quality and more diverse than those obtained via distillation, leading to better performance on the AlpacaEval leaderboard.\n\nFuture research directions include exploring the application of these methods to more domain-specific tasks and scaling up the data generation pipeline to incorporate larger and more diverse web corpora. Additionally, incorporating existing curation techniques into these methods could lead to further performance gains. The ethical considerations of using web-sourced data for fine-tuning LLMs, such as the potential for hallucination and factuality issues, should also be addressed in future work."
}