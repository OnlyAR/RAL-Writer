{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{\\LARGE \\bf\nOPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication}\n\n\\begin{document}\n\n\\maketitle\n\\thispagestyle{empty}\n\\pagestyle{empty}\n\n\\begin{abstract}\n\nEmploying Vehicle-to-Vehicle communication to enhance perception performance in self-driving technology has attracted considerable attention recently; however, the absence of a suitable open dataset for benchmarking algorithms has made it difficult to develop and assess cooperative perception technologies. To this end, we present the first large-scale open simulated dataset for Vehicle-to-Vehicle perception. It contains over 70 interesting scenes, 11,464 frames, and 232,913 annotated 3D vehicle bounding boxes, collected from 8 towns in CARLA and a digital town of Culver City, Los Angeles. We then construct a comprehensive benchmark with a total of 16 implemented models to evaluate several information fusion strategies~(i.e. early, late, and intermediate fusion) with state-of-the-art LiDAR detection algorithms. Moreover, we propose a new Attentive Intermediate Fusion pipeline to aggregate information from multiple connected vehicles. Our experiments show that the proposed pipeline can be easily integrated with existing 3D LiDAR detectors and achieve outstanding performance even with large compression rates. To encourage more researchers to investigate Vehicle-to-Vehicle perception, we will release the dataset, benchmark methods, and all related codes in \n\\textcolor{blue}{\\href{https://mobility-lab.seas.ucla.edu/opv2v/}{https://mobility-lab.seas.ucla.edu/opv2v/}}.\n\n\\end{abstract}\n\n\\section{INTRODUCTION}\n\nPerceiving the dynamic environment accurately is critical for robust intelligent driving. With recent advancements in robotic sensing and machine learning, the reliability of perception has been significantly improved~\\cite{liu2019auto, he2016deep, qi2017pointnet}, and 3D object detection algorithms have achieved outstanding performance either with LiDAR point clouds~\\cite{voxelnet,pointpillar,Shi_2019_CVPR, Shi2020PVRCNNPF} or multi-sensor data~\\cite{Liang2009, Liang2018}. \n\nDespite the recent breakthroughs in the perception field, challenges remain. When the objects are heavily occluded or have small scales, the detection performance will dramatically drop. Such problems can lead to catastrophic accidents and are difficult to solve by any algorithms since the sensor observations are too sparse. An example is revealed in Fig.~\\ref{fig:overview-a}. Such circumstances are very common but dangerous in real-world scenarios, and these blind spot issues are extremely tough to handle by a single self-driving car. \n\n\\begin{figure*}[!t]\n\\centering\n\\begin{subfigure}[c]{0.9\\linewidth}\n    \\centering{\\includegraphics[width=1\\linewidth]{images/figure1_a_compressed.jpeg}}\n    \\caption{}\n    \\label{fig:overview-a}\n\\end{subfigure}\n\\begin{subfigure}[c]{0.9\\linewidth}\n    \\centering{\\includegraphics[width=1\\linewidth]{images/figure1_b_compressed.jpeg}}\n    \\caption{}\n\\end{subfigure}\n\\caption{Two examples from our dataset. \\emph{Left}: Screenshot of the constructed scenarios in CARLA. \\emph{Middle}: The LiDAR point cloud collected by the ego vehicle. \\emph{Right}: The aggregated point clouds from all surrounding CAVs. The red circles represent the cars that are invisible to the ego vehicle due to the occlusion but can be seen by other connected vehicles. (a): The ego vehicle plans to turn left in a T-intersection and the roadside vehicles block its sight to the incoming traffic. (b): Ego-vehicle's LiDAR has no measurements on several cars because of the occlusion caused by the dense traffic.}\n\\label{fig:overview}\n\\end{figure*}\n\nTo this end, researchers started recently investigating dynamic agent detection in a cooperative fashion, such as USDOT CARMA~\\cite{lochrane2020carma} and Cooper~\\cite{cooper}. By leveraging the Vehicle-to-Vehicle~(V2V) communication technology, different Connected Automated Vehicles~(CAVs) can share their sensing information and thus provide multiple viewpoints for the same obstacle to compensate each other. The shared information could be raw data, intermediate features, single CAV's detection output, and metadata e.g., timestamps and poses. Despite the big potential in this field, it is still in its infancy. One of the major barriers is the lack of a large open-source dataset. Unlike the single vehicle's perception area where multiple large-scale public datasets exist~\\cite{nuscenes2019, sun2020scalability, Geiger2013IJRR}, most of the current V2V perception algorithms conduct experiments based on their customized data~\\cite{Wang2020V2VNetVC, rawaw018, Zhang2021DistributedDM}. These datasets are either too small in scale and variance or they are not publicly available. Consequently, there is no large-scale dataset suitable for benchmarking distinct V2V perception algorithms, and such deficiency will preclude further progress in this research field. \n\nTo address this gap, we present OPV2V, the first large-scale \\textbf{O}pen Dataset for \\textbf{P}erception with \\textbf{V2V} communication. By utilizing a cooperative driving co-simulation framework named OpenCDA~\\cite{xu2021opencda} and CARLA simulator~\\cite{Dosovitskiy17}, we collect 73 divergent scenes with a various number of connected vehicles to cover challenging driving situations like severe occlusions. To narrow down the gap between the simulation and real-world traffic, we further build a digital town of Culver City, Los Angeles with the same road topology and spawn dynamic agents that mimic the realistic traffic flow on it.  Data samples are shown in Fig.~\\ref{fig:overview} and Fig.~\\ref{fig:culver}. We benchmark several state-of-the-art 3D object detection algorithms combined with different multi-vehicle fusion strategies. On top of that, we propose an Attentive Intermediate Fusion pipeline to better capture interactions between connected agents within the network. Our experiments show that the proposed pipeline can efficiently reduce the bandwidth requirements while achieving state-of-the-art performance.\n\n\\section{Related Work}\n\\noindent\\textbf{Vehicle-to-Vehicle Perception: }V2V perception methods can be divided into three categories: early fusion, late fusion, and intermediate fusion. Early fusion methods~\\cite{cooper} share raw data with CAVs within the communication range, and the ego vehicle will predict the objects based on the aggregated data. These methods preserve the complete sensor measurements but require large bandwidth and are hard to operate in real time~\\cite{Wang2020V2VNetVC}. In contrast, late fusion methods transmit the detection outputs and fuse received proposals into a consistent prediction. Following this idea, Rauch \\etal~\\cite{Rauch2012} propose a Car2X-based perception module to jointly align the shared bounding box proposals spatially and temporally via an EKF. In~\\cite{Rawashdeh2018}, a machine learning-based method is utilized to fuse proposals generated by different connected agents. This stream of work requires less bandwidth, but the performance of the model is highly dependent on each agent's performance within the vehicular network. To meet requirements of both bandwidth and detection accuracy, intermediate fusion~\\cite{f-cooper, Wang2020V2VNetVC} has been investigated, where intermediate features are shared among connected vehicles and fused to infer the surrounding objects. F-Cooper~\\cite{f-cooper} utilizes max pooling to aggregate shared Voxel features, and V2VNet~\\cite{Wang2020V2VNetVC} jointly reason the bounding boxes and trajectories based on shared messages. \n\n\\begin{table}[]\n\\centering\n\\begin{tabular}{l|l}\nSensors    & Details                                \\\\ \\hline\n4x Camera  & RGB, $800 \\times 600$ resolution, $110\\degree$ FOV     \\\\ \\hline\n1x LiDAR & \\begin{tabular}[c]{@{}l@{}}$64$~channels, $1.3~M$ points per second, \\\\ $120~m$ capturaing range, $-25\\degree$ to $5\\degree$ \\\\ vertical FOV, $\\pm2~cm$ error\\end{tabular} \\\\ \\hline\nGPS \\& IMU & $20~mm$ positional error, $2\\degree$ heading error \\\\ \\hline\n\\end{tabular}\n\\caption{Sensor specifications.}\n\\label{table:sensor-details}\n\n\\end{table}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=2.8in]{images/sensor_configuration.PNG}\n    \\caption{Sensor setup for each CAV in OPV2V. }\n    \\label{fig:sensor_config}\n\\end{figure}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=3.5in]{images/opv2v_stich2.png}\n    \\caption{Examples of the front camera data and BEV map of two CAVs in OPV2V. The yellow, green, red, and white lanes in the BEV map represent the lanes without traffic light control, under green light control, under red light control, and crosswalks.}\n    \\label{fig:camera_example}\n\\end{figure}\n\n\\noindent\\textbf{Vehicle-to-Vehicle Dataset: }To the best of our knowledge, there is no large-scale open-source dataset for V2V perception in the literature. Some work~\\cite{cooper, f-cooper} adapts KITTI~\\cite{Geiger2013IJRR} to emulate V2V settings by regarding the ego vehicle at different timestamps as multiple CAVs. Such synthetic procedure is unrealistic and not appropriate for V2V tasks since the dynamic agents will appear at different locations, leading to spatial and temporal inconsistency. \\cite{Wang2020V2VNetVC} utilizes a high-fidelity LiDAR simulator~\\cite{Lidarsim} to generate a large-scale V2V dataset. However, neither the LiDAR simulator nor the dataset is publicly available. Recently, several works~\\cite{Zhang2021DistributedDM, marvasti2020cooperative} manage to evaluate their V2V perception algorithms on the CARLA simulator, but the collected data has a limited size and is restricted to a small area with a fixed number of connected vehicles. More importantly, their dataset is not released and difficult to reproduce the identical data based on their generation approach. T\\&J dataset~\\cite{cooper, f-cooper} utilizes two golf carts equipped with 16-channel LiDAR for data collection. Nevertheless, the released version only has 100 frames without ground truth labels and only covers a restricted number of road types. A comparison to existing dataset is provided in Table~\\ref{table:dataset-comparison}.\n\n\\begin{table*}[]\n    \\centering\n    \\caption{Dataset comparison. ($^\\dagger$) The number is reported based on data used during their experiment.\n    ($^{\\dagger\\dagger}$) Single LiDAR resolution's data is counted.\n    ($^\\ddagger$) Ground truth data is not released in the T\\&J dataset and it only has 100 frames and LiDAR data. (-) means that the number is not reported in the paper and can't be found in open dataset. ($^*$) means the data has the format mean$\\pm$std.}\n    \\begin{tabular}{c|ccccc|ccc}\n    \\toprule\n         \\multirow{2}{*}{Dataset}  &  \\multirow{2}{*}{frames}&  GT  &  Dataset  &  CAV  &  cities  &  Code  &  Open Dataset  &  Reproducibility\\& \\\\\n         &    &    3D boxes  &  Size  &  range  &    &    &    &Extensibility\\\\\n         \\hline\n         V2V-Sim~\\cite{Wang2020V2VNetVC}&51,200&-&-&10 $\\pm$ 7$^*$&  $>1$  & &&  \\\\\n         \\cite{Zhang2021DistributedDM}  &  1,310$^\\dagger$  &  -  &  -  &     3, 5 &  1  &    &    &\\\\\n         \\cite{marvasti2020cooperative}  &  6,000$^{\\dagger\\dagger}$  &  -  &  -  &  2  & 1  &  \\checkmark  &    &     \\\\\n         T\\&J~\\cite{cooper, f-cooper}  &  100$^\\ddagger$  &  0$^\\ddagger$  &  183.7MB  &  2  &  1  &  \\checkmark  &  \\checkmark \\\\ \n         OPV2V&11,464 & 232,913 &249.4GB &2.89 $\\pm$ 1.06$^*$& 9&\\checkmark&\\checkmark&\\checkmark\\\\ \n    \\bottomrule\n    \\end{tabular}\n    \\label{table:dataset-comparison}\n\\end{table*}\n\n\\begin{figure}\n    \\centering\n    \\begin{subfigure}[c]{1\\linewidth}\n        \\centering\\includegraphics[width=1\\linewidth]{images/culver_city_1_a.png}\n        \\caption{}\n    \\end{subfigure}\n    \\begin{subfigure}[c]{1\\linewidth}\n        \\centering\\includegraphics[width=1\\linewidth]{images/culver_city_2_a.png}\n        \\caption{}\n    \\end{subfigure}\n    \\caption{A caparison between the real Culver City and its digital town. (a) The RGB image and LiDAR point cloud captured by our vehicle in Culver City. (b) The corresponding frame in the digital town. The road topology, building layout, and traffic distribution are similar to reality.  }\n    \\label{fig:culver}\n\\end{figure}\n\n\\section{OPV2V Dataset}\n\\subsection{Data Collection}\n\\noindent\\textbf{Simulator Selection.} CARLA is selected as our simulator to collect the dataset, but CARLA itself doesn't have V2V communication and cooperative driving functionalities by default. Hence, we employ OpenCDA~\\cite{xu2021opencda}, a co-simulation tool integrated with CARLA and SUMO~\\cite{OlaverriMonreal2018ConnectionOT}, to generate our dataset\\footnote{Codes for generating our dataset have been recently released in https://github.com/ucla-mobility/OpenCDA/tree/feature/data\\_collection}. It is featured with easy control of multiple CAVs, embedded vehicle network communication protocols, and more convenient and realistic traffic management.  \n\n\\noindent\\textbf{Sensor Configuration.} The majority of our data comes from eight default towns provided by CARLA.  Our dataset has on average approximately 3 connected vehicles with a minimum of 2 and a maximum of 7 in each frame. As Fig.~\\ref{fig:sensor_config} shows, each CAV is equipped with 4 cameras that can cover 360\\degree~view together, a 64-channel LiDAR, and GPS/IMU sensors.  The sensor data is streamed at 20 Hz and recorded at 10 Hz. A more detailed description of the sensor configurations is depicted in Table~\\ref{table:sensor-details}.\n\n\\noindent\\textbf{Culver City Digital Town.} To incorporate scenarios that can better imitate real-world challenging driving environments and evaluate models' domain adaptation capability, we further gather several scenes imitating realistic configurations. An automated vehicle equipped with a 32-channel LiDAR and two cameras is sent out to Culver City during rush hour to collect sensing data. Then, we populate the road topology of digital town via RoadRunner~\\cite{roadrunner}, select buildings based on agreement with collected data, and then spawn cars mimicking the real-world traffic flow with the support of OpenCDA. We collect 4 scenes in Culver City with around 600 frames in total (See Fig.~\\ref{fig:culver}). These scenes will be used for validation of models trained with simulated datasets purely generated in CARLA. Future addition of data from real environments is planned and can be added to the model training set.\n\n\\noindent\\textbf{Data Size.} Overall, 11,464 frames~(i.e. time steps) of LiDAR point clouds~(see Fig.~\\ref{fig:overview}) and RGB images~(see Fig.~\\ref{fig:camera_example}) are collected with a total file size of 249.4 GB. Moreover, we also generate Bird Eye View~(BEV) maps for each CAV in each frame to facilitate the fundamental BEV semantic segmentation task.\n\n\\noindent\\textbf{Downstream Tasks.} By default, OPV2V supports cooperative 3D object detection, BEV semantic segmentation, tracking, and prediction either employing camera rigs or LiDAR sensors. To enable users to extend the initial data, we also provide a driving log replay tool\\footnote{The tool can be found  \\textcolor{blue}{\\href{https://github.com/DerrickXuNu/OpenCOOD/tree/feature/log_replay/logreplay}{here}}.}.  along with the dataset. By utilizing this tool, users can define their own tasks~(e.g., depth estimation, sensor fusion) and set up additional sensors~(e.g., depth camera) without changing any original driving events. Note that in this paper, we only report the benchmark results on 3D Lidar-based object detection.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=2in]{images/density_map.png}\n    \\caption{Polar density map in log scale for ground truth bounding boxes. The polar and radial axes indicate the angle and distance (in meters) of the bounding boxes with respect to the ego vehicle. The color indicates the number of bounding boxes~(log scale) in the bin. The darker color means a larger number of boxes in the bin.}\n    \\label{fig:angle_density_map}\n\\end{figure}\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=3.4in]{images/bbx.png}\n    \\caption{\\emph{Left}: Number of points in log scale within the ground truth bounding boxes with respect to radial distance from ego vehicles. \\emph{Right}: Bounding box size distributions. }\n    \\label{fig:bbx_dist}\n\\end{figure}\n\n\\begin{table*}[]\n\\centering\n\\caption{Summary of OPV2V dataset statistics. Traffic density means the number of vehicles spawned around the ego vehicle within a 140m radius and aggressiveness represents the probability of a vehicle operating aggressive overtakes.}\n\\begin{tabular}{l|lllllll}\n\\toprule\nRoad Type &\n  Percentage(\\%) &\n  \\begin{tabular}[c]{@{}l@{}}Length(s)\\\\ mean/std\\end{tabular} &\n  \\begin{tabular}[c]{@{}l@{}}CAV number\\\\   mean/std\\end{tabular} &\n  \\begin{tabular}[c]{@{}l@{}}Traffic density\\\\      mean/std\\end{tabular} &\n  \\begin{tabular}[c]{@{}l@{}}Traffic Speed(km/h)\\\\           mean/std\\end{tabular} &\n  \\begin{tabular}[c]{@{}l@{}}CAV speed(km/h)\\\\           mean/std\\end{tabular} &\n  \\begin{tabular}[c]{@{}l@{}}Aggressiveness\\\\       mean/std\\end{tabular} \\\\ \\hline\n4-way Intersection & 24.5 & 12.5/4.2  & 2.69/0.67 & 29.6/26.1 & 19.3/8.8  & 21.3/10.2 & 0.09/0.30 \\\\\nT Intersection     & 24.1 & 14.3/12.8  & 2.55/1.3 & 27.9/18.65  & 26.3/7.5  & 26.2/10.0  & 0.11/0.32 \\\\\nStraight Segment   & 20.7 & 20.2/12.7 & 3.54/1.21 & 38.0/36.3 & 45.7/14.8 & 54.3/20.1 & 0.82/0.40 \\\\\nCurvy Segment      & 23.3 & 17.8/6.8  & 2.86/0.95 & 19.1/9.2  & 45.8/15.1 & 51.6/19.2 & 0.50/0.51 \\\\\nMidblock           & 4.7  & 10.0/1.3  & 3.00/1.22 & 21.8/8.2  & 45.1/8.3  & 50.7/11.5 & 0.20/0.44 \\\\\nEntrance Ramp      & 2.7  & 9.3/0.9   & 2.67/0.57 & 20.3/2.8  & 54.8/1.7  & 66.7/4.8  & 0.67/0.57 \\\\\nOverall            & 100  & 16.4/9.1  & 2.89/1.06 & 26.5/17.2 & 33.1/15.8 & 37.5/21.0 & 0.34/0.47 \\\\ \\bottomrule\n\\end{tabular}\n\\label{table:over_view}\n\\end{table*}\n\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=0.8\\textwidth]{images/intermedaite_fusion_hd.png}\n\\caption{The architecture of Attentive Intermediate Fusion pipeline. Our model consists of 6 parts: 1) Metadata Sharing: build connection graph and broadcast locations among neighboring CAVs. 2) Feature Extraction: extract features based on each detector's backbone. 3) Compression (optional): use Encoder-Decoder to compress/decompress features. 4) Feature sharing: share (compressed) features with connected vehicles. 5) Attentive Fusion: leverage self-attention to learn interactions among features in the same spatial location. 6) Prediction Header: generate final object predictions.}\n\\label{fig:intermediate}\n\\end{figure*} \n\n\\subsection{Data Analysis}\nAs Table~\\ref{table:over_view} depicts, six distinct categories of road types are included in our dataset for simulating the most common driving scenarios in real life. To minimize data redundancy, we attempt to avoid overlong clips and assign the ego vehicles short travels with an average length of 16.4 seconds,  dissimilar locations,  and divergent maneuvers for each scenario. We also allocate the gathered 73 scenes with diverse traffic and CAV configurations to enlarge dataset variance.  \n\nFig.~\\ref{fig:angle_density_map} and Fig.~\\ref{fig:bbx_dist} reveal the statistics of the 3D bounding box annotations in our dataset.  Generally, the cars around the ego vehicle are well-distributed with divergent orientations and bounding box sizes. This distribution is in agreement with the data collection process where the object positions are randomly selected around CAVs and vehicle models are also arbitrarily chosen. As shown in Fig.~\\ref{fig:angle_density_map}, unlike the dataset for the single self-driving car, our dataset still has a large portion of objects in view with distance $\\geq$ 100m, given that the ground truth boxes are defined with respect to the aggregated lidar points from all CAVs. As displayed in Fig.~\\ref{fig:bbx_dist}, although a single vehicle's LiDAR points for distant objects are especially sparse, other CAVs are able to provide compensations to remarkably boost the LiDAR points density. This demonstrates the capability of V2V technology to drastically increase perception range and provide compensation for occlusions.\n\n\\section{Attentive Intermediate Fusion Pipeline}\nAs sensor observations from different connected vehicles potentially carry various noise levels (e.g., due to distance between vehicles), a method that can pay attention to important observations while ignoring disrupted ones is crucial for robust detection. Therefore, we propose an Attentive Intermediate Fusion pipeline to capture the interactions between features of neighboring connected vehicles, helping the network attend to key observations. The proposed Attentive Intermediate Fusion pipeline consists of 6 modules: Metadata sharing, Feature Extraction, Compression, Feature sharing, Attentive Fusion, and Prediction. The overall architecture is shown in Fig.~\\ref{fig:intermediate}. The proposed pipeline is flexible and can be easily integrated with existing Deep Learning-based LiDAR detectors~(see Table~\\ref{table:benchmark_analysis}). \n\n\\noindent\\textbf{Metadata Sharing and Feature Extraction: }We first broadcast each CAVs' relative pose and extrinsics to build a spatial graph where each node is a CAV within the communication range and each edge represents a communication channel between a pair of nodes. After constructing the graph, an ego vehicle will be selected within the group.\\footnote{During training, a random CAV within the group is selected as ego vehicle while in the inference, the ego vehicle is fixed for a fair comparison.} And all the neighboring  CAVs will project their own point clouds to the ego vehicle's LiDAR frame and extract features based on the projected point clouds. The feature extractor here can be the backbones of existing 3D object detectors.\n\n\\noindent\\textbf{Compression and Feature sharing: }An essential factor in V2V communication is the hardware restriction on transmission bandwidth. The transmission of the original high-dimensional feature maps usually requires large bandwidth and hence compression is necessary.  One key advantage of intermediate fusion over sharing raw point clouds is the marginal accuracy loss after compression~\\cite{Wang2020V2VNetVC}. Here we deploy an Encoder-Decoder architecture to compress the shared message. The Encoder is composed of a series of 2D convolutions and max pooling, and the feature maps in the bottleneck will broadcast to the ego vehicle. The Decoder that contains several deconvolution layers~\\cite{noh2015learning} on the ego-vehicles' side will recover the compressed information and send it to the Attentive Fusion module. \n\n\\noindent\\textbf{Attentive Fusion: }Self-attention models~\\cite{vaswani2017attention} are adopted to fuse those decompressed features. Each feature vector (green/blue circles shown in Fig.\\ref{fig:intermediate}) within the same feature map corresponds to certain spatial areas in the original point clouds. Thus, simply flattening the feature maps and calculating the weighted sum of features will break spatial correlations. Instead, we construct a local graph for each feature vector in the feature map, where edges are built for feature vectors in the same spatial locations from disparate connected vehicles. One such local graph is shown in Fig.\\ref{fig:intermediate} and self-attention will operate on the graph to reason the interactions for better capturing the representative features. \n\n\\noindent\\textbf{Prediction Header: }The fused features will be fed to the prediction header to generate bounding box proposals and associated confidence scores. \n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1.8in]{images/pixor_csa.PNG}\n    \\caption{The architecture of PIXOR with Attentive Fusion. }\n    \\label{fig:pixor}\n\\end{figure}\n\n\\section{Experiments}\n\n\\subsection{Benchmark models}\nWe implement four state-of-the-art LiDAR-based 3D object detectors on our dataset and integrate these detectors with three different fusion strategies i.e., early fusion, late fusion, and intermediate fusion. We also investigate the model performance under a single-vehicle setting, named no fusion, which neglects V2V communication. Therefore, in total 16 models will be evaluated in the benchmark. All the models are implemented in a unified code framework, and  our code and develop tutorial can be found in the \\textcolor{blue}{\\href{https://mobility-lab.seas.ucla.edu/opv2v/}{project website}}.\n\n\\noindent\\textbf{Selected 3D Object Detectors: } \nWe pick SECOND~\\cite{Yan2018SECONDSE}, VoxelNet~\\cite{voxelnet}, PIXOR~\\cite{Yang2018PIXORR3}, and PointPillar~\\cite{pointpillar} as our 3D LiDAR detectors for benchmarking analysis.\n\n\\noindent\\textbf{Early fusion baseline: }All the LiDAR point clouds will be projected into ego-vehicles' coordinate frame, based on the pose information shared among CAVs, and then the ego vehicle will aggregate all received point clouds and feed them to the detector.\n\n\\noindent\\textbf{Late fusion baseline: } Each CAV will predict the bounding boxes with confidence scores independently and broadcast these outputs to the ego vehicle. Non-maximum suppression~(NMS) will be applied to these proposals afterwards to generate the final object predictions.\n\n\\noindent\\textbf{Intermediate fusion: }The Attentive Fusion pipeline is flexible and can be easily generalized to other object detection networks. To evaluate the proposed pipeline, we only need to add the Compression, Sharing, and Attention~(CSA) module to the existing network architecture. Since 4 different detectors add CSA modules in a similar way, here we only show the architecture of intermediate fusion with the PIXOR model as Fig.~\\ref{fig:pixor} displays.  Three CSA modules are added at the 2D backbone of PIXOR to aggregate multi-scale features while all other parts of the network remain the same. \n\n\\begin{table}[]\n    \\centering\n    \\caption{Object detection results on Default CARLA Towns and digital Culver City.}\n    \\begin{tabular}{c|c|cc|cc}\n        \\toprule\n        \\multicolumn{2}{c|}{\\multirow{3}{*}{Method}}&\\multicolumn{2}{c|}{Default}&\\multicolumn{2}{c}{Culver}\\\\\n        \\multicolumn{2}{c|}{}&\\multicolumn{2}{c|}{AP@IoU}&\\multicolumn{2}{c}{AP@IoU}\\\\\n         \\multicolumn{2}{c|}{}&$0.5$&$0.7$&$0.5$&$0.7$ \\\\\n         \\hline\n        \\multirow{4}{*}{PIXOR}&No Fusion&0.635 &0.406 &0.505&0.290\\\\\n        &Late Fusion&0.769&0.578&0.622&0.360\\\\\n        &Early Fusion&0.810&0.678&\\textbf{0.734}&\\textbf{0.558}\\\\\n        &Intermediate Fusion&\\textbf{0.815}&\\textbf{0.687}&0.716&0.549\\\\\n        \\hline\n        \\multirow{4}{*}{PointPillar}&No Fusion&0.679&0.602&0.557&0.471\\\\\n        &Late Fusion&0.858&0.781&0.799&0.668\\\\\n        &Early Fusion&0.891&0.800&0.829&0.696\\\\\n        &Intermediate Fusion&\\textbf{0.908}&\\textbf{0.815}&\\textbf{0.854}&\\textbf{0.735}\\\\\n        \\hline\n        \\multirow{4}{*}{SECOND}&No Fusion&0.713&0.604&0.646&0.517\\\\\n        &Late Fusion&0.846&0.775&0.808&0.682\\\\\n        &Early Fusion&0.877&0.813&0.821&0.738\\\\\n        &Intermediate Fusion&\\textbf{0.893}&\\textbf{0.826}&\\textbf{0.875} &\\textbf{0.760}\\\\\n        \\hline\n        \\multirow{4}{*}{VoxelNet}&No Fusion&0.688&0.526&0.605&0.431\\\\\n        &Late Fusion&0.801&0.738&0.722&0.588\\\\\n        &Early Fusion&0.852&0.758&0.815&0.677\\\\\n        &Intermediate Fusion&\\textbf{0.906}&\\textbf{0.864}&\\textbf{0.854}&\\textbf{0.775}\\\\\n        \\bottomrule\n    \\end{tabular}\n    \\label{table:benchmark_analysis}\n\\end{table}\n\n\\subsection{Metrics}\nWe select a fixed vehicle as the ego vehicle among all spawned CAVs for each scenario in the test and validation set. Detection performance is evaluated near the ego vehicle in a range of $x\\in[-140, 140]m, y\\in[-40, 40]m$. Following \\cite{Wang2020V2VNetVC}, we set the broadcast range among CAVs to be 70 meters. Sensing messages outside of this communication range will be ignored by the ego vehicle. Average Precisions~(AP) at Intersection-over-Union (IoU) threshold of both 0.5 and 0.7 are adopted to assess different models. Since PIXOR ignores the $z$ coordinates of the bounding box, we compute IoU only on x-y plane to make the comparison fair. For the evaluation targets, we include vehicles that are hit by at least one LiDAR point from any connected vehicle. \n\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=0.37\\textwidth]{images/cav_num.png}\n\\caption{Average Precision at IoU=0.7 with respect to CAV number.}\n\\label{fig:cav_num}\n\\end{figure}\n\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=0.37\\textwidth]{images/COMPRESSION.png}\n\\caption{Average Precision at IoU=0.7 with respect to data size in log scale based on VoxelNet detector. The number$\\times$ refers to the compression rate.}\n\\label{fig:compression}\n\\end{figure}\n\\subsection{Experiment Details}\nThe train/validation/test splits are 6764/1981/2719 frames. The testing frames contain all road types and are further split into two parts--CARLA default maps and Culver City digital town. For each frame, we assure that the minimum and maximum numbers of CAVs are 2 and 7 respectively. We use Adam Optizer~\\cite{kingma2014adam} and early stop to train all models, and it takes us 14 days to finish all training on  4 RTX 3090 GPUs.\n\n\\subsection{Benchmark Analysis}\nTable~\\ref{table:benchmark_analysis} depicts the performance of the selected four LiDAR detectors combined with different fusion strategies. All fusion methods achieve $\\geq$10\\% AP gains at IoU 0.7 over no fusion counterparts for both default CARLA towns and Culver City, showing the advantage of aggregating information from all CAVs for V2V perception. Generally, because of the capability of preserving more sensing measurements and visual cues, early fusion methods outperform late fusion methods. Except for PIXOR at Culver City, intermediate fusion achieves the best performance on both testing sets compared with all other methods. We argue that the AP gains over early fusion originate from the mechanism of the self-attention module, which can effectively capture the inherent correlation between each CAV's perception information. It is also worth noting that the prediction results for Culver City are generally inferior to CARLA towns. Such a phenomenon is expected as the traffic pattern in Culver City is more similar to real life, which causes a domain gap with the training data. Furthermore, we collect the Culver City data in a busy hour under a very congested driving environment, which leads to vastly severe occlusions and makes the detection task very challenging.\n\n\\subsection{Effect of CAV Quantity}\nWe explore the detection performance as affected by the number of CAVs in a complex intersection scenario where 150 vehicles are spawned in the surrounding area. A portion of them will be transformed into CAVs that can share information. We gradually increase the number of the CAVs up to 7 and apply VoxelNet with different fusion methods for object detection. As shown in Fig.~\\ref{fig:cav_num}, the AP has a positive correlation with the number of CAVs. However, when the quantity reaches 4, the increasing rate becomes lower. This can be due to the fact that the CAVs are distributed on different sides of the intersection and four of them can already provide enough viewpoints to cover most of the blind spots. Additional enhancements with 5 or more vehicles come from denser measurements on the same object.\n\n\\subsection{Effect of Compression Rates}\nFig.~\\ref{fig:compression} exhibits the data size needed for a single transmission between a pair of vehicles and corresponding AP for all fusion methods on the testing set in CARLA towns. We pick VoxelNet for all fusion methods here and simulate distinct compression rates by modifying the number of layers in Encoder-Decoder. By applying a straightforward Encoder-Decoder architecture to squeeze the data, the Attentive Intermediate Fusion obtains an outstanding trade-off between the accuracy and bandwidth. Even with a 4096x compression rate, the performance still just drop marginally~(around 3\\%) and surpass the early fusion and late fusion. Based on the V2V communication protocol~\\cite{arena2019overview}, data broadcasting can achieve 27 Mbps at the range of 300~m. This represents that the time delay to deliver the message with a 4096x compression rate is only about 5~ms. \n\n        \n        \n\n\\section{CONCLUSIONS}\nIn this paper, we present the first open dataset and benchmark fusion strategies for V2V perception. We further come up with an Attentive Intermediate Fusion pipeline, and the experiments show that the proposed approach can outperform all other fusion methods and achieve state-of-the-art performance even under large compression rates. \n\nIn the future, we plan to extend the dataset with more tasks as well as sensors suites and investigate more multi-modal sensor fusion methods in the V2V and Vehicle-to-infrastructure (V2I) setting. We hope our open-source efforts can make a step forward for the standardizing process of the V2V perception and encourage more researchers to investigate this new direction. \n\n\\addtolength{\\textheight}{-3cm}   % This command serves to balance the column lengths\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{V2X-Sim: Multi-Agent Collaborative Perception \\\\Dataset and Benchmark for Autonomous Driving}\n\n\\begin{document}\n\n\\author{Yiming Li$^{1}$, Dekun Ma$^{1}$, Ziyan An$^{1}$, Zixun Wang$^{1}$, Yiqi Zhong$^{2}$,  Siheng Chen\\textsuperscript{3, \\ding{41}}, and Chen Feng\\textsuperscript{1, \\ding{41}}% <-this % stops a space\n\\\\\n\\thanks{Manuscript received February 24, 2022; Revised June 7, 2022; Accepted June 30, 2022.}%Use only for final RAL version\n\\thanks{This paper was recommended for publication by Editor Cesar Cadena upon evaluation of the Associate Editor and Reviewers' comments. This work was supported by the NSF CPS program under CMMI-1932187, the National Natural Science Foundation of China under Grant 62171276, the Science and Technology Commission of Shanghai Municipal under Grant 21511100900 and CALT Grant 2021-01.}\n\\thanks{\\ding{41} Corresponding author.}% <-this % stops a space\n\\thanks{$^{1}$Yiming Li, Dekun Ma, Ziyan An, Zixun Wang, and Chen Feng are with are with New York University,\nBrooklyn, NY 11201, USA  {\\tt\\small cfeng@nyu.edu}}%\n\\thanks{$^{2}$Yiqi Zhong is with University of Southern California, Los Angeles, USA {\\tt\\small yiqizhon@usc.edu}}%\n\\thanks{$^{3}$Siheng Chen is with Cooperative Medianet Innovation Center, Shanghai Jiao Tong University and Shanghai AI Laboratory, Shanghai, China {\\tt\\small sihengc@sjtu.edu.cn}}%\n\\thanks{Digital Object Identifier (DOI): see top of this page.}\n}\n\n\\title{V2X-Sim: Multi-Agent Collaborative Perception \\\\Dataset and Benchmark for Autonomous Driving\n}\n\n\\markboth{IEEE Robotics and Automation Letters. Preprint Version. JULY, 2022}\n{LI \\MakeLowercase{\\textit{et al.}}: V2X-Sim: Multi-Agent Collaborative Perception Dataset and Benchmark for Autonomous Driving} \n\n\\maketitle\n\n\\begin{abstract}\n\tVehicle-to-everything (V2X) communication techniques enable the collaboration between vehicles and many other entities in the neighboring environment, which could fundamentally improve the perception system for autonomous driving. However, the lack of a public dataset significantly restricts the research progress of collaborative perception. To fill this gap, we present V2X-Sim, a comprehensive simulated multi-agent perception dataset for V2X-aided autonomous driving. V2X-Sim provides: (1) \\hl{multi-agent} sensor recordings from the road-side unit (RSU) and multiple vehicles that enable collaborative perception, (2) multi-modality sensor streams that facilitate multi-modality perception, and (3) diverse ground truths that support various perception tasks. Meanwhile, we build an open-source testbed and provide a benchmark for the state-of-the-art collaborative perception algorithms on three tasks, including detection, tracking and segmentation. V2X-Sim seeks to stimulate collaborative perception research for autonomous driving before realistic datasets become widely available.  Our dataset and code are available at \\url{https://ai4ce.github.io/V2X-Sim/}.\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nDeep learning for visual perception, multi-robot systems, data sets for robotic vision.\n\\end{IEEEkeywords}\n\n\\section{Introduction}\\label{sec:intro}\n\n\\IEEEPARstart{P}{erception} is a fundamental \\hl{capability} for autonomous vehicles, which allows them to represent, identify, and interpret sensory input for understanding the complex surroundings. In literature, single-vehicle perception has been intensively studied thanks to the well-established driving datasets~\\cite{Caesar2020nuScenesAM,Geiger2012AreWR,Sun2020ScalabilityIP}, and researchers have proposed various algorithms to deal with different downstream tasks~\\cite{arnold2019survey,marvasti2021deep,Minaee2021ImageSU}. \n\nDespite recent advances in single-vehicle perception, the individual viewpoint often results in degraded perception in long-range or occluded areas. A promising solution to this problem is through vehicle-to-everything (V2X)~\\cite{machardy2018v2x}, a cutting-edge communication technology that enables dialogue between a vehicle and other entities, including vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I). With the aid of V2X communication, we are able to upgrade single-vehicle perception to~{collaborative perception}, which introduces more viewpoints to help autonomous vehicles see further, better and even see through occlusion, thereby fundamentally enhancing the capability of perception. \n\nCollaborative perception naturally draws on communication and perception. Its development requires expertise from both communities. Recently, the communication community has made enormous efforts to promote such a study~\\cite{muhammad2018survey,hasan2020securing,mannoni2019comparison}; however, only a few works have been proposed from the perspective of perception~\\cite{Chen2019CooperCP,Wang2020V2VNetVC, Li_2021_NeurIPS,yuan2021comap,yuan2022keypoints}.\nOne major reason for this is the lack of well-designed and organized collaborative perception datasets. Due to the immaturity of V2X and the cost of simultaneously operating multiple autonomous vehicles, it is very expensive and laborious to build such a real dataset for research communities. Therefore, \\textit{we synthesize a comprehensive and publicly available dataset, named as V2X-Sim, to advance the study of collaborative perception for V2X-communication-aided autonomous driving.}\n\n \\begin{figure}[t]\n\t\\centering\t\\includegraphics[width=0.95 \\columnwidth]{fig-teaser.pdf}% \t\\vspace{-0.2cm}\n\t\\captionsetup{font={scriptsize}}\n\t\\caption{ (a) \\textbf{Intersection} for vehicle-to-everything (V2X) communication. (b) \\textbf{Workflow} of multi-agent collaborative perception with intermediate-/feature-based strategy. We benchmark collaborative object detection, multi-object tracking, and semantic segmentation in the bird's eye view (BEV).}\n\t\\label{fig:1}\n\t\\vspace{-0.5cm}\n\\end{figure}\n\nTo generate V2X-Sim, we employ SUMO~\\cite{krajzewicz2012recent}, a micro-traffic simulation, to produce numerically-realistic traffic flow, and CARLA~\\cite{Dosovitskiy17}, a widely-used open-source simulator for autonomous driving research, to retrieve well-synchronized sensor streams from multiple vehicles as well as the road-side unit (RSU). Meanwhile, multi-modality sensor streams of different entities are recorded to enable cross-modality perception. In addition, diverse annotations including bounding boxes, vehicle trajectories, and semantic labels are provided to facilitate various downstream tasks. To better serve multi-agent, multi-modality, and multi-task perception research for autonomous driving, we further provide a benchmark for three crucial perception tasks (collaborative detection, tracking, and segmentation) on the proposed dataset using the state-of-the-art collaboration strategies~\\cite{Li_2021_NeurIPS,Wang2020V2VNetVC,Liu2020When2comMP,Liu2020Who2comCP}. In summary, our contributions are two-fold: \n\\begin{itemize}\n     \\item We propose V2X-Sim, a comprehensive V2X perception dataset for autonomous driving, to support multi-agent multi-modality multi-task perception research.\n    \\item We create an open-source testbed for collaborative perception methods, and provide a benchmark on three tasks to encourage more research in this field. \n\\end{itemize}\n\n\\begin{table}[t]\n\\captionsetup{font={scriptsize, sc, stretch=1.3}, justification=centering, labelsep=newline}\n\\caption{\n\\label{tab:datasetcomp} Comparison of collaborative perception datasets for autonomous driving. There are no public datasets which support both V2V and V2I research: multi-agent data are either generated by simulators~\\cite{xu2021opv2v,Wang2020V2VNetVC} or created by selecting consecutive frames from single-agent real datasets~\\cite{Chen2019CooperCP,Xiao2018MultimediaFA,Maalej2017VANETsMA}. \\hl{Several works collect data from multiple infrastructure sensors:} \\cite{arnold2020cooperative} in simulation,\\cite{howe2021weakly,yu2022dairv2x} in real world.  Our dataset is the first public multi-agent multi-modality dataset which supports different collaborative perception tasks.}\n\\tiny\n  \\centering\n   \\setlength{\\tabcolsep}{0.1mm}{\n  \\begin{tabular}{@{}c|c|c|cccc|ccc|c@{}}\n      \\toprule\n    \\multirow{2}{*}{\\bf Dataset} & \\multirow{2}{*}{\\bf Scenario}& \\multirow{2}{*}{\\bf Source} & \\multicolumn{4}{c|}{\\bf Sensor} &  \\multicolumn{3}{c|}{\\bf Tasks}& \\multirow{2}{*}{\\bf Public} \\\\\n      &&& {\\bf RGB} & {\\bf Depth} &  {\\bf LiDAR} &  {\\bf IMU/GPS} & {\\bf Det.} & {\\bf Track.} & {\\bf Seg.}& \n      \\\\\n    \\midrule\n    V2V-Sim~\\cite{Wang2020V2VNetVC} & V2V & LiDARsim~\\cite{Manivasagam2020LiDARsimRL} & \\xmark & \\xmark & \\cmark & \\xmark & \\cmark &\n    {\\cmark} & \\xmark  &\\xmark \\\\\n    Cooper~\\cite{Chen2019CooperCP} & V2V & KITTI~\\cite{Geiger2012AreWR} & \\xmark & \\xmark & \\cmark & \\cmark & \\cmark &\n    {\\cmark} & \\xmark  &\\xmark \\\\\n    MFSL~\\cite{Xiao2018MultimediaFA} & V2V & KITTI~\\cite{Geiger2012AreWR} & \\cmark & \\xmark & \\xmark & \\cmark & \\xmark &\n    \\xmark & \\cmark  &\\xmark \\\\\n    VANETs~\\cite{Maalej2017VANETsMA} & V2V & KITTI~\\cite{Geiger2012AreWR} & \\cmark & \\xmark & \\cmark & \\cmark & \\cmark &\n    \\xmark & \\xmark  &\\xmark \\\\ \n    OPV2V~\\cite{xu2021opv2v} & V2V & OpenCDA~\\cite{xu2021opencda} \\& CARLA~\\cite{Dosovitskiy17} & \\cmark & \\xmark & \\cmark & \\cmark & \\cmark &\n    {\\cmark} & \\xmark  &\\cmark \\\\ \n    CODD~\\cite{arnold2021fastreg} & V2V &  CARLA~\\cite{Dosovitskiy17} & \\xmark & \\xmark & \\cmark & \\xmark & \\cmark &\n    {\\cmark} & \\xmark  &\\cmark \\\\ \n    \\hl{Cooper (inf)}~\\cite{arnold2020cooperative}  & V2I & CARLA~\\cite{Dosovitskiy17} & \\cmark & \\cmark & \\xmark & \\xmark & \\cmark & \\cmark & \\xmark  &\\cmark\\\\ \n    \\hl{WIBAM}~\\cite{howe2021weakly} & V2I & Real-world & \\cmark & \\xmark & \\xmark & \\xmark & \\cmark &\n    \\cmark & \\xmark  &\\cmark \\\\ \n    \\hl{DAIR-V2X}~\\cite{yu2022dairv2x} & V2I & Real-world & \\cmark & \\xmark & \\cmark & \\cmark & \\cmark &\n    \\xmark & \\xmark  &\\cmark \\\\ \n    \\midrule\n    V2X-Sim (\\textbf{Ours}) & V2V\\&V2I & CARLA~\\cite{Dosovitskiy17} \\& SUMO~\\cite{krajzewicz2012recent} & \\cmark & \\cmark & \\cmark & \\cmark & \\cmark &\n    \\cmark & \\cmark &\\cmark \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n   \\vspace{-4mm}\n\\end{table}\n\n\\section{Related Work}\\label{sec:related}\n\\textbf{Autonomous driving dataset.} \nSince the pioneering dataset KITTI~\\cite{Geiger2012AreWR} was released, the autonomous driving community has been trying to increase the dataset comprehensiveness in terms of driving scenarios, sensor modalities, and data annotations. Regarding driving scenarios, current datasets cover crowded urban scenes~\\cite{Patil2019TheHD}, adverse weather conditions~\\cite{Pitropov2021CanadianAD}, night scenes~\\cite{Pham2020A3DDT}, and multiple cities~\\cite{Caesar2020nuScenesAM} to enrich the data distribution. As for sensor modalities, nuScenes~\\cite{Caesar2020nuScenesAM} collects data with Radar, RGB cameras, and LiDAR in a 360$^{\\circ}$ viewpoint; WoodScape~\\cite{Yogamani2019WoodScapeAM} captures data with fisheye cameras. Regarding data annotations, semantic labels in both images~\\cite{Huang2018TheAD, Cordts2016TheCD, Ros2016TheSD, Neuhold2017TheMV} and point clouds~\\cite{Behley2019SemanticKITTIAD, Hu2020TowardsSS} are provided to enable semantic segmentation; 2D/3D box trajectories are offered~\\cite{Chang2019Argoverse3T,Ettinger2021LargeSI} to facilitate tracking and prediction. In summary, most real datasets emphasize the data comprehensiveness in single-vehicle situations, but ignore the multi-vehicle scenarios.\n\n\\textbf{V2X system and dataset.} By sharing information with other vehicles or the RSU, V2X mitigates the shortcomings of single-vehicle perception and planning such as the limited sensing range and frequent occlusion~\\cite{machardy2018v2x}. Previous research~\\cite{Jia2016EnhancedCC} has developed an enhanced cooperative microscopic traffic model in V2X scenarios, and studied the effect of V2X in traffic disturbance scenarios. \\cite{Kim2015MultivehicleCD} proposes a multi-modal cooperative perception system that provides see-through, lifted-seat, satellite and all-around views to drivers. \\hl{More recently, COOPER}~\\cite{Chen2019CooperCP} \\hl{investigates raw-data level collaborative perception to improve the detection capability for autonomous driving. V2VNet}~\\cite{Wang2020V2VNetVC}\n\\hl{proposes intermediate-feature level collaboration to promote the vehicle's perception and prediction capability. Several works use multiple infrastructure sensors to jointly perceive the environment and employ output-level collaboration with vehicle-to-infrastructure communication}~\\cite{arnold2020cooperative, howe2021weakly}. As for the datasets, ~\\cite{Chen2019CooperCP,Xiao2018MultimediaFA,Maalej2017VANETsMA} simulate the V2V scenarios with different frames from KITTI~\\cite{Geiger2012AreWR}. Yet, these datasets are unrealistic multi-agent datasets for the measurements are not captured by different viewpoints. Some other works use a platoon strategy for data capture~\\cite{Rawashdeh2018CollaborativeAD, Chen2015DSRCAR}, but they are biased because the observations were highly correlated with each other.\nThe most relevant work is V2V-Sim~\\cite{Wang2020V2VNetVC}, which is based on a high-quality LiDAR simulator~\\cite{Manivasagam2020LiDARsimRL}. Unfortunately, V2V-Sim does not include the V2I scenario and is not publicly available. Moreover, OPV2V~\\cite{xu2021opv2v} and CODD~\\cite{arnold2021fastreg} only support the detection task in the V2V scenario. Existing collaborative perception datasets are summarized in Table~\\ref{tab:datasetcomp}: \\textit{V2X-Sim\\footnote{This work extends the LiDAR-based V2V data in our previous work~\\cite{Li_2021_NeurIPS} with more modalities, scenarios and downstream tasks.} is currently the most comprehensive one with multi-agent multi-modality sensory streams in both V2V and V2I scenarios, and can support various downstream tasks such as multi-agent collaborative detection, tracking, and semantic segmentation.}\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}{.5\\linewidth}\n    \\centering\n    \\captionsetup{font={scriptsize, sc, stretch=1.3}, justification=centering, labelsep=newline}\n    \\captionof{table}{Sensor suite of vehicle (\\textbf{V}) and intersection (\\textbf{I}). }\n        \\scriptsize\n        \\vspace{-2mm}\n        \\begin{center}\n        \\resizebox{1\\textwidth}{!}{%\n            \\setlength{\\tabcolsep}{1mm}{\n        \\begin{tabular}[H]{ p{2cm}|p{5cm}}\n         \\toprule\n         Sensor & Description\\\\\n         \\midrule\n         \\textbf{V:} 6 $\\times$ RGB camera \\textbf{I:} 4 $\\times$ RGB camera  & Each vehicle is equipped with 6 cameras. Each camera has a FoV of 70$^{\\circ}$, except for the back camera that has a FoV of 110$^{\\circ}$. Each RSU has 4 cameras looking diagonally downward at 35$^{\\circ}$ with a 70$^{\\circ}$ FoV. \n         The image size is 1600$\\times$900.  \\\\\n         \\midrule\n         \\textbf{V:} 6 $\\times$ Depth camera \\textbf{I:} 4 $\\times$ Depth camera & Each vehicle has 6 depth cameras with  the same setting as RGB cameras. Each RSU has 4 depth cameras with  the same setting as its RGB cameras. \\\\\n          \\midrule\n         \\textbf{V:} 6 $\\times$ Semantic camera \\textbf{I:} 4 $\\times$ Semantic camera  & Each vehicle has 6 semantic segmentation cameras with the same setting as RGB cameras. Each RSU has 4 semantic segmentation cameras with the same setting as its RGB cameras. \\\\\n          \\midrule\n         \\textbf{V\\&I:} 1 $\\times$ BEV semantic camera & Each vehicle and RSU has one BEV semantic camera at the top, looking downward. Both the raw images (semantic tags encoded in the red channel) and the converted colored images are provided. The image size is 900$\\times$900. \\\\\n          \\midrule\n         \\textbf{V\\&I:} 1 $\\times$ LiDAR and Semantic LiDAR &  We attach one LiDAR and one semantic LiDAR on top of the ego vehicle and the intersection center. Specs: 32 channels, 70m max range, 250,000 points per second, 20 Hz rotation frequency.\\\\\n         \\bottomrule\n        \\end{tabular}}}\n                \n        \\label{tab:setup}\n        \\end{center}\n    \\vspace{-4mm}\n    \\end{minipage}~~\n    \\begin{minipage}{.47\\linewidth}\n    \\centering\n  \\includegraphics[width=1.0\\textwidth]{fig-sensorlayout_compressed.pdf}\n  \\captionsetup{font={scriptsize}}\n  \\caption{Sensor layout and coordinate systems.}\n  \\label{fig:setup}\n    \\end{minipage}~~\n\\vspace{-2mm}\n\\end{figure}\n\n\\begin{figure*}[t]\n\\begin{center}\n\\includegraphics[width=0.95\\textwidth]{fig-V2X-bev-vis-and-stats.pdf}\n\\vspace{-1mm}\n\\captionsetup{font={scriptsize}}\n\\caption{\\hl{(a) Visualizations of the bird's eye view point cloud from different scenes. Gray denotes the point cloud captured by the RSU LiDAR. Each color (except for gray) represents a vehicle, and the orange boxes denote the vehicles in the scene. (b) Statistics for car bounding box sizes. (c) Counts for annotations per keyframe where the annotated vehicles are presented within 0-30m, 30-50m, and 50-70m of the ego vehicles. (d) Counts for LiDAR points per annotation. (e) Statistics of the distance between every two ego vehicles for all frames. (f) Speed of cars located within 70m from ego vehicles. (g) Percentage of annotated vehicles observed by 1-5 ego vehicles.}}\n\\label{fig:scene}\n\\end{center}\n\\vspace{-8mm}\n\\end{figure*}\n\\section{V2X-Sim Dataset}\nV2X-Sim could enable more research on the collaboration strategy among vehicles to achieve a more robust perception. This could fundamentally benefit autonomous driving, intelligent transportation systems, smart cities, \\textit{etc.}.\n\\subsection{Sensor suite on vehicles and RSU}\nMulti-modality data is essential for robust perception. To ensure the comprehensiveness of our dataset, we equip each vehicle with a sensor suite composed of RGB cameras, LiDAR, GPS, IMU, and RSU with RGB cameras and LiDAR. \n\n\\textbf{Sensor configuration.} On both the vehicle and RSU, the camera and LiDAR cover 360$^{\\circ}$ horizontally to enable full-view perception. Specifically, each vehicle carries six RGB cameras following the nuScenes configuration~\\cite{Caesar2020nuScenesAM}; the RSU is equipped with four RGB cameras toward four directions at the crossroad. We employ depth camera, semantic segmentation camera, semantic LiDAR, and BEV semantic segmentation camera in CARLA to obtain the corresponding ground-truth for each RGB camera. Note that the BEV semantic segmentation camera is based on orthogonal projection while the ego-vehicle semantic segmentation camera uses perspective projection. Table~\\ref{tab:setup} summarizes the detailed sensor specification. \n\n\\textbf{Sensor layout and coordinate system.}\nThe overall sensor layout and coordinate system is shown in Fig.~\\ref{fig:setup}. The BEV semantic camera shares the same x, y position with LiDAR yet is placed higher to ensure a certain size of field of view. Note that we invert the y-axis in CARLA and use a right-hand coordinate system following nuScenes~\\cite{Caesar2020nuScenesAM}. \n\n\\textbf{Diverse annotations.}\nTo assist downstream tasks including detection, tracking and semantic segmentation, we provide various annotations such as 3D bounding boxes, pixel-wise and point-wise semantic labels. Each box is defined by the location of its center in x, y, z coordinates, and its width, length, and height. In total, there are 23 categories such as the pedestrian, building, ground, etc. In addition, precise depth values are provided for depth estimation.\n\n\\subsection{CARLA-SUMO co-simulation}\nWe consider it a realistic V2X scenario when multiple vehicles with their own routes are simultaneously located in the same intersection. \\hl{ Each intersection is also equipped with one RSU with sensing capability. Regarding the traffic simulation, there are several non-public simulators which can explicitly generate data tailored for collaborative perception such as scenes with occlusion, and sensor range limitations}~\\cite{suo2021trafficsim,tan2021scenegen}. \\hl{Yet in this work, we use open-source CARLA-SUMO co-simulation for traffic flow simulation and data recording. Vehicles are spawned in CARLA via SUMO to roam around in the town with random routes. Hundreds of vehicles are spawned in different towns ($Town03$, $Town04$ and $Town05$ that have crossroads as well as T-junctions in both the crowded downtown and suburb highway). We record several log files in different towns. Afterwards, at different junctions, we read out 100 scenes from the log files. Each scene has a 20-second duration, and we choose $M (M=2,3,4,5)$ vehicles as well as one RSU in each scene as intelligent agents to share information. Example scenarios are shown in} Fig.~\\ref{fig:scene} (a).\n\n\\subsection{Downstream tasks}\nOur dataset not only supports single-agent perception tasks such as 3D object detection, tracking, image-/point-cloud-based semantic segmentation, depth estimation, but also enables collaborative perception such as collaborative 3D object detection, tracking, and collaborative BEV semantic segmentation in urban driving scenes. We provide a benchmark for collaborative perception algorithms.\n\n\\subsection{Dataset statistics}\n\\textbf{Annotation statistics.} \nWe provide statistics on the annotations and objects to highlight the inclusiveness and diversity of our dataset. \nIn Fig.~\\ref{fig:scene} (b) we analyze the size of the cars' bounding boxes within a 70m range from ego vehicles in each frame. The great variation of car sizes indicates that our scenes contain a diverse set of car makes and models that well includes most of the common real-world vehicles. \nFigure~\\ref{fig:scene} (c) shows the annotation count in each frame for vehicles within 0-30m, 30-50m, and 50-70m ranges from each ego vehicle. It suggests that our dataset features both crowded scenes (up to 100 annotations within 50-70m from the ego vehicle) and less crowded driving scenarios (as low as 10 annotations within 30m from the ego vehicle). Figure~\\ref{fig:scene} (d) contains statistics on the number of LiDAR points per annotation for single-agent and multi-agent scenarios respectively. The number of total LiDAR points of each object annotation increases when there are more than one agents observing the same object. Specifically, for a single agent, there are 183.83 points in each annotation on average, but the number goes up to 875.59 points per annotation for multiple agents. \n\n\\textbf{Scene features.}\nWe analyze the distance between each two ego vehicles for every frame, as shown in Fig.~\\ref{fig:scene} (e). An overwhelming percentage of the ego vehicles are presented within 20-30 meters from each other, suggesting they are geographically closely connected. The speed of cars within 70m from ego vehicles are shown in Fig.~\\ref{fig:scene} (f). Given the fact that our scenes are selected near intersections, we notice that a major fraction of vehicles are slower than 10 km/h. However, the maximum speed is as high as 90+ km/h. \nFigure~\\ref{fig:scene} (g) shows the percentage of annotations observed by a certain number of ego vehicles, up to 5. Over 60\\% of the annotations are observed by at least two ego vehicles.\n\n\\section{Collaborative Perception Benchmark}\\label{sec:benchmark}\n\nWe benchmark three crucial perception tasks in autonomous driving within the collaboration setting: detection, tracking, and semantic segmentation. The three tasks have been extensively studied since they generate essential perception knowledge for autonomous vehicles to make safer decisions. For performance evaluation, we follow the same evaluation protocol of the three tasks in the single-agent scenario, except that we utilize the information shared by other agents while the single-agent perception do not have access to such information. We report the results in two scenarios: (1) V2V only, and (2) V2X (V2V + V2I). \n\n\\textbf{Dataset format and split.} Our V2X-Sim dataset follows the same storage format of nuScenes~\\cite{Caesar2020nuScenesAM}, an authoritative multi-modality single-agent autonomous driving dataset. nuScenes collected real-world single-agent driving data to promote the single-agent autonomous driving research; while we simulate multi-agent scenarios to facilitate the next-generation V2X-aided collaborative perception technology. Each scene in our dataset contains a 20 second traffic flow at a certain intersection of three CARLA towns, and the multi-modality multi-agent sensory streams are recorded at 5Hz, meaning each scene is composed of 100 frames. We generate 100 scenes with a total of 10,000 frames, and in each scene, 2-5 vehicles are selected as the collaboration agents. We use 8,000/1,000/1,000 frames for training/validation/testing respectively, \\hl{and we ensure that there is no overlap in terms of the intersections across training/validation/testing set.} Each frame has data sampled from multiple agents (vehicles and RSU). \\hl{There are 37,200 samples in the training set, 5,000 samples in the validation set, and 5,000 samples in the test set. The split is shared across tasks.}\n\n\\textbf{Implementation details.} Bird's-eye-view (BEV) is a widely used and powerful representation in autonomous driving because it can describe the surrounding objects and overall context via a compact top-down 2D map~\\cite{Wu2020MotionNetJP}. Therefore, BEV-based representation is adopted in all three tasks: we use a 3D voxel grid to represent the 3D world, employ binary representation, and assign each voxel a positive label if the voxel contains point cloud data. Since the generated 3D voxel grid can be considered as a pseudo-image whose height dimension is the channel dimension, we can perform the efficient 2D convolution instead of the heavy 3D convolution. Specifically, we crop the points located in the region of $[-32, 32] \\times [-32, 32] \\times[-3, 2]$ meters for vehicles defined in the ego-vehicle Euclidean coordinate and $[-32, 32] \\times [-32, 32] \\times[-8, -3]$ meters for RSU. The width and length of each voxel are 0.25 meter, and the height is 0.4 meter, meaning the generated BEV-based pseudo-image has a dimension of $256\\times256\\times13$ ($W\\times L\\times H$). Note that the models in all the tasks consume the 3D BEV map and generate perception results in 2D BEV. \n\n\\begin{figure*}[t]\n    \\begin{minipage}{.59\\linewidth}\n        \\centering\n        \\includegraphics[width=1\\textwidth]{fig-V2x-Sim-det.pdf}\n        \\captionsetup{font={scriptsize}}\n        \\caption{\\hl{Visualizations of BEV detection on V2X-Sim.} \\textcolor{red}{Red} and \\textcolor{green}{green} boxes are the predictions and ground-truths respectively.}\n        \\label{fig:det-baseline}\n    \\end{minipage}~~\n    \\centering\n    \\begin{minipage}{.38\\linewidth}\n    \\setlength{\\tabcolsep}{0.1mm}{\n    \\captionsetup{font={scriptsize, sc, stretch=1.3}, justification=centering, labelsep=newline}\n    \\captionof{table}{\\hl{Quantitative results of collaborative BEV detection on V2X-Sim. RSU denotes the Road-Side Unit. AP denotes the Average Precision. $\\Delta$ is the absolute gain in AP introduced by RSU.}}\n    \\label{table:collaborative-bev-detection}\n        \\scriptsize\n        \\setlength{\\extrarowheight}{0.3mm}{\n        \\begin{tabular}{cccccccc}\n        \\toprule\n        \\multicolumn{2}{c}{\\multirow{2}{*}{\\textbf{Method}}} & \\multicolumn{3}{c}{\\textbf{AP@IoU=0.5}} & \\multicolumn{3}{c}{\\textbf{AP@IoU=0.7}} \\\\\n        \\multicolumn{2}{c}{} & w/o RSU & w/ RSU & $\\Delta$ & w/o RSU & w/ RSU & $\\Delta$  \\\\ \\midrule\n        \\multicolumn{2}{c}{Lower-bound} & 49.90 & 46.96 & $\\downarrow$ 2.94 & 44.21 & 42.33 & $\\downarrow$ 1.88 \\\\\n        \\multicolumn{2}{c}{Co-lower-bound} & 43.99 & 42.98 & $\\downarrow$ 1.01 & 39.10 & 38.26 & $\\downarrow$ 0.84 \\\\\n        \\multicolumn{2}{c}{When2com~\\cite{Liu2020When2comMP}} & 44.02 & 46.39 & $\\uparrow$ 2.37 & 39.89 & 40.32 & $\\uparrow$ 0.43 \\\\\n        \\multicolumn{2}{c}{When2com*~\\cite{Liu2020When2comMP}} & 45.35 & 48.28 & $\\uparrow$ 2.93 & 40.45 & 41.43 & $\\uparrow$ 0.68 \\\\\n        \\multicolumn{2}{c}{Who2com~\\cite{Liu2020Who2comCP}} & 44.02 & 46.39 & $\\uparrow$ 2.37  & 39.89 & 40.32 & $\\uparrow$ 0.43 \\\\\n        \\multicolumn{2}{c}{Who2com*~\\cite{Liu2020Who2comCP}} & 45.35 & 48.28 & $\\uparrow$ 2.93 & 40.45 & 41.13 & $\\uparrow$ 0.68  \\\\\n        \\multicolumn{2}{c}{V2VNet~\\cite{Wang2020V2VNetVC}} & \\textbf{\\textcolor{blue}{68.35}} & \\textbf{\\textcolor{blue}{72.08}} & $\\uparrow$ 3.73 & \\textbf{\\textcolor{blue}{62.83}} & \\textbf{\\textcolor{blue}{65.85}} & $\\uparrow$ 3.02  \\\\\n        \\multicolumn{2}{c}{DiscoNet~\\cite{Li_2021_NeurIPS}} & \\textbf{\\textcolor{green}{69.03}} & \\textbf{\\textcolor{green}{72.87}} & $\\uparrow$ 3.84 & \\textbf{\\textcolor{green}{63.44}} & \\textbf{\\textcolor{green}{66.40}} & $\\uparrow$ 4.81 \\\\\n        \\multicolumn{2}{c}{Upper-bound} & \\textbf{\\textcolor{red}{70.43}} & \\textbf{\\textcolor{red}{77.08}} & $\\uparrow$ 6.65 & \\textbf{\\textcolor{red}{67.04}} & \\textbf{\\textcolor{red}{72.57}} & $\\uparrow$ 5.53  \\\\\n        \\bottomrule\n        \\end{tabular}}}\n    \\end{minipage}~~\n\\vspace{-2mm}\n\\end{figure*}\n\n\\textbf{Benchmark models.} We aim to benchmark collaborative perception strategies rather than the well-studied individual perception methods. We consider \\hl{early/intermediate/late/no collaboration} models for the benchmark. The intermediate models, including DiscoNet~\\cite{Li_2021_NeurIPS}, V2VNet~\\cite{Wang2020V2VNetVC}, When2com~\\cite{Liu2020When2comMP}, and Who2com~\\cite{Liu2020Who2comCP}, are based on the communication of the intermediate features of the neural network. The methods in our benchmark are as follows:\n\\begin{itemize}\n    \\item \\textbf{Lower-bound}: The single-agent perception model without collaboration which processes a single-view point cloud is considered as the lower-bound. \n    \n    \\item \\hl{\\textbf{Co-lower-bound}: Collaborative lower-bound fuses the output from different single-agent perception models. }\n    \n    \\item \\textbf{Upper-bound}: The early collaboration model which transmits raw point cloud data is the upper-bound.\n     \\item \\textbf{DiscoNet~\\cite{Li_2021_NeurIPS}}: DiscoNet uses a directed collaboration graph with matrix-valued edge weight to adaptively highlight the informative spatial regions and reject the noisy regions of the messages sent by the partners. After adaptive message fusion, the updated features will be transmitted to the output head for perception. \n    \\item \\textbf{V2VNet~\\cite{Wang2020V2VNetVC}}: V2VNet uses a pose-aware graph neural network to propagate agents’ information, and employs a convolutional gated recurrent unit based module to aggregate other agents’ information. After several rounds of neural message passing, the updated features are fed into the output head to generate perception results.\n    \\item \\textbf{When2com~\\cite{Liu2020When2comMP}}: When2com employs attention-based mechanism for communication group construction: the partners with satisfactory correlation scores will be selected as the collaborators. After the attention-score-based weighted fusion, the updated features will be fed into the output head for perceptions. The model with pose information is marked by $*$.\n    \\item \\textbf{Who2com~\\cite{Liu2020Who2comCP}}: Who2com shares a similar idea with When2com, yet it uses handshake mechanism to select the collaborator: the partner with the highest score will be selected as the collaborator. The model with pose information is marked by $*$.\n\\end{itemize}\n\nWe implement a 3D perception pipeline that can be integrated with all of the communication methods mentioned above. Since the source codes of V2VNet is not publicly available, we re-implement the V2VNet in PyTorch according to its pseudo-code. For when2com/who2com, we borrow their communication modules from its official code. All of the intermediate collaboration modules use the same architecture and conduct collaboration at the same intermediate feature layer. Moreover, all of the methods are trained with the same setting to ensure that the performance gain comes from the collaboration instead of irrelevant techniques. \n\n\\begin{table*}[t]\n\\scriptsize\n\\centering\n\\captionsetup{font=sc}\n\\captionsetup{font={scriptsize, sc, stretch=1.3}, justification=centering, labelsep=newline}\n\\caption{\\hl{Quantitative results of BEV tracking on V2X-Sim. \\textbf{MOTA}: Multiple Object Tracking Accuracy. \\textbf{MOTP}: Multiple Object Tracking Precision. \\textbf{HOTA}: Higher Order\nTracking Accuracy. \\textbf{DetA}: Detection Accuracy. \\textbf{AssA}: Association Accuracy. \\textbf{DetRe}: Detection Recall. \\textbf{DetPr}: Detection Precision. \\textbf{AssRe}: Association Recall. \\textbf{AssPr}: Association Precision. \\textbf{LocA}: Localization Accuracy. The number to the left of () denotes the performance in V2V solely. The number in () represents the performance gain by adding V2I.}}\n\\label{table:tracking-baseline}\n\\setlength{\\tabcolsep}{1mm}{\n\\begin{tabular}{@{}cccccccccccc@{}}\n\\toprule\n\\textbf{Method} & \\textbf{MOTA} & \\textbf{MOTP} & \\textbf{HOTA} & \\textbf{DetA} & \\textbf{AssA} & \\textbf{DetRe} & \\textbf{DetPr} & \\textbf{AssRe} & \\textbf{AssPr} & \\textbf{LocA} \\\\ \\midrule\nLower-bound & 35.72 ($\\downarrow$3.87) & 84.16 ($\\downarrow$0.74) & 34.27 ($\\downarrow$1.68) & 33.64 ($\\downarrow$3.24) & 36.18 ($\\downarrow$0.06) & 35.07 ($\\downarrow$3.54) & 82.49 ($\\uparrow$0.96) & 46.70 ($\\uparrow$0.23) & \\textbf{\\textcolor{blue}{58.72}} ($\\uparrow$0.10) & 86.43 ($\\uparrow$0.38) \\\\\nCo-lower-bound & 21.53 ($\\uparrow$0.58) & \\textbf{\\textcolor{blue}{85.76}} ($\\uparrow$0.15) & 39.16 ($\\downarrow$0.71) & 41.14 ($\\downarrow$0.93) & 38.18 ($\\downarrow$0.62) & \\textbf{\\textcolor{red}{59.54}} ($\\downarrow$2.52) & 54.68 ($\\uparrow$0.79) & \\textbf{\\textcolor{red}{50.92}} ($\\downarrow$0.65) & 55.78 ($\\uparrow$0.84) & \\textbf{\\textcolor{blue}{87.64}} ($\\uparrow$0.38)\\\\\nWhen2com~\\cite{Liu2020When2comMP} & 29.48 ($\\uparrow$2.45) & \\textbf{\\textcolor{green}{86.10}} ($\\downarrow$2.79) & 30.94 ($\\uparrow$1.01) & 27.90 ($\\uparrow$2.04) & 35.33 ($\\uparrow$0.06) & 28.67 ($\\uparrow$2.58) & \\textbf{\\textcolor{blue}{86.11}} ($\\downarrow$4.81) & 46.30 ($\\downarrow$0.15) & \\textbf{\\textcolor{red}{59.20}} ($\\downarrow$0.36) & \\textbf{\\textcolor{green}{87.98}} ($\\downarrow$1.98)  \\\\\nWhen2com$^*$~\\cite{Liu2020When2comMP} & 30.17 ($\\uparrow$1.43) & 84.95 ($\\downarrow$1.44) & 31.34 ($\\uparrow$0.43) & 29.11 ($\\uparrow$1.05) & 35.42 ($\\uparrow$0.21) & 30.28 ($\\uparrow$1.32) & 83.81 ($\\uparrow$0.29) & 46.65 ($\\downarrow$0.29) & 58.61 ($\\uparrow$0.18) & 86.14 ($\\uparrow$0.17) \\\\\nWho2com~\\cite{Liu2020Who2comCP} & 29.48 ($\\uparrow$2.46) & \\textbf{\\textcolor{green}{86.10}} ($\\downarrow$2.79) & 30.94 ($\\uparrow$1.01) & 27.90 ($\\uparrow$2.04) & 35.33 ($\\uparrow$0.06) & 28.67 ($\\uparrow$2.58) & \\textbf{\\textcolor{blue}{86.11}} ($\\downarrow$4.81) & 46.30 ($\\downarrow$0.15) & \\textbf{\\textcolor{red}{59.20}} ($\\downarrow$0.36) & \\textbf{\\textcolor{green}{87.98}} ($\\downarrow$1.98)  \\\\\nWho2com$^*$~\\cite{Liu2020Who2comCP} & 30.17 ($\\uparrow$1.43) & 84.95 ($\\downarrow$1.44) & 31.34 ($\\uparrow$0.43) & 29.11 ($\\uparrow$1.06) & 35.42 ($\\uparrow$0.21) & 30.28 ($\\uparrow$1.33) & 83.81 ($\\uparrow$0.29) & 46.65 ($\\downarrow$0.29) & 58.61 ($\\uparrow$0.18) & 86.14 ($\\uparrow$0.17) \\\\\nV2VNet~\\cite{Wang2020V2VNetVC} & \\textbf{\\textcolor{blue}{55.29}} ($\\uparrow$2.29) & 85.21 ($\\downarrow$0.53) & \\textbf{\\textcolor{blue}{43.68}} ($\\uparrow$0.91) & \\textbf{\\textcolor{blue}{50.71}} ($\\uparrow$1.93) & \\textbf{\\textcolor{blue}{38.76}} ($\\uparrow$0.24) & 53.40 ($\\uparrow$2.51) & 84.45 ($\\downarrow$1.07) & 50.22 ($\\uparrow$0.53) & 58.50 ($\\downarrow$0.07) & 87.22 ($\\uparrow$0.38) \\\\\nDiscoNet~\\cite{Li_2021_NeurIPS} & \\textbf{\\textcolor{green}{56.69}} ($\\uparrow$2.26) & \\textbf{\\textcolor{red}{86.23}} ($\\downarrow$0.41) & \\textbf{\\textcolor{green}{44.76}} ($\\uparrow$1.09) & \\textbf{\\textcolor{green}{52.41}} ($\\uparrow$2.18) & \\textbf{\\textcolor{red}{39.25}} ($\\uparrow$1.11) & \\textbf{\\textcolor{blue}{54.87}} ($\\uparrow$2.58) & \\textbf{\\textcolor{green}{86.29}} ($\\downarrow$0.95) & \\textbf{\\textcolor{green}{50.86}} ($\\uparrow$1.02) & \\textbf{\\textcolor{green}{58.94}} ($\\downarrow$0.15) & \\textbf{\\textcolor{red}{88.07}} ($\\uparrow$0.34)\\\\\nUpper-bound & \\textbf{\\textcolor{red}{58.00}} ($\\uparrow$3.92) & 85.61 ($\\uparrow$0.25) & \\textbf{\\textcolor{red}{44.83}} ($\\uparrow$4.24) & \\textbf{\\textcolor{red}{52.94}} ($\\uparrow$4.24) & \\textbf{\\textcolor{green}{38.95}} ($\\downarrow$0.75) & \\textbf{\\textcolor{green}{55.07}} ($\\uparrow$4.68) & \\textbf{\\textcolor{red}{86.54}} ($\\downarrow$0.30) & \\textbf{\\textcolor{blue}{50.35}} ($\\downarrow$0.86) & 58.71 ($\\uparrow$0.15) & 87.48 ($\\uparrow$0.06) \\\\\n\\bottomrule\n\\vspace{-8mm}\n\\end{tabular}}\n\\end{table*}\n\n\\subsection{Collaborative object detection in BEV}\n\\textbf{Problem definition.} As the most crucial perception task in autonomous driving, 3D object detection aims to recognize and localize the objects in 3D scenes given a single frame, with the following tracking, prediction and planning modules all heavily relying on the detections. The models consume voxelized point cloud and output BEV bounding boxes.\n\n\\textbf{Backbone and evaluation.} We use a classic anchor-based detector composed of a convolutional encoder, a convolutional decoder, and an output header for classification and regression~\\cite{Luo2018FastAF}. Regarding the loss function, we use the binary cross-entropy loss to supervise the box classification and the smooth $L_1$ loss to supervise the box regression, following~\\cite{Luo2018FastAF}. We employ the generic BEV detection evaluation metric: Average Precision (AP) at Intersection-over-Union (IoU) thresholds of 0.5 and 0.7. We target the vehicle detection and report the results on the test set.\n\n\\textbf{Quantitative results.} Table~\\ref{table:collaborative-bev-detection} demonstrates the quantitative comparisons on AP (@IoU = 0.5/0.7). We find that: (1) the upper-bound performs best amongst all methods, and it improves lower-bound significantly by 41.1$\\%$ and 51.6$\\%$ in terms of AP@0.5 and AP@0.7 in the scenario of V2V only, validating the necessity of collaboration; (2) V2V and V2I jointly can generally improve the perception over V2V only with more viewpoints, e.g., adding V2I can bring an improvement of 9.4$\\%$ for upper-bound, and 5.5$\\%$ for V2VNet in terms of AP@0.5; (3) among the intermediate models, DiscoNet achieves the best performance via the well-designed distilled collaboration graph, V2VNet achieves the second best performance via the powerful neural message passing, and When2com/Who2com only achieve comparable performance with lower-bound since the attention-mechanism is not suitable in point-cloud-based collaborative perception: the agents usually need complementary information rather than a highly similar one; \\hl{ (4) the late collaboration model (co-lower-bound) hurts the detection performance because of introducing extra false positives from other vehicles.}\n\n\\textbf{Qualitative results.} The qualitative results are shown in Fig.~\\ref{fig:det-baseline}. We see that the collaboration can fundamentally mitigate the problems of long-range perception and occlusion. \n\n\\begin{figure*}[t]\n\\begin{center}\n\\hsize=\\textwidth\n\\includegraphics[width=0.87\\textwidth]{fig-V2x-Sim-Seg.pdf}\n\\captionsetup{font={scriptsize}}\n\\caption{\\hl{Visualizations of collaborative BEV semantic segmentation.}}\n\\label{fig:seg-baseline}\n\\end{center}\n\\vspace{-5mm}\n\\end{figure*}\n\n\\begin{table*}[t]\n\\scriptsize\n\\centering\n\\captionsetup{font={scriptsize, sc, stretch=1.3}, justification=centering, labelsep=newline}\n\\caption{\\hl{Quantitative results of BEV segmentation on V2X-Sim. The number to the left of () denotes the performance in V2V solely. The number in () represents the performance gain by adding V2I.}}\n\\label{table:segmentation-baseline}\n\\setlength{\\tabcolsep}{1.8mm}{\n\\begin{tabular}{@{}ccccccccc@{}}\n\\toprule\n\\textbf{Method} & \\textbf{Vehicle} & \\textbf{Sidewalk} & \\textbf{Terrain} & \\textbf{Road} & \\textbf{Building} & \\textbf{Pedestrian} & \\textbf{Vegetation} & \\textbf{mIoU} \\\\ \\midrule\nLower-bound & 45.93 ($\\uparrow$2.22) & 42.39 ($\\downarrow$2.75) & 47.03 ($\\uparrow$0.20) & 65.76 ($\\downarrow$1.27) & 25.38 ($\\downarrow$1.89) & 20.59 ($\\downarrow$3.09) & 35.83 ($\\uparrow$0.66) & 36.64 ($\\downarrow$0.87)\\\\\nCo-lower-bound & 47.67 ($\\uparrow$2.43) & \\textbf{\\textcolor{green}{48.79}} ($\\downarrow$1.41) & \\textbf{\\textcolor{red}{50.92}} ($\\uparrow$0.85) & \\textbf{\\textcolor{green}{70.00}} ($\\downarrow$0.65) & 25.26 ($\\uparrow$0.17) & 10.78 ($\\downarrow$1.77) & 39.46 ($\\uparrow$2.69) & 38.38 ($\\uparrow$0.46)\\\\\nWhen2com~\\cite{Liu2020When2comMP} & 48.43 ($\\uparrow$0.03) & 33.06 ($\\uparrow$1.38) & 36.89 ($\\uparrow$1.76) & 57.74 ($\\uparrow$1.56) & \\textbf{\\textcolor{red}{29.20}} ($\\uparrow$1.18) & 20.37 ($\\uparrow$0.57) & 39.17 ($\\downarrow$0.01) & 34.49 ($\\uparrow$0.88)\\\\\nWhen2com$*$~\\cite{Liu2020When2comMP} & 47.74 ($\\uparrow$1.23) & 33.60 ($\\downarrow$0.40) & 35.81 ($\\uparrow$1.05) & 56.75 ($\\uparrow$0.48) & 26.11 ($\\downarrow$0.92) & 19.16 ($\\uparrow$0.04) & 39.64 ($\\downarrow$2.55) & 33.81 ($\\downarrow$0.47)\\\\\nWho2com~\\cite{Liu2020Who2comCP} & 48.40 ($\\uparrow$0.06) & 32.76 ($\\uparrow$1.68) & 36.04 ($\\uparrow$2.61) & 57.51 ($\\uparrow$1.79) & \\textbf{\\textcolor{green}{29.17}} ($\\uparrow$1.21) & 20.36 ($\\uparrow$0.58) & 39.08 ($\\uparrow$0.08) & 34.31 ($\\uparrow$1.06) \\\\\nWho2com$*$~\\cite{Liu2020Who2comCP} & 47.74 ($\\uparrow$1.23) & 33.60 ($\\downarrow$0.40) & 35.81 ($\\uparrow$1.05) & 56.75 ($\\uparrow$0.48) & 26.11 ($\\downarrow$0.92) & 19.16 ($\\uparrow$0.04) & 39.64 ($\\downarrow$2.55) & 33.81 ($\\downarrow$0.47) \\\\\nV2VNet~\\cite{Wang2020V2VNetVC} & \\textbf{\\textcolor{green}{58.42}} ($\\uparrow$3.09) & \\textbf{\\textcolor{red}{48.33}} ($\\downarrow$3.87) & 48.51 ($\\downarrow$1.59) & \\textbf{\\textcolor{red}{70.02}} ($\\uparrow$0.46) & 28.58 ($\\uparrow$5.18) & \\textbf{\\textcolor{blue}{21.99}} ($\\uparrow$0.57) & \\textbf{\\textcolor{blue}{41.42}} ($\\uparrow$0.35) & \\textbf{\\textcolor{green}{41.11}} ($\\uparrow$0.74) \\\\ \nDiscoNet~\\cite{Li_2021_NeurIPS} & \\textbf{\\textcolor{blue}{56.66}} ($\\uparrow$1.19) & \\textbf{\\textcolor{blue}{46.98}} ($\\downarrow$1.74) & \\textbf{\\textcolor{green}{50.22}} ($\\downarrow$1.05) & \\textbf{\\textcolor{blue}{68.62}} ($\\downarrow$0.25) & 27.36 ($\\uparrow$5.58) & \\textbf{\\textcolor{green}{22.02}} ($\\uparrow$0.82) & \\textbf{\\textcolor{green}{42.50}} ($\\uparrow$0.95) & \\textbf{\\textcolor{blue}{40.84}} ($\\uparrow$0.53) \\\\ \nUpper-bound & \\textbf{\\textcolor{red}{64.09}} ($\\uparrow$5.34) & 41.34 ($\\uparrow$2.42) & \\textbf{\\textcolor{blue}{48.20}} ($\\uparrow$0.74) & 67.05 ($\\uparrow$2.04) & \\textbf{\\textcolor{blue}{29.07}} ($\\uparrow$0.74) & \\textbf{\\textcolor{red}{31.54}} ($\\uparrow$3.15) & \\textbf{\\textcolor{red}{45.04}} ($\\uparrow$0.70) & \\textbf{\\textcolor{red}{42.29}} ($\\uparrow$1.98)\\\\\n\\bottomrule\n\\end{tabular}}\n\\vspace{-5mm}\n\\end{table*}\n\n\\subsection{Collaborative multi-object tracking in BEV}\n\\textbf{Problem definition.}\nDifferent from detection, multi-object tracking requires the generation of temporally consistent perception results. Multi-object tracking in BEV is to use bounding boxes, object categories, and object identities to track different objects within a temporal sequence. \n\n\\textbf{Evaluation metrics.} \nWe mainly utilize HOTA (Higher Order Tracking Accuracy)~\\cite{Luiten_2020} to evaluate our BEV tracking performance. HOTA can evaluate detection, association, and\nlocalization performance via a single unified metric. In addition, the classic multi-object tracking accuracy (MOTA) and multi-object tracking precision (MOTP)~\\cite{journals/ejivp/BernardinS08} are also employed. MOTA can measure detection errors and association errors. MOTP solely measures localization accuracy. \n\n\\textbf{Baseline tracker.} \nWe implement SORT~\\cite{Bewley2016_sort} as our baseline tracker. Given the detection results, SORT will combine the Kalman Filter and Hungarian algorithm to achieve an accurate and efficient tracking performance. \n\n\\textbf{Quantitative results.} Quantitative comparisons of BEV tracking are shown in Table~\\ref{table:tracking-baseline}. Similar to BEV detection, upper-bound achieves the best performance in terms of MOTA and HOTA. Meanwhile, adding V2I can improve MOTA largely yet cannot help too much in MOTP. Co-lower-bound shows good performance in localization accuracy (MOTP). A more advanced tracker is required to exploit the collaboration for filling the performance gap.\n\n\\subsection{Collaborative semantic segmentation in BEV}\n\n\\textbf{Problem definition.} \nWe aim to conduct semantic segmentation in BEV using only geometry point cloud. In the collaborative perception scenarios, there are measurements collected by multiple agents with distinct viewpoints. Therefore, there are more information in the scene, facilitating the semantic scene understanding.\n\n\\textbf{Baseline segmentation method and evaluation metrics.} \nWe follow the backbone architecture as well as the loss function of U-Net~\\cite{Ronneberger2015} in our baseline method. The input is a BEV-based voxelized point cloud, and the output is BEV semantic segmentation. We label and predict seven categories as listed in Table~\\ref{table:segmentation-baseline}, while the remaining is unlabeled. In our benchmark, we evaluate the segmentation performance using mean Intersection-over-Union (mIoU).\n\n\\textbf{Quantitative results.} As illustrated in Table \\ref{table:segmentation-baseline}, we find that: (1) V2VNet, DiscoNet, and upper-bound achieve comparable performance in terms of terrain and road categories; \\hl{(2) the attention-based methods (i.e., when2com, who2com) performs worse because the attention-based mechanisms try to find the collaboration partners with high correlation scores. Whereas, in 3D perception, the collaborators with complementary information should be prioritized during the collaboration. Such nonalignment can make it quite hard for the attention model to learn;} (3) there is a large gap between lower-bound and upper-bound regarding the two safety-critical categories: vehicle ($45.93\\%$ v.s. $64.09\\%$) and pedestrian ($20.59\\%$ v.s. $31.54\\%$), proving the values of collaboration; \\hl{(4) employing V2V and V2I jointly can generally enhance the vehicle segmentation over using V2V solely; (5) co-lower-bound  performs better than the lower-bound.}\n\n\\textbf{Qualitative results.}\nFigure \\ref{fig:seg-baseline} shows the semantics segmentation results. The results of upper-bound restore the semantic information with rich point cloud data. Intermediate-based collaboration strategies V2VNet and DiscoNet can achieve satisfactory performance yet When2com and Who2com hurt the performance compared to the lower-bound.\n\n\\begin{figure*}[t]\n\\begin{center}\n\\hsize=\\textwidth\n\\includegraphics[width=0.85\\textwidth]{fig-noise.pdf}\n\\captionsetup{font={scriptsize}}\n\\caption{\\hl{Experimental results of a robustness test conducted under various magnitudes of pose noise. AUG. means augmentation which adds pose noise during training.}}\n\\label{fig:pose}\n\\end{center}\n\\end{figure*}\n\n\\begin{figure*}[t]\n\\begin{center}\n\\hsize=\\textwidth\n\\includegraphics[width=0.85\\textwidth]{fig-compression.pdf}\n\\captionsetup{font={scriptsize}}\n\\caption{\\hl{Experimental results of a bandwidth test conducted under various compression ratios. }}\n\\label{fig:compression}\n\\end{center}\n\\vspace{-5mm}\n\\end{figure*}\n\n\\subsection{Discussions on pose noise and compression ratio}\n\\hl{We further examine the robustness of different intermediate models against realistic pose noise (Gaussian noise with a mean of $0.05m-0.25m$ and a standard deviation of $0.02cm$), as shown in Fig.}~\\ref{fig:pose}. \\hl{We can see that the models perform comparably with or without pose noise in the training phase, and all the intermediate methods have shown stable performance against the pose noise. The reason is that the intermediate feature map has a relatively low spatial resolution (each grid in the feature map denotes a coverage of $2m\\times2m$), thus is not vulnerable to noisy pose. Meanwhile, we employ an $1\\times1$ autoencoder to further compress the feature channel of the transmitted feature map. We test the models with different compression ratios, and we found that the jointly learned $1\\times1$ autoencoder can even improve the performance a little bit, and most intermediate models achieve comparable performance at different levels of compression, as shown in Fig.}~\\ref{fig:compression}.\n\n\\section{Conclusion}\\label{sec:conclusion}\nWe propose V2X-Sim dataset based on CARLA-SUMO co-simulation, in order to enable multi-agent collaborative perception research in autonomous driving. Our dataset provides multi-agent multi-modality sensor streams captured by the vehicles and road-side unit (RSU) in realistic traffic flows. Diverse annotations are provided to support a variety of 3D perception tasks. In addition, we benchmark several state-of-the-art collaborative perception methods in collaborative BEV detection, tracking, and semantic segmentation tasks. Future works include the simulation of latency issues as well as the development of novel evaluation metrics in collaborative perception tasks. We believe our work can inspire many relevant research areas including but not limited to autonomous driving, computer vision, multi-robot system, communication engineering, and machine learning. \n\n\\textbf{Acknowledgment.} The authors would like to thank anonymous reviewers for their helpful suggestions, and NYU high performance computing (HPC) for the support.\n\n\\normalem\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{\\thedataset: A Synthetic Driving Dataset for Continuous Multi-Task \\\\Domain Adaptation}\n\n\\begin{document}\n\n\\title{\\thedataset: A Synthetic Driving Dataset for Continuous Multi-Task \\\\Domain Adaptation}\n\n\\author{Tao Sun$^{1*}$ \\quad Mattia Segu$^{1}$\\thanks{Equal contribution.} \\quad Janis Postels$^1$ \\quad Yuxuan Wang$^1$ \\quad \\\\\nLuc Van Gool$^1$ \\quad Bernt Schiele$^2$ \\quad Federico Tombari$^{3,4}$ \\quad Fisher Yu$^1$\\\\[0.4cm]\n$^1$ETH Z\\\"urich \\quad $^2$MPI Informatics \\quad $^3$Google \\quad $^4$Technical University of Munich \\\\\n{\\tt\\small \\{taosun47, segum, jpostels, yuxuwang\\}@ethz.ch}\\\\ {\\tt\\small vangool@vision.ee.ethz.ch, schiele@mpi-inf.mpg.de, tombari@in.tum.de, i@yf.io}}\n\n\\maketitle\n\n\\begin{abstract}\nAdapting to a continuously evolving environment is a safety-critical challenge inevitably faced by all autonomous driving systems.\nExisting image and video driving datasets, however, fall short of capturing the mutable nature of the real world.\nIn this paper, we introduce the largest multi-task synthetic dataset for autonomous driving, \\thedataset{}. \nIt presents discrete and continuous shifts in cloudiness, rain and fog intensity, time of day, and vehicle and pedestrian density. \nFeaturing a comprehensive sensor suite and annotations for several mainstream perception tasks, \\thedataset{} allows investigating the degradation of a perception system performance at increasing levels of domain shift, fostering the development of continuous adaptation strategies to mitigate this problem and assess model robustness and generality.\nOur dataset and benchmark toolkit are publicly available at \\href{https://www.vis.xyz/shift}{\\texttt{www.vis.xyz/shift}}. \n\\end{abstract}\n\n    \n\n\\section{Introduction}\n\nRecent years have witnessed the remarkable progress of perception systems for autonomous driving.\nBetting on the role that autonomous driving will serve for society,\nindustry\nand academia have joined forces to collect and release several large-scale driving datasets, raising hopes for a forthcoming successful deployment of self-driving cars.\n\nLarge-scale driving datasets have \nplayed a pivotal role in the prosperity of perception algorithms and provide a playground for different techniques to compete and thrive on multiple tasks.\nHowever, while the algorithm accuracy surges, progress in terms of generalization to unforeseen environmental conditions has been underwhelming~\\cite{dai2018dark,michaelis2019benchmarking}.\n\n\\begin{figure}[!t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/teaser_v3.pdf}\n    \\caption{\\thedataset{} provides: (a) \\textbf{discrete domain shifts}, a set of sequences each collected using different domain parameters and initial states;\n    (b) \\textbf{continuous domain shifts}, a set of sequences where domain parameters change continuously during driving.\n    \\label{fig:teaser}}\n\\end{figure}\nTo achieve full autonomy, self-driving cars must adapt to new environments and identify life-threatening failure cases to promptly prevent crashes.\nExamples of domain shifts affecting driving are changes in weather and lighting conditions, scenery, and behavior, appearance, and quantity of agents\non the road.\nDomain shift~\\cite{ben2010theory} is a well-known problem for learning algorithms, causing unforeseeable performance drops under conditions different from the training ones. \nTechniques to prevent, counteract or assess its impact have been developed in the form of domain generalization~\\cite{khosla2012undoing,muandet2013domain,tobin2017domain,volpi2018generalizing}, domain adaptation~\\cite{wang2018deep,ganin2015unsupervised,long2015learning,tzeng2017adversarial}, uncertainty estimation~\\cite{gal2016dropout,lakshminarayanan2017simple,postels2019sampling,loquercio2020general} and \\ac{ood} detection~\\cite{yang2021generalized,ruff2021unifying,hendrycks2016baseline,postels2020hidden}.\nHowever, such approaches are typically deployed and tested on toy datasets~\\cite{saenko2010adapting,venkateswara2017deep,li2017deeper} or synthetically corrupted ones~\\cite{hendrycks2019robustness}. \nAlthough there are preliminary attempts at providing driving datasets with different domains~\\cite{bdd100k,caesar2020nuscenes,dai2018dark,pitropov2021canadian,sakaridis2018semantic,tung2017raincouver,sakaridis2021acdc,ros2016synthia}, each only covers a limited amount of perception tasks (\\eg{} only semantic segmentation~\\cite{sakaridis2018semantic,sakaridis2021acdc}) and a narrow selection of domain shift directions (\\eg{} only rain~\\cite{tung2017raincouver} or snow~\\cite{pitropov2021canadian}). \nConsequently, current solutions to domain shift cannot undergo scrutiny in controlled autonomous driving scenarios, making it difficult to verify their safety without risking real-world car crashes.\n\nGiven their short length, sequences from existing driving datasets are captured under approximately stationary conditions, and only \\textit{discrete shifts} are witnessed among sets of sequences presenting different homogeneous conditions from one set to another (\\eg{} clear weather and rainy).\nHowever, nothing in this world is constant except change and becoming.\n\\textit{Continuous shifts} - the intra-sequence shifts from one domain into another - are a certainty in the real world, where a sunny day can rapidly turn into a rainy one, or a quiet road can quickly become busy.\nMoreover, continuous distributional shift has recently been shown to represent a critical challenge for current learning systems~\\cite{postels2021practicality}.\n\nAn adequate dataset design is thus needed to quantify and address domain shift both at discrete and continuous levels.\nConsequently, we set the goal of overcoming the outdated paradigm of previous driving datasets and introduce \\thedataset{}, a new synthetic dataset capturing the continuously evolving nature of the real world through realistic discrete and continuous shifts along safety-critical environmental directions: time of day, cloudiness, rain, fog strength, and vehicle and pedestrian density.\nCollected in the CARLA simulator~\\cite{Dosovitskiy17}, \\thedataset{} includes a comprehensive sensor suite and covers the most important perception tasks. Counting 4,800+ sequences captured from a multi-view sensor suite in 8 different locations, our dataset supports 13 perception tasks for multi-task driving systems: semantic/instance segmentation, monocular/stereo depth regression, 2D/3D object detection, 2D/3D multiple object tracking (MOT), optical flow estimation, point cloud registration, visual odometry, trajectory forecasting and human pose estimation.\n\nOur dataset aims to foster research in several under-explored fields related to the generality and reliability of perception systems for autonomous driving, \\eg{} domain generalization, domain adaptation, and uncertainty estimation.\nMoreover, by collecting incremental discrete shifts from one domain to another, we hope to foster research in the field of continual learning~\\cite{wang2020tent,volpi2021continual,hacohen2019power} for autonomous driving, so far only studied on discrete levels of synthetic corruptions~\\cite{hendrycks2019robustness} of traditional image classification datasets~\\cite{deng2009imagenet,krizhevsky2014cifar}.\nFinally, by collecting sequences with realistic intra-sequence continuous domain shifts, we provide the first driving dataset allowing research on continuous test-time learning and adaptation~\\cite{wang2020tent,sun2020test,tonioni2019learning,tonioni2019real,poggi2021continual}.  \n\nThe main contributions of this work are:\n\\begin{itemize}[leftmargin=1.25em]\n\\vspace{-0.5em}\n\\setlength\\itemsep{0em}\n    \\item We introduce \\thedataset{}, a multi-task driving dataset featuring the most important perception tasks under a variety of conditions and with a comprehensive sensor setup.\n    To the best of our knowledge, it is the largest synthetic dataset for autonomous driving and provides the most inclusive set of annotations and conditions.\n    \\item Using \\thedataset{}, we analyze the importance of modeling discrete and continuous domain shifts, and demonstrate new findings on different adaptation and uncertainty estimation methods under continuous shifts.\n\\end{itemize}\n\\section{Related Work}\n\\begin{table*}[!t]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{5pt}\n\\begin{tabular}{clccccccccccc}\n\\toprule\n& \\multirow{2}{*}{\\textbf{Dataset}}                                                     & \\multirow{2}{*}{\\textbf{Cities}} & \\textbf{Tracking} & \\textbf{\\textbf{Max length for}} & \\textbf{\\textbf{Domain}} & \\multicolumn{6}{c}{\\textbf{Annotated frames for}}                                                                           \\\\ \\cline{7-13} \n                         &                                                              &                                  & \\textbf{sequences}                                    & \\textbf{sequence}                & \\textbf{shift$^\\dagger$}       & Seg.                 & 2D Det.              & 3D Det.              & MOT           & Depth    & Flow        & Pose$^\\diamondsuit$         \\\\ \\midrule\n\\parbox[t]{2mm}{\\multirow{10}{*}{\\rotatebox[origin=c]{90}{\\textbf{Real-world}}}} & KITTI~\\cite{geiger2013vision}                                   & 1                                & 22                                  &  106 sec                                                & no                                            & 200                  & 15k                  & 15k                  & 15k                  &    93k      & 389         & -          \\\\\n& CamVid~\\cite{brostow2009semantic}                               & 4                                & -                                   & -                                                  & no                                            & 700                  & -                    & -                    & -                    & -        & -        & -            \\\\\n & Cityscapes~\\cite{cordts2016cityscapes}                          & 27                               & -                                   & -                                                  & no                                            & 25k                  & 25k                    & 25k                    & -                    & -        & -           & -          \\\\\n& Cityscapes-C$^\\ddagger$ ~\\cite{michaelis2019benchmarking}                   & 27                               & -                                   & -                                                 & discrete                                      & 25k                  & 25k                    & 25k                    & -                    & -        & -              & -       \\\\\n& H3D~\\cite{patil2019h3d}                                         & 4                                & 160                                 &    20 sec                                                & discrete                                      & -                    & -                    & 27k                  & 27k                  & -        & -                & -    \\\\\n& HD1K~\\cite{kondermann2016hci}                                   & 1                                & -                                   & -                                                  & discrete                                      & -                    & -                    & -                    & -                    & -        & 1k                 & -  \\\\\n& A*3D~\\cite{pham2020a3d}                                         & 1                                & -                                   & -                                                 & discrete                                      & -                    & -                    & 39k                  & -                    & -        & -                 & -   \\\\\n& nuScenes~\\cite{caesar2020nuscenes}                              & 2                                & 1,000                                & 20 sec                                            & discrete                                      & -                    & -                    & 40k                  & 40k                  & -        & -                 & -   \\\\\n& Waymo Open~\\cite{sun2020scalability}                            & 3                                & 1,150                                & 20 sec                                            & discrete                                      & -                    & 200k                 & 230k                 & 230k                 & -        & -          & 230k          \\\\\n& BDD100K~\\cite{bdd100k}                                          & -$^\\mathsection$                         & 2,000                                & 40 sec                                           & discrete                                      & 10k                  & 100k                 & -                    & 318k                 & -        & -             & -       \\\\ \\midrule\n\\parbox[t]{2mm}{\\multirow{6}{*}{\\rotatebox[origin=c]{90}{\\textbf{Synthetic}}}} & SYNTHIA~\\cite{ros2016synthia}                                   & 3                                & -                                   & -                                                  & discrete                                      & 9,000                 & 200k                 & 200k                 & -                    & -        & -               & -      \\\\ \n& GTA-V~\\cite{richter2016playing}                                 & 1                                & -                                   & -                                                 & no                                            & 25k                  & -                    & -                    & -                    & -        & -          & -           \\\\\n& VIPER~\\cite{Richter_2017} & 1    &  184  &  10 min & discrete & 320k & 320k & - & 320k & - & 320k & - \\\\\n& AIODrive~\\cite{Weng2020_AIODrive}                              & 8                                & 100                                 & 100 sec                                            & discrete                                      & 100k                 & 100k                 & 100k                 & 100k                 & 100k     & -          & -          \\\\\n\\cmidrule{2-13}\n& \\textbf{\\thedataset (ours)}  & {8}                            & 4,850                                & 33 min                                          & \n\\makecell{discrete +\\\\continuous}                     & {2.5M}             & {2.5M}             & {2.5M}             & {2.5M}             & {2.5M} & {2.5M} & {2.5M} \\\\\n\\bottomrule\n\\end{tabular}\\vspace{-0.5em}\n\\caption{Comparison of size and supported tasks of existing driving datasets. \\thedataset{} is the largest synthetic dataset and, most notably, the only dataset providing realistic continuous domain shifts, diverse annotations, and longer annotated sequences. $^\\dagger$ indicates whether the dataset presents domain annotations. $^\\ddagger$ artificially corrupted. $^\\mathsection$ multiple cities; exact number not known. $^\\diamondsuit$ key points for human pose.}\n\\label{tab:datasets}\n\\vspace{-0.5em}\n\\end{table*}\nDuring the past decade, a large variety of realistic and synthetic driving datasets emerged, providing a playground for researchers to develop novel algorithms. Domain shift is a common threat to the performance and safety of learning-based methods.\n\nWe here introduce the most notable driving datasets and the techniques to mitigate the domain shift effect.\nFor an overview of the\ncurrent driving datasets, refer to \\autoref{tab:datasets}.\n\n\\paragraph{Real-world driving datasets} typically focus on a specific subset of perception tasks due to the high cost of data collection and annotation.  \nAfter almost a decade of development, the pioneering real-world dataset KITTI~\\cite{geiger2013vision} supports almost all the perception tasks for autonomous driving, including semantic / instance segmentation, depth estimation, 2D and 3D object detection and tracking, optical flow, scene flow, and visual odometry. However, its small scale represents an obvious problem and its diversity is severely limited compared to modern large-scale datasets. \nCamVid~\\cite{brostow2009semantic}, Cityscapes~\\cite{cordts2016cityscapes}, and Mapillary~\\cite{neuhold2017mapillary} are image-based driving datasets for segmentation, A*3D~\\cite{pham20203d} for 3D object detection, and HD1K~\\cite{kondermann2016hci} for optical flow estimation.\nRecently, many large-scale datasets, \\textit{e.g.}, BDD100K~\\cite{bdd100k}, Waymo Open~\\cite{sun2020scalability}, H3D~\\cite{patil2019h3d}, and nuScenes~\\cite{caesar2020nuscenes}, have been released with multi-task annotations, although mainly focusing on object detection and tracking. \nOur dataset offers a complete set of annotations for all the frames, comprehensive of all the most important perception tasks supported by other datasets, and enabling multi-task learning on a broader range of tasks and conditions.\n\n\\paragraph{Synthetic driving datasets} are collected using graphic engines and physical simulators. SYNTHIA~\\cite{ros2016synthia} contains images and segmentation annotations generated by its simulator.\nAIODrive~\\cite{Weng2020_AIODrive} is produced using CARLA Simulator with multiple sensor support, focusing on high-density long-range LiDAR sets. \nCompared to ours, these datasets present sequences of limited length and are restricted to discrete domain labels (\\autoref{tab:datasets}).\nFurther, video games have also been used for data generation. GTA-V~\\cite{richter2016playing,hu2021monocular}  provides images and segmentation masks captured from a popular game. VIPER~\\cite{Richter_2017} extends GTA-V by providing optical flow masks and discrete environmental labels. However, low-level control of video game engines is hardly accessible, impeding fine-grained environmental control and the collection of continuous shifts.\n\n\\paragraph{Adverse conditions datasets} support the evaluation of robustness under different \\ac{ood} conditions.\nA recent work~\\cite{malinin2021shifts} collects meteorological and air temperature measurements under discrete real-world shifts.\nImage-based datasets, \\eg{}  CIFAR10/100-C~\\cite{michaelis2019benchmarking}, ImageNet-R~\\cite{hendrycks2021many} and Cityscapes-C~\\cite{hendrycks2019robustness}, have been generated by applying artificial corruptions such as blurring, additive Gaussian noise and addition of specific patterns on the original dataset. Though carefully designed, such ad-hoc corruptions cannot fully represent the challenges presented by visual shifts in the real world.\nTo this end, recent driving datasets~\\cite{pham2020a3d, bdd100k, caesar2020nuscenes, sun2020scalability,mao2021one} provide manually labeled tags for various weather conditions, scene categories, and day periods. \nHowever, each only covers a limited amount of perception tasks (see \\autoref{tab:datasets}) and a narrow selection of domain shift directions. \nMoreover, ad-hoc datasets have been collected for specific underrepresented domains, \\eg{} rain~\\cite{tung2017raincouver,jin2021raidar}, fog~\\cite{tarel2012vision,sakaridis2018semantic,sakaridis2018model}, night~\\cite{dai2018dark}, snow~\\cite{pitropov2021canadian}.\nHowever, domain tags remain coarse-grained and only certain tasks and domain shift directions are supported.\nRecently, the ACDC dataset~\\cite{sakaridis2021acdc} has been proposed, featuring images evenly distributed between fog, nighttime, rain, and snow. However, it supports only semantic segmentation.\nInterestingly, the India Driving Dataset~\\cite{varma2019idd} is the only dataset to provide extremely busy roads as adverse conditions.\nOverall, BDD100K~\\cite{bdd100k} is the large-scale real-world dataset presenting the largest diversity of perception driving tasks and discrete domain labels for the time of day and weather conditions.\nFor this reason, we use it as a reference to validate empirical observations drawn from our dataset.\nNevertheless, compared to our dataset, BDD100K only provides annotated images from single cameras, does not provide 3D bounding boxes and optical flow annotations, distribution of domains is highly imbalanced and the domain is stationary within each sequence.\nIn contrast, our dataset provides a full sensor suite, annotations for multiple tasks, balanced domain distribution and sets of sequences with continuously changing time of day, weather conditions (cloudiness, rain and fog strength), and vehicle and pedestrians density.\n\n\\begin{figure*}[!t]\n    \\centering\n    \\vspace{-1em}\n    \\includegraphics[width=0.99\\linewidth]{images/annotations3.pdf}\n    \\caption{The annotation set of the RGB camera in our dataset. Each frame is associated with annotations of 2D/3D bounding boxes with tracking identities (visualized by different colors), semantic/instance segmentation, depth map and optical flow label. \\label{figure:ann}}\n\\end{figure*}\n\n\\paragraph{Unsupervised domain adaptation (UDA)} means simultaneously learning on a labeled source and an unlabeled target domain to find transferable features across domains. UDA is mainly achieved via feature-space alignment~\\cite{quinonero2008covariate, sun2017correlation}, domain-consistent regularization~\\cite{ganin2015unsupervised, ganin2016domain,hoffman2018cycada} and minimization of surrogate functions of domain gaps~\\cite{vu2019advent, saito2019semi}. The discrete shifts provided in our dataset can be directly used for training and evaluating UDA approaches.\n\n\\paragraph{Continual domain adaptation} aims at performing consecutive discrete adaptation steps from one domain to multiple others.\nIncremental domain adaptation (IncDA) is a subset of continual DA that requires the source data and assumes availability of intermediate domains where domain shifts occur gradually~\\cite{wang2020tent,volpi2021continual,lao2020continuous}, allowing to minimize the gap between adaptation steps and performing adaptation from the source to the final target domain more effectively than with direct UDA.\nProviding different strengths of variations along natural axes, our dataset is suitable for IncDA.\n\n\\paragraph{Continuous test-time adaptation} (ContinuousTTA) assumes that gradual domain shifts occur within the same test sequence, and adaptation is performed at test time on the incoming data stream. \nContinuousTTA is a suitable choice for any scenario where a model is required to adapt on the go to a shifting domain and no large labeled or unlabeled collection of data from the target domain is available in advance. \nRecent works~\\cite{wang2020tent,mummadi2021test,sun2020test} show the efficiency of TTA when applied to artificial corruptions in the image-based datasets ImageNet-C/-R~\\cite{hendrycks2019robustness,hendrycks2021many}. \nThe continuously shifting video sequences in our dataset provide instead realistic domain shift along natural directions, facilitating the development of ContinuousTTA methods transferable to the real world.\n\n\\paragraph{Uncertainty Estimation} is a fundamental task for safety-critical vision applications. Quantifying the confidence about a model's prediction allows avoiding dangerous failures in autonomous driving.\nHowever, current uncertainty estimation techniques~\\cite{lakshminarayanan2017simple,gal2016dropout,liu2020simple,postels2020hidden} mainly focus on classification on toy datasets~\\cite{krizhevsky2014cifar,lecun1998mnist}, while recent work~\\cite{postels2021practicality} has observed poor calibration, \\ie{} uncertainty uncorrelated with prediction's error, when such techniques are extended to more difficult datasets~\\cite{hendrycks2019benchmarking} and tasks under distributional shift.\nWe hope that the domain shifts and multiple tasks supported in \\thedataset{} will enable the study of uncertainty estimation methods on a wide variety of tasks for autonomous driving and their calibration under distributional shift. \n\n\\section{The \\thedataset{} Dataset}\nWe provide a driving dataset with a comprehensive sensor suite (\\autoref{ssec:sensors}) and a rich set of annotations (\\autoref{ssec:annotations}), supporting multiple image- and video-based perception and forecasting tasks against environmental changes. \nWe detail our design choices regarding domain shifts in \\autoref{ssec:dataset_design}.\n\n\\subsection{Sensor Suite} \\label{ssec:sensors}\nWe collect the data through a comprehensive sensor suite. \nOur sensor suite features 11 different sensors, including a multi-view RGB camera set with 5 cameras, a stereo RGB camera set, an optical flow sensor, a depth camera, a GNSS sensor, and an IMU. All the cameras have a field-of-view of $90\\degree$ and resolution of $1280 \\times 800$ pixel.\nMoreover, we provide point clouds captured by a 128-channel LiDAR sensor.\nAll sensors are synchronized and captured at a $10 \\mathrm{Hz}$ rate. \nWe follow the Scalabel~\\cite{scalabel} format and right-hand coordinate systems for storing all the annotations.\nMore details are in the Appendix.\n\n\\subsection{Annotations} \\label{ssec:annotations}\nWe provide annotations for multiple mainstream perception tasks in autonomous driving, including 2D/3D bounding box trajectories,\ninstance/semantic segmentation, optical flow and dense depth.\nUnlike real-world datasets, whose annotations are often limited to a group of keyframes due to prohibitive labeling cost, we offer full annotations for each frame in the sequences.\nMore details are in the Appendix.\n\n\\begin{figure*}[t!]\n    \\centering\n    \\includegraphics[width=0.95\\linewidth]{images/adaptation_style.pdf}\n    \\caption{We evaluate four adaptation strategies: targeted domain adaptation (Targeted DA), untargeted domain adaptation (Untargeted DA), incremental domain adaptation (Incremental DA) and continuous test-time adaptation (Continuous TTA). The dots in the same row represent frames from the same sequence; their grayscale marks the degree of domain shift (white dots = source, dark gray dots = target.) }\n    \\label{fig:adaptation_strategies}\n\\end{figure*}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1.02\\linewidth]{images/cont_vs_discrete_2.pdf}\n    \\vspace{-2em}\n    \\caption{Examples of the two-level structure for domain labels. Each discrete label (tag above images) corresponds to an interval of continuous labels (\\textit{i.e.}, severity, axis below images). }\n    \\label{fig:cont_vs_}\n    \\vspace{-1em}\n\\end{figure}\n\n\\subsection{Dataset Design} \\label{ssec:dataset_design}\nGiven their short sequence length, existing driving datasets are captured under approximately stationary conditions, and only discrete shifts are witnessed among sets of sequences presenting different homogeneous conditions (\\eg{} clear weather and rainy).\nWe set the goal of overcoming the outdated paradigm of previous driving datasets and introduce \\thedataset{}, a new synthetic dataset capturing the continuously evolving nature of the real world through realistic discrete and continuous shifts along safety-critical environmental directions: time of day, cloudiness, rain, fog strength, and vehicle and pedestrian density.\nWe collect 5,250 sequences, of which 4,250 contain stationary \nenvironmental conditions, \\ie{} inter-sequence domain shift. Each sequence is composed of 500 frames collected at 10 Hz, equivalent to 50 seconds of driving time. The remaining 600 sequences have continuously shifting conditions, \\ie{} inter-sequence domain shift. \nTotalling 70+ hours of driving and 2,500,000 annotated frames, \\thedataset{} is the largest synthetic driving dataset available.\n\n\\paragraph{Domain shift types.} We consider the most-frequent real-world environmental changes. \\thedataset{} provides domain shifts in (a) weather conditions, including cloudiness, rain, and fog intensity, (b) time of day, (c) the density of vehicles and pedestrians, and (d) camera orientation. \n\n\\paragraph{Domain shifts level.} To facilitate research on domain adaptation in different scenarios, \\thedataset{} provides two levels of domain shifts, namely discrete domain shifts and continuous domain shifts. \nThe \\textit{discrete} set contains 4,250 sequences generated with fixed environmental parameters and random initial states. We group these sequences into different domains, according to their severity. \\autoref{fig:cont_vs_} shows grouping examples. All possible domain combinations are uniformly distributed across all sequences. \nThe \\textit{continuous} set contains additional 600 sequences with continuous domain variations. In particular, each sequence presents a gradual shift from one domain to another, where the shift happens through the intermediate domains that would naturally occur in the real world. In total, we collect 500 sequences of a basic 40 seconds length (1x), 80 sequences 10x longer than the basic length, and 20 100x longer. Each set is uniformly divided among the following shifts, each of which also loops back to the source domain: day $\\xrightarrow{}$ night,  clear $\\xrightarrow{}$ rain, clear $\\xrightarrow{}$ foggy, clear $\\xrightarrow{}$ overcast. Given a domain shift direction, \\eg{} day to night, all other domain parameters are uniformly distributed across all sequences. \nDifferent sequence lengths allow analyzing the impact of domain shift speed on continuous TTA strategies (\\autoref{ssec:exp_continuous}).\n\n\\begin{figure*}[t!]\n    \\centering\n    \\vspace{-1em}\n    \\subfloat[Object detection]{\\includegraphics[width=0.46\\linewidth]{images/2d_det_ood.pdf}} \\hspace{8mm}\n    \\subfloat[Semantic segmentation]{\\includegraphics[width=0.46\\linewidth]{images/sem_seg_ood.pdf}}\n    \\vspace{-0.5em}\n    \\caption{Performance degradation for different object detection (left) and semantic segmentation (right) methods under different weather conditions. Every model is trained under clear weather conditions and tested on other domains. \\thedataset shows a similar trend as BDD100K. }\n    \\label{fig:trend}\n\\end{figure*}\n\n\\begin{table*}[t!]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{5pt}\n\\begin{tabular}{lllccccccc}\n\\toprule\n \\textbf{Task}    & \\textbf{Method}  &  \\textbf{Metric}     &  \\underline{clear-daytime}     & partly cloudy & overcast & foggy & rainy  & dawn/dusk & night        \\\\ \\midrule\n Semantic segmentation & DRN-D~\\cite{yu2017dilated} & mIoU (\\%) $\\uparrow$ & 83.6 & 79.3 & 79.4 & 62.4 & 54.6 & 60.8 & 42.8  \\\\\nInstance segmentation & Mask R-CNN~\\cite{he2017mask} & mAP (\\%) $\\uparrow$ &  39.3 & 39.4 & 34.0 & 18.7 &  35.0 & 30.7 & 13.1 \\\\\nObject detection & Faster R-CNN~\\cite{cai2018cascade} & mAP (\\%) $\\uparrow$& 46.9 & 47.4 & 41.1 & 21.0 & 41.3 & 37.3 & 15.4  \\\\\nMOT & QDTrack~\\cite{pang2021quasi} & MOTA (\\%) $\\uparrow$ & 56.2 & 53.4 & 46.2 & 25.0 & 41.9 & 44.7 & 16.5 \\\\\nMono. depth estimation & AdaBins-UNet~\\cite{bhat2021adabins} & SILog $\\downarrow$ & 9.6 & 10.0 &  8.9 & 12.0 & 10.3 & 19.7 & 27.9 \\\\\nOptical flow estimation & RAFT~\\cite{teed2020raft} & EPE (px) $\\downarrow$ &  2.26 & 2.01 & 2.35 & 2.60 &  2.43 &  4.17 & 8.85 \\\\\n\\bottomrule\n\\end{tabular}\\caption{Performance degradation on \\thedataset of different methods for different perception tasks under discrete domain shifts. Training domain is underlined. The test domains are weather variations in daytime (partly cloudy, overcast, foggy, rainy) and time of day variations in clear weather (dawn/dusk, night). $\\uparrow$ ($\\downarrow$): the higher (lower) the better.} \\label{tab:ood_performance}\n\\vspace{-3mm}\n\\end{table*}\n\n\\begin{table}[t]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{6pt}\n\n\\begin{tabular}{lccc}\n\\toprule\n\\textbf{Scenario}             & \\multicolumn{1}{c}{{Baseline}} & {Targeted DA} & \\multicolumn{1}{c}{{Incremental DA}} \\\\ \\midrule\n\\underline{daytime} $\\rightarrow$ night & 42.8                           & 45.3                                  & \\textbf{47.3}                        \\\\\n\\underline{clear} $\\rightarrow$ foggy   & \\textbf{62.4}                  & 59.1                                  & 57.3                                 \\\\\n\\underline{clear} $\\rightarrow$ rainy            & 54.6                           & 61.0                                  & \\textbf{64.9}                        \\\\\n\\bottomrule\n\\end{tabular}\\caption{Comparison of different adaptation strategies for semantic segmentation under three directions of domain shift. The source domain is underlined. Incremental DA improves over Targeted DA, except for the case when Targeted DA underperforms the baseline. (Baseline = without DA)} \\label{tab:incremental_vs_targeted}\n\\vspace{-1em}\n\\end{table}\n\n\\section{Experiments}\n\\thedataset allows studying the robustness of perception systems for driving under both discrete and continuous distributional shifts.\nWe first (\\autoref{ssec:exp_discrete}) assess the impact of discrete domain shifts on model performance for multiple perception tasks available in our dataset and empirically demonstrate that observations from our simulation dataset transfer to real-world datasets. Moreover, we compare different discrete adaptation strategies and assess the calibration of uncertainty estimation methods under domain shifts. \nIn \\autoref{ssec:exp_continuous} we extend the analysis to continuous domain shifts and investigate properties of continuous domain adaptation methods~\\cite{wang2020tent} against incremental adaptation and unsupervised domain adaptation~\\cite{vu2019advent}.\nFurther experiments, implementation details, and ablations on the data collection choices are reported in the Appendix,\ntogether with additional experiments on multitask learning.\n\n\\paragraph{Domain adaptation strategies.} \\label{ssec:adaptation_strategies}\nTo analyze the impact of our dataset design choice, we examine the four domain adaptation strategies allowed by our dataset (\\autoref{fig:adaptation_strategies}).\nAs \\textit{Baseline}, we consider the model trained on the source domain only and directly tested on the other domains.\n\\textit{Targeted DA}~\\cite{wang2018deep} is a traditional computer vision problem consisting of adapting from a labeled source domain to a specific unlabeled target domain.\nWe define \\textit{Untargeted DA}~\\cite{li2017deeper,segu2020batch} as adapting from a labeled source domain to a set of various unlabeled shifted domains. \n\\textit{Incremental DA}~\\cite{volpi2021continual}  consists in performing incremental steps of targeted DA from the source domain to the target domain passing through intermediate discretely-shifted domains.\n\\textit{Continuous TTA}~\\cite{wang2020tent} aims at adapting frame by frame to a sequence presenting a continuously shifted domain from source to target domain.\n\n\\paragraph{Implementation details.} For the adaptation tasks, we focus on semantic segmentation and use ADVENT~\\cite{vu2019advent} for the Targeted and Untargeted DA. \nThe segmentation backbone is DRN-D-54~\\cite{Yu2017}.\nIncremental DA is performed as a series of Targeted DA steps, while for Continuous TTA we extend TENT~\\cite{wang2020tent} to semantic segmentation and iteratively apply it on every incoming frame. Every model is trained in the clear-daytime domain and tested under different weather domains. While our dataset provides finer domain labels depending on the severity of the perturbation, we group different degrees of severity to match the environmental labels in BDD100K~\\cite{bdd100k} in order to assess the compatibility of conclusions drawn from our dataset with real-world trends. \n\n\\subsection{Discrete Shifts} \\label{ssec:exp_discrete}\nAs outlined in \\autoref{ssec:dataset_design}, our dataset provides incremental discrete shifts along natural environmental directions.\nWe investigate properties of discrete shifts on the multitude of supported tasks and report findings on domain adaptation and uncertainty estimation performance.\n\n\\paragraph{Impact of domain shift.} We find that many mainstream algorithms for different perception tasks suffer performance drops under domain shift (\\autoref{tab:ood_performance}), where the severity increases with the distance from the source domain.\nIn particular, we train all models in the clear-daytime domain and test under different weather conditions, showing the overall negative impact of domain shift on all the vision tasks supported by our dataset. Nevertheless, in some specific cases a model may even perform better on a shifted domain, \\eg{} instance segmentation on overcast. \nWe leverage the incremental domain shifts provided in our dataset to investigate in \\autoref{tab:incremental_vs_targeted} different discrete adaptation strategies for semantic segmentation, \\ie{} Incremental DA and Targeted DA. We find that incrementally adapting from source to target domain improves the generalization to the target domain compared to direct Targeted DA. \nHowever, clear~$\\xrightarrow{}$~foggy represents a challenging scenario for which both the adaptation strategies worsen the baseline performance.\n\n\\paragraph{Real-world compatibility.} To establish a reliable benchmark we must first confirm that trends witnessed in our simulation dataset are compatible with real-world observations.\nWe use BDD100K~\\cite{bdd100k} for  comparison because it features the largest subset of our tasks available in a real-world dataset with discrete domain labels.\nWe study the domain shift effect on two fundamental perception tasks, \\ie{} 2D object detection and semantic segmentation, and show compatible trends for different methods trained on \\thedataset{} and BDD100K (\\autoref{fig:trend}). \nWe evaluate the one-stage method YOLO v3~\\cite{redmon2018yolov3}, as well as the two-stage methods Faster R-CNN~\\cite{ren2015faster} and Cascade R-CNN~\\cite{cai2018cascade} for object detection. For semantic segmentation, we consider three different methods, FCN~\\cite{long2015fully}, DRN-D~\\cite{yu2017dilated}, and DeepLab v3+~\\cite{chen2017deeplab}. \nOur experiments suggest that the performance of different methods for semantic segmentation and object detection degrades under different domain shifts. Moreover, we find that the ranking of methods and the relative degradation trend is compatible between SHIFT and the real-world dataset BDD100K, confirming the usefulness of SHIFT and its consistency with the real world.\n\n\\begin{table}[t]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{8pt}\n\\setlength{\\tabcolsep}{4pt}\n\\begin{tabular}{llcccccccc}\n & \\textbf{Method}  &\n  \\parbox[t]{8mm}{\\rotatebox[origin=l]{90}{\\shortstack[l]{\\underline{clear-}\\\\\\underline{daytime}}}} &\n  \\parbox[t]{2mm}{\\rotatebox[origin=l]{90}{cloudy}} &\n  \\parbox[t]{2mm}{\\rotatebox[origin=l]{90}{overcast}} &\n  \\parbox[t]{2mm}{\\rotatebox[origin=l]{90}{foggy}} &\n  \\parbox[t]{2mm}{\\rotatebox[origin=l]{90}{rainy}} &\n  \\parbox[t]{2mm}{\\rotatebox[origin=l]{90}{dawn/\\\\dusk}} &\n  \\parbox[t]{2mm}{\\rotatebox[origin=l]{90}{night}} & \n  \\parbox[t]{5mm}{\\rotatebox[origin=l]{90}{\\textbf{\\shortstack[l]{OOD\\\\avg.}}}} \\\\ \\midrule\n\\parbox[t]{2mm}{\\multirow{3}{*}{\\rotatebox[origin=c]{90}{\\textbf{SHIFT}}}}  & Softmax  & 3.3 & 32.6 & 14.2 & 48.8 & 64.3 & 43.7 & 64.7 & 45.2\\\\\n& MCDO & 1.2 & 13.1 & 7.6 & 20.8 & 10.0 & 27.2 & 39.6 & 19.7 \\\\\n    & Ensemble & 1.4 & 12.3 & 7.5 & 23.4 & 8.9 & 18.7 & 36.9 & 18.0 \\\\\\midrule\n\\parbox[t]{2mm}{\\multirow{3}{*}{\\rotatebox[origin=c]{90}{\\textbf{BDD}}}} & Softmax & 9.6 & 23.2 & 9.9 & 9.7 & 7.7 & 10.6 & 48.6 & 18.4 \\\\\n& MCDO &  12.3 & 22.0 & 7.8 & 13.0 & 11.4 & 13.1 & 41.4 & 18.1 \\\\\n& Ensemble & 12.6 & 18.8 & 9.2 & 11.7 & 11.8 & 13.9 & 39.8 & 17.5\\\\\n                      \\bottomrule\n\\end{tabular}\\caption{Calibration (ECE, \\%) of uncertainty estimation methods under distributional shift for semantic segmentation. The lower, the better. Source domain is clear-daytime. We find that calibration worsens far from the source, both for \\thedataset and BDD100K.} \\label{tab:segmentation_uncertainty} \n\\vspace{-3mm}\n\\end{table}\n\n\\paragraph{Uncertainty estimation.} Autonomous driving systems must deal with life-threatening failure cases. To this end, uncertainty estimation represents a powerful tool to assess the reliability of a model's predictions. \nFollowing~\\cite{guo2017calibration}, we evaluate the Expected Calibration Error (ECE) to assess the calibration, \\ie{} correlation with model error, of uncertainty estimation methods under domain shift.\nIn particular, we evaluate the Softmax Entropy baseline and traditional Bayesian techniques such as Monte-Carlo Dropout (MCDO)~\\cite{gal2016dropout} and Deep Ensembles~\\cite{lakshminarayanan2017simple}.\nWe observe that such uncertainty estimation methods are not well calibrated under domain shift, and that calibration worsens under incremental shifts on both \\thedataset and BDD100K (\\autoref{tab:segmentation_uncertainty}).\nWhile some domains are more challenging in \\thedataset than in BDD100K, the overall degradation of calibration observed on \\thedataset is confirmed on BDD100K and the ranking of methods is preserved, further highlighting that conclusions drawn from our dataset transfer to the real world.\n\nWe hope that our dataset will help researchers providing solutions to the potentially life-threatening shortcomings of current DA and uncertainty estimation techniques.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{images/adaptation_timeofday.pdf}\n  \\caption{Comparison of different adaptation strategies for semantic segmentation on daytime~$\\xrightarrow{}$~night shifts at varying amounts of available sequences. TTA is the most effective under limited amounts of data. When enough data becomes available, Incremental DA outperforms all other alternatives.}\n    \\label{fig:compare_adaptation_strategies}\n\\end{figure}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{images/TTA_source_target.pdf}\n    \\caption{Performance on the target domain of TTA for different sequence lengths. Best learning rate on target domain is highlighted by black boxes. Both source and target performance are highly sensitive to the learning rates. Dashed lines = before TTA.}\n    \\label{fig:tta_histograms}\n    \\vspace{-1em}\n\\end{figure}\n\n\\begin{figure*}[t]\n    \\centering\n    \\vspace{-1em}\n    \\includegraphics[width=1\\linewidth]{images/TTA3.pdf} \\\\\n    \\includegraphics[width=1\\linewidth]{images/TTA2.pdf}\n  \\caption{Performance of TTA for semantic segmentation under three types of domain shift: daytime $\\xrightarrow{}$ night, clear $\\xrightarrow{}$ foggy, clear $\\xrightarrow{}$ rainy. Each point corresponds to the performance of the model on the source (top-blue) / target (top-red) / current (bottom) domain finetuned up to that level of domain shift in the sequence. Horizontal lines in the bottom figure represent the original performance on source (blue) and target domain (red). After reaching the target domain, every sequence loops back to the original source domain. Catastrophic forgetting can be observed by the drop in source performance during TTA.}\n    \\label{fig:tta_loop_back}\n\\end{figure*}\n\n\\subsection{Continuous Shifts} \\label{ssec:exp_continuous}\nA key feature of \\thedataset{} is that of providing a set with continuous intra-sequence domain shifts, allowing to compare different adaptation strategies under continuous shifts and provide an in-depth analysis on TTA and its properties.\n\n\\paragraph{Continual domain adaptation.} \\autoref{fig:compare_adaptation_strategies} compares four different adaptation strategies for semantic segmentation on an increasing number of sequences.\nGiven a model pretrained on the source domain, \\ie{} clear-daytime, and the set of continuously shifting sequences from one domain to another, \\ie{} clear-daytime~$\\xrightarrow{}$~night, we train the TTA algorithm on each frame of the incoming data stream. TTA is thus performed independently on each sequence. Final performance is averaged over all the sequences.\nFor the other adaptation strategies, we divide the length of the sequence in 20 bins, consider each bin as a separate domain, and group corresponding bins from all the provided sequences. For Targeted DA, we thus adapt directly to the last bin, corresponding to the night domain. Untargeted DA is instead applied on all the bins but the source one. Incremental DA is performed by incrementally adapting from one bin to the consecutive one until the end of the sequence is reached. \nIn particular, we plot the average mIoU against the number of training sequences (\\autoref{fig:compare_adaptation_strategies}).\nWe find that TTA is extremely efficient under small target data availability compared to all other alternatives, and that Incremental DA is consistently more effective than both Targeted and Untargeted DA.\n\n\\paragraph{Test-time adaptation.} As intra-sequence continuous shifts \nrepresent one of the main contributions of \\thedataset, we further focus on TTA by using TENT~\\cite{wang2020tent} and evaluate the effect of the speed at which domain shift happens within a sequence on TTA performance (\\autoref{fig:tta_histograms}). This is made possible by the sets of sequences of different lengths (1x, 10x, 100x the basic sequence length). \n\nGiven a source and a target domain, \\eg{} daytime and night, each sequence starts from the source domain and reaches the target domain at mid-sequence length; then, it loops back to the original domain. \nWe first observe that, depending on the domain shift speed, the learning rate can highly affect the outcome of the TTA (\\autoref{fig:tta_histograms}). Slower (faster) shifts will require lower (higher) learning rates. \nMoreover, after reaching the target domain at mid-sequence, the performance on the target domain has improved compared to its original value, while that on the source domain has dropped.  \nAccording to \\autoref{fig:tta_histograms} (1x), we find that the optimal learning rate in terms of adaptation to the target domain leads to the largest performance drop on the original source (\\autoref{fig:tta_loop_back}, top). This problem, known as catastrophic forgetting~\\cite{kirkpatrick2017overcoming} in the continual learning literature, has already been observed for class- and task-incremental learning.\n\nTo further investigate this issue, we loop back to the original domain after adapting to the target and find that, while the performance on the current target domains largely improves over the baseline (\\autoref{fig:tta_loop_back}, bottom), the original source domain accuracy cannot be recovered (\\autoref{fig:tta_loop_back}, top).\nWhile TTA has shown to be extremely effective to adapt on the go, a model adapted with TTA cannot be safely deployed on the original source domain.\nShowing that catastrophic forgetting also affects test-time adaptation further demonstrates the importance of providing continuously shifted sequences in driving datasets, and we hope that future research will attempt to mitigate this problem. \n\n\\section{Conclusion}\nWe introduce \\thedataset{}, a multi-task driving dataset featuring the most important perception tasks under discrete and continuous domain shifts.\nThanks to our dataset design, we demonstrate several new findings on different adaptation strategies and uncertainty estimation methods.\nAlthough simulation environments are still far from being a perfect representation of the real world, they allow inexpensive data collection and annotation.\nMoreover, we empirically demonstrate that conclusions drawn from our dataset hold in real-world datasets.\nTo the best of our knowledge, \\thedataset{} is the largest synthetic dataset for autonomous driving, providing the most inclusive set of annotations and conditions.\nWe hope that providing the first dataset with realistic continuous domain shifts will contribute to shaping the data collection paradigm for real-world driving datasets and promote advances in test-time learning and adaptation.\n\n\\clearpage\n{\\small\n}\n\n\\clearpage\n\n\\section*{Appendix}\nWe provide additional details on our dataset in \\autoref{sec:dataset_details}.\nIn particular, we report the sensor layout (\\autoref{ssec:dataset_sensors}), annotation details (\\autoref{ssec:dataset_annotations}), extensive information on dataset generation (\\autoref{ssec:dataset_generation}) and dataset statistics (\\autoref{ssec:dataset_statistics}).\n\nMoreover, we conduct additional experiments in \\autoref{sec:suppl_exp}. \nWe provide baselines on multitask learning under continual domain shift (\\autoref{sec:suppl_exp_multitask}), and conduct ablation studies on joint training with real-world data (\\autoref{sec:suppl_exp_joint_training}) and the optimal dataset size for each task (\\autoref{sec:suppl_exp_dataset_size}). Further, we propose a qualitative comparison between properties of \\thedataset{} and the VIPER dataset~\\cite{Richter_2017} (\\autoref{sec:viper}), and ablate on the model failures on the rainy and foggy domains (\\autoref{sec:error}).\n\nImplementation details for each experiment conducted in this work are reported in \\autoref{sec:implementation_details} for full reproducibility.\n\n\\def\\thesection{\\Alph{section}}\n\\setcounter{section}{0}\n\n\\begin{figure}[b]\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/sensors_2.pdf}\n    \\caption{The vehicle system and the sensor layout. Except for stereo cameras, all the cameras are located on a circle centered at the vehicle reference system's origin (blue dot). LiDAR and motion sensors are located at the origin. Axes directions of the vehicle system are shown at the bottom left corner. \\textit{Best viewed in color.}}\n    \\label{fig:sensors}\n\\end{figure}\n\n\\section{Dataset Details} \\label{sec:dataset_details}\nThe detailed user guide and additional information can be found at \\url{https://www.vis.xyz/shift}.\n\n\\subsection{Reference systems and sensor layout} \\label{ssec:dataset_sensors}\nThe dataset has three levels of reference systems: \\textit{world}, \\textit{vehicle}, and \\textit{camera}. The world system represents the absolute position of objects. The vehicle system is used for storing all 3D annotations. The camera systems are the reference systems used for each individual camera.\n\n\\autoref{tab:sensors} summarizes the supported sensors. We set up the vehicle system following KITTI's convention and the right-hand rule.\nSpecifically, the origin is located at the center of the ego-vehicle (marked as the blue dot in \\autoref{fig:sensors}).\nIts \\textit{x}, \\textit{y}, and \\textit{z} axes point in the right, down, and front directions, respectively (\\autoref{fig:sensors}, bottom left). \nAll the sensors are located on a circle centered at the vehicle reference system's origin, except for the stereo cameras that are placed on the left to the front camera, with a horizontal displacement of $20 \\mathrm{cm}$. \nAll the cameras have a field-of-view (FoV) of $90\\degree$.\nThe 128-channel LiDAR sensor has a vertical FoV range of $[-10\\degree, +10\\degree]$ and a scan rate of 1.12M points per second. \n\n\\begin{table}[t]\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{ll p{0.48\\linewidth}}\n    \\toprule\n      \\textbf{Sensor}  &  \\textbf{Data type} & \\textbf{Position} \\\\\n         \\midrule\n        RGB camera & 24-bit RGB & 5 $\\times$ RGB cameras (front, left / right $45\\degree$, left / right $90\\degree$). \\\\\n        Stereo camera & 24-bit RGB & Additional RGB camera offsetting $20\\mathrm{cm}$ toward left from the center. \\\\\n        Depth camera & 24-bit Gray & Same as front view RGB camera. \\\\\n        Optical flow & 32-bit UV & Same as front view RGB camera. \\\\\n        GNSS / IMU & Vector & Center of vehicle. \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{The data type and position settings of the sensors.}\n    \\label{tab:sensors}\n\\end{table}\n\nAnnotations stored in the vehicle system can be easily converted into the camera systems. Here, the front cameras and LiDAR sensor have camera systems identical to the vehicle system, so no conversion is needed for them. For other cameras, a vehicle-to-camera matrix (\\ie{} intrinsic and extrinsic parameters) is provided to transform the annotations so that they fit each camera. \n\n\\subsection{Annotation details} \\label{ssec:dataset_annotations}\nWe present detailed specifications for the annotation set provided in \\thedataset{}.\n\n\\paragraph{Object detection} is a fundamental localization task for scene understanding and a basis for numerous downstream driving tasks, including multiple object tracking (MOT) and object re-identification (ReID). \nWe provide 2D/3D bounding box annotations and object identities for six categories of traffic participants, \\ie{} car, truck, bus, bicycle, motorcycle, and pedestrian, together with the visibility attributes `occluded' and `truncated'.\nMoreover, for each box, we provide fine-grained object classes (\\eg{} vehicle model type).\n\nWhile previous datasets only provide 7 DoF (\\ie{} only yaw angle) 3D boxes~\\cite{geiger2013vision,caesar2020nuscenes,sun2020scalability}, we provide 9 DoF annotations and use the Euler angle system (\\textit{i.e.}, yaw, roll, pitch) to represent the orientation for bounding boxes in 3D space.\n\n\\paragraph{Image segmentation} is a fundamental pixel-level perception task. \nFor each frame, we provide panoptic (\\ie{} instance and semantic) segmentation labels on the 23 classes of the Cityscapes~\\cite{cordts2016cityscapes} annotation scheme. \nTogether with 2D bounding boxes, segmentation labels can be used in multi-object tracking and segmentation (MOTS) and multi-object panoptic tracking (MOTP) tasks.\n\n\\begin{table*}[t]\n\\footnotesize\n\\setlength{\\tabcolsep}{3pt}\n\\begin{tabular}{@{}lllll@{}}\n\\toprule\n\\textbf{Category} $\\mathcal{H}_i$ &\n  \\textbf{Candidate dom.} $h_i^{(j)}$ &\n  \\textbf{BDD100K eq.} &\n  \\textbf{Environmental parameters } &\n  \\textbf{Degrees of shift} \\\\ \\midrule\n\\multirow{6}{*}{Time of day} &\n  {noon} &\n  \\multirow{2}{*}[1pt]{$\\left.\\rule{0cm}{0.3cm}\\right\\}$ daytime} &\n  Sun altitude angle = \\{90, 75, 60, 45, 30\\} &\n  \\multirow{6}{*}{altitude angle $\\in [-5, 90]$} \\\\\n & morning / afternoon &                                & Sun altitude angle = \\{15, 10, 5\\}                                    &  \\\\ \n & dawn / dusk         & \\multirow{2}{*}[1pt]{$\\left.\\rule{0cm}{0.3cm}\\right\\}$ dawn / dusk} & Sun altitude angle = \\{4, 3, 2\\}                                      &  \\\\\n & sunrise / sunset    &                                & Sun altitude angle = \\{1, 0, -1\\}                                     &  \\\\ \n & night               & \\multirow{2}{*}[1pt]{$\\left.\\rule{0cm}{0.3cm}\\right\\}$ night}         & Sun altitude angle = \\{-2, -3\\}                                       &  \\\\\n & {dark night}          &                                & Sun altitude angle = \\{-4, -5\\}                                       &  \\\\ \\midrule\n\\multirow{10}{*}{Weather} &\n  {clear} &\n  \\multirow{2}{*}[1pt]{$\\left.\\rule{0cm}{0.3cm}\\right\\}$ clear}  &\n  cloudiness = \\{0, 5\\} &\n  \\multirow{4}{*}{cloudiness $\\in [0, 100]$} \\\\\n & slight cloudy       & \n& cloudiness = \\{10, 15\\}        \n&  \\\\\n & partly cloudy              &     partly cloudy                           & cloudiness = \\{25, 50, 70\\}                                               &  \\\\\n & {overcast}            & overcast                       & cloudiness = 100                                                      &  \\\\ \\cmidrule(rr){2-5} \n &\n  {small rain} &\n  \\multirow{3}{*}{rainy} &\n  cloudiness = 70; precipitation = 20; deposit = 60; fog den. = 3 &\n  \\multirow{3}{*}{precipitation $\\in [0, 100]$} \\\\\n & mid rain            &                                & cloudiness = 80; precipitation = 50; deposit = 80; fog den. = 3    &  \\\\\n & {heavy rain}          &                                & cloudiness = 100; precipitation = 100; deposit = 100; fog den. = 7 &  \\\\ \\cmidrule(l){2-5} \n &\n  {small fog} &\n  \\multirow{2}{*}{foggy} &\n  cloudiness = 60; fog density = 30; fog distance = 15 &\n  \\multirow{2}{*}{fog density $\\in [0, 100]$} \\\\\n & {heavy fog}           &                                & cloudiness = 80; fog density = 90; fog distance = 20                  &  \\\\ \\midrule\n\\multirow{3}{*}{Vehicle density} &\n  sparse &\n  - &\n  num of vehicle = 50 &\n  \\multirow{3}{*}{\\begin{tabular}[c]{@{}l@{}}vehicle per map,\\\\vehicle per frame\\end{tabular}} \\\\\n & moderate            & -                              & num of vehicle = 100                                                  &  \\\\\n & crowded             & -                              & num of vehicle = 250                                                  &  \\\\ \\midrule\n\\multirow{3}{*}{Pedestrian density} &\n  sparse &\n  - &\n  num of pedestrians = 100 &\n  \\multirow{3}{*}{\\begin{tabular}[c]{@{}l@{}}pedestrian per map,\\\\ pedestrian per frame\\end{tabular}} \\\\\n & moderate            & -                              & num of pedestrians = 200                                              &  \\\\\n & crowded             & -                              & num of pedestrians = 400                                              &  \\\\ \\bottomrule\n\\end{tabular}\n\\caption{Definitions of the domain category and candidate domains, used for discrete domain shifts. Each category has a group of candidate domains.  For each candidate domain, we show its equivalent domain label in BDD100K and the environmental parameters for simulation. }\n\\label{tab:parameters}\n\\end{table*}\n\\paragraph{Depth estimation} is an essential step to extend the 2D perception tasks into the 3D setting. We provide the depth labels aligned with the front-view RGB camera to enable image- and video-based monocular and stereo depth estimation. Depth resolution is $1\\mathrm{mm}$.\n\n\\paragraph{Optical flow estimation} is an essential task for driving algorithms involving motion. However, existing large-scale datasets typically do not provide optical flow annotations due to the high labeling cost. Representing the relative motion between each pixel in a pair of images, optical flow can be instrumental in object tracking and ego-motion tasks. \nWe provide the optical flow labels in the UV map format, also used in KITTI~\\cite{geiger2013vision}.\n\n\\subsection{Data generation pipeline} \\label{ssec:dataset_generation}\n\nWe introduce the pipeline that used to generate the discrete and continuous domain shifts.\n\n\\paragraph{Disrete shift.}\nAs discussed in \\autoref{ssec:dataset_design}, we set up an efficient sampling pipeline that can cover a diverse combination of conditions. To determine the environmental parameters of each sequence, we use a technique similar to random search. In \\autoref{tab:parameters}, we define 4 categories of domain shifts, \\textit{e.g.}, time of day, weather, vehicle density, and pedestrian density. For the $i$-th category ($1\\leq i \\leq 4$), we define a \nset of \\textit{candidate} domains, $\\mathcal{H}_{i} = \\{h_i^{(1)}, \\cdots , h_i^{(n_i)} \\}$, where each candidate $h_i^{(j)}$ corresponds to a certain group of environmental parameters, defined in the \\autoref{tab:parameters}. Note that the parameter can be a fixed value or a set of values. For the set of values, we again uniformly sample one value out of the set. \n\nOur sampling method for the discrete domain shifts can be summarized as following. A sequence is generated with a fixed parameter vector $\\bm{\\theta} = \\theta_1 \\cup \\cdots \\cup \\theta_m$, where each $\\theta_i$ is sampled uniformly across all candidates in the $i$-th category, \\textit{i.e.},\n\\begin{align}\n    h_i^{(j)} & \\sim \\mathrm{Uniform}(\\mathcal{H}_{i}), \\quad \\forall i = \\{1,2,3,4\\} \\\\\n    \\theta_i & \\sim h_i^{(j)}\n\\end{align}\nThis pipeline guarantees the uniform marginal distribution of candidates conditioned on any category. Using this pipeline, we can easily add data without breaking the distribution of domains. Moreover, any subset of sequences of \\thedataset{} has the same distribution, allowing a fair experiment on the impact of data amount.\n\n\\begin{table*}[t]\n\\footnotesize\n\\centering\n\\setlength{\\tabcolsep}{14pt}\n\\begin{tabular}{@{}llll@{}}\n\\toprule\n\\multirow{2}{*}{\\textbf{Continuous shift type}} & \\multicolumn{3}{c}{\\textbf{Environmental parameters}}                          \\\\ \\cmidrule(l){2-4} \n                                                & \\textbf{Beginning state} ($t=0$)                   & \\textbf{Intermediate state} ($t=0.2$)  & \\textbf{End state} ($t=1$)                     \\\\ \\midrule\nTime of day                                     & Sun altitude angle = 90 & -            & Sun altitude angle = -5 \\\\ \\hline\nCloudiness                                      & cloudiness = 0          &  -            & cloudiness = 100        \\\\ \\hline\nRaininess &\n  \\begin{tabular}[c]{@{}l@{}}cloudiness = 0, precipitation = 0, \\\\ deposit = 0, fog density = 0\\end{tabular} &\n  \\begin{tabular}[c]{@{}l@{}}cloudiness = 80, precipitation = 50,\\\\ deposit = 80, fog density = 3\\end{tabular} &\n  \\begin{tabular}[c]{@{}l@{}}cloudiness = 100, precipitation = 100,\\\\ deposit = 100, fog density = 7\\end{tabular} \\\\ \\hline\nFogginess &\n  \\begin{tabular}[c]{@{}l@{}}cloudiness = 0, fog density = 0,\\\\ fog distance = 0\\end{tabular} &\n  \\begin{tabular}[c]{@{}l@{}}cloudiness = 60, fog density = 30,\\\\ fog distance = 15\\end{tabular} &\n  \\begin{tabular}[c]{@{}l@{}}cloudiness = 80, fog density = 90,\\\\ fog distance = 20\\end{tabular} \\\\ \\bottomrule\n\\end{tabular}\n\\caption{Definitions of parameters used for continuous domain shifts. The parameters are updated for every frames during driving. The value of parameters are determined by linear interpolation between the state of beginning, intermediate (if applicable) and end. }\n\\label{tab:parameters_continuous}\n\\end{table*}\\paragraph{Continuous shifts.} For sequences with continuous domain shift, the change of parameters happens on one specific domain category $c$, while others are kept unchanged, \\ie{} the frame at time $t$ is generated with the parameter vector\n\\begin{equation}\n    \\bm{\\theta}(t) = \\theta_1 \\cup \\cdots \\cup f_c(t) \\cup \\cdots \\cup \\theta_4 \\; ,\n\\end{equation}\nwhere $f_c(t)$ is obtained by linear interpolation of the states listed in the `Environmental parameters' column in  \\autoref{tab:parameters}. Specifically, $f_c(t)$ is obtained by interpolating the points\n\\begin{equation}\n   (t, \\theta) = [ \\left(0, \\theta_{c, \\mathrm{begin}}\\right),  \\left(0.2, \\theta_{c, \\mathrm{intermediate}}\\right), \\left(1, \\theta_{c, \\mathrm{end}}\\right)] \\; ,\n\\end{equation}\nwhere $t \\in [0, 1]$ represents the degree of continuous shift from the minimum to the maximum parameter allowed for a given domain category.\n\nOur dataset provides 300, 120, 30 continuous shift sequences in 1x, 10x, 100x length respectively, where the base length (1x) is 200 frames. Since continuous domain shift is generated by interpolating between the state of the initial frame and the state of the final frame, the domain shift speed is inversely proportional to the sequence length in frames.\nFurthermore, we provide an additional set of 150 sequences of base length presenting domain shifts simultaneously happening along multiple domain shift directions within the same sequence. \n\n\\paragraph{Domain labeling details.}\nThe degree of shift for each domain category is quantified by a numerical value called \\textit{severity}. For weather conditions, we use percentage values to indicate the degree of severity, where $0\\%$ corresponds to clear weather conditions and $100\\%$ represents the most extreme condition allowed by the CARLA simulator for a given weather direction, \\eg{} cloudiness, precipitation, fog density, or fog distance. We describe the time of day using the Sun's altitude angle to disentangle the lighting condition with the sunrise/sunset time. For the object densities, we use the number of objects per frame as the severity (\\autoref{tab:parameters}).\n\n\\subsection{Dataset statistics} \\label{ssec:dataset_statistics}\n\\thedataset{} is diverse in bounding box scale. \\autoref{fig:bounding_box} (left) plots the object density measured by boxes per frame and shows coverage from 0 to 30 boxes/frame for \\thedataset{}. We compare the distribution with the BDD100K's MOT set. Due to the sparsity of the vehicle/pedestrians density domains, our dataset has on average a higher density of frames counting less bounding boxes than BDD100K, but the crowded frames ($\\geq20$ boxes/frame) show similar trends. Moreover, \\autoref{fig:bounding_box} (right) shows the distribution of bounding box sizes, defined as $\\sqrt{wh}$ where $w$ and $h$ are the width and height of a box. \\thedataset{} covers diverse box sizes ranging from 10 to 650 pixels. We also observe that our dataset has 41.2\\% bounding boxes smaller than 15 pixels while BDD100K has 30.9\\%, showing that our dataset provides challenging conditions for small object detection and tracking.\n\n\\begin{table*}[t]\n\\setlength{\\tabcolsep}{3.3pt}\n\\center\n\\footnotesize\n\\begin{tabular}{@{}lllcccccccc|ccc@{}}\n\\toprule\n\\multirow{2}{*}{\\textbf{Task}}                        & \\multirow{2}{*}{\\textbf{Train}} & \\multirow{2}{*}{\\textbf{Metric}}                                                   & \\multicolumn{1}{c}{\\textbf{Source}} & \\multicolumn{6}{c}{\\textbf{OOD}}                                                                           & \\multicolumn{1}{c}{\\multirow{2}{*}{\\textbf{OOD Avg.}}} & \\multicolumn{1}{c}{\\multirow{2}{*}{$\\Delta_{\\text{Source}}$}} & \\multicolumn{1}{c}{\\multirow{2}{*}{$\\Delta_{\\text{OOD}}$}} & \\multicolumn{1}{c}{\\multirow{2}{*}{$\\Delta_{\\text{S} \\rightarrow \\text{O}}$}} \\\\  \\cmidrule(lr){4-4} \\cmidrule{5-10} \n                                           &                        &                           & {clear-daytime}                      & {cloudy} & {overcast} & {foggy} & {rain} & {dawn/dusk} & {night} & \\multicolumn{1}{l}{}                         & \\multicolumn{1}{l}{}                          & \\multicolumn{1}{l}{}                       & \\multicolumn{1}{l}{}                       \\\\ \\midrule\n\\multirow{8}{*}{\\begin{tabular}[c]{@{}l@{}}Semantic \\\\ segmentation (S)\\end{tabular}}   & S                      & \\multirow{8}{*}{mIoU (\\%) $\\uparrow$}                                                & 69.1                                & 40.6            & 40.6              & 21.5           & 19.6          & 18.1               & 8.9            & 24.9                                         & -                                             & -                                          & -64.0\\%                                    \\\\\n                                                                                        & S + D                  &                                                                           & \\textbf{75.2}                       & 53.8            & 52.6              & 24.3           & 26.6          & 24.0               & 9.9            & 31.9                                         & \\textbf{8.9\\%}                                & 28.1\\%                                     & -57.6\\%                                    \\\\\n                                                                                        & S + F                  &                                                                           & 69.4                                & 51.8            & 54.7              & 26.4           & 22.4          & 22.7               & 9.8            & 31.3                                         & 0.4\\%                                         & 25.8\\%                                     & -54.9\\%                                    \\\\\n                                                                                        & S + D + F              &                                                                           & 71.8                                & 50.0            & 51.9              & 23.5           & 24.0          & 22.1               & 9.5            & 30.2                                         & 3.8\\%                                         & 21.2\\%                                     & -58.0\\%                                    \\\\\n                                                                                        & S + I                  &                                                                           & 74.8                                & \\textbf{63.9}   & \\textbf{68.1}     & \\textbf{41.0}  & \\textbf{36.8} & \\textbf{37.3}      & \\textbf{23.6}  & \\textbf{45.1}                                & 8.2\\%                                         & \\textbf{81.3\\%}                            & \\textbf{-39.7\\%}                           \\\\\n                                                                                        & S + D + I              &                                                                           & 75.0                                & 62.4            & 65.1              & 37.4           & 35.4          & 35.3               & 20.5           & 42.7                                         & 8.6\\%                                         & 71.6\\%                                     & -43.1\\%                                    \\\\\n                                                                                        & S + F + I              &                                                                           & 72.5                                & 58.3            & 59.6              & 35.8           & 27.2          & 28.8               & 14.4           & 37.3                                         & 4.9\\%                                         & 50.1\\%                                     & -48.5\\%                                    \\\\\n                                                                                        & S + D + F + I          &                                                                           & 74.7                                & 60.6            & 59.6              & 37.1           & 32.7          & 33.1               & 19.3           & 40.4                                         & 8.1\\%                                         & 62.3\\%                                     & -45.9\\%                                    \\\\ \\midrule\n\\multirow{8}{*}{\\begin{tabular}[c]{@{}l@{}}Depth\\\\ estimation (D)\\end{tabular}}                                                   & D                      & \\multirow{8}{*}{SILog $\\downarrow$}                                                    & 17.8                                & 28.3            & 23.1              & 81.9           & 46.3          & 54.6               & 63.2           & 49.6                                         & -                                             & -                                          & -64.1\\%                                    \\\\\n                                                                                        & D + S                  &                                                                           & 16.9                                & 25.2            & 22.4              & 65.7           & 43.0          & 49.4               & 57.6           & 43.9                                         & 5.6\\%                                         & 13.0\\%                                     & -61.6\\%                                    \\\\\n                                                                                        & D + F                  &                                                                           & 19.3                                & 25.3            & 20.4              & 66.6           & 45.3          & 50.3               & 54.4           & 43.7                                         & -7.8\\%                                        & 13.4\\%                                     & -55.8\\%                                    \\\\\n                                                                                        & D + S + F              &                                                                           & 19.6                                & 26.9            & 24.7              & 67.8           & 45.1          & 52.1               & 56.6           & 45.5                                         & -9.2\\%                                        & 8.9\\%                                      & -56.9\\%                                    \\\\\n                                                                                        & D + I                  &                                                                           & 17.3                                & 21.0            & 16.8              & 66.4           & 35.1          & 42.4               & 48.4           & 38.3                                         & 2.9\\%                                         & 29.3\\%                                     & -54.8\\%                                    \\\\\n                                                                                        & D + S + I              &                                                                           & \\textbf{16.0}                       & \\textbf{19.5}   & \\textbf{15.4}     & 61.1           & \\textbf{31.2} & \\textbf{38.5}      & \\textbf{42.7}  & \\textbf{34.7}                                & \\textbf{11.0\\%}                               & \\textbf{42.8\\%}                            & -53.8\\%                                    \\\\\n                                                                                        & D + F + I              &                                                                           & 17.8                                & 21.4            & 17.9              & \\textbf{47.9}  & 36.4          & 39.9               & 46.3           & 35.0                                         & 0.1\\%                                         & 41.8\\%                                     & \\textbf{-49.1\\%}                           \\\\\n                                                                                        & D + S + F + I          &                                                                           & 17.6                                & 21.9            & 18.3              & 53.2           & 37.1          & 42.7               & 50.5           & 37.3                                         & 1.0\\%                                         & 33.0\\%                                     & -52.7\\%                                    \\\\ \\midrule\n\\multirow{8}{*}{\\begin{tabular}[c]{@{}l@{}}Optical flow \\\\ estimation (F)\\end{tabular}} & F                      & \\multirow{8}{*}{EPE (px) $\\downarrow$}                                                 & \\textbf{6.0}                        & \\textbf{6.7}    & \\textbf{6.4}      & \\textbf{9.0}   & \\textbf{9.7}  & \\textbf{9.1}       & \\textbf{11.0}  & \\textbf{8.6}                                 & \\textbf{-}                                    & \\textbf{-}                                 & -30.8\\%                                    \\\\\n                                                                                        & F + S                  &                                                                           & 7.8                                 & 8.3             & 8.5               & 10.4           & 12.0          & 10.9               & 12.6           & 10.4                                         & -23.1\\%                                       & -17.4\\%                                    & -25.7\\%                                    \\\\\n                                                                                        & F + D                  &                                                                           & 6.0                                 & 6.9             & 6.4               & 9.4            & 10.4          & 9.6                & 11.8           & 9.1                                          & -0.2\\%                                        & -5.0\\%                                     & -34.2\\%                                    \\\\\n                                                                                        & F + D + S              &                                                                           & 6.1                                 & 8.5             & 8.3               & 10.6           & 12.1          & 11.0               & 13.0           & 10.6                                         & -2.1\\%                                        & -18.5\\%                                    & -42.4\\%                                    \\\\\n                                                                                        & F + I                  &                                                                           & 9.8                                 & 9.6             & 9.6               & 10.4           & 11.3          & 10.6               & 12.1           & 10.6                                         & -38.9\\%                                       & -18.5\\%                                    & \\textbf{-7.7\\%}                            \\\\\n                                                                                        & F + S + I              &                                                                           & 7.7                                 & 8.2             & 7.9               & 9.7            & 10.6          & 9.8                & 11.8           & 9.7                                          & -22.7\\%                                       & -10.8\\%                                    & -20.2\\%                                    \\\\\n                                                                                        & F + D + I              &                                                                           & 8.0                                 & 8.3             & 8.2               & 9.9            & 10.6          & 10.0               & 11.9           & 9.8                                          & -25.5\\%                                       & -12.1\\%                                    & -18.4\\%                                    \\\\\n                                                                                        & F + D + S + I          &                                                                           & 8.1                                 & 8.4             & 8.4               & 10.1           & 11.0          & 10.2               & 12.1           & 10.0                                         & -26.4\\%                                       & -14.0\\%                                    & -19.2\\%                                    \\\\ \\midrule\n\\multirow{8}{*}{\\begin{tabular}[c]{@{}l@{}}Instance \\\\ segmentation (I)\\end{tabular}}   & I                      & \\multirow{8}{*}{\\begin{tabular}[c]{@{}l@{}}mAP (\\%) $\\uparrow$, \\\\ vehicles\\end{tabular}} & 63.9                                & 57.4            & 65.7              & 21.9           & 31.2          & 22.7               & 6.6            & 34.2                                         & -                                             & -                                          & 46.4\\%                                     \\\\\n                                                                                        & I + S                  &                                                                           & 64.9                                & 59.1            & 66.2              & 26.4           & \\textbf{34.4} & 27.1               & \\textbf{14.3}  & \\textbf{37.9}                                & 1.5\\%                                         & \\textbf{10.7\\%}                            & \\textbf{41.6\\%}                            \\\\\n                                                                                        & I + S + D              &                                                                           & 65.0                                & 57.9            & 64.9              & 25.9           & 32.6          & 26.1               & 10.9           & 36.4                                         & 1.6\\%                                         & 6.3\\%                                      & 44.0\\%                                     \\\\\n                                                                                        & I + S + F              &                                                                           & 62.3                                & 57.1            & 64.2              & 21.6           & 31.6          & 23.6               & 7.7            & 34.3                                         & -2.5\\%                                        & 0.1\\%                                      & 45.0\\%                                     \\\\\n                                                                                        & I + D                  &                                                                           & \\textbf{65.9}                       & \\textbf{59.3}   & 66.9              & \\textbf{26.8}  & 32.2          & \\textbf{26.3}      & 11.4           & 37.2                                         & \\textbf{3.1\\%}                                & 8.5\\%                                      & 43.6\\%                                     \\\\\n                                                                                        & I + D + F              &                                                                           & 65.8                                & 50.2            & \\textbf{67.0}     & 21.5           & 31.0          & 22.9               & 6.7            & 33.2                                         & 2.9\\%                                         & -3.0\\%                                     & 49.5\\%                                     \\\\\n                                                                                        & I + F                  &                                                                           & 63.1                                & 56.9            & 65.1              & 20.4           & 28.9          & 21.7               & 5.0            & 33.0                                         & -1.3\\%                                        & -3.6\\%                                     & 47.7\\%                                     \\\\\n                                                                                        & I + S + D + F          &                                                                           & 64.8                                & 57.9            & 65.3              & 22.8           & 31.5          & 23.1               & 8.3            & 34.8                                         & 1.4\\%                                         & 1.6\\%                                      & 46.3\\%                                     \\\\ \\bottomrule\n\\end{tabular}\n\\caption{Multitask learning performances. We evaluate 15 combinations of 4 perception tasks: semantic segmentation (S), monocular depth estimation (D), optical flow estimation (F), and instance segmentation (I). The combinations of S + I, S + D, and S + D + I significantly improve on both tasks' source and OOD performance in their respective tasks. $\\uparrow$ ($\\downarrow$): the higher (lower) the better.}\n\\label{tab:multitask_four_tasks}\n\\end{table*}\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.49\\linewidth]{images/box_per_frame.pdf} \\hfill\n    \\includegraphics[width=0.48\\linewidth]{images/box_size.pdf}\n    \\caption{Cumulative distributions of the bounding box per frame (left) and  bounding box size measured in $\\sqrt{wh}$ (right). We only count the objects in the front camera view.  \\thedataset{} covers various object densities and a wide range of object scale.}\n    \\label{fig:bounding_box}\n\\end{figure}\n\n\\section{Additional Experiments} \\label{sec:suppl_exp}\nTo further highlight the usefulness of \\thedataset{},  we conduct experiments on multitask learning (\\autoref{sec:suppl_exp_multitask}) and joint training with real-world data (\\autoref{sec:suppl_exp_joint_training}). We also investigate the optimal dataset size and sampling rate (\\autoref{sec:suppl_exp_dataset_size}).\n\n\\subsection{Multitask learning} \\label{sec:suppl_exp_multitask}\nIn this experiment, we study whether different perception tasks mutually benefit or interfere with each other when jointly learned with a shared feature extractor.  \nThe wide variety of tasks supported in \\thedataset{} unlocks new opportunities to  investigate different combinations of perception tasks. \nSpecial attention is also paid to the robustness of multitask models under incrementally shifted domains.\n\nSpecifically, we consider four different perception tasks: semantic segmentation, instance segmentation, monocular depth estimation, and optical flow estimation. \nEach task requires the model to learn a distinct encoding function: semantic segmentation requires intermediate activations to encode pixel-level information, instance segmentation requires instance-level information, depth estimation requires contextual information and object priors that allow to convert 2D images to 3D cues, and optical flow requires to encode a function of two images that embodies information on motion perception.\n\n\\paragraph{Multitask model.} To compose a unified multitask model, we use the segmentation model DRN-D-54~\\citesupp{yu2017dilated} as feature extractor and combine it with the heads required for other tasks. The DRN-D-54 model has 8 sequential residual blocks with dilated convolutions and transposed convolutions at the end to generate segmentation results. \nHere, all the modules of DRN-D-54 are used for semantic segmentation. For instance segmentation, we rely on the Feature Pyramid Network (FPN)~\\citesupp{lin2017feature}, Region Proposal Network (RPN), and ROIAlign modules identical to those introduce in Mask R-CNN~\\citesupp{he2017mask}. \nFPN uses the 2nd to 5th blocks' outputs of the DRN-D network. For the optical flow and depth estimation, we adapt the decoders similar to FlowNet~\\citesupp{dosovitskiy2015flownet} and U-Net~\\citesupp{ronneberger2015u}. The decoder has 5 sequential blocks, where each block has one up-sampling layer, followed by a shortcut connection from the feature extractor's corresponding block, and a series of convolution layers. Together with the feature extractor, we obtain an encoder-decoder structure commonly used in dense prediction tasks.\n\n\\begin{figure*}[!t]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{images/multitask_seg.pdf}\n    \\caption{Qualitative results on semantic segmentation for models trained on the clear-daytime domain. Each column represents a model trained on a different task combination: semantic segmentation (S), semantic segmentation + depth estimation (S + D), semantic segmentation + depth estimation + instance segmentation (S + D + I). The three rows show the results on respectively the clear-daytime, rainy, and night domain. The combinations S + D and S + D + I improve the performance against domain shifts.}\n    \\label{fig:multitask_seg}\n\\end{figure*}\n\n\\paragraph{Experiment setup.} We traverse all 15 combinations for the 4 tasks mentioned above.\nOur multitask model is trained with 5,000 frames sampled from the clear-daytime domain in \\thedataset{} and evaluated under different discrete domain shifts. To fit the multitask model into the GPU memory, we reduce the image size to $640 \\times 400$ pixels. \nPlease note that the performance will be slightly affected by the size-reduced images and thus, it is not directly comparable to our baseline experiments in \\ref{tab:ood_performance} of the main paper.\nAll combinations are trained for 100 epochs, when convergence is reached for all tasks. \n\n\\paragraph{Experimental results} are summarized in \\autoref{tab:multitask_four_tasks}. \nEvery model is trained on the clear-daytime domain and tested on different types of shifted domains, indicated with OOD in the Table.\nWe report the average performance on the out-of-distribution domains as OOD avg.\nThe columns $\\Delta_{\\text{Source}}$   and $\\Delta_{\\text{OOD}}$  report for different multitask models the relative Source / OOD avg. performance change on a given task with respect to the performance of a single-task model trained on that specific task. \nThe column $\\Delta_{S~\\xrightarrow{}~O.}$ reports for different multitask models the relative change from Source to OOD avg. performance on a given task. \nBelow are our observations.\n\n\\textit{\\textbf{Multitask learning improves robustness.}} \nWe observe that specific combinations of tasks largely improve the single-task model performance on the source domain. For instance, the combination of semantic segmentation (S) + depth (D) + instance segmentation (I) boosts the source domain performance by 8.6\\% / 11.0\\% / 1.6\\% on the respective tasks. \nSimilar improvements are observed for other combinations, including S + I and S + D.  We visualize the results of these combinations in \\autoref{fig:multitask_seg}. \nThis is possibly due to the intertwined nature of such tasks. In particular, depth and semantics both need to learn contextual features from neighboring pixels, and both instance and semantics segmentation need to segment parts of the image.\n\nFurther, multitask learning often significantly increases the generalization of a model to domain shifts. For example, the combination of S + D + I improves the OOD performance in the respective tasks by 71.6\\% / 42.8\\% / 6.3\\%. \nThe improvements are substantially greater than the improvements on the source domain, suggesting that the increase in model's robustness is not attributable to the increase in the overall model's performance as seen on the source domain. \nWe argue that this is potentially due to the model learning more general features that are shareable across tasks and, consequently, also more general under domain shifts. \nFor example, the addition of instance segmentation typically causes the greatest robustness improvements. This might be due to the complex nature of the instance segmentation task, which requires to encode features capable of both detecting and segmenting objects in an image.\n\n\\textit{\\textbf{Instance segmentation can only be improved mildly.}} Instance segmentation is only improved at most by 10.7\\% on OOD performance by other tasks. \nAs previously mentioned, we hypothesize that instance segmentation already learns more general features due to its nature. Thus, the addition of other tasks provides only mild improvements.\nOn the other hand, however, when combined with other tasks, instance segmentation largely boosts their robustness, \\eg{} S + I and D + I.\n\n\\textit{\\textbf{Optical flow is heavily affected by other tasks.}} Unlike the previous tasks that benefit from multitask learning, optical flow shows a different behavior.  \nAlthough optical flow can improve other tasks' robustness (\\eg{} S + F and D + F), the optical flow itself is negatively affected by the addition of other tasks. \nWhen jointly trained with other tasks, its performance drops by a large margin, ranging from -0.2\\% to -38.9\\%. \nA possible explanation is that the optical flow task, which takes a pair of frames as input, learns a different encoding function than other non-temporal tasks requiring only one frame.\nTo learn a feature extractor shared across the two different types of inputs, the model shows to sacrifice its effectiveness on the task requiring two images.\nThis suggests that combining different tasks is not trivial; instead, it requires extensive evaluation and comparison.\n\\thedataset{} provides a playground to develop novel multitask learning techniques and to investigate and solve the multiple challenges presented by such an interesting problem. \n\n\\textit{\\textbf{Domain shift is only partially mitigated.}}  \nWhile the model's robustness can be improved by multitask learning, the domain shifts provided in \\thedataset{} still pose a tremendous threat to the robustness under domain shift. \nFor all the evaluated tasks, the minimum average OOD performance drop with respect to the corresponding source performance ($\\Delta_{S~\\xrightarrow{}~O.}$) amounts to $\\sim$~40\\%. \nUnder extreme conditions, \\eg{} foggy and night, the performances are degraded even more than 60\\%, which indicates real-life risks if autonomous vehicles heavily rely on such models.\n\nBy introducing \\thedataset{}, which supports multi-domain and multitask studies in a single dataset, we hope to foster future research on multitask domain adaptation algorithms to counteract these domain gaps effectively.\nMoreover, we hope that the continuous domain shifts provided in our dataset will shed new light on this challenging problem.    \n\n \n\n\\subsection{Joint training with real-world data} \\label{sec:suppl_exp_joint_training}\n\nWe investigate whether the domain variations in our dataset in combination with a specific domain of real-world data can make a model more robust to domain shift compared to a model only trained on the real-world data. \nSpecifically, we jointly train the model with the source domain data (\\textit{i.e.}, clear daytime) from BDD100K and all domain variations from ours. The model is then evaluated on other domains of BDD100K.\nWe employ the Faster R-CNN~\\citesupp{ren2015faster} as the model for object detection and DRN-D-54~\\citesupp{yu2017dilated} for semantic segmentation. The models are learned with the same amount of data from BDD100K but with different amounts of data from \\thedataset{}. \n\n\\paragraph{Object detection} results are shown in \\autoref{tab:det_joint}. We observe that the joint training provides a relative improvement of the source domain and OOD performance amounting to 2.52\\% and 3.40\\%, respectively.\n\n\\paragraph{Semantic segmentation} has  similar trends. As shown in \\autoref{tab:seg_joint}, source domain mIoU improves from 46.04\\% to 51.20\\%, with a relative improvement of 10.34\\%. Moreover, out-of-domain mIoU rises by a relative 5.30\\% from 37.37\\% to 39.76\\%. \n\nThese results suggest that, if a model is trained on a limited real-world domain, jointly training with the variety of domains provided by our dataset will improve the robustness of the model to real-world shifts.\n\n\\begin{table}[h]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{8pt}\n\\begin{tabular}{lcccc}\n\\toprule\n\\multirow{2}{*}{\\textbf{Training set}} & \\multicolumn{2}{c}{\\textbf{Source domain}} & \\multicolumn{2}{c}{\\textbf{OOD avg.}}    \\\\ \n\\cmidrule(lr){2-3} \\cmidrule(lr){4-5}\n                            & \\multicolumn{1}{c}{AP} & AP$_{75}$ & AP    & AP$_{75}$ \\\\ \\midrule\nBDD100K                     & 0.318                  & 0.312     & 0.265 & 0.251     \\\\\nBDD100K + 2k frames  & 0.320                   & 0.327     & 0.267 & 0.267     \\\\\nBDD100K + 5k frames    & \\textbf{0.326} & \\textbf{0.334} & \\textbf{0.274} & \\textbf{0.271} \\\\\nBDD100K + 10k frames & 0.325                  & 0.329     & 0.254 & 0.238     \\\\\n\\bottomrule\n\\end{tabular}\\caption{Joint training for object detection. Generalization ability is improved with a proper amount of data. \\label{tab:det_joint}}\n\\end{table}\n\\begin{table}[h]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{8pt}\n\n\\begin{tabular}{lcc}\n\\toprule\n \\textbf{Training set}        &  \\textbf{Source domain}   & \\textbf{OOD avg.} \\\\ \\midrule\nBDD100K                     & 46.04   & 37.37                             \\\\\nBDD100K + 6k frames  & 47.11 &  38.56                             \\\\\nBDD100K + 12k frames &\n  \\textbf{51.20} &\n  \\textbf{39.76} \\\\\nBDD100K + 24k frames & 51.09 & 39.23   \\\\\n\\bottomrule\n\\end{tabular}\\caption{Joint training for semantic segmentation. We report the mIoU. Generalization ability is improved with a proper amount of data. \\label{tab:seg_joint}}\n\\end{table}\n\n\\subsection{Dataset size} \\label{sec:suppl_exp_dataset_size}\nTo understand the impact of dataset size and optimize the design of the dataset, we conduct ablation studies on: (1) sampling rate and (2) amount of sequences. \nEvery model is trained on clear-daytime sequences.\n\n\\begin{table}[t]\n    \\centering\n    \\footnotesize\n    \\setlength{\\tabcolsep}{5pt}\n    \\begin{tabular}{l|cccccc}\n    \\toprule\n       \\textbf{Frame rate (Hz)}  & 0.1 & 0.2 & 0.5 & 1 & 5 & 10 \\\\\n       \\midrule\n       \\textbf{\\# Frames ($\\times$1k)} & 7.5 & 15 & 37.5 & 75 & 375 & 750 \\\\ \\midrule\n        Seg. (mIoU, \\%) & 62.6 & 62.9 & \\textbf{63.1} & 63.0 & 62.9 & - \\\\\n        Det. (mAP, \\%) & 40.6 & 43.1 & 45.8  & 46.8 & \\textbf{48.4} & - \\\\\n        MOT (MOTA, \\%) & 25.6  & 34.7 & 45.2 & 49.3 & 54.1 &  \\textbf{54.9} \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{\\footnotesize{Performance of different tasks at increasing sampling rates. Training and testing on the same 1500 sequences from all domains.}}\n    \\label{tab:frame_rate}\n    \\vspace{-1em}\n\\end{table}\n\n\\begin{table}[h]\n    \\centering\n    \\footnotesize\n    \\setlength{\\tabcolsep}{5pt}\n    \\begin{tabular}{l|cccccc}\n    \\toprule\n       \\textbf{Training sequence }  & 350 & 750 & 1500 & 2000 & 3000  \\\\\n       \\midrule\n        Seg. (mIoU, \\%) & 59.4 & 61.4 & 63.0 & 62.6 & \\textbf{63.1}\\\\\n        Det. (mAP, \\%) & 41.2 & 45.1 & 46.8  & 48.0 & \\textbf{50.1} \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{\\footnotesize{Performance of different tasks at increasing sequences number. Training and testing on the data of 1Hz from all domains.}}\n    \\label{tab:sequences}\n    \\vspace{-1em}\n\\end{table}\n\n\\paragraph{Frame rate.} \nTo avoid the model learning from redundant information, we study what is the optimal sampling rate to achieve the best performance on a given task.\nHere, we test the semantic segmentation, object detection, and multiple obeject tracking performance on a set of images sampled at different frame rates from a fixed set of 2000 sequences. \nWe notice that performance of different tasks starts to saturate at different sampling rates  (\\autoref{tab:frame_rate}). \nFor image-based tasks, such as segmentation and detection, we argue that the information provided by adjacent frames can be redundant, and increasing the sampling rate over a certain threshold have insignificant benefits on the resulting model performance.\nHowever, for video-based tasks, like multi-object tracking, the inter-frame information is crucial. A lower frame rate leads to lose a considerable amount of information, thus severely reducing the model performance (\\autoref{tab:frame_rate}, third row). \n\nOur dataset is collected at a fixed frame rate of 10Hz, which is necessary to support a wide range of perception tasks.  \nHowever, according to the experiments on the sampling rate, we also provide a subset sampled at 1Hz for image-based perception tasks.\n\n\\paragraph{Amount of sequences} is another factor affecting the performance. \nHere, we test semantic segmentation and object detection performance on a varying number of sequences sampled at 1Hz.\nIn \\autoref{tab:sequences}, we find that the performance continuously increases up to 3000 sequences. \nHowever, the performance gain is diminishing the more sequences we add. This is potentially due to the limited environmental variation in the simulator. \nTo balance between size and learning performance, we set the total number of sequences to 3000 for the discrete set. \nTogether with our sampling pipeline (\\autoref{ssec:dataset_generation}), the current size of \\thedataset{} guarantees \nthat for each BDD100K's domain label, we have more than 500 corresponding sequences for training and testing. \n\n\\subsection{Comparison with VIPER} \\label{sec:viper}\nAs a synthetic dataset, VIPER~\\cite{Richter_2017} also presents sequences from discrete domain shifts. \nHere, we compare the segmentation performance under domain shifts in VIPER, SHIFT and BDD100K (\\autoref{tab:viper}). We find that the adverse conditions presented in VIPER provide a less relevant threat to model generalization, highlighting how SHIFT mimics more closely real-world trends.\n\n\\begin{table}[!ht]\n    \\centering\n    \\footnotesize\n    \\setlength{\\tabcolsep}{5pt}\n    \\begin{tabular}{l|cccc|c}\n    \\toprule\n       \\textbf{Dataset} & \\underline{daytime} ($M_0$) & sunset & night & rain &  $\\frac{\\max{\\Delta M}}{M_{0}}$ \\\\\n       \\midrule\n        VIPER & 59.3 & 57.6 & 55.1 & 53.0  & -10.6\\% \\\\\n        SHIFT (\\textit{ours})  & 83.6 & 60.4 & 42.8 & 54.6 & -48.8\\% \\\\ \\midrule\n        BDD100K & 47.9 & - & 20.6 & 37.6  &  -57.0\\% \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{\\footnotesize{Out-of-distribution performance on different datasets of a segmentation model (DRN-D) trained on the daytime domain. The last column represents the maximal relative performance drop w.r.t. source.}}\n    \\label{tab:viper}\n    \\vspace{-1em}\n\\end{table}\n\\subsection{Error analysis for foggy and rainy domains} \\label{sec:error}\nAs noticeable in \\autoref{fig:trend}, detection and segmentation models show a slightly different behavior under different types of domain shift. While it is worth noticing that segmentation and detection have different label sets, we here analyze the differences in performance on the two domains presenting the largest discrepancy across the two tasks, \\ie{} foggy and rainy. \nFor example, we find that the most drastically affected class for segmentation in the rainy domain is `sky' (-69\\% mIoU w.r.t. clear-daytime), with 25\\% of the corresponding pixels misclassified as `building', as opposed to only 2\\% under foggy conditions. \nFor object detection, we find that most of the errors come from missed detections. The shifted domains lower the classification confidence below the pre-selected threshold, with foggy posing a greater challenge (car AP drops by 74\\% on foggy vs 40\\% on rainy).\n\n\\section{Implementation Details} \\label{sec:implementation_details}\nIn this section, we describe the implementation details and metrics for each task in \\autoref{tab:ood_performance} and \\autoref{fig:trend}.\n\n\\paragraph{Object detection.} We compare Faster R-CNN~\\cite{ren2015faster}, Cascade R-CNN~\\cite{cai2018cascade}, and YOLO v3~\\cite{redmon2018yolov3}. The backbone network for the first two methods is ResNet-50~\\cite{he2016deep}, while YOLO v3 uses DarkNet~\\cite{redmon2017yolo9000} as its backbone. We use the mean Average Precision (mAP) as the metric for 2D bounding boxes. We train the models on 50k frames of data, following the ``1x'' schedule provided in the \\texttt{mmdetection} library~\\citesupp{mmdetection}.\n\n\\paragraph{Semantic segmentation.} We also compare three models for semantic segmentation, DeepLab v3+~\\cite{deeplabv3plus2018}, Fully Convolutional Network (FCN)~\\cite{long2015fully}, and DRN-D-54~\\cite{yu2017dilated}. All three models use the ResNet-50~\\cite{he2016deep} as the backbone. We train the models with 20k frames of data until they converge (approximately 150 epochs). We use the mean IoU (mIoU) metric for all evaluations on semantic segmentation.\n\n\\paragraph{Instance segmentation.} We use Mask R-CNN~\\cite{he2017mask} with ResNet-50 backbone and follow the same training routine as  Faster R-CNN~\\cite{ren2015faster}. A segmentation mAP metric is used for evaluation.\n\n\\paragraph{Depth estimation.} We use AdaBins~\\cite{bhat2021adabins} for the depth estimation experiments. It uses a U-Net-like~\\cite{ronneberger2015u} backbone structure and predicts depth with adaptive bins. The model is trained using its official implementation. We follow KITTI's benchmark on depth estimation~\\cite{geiger2013vision}. Specifically, we use the Scale-invariant Logarithm (SILog) metric evaluated on the central crop of the image (\\ie{} Eigen Crop).\n\n\\paragraph{Optical flow estimation.} We use RAFT~\\cite{teed2020raft} for optical flow estimation. The model is fine-tuned from pre-trained weights on the Things Dataset~\\cite{mayer2016large}, with 10k frames of our data. The End-point Error (EPE) metric is used for evaluation.\n\n{\\small\n}\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2109.07644v5.tex",
        "arXiv-2202.08449v2.tex",
        "arXiv-2206.08367v1.tex"
    ],
    "group_id": "group_80",
    "response": "### Title: Comparative Analysis of Simulated Datasets for Autonomous Vehicle Perception and Collaborative Learning\n\n### Introduction\nAutonomous vehicle (AV) perception and collaborative learning have been pivotal research areas in recent years, driven by the increasing need for robust and reliable self-driving systems. The field has seen significant advancements in single-vehicle perception algorithms, largely due to the availability of large-scale datasets and benchmarks that have facilitated the development of 3D object detection, tracking, and semantic segmentation techniques. However, challenges remain in handling occlusions, long-range detection, and adapting to varying environmental conditions. To address these issues, researchers have started exploring cooperative perception methods that leverage Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communication to enhance the performance of perception systems by aggregating information from multiple viewpoints. This approach can compensate for the limitations of individual sensors and improve the overall robustness of AV systems. However, the lack of comprehensive, publicly available datasets for benchmarking these cooperative methods has hindered progress in this area. This summary focuses on three recent papers that introduce new simulated datasets and benchmarks for V2V perception, multi-agent collaborative perception, and continuous multi-task domain adaptation.\n\n### Main Content of Each Paper\n\n#### Paper 1: OPV2V - An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication\nThe first paper introduces OPV2V, a large-scale open simulated dataset designed specifically for V2V perception. The dataset is collected using the CARLA simulator and the OpenCDA co-simulation framework, which enables the generation of realistic traffic scenarios with varying numbers of connected automated vehicles (CAVs). OPV2V includes over 70 diverse scenes, 11,464 frames, and 232,913 annotated 3D vehicle bounding boxes. Each CAV is equipped with 4 cameras, a 64-channel LiDAR, and GPS/IMU sensors, providing rich sensor data for perception tasks. The dataset supports cooperative 3D object detection, Bird's Eye View (BEV) semantic segmentation, tracking, and prediction tasks. The authors also propose an Attentive Intermediate Fusion (AIF) pipeline to aggregate information from multiple CAVs, which can be integrated with existing 3D LiDAR detectors and achieve outstanding performance even with large compression rates. The AIF pipeline consists of metadata sharing, feature extraction, compression, feature sharing, attentive fusion, and prediction header modules. The paper evaluates several state-of-the-art 3D object detectors combined with different fusion strategies (early, late, and intermediate) and demonstrates that intermediate fusion, particularly the AIF pipeline, outperforms other methods in terms of Average Precision (AP) under Intersection-over-Union (IoU) thresholds of 0.5 and 0.7. The dataset and benchmark methods are made publicly available to encourage further research in this field.\n\n#### Paper 2: V2X-Sim - Multi-Agent Collaborative Perception Dataset and Benchmark for Autonomous Driving\nThe second paper presents V2X-Sim, a comprehensive simulated multi-agent perception dataset designed to support V2X-aided autonomous driving. V2X-Sim is generated using the CARLA simulator and SUMO traffic simulation software, providing multi-modality sensor streams from vehicles and Road-Side Units (RSUs) to enable collaborative perception. The dataset includes 10,000 frames, with 2-5 vehicles selected as collaboration agents in each scene. V2X-Sim supports three crucial perception tasks: detection, tracking, and semantic segmentation. The authors benchmark several state-of-the-art collaborative perception methods, including DiscoNet, V2VNet, When2com, and Who2com, using the same evaluation protocol as single-agent perception tasks. The dataset is publicly available, and the authors provide an open-source testbed for researchers to evaluate their models. The paper highlights that collaborative perception methods can significantly improve performance over single-agent perception, especially in scenarios with occlusions and long-range detection. The authors also investigate the robustness of intermediate models against pose noise and compression rates, finding that these models can maintain stable performance under realistic conditions.\n\n#### Paper 3: SHIFT - A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation\nThe third paper introduces SHIFT, a synthetic driving dataset designed to study continuous multi-task domain adaptation in autonomous driving. SHIFT is collected using the CARLA simulator and features a comprehensive sensor suite, including multi-view RGB cameras, stereo cameras, LiDAR, GNSS, IMU, and optical flow sensors. The dataset includes 2,500,000 annotated frames, with sequences of varying lengths that present both discrete and continuous domain shifts along critical environmental directions such as time of day, weather conditions, and object density. The authors provide annotations for 13 perception tasks, including semantic segmentation, instance segmentation, monocular depth estimation, optical flow estimation, and object detection. The paper evaluates the performance of different perception models under domain shifts and investigates the impact of multitask learning on robustness. The authors find that specific combinations of tasks, such as semantic segmentation + depth estimation + instance segmentation, can significantly improve the robustness of models to domain shifts. They also demonstrate that joint training with real-world data can enhance the generalization ability of models. The dataset is publicly available, and the authors hope it will contribute to the development of more robust and adaptable perception systems for autonomous driving.\n\n### Commonalities and Innovations\nAll three papers address the critical need for large-scale, publicly available datasets to support research in autonomous vehicle perception and collaborative learning. They utilize advanced simulation tools like CARLA and SUMO to generate realistic traffic scenarios, providing a rich set of sensor data and annotations for various perception tasks. The datasets are designed to cover a wide range of environmental conditions and scenarios, enabling researchers to study the robustness and generalization of perception models under different domain shifts.\n\n**Commonalities:**\n- **Simulation Tools:** All datasets are generated using CARLA, a widely-used open-source simulator for autonomous driving research.\n- **Sensor Suite:** Each dataset includes a comprehensive set of sensors, such as RGB cameras, LiDAR, GPS, IMU, and RSUs, to provide multi-modality data.\n- **Perception Tasks:** All datasets support multiple perception tasks, including 3D object detection, tracking, and semantic segmentation.\n- **Public Availability:** The datasets and benchmark methods are made publicly available to encourage further research in the field.\n\n**Innovations:**\n- **OPV2V:** OPV2V introduces an Attentive Intermediate Fusion (AIF) pipeline to capture interactions between features from neighboring CAVs, demonstrating superior performance even under large compression rates.\n- **V2X-Sim:** V2X-Sim is the first publicly available dataset that supports both V2V and V2I scenarios, providing a more realistic and comprehensive view of collaborative perception.\n- **SHIFT:** SHIFT is the largest synthetic driving dataset that captures continuous domain shifts, enabling research on continuous test-time adaptation and domain generalization.\n\n### Comparison of Results and Discussion\nThe three papers present different benchmarks and evaluation metrics to assess the performance of perception models under varying conditions. OPV2V focuses on 3D object detection and demonstrates that intermediate fusion, particularly the AIF pipeline, outperforms other fusion methods. V2X-Sim evaluates collaborative perception methods on detection, tracking, and semantic segmentation tasks and finds that intermediate feature-based collaboration strategies, such as V2VNet and DiscoNet, achieve the best performance. SHIFT investigates the impact of domain shifts on multiple perception tasks and demonstrates that multitask learning can significantly improve the robustness of models to domain shifts.\n\n**Performance Metrics:**\n- **OPV2V:** Evaluates 3D object detection using AP at IoU thresholds of 0.5 and 0.7.\n- **V2X-Sim:** Uses HOTA, MOTA, MOTP, and mIoU metrics for tracking and segmentation tasks.\n- **SHIFT:** Employs mIoU for semantic segmentation, mAP for instance segmentation, and SILog for depth estimation.\n\n**Collaborative Perception:**\n- **OPV2V:** Demonstrates that intermediate fusion, especially the AIF pipeline, outperforms early and late fusion methods in terms of AP.\n- **V2X-Sim:** Shows that intermediate feature-based collaboration strategies (V2VNet, DiscoNet) achieve the best performance in collaborative detection, tracking, and segmentation tasks.\n- **SHIFT:** Finds that multitask learning can significantly improve the robustness of models to domain shifts, with specific combinations of tasks providing the most benefits.\n\n**Domain Shifts:**\n- **OPV2V:** Does not explicitly address domain shifts but focuses on V2V communication and information fusion.\n- **V2X-Sim:** Evaluates the performance of collaborative perception methods under different communication scenarios (V2V and V2I).\n- **SHIFT:** Provides a detailed analysis of the impact of domain shifts on different perception tasks and demonstrates that continuous domain shifts represent a critical challenge for current learning systems.\n\n### Conclusion\nThe introduction of OPV2V, V2X-Sim, and SHIFT represents significant advancements in the field of autonomous vehicle perception and collaborative learning. These datasets provide researchers with the tools to study the robustness and generalization of perception models under varying conditions, including occlusions, long-range detection, and continuous domain shifts. The commonality among these datasets is their use of advanced simulation tools to generate realistic traffic scenarios and their comprehensive sensor setups. However, each dataset introduces unique innovations, such as OPV2V's AIF pipeline, V2X-Sim's support for both V2V and V2I scenarios, and SHIFT's focus on continuous domain shifts. Future research directions include the development of more comprehensive datasets that cover a wider range of real-world scenarios, the investigation of novel evaluation metrics for collaborative perception tasks, and the exploration of techniques to mitigate catastrophic forgetting during continuous test-time adaptation. These datasets and benchmarks serve as a foundation for advancing the field of autonomous vehicle perception and collaborative learning, encouraging researchers to develop more robust and adaptable systems for real-world deployment."
}