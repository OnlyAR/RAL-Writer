{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models}\n\n\\begin{document}\n\n\\title{\\vspace{-2.7em} Align your Latents: \\\\ High-Resolution Video Synthesis with Latent Diffusion Models}\n\n\\author{\nAndreas Blattmann\\textsuperscript{1\\;\\,*,\\dag}\n\\quad\\quad\nRobin Rombach\\textsuperscript{1\\;\\,*,\\dag}\n\\quad\\quad\nHuan Ling\\textsuperscript{2,3,4\\;\\,*}\n\\quad\\quad\nTim Dockhorn\\textsuperscript{2,3,5\\;\\,*,\\dag}\n\\vspace{0.15cm}\n\\\\\nSeung Wook Kim\\textsuperscript{2,3,4}\n\\quad\\quad\nSanja Fidler\\textsuperscript{2,3,4}\n\\quad\\quad\nKarsten Kreis\\textsuperscript{2}\n\\vspace{0.25cm}\n\\\\\n{\\small\\textsuperscript{1}LMU Munich \\quad \\textsuperscript{2}NVIDIA \\quad \\textsuperscript{3}Vector Institute \\quad \\textsuperscript{4}University of Toronto \\quad \\textsuperscript{5}University of Waterloo \\vspace{0pt}}\n\\vspace{0.15cm}\n\\\\\n\\small \\textit{Project page:} \\url{https://research.nvidia.com/labs/toronto-ai/VideoLDM/}\n}\n\n\\twocolumn[{\n    \\renewcommand\\twocolumn[1][]{#1}\n    \\maketitle\n    \\begin{center}\n    \\vspace{-15pt}\n        \\centering\n        \\includegraphics[width=\\linewidth]{figures/teaser_fig_10.pdf}\n    \\vspace{-0.7cm}\n    \\captionof{figure}{\n        \\small \\textbf{Video LDM samples}. \\emph{Top:} Text-to-Video generation. \\emph{Bottom:} $512\\times 1024$ resolution real driving scene video generation.\n    }\n    \\label{fig:teaser}\n\n    \\end{center}\n}]\n\n\\begin{figure*}[t!]\n  \\begin{minipage}[c]{0.69\\textwidth}\n  \\vspace{-7mm}\n    \\includegraphics[width=1.0\\textwidth]{figures/pipeline_fig.pdf}\n  \\end{minipage}\\hfill\n  \\begin{minipage}[c]{0.31\\textwidth}\n  \\vspace{-1mm}\n    \\caption{\\small \\textbf{Temporal Video Fine-Tuning.} We turn pre-trained image diffusion models into temporally consistent video generators. Initially, different samples of a batch synthesized by the model are independent. After temporal video fine-tuning, the samples are temporally aligned and form coherent videos. The stochastic generation process before and after fine-tuning is visualised for a diffusion model of a one-dim. toy distribution. For clarity, the figure corresponds to alignment in pixel space. In practice, we perform alignment in LDM's latent space and obtain videos after applying LDM's decoder (see \\Cref{fig:ldm_figure}). We also video fine-tune diffusion model upsamplers in pixel or latent space (\\Cref{sec:method_upsampler}).}\n    \\label{fig:pipeline}\n  \\end{minipage}\n  \\vspace{-7mm}\n\\end{figure*}\\begin{abstract}\n\\vspace{-0.2cm}\nLatent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. \nWe first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and fine-tuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. \nWe focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our \\textbf{Video LDM} on real driving videos \nof resolution $512\\times1024$, achieving state-of-the-art performance. %\nFurthermore, our approach can easily leverage off-the-shelf pre-trained image LDMs, as we only need to train a temporal alignment model in that case.  Doing so, we turn the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model with resolution up to $1280\\times2048$.\nWe show that the temporal layers trained in this way generalize to different fine-tuned text-to-image LDMs. Utilizing this property, we show the first results for personalized text-to-video generation, opening exciting directions for future content creation.\n{\\let\\thefootnote\\relax\\footnote{{\\textsuperscript{*}Equal contribution.}}}\n{\\let\\thefootnote\\relax\\footnote{{\\textsuperscript{\\dag}Andreas, Robin and Tim did the work during internships at NVIDIA.}}}\n\\end{abstract}\n\n\\vspace{-3mm}\n\\section{Introduction}\\label{sec:intro}\n\\vspace{-1mm}\n\n \nGenerative models of images have received unprecedented attention, owing to recent breakthroughs in the underlying modeling methodology. The most powerful models today are built on generative adversarial networks~\\cite{goodfellow2014generative,karras2019style,karras2020analyzing,karras2021aliasfree,sauer2021styleganxl}, autoregressive transformers~\\cite{esser2020taming,ramesh2021dalle,yu2022parti}, and most recently diffusion models~\\cite{sohl2015deep,ho2020ddpm,song2020score,nichol2021improved,dhariwal2021diffusion,ho2021cascaded,nichol2021glide,rombach2021highresolution,ramesh2022dalle2,saharia2022imagen}.\nDiffusion models (DMs) in particular have desirable advantages; they offer a robust and scalable training objective and are typically less parameter intensive than their transformer-based counterparts. \nHowever, while the image domain has seen great progress, \\emph{video} modeling has lagged behind---mainly due to the significant computational cost associated with training on video data, and the lack of large-scale, general, and publicly available video datasets. While there is a rich literature on video synthesis~\\cite{babaeizadeh2018stochastic,svg,lee2018savp,hvrnn,lsvg,Weissenborn2020Scaling,yan2021videogpt,hong2022cogvideo,wu2021godiva,wu2022nuwa,ge2022longvideo,Gupta_2022_CVPR,scene_dyn,yu2022generating,tian2021a, villegas17mcnet,Luc2020TransformationbasedAV,TGAN2020,brooks2022generating,Skorokhodov_2022_CVPR,kahembwe2020lower,hong2022cogvideo,mittal2017sync,Pan2017ToCW,marwah2017attentive,li2017video,gupta2018imagine}, most works, including previous video DMs~\\cite{yang2022video,ho2022video,hoeppe2022diffusion,voleti2022mcvd,harvey2022flexible}, only generate relatively low-resolution, often short, videos.\nHere, we apply video models to real-world problems and generate high-resolution, long videos. \nSpecifically, we focus on two relevant real-world video generation problems: (i) video synthesis of high-resolution real-word driving data, which has great potential as a simulation engine in the context of autonomous driving, and (ii) text-guided video synthesis for creative content generation; see \\cref{fig:teaser}.\n\nTo this end, we build on latent diffusion models (LDMs), which can reduce the heavy computational burden when training on high-resolution images~\\cite{rombach2021highresolution}. \nWe propose \\emph{Video LDMs} and extend LDMs to high-resolution \\emph{video} generation, a particularly compute-intensive task. In contrast to previous work on DMs for video generation~\\cite{yang2022video,ho2022video,hoeppe2022diffusion,voleti2022mcvd,harvey2022flexible}, we first pre-train our Video LDMs on images only (or use available pre-trained image LDMs), thereby allowing us to leverage large-scale image datasets. \nWe then transform the LDM image generator into a video generator by introducing a temporal dimension into the latent space DM and training only these temporal layers on encoded image sequences, \\ie, videos (\\Cref{fig:pipeline}), while fixing the pre-trained spatial layers.\nWe similarly fine-tune LDM's decoder to achieve temporal consistency in pixel space (\\Cref{fig:ldm_figure}).\nTo further enhance the spatial resolution, we also temporally align pixel-space and latent DM upsamplers~\\cite{ho2021cascaded}, which are widely used for image super resolution~\\cite{saharia2021image,li2022srdiff,saharia2022imagen,rombach2021highresolution}, turning them into temporally consistent video super resolution models.\nBuilding on LDMs, our method can generate globally coherent and long videos in a computationally and memory efficient manner. For synthesis at very high resolutions, the video upsampler only needs to operate locally, keeping training and computational requirements low.\nWe ablate our method and test on $512\\times1024$ real driving scene videos, achieving state-of-the-art video quality, and synthesize videos of several minutes length. \nWe also video fine-tune a powerful, publicly available text-to-image LDM, \\textit{Stable Diffusion}~\\cite{rombach2021highresolution}, and turn it into an efficient and powerful text-to-video generator with resolution up to $1280\\times2048$. \nSince we only need to train the temporal alignment layers in that case, we can use a relatively small training set of captioned videos. \nBy transferring the trained temporal layers to differently fine-tuned text-to-image LDMs, we demonstrate personalized text-to-video generation for the first time.\nWe hope our work opens new avenues for efficient digital content creation and autonomous driving simulation.\n\n\\looseness=-1\n\n\\textbf{Contributions.} \\textit{(i)} We present an efficient approach for training high-resolution, long-term consistent video generation models based on LDMs. Our key insight is to leverage pre-trained image DMs and turn them into video generators by inserting temporal layers that learn to align images in a temporally consistent manner (\\Cref{fig:pipeline,fig:ldm_figure}). \\textit{(ii)} We further temporally fine-tune super resolution DMs, which are ubiquitous in the literature. \\textit{(iii)} We achieve state-of-the-art high-resolution video synthesis performance on real driving scene videos, and we can generate multiple minute long videos. \\textit{(iv)} We transform the publicly available \\emph{Stable Diffusion} text-to-image LDM into a powerful and expressive text-to-video LDM, and \\textit{(v)} show that the learned temporal layers can be combined with different image model checkpoints (\\eg, \\emph{DreamBooth}~\\cite{ruiz2022dreambooth}).\n\n\\vspace{-2mm}\n\\section{Background}\\label{sec:background}\n\\vspace{-1mm}\n\\begin{figure}[t!]\n  \\vspace{-0.2cm}\n    \\includegraphics[width=0.49\\textwidth]{figures/ldm_figure.pdf}\n    \\caption{\\small \\textit{Top:} During temporal decoder fine-tuning, we process video sequences with a frozen encoder, which processes frames independently, and enforce temporally coherent reconstructions across frames. We additionally employ a video-aware discriminator. \\textit{Bottom:} in LDMs, a diffusion model is trained in latent space. It synthesizes latent features, which are then transformed through the decoder into images. Note that the bottom visualization is for individual frames; see \\Cref{fig:pipeline} for the video fine-tuning framework that generates temporally consistent frame sequences.} \n    \\label{fig:ldm_figure}\n  \\vspace{-4mm}\n\\end{figure}DMs~\\cite{sohl2015deep,ho2020ddpm,song2020score} learn to model a data distribution $p_{\\text{data}}(\\rvx)$ via \\emph{iterative denoising} and are trained with \\textit{denoising score matching}~\\cite{hyvarinen2005scorematching,lyu2009scorematching,vincent2011,sohl2015deep,song2019generative,ho2020ddpm,song2020score}: Given samples $\\rvx \\sim p_{\\text{data}}$, \\emph{diffused} inputs $\\rvx_\\tau = \\alpha_\\tau \\rvx + \\sigma_\\tau \\rvepsilon, \\; \\rvepsilon \\sim \\gN(\\mathbf{0}, \\mI)$ are constructed; $\\alpha_{\\tau}$ and $\\sigma_\\tau$ define a \\emph{noise schedule}, parameterized via \na diffusion-time $\\tau$, such that the logarithmic signal-to-noise ratio $\\lambda_\\tau = \\log(\\alpha_{\\tau}^2/\\sigma_\\tau^2)$ monotonically decreases. A denoiser model $\\rvf_\\theta$ (parameterized with learnable parameters $\\theta$) receives the diffused $\\rvx_\\tau$ as input and is\noptimized minimizing the denoising score matching objective\n\\begin{align}\n\\E_{\\rvx \\sim p_{\\text{data}}, \\tau \\sim p_{\\tau}, \\rvepsilon \\sim \\gN(\\mathbf{0}, \\mI)} \\left[\\Vert \\rvy - \\rvf_\\theta(\\rvx_\\tau; \\rvc, \\tau) \\Vert_2^2 \\right],\n\\label{eq:diffusionobjective}\n\\end{align}\nwhere $\\rvc$ is optional conditioning information, such as a text prompt, and the target vector $\\rvy$ is either the random noise $\\rvepsilon$ or $\\rvv = \\alpha_\\tau \\rvepsilon - \\sigma_\\tau \\rvx$.\nThe latter objective (often referred to as \\emph{$\\rvv$-prediction}) has been introduced in the context of progressive distillation~\\cite{salimans2022progressive} and empirically often yields faster convergence of the model (here, we use both objectives). Furthermore,\n$p_\\tau$ is a uniform distribution over the diffusion time $\\tau$. The forward diffusion as well as the reverse generation process in diffusion models can be described via stochastic differential equations in a continuous-time framework~\\cite{song2020score} (see~\\Cref{fig:pipeline,fig:ldm_figure}), but in practice a fixed discretization can be used~\\cite{ho2020ddpm}.\nThe maximum diffusion time is generally chosen such that the input data is entirely perturbed into Gaussian random noise and an iterative generative denoising process that employs the learned denoiser $\\rvf_\\theta$ can be initialized from such Gaussian noise to synthesize novel data.\nHere, we use $p_\\tau\\sim\\mathcal{U}\\{0,1000\\}$ and rely on a \\emph{variance-preserving} noise schedule~\\cite{song2020score}, for which $\\sigma_\\tau^2 = 1 - \\alpha_\\tau^2$ (see \\Cref{app:arch,app:exp_details} for details). \n\n\\textbf{Latent Diffusion Models (LDMs)}~\\cite{rombach2021highresolution} improve in computational and memory efficiency over pixel-space DMs by first training a compression model to transform input images $\\rvx{\\sim}p_{\\text{data}}$ into a spatially lower-dimensional latent space of reduced complexity, from which the original data can be reconstructed at high fidelity.\nIn practice, this approach is implemented with a regularized autoencoder, which reconstructs inputs $\\rvx$ via an encoder module $\\gE$ and a decoder $\\gD$, such that the reconstruction $\\hat{\\rvx}{=}\\gD(\\gE(\\rvx)){\\approx}\\rvx$ (\\Cref{fig:ldm_figure}). To ensure photorealistic reconstructions, an adversarial objective can be added to the autoencoder training~\\cite{rombach2021highresolution}, which is implemented using a patch-based discriminator~\\cite{isola2017image}. \nA DM can then be trained in the compressed latent space and $\\rvx$ in \\Cref{eq:diffusionobjective} is replaced by its latent representation $\\rvz{=}\\gE(\\rvx)$. This latent space DM can be typically smaller in terms of parameter count and memory consumption compared to corresponding pixel-space DMs of similar performance.\n\n\\vspace{-1.5mm}\n\\section{Latent Video Diffusion Models}\\label{sec:video_ldm}\nHere we describe how we \\emph{video fine-tune} pre-trained image LDMs (and DM upsamplers) for high-resolution video synthesis. We assume access to a dataset $p_{\\text{data}}$ of videos, such that $\\rvx \\in \\R^{T \\times 3 \\times \\tilde{H} \\times \\tilde{W}}, \\; \\rvx \\sim p_{\\text{data}}$ is a sequence of $T$ RGB frames, with height and width $\\tilde{H}$ and $\\tilde{W}$.\n\\begin{figure}[t!]\n  \\vspace{-0.3cm}\n    \\includegraphics[width=0.49\\textwidth]{figures/video_merge_block2.pdf}\n    \\caption{\\small \\textit{Left:} We turn a pre-trained LDM into a video generator by inserting \\emph{temporal} layers that learn to align frames into temporally consistent sequences. During optimization, the image backbone $\\theta$ remains fixed and only the parameters $\\phi$ of the temporal layers $l_\\phi^i$ are trained, \\cf \\Cref{eq:videoobjective}. \\textit{Right:}\nDuring training, the base model $\\theta$ interprets the input sequence of length $T$ as a batch of images. For the temporal layers $\\l_\\phi^i$, these batches are reshaped into video format. Their output $\\rvz'$ is combined with the spatial output $\\rvz$, using a learned merge parameter $\\alpha$.\nDuring inference, skipping the temporal layers ($\\alpha_\\phi^i{=}1$)  yields the original image model.\n    For illustration purposes, only a single U-Net Block is shown.\n    $B$ denotes batch size, $T$ sequence length, $C$ input channels and $H$ and $W$ the spatial dimensions of the input. $\\rvc_S$ is optional context frame conditioning, when training prediction models (\\Cref{sec:prediction}).\n    } \n    \\label{fig:architecture}\n\n   \\vspace{-5.5mm}\n\\end{figure}\\subsection{Turning Latent Image into Video Generators}\n\\label{sec:turning}\nOur key insight for efficiently training a video generation model is to re-use a pre-trained, fixed image generation model; an LDM parameterized by parameters $\\theta$. Formally, let us denote the neural network layers that comprise the image LDM and process inputs over the pixel dimensions as \\emph{spatial} layers $l_\\theta^i$, with layer index $i$.\nHowever, although such a model is able to synthesize individual frames at high quality, using it directly to render a video of $T$ consecutive frames will fail, as the model has no temporal awareness. \nWe thus introduce additional \\emph{temporal} neural network layers $l_\\phi^i$, which are interleaved with the existing \\emph{spatial} layers $l_\\theta^i$ and learn to align individual frames in a temporally consistent manner. These $L$ additional temporal layers $\\{l_\\phi^i\\}_{i=1}^L$ define the \\emph{video-aware} temporal backbone of our model, and the full model $\\rvf_{\\theta, \\phi}$ is thus the combination of the spatial and temporal layers; see~\\Cref{fig:architecture} for a visualization.\n\nWe start from a frame-wise encoded input video $\\mathcal{E}(\\rvx)=\\rvz \\in \\R^{T \\times C \\times H \\times W}$, \nwhere $C$ is the number of latent channels and $H$ and $W$ are the spatial latent dimensions.\nThe spatial layers interpret the video as a batch of independent images (by shifting the temporal axis into the batch dimension), and for each \\emph{temporal mixing layer} $l_\\phi^i$, we reshape back to video dimensions as follows (using \\texttt{einops}~\\cite{rogozhnikov2022einops} notation):\n\\begin{align*}\n\\rvz' &\\leftarrow \\texttt{rearrange}(\\rvz, \\; \\texttt{(b t) c h w} \\rightarrow \\texttt{b c t h w}) \\\\\n\\rvz' &\\leftarrow l_\\phi^i(\\rvz', \\rvc) \\\\\n\\rvz' &\\leftarrow \\texttt{rearrange}(\\rvz', \\; \\texttt{b c t h w} \\rightarrow \\texttt{(b t) c h w}) ,\n\\end{align*}\nwhere we added the batch dimension $\\texttt{b}$ for clarity. %\nIn other words, the spatial layers treat all $B{\\cdot}T$ encoded video frames independently in the batch dimension $\\texttt{b}$, while the temporal layers $l_\\phi^i(\\rvz', \\rvc)$ process entire videos in a new temporal dimension $\\texttt{t}$. Furthermore, $\\rvc$ is (optional) conditioning information such as a text prompt.\nAfter each temporal layer, the output $\\rvz'$ is combined with $\\rvz$ as\n$\\alpha_\\phi^i \\rvz + (1 - \\alpha_\\phi^i) \\rvz'$; \n$\\alpha_\\phi^i\\in[0,1]$ denotes a (learnable) parameter (also \\Cref{app:convolutional_ldm}).\n\nIn practice, we implement two different kinds of temporal mixing layers: (i) temporal attention and (ii) residual blocks based on 3D convolutions, \\cf \\Cref{fig:architecture}. We use sinusoidal embeddings~\\cite{vaswani2017attention,ho2020ddpm} to provide the model with a positional encoding for time. \n\nOur video-aware temporal backbone is then trained using the same noise schedule as the underlying image model, and, importantly, we fix the spatial layers $l_\\theta^i$ and \\emph{only} optimize the temporal layers $l_\\phi^i$ via\n{\\small\\begin{align}\n\\argmin_{\\phi} \\E_{\\rvx\\sim p_{\\text{data}}, \\tau \\sim p_{\\tau}, \\rvepsilon \\sim \\gN(\\mathbf{0}, \\mI)} \\left[\\Vert \\rvy - \\rvf_{\\theta, \\phi}(\\rvz_\\tau; \\rvc, \\tau) \\Vert_2^2 \\right],\n\\label{eq:videoobjective}\n\\end{align}}\nwhere $\\rvz_\\tau$ denotes diffused encodings $\\rvz=\\mathcal{E}(\\rvx)$.\nThis way, we retain the native image generation capabilities by simply skipping the temporal blocks, \\eg by setting $\\alpha_\\phi^i = 1$ for each layer. A crucial advantage of our strategy is that huge image datasets can be used to pre-train the spatial layers, while the video data, which is often less widely available, can be utilized for focused training of the temporal layers.\n\n\\subsubsection{Temporal Autoencoder Finetuning}\n\\label{sec:dec_finetuna}\n\\begin{figure}[t!]\n  \\vspace{-0.4cm}\n    \\includegraphics[width=0.49\\textwidth]{figures/stack_figure3.pdf}\n    \\caption{\\small \\textbf{Video LDM Stack.} We first generate sparse key frames. Then we temporally interpolate in two steps with the same interpolation model to achieve high frame rates. These operations are all based on latent diffusion models (LDMs) that share the same image backbone. Finally, the latent video is decoded to pixel space and optionally a video upsampler diffusion model is applied.} \n    \\label{fig:stack_figure}\n  \\vspace{-4mm}\n\\end{figure}Our video models build on pre-trained image LDMs. While this increases efficiency, the autoencoder of the LDM is trained on images only, causing flickering artifacts when encoding and decoding a temporally coherent sequence of images. %\nTo counteract this, we introduce additional temporal layers for the autoencoder's decoder, which we finetune on video data with a (patch-wise) temporal discriminator built from 3D convolutions, \\cf ~\\Cref{fig:ldm_figure}. Note that the encoder remains unchanged from image training such that the image DM that operates in latent space on encoded video frames can be re-used.\nAs demonstrated by computing reconstruction FVD~\\cite{unterthiner2018towards} scores in~\\Cref{tab:av_upsampler}, this step is critical for achieving good results.\n\n\\begin{figure*}[t!]\n  \\vspace{-0.8cm}\n    \\includegraphics[width=\\textwidth]{figures/txt2video6.pdf}\n    \\vspace{-8mm}\n    \\caption{\\small $1280\\times 2048$ resolution samples from our Stable Diffusion-based text-to-video LDM, including video fine-tuned upsampler. Prompts: ``An astronaut flying in space, 4k, high resolution''  and ``Milk dripping into a cup of coffee, high definition, 4k''.} \\vspace{-0.5em}\n    \\label{fig:text2image_samples}\n  \\vspace{-4mm}\n\\end{figure*}\\subsection{Prediction Models for Long-Term Generation}\n\\label{sec:prediction}\nAlthough the approach described in \\Cref{sec:turning} is efficient for generating short video sequences, it reaches its limits when it comes to synthesizing very long videos. Therefore, we also train models as \\emph{prediction models} given a number of (first) $S$ context frames. We implement this by introducing a temporal binary mask $\\rvm_S$ \nwhich masks the $T-S$ frames the model has to predict, where $T$ is the total sequence length as in \\Cref{sec:turning}. We feed this mask and the masked encoded video frames into the model for conditioning.\nSpecifically, the frames are encoded with LDM's image encoder $\\gE$, multiplied by the mask, and then fed (channel-wise concatenated with the masks) into the temporal layers $l_\\phi^i$ after being processed with a learned downsampling operation, see~\\Cref{fig:architecture}. Let $\\rvc_S = (\\rvm_S \\circ \\rvz, \\rvm_S)$ denote the concatenated spatial conditioning of masks and masked (encoded) images. Then, the objective from \\Cref{eq:videoobjective} reads\n\\begin{align}\n\\E_{\\rvx \\sim p_{\\text{data}}, \\rvm_S\\sim p_S, \\tau \\sim p_{\\tau}, \\rvepsilon} \\left[\\Vert \\rvy - \\rvf_{\\theta, \\phi}(\\rvz_\\tau; \\rvc_S, \\rvc, \\tau) \\Vert_2^2 \\right],\n\\label{eq:predictionobjective}\n\\end{align}\nwhere $p_S$ represents the (categorical) mask sampling distribution.\nIn practice, we learn prediction models that condition either on 0, 1 or 2 context frames, allowing for classifier-free guidance as discussed below.\n\nDuring inference, for generating long videos, we can apply the sampling process iteratively, re-using the latest predictions as new context. The first initial sequence is generated by synthesizing a single context frame from the base image model and generating a sequence based on that; afterwards, we condition on two context frames to encode movement (details in Appendix). \nTo stabilize this process, we found it beneficial to use \\emph{classifier-free diffusion guidance}~\\cite{ho2021classifierfree}, where we guide the model during sampling via\n{\\small\\begin{equation}\n\\rvf_{\\theta, \\phi}'(\\rvz_\\tau; \\rvc_S) = \\rvf_{\\theta, \\phi}(\\rvz_\\tau) + s \\cdot \\left(\\rvf_{\\theta, \\phi}(\\rvz_\\tau; \\rvc_S) - \\rvf_{\\theta, \\phi}(\\rvz_\\tau) \\right)\n\\end{equation}}\nwhere $s{\\geq}1$ denotes the guidance scale and we dropped the explicit conditioning on $\\tau$ and other information $\\rvc$ for readability. \nWe refer to this guidance as \\emph{context guidance}.\n\n\\subsection{Temporal Interpolation for High Frame Rates}\nHigh-resolution video is characterized not only by high spatial resolution, but also by high temporal resolution, \\ie, a high frame rate. \nTo achieve this, we divide the synthesis process for high-resolution video into two parts: The first is the process described in \\Cref{sec:turning} and \\Cref{sec:prediction}, which can generate \\emph{key frames} with large semantic changes, but (due to memory constraints) only at a relatively low frame rate. \nFor the second part, we introduce an additional model whose task is to interpolate between given key frames. \nTo implement this, we use the masking-conditioning mechanism introduced in \\Cref{sec:prediction}. However, unlike the prediction task, we now mask the frames to be interpolated---otherwise, the mechanism remains the same, \\ie, the image model is refined into a video interpolation model. In our experiments, we predict three frames between two given key frames, thereby training a $T \\rightarrow 4T$ interpolation model. To achieve even larger frame rates, we train the model simultaneously in the $T \\rightarrow 4T$ and $4T \\rightarrow 16T$ regimes (using videos with different fps), specified by binary conditioning. \n\nOur training approach for prediction and interpolation models is inspired by recent works~\\cite{voleti2022mcvd,harvey2022flexible,hoeppe2022diffusion} that use similar masking techniques (also see \\Cref{sec:related_extended}).\n\n\\begin{figure*}[t!]\n  \\vspace{-0.8cm}\n    \\includegraphics[width=\\textwidth]{figures/driving_main_fig2.pdf}\n    \\vspace{-7mm}\n    \\caption{\\small $512\\times1024$ resolution video modeling of real-world driving scenes with our Video LDM and video upsampler. \\emph{Top:} (Night time) \\textbf{Driving Video Generation}. \\emph{Middle:} \\textbf{Multimodal Driving Scenario Prediction}: We simulate two different scenarios given the same initial frame (red). \\emph{Bottom:} \\textbf{Specific Driving Scenario Simulation}: We synthesize a scenario based on a manually designed, initial scene generated with a bounding box-conditioned Image LDM (yellow). More examples in the \\Cref{app:extended_driving}.  \\vspace{-0.5em}}\n    \\label{fig:av_samples}\n  \\vspace{-4mm}\n\\end{figure*}\\subsection{Temporal Fine-tuning of SR Models} \\label{sec:method_upsampler}\nAlthough the LDM mechanism already provides a good native resolution we aim to push this towards the megapixel range. We take inspiration from cascaded DMs~\\cite{ho2021cascaded} and use a DM to further scale up the Video LDM outputs by $4\\times$.\nFor our driving video synthesis experiments, we use a pixel-space DM~\\cite{ho2021cascaded} (\\Cref{sec:driving_exp}) and scale to $512\\times1024$; for our text-to-video models, we use an LDM upsampler~\\cite{rombach2021highresolution} (\\Cref{sec:text-to-video}) and scale to $1280\\times2048$.\nWe use noise augmentation with noise level conditioning~\\cite{ho2021cascaded,saharia2022imagen} and train the super resolution (SR) model $\\rvg_{\\theta, \\phi}$ (on images or latents) via\n{\\small\\begin{align}\n\\E_{\\rvx \\sim p_{\\text{data}}, (\\tau,\\tau_{\\gamma}) \\sim p_{\\tau}, \\rvepsilon \\sim \\gN(\\mathbf{0}, \\mI)} \\left[\\Vert \\rvy - \\rvg_{\\theta, \\phi}(\\rvx_\\tau; \\rvc_{\\tau_{\\gamma}}, \\tau_{\\gamma}, \\tau) \\Vert_2^2 \\right]\n\\label{eq:upscaleobjective}\n\\end{align}}\nwhere $\\rvc_{\\tau_{\\gamma}} = \\alpha_{\\tau_\\gamma} \\rvx + \\sigma_{\\tau_\\gamma} \\rvepsilon, \\; \\rvepsilon \\sim \\gN(\\mathbf{0}, \\mI)$, denotes a noisy low-resolution image given to the model via concatenation, and $\\tau_\\gamma$ the amount of noise added to the low-resolution image following the noise schedule $\\alpha_\\tau$, $\\sigma_\\tau$.\n\nSince upsampling video frames independently would result in poor temporal consistency, we also\nmake this SR model video-aware. We follow the mechanism introduced in~\\Cref{sec:turning} with spatial layers $l_\\theta^i$ and temporal layers $l_\\phi^i$ and similarly video fine-tune the upscaler, conditioning on a low-resolution sequence of length $T$ and concatenating low-resolution video images frame-by-frame. \nSince the upscaler operates locally, we conduct all upscaler training efficiently on patches only and later apply the model convolutionally.\n\nOverall, we believe that the combination of an LDM with an upsampler DM is ideal for efficient high-resolution video synthesis. On the one hand, the main LDM component of our Video LDM leverages a computationally efficient, compressed latent space to perform all video modeling. This allows us to use large batch sizes and jointly encode more video frames, which benefits long-term video modeling, without excessive memory demands, as all video predictions and interpolations are carried out in latent space. On the other hand, the upsampler can be trained in an efficient patch-wise manner, therefore similarly saving computational resources and reducing memory consumption, and it also does not need to capture long-term temporal correlations due to the low-resolution conditioning. Therefore, no prediction and interpolation framework is required for this component. A model overview, bringing together all components from \\Cref{sec:turning} to \\Cref{sec:method_upsampler}, is depicted in~\\Cref{fig:stack_figure}.\n\n\\textit{A discussion of related work can be found in \\Cref{sec:related_extended}.}\n\n\\vspace{-1mm}\n\\section{Experiments}\\label{sec:experiments}\n\\vspace{-1mm}\n\\textbf{Datasets.} Since we focus on driving scene video generation as well as text-to-video, we use two corresponding datasets/models: \\textit{(i)} An in-house dataset of real driving scene (RDS) videos. The dataset consists of 683,060 videos of 8 seconds each at resolution $512\\times1024$ ($H\\times W$) and frame rate up to 30 fps. Furthermore, the videos have binary night/day labels, annotations for the number of cars in a scene (``crowdedness''), and a subset of the data also has car bounding boxes. \\textit{(ii)} We use the  WebVid-10M~\\cite{bain21frozen} dataset to turn the publicly available \\textit{Stable Diffusion} Image LDM~\\cite{rombach2021highresolution} into a Video LDM. WebVid-10M consists of 10.7M video-caption pairs with a total of 52K video hours. We resize the videos into resolution $320\\times512$. \n\\textit{(iii)} Moreover, in \\Cref{app:mountain_bike}, we show experiments on the Mountain Biking dataset by Brooks et al.~\\cite{brooks2022generating}. %\n\n\\textbf{Evaluation Metrics.} To evaluate our models, we use frame-wise Fr\\'echet Inception Distance (FID)~\\cite{heusel2017gans} as well as Fr\\'echet Video Distance (FVD)~\\cite{unterthiner2018towards}. Since FVD can be unreliable (discussed, for instance, by Brooks et al.~\\cite{brooks2022generating}), we additionally perform human evaluation. For our text-to-video experiments, we also evaluate CLIP similarity (CLIPSIM)~\\cite{wu2021godiva} and (video) inception score (IS) (\\Cref{app:quant_eval}).\n\n\\textbf{Model Architectures and Sampling.} Our Image LDMs are based on Rombach et al.~\\cite{rombach2021highresolution}. They use convolutional encoders and decoders, and their latent space DM architecture build on the U-Net by Dhariwal et al.\\cite{dhariwal2021diffusion}. Our pixel-space upsampler DMs use the same Image DM backbone~\\cite{dhariwal2021diffusion}. DM sampling is performed using DDIM~\\cite{song2021denoising} in all experiments.\\looseness=-1\n\nFurther architecture, training, evaluation, sampling and dataset details can be found in the Appendix.\n\n\\subsection{High-Resolution Driving Video Synthesis} \\label{sec:driving_exp}\nWe train our Video LDM pipeline, including a $4\\times$ pixel-space video upsampler, on the real driving scene (RDS) data. We condition on day/night labels and crowdedness, and randomly drop these labels during training to allow for classifier-free guidance and unconditional synthesis (we do not condition on bounding boxes here). Following the proposed training strategy above, we first train the image backbone LDM (spatial layers) on video frames independently, before we then train the temporal layers on videos. We also train Long Video GAN (LVG)~\\cite{brooks2022generating}, the previous state-of-the-art in long-term high-resolution video synthesis, on the RDS data to serve as main baseline. \\Cref{tab:av_main} (left) shows our main results for the Video LDM at $128\\times256$ resolution, without upsampler. We show both performance of our model with and without conditioning on crowdedness and day/night. Our Video LDM generally outperforms LVG and adding conditioning further reduces FVD. \\Cref{tab:av_user_study} shows our human evaluation: Our samples are generally preferred over LVG in terms of realism, and samples from our conditional model are also preferred over unconditional samples.\\looseness=-1\n\nNext, we compare our video fine-tuned pixel-space upsampler with independent frame-wise image upsampling (\\Cref{tab:av_upsampler}), using $128\\times256$ 30 fps ground truth videos for conditioning. We find that temporal alignment of the upsampler is crucial for high performance. FVD degrades significantly, if the video frames are upsampled independently, indicating loss of temporal consistency. As expected, FID is essentially unaffected, because the individual frames are still of high quality when upsampled independently.\n\nIn~\\Cref{fig:teaser} (bottom) and \\Cref{fig:av_samples} (top), we show conditional samples from the combined Video LDM and video upsampler model. We observe high-quality videos. Moreover, using our prediction approach, we find that we can generate very long, temporally coherent high-resolution driving videos of multiple minutes. We validated this for up to 5 minutes; see Appendix and supplementary video for results.\n\n\\begin{table}\n    \\vspace{-0.1cm}\n    \\centering\n    \\caption{\\emph{Left:} Comparison with LVG on RDS; \\emph{Right:}~Ablations.\\vspace{-1em}}\n    \\label{tab:av_main}\n    \\resizebox{.48\\linewidth}{!}{%\n    \\begin{tabular}{l c c}\n        \\toprule\n        \\textbf{Method} & FVD & FID \\\\\n        \\midrule\n        LVG~\\cite{brooks2022generating} & 478 & 53.5 \\\\\n        \\emph{Ours} & 389 & \\textbf{31.6} \\\\\n        \\emph{Ours} (cond.) & \\textbf{356} & 51.9 \\\\\n        \\bottomrule\n    \\end{tabular}\n    }\n    \\hfill\n    \\resizebox{.48\\linewidth}{!}{%\n    \\begin{tabular}{l c c}\n        \\toprule\n        \\textbf{Method} & FVD & FID \\\\\n        \\midrule\n        Pixel-baseline & 639,56 & 59.70 \\\\\n        End-to-end LDM & 1155.10 & 71.26 \\\\\n        Attention-only & 704.41 & 50.01\\\\\n        \\midrule\n        \\emph{Ours} & 534.17 & \\textbf{48.26} \\\\\n        \\emph{Ours} (context-guided) & \\textbf{508.82} & 54.16 \\\\\n        \\bottomrule\n    \\end{tabular}\n    }\n    \\vspace{-0.3em}\n\\end{table}\n\n\\begin{table}\n    \\centering\n        \\caption{User study on Driving Video Synthesis on RDS. \\vspace{-1em}}\n    \\label{tab:av_user_study}\n    \n    \\resizebox{ \\linewidth}{!}{%\n    \\begin{tabular}{l  c c c}\n        \\toprule\n        \\textbf{Method} &   Pref. A &  Pref. B & Equal  \\\\\n        \\midrule\n        \\emph{Ours} (cond.) v.s \\emph{Ours} (uncond.) & \\textbf{49.33} & 42.67 &8.0  \\\\\n\n        \\emph{Ours} (uncond.) v.s LVG   & \\textbf{54.02} & 40.23 & 5.74 \\\\\n        \\emph{Ours} (cond.) v.s LVG   & \\textbf{62.03} & 31.65 & 6.33 \\\\\n        \\bottomrule\n        \\vspace{-1.5em}\n    \\end{tabular}\n    }\n\\end{table}\n\n\\vspace{-1em}\n\n\\begin{table}[]\n    \\centering\n    \\caption{\\emph{Left:} Evaluating temporal fine-tuning for diffusion upsamplers on RDS data; \\emph{Right:} Video fine-tuning of the first stage decoder network leads to significantly improved consistency.~\\vspace{-1.em}}\n    \\label{tab:av_upsampler}\n    \\hfill\n    \\resizebox{.53\\linewidth}{!}{%\n    \\begin{tabular}{l c c}\n        \\toprule\n        \\textbf{Method} & FVD & FID \\\\\n        \\midrule\n        \\emph{Ours} Image Upsampler & 165.98 & \\textbf{19.71}  \\\\\n        \\emph{Ours} Video Upsampler & \\textbf{45.39} & 19.85 \\\\\n        \\bottomrule\n    \\end{tabular}\n}\n    \\hfill\n    \\resizebox{.45\\linewidth}{!}{%\n    \\begin{tabular}{l c c }\n        \\toprule\n         \\textbf{Decoder} & \\footnotesize\\emph{image-only}  & \\footnotesize\\emph{finetuned} \\\\\n        \\midrule\n        FVD & 390.88 & \\textbf{32.94} \\\\\n        FID & \\textbf{7.61} & 9.17 \\\\\n        \\bottomrule\n    \\end{tabular}\n    }\n    \\vspace{-1.3em}\n\\end{table}\\begin{figure*}[t!]\n  \\vspace{-0.8cm}\n    \\includegraphics[width=\\textwidth]{figures/dreambooth_fig3.pdf}\n    \\vspace{-8.5mm}\n    \\caption{\\small \\emph{Left:} DreamBooth Training Images. \\emph{Top row:} Video generated by our Video LDM with DreamBooth Image LDM backbone. \\emph{Bottom row:} Video generated without DreamBooth Image backbone. We see that the DreamBooth model preserves subject identity well. \\vspace{-0.5em}}\n    \\label{fig:dreambooth}\n  \\vspace{-2.5mm}\n\\end{figure*}\\subsubsection{Ablation Studies}\\label{sec:ablations}\nTo show the efficacy of our design choices (\\Cref{sec:video_ldm}), we compare a smaller version of our Video LDM with various baselines on the RDS dataset and present the results in~\\Cref{tab:av_main} (right) (for evaluation details, see \\Cref{app:quant_eval}). First, using the exact same architecture as for our Video LDM, we apply our temporal finetuning strategy to a pre-trained pixel-space image diffusion model, which is clearly outperformed by ours. Further, we train an End-to-End LDM, whose entire set of parameters $\\{\\theta, \\phi\\}$ is learned on RDS videos without image pre-training of $\\theta$, leading to heavy degradations both in FID and FVD, when compared with our Video LDM. Another important architectural choice is the introduction of 3D convolutional temporal layers, since they allow us to feed the context frames $\\boldsymbol{c}_{S}$ to the network spatially. This model achieves both lower FVD and FID scores than an attention-only temporal model, which uses the same set of spatial layers $\\theta$ and has the same number of trainable parameters. Finally, we see that we can further lower FVD scores by applying \\emph{context guidance} while sacrificing a bit of visual quality indicated by increased FID scores.\n\nMoreover, we provide an analysis on the effects of video fine-tuning the decoder of the compression model (\\cf~\\Cref{sec:dec_finetuna}) which encompasses the LDM framework~\\cite{rombach2021highresolution}. We apply our fine-tuning strategy to decoders of these compression models on the RDS\ndataset and compare both the obtained FVD/FID scores of reconstructed videos/image frames with those of their non-video-finetuned counterparts. \nVideo fine-tuning leads to improvements by orders of magnitudes, as can be seen in Table~\\ref{tab:av_upsampler}.\n\n\\vspace{-2mm}\n\\subsubsection{Driving Scenario Simulation}\\label{sec:driving_simulation}\n\\vspace{-1mm}\nA high-resolution video generator trained on in-the-wild driving scenes can potentially serve as a powerful simulation engine. We qualitatively explore this in~\\Cref{fig:av_samples}. Given an initial frame, our video model can generate several different plausible future predictions. Furthermore, we also trained a separate, bounding box-conditioned image LDM on our data (only for image synthesis). A user can now manually create a scene composition of interest by specifying the bounding boxes of different cars, generate a corresponding image, and then use this image as initialization for our Video LDM, which can then predict different scenarios in a multimodal fashion (bottom in~\\Cref{fig:av_samples}). \n\n\\subsection{Text-to-Video with Stable Diffusion}\\label{sec:text-to-video}\n\\vspace{-1mm}\nInstead of first training our own Image LDM backbone, our Video LDM approach can also leverage existing Image LDMs and turn them into video generators. To demonstrate this, we turn the publicly available text-to-image LDM \\emph{Stable Diffusion} into a text-to-video generator. Specifically, using the WebVid-10M text-captioned video dataset, we train a temporally aligned version of Stable Diffusion for text-conditioned video synthesis. We briefly fine-tune Stable Diffusion's spatial layers on frames from WebVid, and then insert the temporal alignment layers and train them (at resolution $320\\times512$). We also add text-conditioning in those alignment layers. Moreover, we further video fine-tune the publicly available latent \\emph{Stable Diffusion upsampler}, which enables $4\\times$ upscaling and allows us to generate videos at resolution $1280\\times 2048$. We generate videos consisting of 113 frames, which we can render, for instance, into clips of 4.7 seconds length at 24 fps or into clips of 3.8 seconds length at 30 fps.\nSamples from the trained models are shown in~\\Cref{fig:teaser,fig:text2image_samples}. While WebVid-10M consists of photo-quality real-life videos, we are able to generate highly expressive and artistic videos beyond the video training data. This demonstrates that the general image generation capabilities of the Image LDM backbone readily translate to video generation, even though the video dataset we trained on is much smaller and limited in diversity and style. The Video LDM effectively combines the styles and expressions from the image model with the movements and temporal consistency learnt from the WebVid videos.\n\nWe evaluate zero-shot text-to-video generation on UCF-101~\\cite{soomro2012ucf101} and MSR-VTT~\\cite{xu2016msr-vtt} (Tabs.~\\ref{tab:ucf} \\&~\\ref{tab:msrvtt}).\nEvaluation details in \\Cref{app:quant_eval}.\nWe significantly outperform all baselines except Make-A-Video~\\cite{singer2022make}, which we still surpass in IS on UCF-101. However, Make-A-Video is concurrent work, focuses entirely on text-to-video and trains with more video data than we do. We use only WebVid-10M; Make-A-Video also uses HD-VILA-100M~\\cite{xue2022hdvila}.\n\n\\begin{table}[t]\n    \\centering\n        \\caption{\\small UCF-101 text-to-video generation.\\vspace{-1em}}\n    \\label{tab:ucf}\n    \n    \\resizebox{ 0.77\\linewidth}{!}{%\n    \\begin{tabular}{l  c c c}\n        \\toprule\n        \\textbf{Method} &  Zero-Shot & IS ($\\uparrow$) & FVD ($\\downarrow$)  \\\\\n        \\midrule\n        CogVideo (Chinese)~\\cite{hong2022cogvideo} & Yes  & 23.55 & 751.34 \\\\\n        CogVideo (English)~\\cite{hong2022cogvideo} & Yes  & 25.27 & 701.59 \\\\\n        MagicVideo~\\cite{zhou2022magicvideo} & Yes  & - & 699.00 \\\\\n        Make-A-Video~\\cite{singer2022make} & Yes  & 33.00 & 367.23 \\\\\n        \\midrule\n        Video LDM \\emph{(Ours)} & Yes & 33.45 & 550.61 \\\\\n        \\bottomrule\n        \\vspace{-2.2em}\n    \\end{tabular}\n    }\n\\end{table}\\begin{table}[t]\n    \\centering\n        \\caption{\\small MSR-VTT text-to-video generation performance.\\vspace{-1em}}\n    \\label{tab:msrvtt}\n    \n    \\resizebox{ 0.77\\linewidth}{!}{%\n    \\begin{tabular}{l  c c}\n        \\toprule\n        \\textbf{Method} &  Zero-Shot & CLIPSIM ($\\uparrow$)  \\\\\n        \\midrule\n        GODIVA~\\cite{wu2021godiva} & No  & 0.2402 \\\\\n        N\\\"{U}WA~\\cite{wu2022nuwa} & No & 0.2439 \\\\\n        CogVideo (Chinese)~\\cite{hong2022cogvideo} & Yes  & 0.2614 \\\\\n        CogVideo (English)~\\cite{hong2022cogvideo} & Yes  & 0.2631 \\\\\n        Make-A-Video~\\cite{singer2022make} & Yes  & 0.3049\\\\\n        \\midrule\n        Video LDM \\emph{(Ours)} & Yes & 0.2929 \\\\\n        \\bottomrule\n        \\vspace{-2.9em}\n    \\end{tabular}\n    }\n\\end{table}\nIn \\Cref{app:convolutional_ldm}, we show how we can apply our model ``convolutional in time'' and ``convolutional in space'', enabling longer and spatially-extended generation without upsampler and prediction models. More video samples shown in \\Cref{app:extended_text2video}. Experiment details in \\Cref{app:exp_details_text_to_video}.\n\n\\vspace{-3mm}\n\\subsubsection{Personalized Text-to-Video with Dreambooth} \\label{sec:dreambooth}\n\\vspace{-1mm}\nSince we have separate spatial and temporal layers in our Video LDM, the question arises whether the temporal layers trained on one Image LDM backbone transfer to other model checkpoints (\\eg fine-tuned). \nWe test this for personalized text-to-video generation: Using DreamBooth~\\cite{ruiz2022dreambooth}, we fine-tune our Stable Diffusion spatial backbone on small sets of images of certain objects, tying their identity to a rare text token (``\\emph{sks}''). We then insert the temporal layers from the previously video-tuned Stable Diffusion (without DreamBooth) into the new DreamBooth version of the original Stable Diffusion model and generate videos using the token tied to the training images for DreamBooth (see Fig.~\\ref{fig:dreambooth} and examples in \\Cref{app:more_dreambooth}). We find that we can generate personalized coherent videos that correctly capture the identity of the Dreambooth training images. This validates that our temporal layers generalize to other Image LDMs. To the best of our knowledge, we are the first to demonstrate personalized text-to-video generation.\\looseness=-1\n\n\\textit{Additional results and experiments in \\Cref{app:additional_results}.}\n\n\\vspace{-1.7mm}\n\\section{Conclusions}\\label{sec:conclusions}\n \\vspace{-1.7mm}\nWe presented \\textit{Video Latent Diffusion Models} for efficient high-resolution video generation. Our key design choice is to build on pre-trained image diffusion models and to turn them into video generators by temporally video fine-tuning them with temporal alignment layers. To maintain computational efficiency, we leverage LDMs, optionally combined with a super resolution DM, which we also temporally align. Our Video LDM can synthesize high-resolution and temporally coherent driving scene videos of many minutes. We also turn the publicly available \\textit{Stable Diffusion} text-to-image LDM into an efficient text-to-video LDM and show that the learned temporal layers transfer to different model checkpoints. We leverage this for personalized text-to-video generation. We hope that our work can benefit simulators in the context of autonomous driving research and \nhelp democratize high quality video content creation (see \\Cref{app:impact} for broader impact and limitations).\n\n{\\small\n\\addcontentsline{toc}{section}{References}\n}\n\n\\newpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{VideoComposer: Compositional Video Synthesis \\\\ with Motion Controllability}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n\nThe pursuit of controllability as a higher standard of visual content creation has yielded remarkable progress in customizable image synthesis.\nHowever, achieving controllable video synthesis remains challenging due to the large variation of temporal dynamics and the requirement of cross-frame temporal consistency.\nBased on the paradigm of compositional generation, this work presents \\method that allows users to flexibly compose a video with textual conditions, spatial conditions, and more importantly temporal conditions.\nSpecifically, considering the characteristic of video data, we introduce the motion vector from compressed videos as an explicit control signal to provide guidance regarding temporal dynamics.\nIn addition, we develop a Spatio-Temporal Condition encoder (STC-encoder) that serves as a unified interface to effectively incorporate the spatial and temporal relations of sequential inputs, with which the model could make better use of temporal conditions and hence achieve higher inter-frame consistency.\nExtensive experimental results suggest that \\method is able to control the spatial and temporal patterns simultaneously within a synthesized video in various forms, such as text description, sketch sequence, reference video, or even simply hand-crafted motions.\nThe code and models will be publicly available at \\url{https://videocomposer.github.io}.\n\n\\end{abstract}\n\\section{Introduction}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{Figures/01_motivation.pdf}\n    \\vspace{-1em}\n    \\caption{\n        \\textbf{Compositional video synthesis.}\n        \\textbf{(a-c)} \\method is capable of generating videos that adhere to textual, spatial and temporal conditions or their subsets; \n        \\textbf{(d)} \\method can synthesize videos conforming to expected motion patterns (red stroke) and shape patterns (white stroke) derived from two simple strokes. \n        }\n    \\label{fig:teaser}\n    \\vspace{-2em}\n\\end{figure}\n\nDriven by the advances in computation, data scaling and architectural design, current visual generative models, especially diffusion-based models, have made remarkable strides in automating content creation, empowering designers to generate realistic images or videos from a textual prompt as input~\\cite{rombach2022LDM, singer2022make-a-video,ho2022imagenvideo}.\nThese approaches typically train a powerful diffusion model~\\cite{rombach2022LDM} conditioned by text~\\cite{ho2020denoising_ddpm} on large-scale video-text and image-text datasets~\\cite{2021Frozen,schuhmann2021laion}, reaching unprecedented levels of fidelity and diversity.\nHowever, despite this impressive progress, a significant challenge remains in the limited controllability of the synthesis system, which impedes its practical applications.\n\nMost existing methods typically achieve controllable generation mainly by introducing new conditions, such as segmentation maps~\\cite{rombach2022LDM, wang2022pretraining}, inpainting masks~\\cite{xie2022smartbrush} or sketches~\\cite{zhang2023controlnet, mou2023T2I_Adapter}, in addition to texts.\nExpanding upon this idea, Composer~\\cite{huang2023composer} proposes a new generative paradigm centered on the concept of \\emph{compositionality}, which is capable of composing an image with various input conditions, leading to remarkable flexibility.\nHowever, Composer primarily focuses on considering multi-level conditions within the spatial dimension, hence it may encounter difficulties when comes to video generation due to the inherent properties of video data.\nThis challenge arises from the complex temporal structure of videos, which exhibits a large variation of temporal dynamics while simultaneously maintaining temporal continuity among different frames.\nTherefore, incorporating suitable temporal conditions with spatial clues to facilitate controllable video synthesis becomes significantly essential.\n\nAbove observations motivate the proposed \\method, which equips video synthesis with improved controllability in both spatial and temporal perception.\nFor this purpose, we decompose a video into three kinds of representative factors, \\emph{i.e.}, textual condition, spatial conditions and the crucial temporal conditions, and then train a latent diffusion model to recompose the input video conditioned by them.\nIn particular, we introduce the video-specific \\emph{motion vector} as a kind of temporal guidance during video synthesis to explicitly capture the inter-frame dynamics, thereby providing direct control over the internal motions.\nTo ensure temporal consistency, we additionally present a unified STC-encoder that captures the spatio-temporal relations within sequential input utilizing cross-frame attention mechanisms, leading to an enhanced cross-frame consistency of the output videos.\nMoreover, STC-encoder serves as an interface that allows for efficient and unified utilization of the control signals from various condition sequences.\nAs a result, \\method is capable of flexibly composing a video with diverse conditions while simultaneously maintaining the synthesis quality, as shown in~\\cref{fig:teaser}.\nNotably, we can even control the motion patterns with simple hand-crafted motions, such as an arrow indicating the moon's trajectory in~\\cref{fig:teaser}d, a feat that is nearly impossible with current methods.\nFinally, we demonstrate the efficacy of \\method through extensive qualitative and quantitative results, and achieve exceptional creativity in the various downstream generative tasks.\n\n\\section{Related work}\n\n\\textbf{Image synthesis with diffusion models.}\nRecently, research efforts on image synthesis have shifted from utilizing GANs~\\cite{goodfellow2020GAN}, VAEs~\\cite{kingma2013VAE}, and flow models~\\cite{dinh2014NICE} to diffusion models~\\cite{sohl2015Diffusion_model,ho2020denoising_ddpm,zhang2022gddim,vahdat2021score_generative_latent} due to more stable training, enhanced sample quality, and increased flexibility in a conditional generation.\nRegarding image generation, notable works such as DALL-E 2~\\cite{ramesh2022Dalle-2} and GLIDE~\\cite{nichol2021glide} employ diffusion models for text-to-image generation by conducting the diffusion process in pixel space, guided by CLIP~\\cite{radford2021CLIP} or classifier-free approaches.\nImagen~\\cite{saharia2022Imagen} introduces generic large language models, \\textit{i.e.}, T5~\\cite{raffel2020T5}, improving sample fidelity.\nThe pioneering work LDMs~\\cite{rombach2022LDM} uses an autoencoder~\\cite{PatrickEsser2021TamingTF} to reduce pixel-level redundancy, making LDMs computationally efficient.\nRegarding image editing, pix2pix-zero~\\cite{parmar2023pix2pix-zero} and prompt-to-prompt editing~\\cite{hertz2022prompt-to-prompt} follow instructional texts by manipulating cross-attention maps.\nImagic~\\cite{kawar2022Imagic} interpolates between an optimized embedding and the target embedding derived from text instructions to manipulate images.\nDiffEdit~\\cite{couairon2022diffedit} introduces automatically generated masks to assist text-driven image editing.\nTo enable conditional synthesis with flexible input, ControlNet~\\cite{zhang2023controlnet} and T2I-Adapter~\\cite{mou2023T2I_Adapter} incorporate a specific spatial condition into the model, providing more fine-grained control.\nOne milestone, Composer~\\cite{huang2023composer}, trains a multi-condition diffusion model that broadly expands the control space and displays remarkable results.\nNonetheless, this compositionality has not yet been proven effective in video synthesis, and \\method aims to fill this gap.\n\n\\textbf{Video synthesis with diffusion models.}\nRecent research has demonstrated the potential of employing diffusion models for video synthesis~\\cite{ho2022video_diffusion_models,yang2022DPM_video,harvey2022flexible_diffusion_video,luo2023videofusion,khachatryan2023text2video-zero,blattmann2023align_latents}.\nNotably, ImagenVideo~\\cite{ho2022imagenvideo} and Make-A-Video~\\cite{singer2022make-a-video} both model the video distribution in pixel space, which limits their applicability due to high computational demands.\nIn contrast, MagicVideo~\\cite{zhou2022magicvideo} models the video distribution in the latent space, following the paradigm of LDMs~\\cite{rombach2022LDM}, significantly reducing computational overhead.\nWith the goal of editing videos guided by texts, VideoP2P~\\cite{liu2023video-P2P} and vid2vid-zero~\\cite{wang2023vid2vid-zero} manipulate the cross-attention map, while Dreamix~\\cite{molad2023dreamix} proposes an image-video mixed fine-tuning strategy.\nHowever, their generation or editing processes solely rely on text-based instructions~\\cite{radford2021CLIP,raffel2020T5}.\nA subsequent work, Gen-1~\\cite{esser2023gen-1}, integrates depth maps alongside texts using cross-attention mechanisms to provide structural guidance.\nBoth MCDiff~\\cite{chen2023MCDiff} and LaMD~\\cite{hu2023LaMD} target motion-guided video generation; the former focuses on generating human action videos and encodes the dynamics by tracking the keypoints and reference points, while the latter employs a learnable motion latent to improve quality.\nNevertheless, incorporating the guidance from efficient motion vectors or incorporating multiple guiding conditions within a single model is seldom explored in the general video synthesis field. \n\n\\textbf{Motion modeling.}\nMotion cues play a crucial role in video understanding fields, such as action recognition~\\cite{wang2016temporal,varol2017long,qiu2017learning,carreira2017quo,wang2021tdn,arnab2021vivit,bertasius2021space}, action detection~\\cite{zhao2017temporal,weinzaepfel2015learning,cheng2022tallformer,zeng2019graph}, human video generation~\\cite{ohnishi2018hierarchical,wang2020g3an,ni2023conditional}, \\emph{etc}.\nPioneering works~\\cite{wang2016temporal,qiu2017learning,arnab2021vivit,wang2020g3an,ni2023conditional,carreira2017quo} usually leverage hand-crafted dense optical flow~\\cite{zach2007duality} to embed motion information or design various temporal structures to encode long-range temporal representations.\nDue to the high computational demands of optical flow extraction, several attempts in compressed video recognition~\\cite{zhang2016real,wu2018compressed,shou2019dmc,chen2022mm} have begun to utilize more efficient motion vectors as an alternative to represent motions and have shown promising performance.\nIn contrast to these works, we delve into the role of motions in video synthesis and demonstrate that motion vectors can enhance temporal controllability through a well-designed architecture.\n\n\\section{VideoComposer}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]\n    {Figures/VideoComposer_framework.pdf}\n     \\vspace{-1.5em}\n    \\caption{\n        \\textbf{Overall architecture} of \\method.\n        First, a video is decomposed into three types of conditions, including textual condition, spatial conditions and temporal conditions.\n        Then, we feed these conditions into the unified STC-encoder or the CLIP model to embed control signals.\n        Finally, the resulting conditions are leveraged to jointly guide VLDMs for denoising.\n        }\n    \\label{fig:overall_framework}\n    \\vspace{-1em}\n\\end{figure}\n\nIn this section, we will comprehensively present \\method to showcase how it can enhance the controllability of video synthesis and enable the creation of highly customized videos.\nFirstly, we in brief introduce Video Latent Diffusion Models (VLDMs) upon which VideoComposer is designed, given their impressive success in various generative tasks.\nSubsequently, we delve into the details of \\method's architecture, including the composable conditions and unified Spatio-Temporal Condition encoder (STC-encoder) as illustrated in Fig.~\\ref{fig:overall_framework}.\nFinally, the concrete implementations, including the training and inference processes, will be analyzed.\n\n\\subsection{Preliminaries}\\label{sec:VLDMs}\n\nCompared to images, processing video requires substantial computational resources.\nIntuitively, adapting image diffusion models that process in the pixel space~\\cite{ramesh2022Dalle-2,nichol2021glide} to the video domain impedes the scaling of \\method to web-scale data.\nConsequently, we adopt a variant of LDMs that operate in the latent space, where local fidelity could be maintained to preserve the visual manifold. \n\n\\textbf{Perceptual video compression.}\nTo efficiently process video data, we follow LDMs by introducing a pre-trained encoder~\\cite{PatrickEsser2021TamingTF} to project a given video $\\bm{x} \\in \\mathbb{R}^{F \\times H \\times W \\times 3}$ into a latent representation $\\bm{z} = \\mathcal{E}(\\bm{x})$, where $\\bm{z} \\in \\mathbb{R}^{F \\times h \\times w \\times c}$.\nSubsequently, a decoder $\\mathcal{D}$ is adopted to map the latent representations back to the pixel space $\\bar{\\bm{x}} = \\mathcal{D}(\\bm{z})$.\nWe set $H/h = W/w = 8$ for rapid processing.\n\n\\textbf{Diffusion models in the latent space.}\nTo learn the actual video distribution $\\mathbb{P}(x)$, diffusion models~\\cite{sohl2015Diffusion_model,ho2020denoising_ddpm} learn to denoise a normally-distributed noise, aiming to recover realistic visual content.\nThis process simulates the reverse process of a Markov Chain of length $T$.\n$T$ is set to 1000 by default.\nTo perform the reverse process on the latent, it injects noise to $\\bm{z}$ to obtain a noise-corrupted latent $\\bm{z}_{t}$ following~\\cite{rombach2022LDM}.\nSubsequently, we apply a denoising function $\\epsilon_{\\theta}(\\cdot, \\cdot, t)$ on $\\bm{z}_{t}$ and selected conditions $\\bm{c}$, where $t \\in \\{1,...,T\\}$.\nThe optimized objective can be formulated as:\n\\begin{equation}\n    \\mathcal{L}_{VLDM} = \\mathbb{E}_{\\mathcal{E}\\bm(x), \\epsilon \\in \\mathcal{N}(0,1), \\bm{c}, t} \\left[\\| \\epsilon - \\epsilon_{\\theta}(\\bm{z}_{t}, \\bm{c}, t) \\|_{2}^{2}\\right]\n\\end{equation}\nTo exploit the inductive bias of locality and temporal inductive bias of sequentiality during denoising, we instantiate $\\epsilon_{\\theta}(\\cdot, \\cdot, t)$ as a 3D UNet augmented with temporal convolution and cross-attention mechanism following~\\cite{modelscope2023, ronneberger2015UNet, ho2022video_diffusion_models}.\n\n\\subsection{VideoComposer}\n\n\\textbf{Videos as composable conditions.} \nWe decompose videos into three distinct types of conditions, \\emph{i.e.}, textual conditions, spatial conditions and crucially temporal conditions, which can jointly determine the spatial and temporal patterns in videos.\nNotably, \\method is a generic compositional framework.\nTherefore, more customized conditions can be incorporated into \\method depending on the downstream application and are not limited to the decompositions listed above. \n\n\\emph{Textual condition.}\nTextual descriptions provide an intuitive indication of videos in terms of coarse-grained visual content and motions.\nIn our implementation, we employ the widely used pre-trained text encoder from OpenCLIP\\footnote{https://github.com/mlfoundations/open\\_clip} ViT-H/14  to obtain semantic embeddings of text descriptions.\n\n\\emph{Spatial conditions.}\nTo achieve fine-grained spatial control and diverse stylization, we apply three spatial conditions to provide\nstructural and stylistic guidance:\n\\emph{i)} Single image.\nVideo is made up of consecutive images, and a single image usually reveals the content and structure of this video.\nWe select the first frame of a given video as a spatial condition to perform image-to-video generation.\n\\emph{ii)} Single sketch.\nWe extract sketch of the first video frame using PiDiNet~\\cite{su2021pixel_diff} as the second spatial condition and encourage \\method to synthesize temporal-consistent video according to the structure and texture within the single sketch.\n\\emph{iii)} Style.\nTo further transfer the style from one image to the synthesized video, \nwe choose the image embedding as the stylistic guidance,  following~\\cite{balaji2022ediffi,huang2023composer}.\nWe apply a pre-trained image encoder from OpenCLIP ViT-H/14 to extract the stylistic representation.\n\\begin{wrapfigure}{r}{0.4\\linewidth}\n    \\vspace{-0.2em}\n    \\includegraphics[width=0.24\\textheight]{Figures/06_motion_vectors.pdf}\n    \\caption{\\textbf{Examples of motion vectors}.}\n    \\label{fig:motion_vector}\n\\end{wrapfigure}\n\\vspace{-1em}\n\n\\emph{Temporal conditions.}\nTo accomplish finer control along the temporal dimension,  we introduce four temporal conditions:\n\\emph{i)} Motion vector.\nMotion vector as a video-specific element is represented as two-dimension vectors, \\emph{i.e.}, horizontal and vertical orientations. \nIt explicitly encodes the pixel-wise movements between two adjacent frames, as visualized by red arrows in~\\cref{fig:motion_vector}.\nDue to the natural properties of motion vector,\nwe treat this condition as a motion control signal for temporal-smooth synthesis.\nFollowing~\\cite{wu2018compressed,shou2019dmc}, we extract motion vectors in standard MPEG-4 format from compressed videos.\n\\emph{ii)} Depth sequence.\nTo introduce depth information, we utilize the pre-trained model from~\\cite{ranftl2020robust_depth} to extract depth maps of video frames.\n\\emph{iii)} Mask sequence.\nTo facilitate video regional editing and inpainting, we manually add masks.\nWe introduce tube masks~\\cite{tong2022videomae,feichtenhofer2022masked_spatiotemporal} to mask out videos and enforce the model to predict the masked regions based on observable information.\n\\emph{iv)} Sketch sequence.\nCompared with the single sketch, sketch sequence can provide more control details and thus achieve precisely customized synthesis.\n\n\\textbf{STC-encoder.}\nSequential conditions contain rich and complex space-time dependencies, posing challenges for controllable guidance.\nIn order to enhance the temporal awareness of input conditions, we design a Spatio-Temporal Condition encoder (STC-encoder) to incorporate the space-time relations, as shown in~\\cref{fig:overall_framework}.  % \nSpecifically, a light-weight spatial architecture consisting of two 2D convolutions and an average pooling layer is first applied to the input sequences, aiming to extract local spatial information.\nSubsequently, the resulting condition sequence is fed into a temporal Transformer layer~\\cite{vaswani2017Transformer} for temporal modeling.\nIn this way, STC-encoder facilitates the explicit embedding of temporal cues, allowing for a unified condition interface for diverse inputs, thereby enhancing inter-frame consistency.\nIt is worth noting that we repeat the spatial conditions of a single image and single sketch along the temporal dimension to ensure their consistency with temporal conditions, hence facilitating the condition fusion process.\n\nAfter processing the conditions by STC-encoder, the final condition sequences are all in an identical spatial shape to $\\bm{z}_t$ and then fused by element-wise addition.\nFinally, we concatenate the merged condition sequence with $\\bm{z}_t$ along the channel dimension as control signals.\nFor textual and stylistic conditions organized as a sequence of embeddings, we utilize the cross-attention mechanism to inject textual and stylistic guidance.\n\n\\subsection{Training and inference}\n\\textbf{Two-stage training strategy.} \nAlthough \\method can initialize with the pre-training of LDMs~\\cite{rombach2022LDM}, which mitigates the training difficulty to some extent, the model still struggles in learning to simultaneously handle temporal dynamics and synthesize video content from multiple compositions.\nTo address this issue, we leverage a two-stage training strategy to optimize \\method.\nSpecifically, the first stage targets pre-training the model to specialize in temporal modeling through text-to-video generation. \nIn the second stage, we optimize \\method to excel in video synthesis controlled by the diverse conditions through compositional training.\n\n\\textbf{Inference.} \nDuring inference, DDIM~\\cite{zhang2022gddim} is employed to enhance the sample quality and improve inference efficiency.\nWe incorporate classifier-free guidance~\\cite{ho2022classifier} to ensure that the generative results adhere to specified conditions.\nThe generative process can be formalized as:\n\\begin{equation}\n    \\hat{\\epsilon}_{\\theta}(\\bm{z}_{t}, \\bm{c}, t) = \\epsilon_{\\theta}(\\bm{z}_{t}, \\bm{c}_{1}, t) + \\omega \\left(\\epsilon_{\\theta}(\\bm{z}_{t}, \\bm{c}_{2}, t) - \\epsilon_{\\theta}(\\bm{z}_{t}, \\bm{c}_{1}, t)\\right)\n\\end{equation}\nwhere $\\omega$ is the guidance scale; $\\bm{c}_{1}$ and $\\bm{c}_{2}$ are two sets of conditions.\nThis guidance mechanism extrapolates between two condition sets, placing emphasis on the elements in $(\\bm{c}_{2} \\setminus \\bm{c}_{1})$ and empowering flexible application.\nFor instance, in text-driven video inpainting, $\\bm{c}_{2}$ represents the expected caption and a masked video, while $\\bm{c}_{1}$ is an empty caption and the same masked video.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{Figures/02_Driving_Single_Image.pdf}\n     \\vspace{-1.5em}\n    \\caption{\n    \\small\n    \\textbf{Compositional image-to-video generation}.\n    We showcase six examples, each displaying two generated videos.\n    The upper video is generated using a given single frame as the spatial condition and a textual condition describing the scene.\n    The lower video is generated by incorporating an additional sequence of temporal conditions to facilitate finer control over the temporally evolving structure.\n    }\n    \\label{fig:local_image}\n    \\vspace{-4mm}\n\\end{figure}\n\n\\subsection{Experimental setup}\n\\textbf{Datasets.} \nTo optimize \\method, we leverage two widely recognized and publicly accessible datasets: WebVid10M~\\cite{2021Frozen} and LAION-400M~\\cite{schuhmann2021laion}.\nWebVid10M~\\cite{2021Frozen} is a large-scale benchmark scrapped from the web that contains 10.3M video-caption pairs.\nLAION-400M~\\cite{schuhmann2021laion} is an image-caption paired dataset, filtered using CLIP~\\cite{radford2021CLIP}.\n\n\\textbf{Evaluation metrics.} \nWe utilize two metrics to evaluate \\method:\n\\textit{i)} To evaluate video continuity, we follow Gen-1~\\cite{esser2023gen-1} to compute the average CLIP cosine similarity of two consecutive frames, serving as a \\textbf{frame consistency metric};\n\\textit{ii)} To evaluate motion controllability, we adopt end-point-error~\\cite{xu2022gmflow,teed2020raft} as a \\textbf{motion control metric}, \nwhich measures the Euclidean distance between the predicted and the ground truth optical flow for each pixel.\n\n\\vspace{-2mm}\n\n\\subsection{Composable video generation with versatile conditions}\n\n\\vspace{-1mm}\nIn this section, we demonstrate the ability of \\method to tackle various tasks in a controllable and versatile manner, leveraging its inherent compositionality. \nIt's important to note that the conditions employed in these examples are customizable to specific requirements.\nWe also provide additional results in the supplementary material for further reference.\n\n\\textbf{Compositional Image-to-video generation.}\nCompositional training with a single image endows \\method with the ability of animating static images.\nIn~\\cref{fig:local_image}, we present six examples to demonstrate this ability.\n\\method is capable of synthesizing videos conformed to texts and the initial frame.\nTo further obtain enhanced control over the structure, we can incorporate additional temporal conditions.\nWe observe resultant videos consistently adhere to the given conditions.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{Figures/03_inpainting.pdf}\n     \\vspace{-1.5em}\n    \\caption{\n    \\small\n    \\textbf{Compositional video inpainting.}\n    By manually adding masks to videos, \\method can perform video inpainting, facilitating the restoration of the corrupted parts according to textual instructions.\n    Furthermore, by incorporating temporal conditions specifying the visual structure, \\method can perform customized inpainting that conforms to the prescribed structure.\n    }\n    \\label{fig:mask}\n    \\vspace{-5mm}\n\\end{figure}\n\n\\begin{figure}[t]\n    \\centering\n\\includegraphics[width=1.0\\linewidth]{Figures/04_sketch.pdf}\n     \\vspace{-1.5em}\n    \\caption{\n    \\small\n    \\textbf{Compositional sketch-to-video generation}.\n    In the first example, the upper video is generated using text and a single sketch as the conditions, while the lower is generated by using an additional mask sequence for finer control over the temporal patterns.\n    For the last two examples, the upper video is generated using a single sketch and a textual condition, while the lower is generated with an additional style from a specified image.\n    }\n    \\label{fig:single_sketch}\n    \\vspace{-3mm}\n\\end{figure}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{Figures/07_mv_difference.pdf}\n     \\vspace{-1em}\n    \\caption{\n    \\small\n    \\textbf{Video-to-video translation}.\n    We extract a sequence of depth maps, sketches or motion vectors from the source video, along with textual descriptions, to perform the translation.\n    By utilizing motion vectors, we achieve \\textbf{static-background removal}.\n    }\n    \\label{fig:motion_prioritization}\n    \\vspace{-5mm}\n\\end{figure}\n\n\\textbf{Compositional video inpainting.}\nJointly training with masked video endows the model with the ability of filling the masked regions with prescribed content, as shown in~\\cref{fig:mask}.\n\\method can replenish the mask-corrupted regions based on textual descriptions.\nBy further incorporating temporal conditions, \\emph{i.e}, depth maps and sketches, we obtain more advanced control over the structure.\n\n\\begin{wraptable}{r}{0.46\\textwidth}\n    \\vspace{-1.1em}\n    \\tablestyle{3pt}{1.0}\n    \\caption{\\small\n        \\textbf{Evaluating the motion controllability}.\n        ``Text\" and ``MV\" represent the utilization of text and motion vectors as conditions for generation.}\n        \\vspace{-2pt}\n    \\centering\n    \\renewcommand{\\arraystretch}{1.1}\n    \\begin{tabular}{l|c|c|c}\n        Method & Text & MV &  Motion control $\\downarrow$ \\\\\n        \\shline\n        \\textit{w/o} STC-encoder  &  \\checkmark  &  &   4.03     \\\\\n        \\textit{w/o} STC-encoder  &  \\checkmark  & \\checkmark & 2.67       \\\\\n        \\method                   &  \\checkmark  & \\checkmark & \\textbf{2.18}         \\\\\n    \\end{tabular}\n    \\label{tab:motion_control}\n    \\vspace{-5pt}\n\\end{wraptable}\n\\textbf{Compositional sketch-to-video generation.}\nCompositional training with single sketch empowers \\method with the ability of animating static sketches, as illustrated in~\\cref{fig:single_sketch}.\nWe observe that \\method synthesizes videos conforming to texts and the initial sketch.\nFurthermore, we observe that the inclusion of mask and style guidance can facilitate structure and style control.\n\n\\subsection{Experimental results of motion control}\n\n\\textbf{Quantitative evaluation.}\nTo validate superior motion controllability, we utilize the motion control metric.\nWe randomly select 1000 caption-video pairs and synthesize corresponding videos.\nThe results are presented in~\\cref{tab:motion_control}.\nWe observe that the inclusion of motion vectors as a condition reduce the motion control error, indicating an enhancement of motion controllability.\nThe incorporation of STC-encoder further advances the motion controllability.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{Figures/08_motion_control.pdf}\n     \\vspace{-1.5em}\n    \\caption{\n    \\small\n    \\textbf{Versatile motion control using hand-crafted motions.}\n     \\textbf{(a)} Limited motion control using CogVideo~\\cite{hong2022cogvideo}.\n    \\textbf{(b)} Fine-grained and flexible motion control,  empowered by \\method.\n    }\n    \\label{fig:motion_control}\n    \\vspace{-1mm}\n\\end{figure}\n\n\\textbf{Motion vectors prioritizing moving visual cues.}\nThanks to the nature of motion vectors, which encode inter-frame variation, static regions within an image are inherently omitted.\nThis prioritization of moving regions facilitates motion control during synthesis.\nIn~\\cref{fig:motion_prioritization}, we present results of video-to-video translation to substantiate such superiority.\nWe observe that motion vectors exclude the static background, \\textit{i.e.}, human legs, a feat that other temporal conditions such as depth maps and sketches cannot accomplish.\nThis advantage lays the foundation for a broader range of applications.\n\n\\textbf{Versatile motion control with motion vectors.}\nMotion vectors, easily derived from hand-crafted strokes, enable more versatile motion control.\nIn~\\cref{fig:motion_control}, we present visualization comparing CogVideo~\\cite{hong2022cogvideo} and \\method.\nWhile CogVideo is limited to insufficient text-guided motion control, \\method expands this functionality by additionally leveraging motion vectors derived from hand-crafted strokes to facilitate more flexible and precise motion control.\n\n\\subsection{Ablation study}\nIn this subsection, we conduct qualitative and quantitative analysis on \\method, \naiming to demonstrate the effectiveness of incorporating STC-encoder.\n\n\\textbf{Quantitative analysis.}\nIn~\\cref{tab:temporal_consistency}, we present the frame consistency metric computed on 1000 test videos. %\nWe observe that incorporating STC-encoder augments the frame consistency, which we attribute to its temporal modeling capacity.\nThis observation holds for various temporal conditions such as sketches, depth maps and motion vectors.\n\n\\begin{wraptable}{r}{0.52\\textwidth}\n    \\vspace{-1.1em}\n    \\tablestyle{3pt}{1.0}\n    \\caption{\\small\n        \\textbf{Quantitative ablation study of STC-encoder}.\n        ``Conditions\" denotes the conditions utilized for generation.\n    }\n    \\vspace{-5pt}\n    \\centering\n    \\renewcommand{\\arraystretch}{1.2}\n    \\setlength{\\tabcolsep}{1.5pt}\n    \\begin{tabular}{l|c|c}\n        Method   &   Conditions   &  Frame consistency $\\uparrow$ \\\\\n        \\shline\n        \\textit{w/o} STC-encoder  & \\multirow{2}{*}{\\shortstack{Text and \\\\ sketch sequence}}  &  0.910 \\\\\n        \\method   &  & \\textbf{0.923} \\\\\n        \\shline\n        \\textit{w/o} STC-encoder  & \\multirow{2}{*}{\\shortstack{Text and \\\\ depth sequence}}  &  0.922 \\\\\n        \\method   &  & \\textbf{0.928} \\\\\n        \\shline\n        \\textit{w/o} STC-encoder  & \\multirow{2}{*}{\\shortstack{Text and \\\\ motion vectors}}  &  0.915 \\\\\n        \\method   &  & \\textbf{0.927}\n    \\end{tabular}\n    \\label{tab:temporal_consistency}\n    \\vspace{-10pt}\n\\end{wraptable}\n\\textbf{Qualitative analysis.}\nIn~\\cref{fig:ablation_study}, we exemplify the usefulness of STC-encoder.\nWe observe that in the first example, videos generated by \\method without STC-encoder generally adhere to the sketches but omit certain detailed information, such as several round-shaped ingredients.\nFor the left two examples, \\method without STC-encoder generates videos that are structurally inconsistent with conditions.\nWe can also spot the noticeable defects in terms of human faces and poses.\nThus, all the above examples can validate the effectiveness of STC-encoder.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{Figures/05_ablations.pdf}\n     \\vspace{-1.5em}\n    \\caption{\n    \\small\n    \\textbf{Qualitative ablation study}.\n    We present three representative examples.\n    The last two rows of videos display generated videos conditioned on a textual condition and one additional temporal condition (\\textit{i.e.}, sketches, depth maps or motion vectors).\n    Regions exhibiting deficiencies or fidelity are emphasized within red boxes.\n    }\n    \\label{fig:ablation_study}\n    \\vspace{-5mm}\n\\end{figure}\n\n\\section{Conclusion}\nIn this paper, we present \\method, which aims to explore the compositionality within the realm of video synthesis, striving to obtain a flexible and controllable synthesis system.\nIn particular, we explore the use of temporal conditions for videos, specifically motion vectors, as powerful control signals to provide guidance in terms of temporal dynamics.\nAn STC-encoder is further designed as a unified interface to aggregate the spatial and temporal dependencies of the sequential inputs for inter-frame consistency.\nOur experiments, which involve the combination of various conditions to augment controllability, underscore the pivotal role of our design choices and reveal the impressive creativity of the proposed \\method.\n\n{\\small\n}\n\n\\newpage\n\\onecolumn\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{SimDA: Simple Diffusion Adapter for Efficient Video Generation}\n\n\\begin{document}\n\n\\title{SimDA: Simple Diffusion Adapter for Efficient Video Generation}\n\n\\author{Zhen Xing\\textsuperscript{1} \\authorskip Qi Dai\\textsuperscript{2} \\authorskip Han Hu\\textsuperscript{2}  \\authorskip Zuxuan Wu\\textsuperscript{1} \\authorskip Yu-Gang Jiang\\textsuperscript{1} \\\\[0.5mm]\n{\n\\textsuperscript{1}  Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University}\\\\ \n{\\textsuperscript{2} Microsoft Research Asia}\n}\n\n\\twocolumn[{\n\\maketitle\n\\vspace{-1.4em}\n\\renewcommand\\twocolumn[1][]{#1}\n\\begin{center}\n    \\centering\n    \\includegraphics[width=0.94\\textwidth]{videxample2.pdf}\n    \\vspace{-0.0cm}\n    \\captionof{figure}{\n    \\textit{\\textbf{Examples of our SimDA results:}} (a) The results of open-wild Text-to-Video Generation. (b) Text-guided video Editing results using one text-video pair tuning.\n    }\n    \\label{fig:fig1}\n    \\vspace{-0.0cm}\n\\end{center}\n}]\n\n\\begin{abstract}\nThe recent wave of AI-generated content has witnessed the great development and success of Text-to-Image (T2I) technologies.\nBy contrast, Text-to-Video (T2V) still falls short of expectations though attracting increasing interests.\nExisting works either train from scratch or adapt large T2I model to videos, both of which are computation and resource expensive.\nIn this work, we propose a Simple Diffusion Adapter (SimDA) that fine-tunes only 24M out of 1.1B parameters of a strong T2I model, adapting it to video generation in a parameter-efficient way.\nIn particular, we turn the T2I model for T2V by designing light-weight spatial and temporal adapters for transfer learning. Besides, we change the original spatial attention to the proposed Latent-Shift Attention (LSA) for temporal consistency. With similar model architecture, we further train a video super-resolution model to generate high-definition ($1024 \\times 1024$) videos. In addition to T2V generation in the wild, SimDA could also be utilized in one-shot video editing with only 2 minutes tuning. Doing so, our method could minimize the training effort with extremely few tunable parameters for model adaptation.\nMore examples can be found at our website \\url{https://chenhsing.github.io/SimDA/}. \n\\end{abstract}\n\n\\section{Introduction}\n\nImage generation stands on top of the recent AIGC wave.\nIt not only has a significant impact in the academic community but also achieves tremendous success in various applications, such as computer graphics, art and culture, medical imaging, \\emph{etc}. The approaches in this area mainly include methods based on generative adversarial networks (GANs)~\\cite{gan,aliasgan,stylegan,stylegan2,stylegan3}, auto-regressive transformers~\\cite{taming,zeroshot,scaling}, and the latest diffusion models~\\cite{diffusionbeatgan,ddpm,improvedddpm,cascaded,glide,dalle,stablediffusion,imagen, song2020score,feng2023ernie,liu2022compositional,xue2023raphael}.\nAmong them, diffusion models are the most popular owing to the strong controllability, simple stability, and amazing realism. \nHowever, video generation research lags behind due to challenges like the scarcity of publicly available datasets, difficulty in modeling temporal information, and high training costs, hindering the progress in this area.\n\nThere have been several research endeavors dedicated to exploring video synthesis~\\cite{babaeizadeh2017stochastic,brooks2022generating,castrejon2019improved,denton2018stochastic,franceschi2020stochastic,ge2022long,gupta2022rv,gupta2018imagine, hong2022cogvideo, kahembwe2020lower,lee2018stochastic, li2018video, luc2020transformation, mittal2017sync, marwah2017attentive, pan2017create, saito2020train, skorokhodov2022stylegan, tian2021good, villegas2017decomposing, vondrick2016generating,weissenborn2019scaling,wu2021godiva, yan2021videogpt, yu2022generating}. In addition, some studies have employed popular diffusion models for video generation~\\cite{harvey2022flexible,vdm,hoppe2022diffusion,voleti2022masked,yang2022diffusion, videofusion}. However, most of them involve training models from scratch, which can be time-consuming due to the complex video data. Early attempts were also constrained by GPU memory or hardware limitations.\n\nMore recently, a small number of T2V (Text-to-Video) approaches have emerged, aiming to fine-tune well-established T2I (Text-to-Image) models~\\cite{ho2022imagenvideo, videoLDM, latentshift,videofactory, videofusion, magicvideo, preserve}. \nThey have incorporated temporal modeling modules (\\emph{e.g.} Imagen video~\\cite{ho2022imagenvideo}, Video LDM~\\cite{videoLDM}) into\nT2I models, which effectively accelerate the model convergence.\nHowever, it should be noted that training such models is still a challenging task due to the massive number of parameters (4B or even 16B) involved in the network architecture. \n\nIn the NLP field, state-of-the-art results of various tasks are generally achieved by adaptation from large pretrained models (\\emph{i.e.}, BERT~\\cite{bert}, LLMs~\\cite{dai2019transformer,radford2019language, raffel2020exploring, zhang2022opt}).\nHowever, with the advent of increasingly larger and more powerful foundation models (\\emph{e.g.}, GPT-4 with 100T parameters), conducting full fine-tuning of the entire models has become prohibitively expensive and infeasible in terms of training cost and GPU storage. \nTo address the issue, numerous methods based on efficient fine-tuning have emerged rapidly in NLP~\\cite{houlsby2019parameter, hu2021lora, li2021prefix, lester2021power} and computer vision~\\cite{st-adapter, yang2023aim, multimodaladapter, chen2022adaptformer}.\n\nIn this work, we propose a parameter-efficient video diffusion model, namely Simple Diffusion Adapter (SimDA), that fine-tunes the large T2I (\\emph{i.e.} Stable Diffusion~\\cite{stablediffusion}) model for improved video generation. We only add $0.02\\%$ parameters compared to the T2I model. During training, we freeze the original T2I model, and only tune the newly added modules. \nWe further propose a Latent-Shift Attention (LSA) to replace the original spatial attention, which significantly improves the temporal modeling capability and retains consistency without adding new parameters.\nTo this end, our model demands less than 8GB GPU memory for training with a resolution of $16\\times256\\times256$, while the inference time speeds up by $~39\\times$ compared to the auto-regressive method CogVideo~\\cite{hong2022cogvideo}. \nBesides, we turn an image super resolution framework into the video counterpart with similar architecture, which allows generating high-definition videos of $1024\\times1024$. \nOur model can also be extended to the recently popular diffusion-based video editing~\\cite{tuneavideo}, achieving significant $3\\times$ faster training while retaining comparable results, as evidenced by the editing examples presented in Fig~\\ref{fig:fig1} (b).\nIn conclusion, the contributions of this work can be summarized as follows:\n\\begin{itemize}\n\\item We explore the simple adaptation from image diffusion to video diffusion, exhibiting that tuning extremely few parameters can achieve surprisingly good results.\n\\item With the helpful light-weight adapters and the proposed latent-shift attention, our method can effectively model the temporal relations with negligible cost.\n\\item Our diffusion adapter could be extended to text-guided video super-resolution and video editing, significantly facilitating the model training.\n\\item SimDA significantly alleviates the training cost and speeds up the inference time, while remaining competitive results compared to other methods.\n\n\\end{itemize}\n\n\\section{Related Work}\n\n\\paragraph{Text-to-Video Generation}\n\nSimilar to the advancements in Text-to-Image (T2I) generation, early approaches for Text-to-Video (T2V) generation~\\cite{li2018video, mittal2017sync, pan2017create} were based on Generative Adversarial Networks (GANs) and primarily applied to domain-specific videos such as simple human actions~\\cite{ucf101} or clouds moving~\\cite{time-lapse}. Due to the inherent challenges of video data modeling and the requirements for large-scale high-quality text-video datasets, the development of open-wild T2V generation is limited. However, learning a prior from T2I generation can effectively alleviate this problem.\n\nFor instance, NÜWA~\\cite{wu2022nuwa} formulates a unified representation space for images and videos, enabling multitask learning for both T2I and T2V generation. CogVideo~\\cite{hong2022cogvideo} incorporates temporal attention layers into the pretrained and frozen CogView2~\\cite{ding2022cogview2} model to capture motion dynamics. Make-A-Video \\cite{singer2022make} proposes fine-tuning a pretrained DALLE2~\\cite{dalle} model solely on video data to learn motion patterns, enabling T2V generation without explicitly training on text-video pairs. Video Diffusion Models~\\cite{vdm} and Imagen Video~\\cite{ho2022imagenvideo} perform joint text-image and text-video training, treating images as independent frames and disabling temporal layers in the U-Net~\\cite{unet} architecture. Phenaki~\\cite{villegas2022phenaki} also conducts joint training for T2I and T2V generation using the Transformer model, considering an image as a frozen video. Besides, Video LDM~\\cite{videoLDM}, Latent-Shift~\\cite{latentshift}, VideoFactory~\\cite{videofactory}, MagicVideo~\\cite{magicvideo} and our methods utilize the popular open-sourced T2I Stable Diffusion~\\cite{stablediffusion} model. While the progress in video generation is impressive, the parameters of video generation can be highly large. As shown in Table~\\ref{tab:model_size}, Make-A-Video~\\cite{singer2022make} require six models and 9.7B parameters and Imagen Video~\\cite{ho2022imagenvideo} utilizes eight models with 16.3B parameters, which limits the training efficiency of T2V models.\n\n\\paragraph{Text guided Video Editing}\nIn the realm of content generation, an alternative avenue is the manipulation of existing images~\\cite{brooks2023instructpix2pix, hertz2022prompt2prompt, meng2021sdedit, tumanyan2023plug} and videos~\\cite{tuneavideo,bar2022text2live, gen1, liu2023video, qi2023fatezero, shin2023edit,zhang2023towards, yang2023rerender} using textual input as a means of control, rather than relying solely on unbridled text-based generation. SDEdit~\\cite{meng2021sdedit} introduces noise to images and then reconstructs them for the purpose of editing. Prompt-to-prompt~\\cite{hertz2022prompt2prompt} and Plug-and-Play~\\cite{tumanyan2023plug} modify the cross-attention map by altering the textual description, thus influencing the editing process. When it comes to video editing, Tune-A-Video~\\cite{tuneavideo} fine-tunes the T2I (Text-to-Image) model on a single video, enabling the generation of new videos with similar motion patterns. Video-P2P~\\cite{liu2023video} and FateZero~\\cite{qi2023fatezero} extend the concept of Prompt-to-prompt editing to the realm of videos. Text2Live~\\cite{bar2022text2live} divides videos into layers and enables separate editing of each layer based on textual descriptions.\n\n\\paragraph{Parameter-Efficient Transfer Learning}\nIn the field of NLP, parameter-efficient fine-tuning techniques~\\cite{houlsby2019parameter, hu2021lora, lester2021power, li2021prefix, he2021towards,zaken2021bitfit, sung2021training} were initially proposed to address the heavy computation of full fine-tuning large language models for various downstream tasks. These techniques aim to reduce the number of trainable parameters, thereby lowering computation costs, while still achieving or surpassing the performance of full fine-tuning.\nRecently, parameter-efficient transfer learning has also been explored in the field of computer vision~\\cite{jia2022visual, bahng2022exploring, chen2022adaptformer, jie2022convolutional, gao2022visual, yang2023aim, st-adapter, multimodaladapter,ssl3d, mpcn}. These methods mainly focus on adapting models within simple classification or detection tasks. In contrast, our approach focuses on adapting an T2I model for T2V generation task.\n\n\\paragraph{Temporal Shift Module}\nTSM~\\cite{tsm} pioneered the introduction of the temporal shift module for action recognition, employing a partial channel shift along the temporal dimension. This approach seamlessly integrates temporal cues from both preceding and succeeding frames into the current frame without incurring additional computational overhead. Subsequently, TokShift~\\cite{tokenshift} implemented channel shifting along the temporal dimension for transformer architectures.\nTPS~\\cite{tps} further shifted patches instead of channels to model the temporal correlations.\nHowever, such direct patch shifting would lead to inconsistency in generation task.\nAdditionally, Latent-shift~\\cite{latentshift} and TSB~\\cite{shiftgan} adapted shift module as TSM~\\cite{tsm} within convolution blocks for video generation tasks.\nIn this work, our latent-shift attention (LSA) employs the patch-level shifting manner. In contrast to TPS, we further propose to involve all tokens in current frame as the keys and values, which guarantees the temporal consistency during generation and significantly improves the video quality.\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=1.0\\textwidth]{pipeline7.pdf}\n    \\caption{Pipeline of our Parameter-Efficient Text-to-Video Framework. We utilize the pre-trained auto-encoder as in Stable Diffusion~\\cite{stablediffusion} to obtain latent representation. During training, we only update the parameters of newly added adapter module, highlighted in \\textcolor{myred}{yellow}. The parameters of other modules are frozen, highlighted in \\textcolor{mygreen}{blue}. }\n    \\label{fig:pipeline}\n\\end{figure*}\n\n\\section{Method}\nIn this section, we first introduce the preliminaries of Latent Diffusion Model~\\cite{stablediffusion} in Sec.~\\ref{preliminaries}. The pipeline of SimDA is described in Sec.~\\ref{Pipeline}. Then we detail the proposed spatial and temporal adapters as well as latent-shift attention in Sec.~\\ref{model}. Finally, we introduce the super resolution and text-guided video editing model in Sec.~\\ref{editing}.\n\n\\subsection{Preliminaries of Stable Diffusion}\n\\label{preliminaries}\nIn this subsection, we introduce the preliminaries of Stable Diffusion~\\cite{stablediffusion} model.\nIt is a latent diffusion model that operates in the latent space of an autoencoder $\\mathcal{D}(\\mathcal{E}(\\cdot))$, where $\\mathcal{E}$ is the encoder and $\\mathcal{D}$ is the decoder.\nIn this model, for an image $I$ with its corresponding latent feature $\\bm{x}_0=\\mathcal{E}(I)$, the diffusion forward process involves iteratively adding noise to the latent space.\n\\begin{equation}\n  q(\\bm{x}_t|\\bm{x}_{t-1})=\\mathcal{N}(\\bm{x}_t;\\sqrt{\\alpha_t}\\bm{x}_{t-1}, (1-\\alpha_t)\\mathbf{I}),\n\\end{equation}\nwhere $t\\in\\{1,...,T\\}$ is the time step, $q(\\bm{x}_t|\\bm{x}_{t-1})$ is the conditional density of $\\bm{x}_t$ given $\\bm{x}_{t-1}$, $\\mathbf{I}$ is identity matrix, and $\\alpha_t$ is hyperparameter.\nAlternatively, we can directly sample $\\bm{x}_t$ at any time step from $\\bm{x}_0$ with,\n\\begin{equation}\\label{eq:forward_sample}\n  q(\\bm{x}_t|\\bm{x}_{0})=\\mathcal{N}(\\bm{x}_t;\\sqrt{\\bar{\\alpha}_t}\\bm{x}_0, (1-\\bar{\\alpha}_t)\\mathbf{I}),\n\\end{equation}\nwhere $\\bar{\\alpha}_t=\\prod_{i=1}^t\\alpha_i$.\n\nIn the diffusion backward process, a U-Net denoted as $\\bm{\\epsilon}_\\theta$ is trained to predict the noise in the latent space, aiming to iteratively recover $\\bm{x}_0$ from $\\bm{x}_T$. In this process, as the diffusion progresses and approaches a large value of $T$, $\\bm{x}_0$ is completely disrupted and the latent representation $\\bm{x}_T$ approximates a standard Gaussian distribution. Consequently, the U-Net $\\bm{\\epsilon}_\\theta$ is trained to infer meaningful and valid $\\bm{x}_0$ from random Gaussian noises. The training object can be simplified as,\n\\begin{equation}\n\\mathbb{E}_{\\bm{x},\\bm{\\epsilon} \\sim \\mathcal{N}(\\bm{0},\\mathbf{I}),t}[\\lvert\\lvert \\bm{\\epsilon} - \\bm{\\epsilon}_\\theta(\\bm{x}_t,\\bm{c},t)\\rvert\\rvert^2_2],\n\\end{equation}\nwhere $\\bm{c}$ is the embedding of condition text.\n\nDuring the inference stage, it samples a valid latent representation $\\bm{x}_0$ from the standard Gaussian noise $\\bm{x}_T=\\bm{z}_T, \\bm{z}_T\\sim\\mathcal{N}(\\bm{0}, \\mathbf{I})$ using DDIM~\\cite{ddim} sampling. Then, the model can decode $\\bm{x}_0$ using the decoder $\\mathcal{D}$ to generate the final image $I=\\mathcal{D}(\\bm{x}_0)$. This process could generate diverse and high-quality images based on the sampled latent representations. In contrast, our method focus on more challenge high-quality video generation.\n\n\\begin{table*}[]\n\\centering\n    \\caption{Model size and inference speed comparisons. The speed is measured in seconds on one A100 (80GB) GPU. The majority of results are sourced from~\\cite{latentshift}.}\n    \\scalebox{0.77}{\n    \\begin{tabular}{l|c|c|c|c|c|c|c|c|c}\n    \\toprule\n    \\multirow{2}*{\\bf Method} & \\multicolumn{8}{c|}{\\bf Parameters (Billion)} & \\multirow{2}*{\\bf Speed (s)} \\\\ \\cline{2-9}\n    & T2V Core & Auto Encoder & Text Encoder & Prior Model & Super Resolution & Frame Interpolation & Overall &Tuned & \\\\\n    \\midrule\n    CogVideo~\\cite{hong2022cogvideo} & $7.7$ & $0.10$ & $-$ & $-$ & $-$ & $7.7$ & $15.5$ &$15.5 $ &  $434.53$  \\\\ \n    Make-A-Video~\\cite{singer2022make} & $3.1$ & $-$ & $0.12$ & $1.3$ & $1.4+0.7$ & $3.1$ & $9.72$ & $9.72 $ & $-$  \\\\\n    Imagen Video~\\cite{ho2022imagenvideo} & $5.6$ & $-$ & $4.6$ & $-$ & $1.2+1.4+0.34$ & $1.7+0.78+0.63$ & $16.25$ &$16.25 $ & $-$  \\\\\n    Video LDM~\\cite{videoLDM}  & $1.51$  & $0.08$  &$0.12$ &$-$  &$0.98$  &$1.51$  &$4.20$  &$2.65$  &$-$ \\\\\n    Latent-VDM~\\cite{latentshift} & $0.92$ & $0.08$ & $0.58$ & $-$ & $-$ & $-$ & ${1.58}$ & ${0.92 }$  & $ {28.62}$  \\\\\n    Latent-Shift~\\cite{latentshift} & $0.88$ & $0.08$ & $0.58$ & $-$ & $-$ & $-$ & $ 1.53$ & $ 0.88 $ & $ 23.40$  \\\\\n    LVDM~\\cite{lvdm} & $0.96$ & $0.08$ & $0.12$ & $-$ & $-$ & $-$ & $ 1.16$ & $ 1.04 $ & $ 21.23$  \\\\ \\hline\n    SimDA (Ours) & $0.88$ & $0.08$ & $0.12$ & $-$ & $-$ & $-$ & $\\bf1.08$ & $\\bf 0.025 $ &\\bf $11.20$   \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\vspace{-1mm}\n    \\label{tab:model_size}\n\\end{table*}\n\n\\subsection{Pipeline}\n\\label{Pipeline}\nOur SimDA, as shown in Fig.~\\ref{fig:pipeline}, is built upon the previously introduced Stable Diffusion~\\cite{stablediffusion}.\nFor a video clip with $t$ frames, denoted as $\\{I_i\\}_{i=1}^t$, we first pass it through a pre-trained encoder $\\mathcal{E}$ to obtain the corresponding latent feature $\\{\\bm{x}_i\\}_{i=1}^t$. We then input the latent features to the forward diffusion process, where noise is incrementally added to the latents. In the backward diffusion process, we utilize an inflated U-Net architecture to predict the noise for the noisy video latents. \nSpecifically, for the Convolution block, we inflate the 2D ResNet~\\cite{resnet} block to a 3D block to accommodate video inputs. Additionally, we incorporate a lightweight Temporal Adapter module for temporal modeling. In the Attention block, we employ a latent-shift attention mechanism for spatial self-attention and introduce two spatial adapter modules to facilitate the transfer of video information. Further details will be presented in Sec.~\\ref{model}. During the inference stage, we employ DDIM~\\cite{ddim} sampling to progressively denoise the latent representation sampled from a standard Gaussian distribution. Finally, we utilize a pre-trained decoder $\\mathcal{D}$ to reconstruct the video from the denoised latents.\n\n\\subsection{Modeling}\n\\label{model}\n\nIn this section, we describe the proposed Spatial Adapter, Temporal Adapter, and Latent-Shift Attention in detail, which are the key components of our model.\n\\paragraph{Spatial Adapter}\nThe large-scale text-image pre-trained T2I model exhibits significant transferability, as evidenced by its remarkable accomplishments in tasks such as personalized T2I generation~\\cite{ruiz2023dreambooth,mou2023t2iadapter} and image editing~\\cite{hertz2022prompt2prompt, controlnet}. Consequently, we believe that employing a lightweight fine-tuning approach can effectively harness spatial information in the realm of video generation.\nInspired by efficient fine-tuning techniques in NLP~\\cite{hu2021lora, li2021prefix } and vision tasks~\\cite{chen2022adaptformer, yang2023aim}, we adopt adapters due to their simplicity. \n\nIn our T2V framework, we design two types of spatial adapters (\\emph{i.e.}, Attention Adapter and FFN Adapter) for transferring video spatial information. As shown in the bottom right of Fig.~\\ref{fig:pipeline}, both adapters employ a bottleneck architecture consisting of two fully connected (FC) layers with an intermediate activation layer. The first FC layer maps the input to a lower-dimensional space, while the second FC layer maps it back to the original dimensional. Formally, for an input feature matrix $\\mathbf{X}\\in \\mathbb{R}^{N\\times d} $, the spatial adapter could be written as:\n\\begin{equation}\n    \\texttt{S\\text{-}Adapter}(\\mathbf{X}) = \\mathbf{X} + \\mathbf{W}_{\\texttt{up}}(\\texttt{GELU}(\\mathbf{W}_{\\texttt{down}}(\\mathbf{X}))),\n\\end{equation}\nwhere $\\mathbf{W}_{\\texttt{up}}$ and $\\mathbf{W}_{\\texttt{down}}$ are the learnable matrix with dimension $d \\times l$ and $l \\times d$, $l<d$.\nTo preserve the structure of the original network and the pretrained weights, we initialize the second FC layer $\\mathbf{W}_{\\texttt{down}}$ with zeros. To adapt to the spatial features of videos, we incorporate the adapter after the latent-shift attention layer. Additionally, we observe that adding adapter to the feed forward network (FFN) also helps the network transfer spatial information to videos. We will provide examples in Sec.~\\ref{sec:ablation}. During training, all layers of the attention block are fixed, and only the adapters are updated.\n\n\\paragraph{Temporal Adapter}\nWhile the spatial adapter effectively transfers spatial information to video data, modeling temporal information is crucial for T2V generation tasks. Previous approaches incorporate temporal convolution or temporal attention~\\cite{videoLDM,videofactory, singer2022make} modules to capture temporal relationships. Although these modules are effective in modeling temporal dynamics, they often come with a huge number of parameters and high-dimensional input feature, resulting in significant computational and training costs.\n\nTo address this issue, we utilize the temporal adapter module for temporal modeling as~\\cite{multimodaladapter, st-adapter}. In contrast to conventional spatial adapter modules, the temporal adapter module employs depth-wise 3D convolution instead of an intermediate activation layer~\\cite{GELU}. The temporal adapter could be formally written as:\n\\begin{equation}\n    \\texttt{T\\text{-}Adapter}(\\mathbf{X}) = \\mathbf{X} + \\mathbf{W}_{\\texttt{up}} (\\texttt{3D-Conv}(\\mathbf{W}_{\\texttt{down}}(\\mathbf{X}))).\n\\end{equation}\n\nBy utilizing 3D convolutions in lower-dimensional input, our approach significantly alleviates the complexity of temporal modeling. As a result, our method achieves efficient memory usage during training and exhibits the fastest inference speed among competitive approaches, as demonstrated in Table~\\ref{tab:model_size}.\n\n\\paragraph{Temporal Latent-Shift Attention}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{shift2.pdf}\n    \\vspace{-0.3cm}\n    \\caption{The overview of Temporal Latent-shift Attention module. It is noted that the Latent-shift attention is performed on latent space, but the visualization overview is shown on image-level for clear presentation.}\n    \\label{fig:shift}\n    \\vspace{-0.3cm}\n\\end{figure}\n\nIn the original T2I framework, the attention block of the U-Net only performs self-attention for individual frames, neglecting the information from other frames. While joint-space-time attention, as demonstrated in ~\\cite{vivit,timesformer,xing2023svformer}, can effectively model temporal dependencies, it introduces a quadratic complexity in terms of attention calculation. \nFor a video with $L$ frames and $N$ tokens, the complexity of global spatial-temporal attention becomes $O(L^2N^2)$. \nTo address this issue, we propose a latent-shift attention module as shown in Fig.~\\ref{fig:shift}. \nIn addition to considering tokens within the current frame, we further conduct a patch-level shifting operation along the temporal dimension to shift tokens from the preceding $T$ frames onto the current frame, thereby composing a new latent feature frame.\nWe concatenate the latent feature of the current frame with the temporally shifted latent feature, forming the keys and values for attention calculation. The latent-shift attention can be formally written as:\n\\begin{equation}\n    \\mathbf{Q}=\\mathbf{W}_\\texttt{q}(\\bm{x}_{z_i}), \n\\end{equation}\n\\vspace{-0.5cm}\n\\begin{equation}\n    \\mathbf{K}=\\mathbf{W}_\\texttt{k}[\\bm{x}_{z_i}, \\bm{x}_{z_{shift}}],\n\\end{equation}\n\\vspace{-0.5cm}\n\\begin{equation}\n    \\mathbf{V}=\\mathbf{W}_\\texttt{v}[\\bm{x}_{z_i}, \\bm{x}_{z_{shift}}],\n\\end{equation}\nwhere $\\bm{x}_{z_i}$ denotes the query frame and $[ \\cdot ]$ means concatenate.\nThis approach reduces the complexity of attention to $O(2LN^2)$, significantly lowering the computational burden compared to global attention. Moreover, it allows the model to learn the relationships between adjacent frames, ensuring better temporal consistency in video generation.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{inv2.pdf}\n    \\caption{During inference, we sample a novel video from the latent noise inverted from the input video, guided by an edited prompt (\\emph{e.g.}, ``a Porsche sports car is moving on the road\"). }\n    \\label{fig:inference}\n\\end{figure}\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=1.0\\textwidth]{contra.pdf}\n    \\vspace{-0.3cm}\n    \\caption{Text-to-Video generation comparison with VDM~\\cite{vdm} CogVideo~\\cite{hong2022cogvideo}, VideoFusion~\\cite{videofusion} and LVDM~\\cite{lvdm} on the user study evaluation set.}\n    \\label{fig:contra}\n    \\vspace{-0.3cm}\n\\end{figure*}\n\n\\subsection{Super Resolution and Editing Models}\n\\label{editing}\n\n\\begin{table*}[]\n\\centering\n\\caption{Text-to-Video generation comparison on MSR-VTT~\\cite{msrvtt} dataset. We report the Fréchet Video Distance (FVD) scores and CLIPSIM scores.}\n\\vspace{-0.3cm}\n\\scalebox{0.9}{\n\\begin{tabular}{@{}ccccccc@{}}\n\\toprule\n\\bf Method       & \\bf Training Data           & \\bf Resolution &\\bf Zero-shot &\\bf Params(B) & \\bf FVD($\\downarrow$)   &  \\bf CLIPSIM($\\uparrow$)   \\\\ \\midrule\nGODIVA~\\cite{wu2021godiva}       & MSR-VTT        & 128x128    & No  & -  &-  & 0.2402  \\\\\nNÜWA~\\cite{wu2022nuwa}         & MSR-VTT         & 128x128    & No & 0.87 &-  & 0.2439  \\\\ \\midrule\n\n\\color{dt}Make-A-Video~\\cite{singer2022make} & WebVid-10M + HD-VILA-10M         & 256x256    & Yes  &9.72 & -  &\\color{dt} 0.3049  \\\\\n\\color{dt}VideoFactory~\\cite{videofactory}  & WebVid-10M + HD-VG-130M &256x256   &Yes & 2.04  &-  & \\color{dt}0.3005 \\\\ \nLVDM~\\cite{lvdm}         & WebVid-2M   & 256x256    &Yes   &1.16  &742   & 0.2381  \\\\\nMMVG~\\cite{mmvg}  & WebVid-2.5M   & 256x256    &Yes   &-  &-   & 0.2644  \\\\      \nCogVideo~\\cite{hong2022cogvideo}     & WebVid-5.4M         & 256x256    & Yes &15.5  &1294  & 0.2631  \\\\\nED-T2V~\\cite{edt2v}    &WebVid-10M   & 256x256 &Yes & 1.30  & - & 0.2763 \\\\\nMagicVideo~\\cite{magicvideo}  &WebVid-10M & 256x256 &Yes & -  & 998 & - \\\\\nVideo-LDM~\\cite{videoLDM}    & WebVid-10M         & 256x256    & Yes  &4.20  &-  & 0.2929  \\\\\nVideoComposer~\\cite{videocomposer}    & WebVid-10M     & 256x256 & Yes & 1.85  & 580  & 0.2932 \\\\\nLatent-Shift~\\cite{latentshift} & WebVid-10M & 256x256    & Yes   &1.53 &- & 0.2773  \\\\\nVideoFusion~\\cite{videofusion}   & WebVid-10M         &256x256     &Yes    &  1.83  &581   &0.2795  \\\\ \\midrule\nSimDA (Ours)         & WebVid-10M       & 256x256    & Yes  &\\textbf{1.08} &\\bf 456  & \\textbf{0.2945}  \\\\ \\bottomrule\n\\end{tabular}\n}\n\\label{Tab:msrvtt}\n\\vspace{-0.1em}\n\\end{table*}\n\n\\paragraph{Super Resolution (SR)} Due to constraints of limited GPU memory and the lack of high-resolution video-text datasets, most existing methods~\\cite{lvdm, videoLDM, latentshift}, including ours, are only able to generate images at a resolution of $256\\times256$. To overcome this limitation and generate higher-resolution outputs, we adopt two-stage training approach similar to cascaded Diffusion Models~\\cite{cascaded}. In the first stage, we generate videos with a  $256\\times 256$ resolution using our SimDA methods. In the second stage, we employ an LDM upsampler~\\cite{stablediffusion} to enhance the resolution of the videos to $1024\\times1024$. We incorporate noise augmentation and noise level conditioning, and train a super-resolution model using the following equation:\n\\begin{equation}\n\\mathbb{E}_{\\bm{x},\\bm{\\epsilon} \\sim \\mathcal{N}(\\bm{0},\\mathbf{I}),t}[\\lvert\\lvert \\bm{\\epsilon} - \\bm{\\epsilon}_\\theta([\\bm{x}_t,\\bm{x}_{low}],\\bm{c},t)\\rvert\\rvert^2_2],\n\\end{equation}\nwhere $\\bm{x}_{low}$ is the low-resolution video, we concatenate it with $\\bm{x}_t$ frame by frame following Video LDM~\\cite{videoLDM}. The architecture of SR is similar to T2V model in first stage, we change the original U-Net block by adding Spatial and Temporal Adapters as described in Sec.~\\ref{model} and only finetune the new added modules.\n\n\\paragraph{Text-guided Video Editing} In addition to performing T2V generation, our method could turn into one-shot tuning for text-guided video editing following Tune-A-Video~\\cite{tuneavideo}. The training pipeline of editing model is the same to our T2V method. However, for the inference stage, we adopt the DDIM inversion latents instead of random noisy latents together with edited prompt for novel video generation as shown in Fig.~\\ref{fig:inference}. By doing so, the pixel-level information control could remain in the inversion latent as demonstrated in ~\\cite{tuneavideo}. Owing to the light-weight module and efficient pipeline of our method, SimDA needs fewer training steps (200 steps compared to 500 steps) and thus the training time and inference time is much faster than Tune-A-Video~\\cite{tuneavideo}.\n \n\n\\section{Experiments}\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.85\\textwidth]{example.pdf}\n    \\caption{Results of extending our SimDA to text-to-video generation.}\n    \\label{fig:text2video}\n\\end{figure*}\n\n\\subsection{Implementation Details}\nOur T2V method is composed of two-stage models. The first model predicts video frames with a resolution $256\\times256$ (with a latent size of $32\\times32$), while the second model is a 4$\\times$ upsampler, producing a resolution of $1024\\times1024$. \nWe train the general T2V model on WebVid-10M~\\cite{webvid} dataset following~\\cite{videoLDM,latentshift}. We follow previous methods~\\cite{videofactory, latentshift, magicvideo} to report the CLIP score~\\cite{clip} and FVD (Fréchet Video Distance) score~\\cite{fvd} on MSR-VTT~\\cite{msrvtt}. Besides, we compare the FVD score and CLIP score on evaluation set of WebVid~\\cite{webvid} as in VideoFactory~\\cite{videofactory}. We also compare the parameter scale and inference speed of our method with some open-sourced methods~\\cite{lvdm,videofusion, latentshift}. Finally, we also provide a user study between our work and VDM~\\cite{vdm}, Latent-shift~\\cite{latentshift}, Video-Fusion~\\cite{videofusion} and LVDM~\\cite{lvdm}. \n\n\\subsection{Evaluation on Text-to-Video Generation}\nTo fully evaluate the generation performance of our SimDA, we conduct automatic evaluations on two distinct datasets: WebVid~\\cite{webvid} (Val), which shares the same domain as the training data, and MSR-VTT ~\\cite{msrvtt} in a zero-shot setting.\n\n\\paragraph{Evaluation on MSR-VTT}\nAs shown in Table~\\ref{Tab:msrvtt}, we evaluate CLIPSIM~\\cite{clip} and FVD~\\cite{fvd} on the widely used video generation benchmarks, MSR-VTT~\\cite{msrvtt}. We randomly select one text prompt per example from MSR-VTT~\\cite{msrvtt} and generate a total of 2,990 videos. Despite being a zero-shot setting, our method achieves an average CLIPSIM of 0.2945 that surpasses most of the competitors, indicating a strong semantic alignment between the generated videos and the input text. \nThough Make-A-Video~\\cite{singer2022make} and VideoFactory~\\cite{videofactory} offer higher CLIP scores, they utilize additional large-scale HD-VILA~\\cite{hd-vila} datasets for training.\n\n\\paragraph{Evaluation on WebVid}\nAs shown in Table~\\ref{Tab:webvid}, we create a validation set consisting of 4,476 randomly extracted text-video pairs from WebVid-10M. These pairs are not included in the training data following~\\cite{videofactory}. We conduct evaluations on this validation set and obtain impressive results. Our method achieves an FVD score of 363.98 and a CLIPSIM score of 0.3054. These scores are significantly higher than those achieved by existing methods such as VideoFusion~\\cite{videofusion} and LVDM~\\cite{lvdm}. Besides, our method shows competitive results compared to VideoFactory~\\cite{videofactory} which is trained with much larger datasets. These results clearly demonstrate the superiority of our approach.\n\n\\paragraph{Human Evaluation}\nIn order to address the limitations of existing metrics and assess the performance of our SimDA from a human perspective, we conduct an extensive user study. The study involves comparing our method with four state-of-the-art methods. Specifically, we select two publicly available models, namely VideoFusion~\\cite{videofusion} and LVDM~\\cite{lvdm}. Additionally, we consider two methods with similar scale parameters, VDM~\\cite{vdm} and Latent-shift~\\cite{latentshift}, which only showcase some samples on their websites.\n\nFor each case, participants were provided with two video samples, one is generated by our method and the other is from a competitor. They were then asked to compare the two samples in terms of video quality and text-video similarity. To ensure fairness in the comparisons, we also report the ratio of network parameter compared to ours.\nThe results, along with the parameter ratios, are presented in Table~\\ref{Tab:user}. The user study approach allows us to gain in-depth insights into the subjective evaluation of our method.\n\n\\begin{table}[]\\small\n\\centering\n\\caption{Text-to-video generation on the validation set of WebVid~\\cite{webvid}. We report the FVD and CLIPSIM scores.}\n\\vspace{-0.2cm}\n\\begin{tabular}{@{}lccc@{}}\n\\toprule\n\\bf Method    &\\bf Params(B)   & \\bf FVD($\\downarrow$)    & \\bf CLIPSIM($\\uparrow$) \\\\ \\midrule\nLVDM~\\cite{lvdm}      &1.16   & 455.53 & 0.2751  \\\\\nVideoFusion~\\cite{videofusion} &1.83  & 414.11 & 0.3000  \\\\\n\\color{dt}VideoFactory~\\cite{videofactory} &2.04   & \\color{dt}292.35 & \\color{dt}0.3070  \\\\ \\midrule\nSimDA (Ours)      &\\bf 1.08   & \\bf 363.98       & \\bf 0.3054        \\\\ \\bottomrule\n\\label{Tab:webvid}\n\\vspace{-0.3cm}\n\\end{tabular}\n\\end{table}\n\n\\paragraph{Qualitative Results} The visualization of T2V generation results are shown in Fig.~\\ref{fig:text2video} and  Fig.~\\ref{fig:fig1}(a). Besides, we show the comparison results in  Fig.~\\ref{fig:contra}.\nMore examples can be found at our website.\n\n\\begin{table}[]\n\\centering\n\\caption{User preference is depicted as a percentage indicating the proportion of individuals favoring our method over the compared approach. Param Ratio means the ratio of the network parameter \\emph{v.s.} Ours.}\n\\scalebox{0.8}{\n\\begin{tabular}{@{}ccccc@{}}\n\\toprule\n\\multicolumn{1}{l}{\\bf Sample}                                                   &\\bf Method       &\\bf Param Ratio &\\bf Quality &\\bf Faithfulness \\\\ \\midrule\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Open\\\\  Website\\end{tabular}}     & VDM~\\cite{vdm}          & $0.83\\times$            & 85.2\\%              &  81.4\\%                    \\\\ \\cmidrule(l){2-5} \n                                                                             & Latent-Shift~\\cite{latentshift} & $1.41\\times$          &  81.5\\%             &    79.3\\%                   \\\\ \\midrule\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Pretrained\\\\  Model\\end{tabular}} & VideoFusion~\\cite{videofusion}  &  $1.69\\times$        & 78.3\\%              & 79.5\\%                     \\\\ \\cmidrule(l){2-5} \n                                                                             & LVDM~\\cite{lvdm}         & $1.07\\times$            & 83.4\\%              & 84.7\\%                     \\\\ \\bottomrule\n\\label{Tab:user}\n\\end{tabular}\n}\n\\end{table}\n\n\\begin{table*}[t]\n\\caption{Quantitative comparison with evaluated baseline~\\cite{tuneavideo}. The ``Training'' refers to the process of optimization, and ``Memory'' refers to the GPU memory.}\n\\vspace{-2mm}\n\\centering\n\\setlength\\tabcolsep{0.3pt}\n\\small\n\\scalebox{1.06}{\n\\begin{tabular}{c|cc|cc|cc|cc|c}\n\\toprule\n\\multirow{2}{*}{\\textbf{Method}} & \\multicolumn{2}{c|}{\\textbf{Frame consistency}} &  \\multicolumn{2}{c|}{\\textbf{Textual alignment}} & \\multicolumn{2}{c|}{\\textbf{Runtime} [min]} & \\multicolumn{2}{c|}{\\textbf{Memory} [Gib]} & \\textbf{Params} [Mb]  \\\\ \\cline{2-10}\n              & ~{\\footnotesize CLIP Score$\\uparrow$} ~~& {\\footnotesize User Vote$\\uparrow$}~ & ~{\\footnotesize CLIP Score$\\uparrow$}~~ & {\\footnotesize User Vote$\\uparrow$}~ & ~{\\footnotesize Training$\\downarrow$}~~ & {\\footnotesize Inference$\\downarrow$}~  & ~{\\footnotesize Training$\\downarrow$} ~~& {\\footnotesize Inference$\\downarrow$}~ &\n              {\\footnotesize Tuned$\\downarrow$} \\\\ \\hline\nTune-A-Video~\\cite{tuneavideo}~      & 94.1      &  31.2\\%           &31.8      &  39.5\\%  & 9.3 & 0.8 & 31.3 & 11.4  &74.4     \\\\ \n\nSimDA(Ours)            & \\bf 94.9 & \\bf 68.8\\% & \\bf 31.9  & \\bf 60.5\\%  & \\bf 2.5 & \\bf 0.4 &\\bf 28.6 &\\bf 8.8 &\\bf 24.9 \\\\ \\bottomrule\n\\end{tabular}%\n}\n\\label{tab:tuneavideo}\n\\vspace{-2mm}\n\\end{table*}\n\n\\paragraph{Parameter Size and Inference Speed}\nWe conduct a comparison of number of parameters and inference speed, and the results are presented in Table~\\ref{tab:model_size}. For the speed comparison, we select CogVideo~\\cite{hong2022cogvideo}, Latent-Shift~\\cite{latentshift} and LVDM~\\cite{lvdm}.\nSimDA, on the other hand, stands out as it is significantly smaller than previous works and exhibits faster inference speed compared to other methods. Despite having fewer parameters, SimDA achieves superior performance in various benchmarks when compared to other methods. This validation further highlights our advantages in terms of model efficiency and performance.\n\n\\subsection{Evaluation on Text-guided Video Editing}\nFollowing the methodology of previous studies~\\cite{tuneavideo}, we employ CLIP score~\\cite{clip} and a user study to evaluate the performance of different methods in terms of frame consistency and textual alignment.\n\nFirst, we calculate the CLIP image embedding for all frames in the edited videos to measure frame consistency. The average cosine similarity between pairs of video frames is reported. Additionally, to assess textual faithfulness, we compute the average CLIP score between frames of the output videos and the corresponding edited prompts. A total of 15 videos from the dataset~\\cite{davis} were selected and edited based on object, background and style, resulting in 75 edited videos for each model. \nThe average results, presented in Table~\\ref{tab:tuneavideo}, highlight our method's exceptional ability to achieve semantic alignment. \n\nSecondly, we conduct a user study involving videos and text prompts. Participants were asked to vote for the edited videos that exhibited the best temporal consistency and those most accurately matched the textual description. Table~\\ref{tab:tuneavideo} demonstrates that our method, SimDA, receives the highest number of votes in both aspects, indicating superior editing quality and a strong preference from users in practical scenarios.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{ablation.pdf}\n    \\caption{Ablation of T2V generation results. TA, SA, AA, FA and LSA refer to Temporal Adapter, Spatial Adapter, Attention Adapter, FFN Adapter and Latent-Shift Attention. }\n    \\label{fig:ablation}\n\\end{figure}\n\n\\subsection{Ablation Study}\n\\label{sec:ablation}\nIn this section, we will discuss the effect of our proposed modules, we perform experiments on 1$K$ samples from validation set of WebVid~\\cite{webvid}.\n\n\\begin{table}[]\n\\centering\n\\caption{Ablation study on different modules. We report the FVD~\\cite{fvd} and CLIPSIM~\\cite{clip} on 1$K$ samples from the validation set of WebVid-10M~\\cite{webvid}. TA, SA, AA, FA and LSA represent Temporal Adapter, Spatial Adapter, Attention Adapter, FFN Adapter and Latent-shift Attention, respectively.}\n\\scalebox{0.93}{\n\\begin{tabular}{@{}ccccccc@{}}\n\\toprule\n       & \\bf TA &\\bf AA &\\bf FA &\\bf LSA & \\bf FVD($\\downarrow$) &\\bf CLIPSIM($\\uparrow$) \\\\ \\midrule\nw/o TA & & \\checkmark & \\checkmark & \\checkmark & 1470.1    & 0.2629  \\\\\nw/o SA & \\checkmark &  &  & \\checkmark & 811.3    & 0.2822    \\\\\nw/o AA &  \\checkmark &   & \\checkmark & \\checkmark &  764.8     &  0.2851        \\\\\nw/o FA & \\checkmark & \\checkmark &    & \\checkmark   &623.7     &  0.2962        \\\\\nw/o LSA &  \\checkmark & \\checkmark & \\checkmark &                 &618.2     & 0.3011       \\\\ \\midrule\nOurs   &   \\checkmark  &   \\checkmark                 &    \\checkmark          &   \\checkmark       &\\bf 530.2     & \\bf 0.3034        \\\\ \\bottomrule\n\\end{tabular}\n}\n\\label{Tab:ablation}\n\\end{table}\n\n\\paragraph{Effect of Temporal Adapter}\nTemporal modeling is a crucial component of video generation. In our video editing task, when compared to methods that rely on temporal attention modeling like Tune-A-Video~\\cite{tuneavideo}, we observe that our temporal adapter is more lightweight and achieves superior editing results as in Table~\\ref{tab:tuneavideo}. Additionally, we conduct ablation experiments, as shown in Table~\\ref{Tab:ablation} and Fig.~\\ref{fig:ablation}, where the lack of Temporal Adapter (TA) results in significantly higher FVD score and chaotic temporal sequences in the generated videos.\n\n\\paragraph{Effect of Spatial Adapter}\nWe also validate the effectiveness of the Spatial Adapter (SA) in transferring spatial knowledge of videos. As shown in Table~\\ref{Tab:ablation}, without the Attention Adapter (AA) and FFN Adapter (FA), the model's FVD and CLIPSIM scores for generated videos will become worse. Additionally, it can be observed from the Fig.~\\ref{fig:ablation} that the model exhibits misconceptions in understanding the text prompt without the spatial adapter.\n\n\\paragraph{Effect of Latent Shift Attention}\nTo investigate the impact of Latent-shift Attention (LSA) on the model, we replace it with regular single-frame spatial attention. Besides observing a decline in FVD and text alignment CLIPSIM scores in Table~\\ref{Tab:ablation}, we also test the CLIPSIM of each frame within the same video, which decreased from 96.4 to 94.5. This demonstrates that our LSA module can effectively model the adjacent frames relationship, leading to more consistent videos.\n\n\\section{Conclusion}\nIn this paper, we proposed SimDA, a parameter efficient video diffusion model for text guided video generation and editing. With the proposed light-weight spatial and temporal adapters, our method not only transferred from powerful spatial information but also modeled temporal relationship with least new parameters. The experimental results demonstrated that our approach has the fastest training and inference speed, while maintaining competitive generation and editing results. Our work is the first parameter-efficient video diffusion method serving as an efficient T2V fine-tuning baseline and paved the way for future research.\n\n{\\small\n}\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2304.08818v2.tex",
        "arXiv-2306.02018v2.tex",
        "arXiv-2308.09710v1.tex"
    ],
    "group_id": "group_59",
    "response": "### Summary: High-Resolution Video Synthesis with Latent Diffusion Models\n\n#### Introduction\nThe field of generative models has seen significant advancements in recent years, especially in the domain of image synthesis. Techniques such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and diffusion models have been pivotal in this progress. However, video synthesis, which involves generating coherent sequences of frames, has lagged behind due to the inherent challenges of modeling temporal dynamics and the scarcity of large-scale video datasets. The computational demands of training on video data have also been a major hurdle. Recent breakthroughs in diffusion models have enabled more efficient and scalable training objectives, leading to the development of Latent Diffusion Models (LDMs) that operate in a compressed latent space, reducing the computational burden significantly. This paper discusses three research papers that explore the application of LDMs to video synthesis, each proposing unique methods to address the challenges of generating high-resolution, temporally coherent videos.\n\nThe first paper, \"Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models,\" introduces Video LDMs, which leverage pre-trained image LDMs and add temporal layers to achieve video generation. The second paper, \"VideoComposer: Compositional Video Synthesis with Motion Controllability,\" presents a compositional framework for video synthesis that incorporates various conditions, including motion vectors, to enhance temporal consistency and controllability. The third paper, \"SimDA: Simple Diffusion Adapter for Efficient Video Generation,\" proposes a lightweight adaptation of image diffusion models to video generation, introducing a Latent-Shift Attention (LSA) mechanism for better temporal modeling.\n\nThese papers collectively address the challenges of video synthesis by focusing on efficient training strategies, leveraging pre-trained models, and introducing novel components to handle temporal dynamics. The main goal is to generate high-resolution videos that are temporally consistent and can be controlled by various inputs, including text, spatial conditions, and motion vectors. This summary will delve into the details of each paper, their commonalities, and innovations, and will also compare their results and discuss the implications for future research.\n\n#### Main Content of Each Paper\n\n**1. \"Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models\"**\nThis paper introduces Video LDMs, which are designed to generate high-resolution videos efficiently. The authors propose to first pre-train an LDM on images, and then introduce a temporal dimension to the latent space diffusion model, fine-tuning it on encoded image sequences (videos). This approach allows the model to leverage large-scale image datasets while focusing on the temporal alignment of video frames. The key innovation is the introduction of temporal layers that learn to align frames in a temporally consistent manner, without the need to retrain the entire model on video data. The authors also fine-tune the decoder to ensure temporal coherence in pixel space and introduce a video super resolution model to enhance the spatial resolution of the generated videos. They validate their approach on real driving scene videos of resolution $512\\times1024$, achieving state-of-the-art performance. Additionally, they demonstrate personalized text-to-video generation by transferring the trained temporal layers to differently fine-tuned text-to-image LDMs.\n\n**2. \"VideoComposer: Compositional Video Synthesis with Motion Controllability\"**\nVideoComposer is a framework that aims to enhance the controllability of video synthesis. It builds upon the concept of compositional generation, allowing users to flexibly compose videos with textual, spatial, and temporal conditions. The authors introduce motion vectors extracted from compressed videos as an explicit control signal for temporal dynamics. They also develop a Spatio-Temporal Condition encoder (STC-encoder) to effectively incorporate spatial and temporal relations, ensuring higher inter-frame consistency. The framework can generate videos based on text descriptions, sketches, reference videos, or even hand-crafted motions. VideoComposer achieves significant improvements in terms of inter-frame consistency and motion controllability, as demonstrated through extensive experimental results on various datasets.\n\n**3. \"SimDA: Simple Diffusion Adapter for Efficient Video Generation\"**\nSimDA presents a parameter-efficient approach to adapt image diffusion models for video generation. The authors introduce lightweight spatial and temporal adapters, as well as a Latent-Shift Attention (LSA) mechanism, to model temporal dynamics without adding significant new parameters. SimDA is built upon the Stable Diffusion model and uses a two-stage training approach to generate high-resolution videos. In the first stage, it generates videos at a resolution of $256\\times256$, and in the second stage, it employs an LDM upsampler to enhance the resolution to $1024\\times1024$. The authors also extend SimDA to text-guided video editing, demonstrating its efficiency and effectiveness in this task. SimDA achieves superior performance in various benchmarks while maintaining a small parameter size and fast inference speed.\n\n#### Commonalities and Innovations\n\n**Commonalities:**\n- **Latent Diffusion Models (LDMs):** All three papers build upon the concept of LDMs, which operate in a compressed latent space to reduce computational demands.\n- **Temporal Modeling:** Each paper introduces mechanisms to handle temporal dynamics in video generation, ensuring that the generated videos are coherent over time.\n- **Efficient Training Strategies:** The authors of each paper propose methods to fine-tune existing models rather than training from scratch, significantly reducing the computational and resource requirements.\n- **Control Signals:** All papers incorporate various forms of control signals, such as text prompts, spatial conditions, and motion vectors, to guide the generation process.\n\n**Innovations:**\n- **Video LDMs:** The first paper introduces temporal layers to align frames in a temporally consistent manner, turning pre-trained image LDMs into video generators.\n- **STC-Encoder:** VideoComposer develops a unified interface, the STC-encoder, to effectively incorporate spatial and temporal conditions, enhancing inter-frame consistency.\n- **Latent-Shift Attention (LSA):** SimDA proposes a novel attention mechanism that shifts latent features along the temporal dimension, reducing the complexity of temporal modeling and improving video quality.\n\n#### Comparison of Results\n\n**Evaluation Metrics:**\n- **FVD (Fréchet Video Distance):** This metric measures the distance between the generated video distribution and the real video distribution, indicating the quality of the generated videos.\n- **CLIPSIM (CLIP Similarity):** This metric evaluates the semantic alignment between the generated videos and the input text, indicating how well the videos adhere to the given prompts.\n\n**Performance on Real Driving Scene Videos:**\n- **Video LDMs:** Achieve an FVD score of 356 and an FID score of 51.9, outperforming the previous state-of-the-art Long Video GAN (LVG)~\\cite{brooks2022generating} in terms of FVD.\n- **VideoComposer:** Achieves an FVD score of 450.61 and a CLIPSIM score of 0.2932, demonstrating strong semantic alignment and inter-frame consistency.\n- **SimDA:** Achieves an FVD score of 456 and a CLIPSIM score of 0.3034, surpassing other methods in terms of both metrics.\n\n**Efficiency and Scalability:**\n- **Video LDMs:** Require only a small training set of captioned videos for fine-tuning, leveraging the power of pre-trained image LDMs.\n- **VideoComposer:** Demonstrates the ability to control video synthesis with diverse conditions, including motion vectors, sketches, and depth maps, while maintaining high synthesis quality.\n- **SimDA:** Achieves superior performance with fewer parameters (1.08B) and faster inference speed (11.20 seconds per video on an A100 GPU), making it highly efficient for both generation and editing tasks.\n\n**Personalized Text-to-Video Generation:**\n- **Video LDMs:** Show the first results for personalized text-to-video generation by transferring temporal layers to differently fine-tuned text-to-image LDMs.\n- **SimDA:** Demonstrates the ability to perform text-guided video editing with a one-shot tuning approach, significantly reducing the training time and maintaining high-quality results.\n\n**Human Evaluation:**\n- **Video LDMs:** Conduct human evaluation on driving video synthesis, showing that samples from the conditional model are preferred over unconditional samples in terms of realism.\n- **VideoComposer:** Conducts user studies to validate the effectiveness of the STC-encoder in terms of inter-frame consistency and motion controllability.\n- **SimDA:** Conducts a user study comparing the quality and text-video similarity of generated videos, showing that SimDA receives the highest number of votes for both aspects.\n\n#### Conclusion\nThe three papers discussed in this summary present innovative approaches to high-resolution video synthesis using latent diffusion models. Video LDMs introduce temporal alignment layers to pre-trained image models, achieving state-of-the-art performance on real driving scene videos. VideoComposer enhances temporal consistency by incorporating motion vectors and a unified STC-encoder, demonstrating superior controllability in various forms. SimDA proposes a parameter-efficient adaptation of image diffusion models to video generation, introducing lightweight adapters and LSA to model temporal dynamics effectively.\n\nThe common theme across these papers is the efficient use of pre-trained models and the introduction of mechanisms to handle temporal dynamics. Video LDMs focus on temporal alignment and super resolution, VideoComposer emphasizes the integration of diverse control signals, and SimDA aims to minimize the number of new parameters while maintaining high performance. These approaches collectively pave the way for efficient and high-quality video generation, opening new avenues for digital content creation and autonomous driving simulation.\n\n**Future Research Directions:**\n- **Scalability:** Further research could focus on scaling these models to even higher resolutions and longer video sequences.\n- **Diverse Control Signals:** Exploring the integration of more diverse control signals, such as audio and depth maps, could enhance the controllability and expressiveness of video generation.\n- **Personalization:** Developing methods for personalized video generation, where users can specify unique characteristics and styles, could lead to more tailored and creative content.\n- **Real-World Applications:** Investigating the application of these models in real-world scenarios, such as autonomous driving simulation and video editing, could provide valuable insights and practical benefits.\n\nIn summary, the research in high-resolution video synthesis using latent diffusion models is rapidly advancing, with each paper contributing unique innovations to address the challenges of temporal coherence and controllability. These advancements not only improve the quality and efficiency of video generation but also open exciting possibilities for future research and applications in the field of digital content creation."
}