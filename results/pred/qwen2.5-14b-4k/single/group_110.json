{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{On the Adequacy of Untuned Warmup for Adaptive Optimization}\n\n\\begin{document}\n\n\\maketitle\n\n{\n  \\begin{abstract}\nAdaptive optimization algorithms such as Adam are widely used in deep learning. The stability of such algorithms is often improved with a warmup schedule for the learning rate. Motivated by the difficulty of choosing and tuning warmup schedules, recent work proposes automatic variance rectification of Adam's adaptive learning rate, claiming that this rectified approach (``RAdam'') surpasses the vanilla Adam algorithm and reduces the need for expensive tuning of Adam with warmup. In this work, we refute this analysis and provide an alternative explanation for the necessity of warmup based on the magnitude of the \\emph{update term}, which is of greater relevance to training stability. We then provide some ``rule-of-thumb'' warmup schedules, and we demonstrate that simple untuned warmup of Adam performs more-or-less identically to RAdam in typical practical settings. We conclude by suggesting that practitioners stick to linear warmup with Adam, with a sensible default being linear warmup over $2 / (1 - \\beta_2)$ training iterations.\\blfootnote{Appendix available at \\url{https://arxiv.org/abs/1910.04209}.}  \\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\n\nStochastic gradient-based optimization serves as the workhorse training approach for many classes of parametric models, including neural networks. Stochastic gradient descent and its various first-order cousins~\\citep{polyak1964momentum, nesterov1983nag} have enabled numerous advances in deep learning across domains~\\citep{krizhevsky2012alexnet, he2016resnet, gehring2017convs2s}. More recently, adaptive optimization algorithms have become prevalent in training the largest deep learning models. These adaptive methods, which include Adagrad~\\citep{duchi2010adagrad}, RMSProp~\\citep{hinton2012rmsprop}, and Adam~\\citep{kingma2014adam}, scale the step size for each individual parameter based on various gradient moments.\n\nMany practitioners have adopted the Adam algorithm for general-purpose use; notably, the preponderance of recent state-of-the-art results in natural language processing~\\citep{devlin2018bert, radford2019gpt2, liu2019roberta, brown2020gpt3} have employed Adam, demonstrating the algorithm's ability to effectively train neural networks with parameter counts from 100 million to several billion.\n\nIn these large-scale settings, Adam's global learning rate is usually annealed with a ``warmup schedule'' which promotes early-stage training stability by regulating the size of the parameter updates. The prevalent warmup schedule is a simple linear warmup, in which the global learning rate starts at zero and increases by a constant at each iteration until reaching its intended value.~\\footnote{Linear warmup has also been deployed for first-order optimization -- see, for example, \\citet{goyal2017imagenet}.} The parameters of these warmup schedules are typically tuned for each problem setting and model.\n\n\\citet{liu2019radam} performed an analysis of Adam with warmup, concluding that Adam requires a warmup schedule to mitigate the large or divergent variance of the per-parameter scale term. They then propose the rectified Adam (``RAdam'') algorithm, which automatically corrects for this high variance. \\citeauthor{liu2019radam} highlight the robustness of RAdam, noting in particular that RAdam reduces or eliminates the need for tuning warmup schedules when using Adam. RAdam has been applied to domains including generative modeling~\\citep{yamamoto2020parallel}, natural language processing~\\citep{nguyen2019trans}, and video retrieval~\\citep{liu2019video}.\n\n\\subsection*{Contributions}\n\nOur contributions in this work are as follows:\n\n\\textbf{Reexamining RAdam and the variance-based motivation for warmup}\n\\quad\nWe dive into the inner operation of RAdam and find that it is precisely Adam with a fixed warmup schedule, with the only deviation being to perform four iterations of heavy-ball momentum~\\citep{polyak1964momentum} at the outset. We then argue that the variance-based motivation for warmup is impaired as it overlooks the correlation between the first and second moment estimators, which is crucial for understanding the actual parameter updates applied by Adam.\n\n\\textbf{Analyzing Adam's early-stage update magnitudes}\n\\quad\nShifting focus from gradients to parameter updates, we then perform a simulation-based analysis on the magnitudes of Adam's parameter updates. We find that even at a simulated local minimum of the objective, Adam exhibits considerable non-regularity in its early-stage parameter updates, shedding light on why Adam may require learning rate warmup to a greater extent than first-order optimization methods.\n\n\\textbf{Demonstrating the sufficiency of untuned warmup}\n\\quad\nWe provide some simple and intuitive ``rule-of-thumb'' warmup schedules for Adam, all of which require no tuning. As our main empirical result, we demonstrate that these schedules result in substantively \\emph{identical} performance and training dynamics to those of RAdam across a wide range of models, problem settings, and hyperparameters, indicating that any claimed benefits can be achieved with lower complexity using off-the-shelf optimization tools. As a sensible untuned default, we recommend linear warmup over $2 \\cdot \\left( 1 - \\beta_2 \\right)^{-1}$ iterations.\\section{Preliminaries}\n\\label{sec:prelim}\n\nWe begin with notation and a brief review of stochastic gradient descent and Adam.\n\n\\textbf{Primitives} \\quad $\\theta \\in \\R^p$ denotes a vector of model parameters. $\\L(\\theta) : \\R^p \\rightarrow \\R$ denotes a loss function to be minimized over the model parameters. $\\hL(\\theta) : \\R^p \\rightarrow \\R$ denotes an unbiased approximator of the loss function (\\eg{} over a minibatch). $\\grL(\\theta)$ and $\\grhL(\\theta)$ denote the gradient of $\\L(\\theta)$ and $\\hL(\\theta)$, respectively. The terms $\\theta$, $\\hL(\\theta)$, and $\\grhL(\\theta)$ are subscriptable by $t \\geq 0$, the optimization time step (``training iteration''). $\\theta_0$ represents the initial model parameters.\n\nWe write optimization algorithms as per-iteration procedures (``update rules''), taking the basic form:\n\\begin{equation*}\n    \\theta_t \\leftarrow \\theta_{t - 1} - \\underbrace{\\left\\{~~~~\\ldots~~~~\\right\\}}_{\\text{``update step''}}\n\\end{equation*}\n\n\\textbf{Stochastic gradient descent} \\quad The SGD algorithm, parameterized by learning rate $\\alpha > 0$, performs the following procedure at each iteration $t$:\n\\begin{equation}\n    \\theta_t \\leftarrow \\theta_{t - 1} - \\alpha \\cdot \\grhL_{t - 1}(\\theta_{t - 1}) \\label{eqn:sgd-update-rule}\n\\end{equation}\n\n\\textbf{Adam} \\quad The Adam algorithm~\\citep{kingma2014adam}, parameterized by global learning rate $\\alpha > 0$, discount factors $\\beta_1, \\beta_2 \\in (0, 1)$, and stability constant $\\epsilon > 0$, performs the following procedure at each iteration $t$:\n\\begin{align}\n    m_t &\\leftarrow \\beta_1 \\cdot m_{t - 1} + (1 - \\beta_1) \\cdot \\grhL_{t - 1}(\\theta_{t - 1}) \\label{eqn:adam-first-moment-update} \\\\\n    v_t &\\leftarrow \\beta_2 \\cdot v_{t - 1} + (1 - \\beta_2) \\cdot \\left[ \\grhL_{t - 1}(\\theta_{t - 1}) \\right]^2 \\label{eqn:adam-second-moment-update} \\\\\n    \\theta_t &\\leftarrow \\theta_{t - 1} - \\alpha \\left[ \\frac{(1 - \\beta_1^t)^{-1} \\cdot m_t}{\\sqrt{(1 - \\beta_2^t)^{-1} \\cdot v_t} + \\epsilon} \\right] \\label{eqn:adam-update-rule}\n\\end{align}\nwhere $m, v \\in \\R^p$ denote auxiliary memory (interpretable as first moment and second moment estimators of $\\grhL_t$, respectively). By convention, $m_0 = v_0 = 0$.\n\n\\textbf{Warmup schedules} \\quad For any optimization algorithm parameterized with a learning rate $\\alpha$, a \\emph{warmup schedule} $\\omega$ can be applied. $\\omega$ is a sequence of ``warmup factors'' $\\omega_t \\in [0, 1]$, which serve to dampen the step size of each iteration $t$. Specifically, a warmup schedule is imposed by replacing $\\alpha$ with $\\alpha_t = \\alpha \\cdot \\omega_t$ in the algorithm's update rule.\n\nPerhaps the most common functional form for the schedule is \\emph{linear warmup}, parameterized by a ``warmup period'' $\\tau$:\n\\begin{equation}\n    \\omega_t^{\\text{linear}, \\tau} = \\min \\left\\{ 1, \\frac{1}{\\tau} \\cdot t \\right\\}\n\\end{equation}\n\n\\textbf{Rectified Adam} \\quad The RAdam algorithm~\\citep{liu2019radam}, parameterized identically to Adam, performs the following procedure at each iteration $t$:\n\\begin{align}\n    \\rho_\\infty &\\leftarrow 2 / (1 - \\beta_2) - 1 \\label{eqn:radam-rhoinf} \\\\\n    \\rho_t &\\leftarrow \\rho_\\infty - 2 t \\cdot \\beta_2^t / (1 - \\beta_2^t) \\label{eqn:radam-rhot} \\\\\n    \\omega_t &\\leftarrow \\sqrt{\\frac{(\\rho_t - 4) (\\rho_t - 2) \\rho_\\infty}{(\\rho_\\infty - 4) (\\rho_\\infty - 2) \\rho_t}} \\label{eqn:radam-warmup} \\\\\n    m_t &\\leftarrow \\beta_1 \\cdot m_{t - 1} + (1 - \\beta_1) \\cdot \\grhL_{t - 1}(\\theta_{t - 1}) \\label{eqn:radam-first-moment-update} \\\\\n    v_t &\\leftarrow \\beta_2 \\cdot v_{t - 1} + (1 - \\beta_2) \\cdot \\left[ \\grhL_{t - 1}(\\theta_{t - 1}) \\right]^2 \\label{eqn:radam-second-moment-update} \\\\\n    \\theta_t &\\leftarrow \\theta_t - \\begin{cases}\n      \\alpha \\cdot (1 - \\beta_1^t)^{-1} \\cdot m_t & \\rho_t \\leq 4 \\\\\n      \\alpha \\cdot \\omega_t \\cdot \\left[ \\frac{(1 - \\beta_1^t)^{-1} \\cdot m_t}{\\sqrt{(1 - \\beta_2^t)^{-1} \\cdot v_t} + \\epsilon} \\right] & \\rho_t > 4\n    \\end{cases} \\label{eqn:radam-update-rule}\n\\end{align}\\section{Rectified Adam, Adaptive Variance, and Update Steps}\n\\label{sec:radam}\n\n\\begin{figure*}[ht]\n\\centering\n\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\n\\begin{subfigure}[t]{.3302\\linewidth}\n  \\centering\n\n  \\captionsetup{\n    justification=centering,\n    margin=0.1in,\n    size=normalsize\n  }\n\n  \\includegraphics[draft=false, width=1.0\\linewidth,trim={1.50cm 0.05cm 0.1cm 0.05cm},clip]{png/emnist_median_coeff_variation.png}\n  \n  \\caption{Median coefficient of variation of gradients (calculated over 256 trials).}\n  \n  \\label{fig:emnist-median-coeff-variation}\n\\end{subfigure}\n\\begin{subfigure}[t]{.3302\\linewidth}\n  \\centering\n\n  \\captionsetup{\n    justification=centering,\n    margin=0.1in,\n    size=normalsize\n  }\n\n  \\includegraphics[draft=false, width=1.0\\linewidth,trim={1.50cm 0.05cm 0.1cm 0.05cm},clip]{png/emnist_moment_corr.png}\n  \n  \\caption{Pearson correlation between Adam's $\\abs{m_t}$ and $\\sqrt{v_t}$.}\n  \n  \\label{fig:emnist-moment-corr}\n\\end{subfigure}\n\\begin{subfigure}[t]{.3302\\linewidth}\n  \\centering\n\n  \\captionsetup{\n    justification=centering,\n    margin=0.1in,\n    size=normalsize\n  }\n\n  \\includegraphics[draft=false, width=1.0\\linewidth,trim={1.55cm 0.05cm 0.1cm 0.00cm},clip]{png/emnist_median_update_magnitude.png}\n  \n  \\caption{Median parameter update magnitude ($\\epsilon = 0$).}\n  \n  \\label{fig:emnist-median-update-magnitude}\n\\end{subfigure}\n\n\\caption{Analysis of gradients and updates during the training of a simple feed-forward network on the EMNIST digit recognition task with the Adam optimizer -- see \\cref{apx:training-details-emnist} for comprehensive details.}\n\n\\label{fig:emnist}\\end{figure*}\n\nWe begin by uncovering the precise behavior of RAdam, before delving into its underlying variance-based motivation.\n\n\\subsection{RAdam: Perform 4 Iterations of Momentum SGD, Then Use Adam with Fixed Warmup}\n\n\\citeauthor{liu2019radam} describe RAdam as having two modes of operation: ``divergent variance'' and ``convergent variance'', corresponding respectively to the cases $\\rho_t \\leq 4$ and $\\rho_t > 4$ in \\cref*{eqn:radam-update-rule}. In the ``divergent'' phase, RAdam performs a variant of heavy-ball momentum SGD~\\citep{polyak1964momentum}.~\\footnote{The departure from standard heavy-ball momentum is in the bias correction by $(1 - \\beta_1^t)$.} Then, in the ``convergent'' phase, RAdam performs Adam, with the learning rate scaled down by $\\omega_t$.\n\nHowever, this is \\emph{not} dynamic scaling based on the training-time behavior of the optimizer \\emph{or} the distribution of the gradients. Rather, $\\omega_t$ is a deterministic function of solely $t$ and $\\beta_2$. Thus, the ``convergent'' phase is simply Adam with a fixed warmup schedule. We find that for all practically relevant values of $\\beta_2$, the condition $\\rho_t \\leq 4$ is simply $t \\leq 4$:\n\n\\begin{restatable}[]{fact}{rhotist} \\label{fact:rhot4-is-t4}\n  Assume that $0.8 \\leq \\beta_2 < 1$ and $t$ is a positive integer. Then, for $\\rho_t$ as defined in \\cref*{eqn:radam-rhot}:\n  \\begin{equation*}\n    \\rho_t \\leq 4 \\iff t \\leq 4\n  \\end{equation*}\n\\end{restatable}\n\\begin{proof}\nSee \\cref*{apx:misc-derivations-rhot-is-t4}.\n\\end{proof}\n\nThus follows a layman's description of RAdam:\n\\begin{enumerate}\n    \\item Perform four iterations of heavy-ball momentum.\n    \\item At iteration five and beyond, use Adam with a fixed warmup schedule.\n\\end{enumerate}\n\nOn its face, using four iterations of momentum at the beginning of training seems arbitrary. In preliminary experimentation (including the experimental settings described in \\cref*{sec:results}), we performed ablations over the following options for these four initial iterations:\n\\begin{itemize}\n  \\item Do absolutely nothing.\n  \\item Use Adam with learning rate $\\alpha \\cdot \\omega_5$ (i.e. do exactly what RAdam does at the fifth iteration).\n  \\item Use Adam with linear warmup to $\\alpha \\cdot \\omega_5$ (i.e. gradually warm up the learning rate to RAdam's fifth iteration).\n\\end{itemize}\nAs expected for a decision affecting only four training iterations, the practical difference between these choices is uniformly negligible. Thus, the only possible benefit of RAdam stems from its custom warmup schedule $\\omega_t$ for the fifth iteration and beyond. We revisit this topic in \\cref*{sec:ruleofthumb,sec:results}.\n\n\\subsection{Variance-Based Motivation for RAdam and Warmup}\n\\label{sec:radam-variance}\n\nGiven the arbitrary nature of RAdam's operation, we proceed to investigate the motivation for RAdam, which \\citeauthor{liu2019radam} also identify as the underlying motivation for warmup's crucial role in Adam.\n\n\\citeauthor{liu2019radam} focus their principal analysis on the term $\\sqrt{\\frac{1 - \\beta_2^t}{v_t}}$. Fixing $\\epsilon = 0$, this term can be interpreted as Adam's ``adaptive learning rate'', which scales the global learning rate for each parameter before computing Adam's final update for that parameter. They identify that the quantity $\\Var \\left[ \\sqrt{\\frac{1 - \\beta_2^t}{v_t}} \\right]$ does not exist during the first few training iterations,~\\footnote{The authors approximate $\\frac{1 - \\beta_2^t}{v_t}$ as having a scaled inverse $\\chi^2$ distribution, under the assumption that (1) all gradients are i.i.d.\\ and zero-mean, and (2) a simple average approximates an exponential moving average.} and even after converging to a finite value, continues to remain elevated for some time.\n\nPerhaps the most immediate observation is that early-stage gradients are \\emph{not} zero-mean. In fact, at the beginning of optimization, the expected magnitude of the gradients $\\grhL_t(\\theta_t)$ (\\ie{} absolute value of the deterministic gradients $\\grL(\\theta_t)$) should dominate the gradient variance, since a randomly-initialized model is exceedingly unlikely to be near a local minimum of $\\L(\\theta_t)$. Indeed, on a demonstration training run of a feed-forward network on the EMNIST digit recognition task, we observe that the median coefficient of variation of the gradients (\\cref*{fig:emnist-median-coeff-variation}) starts at below 1, indicating that for most parameters, the expected value of the gradient exceeds the standard deviation during early-stage training. Only beyond training iteration 50 does the coefficient of variation consistently exceed 1. Relaxing the zero-mean assumption decreases $\\Var \\left[ \\frac{1 - \\beta_2^t}{v_t} \\right]$ considerably.~\\footnote{Although \\citeauthor{liu2019radam} do not comment on the relative magnitudes of $\\Var \\left[ \\frac{1 - \\beta_2^t}{v_t} \\right]$, their Fig. 9 reveals that coefficients of variation below 1 dampen that quantity by an order of magnitude or more.}\n\nMore important, however, is that $m_t$ and $v_t$ are not at all independent. \\cref*{fig:emnist-moment-corr} reveals that in the EMNIST setting, the absolute value of the first moment estimator ($\\abs{m_t}$) is extremely correlated with the square root of the second moment estimator ($\\sqrt{v_t}$). Since Adam's parameter updates are proportional to $m_t / \\sqrt{v_t}$, high correlation between these two quantities implies that the magnitude of the updates are quite regular, despite the high variance of $\\sqrt{\\frac{1}{v_t}}$.\n\nIndeed, during the first training iteration ($t = 1$), it is guaranteed that $\\abs{m_t} = \\sqrt{v_t}$ for all parameters, making all Adam parameter updates either $-\\alpha$ or $\\alpha$ (assuming $\\epsilon = 0$). Thus, even though $\\Var \\left[ \\frac{1 - \\beta_2^t}{v_t} \\right]$ is divergent, the magnitude of the parameter updates themselves are constant. Ironically, it is \\emph{precisely} when the adaptive learning rate's variance is ``divergent'' that the actual parameter update magnitudes have zero variance. This suggests that the adaptive learning rate may not be the best medium of analysis for understanding the role of warmup in Adam.\n\n\\subsection{High Initial Update Step Magnitudes Necessitate Warmup in Adam}\n\\label{sec:radam-update-step-mag}\n\nWe provide an alternative view of the frequent necessity of learning rate warmup when using Adam. We do so by directly investigating the magnitudes of the update steps, perhaps the most proximate determinant of training stability.\n\nIn stochastic gradient descent, parameter updates are simply the gradients multiplied by the learning rate. Warmup for SGD can thus be motivated as mitigating the large expected magnitudes of the gradients (directly proportional to update magnitudes) and rapid change in gradients at the beginning of training~\\citep{goyal2017imagenet, gotmare2019heuristics}. Similar logic can be employed for adaptive methods.\n\nOn the other hand, if a model's gradients have near-zero means and low gradient variances, the update steps are similarly well-regulated and optimization via SGD can be stable without any learning rate warmup. For example, a nearly-converged model (thus having near-zero expected gradients and low gradient magnitudes) trained via SGD can have its optimization be stably restarted without learning rate warmup.\n\nThis is not the case with Adam. We proceed to computationally analyze the magnitude of Adam's update step over the course of training. Specifically, we demonstrate via simulation that even when the model parameters $\\theta_t$ are initialized at an idealized local minimum of $\\L(\\theta)$ (\\ie{} $\\grhL_t(\\theta_t)$ has zero mean and is i.i.d.\\ across time), the magnitude of Adam's update steps will still be quite high at the start of training, only gradually decaying toward a stationary distribution.\n\n\\begin{figure}[t]\n\\centering\n\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\n\\includegraphics[draft=false, width=0.9\\linewidth]{png/update_magnitude.png}\n  \n\\caption{Distribution of Adam's update step magnitudes at a simulated local minimum of $\\L(\\theta)$ (quantiles: $\\left\\{ 2.5\\%, 25\\%, 50\\%, 75\\%, 97.5\\% \\right\\}$).}\n  \n\\label{fig:simulated-update-distribution}\\end{figure}\n\n\\textbf{Simulation configuration} \\quad  All gradients are simulated as i.i.d.\\, normal variables with zero mean and constant isotropic variance $10^{-9}$, thus approximating the optimization dynamics at an exact local minimum of $\\L(\\theta)$.~\\footnote{Note that the behavior of Adam in this setting is invariant to the choice of variance constant.} We sample independent gradient trajectories (each 1000 iterations long) for 25000 parameters. We then run the Adam optimizer with these sampled gradients and evaluate the distribution of the update step magnitudes (before multiplication by the global learning rate $\\alpha$) at each iteration. The Adam optimizer configuration is $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, and $\\epsilon = 0$.\n\n\\textbf{Simulation results} \\quad \\cref*{fig:simulated-update-distribution} depicts the outcome of this computational simulation. As alluded to in \\cref*{sec:radam-variance}, the update magnitudes for all parameters start at $1 \\cdot \\alpha$. The update magnitudes gradually decay but continue to remain high for quite some time, only beginning to settle into a stationary distribution after 40 or so training iterations (with median update magnitude $\\approx 0.16 \\cdot \\alpha$). We extend the trajectory length to 10000 and find that the median update step of the stationary distribution is approximately $0.153 \\cdot \\alpha$.\n\nThese results imply that unlike SGD, Adam will always encounter early-stage training instability by way of large update magnitudes, even when the model is \\emph{already initialized at a local minimum}. This stands as a contributing factor to Adam's need for learning rate warmup above and beyond that of first-order methods.\n\n\\textbf{Comparison to real-world, random initialization settings}\n\\quad\nFinally, we examine the update step distribution of a model initialized away from a local minimum of $\\L(\\theta)$. \\cref*{fig:emnist-median-update-magnitude} depicts the median parameter update magnitudes of Adam in the EMNIST setting from \\cref*{sec:radam-variance}. We observe a qualitative similarity to the local minimum simulation results -- the update magnitudes start at $1 \\cdot \\alpha$, only gradually settling into a stationary distribution around $0.15 \\cdot \\alpha$.\n\nNote that the EMNIST optimization decreases more slowly in update magnitude and takes longer ($\\approx 100$ training iterations) to settle into the stationary distribution. This suggests that the update step non-regularity observed in the idealized local minimum initialization setting is only exacerbated in the more realistic setting of random initialization.\\section{Rules of Thumb}\n\\label{sec:ruleofthumb}\n\n\\begin{figure*}[ht]\n\\centering\n\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\n\\begin{subfigure}[t]{.49\\linewidth}\n  \\centering\n\n  \\captionsetup{\n    justification=centering,\n    margin=0.1in\n  }\n\n  \\includegraphics[draft=false, width=0.9\\linewidth]{png/effective_warmup_period.png}\n  \n  \\caption{Effective warmup periods of RAdam and rule-of-thumb warmup schedules, as a function of $\\beta_2$.}\n  \n  \\label{fig:effective-warmup-period}\n\\end{subfigure}\n\\begin{subfigure}[t]{.49\\linewidth}\n  \\centering\n\n  \\captionsetup{\n    justification=centering,\n    margin=0.1in\n  }\n\n  \\includegraphics[draft=false, width=0.9\\linewidth]{png/warmup_schedule_0_9990.png}\n  \n  \\caption{RAdam and rule-of-thumb warmup schedules over time for $\\beta_2 = 0.999$.}\n  \n  \\label{fig:warmup-schedule}\n\\end{subfigure}\n\n\\caption{Comparison of various characteristics of RAdam and rule-of-thumb warmup schedules.}\n\n\\label{fig:radam-rule-of-thumb-comparison}\\end{figure*}\n\nTurning to the practical application of learning rate warmup, we first define a simple heuristic function, the \\emph{effective warmup period}, to characterize the dampening effect of warmup schedules. We then present and intuitively motivate two Adam warmup schedules that require no tuning and are thus usable as rules of thumb.\n\n\\subsection{Effective Warmup Period}\n\nWe define the \\emph{effective warmup period} $\\mathcal{T}(\\omega)$ of a warmup schedule $\\omega$ as follows:\n\\begin{equation*}\n    \\mathcal{T}(\\omega) = \\sum_{t = 1}^\\infty \\left( 1 - \\omega_t \\right)\n\\end{equation*}\nIntuitively, this is the sum of the warmup's dampening effect across all of training.\n\n\\subsection{Exponential Warmup}\n\nWe propose a simple ``exponential warmup'' schedule based on a decaying exponential and a constant $\\tau$:\n\\begin{equation}\n    \\omega_t^{\\text{expo}, \\tau} = 1 - \\exp \\left( -\\frac{1}{\\tau} \\cdot t \\right)\n\\end{equation}\nThe constant $\\tau$ is analogous to a linear warmup period, and we recommend $\\tau = (1 - \\beta_2)^{-1}$ as a rule of thumb:\n\\begin{equation}\n    \\label{eqn:exp-rule-of-thumb}\n    \\omega_t^{\\text{expo}, \\text{untuned}} = 1 - \\exp \\left( -(1 - \\beta_2) \\cdot t \\right)\n\\end{equation}\n\nIn choosing $\\tau$, our guiding (albeit extremely speculative) intuition is to have the warmup factor $\\omega_t^{\\text{expo}, \\tau}$ be roughly equivalent to Adam's second moment bias correction term in Adam. This term, $1 - \\beta_2^t$, is the sum of the coefficients in the moving average estimation of the second moment, and can thus be interpreted as how ``complete'' the second moment estimator is at any given point in time. We briefly show the approximate correspondence between the bias correction term and the warmup factor:~\\footnote{The second step follows from a first-order Taylor expansion of $\\log(\\beta_2)$ around $\\beta_2 = 1$. In practice, this approximation is extremely accurate for typical values of $\\beta_2$.}\n\\begin{align*}\n    1 - \\beta_2^t &= 1 - \\exp \\left( \\log(\\beta_2) \\cdot t \\right) \\\\\n    &\\approx 1 - \\exp \\left( (\\beta_2 - 1) \\cdot t \\right) \\\\\n    &= 1 - \\exp \\left( -(1 - \\beta_2) \\cdot t \\right)\n\\end{align*}\n\n\\subsection{Linear Warmup}\n\nRecall the formulation of linear warmup:\n\\begin{equation*}\n    \\omega_t^{\\text{linear}, \\tau} = \\min \\left\\{ 1, \\frac{1}{\\tau} \\cdot t \\right\\}\n\\end{equation*}\n\nAs a similar rule of thumb to the exponential warmup schedule, we suggest performing linear warmup over $\\tau = 2 \\cdot \\left( 1 - \\beta_2 \\right)^{-1}$ iterations:\n\\begin{equation}\n    \\label{eqn:linear-rule-of-thumb}\n    \\omega_t^{\\text{linear}, \\text{untuned}} = \\min \\left\\{ 1, \\frac{1 - \\beta_2}{2} \\cdot t \\right\\}\n\\end{equation}\n\nOur choice of $\\tau$ is carried over from exponential warmup as a starting point. To preserve the same effective warmup period, the $\\tau$ from the exponential rule-of-thumb is multiplied by 2 to account for the fact that exponential warmup decelerates over time, whereas linear warmup does not. We elaborate in \\cref*{apx:misc-derivations-linear-exp-equivalence}.\n\n\\subsection{Comparison with RAdam}\n\nWe first compare RAdam with the rule-of-thumb schedules (\\cref*{eqn:exp-rule-of-thumb,eqn:linear-rule-of-thumb}) by computing their effective warmup periods across a range of $\\beta_2$ values.~\\footnote{For the purpose of this analysis, $w_{\\{1, 2, 3, 4\\}}$ are all defined to be zero for RAdam.} \\cref*{fig:effective-warmup-period} reveals that the effective warmup periods of RAdam and the rules of thumb are nearly identical across all practical values of $\\beta_2$, indicating that they have similar dampening effects over early-stage training.\n\nWe then proceed to examine the trajectory of the warmup schedule for the commonly used setting of $\\beta_2 = 0.999$. \\cref*{fig:warmup-schedule} reveals that the functional forms of the warmup factors are qualitatively similar in magnitudes. The warmup schedules for RAdam and the rule-of-thumb exponential warmup closely correspond in shape as well.\n\nWe thus posit that RAdam and the untuned rule-of-thumb warmup schedules are more or less interchangeable. An empirical verification follows.\\section{Experiments}\n\\label{sec:results}\n\nWe evaluate untuned exponential warmup (\\cref*{eqn:exp-rule-of-thumb}), untuned linear warmup (\\cref*{eqn:linear-rule-of-thumb}), and RAdam across a variety of supervised machine learning tasks. For brevity, all experimental settings are summarized in the main text and comprehensively detailed in \\cref*{apx:training-details}.\n\n\\subsection{Image Classification}\n\\label{sec:results-imagenet}\n\nUsing each of the three warmup methods, we train a ResNet-50 model~\\citep{he2016resnet} on the ILSVRC (``ImageNet'') image classification dataset with various configurations of Adam. Specifically, we sweep over:\n\\begin{align*}\n    \\alpha~\\text{(learning rate)} &\\in \\left\\{ {10}^{-4}, {10}^{-3}, {10}^{-2} \\right\\} \\\\\n    \\beta_2 &\\in \\left\\{ 0.99, 0.997, 0.999 \\right\\}\n\\end{align*}\n\n\\cref*{tbl:summary-imagenet} presents the top-1 error rates at the end of training for the three warmup methods. Across all configurations of Adam, the top-1 error rates are indistinguishable between the warmup methods.~\\footnote{The best error rates fall roughly 3\\% behind those from SGD, as is typical with Adam on computer vision tasks.}\n\n\\begin{table}[ht]\n\\centering\n\n\\captionsetup{\n  justification=centering,\n  margin=0.25in\n}\n\n\\scriptsize\n\n\\setlength{\\tabcolsep}{3pt}\n\n\\begin{tabular}{l l | c c c}\n  \\toprule\n  \n  \\textbf{LR} & \\textbf{$\\beta_2$} &          \\textbf{Exponential} &               \\textbf{Linear} &                \\textbf{RAdam} \\\\\n  \n  \\midrule\n  \n $ {10}^{-4} $ &      0.99 &  $ 34.2 \\% \\pm 0.1 $ &  $ 34.2 \\% \\pm 0.1 $ &  $ 34.2 \\% \\pm 0.1 $ \\\\\n $ {10}^{-4} $ &     0.997 &  $ 34.3 \\% \\pm 0.2 $ &  $ 34.2 \\% \\pm 0.2 $ &  $ 34.1 \\% \\pm 0.1 $ \\\\\n $ {10}^{-4} $ &     0.999 &  $ 34.5 \\% \\pm 0.1 $ &  $ 34.4 \\% \\pm 0.1 $ &  $ 34.2 \\% \\pm 0.3 $ \\\\\n $ {10}^{-3} $ &      0.99 &  $ 27.9 \\% \\pm 0.1 $ &  $ 28.0 \\% \\pm 0.1 $ &  $ 28.4 \\% \\pm 0.1 $ \\\\\n $ {10}^{-3} $ &     0.997 &  $ 27.9 \\% \\pm 0.1 $ &  $ 27.9 \\% \\pm 0.1 $ &  $ 28.3 \\% \\pm 0.1 $ \\\\\n $ {10}^{-3} $ &     0.999 &  $ 28.2 \\% \\pm 0.1 $ &  $ 28.3 \\% \\pm 0.1 $ &  $ 28.4 \\% \\pm 0.1 $ \\\\\n $ {10}^{-2} $ &      0.99 &  $ 29.3 \\% \\pm 0.1 $ &  $ 29.3 \\% \\pm 0.3 $ &  $ 29.4 \\% \\pm 0.2 $ \\\\\n $ {10}^{-2} $ &     0.997 &  $ 29.2 \\% \\pm 0.2 $ &  $ 29.3 \\% \\pm 0.1 $ &  $ 29.4 \\% \\pm 0.5 $ \\\\\n $ {10}^{-2} $ &     0.999 &  $ 28.9 \\% \\pm 0.2 $ &  $ 28.7 \\% \\pm 0.1 $ &  $ 29.8 \\% \\pm 0.4 $ \\\\  \n  \\bottomrule\n\\end{tabular}\n\n\\caption{Top-1 error rates of ResNet-50 on ImageNet (means and standard deviations over 5 random seeds).}\n\n\\label{tbl:summary-imagenet}\\end{table}\n\n\\begin{figure}[ht]\n\\centering\n\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\n\\includegraphics[draft=false, width=0.9\\linewidth]{png/imagenet_train_loss_lr_0_0010_beta2_0_9990.png}\n  \n\\caption{Mean training loss (5 seeds) of ResNet-50 on Imagenet, using Adam with $\\alpha = 10^{-3}$ and $\\beta_2 = 0.999$.}\n  \n\\label{fig:imagenet-train-loss}\\end{figure}\n\nWe next examine the course of optimization for individual configurations of Adam's $\\alpha$ and $\\beta_2$. \\cref*{fig:imagenet-train-loss} depicts the training loss using the popular ``default'' Adam configuration of learning rate $\\alpha = 10^{-3}$ and $\\beta_2 = 0.999$, revealing that the behavior of these warmup methods is indeed nearly indistinguishable.\n\n\\cref*{apx:supp-results-imagenet} provides both training and validation metrics (\\cref*{fig:imagenet-train-loss-full,fig:imagenet-valid-error-full} respectively) for all tested configurations, reinforcing this trend.\n\n\\subsection{Language Modeling}\n\\label{sec:results-wiki103}\n\nUsing each of the three warmup methods, we train a state-of-the-art Transformer-based language model from~\\citet{baevski2018adaptivelm} on \\texttt{WIKITEXT-103}. We sweep over the following grid of Adam hyperparmeters:\n\\begin{align*}\n    \\alpha~\\text{(learning rate)} &\\in \\left\\{ 1 \\cdot {10}^{-4}, 3 \\cdot {10}^{-4}, 5 \\cdot {10}^{-4} \\right\\} \\\\\n    \\beta_2 &\\in \\left\\{ 0.99, 0.998, 0.999 \\right\\}\n\\end{align*}\nwith $\\beta_1=0.9$ and $\\epsilon=10^{-7}$ fixed. As with image classification, we observe in \\cref*{tbl:summary-wiki103} that the choice of warmup method has a minimal impact on training across different hyperparameters.\n\n\\begin{table}[ht]\n\\centering\n\n\\captionsetup{\n  justification=centering,\n  margin=0.25in\n}\n\n\\scriptsize\n\n\\setlength{\\tabcolsep}{3pt}\n\n\\begin{tabular}{l l | c c c}\n  \\toprule\n  \n  \\textbf{LR} & \\textbf{$\\beta_2$} &          \\textbf{Exponential} &               \\textbf{Linear} &                \\textbf{RAdam} \\\\\n  \n  \\midrule\n  \n$1 \\cdot {10}^{-4}$ & 0.99& $ 21.0 \\pm  0.1 $& $ 21.0 \\pm  0.1 $& $ 21.1 \\pm  0.1 $\\\\\n$1 \\cdot {10}^{-4}$ & 0.998& $ 19.9 \\pm  0.0 $& $ 19.9 \\pm  0.0 $& $ 20.0 \\pm  0.0 $\\\\\n$1 \\cdot {10}^{-4}$ & 0.999& $ 20.0 \\pm  0.0 $& $ 20.0 \\pm  0.0 $& $ 20.1 \\pm  0.1 $\\\\\n$3 \\cdot {10}^{-4}$ & 0.99& $ 21.3 \\pm  0.3 $& $ 20.8 \\pm  0.1 $& $ 22.4 \\pm  0.0 $\\\\\n$3 \\cdot {10}^{-4}$ & 0.998& $ 19.6 \\pm  0.0 $& $ 19.6 \\pm  0.0 $& $ 19.6 \\pm  0.1 $\\\\\n$3 \\cdot {10}^{-4}$ & 0.999& $ 19.5 \\pm  0.0 $& $ 19.5 \\pm  0.0 $& $ 19.5 \\pm  0.0 $\\\\\n$5 \\cdot {10}^{-4}$ & 0.99& $ 24.4 \\pm  2.4 $& $ 24.1 \\pm  1.4 $& $ 26.0 \\pm  1.8 $\\\\\n$5 \\cdot {10}^{-4}$ & 0.998& $ 20.1 \\pm  0.0 $& $ 20.0 \\pm  0.0 $& $ 20.1 \\pm  0.0 $\\\\\n$5 \\cdot {10}^{-4}$ & 0.999& $ 19.8 \\pm  0.0 $& $ 19.7 \\pm  0.1 $& $ 19.7 \\pm  0.0 $\\\\  \n  \\bottomrule\n\\end{tabular}\n\n\\caption{Validation perplexity of a Transformer LM on the \\texttt{WIKITEXT-103} dataset (means and standard deviations over 3 random seeds).}\n\n\\label{tbl:summary-wiki103}\\end{table}\n\n\\begin{figure}[ht]\n\\centering\n\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\n\\includegraphics[draft=false, width=0.9\\linewidth]{png/wiki103_valid_ppl_lr_0_0001_beta2_0_999.png}\n  \n\\caption{Mean validation perplexity (3 seeds) of Transformer LM on \\texttt{WIKITEXT-103}, using Adam with $\\alpha = 10^{-4}$ and $\\beta_2 = 0.999$.}\n  \n\\label{fig:wiki103-valid-ppl}\\end{figure}\n\n\\cref*{fig:wiki103-valid-ppl} depicts the validation perplexity throughout training for the best Adam parametrization ($\\alpha=10^{-4}$ and $\\beta_2=0.999$), which similarly supports the indistinguishability of the warmup methods.\n\n\\subsection{Machine Translation}\n\\label{sec:results-wmt16-en-de}\n\nFinally, we evaluate the warmup methods on a large scale machine translation task. Using each of the three warmup methods, we train a Transformer model~\\citep{vasmani2017transformer} on the WMT16 English-German (``EN-DE'') dataset. We fix Adam's $\\beta_1=0.9$ and $\\epsilon=10^{-7}$ and sweep over the following grid of Adam hyperparameters:\n\\begin{align*}\n    \\alpha~\\text{(learning rate)} &\\in \\left\\{ 5 \\cdot {10}^{-5}, 8 \\cdot {10}^{-5}, 1 \\cdot {10}^{-4} \\right\\} \\\\\n    \\beta_2 &\\in \\left\\{ 0.98, 0.99, 0.998, 0.999 \\right\\}\n\\end{align*}\n\n\\begin{table}[ht]\n\\centering\n\n\\captionsetup{\n  justification=centering,\n  margin=0.25in\n}\n\n\\scriptsize\n\n\\setlength{\\tabcolsep}{3pt}\n\n\\begin{tabular}{l l | c c c}\n  \\toprule\n  \n  \\textbf{LR} & \\textbf{$\\beta_2$} &          \\textbf{Exponential} &               \\textbf{Linear} &                \\textbf{RAdam} \\\\\n  \n  \\midrule\n  \n$5 \\cdot {10}^{-5}$ & 0.98& $ 24.5 \\pm  0.1 $& $ 24.4 \\pm  0.1 $& $ 24.4 \\pm  0.1 $\\\\\n$5 \\cdot {10}^{-5}$ & 0.99& $ 24.5 \\pm  0.0 $& $ 24.5 \\pm  0.0 $& $ 24.5 \\pm  0.1 $\\\\\n$5 \\cdot {10}^{-5}$ & 0.998& $ 24.3 \\pm  0.2 $& $ 24.4 \\pm  0.2 $& $ 24.4 \\pm  0.1 $\\\\\n$5 \\cdot {10}^{-5}$ & 0.999& $ 24.2 \\pm  0.1 $& $ 24.2 \\pm  0.1 $& $ 24.1 \\pm  0.1 $\\\\\n$8 \\cdot {10}^{-5}$ & 0.98& $ 25.9 \\pm  0.1 $& $ 25.9 \\pm  0.1 $& $ 25.9 \\pm  0.1 $\\\\\n$8 \\cdot {10}^{-5}$ & 0.99& $ 25.9 \\pm  0.2 $& $ 25.9 \\pm  0.1 $& $ 25.9 \\pm  0.0 $\\\\\n$8 \\cdot {10}^{-5}$ & 0.998& $ 26.0 \\pm  0.1 $& $ 25.2 \\pm  1.0 $& $ 25.9 \\pm  0.1 $\\\\\n$8 \\cdot {10}^{-5}$ & 0.999& $ 25.7 \\pm  0.1 $& $ 25.8 \\pm  0.1 $& $ 25.7 \\pm  0.0 $\\\\\n$1 \\cdot {10}^{-4}$ & 0.98& $ 26.5 \\pm  0.1 $& $ 26.6 \\pm  0.1 $& $ 26.6 \\pm  0.1 $\\\\\n$1 \\cdot {10}^{-4}$ & 0.99& $ 26.7 \\pm  0.1 $& $ 26.6 \\pm  0.1 $& $ 26.6 \\pm  0.0 $\\\\\n$1 \\cdot {10}^{-4}$ & 0.998& $ 25.9 \\pm  0.9 $& $ 26.5 \\pm  0.1 $& $ 26.6 \\pm  0.0 $\\\\\n$1 \\cdot {10}^{-4}$ & 0.999& $ 26.2 \\pm  0.2 $& $ 26.4 \\pm  0.0 $& $ 26.4 \\pm  0.0 $\\\\  \n  \\bottomrule\n\\end{tabular}\n\n\\caption{BLEU score of Transformer on WMT16-EN-DE (means and standard deviations over 3 random seeds).}\n\n\\label{tbl:summary-wmt16-en-de}\\end{table}\n\n\\begin{figure}[ht]\n\\centering\n\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\n\\includegraphics[draft=false, width=0.9\\linewidth]{png/wmt16_en_de_valid_ppl_lr_0_0001_beta2_0_999.png}\n  \n\\caption{Mean validation perplexity (3 seeds) of Transformer on WMT16-EN-DE, using Adam with $\\alpha = 10^{-4}$ and $\\beta_2 = 0.999$.}\n  \n\\label{fig:wmt16-en-de-valid-ppl}\\end{figure}\n\nWe observe no perceptible differences between the warmup methods in either final performance (\\cref*{tbl:summary-wmt16-en-de}), or in the training-time metrics of a single canonical configuration ($\\alpha=10^{-4}$ and $\\beta_2=0.999$, shown in \\cref*{fig:wmt16-en-de-valid-ppl}).\\section{Discussion}\n\\label{sec:discussion}\n\nWe discuss various consequences of our findings, along with directions for future work.\n\n\\subsection{Extended Warmup Periods}\n\nThe analysis of the update step magnitudes in \\cref*{sec:radam-update-step-mag} suggests shorter warmup periods than those typically used in practice. For example, using the setting of $\\beta_2 = 0.999$, Adam's update magnitudes in the theoretical model converge to a stationary distribution in roughly 40 iterations. If update magnitudes were the only relevant consideration, then a warmup schedule over a few hundred iterations would suffice to stabilize training. In contrast, the effective warmup periods of both RAdam and our rule-of-thumb schedules are roughly 1000 iterations for $\\beta_2 = 0.999$. State-of-the-art methods with hand-tuned warmup schedules often go well beyond, using up to 10000 iterations of linear warmup in some cases~\\citep{liu2019roberta,baevski2018adaptivelm,ott2019fairseq}.\n\nAccordingly, we surmise that the precise channel by which Adam necessitates an extended period of warmup is still an unresolved question, likely related to the properties of the gradients at random initialization. Future work could rigorously investigate the effect of extended warmup periods on the training dynamics of Adam, beyond simple per-iteration statistics.\n\n\\subsection{Consequences of Update Step Invariance to Gradients}\n\nOne ancillary finding of \\cref*{sec:radam-update-step-mag} is that the magnitudes of Adam's update steps during later stages of training are largely invariant to the properties or dynamics of the gradient distribution -- both the simulated local optimum and real-world random initialization settings result in convergence to similar stationary distributions of update magnitudes. This suggests that learning rate \\emph{decay} at later stages of training could be the only way to improve late-stage convergence, as Adam's late-stage update magnitudes do not appear to be very sensitive to the variance or stationarity of gradients. In particular, we suspect that variance-based methods of improving the late-stage convergence of SGD, such as increasing the batch size~\\citep{smith2018batchsize}, will not yield comparable benefits when applied to Adam, as the stationary distribution of the update magnitudes will remain largely the same. Partially adaptable methods~\\citep{chen2018padam, keskar2017adamsgd, luo2019adabound}, which interpolate between the full adaptivity of Adam and the non-adaptivity of SGD, may hold more promise for improving late-stage convergence.\n\n\\subsection{Dynamic Warmup}\n\nAll methods considered by this work use fixed warmup schedules, computed only as a function of the training iteration $t$ and various hyperparameters. Such schedules will inevitably be brittle to some combination of problem setting, model, and optimizer configuration. Another direction for future work could be to devise truly dynamic mechanisms for scheduling warmup in Adam. Such a mechanism could (among other things) track and utilize auxiliary statistics, such as the running moments of the applied updates, in order to determine the stability of training at each iteration.\n\nThis direction comes dangerously close to seeking the ``holy grail'' of an automatic learning rate tuner; existing attempts to devise such a method have achieved limited adoption as of yet~\\citep{li2017sme, zhang2017yellowfin, baydin2018hgd}. What makes this potentially more tractable is that a maximum learning rate is still tuned and given \\emph{a priori} to the optimizer; the task is then restricted to dynamic scheduling of the learning rate from zero to this known constant, instead of an arbitrary range $(0, \\infty)$.\\section{Conclusion}\n\\label{sec:conclusion}\n\nWe show that the Rectified Adam (RAdam) algorithm can be characterized as four steps of momentum SGD, followed by Adam with a fixed warmup schedule. We also examine the shortcomings of a variance-based approach to analyzing the learning rate warmup heuristic, and we illustrate that Adam's frequent need for learning rate warmup can be partially explained by inspecting Adam's early-stage update step magnitudes when applied to an already-converged model.\n\nRAdam's claimed benefits are its superior performance to Adam and its elimination of costly warmup schedule tuning. We obviate RAdam by providing two simple ``rule-of-thumb'' warmup schedules for Adam, both of which require no tuning. Linear warmup of Adam's learning rate over $2 \\cdot (1 - \\beta_2)^{-1}$ iterations is functionally equivalent to RAdam across a wide range of settings. Hence, we suggest that practitioners considering the need for untuned warmup of Adam's learning rate first try linear warmup over $2 \\cdot (1 - \\beta_2)^{-1}$ training iterations.\n  \\newpage\n    \n  \\onecolumn\n  \\appendix\n  \n  \\newpage\n\\section{Full details of experimental setup}\n\\label{apx:training-details}\n\n\\subsection{System configuration}\n\nAll experiments are performed using Python 3.7 and PyTorch version 1.2~\\citep{paszke2017pytorch} compiled with CUDA 10, on Ubuntu 18.04 systems containing 8 NVIDIA V100 GPUs each.\n\n\\subsection{Image classification}\n\\label{apx:training-details-imagenet}\n\nExperimentation is performed using the ILSVRC 2012 1000-class dataset~\\citep[``ImageNet'';][]{russakovsky2015imagenet} and a 50-layer convolutional residual network model~\\citep[``ResNet-50'';][]{he2016resnet}. The implementation follows that of \\citet{paszke2016pytorchexamples},~\\footnote{Commit hash \\texttt{ee964a2}.} with the only deviations being to enable alternative optimizer configurations, to enable intermediate metric logging, and to drop the last batch from each training epoch.\n\nTraining occurs over 90 epochs, with ten-fold learning rate decay after epochs 30 and 60. The minibatch size is 1024. The optimization objective is cross-entropy, with a decoupled weight decay~\\citep{loshchilov2019adamw} of $10^{-4}$.\n\nData augmentation includes horizontal flipping at random, as well as random 224-pixel crops. Validation is performed on 224-pixel center crops.\n\nFor Adam and RAdam, the following hyperparameters are fixed: $\\beta_1 = 0.9$ and $\\eps = 10^{-8}$. All other Adam parameters (warmup schedule, learning rate $\\alpha$, and $\\beta_2$) are enumerated via parameter sweep as described in \\cref*{sec:results-imagenet}. Each Adam configuration is independently trained with 5 random seeds.\n\n\\subsection{Language modeling}\n\\label{apx:training-details-wiki103}\n\nWe evaluate the state-of-the-art, Transformer-based language model described in~\\cite{baevski2018adaptivelm} on the \\texttt{WIKITEXT-103} dataset, consisting of 100M tokens with a size-260K vocabulary. We leverage the author's implementation provided in fairseq~\\citep{gehring2017convs2s, ott2019fairseq}, and train on 8 GPUs with half-precision floating point.\n\nOur experimentation setup closely follows~\\cite{baevski2018adaptivelm}, except that we sweep over Adam parameters, such as warmup schedule, learning rate $\\alpha$, and $\\beta_2$, while keeping $\\beta_1=0.9$ and $\\eps=10^{-7}$ fixed (both for Adam and RAdam). The hyperparameter grid is presented in~\\cref*{sec:results-wiki103}. Each Adam configuration is independently trained with 3 random seeds.\n\n\\subsection{Machine translation}\n\\label{apx:training-details-wmt16-en-de}\n\nOur setup employs a state-of-the-art Transformer model~\\citep{vasmani2017transformer} implemented in fairseq~\\citep{ott2019fairseq}. We train on the WMT16 English-German large machine translation dataset, and evaluate on the \\textit{newstest14} validation set.\n\nAs observed in~\\cite{ma2019qh}, these state-of-the-art large-scale models are fragile to train with Adam and require either a carefully chosen optimization procedure, or robust optimizers that can sustain gradients with large variance, such as QHAdam~\\citep{ma2019qh}. To eliminate this factor from our studies, we choose to lower the learning rate $\\alpha$ to stabilize training, taking a marginal performance hit in training. Apart from that, our experimentation setup is identical to the one in~\\cite{ott2019fairseq}.\n\nWe fix Adam parameters $\\beta_1=0.9$ and $\\epsilon=10^{-7}$, and sweep over the warmup schedule, learning rate $\\alpha$, and $\\beta_2$, as described in~\\cref*{sec:results-wmt16-en-de}. We again use half-precision floating point and train on 8 GPUs. As~\\cite{ott2019fairseq} trains on 128 GPUs, we accumulate gradients over $16$ minibatches before each optimization step to achieve an identical configuration. The BLEU score is averaged over $3$ random seeds.\n\n\\subsection{Gradient analysis workhorse: EMNIST digit classification}\n\\label{apx:training-details-emnist}\n\nThe EMNIST digit classification task~\\citep{cohen2017emnist} serves as the workhorse for our gradient analysis studies. Our model is a simple feed-forward neural network with three hidden layers (sizes 200, 100, and 50) and uniform weight initialization with range inversely proportional to the square root of layer sizes.\n\nOptimization is performed on the cross-entropy objective with the Adam optimizer. The Adam configuration is $\\alpha = 10^{-3}$, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\eps = 10^{-8}$, and decoupled weight decay $10^{-4}$. The minibatch size is 256,\n\nTraining occurs over 10000 training iterations. At each training iteration, 256 backwards passes are performed with independently sampled batches to collect a sample of the gradient distribution. Due to the cost of storing and analyzing the gradients of all parameters, we randomly sample 500 parameters from each weight matrix and only collect gradients for the sampled parameters. These samples are used to approximate the distribution of the gradient coefficients of variation. After the 256 backwards passes, one final pass is performed as a regular optimization step to update the model parameters and proceed to the next iteration.  \\newpage\n\\section{Miscellaneous derivations}\n\\label{apx:misc-derivations}\n\nThis appendix provides miscellaneous informal derivations of statements in the main text.\n\n\\subsection{Number of RAdam momentum iterations}\n\\label{apx:misc-derivations-rhot-is-t4}\n\n\\rhotist*\n\n\\begin{proof}\n\nWe define $\\rho(t, \\beta_2)$ to be the continuous version of $\\rho_t$, parameterized over both $t$ and $\\beta_2$:\n\\begin{equation*}\n    \\rho(t, \\beta_2) = \\frac{2}{1 - \\beta_2} - 1 - \\frac{2 \\cdot t \\cdot \\beta_2^t}{1 - \\beta_2^t}\n\\end{equation*}\n\nWe then differentiate with respect to $t$:\n\\begin{align*}\n    \\frac{\\partial \\rho(t, \\beta_2)}{\\partial t} &= 2 \\cdot \\beta_2^t \\cdot \\frac{\\beta_2^t - 1 - t \\cdot \\ln \\beta_2}{\\left( 1 - \\beta_2^t \\right)^2}\n\\end{align*}\n$\\frac{\\partial \\rho(t, \\beta_2)}{\\partial t}$ is thus positive for all $t > 0$.\n\nWe also differentiate with respect to $\\beta_2$, and take specific values thereof:\n\\begin{align*}\n    \\frac{\\partial \\rho(t, \\beta_2)}{\\partial \\beta_2} &= 2 \\cdot \\left( \\frac{1}{(1 - \\beta_2)^2} - \\frac{t^2 \\cdot \\beta_2^{t - 1}}{(1 - \\beta_2^t)^2} \\right) \\\\\n    \\frac{\\partial \\rho(4, \\beta_2)}{\\partial \\beta_2} &= 2 \\cdot \\left( \\frac{1}{(1 - \\beta_2)^2} - \\frac{16 \\cdot \\beta^3}{(1 - \\beta_2^4)^2} \\right) \\\\\n    \\frac{\\partial \\rho(5, \\beta_2)}{\\partial \\beta_2} &= 2 \\cdot \\left( \\frac{1}{(1 - \\beta_2)^2} - \\frac{25 \\cdot \\beta_2^4}{(1 - \\beta_2^5)^2} \\right)\n\\end{align*}\n$\\frac{\\partial \\rho(t, \\beta_2)}{\\partial \\beta_2}$ is thus positive for all $\\beta_2 \\in (0, 1)$ at $t = 4$ and $t = 5$.\n\nThen, we take $\\lim\\limits_{\\beta_2 \\rightarrow 1} \\rho(4, \\beta_2)$:\n\\begin{align*}\n    \\lim\\limits_{\\beta_2 \\rightarrow 1} \\rho(4, \\beta_2) &= \\lim\\limits_{\\beta_2 \\rightarrow 1} \\left( \\frac{2}{1 - \\beta_2} - \\frac{8 \\cdot \\beta_2^4}{1 - \\beta_2^4} \\right) - 1 \\\\\n    &= \\lim\\limits_{\\beta_2 \\rightarrow 1} \\left( \\frac{2 \\cdot \\left( 4 \\cdot \\beta_2^3 + 3 \\cdot \\beta_2^2 + 2 \\cdot \\beta_2 + 1 \\right)}{(1 + \\beta_2) (1 + \\beta_2^2)} \\right) - 1 \\\\\n    &= 5 - 1 \\\\\n    &= 4\n\\end{align*}\n\nCombining this result with the fact that $\\frac{\\partial \\rho(4, \\beta_2)}{\\partial \\beta_2}$ is positive for $\\beta_2 \\in (0, 1)$, it follows that $\\rho(4, \\beta_2) < 4$ for all $\\beta_2 \\in (0, 1)$. Then, since $\\frac{\\partial \\rho(t, \\beta_2)}{\\partial t} > 0$ for all $t > 0$, we have that $\\rho(t, \\beta_2) < 4$ for all $\\beta_2 \\in (0, 1)$ and $t \\in (0, 4]$. We have thus shown that $t \\leq 4 \\implies \\rho_t \\leq 4$ for positive integers $t$.\n\nIn the reverse direction, we evaluate $\\rho(5, 0.8)$:\n\\begin{align*}\n    \\rho(5, 0.8) &= \\frac{2}{1 - 0.8} - 1 - \\frac{2 \\cdot 5 \\cdot {0.8}^5}{1 - {0.8}^5} \\\\\n    &\\approx 9 - 4.87 \\\\\n    &\\approx 4.14\n\\end{align*}\n\nSimilarly combining this result with the fact that $\\frac{\\partial \\rho(5, \\beta_2)}{\\partial \\beta_2}$ is positive for $\\beta_2 \\in (0, 1)$, then with the fact that $\\frac{\\partial \\rho(t, \\beta_2)}{\\partial t} > 0$ for all $t > 0$, we have that $\\rho(t, \\beta_2) \\gtrsim 4.14$ for all $t \\geq 5, \\beta_2 \\in [0.8, 1)$. We have thus shown that $t > 4 \\implies \\rho_t > 4$ for positive integers $t$, completing the proof.\n\n\\end{proof}\n\n\\subsection{Linear warmup period (rule-of-thumb)}\n\\label{apx:misc-derivations-linear-exp-equivalence}\n\nWe desire for the effective warmup period to be roughly equivalent between the exponential and linear rule-of-thumb schedules -- that is, $\\mathcal{T}(w^{\\text{expo}, \\text{untuned}}) \\approx \\mathcal{T}(w^{\\text{linear}, \\tau})$. Solving approximately for $\\tau$:\n\\begin{align*}\n    \\mathcal{T}(w^{\\text{expo}, \\text{untuned}}) &= \\sum_{t = 1}^\\infty \\exp \\left( - (1 - \\beta_2) \\cdot t \\right) \\\\\n    &= \\frac{1}{\\exp \\left( 1 - \\beta_2 \\right) - 1} \\\\\n    & \\approx (1 - \\beta_2)^{-1} \\\\\n    \\mathcal{T}(w^{\\text{linear}, \\tau}) &= \\sum_{t = 1}^{\\tau} \\left[ 1 - \\frac{1}{\\tau} \\cdot t \\right] = \\frac{\\tau - 1}{2} \\\\\n    &\\approx \\frac{\\tau}{2} \\\\\n    \\tau = 2 \\cdot \\left(1 - \\beta_2\\right)^{-1} &\\implies \\mathcal{T}(w^{\\text{expo}, \\text{untuned}}) \\approx \\mathcal{T}(w^{\\text{linear}, \\tau})\n\\end{align*}  \\newpage\n\\section{Supplementary experimental results}\n\\label{apx:supp-results}\n\n\\subsection{Image classification}\n\\label{apx:supp-results-imagenet}\n\n\\begin{figure}[!th]\n\\centering\n\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\n\\begin{subfigure}[b]{0.33\\linewidth}\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\\includegraphics[draft=false, width=1.0\\linewidth]{png/imagenet_train_loss_lr_0_0100_beta2_0_9900.png}\n\\caption{$\\alpha = {10}^{-2}$, $\\beta_2 = 0.99$}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.33\\linewidth}\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\\includegraphics[draft=false, width=1.0\\linewidth]{png/imagenet_train_loss_lr_0_0100_beta2_0_9970.png}\n\\caption{$\\alpha = {10}^{-2}$, $\\beta_2 = 0.997$}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.33\\linewidth}\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\\includegraphics[draft=false, width=1.0\\linewidth]{png/imagenet_train_loss_lr_0_0100_beta2_0_9990.png}\n\\caption{$\\alpha = {10}^{-2}$, $\\beta_2 = 0.999$}\n\\end{subfigure}\n\n\\begin{subfigure}[b]{0.33\\linewidth}\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\\includegraphics[draft=false, width=1.0\\linewidth]{png/imagenet_train_loss_lr_0_0010_beta2_0_9900.png}\n\\caption{$\\alpha = {10}^{-3}$, $\\beta_2 = 0.99$}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.33\\linewidth}\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\\includegraphics[draft=false, width=1.0\\linewidth]{png/imagenet_train_loss_lr_0_0010_beta2_0_9970.png}\n\\caption{$\\alpha = {10}^{-3}$, $\\beta_2 = 0.997$}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.33\\linewidth}\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\\includegraphics[draft=false, width=1.0\\linewidth]{png/imagenet_train_loss_lr_0_0010_beta2_0_9990.png}\n\\caption{$\\alpha = {10}^{-3}$, $\\beta_2 = 0.999$}\n\\end{subfigure}\n\n\\begin{subfigure}[b]{0.33\\linewidth}\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\\includegraphics[draft=false, width=1.0\\linewidth]{png/imagenet_train_loss_lr_0_0001_beta2_0_9900.png}\n\\caption{$\\alpha = {10}^{-4}$, $\\beta_2 = 0.99$}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.33\\linewidth}\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\\includegraphics[draft=false, width=1.0\\linewidth]{png/imagenet_train_loss_lr_0_0001_beta2_0_9970.png}\n\\caption{$\\alpha = {10}^{-4}$, $\\beta_2 = 0.997$}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.33\\linewidth}\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\\includegraphics[draft=false, width=1.0\\linewidth]{png/imagenet_train_loss_lr_0_0001_beta2_0_9990.png}\n\\caption{$\\alpha = {10}^{-4}$, $\\beta_2 = 0.999$}\n\\end{subfigure}\n\n\\caption{Mean training loss of ResNet-50 on ImageNet under various configurations of Adam (5 random seeds per configuration). Standard deviations are negligible across configurations.}\n  \n\\label{fig:imagenet-train-loss-full}\\end{figure}\n\\begin{figure}[!th]\n\\centering\n\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\n\\begin{subfigure}[b]{0.33\\linewidth}\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\\includegraphics[draft=false, width=1.0\\linewidth]{png/imagenet_valid_error_lr_0_0100_beta2_0_9900.png}\n\\caption{$\\alpha = {10}^{-2}$, $\\beta_2 = 0.99$}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.33\\linewidth}\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\\includegraphics[draft=false, width=1.0\\linewidth]{png/imagenet_valid_error_lr_0_0100_beta2_0_9970.png}\n\\caption{$\\alpha = {10}^{-2}$, $\\beta_2 = 0.997$}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.33\\linewidth}\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\\includegraphics[draft=false, width=1.0\\linewidth]{png/imagenet_valid_error_lr_0_0100_beta2_0_9990.png}\n\\caption{$\\alpha = {10}^{-2}$, $\\beta_2 = 0.999$}\n\\end{subfigure}\n\n\\begin{subfigure}[b]{0.33\\linewidth}\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\\includegraphics[draft=false, width=1.0\\linewidth]{png/imagenet_valid_error_lr_0_0010_beta2_0_9900.png}\n\\caption{$\\alpha = {10}^{-3}$, $\\beta_2 = 0.99$}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.33\\linewidth}\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\\includegraphics[draft=false, width=1.0\\linewidth]{png/imagenet_valid_error_lr_0_0010_beta2_0_9970.png}\n\\caption{$\\alpha = {10}^{-3}$, $\\beta_2 = 0.997$}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.33\\linewidth}\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\\includegraphics[draft=false, width=1.0\\linewidth]{png/imagenet_valid_error_lr_0_0010_beta2_0_9990.png}\n\\caption{$\\alpha = {10}^{-3}$, $\\beta_2 = 0.999$}\n\\end{subfigure}\n\n\\begin{subfigure}[b]{0.33\\linewidth}\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\\includegraphics[draft=false, width=1.0\\linewidth]{png/imagenet_valid_error_lr_0_0001_beta2_0_9900.png}\n\\caption{$\\alpha = {10}^{-4}$, $\\beta_2 = 0.99$}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.33\\linewidth}\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\\includegraphics[draft=false, width=1.0\\linewidth]{png/imagenet_valid_error_lr_0_0001_beta2_0_9970.png}\n\\caption{$\\alpha = {10}^{-4}$, $\\beta_2 = 0.997$}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.33\\linewidth}\n\\captionsetup{\n  justification=centering,\n  margin=0.1in,\n  size=normalsize\n}\n\\includegraphics[draft=false, width=1.0\\linewidth]{png/imagenet_valid_error_lr_0_0001_beta2_0_9990.png}\n\\caption{$\\alpha = {10}^{-4}$, $\\beta_2 = 0.999$}\n\\end{subfigure}\n\n\\caption{Mean top-1 validation error of ResNet-50 on ImageNet under various configurations of Adam (5 random seeds per configuration). Standard deviations are negligible across configurations.}\n  \n\\label{fig:imagenet-valid-error-full}\\end{figure}}\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{DeLighT: Deep and Light-weight Transformer}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nWe introduce a deep and light-weight transformer, DeLighT , that delivers similar or better performance than standard transformer-based models with significantly fewer parameters. DeLighT ~more efficiently allocates parameters both (1) within each Transformer block using the DeLighT transformation , a deep and light-weight transformation and (2) across blocks using block-wise scaling, that allows for shallower and narrower DeLighT ~blocks near the input and wider and deeper DeLighT ~blocks near the output. Overall, DeLighT ~networks are 2.5 to 4 times deeper than standard transformer models and yet have fewer parameters and operations. Experiments on benchmark machine translation and language modeling tasks show that DeLighT ~matches or improves the performance of baseline Transformers with 2 to 3 times fewer parameters on average. \n\\end{abstract}\n\n\\section{Introduction}\nAttention-based transformer networks~\\citep{vaswani2017attention} are widely used for sequence modeling tasks, including language modeling and machine translation. To improve performance, models are often scaled to be either wider, by increasing the dimension of hidden layers, or deeper, by stacking more transformer blocks. For example, T5 \\citep{raffel2019exploring} uses a dimension of 65K and GPT-3 \\citep{brown2020language} uses 96 transformer blocks. However, such scaling increases the number of network parameters significantly (e.g., T5 and GPT-3 have 11 billion and 175 billion parameters, respectively), and complicates learning, i.e., these models either require very large training corpora \\citep{raffel2019exploring,devlin2018bert,brown2020language} or careful regularization \\citep{hinton2012improving,wan2013regularization,merity2018regularizing}. In this paper, we introduce a new parameter-efficient attention-based architecture that can be easily scaled to be both wide and deep. \n\nOur \\texttt{De}ep and \\texttt{Ligh}t-weight \\texttt{T}ransformer architecture, DeLighT , extends the transformer architecture of \\citet{vaswani2017attention} and delivers similar or better performance with significantly fewer parameters and operations. At the heart of DeLighT ~is the DeLighT transformation ~that uses the group linear transformations (GLTs) of \\citet{mehta2018pyramidal} with an expand-reduce strategy for varying the width and depth of the DeLighT ~block efficiently. Since GLTs are local by nature, the DeLighT transformation ~uses feature shuffling, which is analogous to channel shuffling in convolutional networks \\citep{zhang2018shufflenet}, to share information between different groups. Such wide and deep representations facilitate replacing the multi-head attention and feed-forward layers in transformers with single headed attention and light-weight feed-forward layers, reducing total network parameters and operations. Importantly, unlike transformers, the DeLighT transformation ~decouples the depth and width from the input size, allowing us to allocate parameters more efficiently across blocks by using shallower and narrower DeLighT ~blocks near the input and deeper and wider DeLighT ~blocks near the output.\n\nWe demonstrate that DeLighT ~models achieve similar or better performance than transformer models with significantly fewer parameters and operations, on two common sequence modeling tasks, (i) machine translation and (ii)  language modeling. On the low resource WMT'16 En-Ro machine translation dataset, DeLighT ~attains transformer performance using $2.8\\times$ fewer parameters. On the high resource WMT'14 En-Fr dataset, DeLighT ~delivers better performance (+0.4 BLEU score) with $1.8\\times$ fewer parameters than baseline transformers. Similarly, on language modeling, DeLighT ~matches the performance of  Transformer-XL~\\citep{dai2019transformer} with $1.5\\times$ fewer parameters on the WikiText-103 dataset. Our source code is open-source and is available at: \\textcolor{blue}{\\url{https://github.com/sacmehta/delight}}\n\n\\section{Related Work}\n\\label{sec:related_work}\n\n\\noindent{\\bf Improving transformers:} Several methods have been introduced to improve the transformer architecture. The first line of research addresses the challenge of computing self attention on long input sequences \\citep{child2019generating,Kitaev2020Reformer,Beltagy2020Longformer}. These methods can be combined with our architecture. The second line of research focuses on explaining multi-head attention \\citep{raganato2018analysis,Brunner2020On}. They show that increasing the number of transformer heads can lead to redundant representations \\citep{voita2019bottom,michel2019sixteen} and using fixed attention heads with predefined patterns \\citep{raganato2020fixed} or synthetic attention matrices \\citep{tay2020synthesizer} improves performance. \n%These results support our design choice of using single-head attention. \nThe third line of research focuses on improving transformers by learning better representations \\citep{wu2018pay,Wu2020Lite,so2019evolved}. These works aim to improve the expressiveness of transformers using different transformations -- for example, using convolutions \\citep{wu2018pay, gehring2017convolutional}, gated linear units \\citep{dauphin2017language}, or multi-branch feature extractors \\citep{so2019evolved,Wu2020Lite}. Our work falls into this category. Unlike previous works, we show that it is possible to efficiently allocate parameters both at the block-level using the DeLighT transformation ~and across blocks using block-wise scaling.\n\n\\noindent{\\bf Model scaling:} Model scaling is a standard method to improve the performance of sequence models \\citep{vaswani2017attention,raffel2019exploring,lan2020ALBERT,devlin2018bert,shoeybi2019megatron,tan2019efficientnet,brown2020language}. Model dimensions are increased in width-wise scaling \\citep{vaswani2017attention,devlin2018bert} while more blocks (e.g., Transformer blocks) are stacked in depth-wise scaling \\citep{shoeybi2019megatron,brown2020language,wang2019learning}. In both cases (and their combination), parameters inside each block of the network are the same, which may lead to a sub-optimal solution. To further improve the performance of sequence models, this paper introduces \\textit{block-wise scaling} that allows for variably-sized blocks and efficient allocation of parameters in the network. Our results show that (1) shallower and narrower DeLighT ~blocks near the input and deeper and wider DeLighT ~blocks near the output deliver the best performance, and (2) models with block-wise scaling coupled with model scaling achieve better performance compared to model scaling alone. We note that convolutional neural networks (CNNs) also learn shallower and narrower representations near the input and deeper and wider representations near the output. Unlike CNNs (e.g., ResNet of \\citealt{he2016deep}) that perform a fixed number of operations at each convolutional layer, the proposed block-wise scaling uses a variable number of operations in each layer and block. \n\n\\noindent{\\bf Improving sequence models:} There is also significant recent work on other related methods for improving sequence models, including (1) improving accuracy using better token-level representations -- for example, using BPE \\citep{sennrich2015neural}, adaptive inputs \\citep{baevski2018adaptive} and outputs \\citep{grave2017efficient}, and DeFINE \\citep{mehta2020DeFINE}, and (2) improving efficiency -- for example, using compression \\citep{chen2018groupreduce,sun2020mobilebert}, pruning \\citep{han2015deep,voita2019analyzing}, and distillation \\citep{hinton2015distilling,sanh2019distilbert}. The closest to our work is the DeFINE transformation, which also learns representations using an expand-reduce strategy. The key difference between the DeFINE transformation (Figure \\ref{fig:hgt}) and the DeLighT transformation ~(Figure \\ref{fig:ihgt}) is that the DeLighT transformation ~more efficiently allocates parameters within expansion and reduction layers. Unlike DeFINE, which uses fewer groups in group linear transformations to learn wider representations, DeLighT transformation ~uses more groups to learn wider representations with fewer parameters. The DeLighT transformation ~achieves comparable performance to the DeFINE transformation but with significantly fewer parameters.\n\n\\section{DeLighT: Deep and Light-weight Transformer}\n\\label{sec:arcitecture}\n\nA standard transformer block (Figure \\ref{fig:transformer_sa}) comprises of multi-head attention that uses a query-key-value decomposition to model relationships between sequence tokens, and a feed forward network (FFN) to learn wider representations. Multi-head attention obtains query $\\mathbf{Q}$, key $\\mathbf{K}$, and value $\\mathbf{V}$ by applying three projections to the input, each consisting of $h$ linear layers (or heads) that map the $d_m$-dimensional input into a $d_h$-dimensional space, where $d_h=d_m/h$ is the head dimension. The FFN consists of two linear layers, where the first expands the dimensions from $d_m$ to $d_f$ and the second reduces the dimensions from $d_f$ to $d_m$. The depth of a transformer block is 4, consisting of (1) three parallel branches for queries, keys, and values, (2) a fusion layer that combines the output of multiple heads, and (3) two sequential linear layers in the FFN. In general, transformer-based networks sequentially stacks transformer blocks to increase network capacity and depth.\n\nThis paper extends the transformer architecture and introduces a deep and light-weight transformer, DeLighT . Our model uses a deep and light-weight expand-reduce transformation, DeLighT transformation ~(Section \\ref{ssec:dextra}), that enables learning wider representations efficiently. It also enables replacing multi-head attention and feed forward network (FFN) layers with single-head attention and a light-weight FFN (Section \\ref{ssec:dextra_transformer}). DeLighT transformation ~decouples attention dimensions from the depth and width, allowing us to learn representations efficiently using block-wise scaling instead of uniform stacking of transformer blocks (Section \\ref{ssec:layer_wise_scaling}).\n%\n\\begin{figure}[t!]\n    \\centering\n    \\begin{subfigure}[b]{0.35\\columnwidth}\n        \\centering\n        \\resizebox{!}{160px}{\n            \\input{tikz/self_attention_units.tikz}\\transformer\n        }\n        \\caption{Transformer block}\n        \\label{fig:transformer_sa}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[b]{0.33\\columnwidth}\n        \\centering\n        \\resizebox{!}{165px}{\n            \\input{tikz/self_attention_units.tikz}\\redefine\n        }\n        \\caption{DeLighT ~ block}\n        \\label{fig:redefine_transformer_sa}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[b]{0.3\\columnwidth}\n        \\centering\n        \\begin{subfigure}[b]{\\columnwidth}\n            \\centering\n            \\resizebox{!}{70px}{\n                \\input{tikz/dextra_new}\\define\n            }\n            \\caption{DeFINE transformation}\n            \\label{fig:hgt}\n            \\end{subfigure}\n            \\vfill\n            \\begin{subfigure}[b]{\\columnwidth}\n            \\centering\n            \\resizebox{!}{70px}{\n                \\input{tikz/dextra_new}\\invHGTShuffle\n            }\n            \\caption{DeLighT transformation }\n            \\label{fig:ihgt}\n        \\end{subfigure}\n    \\end{subfigure}\n    \\caption{\\textbf{(a, b)} Block-wise comparison between the standard transformer block of \\citet{vaswani2017attention} and the DeLighT ~block. In the DeLighT transformation , the number of operations in computing attention are reduced by half while the number of parameters (and operations) in the FFN are reduced by $16\\times$. Transformations with learnable parameters (\\colorbox{cyan!40}{Linear} and \\colorbox{green!20}{DeLighT }) are shown in color. The shape of linear transformations indicate their operation (expansion, reduction, etc.). \\textbf{(c, d)} compares the DeFINE transformation \\citep{mehta2020DeFINE} with the DeLighT transformation . Compared to the DeFINE transformation, the DeLighT transformation ~uses group linear transformations (GLTs) with more groups to learn wider representations with fewer parameters. Different colors are used to show groups in GLTs. For simplicity, feature shuffling is not shown in (d).}\n    \\label{fig:compare_tam_ram_sa}\n\\end{figure}\n%%\n%\n\\subsection{DeLighT Transformation}\n\\label{ssec:dextra}\nDeLighT transformation ~maps a  $d_m$ dimensional input vector into a high dimensional space (expansion) and then reduces it down to a  $d_o$ dimensional output vector (reduction) using  $N$ layers of the group transformations of \\citet{mehta2018pyramidal}, as shown in Figure \\ref{fig:ihgt}. During these expansion and reduction phases, DeLighT transformation ~uses group linear transformations (GLTs) because they learn local representations by deriving the output from a specific part of the input and are more efficient than linear transformations. To learn global representations, the DeLighT transformation ~shares information between different groups in the group linear transformation using feature shuffling, analogous to channel shuffling in convolutional networks \\citep{zhang2018shufflenet}. \n\nA standard approach to increase the expressivity and capacity of transformers is to increase the input dimensions, $d_m$. However, increasing $d_m$ linearly also increases the number of operations in multi-head attention ($\\mathcal{O}(n^2 d_m)$, where $n$ is the sequence length) in a standard transformer block (Figure \\ref{fig:transformer_sa}). In contrast, to increase the expressivity and capacity of the DeLighT ~block, we increase the depth and width of its intermediate DeLighT ~ transformations using expansion and reduction phases. This enables us to use smaller dimensions for computing attention, requiring fewer operations. \n\nFormally, the DeLighT transformation ~is controlled by five configuration parameters: (1) number of GLT layers $N$, (2) width multiplier $w_m$, (3) input dimension $d_m$, (4) output dimension $d_o$, and (5) maximum groups $g_{max}$ in a GLT. In the expansion phase, the DeLighT transformation ~projects the $d_m$-dimensional input to a high-dimensional space, $d_{max}=w_m d_m$, linearly using $\\lceil \\frac{N}{2} \\rceil$ layers. In the reduction phase, the DeLighT transformation ~projects the $d_{max}$-dimensional vector to a $d_o$-dimensional space using the remaining $N - \\lceil \\frac{N}{2} \\rceil$ GLT layers. Mathematically, we  define the output $\\mathbf{Y}$ at each GLT layer $l$ as:\n\\begin{equation}\n    \\mathbf{Y}^l = \\left\\{\n    \\begin{array}{ll}\n        \\mathcal{F} \\left(\\mathbf{X}, \\mathbf{W}^l, \\mathbf{b}^l, g^l\\right), & l = 1 \\\\\n        \\mathcal{F} \\left( \\mathcal{H}\\left(\\mathbf{X}, \\mathbf{Y}^{l-1}\\right), \\mathbf{W}^l,  \\mathbf{b}^l, g^l\\right), &  \\text{Otherwise}\n    \\end{array}\n    \\right.\n    \\label{eq:glt}\n\\end{equation}\nwhere $\\mathbf{W}^l = \\left\\{ \\mathbf{W}^l_1, \\cdots, \\mathbf{W}^l_{g^l}\\right\\}$ and $\\mathbf{b}^l = \\left\\{ \\mathbf{b}^l_1, \\cdots, \\mathbf{b}^l_{g^l}\\right\\}$ are the learnable weights and biases of group linear transformation $\\mathcal{F}$ with $g^l$ groups at the $l$-th layer. Briefly, the $\\mathcal{F}$ function takes the input $\\mathbf{X}$ $\\left(\\text{or } \\mathcal{H}\\left(\\mathbf{X}, \\mathbf{Y}^{l-1}\\right)\\right)$ and splits into $g^l$ non-overlapping groups such that $\\mathbf{X} = \\left\\{\\mathbf{X}_1, \\cdots, \\mathbf{X}_{g^l} \\right\\}$. The function $\\mathcal{F}$ then linearly transforms each $\\mathbf{X}_i$ with weights $\\mathbf{W}^l_i$ and bias $\\mathbf{b}^l_i$ to produce output $\\mathbf{Y}^l_i = \\mathbf{X}_i \\mathbf{W}_i^l + \\mathbf{b}_i^l$. The outputs of each group $\\mathbf{Y}^l_i$ are then concatenated to produce the output $\\mathbf{Y}^l$. The function $\\mathcal{H}$ first shuffles the output of each group in $\\mathbf{Y}^{l-1}$ and then combines it with the input $\\mathbf{X}$ using the input mixer connection of \\citet{mehta2020DeFINE} to avoid vanishing gradient problems. Figure \\ref{fig:delight_layer_vis} visualizes the expansion phase in the DeLighT ~transformation with group linear transformation, feature shuffling, and the input mixer connection.\n\nThe number of groups at the $l$-th GLT in DeLighT transformation ~are computed as:\n\\begin{equation}\n    g^l = \\left\\{\n    \\begin{array}{lr}\n       \\text{min}(2^{l-1}, g_{max}), & 1 \\leq l \\leq \\ceil*{N/2} \\\\\n       g^{N-l}, & \\text{Otherwise}\n    \\end{array}\n    \\right.\n    \\label{eq:group}\n\\end{equation}\nIn our experiments, we use $g_{max} = \\lceil \\frac{d_m}{32} \\rceil$ so that each group has at least 32 input elements. \n\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[height=110px]{glt_images/glt_shuffle_inp_hr.pdf}\n    \\caption{Example illustrating the expansion phase in the DeLighT ~transformation that uses GLTs, feature shuffling, and an input mixer connection, to learn deeper and wider representations efficiently. For illustrative purposes, we have used the same input and output dimensions.}\n    \\label{fig:delight_layer_vis}\n\\end{figure}\n\n\\subsection{DeLighT block}  \n\\label{ssec:dextra_transformer} \nFigure \\ref{fig:redefine_transformer_sa} shows how we integrate DeLighT transformation ~into the transformer block to improve its efficiency. The $d_m$-dimensional inputs are first fed to the DeLighT transformation ~to  produce $d_o$-dimensional outputs, where $d_o < d_m$. These $d_o$-dimensional outputs are then fed into a single head attention, followed by a light-weight FFN to model their relationships. \n\n\\noindent{\\bf DeLighT ~layer and single head attention:} Let us assume we have a sequence of $n$ input tokens, each of dimensionality $d_m$. These $n$, $d_m$-dimensional inputs are first fed to the DeLighT transformation ~to produce $n$, $d_o$-dimensional outputs, where $d_o < d_m$.  These $n$, $d_o$-dimensional outputs are then projected simultaneously using three linear layers to produce $d_o$-dimensional queries $\\mathbf{Q}$, keys $\\mathbf{K}$, and  values $\\mathbf{V}$. We then model contextual relationships between these $n$ tokens using scaled dot-product attention (Eq. \\ref{eq:espda}). To enable the use of residual connections \\citep{he2016deep}, the $d_o$-dimensional outputs of this attention operation are  linearly projected into a $d_m$-dimensional space.\n\\begin{equation}\n    \\text{Attention}(\\mathbf{K}, \\mathbf{Q}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_o}}\\right) \\mathbf{V}\n    \\label{eq:espda}\n\\end{equation}\nWe hypothesize that the ability of DeLighT ~to learn wider representations allows us to replace multi-head attention with single-head attention. The computational costs for computing attention in the standard transformer and the DeLighT ~block are $\\mathcal{O}(d_m n^2)$ and $\\mathcal{O}(d_o n^2)$ respectively, where $d_o < d_m$. Therefore, the DeLighT ~block reduces the cost for computing attention by a factor of $d_m/d_o$. In our experiments, we used $d_o=d_m/2$, thus requiring $2\\times$ fewer multiplication-addition operations as compared to the transformer architecture. \n\n\\noindent {\\textbf{Light-weight FFN:}} Similar to FFNs in transformers, this block also consists of two linear layers. Since the DeLighT ~block has already incorporated wider representations using the DeLighT transformation , it allows us to invert the functionality of FFN layers in the transformer. The first layer reduces the dimensionality of the input from $d_m$ to $d_m/r$ while the second layer expands the dimensionality from $d_m/r$ to $d_m$, where $r$ is the reduction factor  (see Figure \\ref{fig:redefine_transformer_sa}). Our light-weight FFN reduces the number of parameters and operations in the FFN by a factor of $r d_f/d_m$. In the standard transformer, the FFN dimensions are expanded by a factor of $4$.\\footnote{Transformer-base uses $d_m$=512 and $d_f$=2048 while Transformer-large uses $d_m$=1024 and $d_f$=4096.} In our experiments, we used $r=4$. Thus, the light-weight FFN reduces the number of parameters in the FFN by $16\\times$. \n\n\\noindent {\\textbf{Block depth:}} The DeLighT ~block stacks (1) a DeLighT transformation ~with $N$ GLTs, (2) three parallel linear layers for key, query, and value, (3) a projection layer, and  (4) two linear layers of a light-weight FFN. Thus, the depth of DeLighT ~block is $N + 4$. Compared to the standard transformer block (depth is 4), DeLighT ~block is deeper.\n\n\\subsection{Block-wise scaling}\n\\label{ssec:layer_wise_scaling}\nStandard methods for improving the performance of sequence models include increasing the model dimensions (width scaling), stacking more blocks (depth scaling), or both. However, such scaling is not very effective on small datasets. For example, when a Transformer-Base ($d_m=512$)  network is replaced with Transformer-Large ($d_m=1024$) on the WMT'16 En-Ro corpus, the number of parameters increases by approximately $4\\times$ while the performance does not change appreciably (BLEU: 34.28 vs. 34.35). We hypothesize that this happens because scaling model width and depth allocates parameters uniformly across blocks, which may lead to learning redundant parameters. To create deep and wide networks, we extend model scaling to the block level (see Figure \\ref{fig:fixed_vs_layer_wise}).\n\\begin{figure}[t!]\n    \\centering\n    \\begin{subfigure}[b]{0.28\\columnwidth}\n        \\centering\n        \\resizebox{!}{80px}{\n            \\input{tikz/scaling_new.tikz}\\scaling\n        }\n        \\caption{Uniform vs. block-wise}\n        \\label{fig:com_tam_ram}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[b]{0.7\\columnwidth}\n        \\centering\n        \\begin{tabular}{cc}\n            \\includegraphics[height=80px]{images/redef_params_uni_layer.pdf} &  \\includegraphics[height=80px]{images/redef_ops_uni_layer.pdf}\n        \\end{tabular}\n        \\caption{Distribution of parameters and operations within each block}\n    \\end{subfigure}\n    \\caption{\\textbf{Block-wise scaling} efficiently allocates parameters and operations across blocks, leading to shallower and narrower DeLighT ~blocks near the input and deeper and wider DeLighT ~blocks near the output. In (b), DeLighT ~networks with both uniform ($N$=$N_{min}$=$N_{max}$=8) and block-wise ($N_{min}$=4, $N_{max}$=8) scaling have about 16.7 M parameters and perform 3.5 B operations (computed for a sequence length of $n=30$), however, the DeLighT ~network with block-wise scaling delivered 2 points better perplexity.} \n    \\label{fig:fixed_vs_layer_wise}\n\\end{figure}\n%%\n\n\\noindent{\\bf Scaling the DeLighT block:} The DeLighT ~block learns deep and wide representations using the DeLighT transformation , whose depth and width are controlled by two configuration parameters: the number of GLT layers $N$ and the width multiplier $w_m$, respectively (Figure \\ref{fig:com_tam_ram}). These configuration parameters allow us to increase the number of learnable parameters inside the DeLighT ~block independently of the input $d_m$ and output $d_o$ dimensions. Such calibration is not possible with the standard transformer block because their expressiveness and capacity are a function of the input (input dimension = number of heads $\\times$ head dimension). Here, we introduce block-wise scaling that creates a network with variably-sized DeLighT ~blocks, allocating shallower and narrower DeLighT ~blocks near the input and deeper and wider DeLighT ~blocks near the output.\n\nTo do so, we introduce two network-wide configuration parameters: minimum $N_{min}$ and maximum $N_{max}$ number of GLTs in a DeLighT transformation . For the $b$-th DeLighT ~block, we  compute the number of GLTs $N^b$ and the width multiplier $w_m^b$ in a DeLighT transformation ~using linear scaling (Eq. \\ref{eq:mw}). With this scaling, each DeLighT ~block has a different depth and width (Figure \\ref{fig:com_tam_ram}). \n\\begin{equation}\n       N^{b} = N_{min} + \\frac{(N_{max} - N_{min})\\ b}{\\mathcal{B}-1}, \\quad w_m^{b} =  w_m + \\frac{(N_{max} - N_{min})\\ b}{N_{min}(\\mathcal{B}-1)}, \\quad 0 \\le b \\le \\mathcal{B}-1\n    \\label{eq:mw}\n\\end{equation}\nHere, $\\mathcal{B}$ denotes the number of DeLighT ~blocks in the network. We add superscript $b$ to number of GLT layers $N$ and width multiplier $w_m$ to indicate that these parameters are for the $b$-th block. \n\n\\noindent{\\bf Network depth:} The depth of transformer block is fixed, i.e., 4. Therefore, previous works \\citep{raffel2019exploring,brown2020language,wang2019learning} have associated the depth of transformer-based networks with the number of transformer blocks. In DeLighT , we present a different perspective to learn deeper representations, wherein each block is variably-sized. To compute the network depth, we use the standard definition across different domains, including computer vision (e.g., ResNet of \\citealt{he2016deep}) and theoretical machine learning \\citep{telgarsky2016benefits}. These works measures network depth as the number of sequential learnable layers (e.g., convolution, linear, or group linear). Similarly, the depth of DeLighT ~and transformer networks with $\\mathcal{B}$ blocks is $\\sum_{b=0}^{\\mathcal{B}-1} (N^b + 4)$ and $4\\mathcal{B}$, respectively.\n\n\\section{Experimental results}\n\\label{sec:results}\nWe evaluate the performance of DeLighT ~on two standard sequence modeling tasks: (1) machine translation (Section \\ref{ssec:machine_translation}) and (2) language modeling (Section \\ref{ssec:language_mdoeling}).\n\n\\subsection{Machine Translation}\n\\label{ssec:machine_translation}\n\\noindent {\\bf Datasets and evaluation:} We benchmark DeLighT ~models on four datasets: (1) IWSLT'14 German-English (De-En), (2) WMT'16 English-Romanian (En-Ro), (3) WMT'14 English-German (WMT'14 En-De), and (4) WMT'14 English-French (WMT'14 En-Fr). For the IWSLT'14 De-En dataset, we replicate the setup of \\citet{wu2018pay} and \\citet{edunov2018classical}, which uses 160K/7K/7K sentence pairs for training, validation, and testing with a joint BPE vocabulary of about 10K tokens, respectively. For the WMT'14 English-German (En-De) dataset, we follow the setup of \\citet{vaswani2017attention}. The dataset has 3.9M/39K/3K sentence pairs for training, validation, and testing respectively with a joint BPE vocabulary size of 44K.\\footnote{We use training and validation data that is compatible with the Tensor2Tensor library \\citep{tensor2tensor} in order to have fair comparisons with recent works (e.g., Evolved Transformer).} For the WMT'14 English-French (En-Fr) dataset, we replicate the setup of \\citet{gehring2017convolutional}, which uses 36M/27K/3K sentence pairs for training, validation, and testing respectively with a joint BPE vocabulary size of 44K. The performance is evaluated in terms of \\textit{BLEU} \\citep{papineni2002bleu} (higher is better) on the test set. We follow \\citet{wu2018pay} for beam search related hyper-parameters.\n\n\\noindent {\\bf Architecture:} We follow the symmetric encoder-decoder architecture of \\citet{vaswani2017attention} with sinusoidal positional encodings. Both the encoder and the decoder have $\\mathcal{B}$ DeLighT ~blocks. Decoder blocks are identical to the encoder blocks (Figure \\ref{fig:redefine_transformer_sa}), except that they have an additional source-target single-head attention unit before the light-weight FFN. In the source-target single-head attention unit, keys and values are projections over the encoder output (full details in Appendix \\ref{sec:appendix_enc_dec_arch}).  In our experiments, we use $w_m=2$, $N_{min}=4$, and $N_{max}=8$ for WMT'16 En-Ro, WMT'14 En-De, and WMT'14 En-Fr; resulting in 222 layer deep DeLighT ~networks. For IWSLT'14 De-En, we used $w_m=1$, $N_{min}=3$, and $N_{max}=9$ for IWSLT'14 De-En; resulting in 289 layer deep network. For simplicity, we set $\\mathcal{B}=N_{max}$. We use a learnable look-up table that maps every token in the vocabulary to a 128-dimensional vector. We implement our models using Fairseq \\citep{ott2019fairseq} and use their provided scripts for data pre-processing, training, and evaluation.\n\n\\noindent {\\bf Training:} For IWSLT'14 De-En models, we follow the setup of \\cite{wu2018pay} and train all our models for 50K iterations with a batch size of 4K tokens on a single NVIDIA GTX 1080 GPU. For WMT'16 En-Ro, we follow the training setup of \\cite{ghazvininejad2019mask} and train models for 100K iterations on 16 NVIDIA Tesla V100 GPUs with an effective batch size of 64K tokens. For WMT'14 En-De and WMT'14 En-Fr, we follow the training set-up of \\cite{wu2018pay} and train our models on 16 V100 GPUs for 30K and 50K iterations, respectively. We use Adam \\citep{kingma2014adam} to minimize cross entropy loss with a label smoothing value of 0.1 during training. For a fair comparison, we trained baseline transformer models using the same training set-up.\n\n\n\\subsubsection{Results}\n\n\\noindent {\\bf Comparison with baseline transformers:} Table \\ref{tab:compare_delight_trans} compares the performance of DeLighT ~with the baseline transformers of \\citet{vaswani2017attention} on different corpora. DeLighT ~delivers better performance with fewer parameters than transformers, across different corpora. Specifically, on low-resource (WMT'16 En-Ro) and high resource (WMT'14 En-De \\& WMT'14 En-Fr) corpora, DeLighT ~delivers similar or better performance with $2.8\\times$ and $1.8\\times$ fewer parameters, respectively. When the number of parameters are increased, DeLighT ~outperforms transformers. For example, on WMT'14 En-Fr dataset, DeLighT ~is $3.7\\times$ deeper than transformers and improves its BLEU score by 1.3 points yet with 13 million fewer parameters and 3 billion fewer operations (see Table \\ref{tab:results_depth_macs}).\n\nParticularly interesting are the performance comparisons of DeLighT ~with the baseline transformers of \\citet{vaswani2017attention} and its neural search variant, i.e., Evolved Transformer of \\citet{so2019evolved}, at two different parametric settings on WMT'14 En-De corpora in Figure \\ref{fig:perf_compare_param}. For small models (< 10 M parameters), DeLighT ~models delivers better performance and for attaining the same performance as these models, DeLighT ~models requires fewer parameters. \n\n\\begin{table}[t!]\n    \\centering\n    \\begin{subtable}[b]{\\columnwidth}\n        \\centering\n    \\resizebox{0.88\\columnwidth}{!}{\n        \\begin{tabular}{lcc||cc c cc||cc}\n            \\toprule[1.5pt]\n            \\multicolumn{1}{c}{} &  \\multicolumn{4}{c}{\\textbf{IWSLT'14 De-En}} & \\hfill & \\multicolumn{4}{c}{\\textbf{WMT'16 En-Ro}} \\\\\n            \\cmidrule[1.25pt]{2-5} \\cmidrule[1.25pt]{7-10}\n            \\textbf{Model} & \\textbf{\\# Params} & \\textbf{Ratio} & \\textbf{BLEU} & $\\Delta$ \\textbf{BLEU} &\\hfill & \\textbf{\\# Params} & \\textbf{Ratio} & \\textbf{BLEU} & $\\Delta$ \\textbf{BLEU} \\\\\n            \\midrule\n            Transformer \\citep{vaswani2017attention} & -- & -- & 34.4$^\\dagger$ & -- && 62 M & -- & 34.3$^\\ddagger$ & -- \\\\\n            \\midrule\n            Transformer (Our impl.) & 42 M & $1.0\\times$ & 34.3 & -- && 62 M & $1.0\\times$ & 34.3 & -- \\\\\n            DeLighT                    & 14 M & $0.3\\times$ & 33.8 & -0.5 && 22 M & $0.35\\times$ & 34.3 & 0.0 \\\\\n            DeLighT                    & 30 M & $0.7\\times$ & \\textbf{35.3} & \\textbf{+1.0} && 53 M & $0.85\\times$ & \\textbf{34.7} & \\textbf{+0.4} \\\\\n            \\bottomrule\n        \\end{tabular}\n    }\n    \\caption{Results on small corpora}\n    \\end{subtable}\n    \\vfill\n    \\begin{subtable}[b]{\\columnwidth}\n        \\centering\n    \\resizebox{0.88\\columnwidth}{!}{\n        \\begin{tabular}{lcc||cc p{0.25cm} cc||cc}\n            \\toprule[1.5pt]\n            \\multicolumn{1}{c}{} &  \\multicolumn{4}{c}{\\textbf{WMT'14 En-De}} & \\hfill & \\multicolumn{4}{c}{\\textbf{WMT'14 En-Fr}} \\\\\n            \\cmidrule[1.25pt]{2-5} \\cmidrule[1.25pt]{7-10}\n            \\textbf{Model} & \\textbf{\\# Params} & \\textbf{Ratio} & \\textbf{BLEU} & $\\Delta$ \\textbf{BLEU} &\\hfill & \\textbf{\\# Params} & \\textbf{Ratio} & \\textbf{BLEU} & $\\Delta$ \\textbf{BLEU} \\\\\n            \\midrule\n            Transformer \\citep{vaswani2017attention} & 62 M & -- & 27.3 & -- && -- & 62 M & 38.1 & -- \\\\\n            \\midrule\n            Transformer (Our impl.) & 67 M & $1.0\\times$ & 27.7 & -- && 67 M & $1.0\\times$ & 39.2 & -- \\\\\n            DeLighT                    & 37 M & $0.55\\times$ & 27.6 & -0.1 && 37 M & $0.55\\times$ & 39.6 & +0.4 \\\\\n            DeLighT                    & 54 M & $0.80\\times$ & \\textbf{28.0} & \\textbf{+0.3} && 54 M & $0.80\\times$ & \\textbf{40.5} & \\textbf{+1.3} \\\\\n            \\bottomrule[1.5pt]\n        \\end{tabular}\n    }\n    \\caption{Results on large corpora}\n    \\end{subtable}\n    \\caption{\\textbf{Comparison with baseline transformers on machine translation corpora}. DeLighT ~models require significantly fewer parameters to achieve similar performance. Here, $^\\dagger$ and $^\\ddagger$ indicate the best reported transformer baselines from \\citet{wu2018pay} and \\citet{ghazvininejad2019mask}, respectively.}\n    \\label{tab:compare_delight_trans}\n\\end{table}\n%\n\\begin{figure}[t!]\n    \\begin{minipage}[b]{0.46\\columnwidth}\n        \\vspace{0pt}\n        \\centering\n        \\resizebox{\\columnwidth}{!}{\n        \\begin{tabular}{lrrrr}\n            \\toprule[1.5pt]\n             & \\textbf{Depth} & \\textbf{\\# Params} & \\textbf{\\# MACs} & \\textbf{BLEU}  \\\\\n             \\midrule[1.25pt]\n            Transformer & 60 & 67 M & 11.1 B & 39.2\\\\\n            DeLighT  & 222 & 37 M  & 5.6 B & 39.6 \\\\\n            DeLighT  & 222 & 54 M & 8.1 B & 40.5 \\\\\n            \\bottomrule[1.5pt]\n        \\end{tabular}\n    }\n        \\captionof{table}{\\textbf{DeLighT ~networks are deep, light-weight and efficient} as compared to transformers. BLEU score is reported on the WMT'14 En-Fr dataset. To compute network depth, we count the number of sequential layers in the network (Section \\ref{ssec:layer_wise_scaling}). We used 20 source and 20 target tokens for computing multiplication-addition operations (MACs). See Appendex \\ref{sec:appendix_mac} for details.}\n        \\label{tab:results_depth_macs}\n    \\end{minipage}\n    \\hfill\n    \\begin{minipage}[b]{0.5\\columnwidth}\n        \\vspace{0pt}\n        \\centering\n        \\includegraphics[height=85px]{graphs/scale_tra_e.pdf}\n        \\caption{Comparison of DeLighT ~with Transformers and Evolved Transformers at two different settings, on the WMT'14 En-De corpus: (1) the number of parameters is the same and (2) the performance is the same.}\n        \\label{fig:perf_compare_param}\n    \\end{minipage}\n\\end{figure}\n\n\\noindent {\\bf Comparison with \\sota~methods:} Most \\sota~methods have evaluated the performance on WMT'14 En-De while some have also evaluated on IWSLT'14 De-En. Table \\ref{tab:comapre_nmt_sota} compares the performance of DeLighT ~with \\sota~methods on these two corpora. DeLighT ~delivers similar or better performance than existing methods. It is important to note that existing methods have improved baseline transformers with different design choices -- for example, the asymmetric encoder-decoder structure \\citep{wang2019learning} and neural architecture search \\citep{so2019evolved}. We believe that DeLighT , in the future, would also benefit from such design choices. \n\n\\begin{table}[t!]\n    \\centering\n    \\begin{subtable}[b]{0.52\\columnwidth}\n        \\resizebox{0.9\\columnwidth}{!}{\n        \\begin{tabular}{lrr}\n            \\toprule[1.5pt]\n          \\textbf{Model} & \\textbf{\\# Params} & \\textbf{BLEU} \\\\\n          \\midrule\n            Transformers \\citep{vaswani2017attention} & 42 M & 34.3 \\\\\n            Variational Attention \\citep{deng2018latent} & -- & 33.1 \\\\\n             Dynamic convolutions \\citep{vaswani2017attention} & 43 M & \\textbf{35.2} \\\\\n             Lite Transformer$^\\ddagger$ \\citep{Wu2020Lite} & -- & 33.6 \\\\\n             DeLighT ~(Ours) & \\textbf{30 M} & \\textbf{35.3} \\\\\n            \\bottomrule[1.5pt]\n        \\end{tabular}\n        }\n        \\caption{IWSLT'14 De-En}\n    \\end{subtable}\n    \\hfill\n    \\begin{subtable}[b]{0.47\\columnwidth}\n        \\resizebox{0.9\\columnwidth}{!}{\n        \\begin{tabular}{lrr}\n            \\toprule[1.5pt]\n          \\textbf{Model} & \\textbf{\\# Params} & \\textbf{BLEU} \\\\\n          \\midrule\n            Transformer \\citep{vaswani2017attention} & 62 M & 27.3 \\\\\n            DLCL \\citep{wang2019learning} & 62 M & 27.3 \\\\\n            Evolved Transformer $^\\dagger$ \\citep{so2019evolved} & 46 M & \\textbf{27.7} \\\\\n            Lite Transformer$^\\ddagger$ \\citep{Wu2020Lite} & -- & 26.5 \\\\\n            DeLighT ~(Ours) & \\textbf{37 M} & \\textbf{27.6} \\\\\n            \\bottomrule[1.5pt]\n        \\end{tabular}\n        }\n        \\caption{WMT'14 En-De}\n    \\end{subtable}\n    \\caption{\\textbf{Comparison with \\sota~methods on machine translation corpora}. DeLighT ~delivers similar or better performance than \\sota~models with fewer parameters. Here, $\\dagger$ indicates that the network uses neural architecture search (NAS) and $\\ddagger$ indicates that full network parameters are not reported. }\n    \\label{tab:comapre_nmt_sota}\n\\end{table}\n\n\\noindent {\\bf Scaling up DeLighT ~models:} Figure \\ref{fig:nmt_perf_curve} shows the performance of DeLighT ~models improves with increase in network parameters; suggesting their ability to learn representations across different corpora, including low-resource.\n\n\\begin{figure}[t!]\n    \\centering\n    \\begin{subfigure}[b]{0.24\\columnwidth}\n        \\centering\n        \\includegraphics[width=\\columnwidth]{graphs/delight_nmt_de2en.pdf}\n        \\caption{IWSLT'14 De-En}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[b]{0.24\\columnwidth}\n        \\centering\n        \\includegraphics[width=\\columnwidth]{graphs/delight_nmt_en2ro.pdf}\n        \\caption{WMT'16 En-Ro}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[b]{0.24\\columnwidth}\n        \\centering\n        \\includegraphics[width=\\columnwidth]{graphs/delight_nmt_en2de.pdf}\n        \\caption{WMT'14 En-De}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[b]{0.24\\columnwidth}\n        \\centering\n        \\includegraphics[width=\\columnwidth]{graphs/delight_nmt_en2fr.pdf}\n        \\caption{WMT'14 En-Fr}\n    \\end{subfigure}\n    \\caption{\\textbf{Scaling up DeLighT ~models.} The performance of DeLighT ~improves with an increase in the number of network parameters, across different corpora, including low-resource (WMT'16 En-Ro).}\n    \\label{fig:nmt_perf_curve}\n\\end{figure}\n\n\\subsection{Language Modeling}\n\\label{ssec:language_mdoeling}\n\n\\noindent {\\bf Datasets and evaluation:} We evaluate on the WikiText-103 dataset \\citep{merity2017pointer} that has 103M/217K/245K tokens for training, validation, and testing. It has a word-level vocabulary of about 260K tokens. Following recent works \\citep{baevski2018adaptive,dai2019transformer}, we report performance in terms of \\textit{perplexity} (lower is better) on the test set.\n\n\\noindent {\\bf Architecture:} We use the transformer-based decoder architecture of \\citet{baevski2018adaptive} with $\\mathcal{B}$ DeLighT ~blocks. We use $w_m$=$2$, $N_{min}$=$4$, and $N_{max}$=$12$. We scale $d_m$ using values $\\{384, 512, 784, 1024\\}$ for increasing network parameters. For simplicity, we set $\\mathcal{B}=N_{max}$. Following standard practice, we use adaptive input \\citep{baevski2018adaptive} as a look-up table and adaptive output \\citep{grave2017efficient} as the classification layer with one head (head dimension is 128) and two tails (tail dimensions are 64 and 32). We also share weights between the input and the output layers.\n\n\\vspace{0.5mm}\n\\noindent {\\bf Training:} We follow the training setup of \\citet{baevski2018adaptive}, except that we train our models on 8 NVIDIA Tesla V100 GPUs for 100K iterations with a context length of 512 and an effective batch size of 64K tokens. We use Adam during training and use a context length of 480 during test.\n\n\\noindent {\\bf Results:} Table \\ref{tab:wiki_sota} compares the performance of DeLighT ~with previous methods on WikiText-103. Table \\ref{tab:wiki_txl} plots the variation of perplexity with number of parameters for DeLighT ~and Transformer-XL \\citep{dai2019transformer} -- which outperforms other transformer-based implementations (e.g., \\citealt{baevski2018adaptive}). Both tables show that DeLighT ~delivers better performance than \\sota~methods (including Transformer-XL) and it does this using a smaller context length and significantly fewer parameters, suggesting that the DeLighT transformation ~helps learn strong contextual relationships.\n\n%\\noindent {\\bf Ablations:} We also did an extensive set of ablation studies to better understand which parts of the architecture contribute the most to improve performance and we include these in the Appendix~\\ref{sec:appendix_ablations}. We varied different hyper-parameters including minimum $N_{min}$ and maximum $N_{max}$ number of GLTs, width multiplier $w_m$, and model dimension $d_m$. Overall, the differing settings either hurt performance or increase the parameter count with no further performance gains. \n\n\\begin{table}[t!]\n    \\centering\n    \\begin{subtable}[b]{0.33\\columnwidth}\n        \\centering\n        \\includegraphics[width=0.8\\columnwidth]{images/wiki_103.pdf}\n        \\caption{DeLighT ~vs. Transformer-XL}\n        \\label{tab:wiki_txl}\n    \\end{subtable}\n    \\hfill\n    \\begin{subtable}[b]{0.65\\columnwidth}\n        \\centering\n        \\resizebox{\\columnwidth}{!}{\n        \\begin{tabular}{lcccc}\n            \\toprule[1.5pt]\n           \\multirow{2}{*}{\\textbf{Method}} & \\textbf{Network} & \\textbf{Context}  & \\textbf{\\# Params} & \\textbf{Perplexity} \\\\\n            & \\textbf{Depth} & \\textbf{Length}  & \\textbf{ (in million)} & \\textbf{(Test)} \\\\\n           \\midrule[1pt]\n            LSTM \\citep{grave2016improving} &--& -- & -- & 48.70 \\\\\n            LSTM + Neural Cache \\citep{grave2016improving} &--&-- & --& 40.80 \\\\\n            QRNN \\citep{merity2018analysis} &--& -- & 151 M & 33.00 \\\\\n            \\midrule\n            Transformer-XL \\citep{dai2019transformer} & 64 & 640 & 151 M & \\textbf{24.03} \\\\\n            Transformer-XL (Our impl.)$^\\dagger$  & 64 & 640 & 151 M & 24.34 \\\\\n            Transformer-XL (Our impl.)$^\\dagger$  & 64 & 480 & 151 M & 24.91 \\\\\n            DeLighT ~(Ours) & 158 & \\textbf{480} & \\textbf{99 M} & \\textbf{24.14} \\\\\n            \\bottomrule[1.5pt]\n        \\end{tabular}\n        }\n        \\caption{Comparison with existing methods}\n        \\label{tab:wiki_sota}\n    \\end{subtable}\n    \\caption{\\textbf{Results on the WikiText-103 dataset}. Compared to Transformer-XL, DeLighT ~delivers similar or better performance (lower perplexity) with fewer parameters. $^\\dagger$For Transformer-XL, we reproduce results using the official source code. For evaluating Transformer-XL with a context length of 480, we set the mem\\_len hyper-parameter to 480 in the official evaluation scripts.}%\\protect\\footnotemark }}\n    \\label{tab:wiki103_results}\n\\end{table}\n\n\\begin{table}[t!]\n    \\centering\n    \\resizebox{0.75\\columnwidth}{!}{\n    \\begin{tabular}{llcccc}\n        \\toprule[1.5pt]\n        \\multirow{2}{*}{\\textbf{Row \\#}} & \\multirow{2}{*}{\\textbf{Model}} & \\textbf{\\# Params} & \\textbf{BLEU} & \\textbf{Training} & \\textbf{Memory} \\\\\n         &  & \\textbf{(in million)} & \\textbf{(WMT'14 En-Fr)} & \\textbf{time} & \\textbf{(in GB)} \\\\\n        \\midrule[1pt]\n        R1 & Transformer (unoptimized) & 67 M & 39.2 & 37 hours & 12.5 GB \\\\\n        R2 & DeLighT ~(unoptimized) & 54 M & 40.5 & 23 hours & 14.5 GB \\\\\n        R3 & Transformer (w/ Apex optimized) & 67 M & 39.2 & 16 hours & 11.9 GB \\\\\n        R4 & DeLighT ~(w/ optimized grouping) & 54 M & 40.5 & 19 hours & 11.5 GB \\\\\n        \\bottomrule[1.5pt]\n    \\end{tabular}\n    }\n    \\caption{Comparison with baseline transformers in terms of training speed and memory consumption. In R4, we implemented CUDA kernels for grouping and ungrouping functions only (see Appendix \\ref{sec:append_source}). We expect DeLighT ~to be more efficient with a single and dedicated CUDA kernel for grouping, transformation, feature shuffling, and ungrouping. Memory consumption is measured on a single NVIDIA GP100 GPU (16 GB memory) with a maximum of 4096 tokens per batch and without any gradient accumulation.}\n    \\label{tab:compute_compare}\n\\end{table}\n\n\\begin{table}[t!]\n    \\centering\n    \\resizebox{0.35\\columnwidth}{!}{\n        \\begin{tabular}{lcc}\n            \\toprule[1.5pt]\n           \\textbf{Model}  & \\textbf{Dropout} & \\textbf{BLEU} \\\\\n           \\midrule[1pt]\n            Transformer (62 M) & 0.10 & 27.3 \\\\\n            Transformer (62 M) & 0.30 & 27.7 \\\\\n            DeLighT ~(37 M) & 0.05 & 27.6 \\\\\n            \\toprule[1.5pt]\n        \\end{tabular}\n    }\n    \\caption{DeLighT ~requires less regularization as compared to baseline transformers (Dataset: WMT'14 En-De).}\n    \\label{tab:dropout_effect}\n\\end{table}\n\n\\section{Analysis and Discussions on Computational Efficiency}\n\n\\noindent \\textbf{Training time and memory consumption:} Table \\ref{tab:compute_compare} compares the training time and memory consumption of DeLighT ~with baseline transformers. For an apples-to-apples comparisons, we implemented the Transformer unit without NVIDIA's dedicated CUDA kernel, and trained both transformer and DeLighT ~full-precision networks for 30K iterations on 16 NVIDIA V100 GPUs. The transformer and DeLighT ~models took about 37 and 23 hours for training and consumed about 12.5 GB and 14.5 GB of GPU memory, respectively (R1 vs. R2). When we enabled the dedicated CUDA kernel provided by APEX library\\footnote{https://github.com/NVIDIA/apex} for multi-head attention in Transformers, the training time of the transformer model reduced from 37 to 16 hours while we did not observe any significant change in memory consumption. Motivated by this observation, we implemented dedicated CUDA kernels for grouping and ungrouping functions in GLTs (see Appendix \\ref{sec:append_source}). With these changes, training time and GPU memory consumption of DeLighT ~reduced by about 4 hours and 3 GB, respectively. We emphasize that grouping, linear transformation, feature shuffling, and ungrouping, can be implemented efficiently using a single CUDA kernel. In future, we expect a dedicated CUDA kernel for these operations would further reduce the memory consumption as well as training/inference time.\n\n\\vspace{0.5mm}\n\\noindent \\textbf{Regularization:} Table \\ref{tab:dropout_effect} shows that DeLighT ~delivers similar performance to  baseline transformers, but with fewer parameters and less regularization. This suggests that learning representations with better transformation functions alleviates the need for dropout.\n\n\\section{Conclusion}\nThis paper introduces a deep and light-weight transformer architecture, DeLighT , that efficiently allocates parameters both within the DeLighT ~block and across DeLighT ~blocks. Compared to state-of-the-art transformer models, DeLighT ~models are (1) deep and light-weight and (2) deliver similar or better performance. In the future, we plan to apply DeLighT ~to other tasks, including language model pre-training, question answering, and language generation.\n\n\\vspace{0.5mm}\n\\noindent \\textbf{Acknowledgements:} This research was supported by ONR N00014-18-1-2826, DARPA N66001-19-2-403, NSF (IIS-1616112, IIS1252835), and an Allen Distinguished Investigator Award. Authors would also like to thank members of the UW-NLP and the H2Lab at The University of Washington for their valuable feedback and comments.\n\n\\small{\n\\bibliographystyle{unsrtnat}\n\\bibliography{main}\n}\n\n\\clearpage\n\n\\appendix\n\n\\section{DeLighT ~Architectures for Language Modeling and Machine Translation}\n\\label{sec:appendix_enc_dec_arch}\nDeLighT ~architectures for language modeling and machine translation are shown in Figure \\ref{fig:appendix_arch}. For language modeling, we follow the architecture in \\citet{baevski2018adaptive} while for machine translation, we follow the architecture in \\citet{vaswani2017attention}. \n\n\\begin{figure}[b!]\n    \\centering\n    \\begin{subfigure}[b]{0.38\\columnwidth}\n        \\centering\n        \\resizebox{\\columnwidth}{!}{\n            \\input{tikz/enc_dec.tikz}\\decattn\n        }\n        \\caption{Language Modeling}\n        \\label{fig:appendix_arch_lm}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[b]{0.6\\columnwidth}\n         \\centering\n        \\resizebox{\\columnwidth}{!}{\n            \\input{tikz/enc_dec.tikz}\\encdecattn\n        }\n        \\caption{Machine translation}\n        \\label{fig:appendix_arch_enc_dec}\n    \\end{subfigure}\n    \\caption{Sequence modeling with DeLighT . Here, \\colorbox{green}{green color hexagon} represents the DeLighT transformation .}\n    \\label{fig:appendix_arch}\n\\end{figure}\n\n\\vspace{1mm}\n\\noindent {\\bf Language modeling:} Figure \\ref{fig:appendix_arch_lm} shows the architecture for language modeling. The architecture stacks $\\mathcal{B}$ DeLighT ~blocks, the configuration of each block is determined using block-wise scaling. Each block has three sub-layers. The first layer is a DeLighT transformation ~that learns representations in high-dimensional space. The second layer is a single-head attention that encodes contextual relationships. The third layer is a position-wise light-weight feed-forward network. Similar to  \\citet{vaswani2017attention}, we employ a residual connections \\citep{he2016deep}. Similar to previous works \\citep{baevski2018adaptive,dai2019transformer}, we use tied adaptive input \\citep{baevski2018adaptive} and adaptive softmax \\citep{grave2017efficient} to map tokens to vectors and vectors to tokens, respectively.\n\n\\vspace{1mm}\n\\noindent {\\bf Machine translation:} Figure \\ref{fig:appendix_arch_enc_dec} shows the architecture for machine translation. The encoder stacks $\\mathcal{B}$ DeLighT ~blocks, the configuration of each block is determined using block-wise scaling. Similar to language modeling, each encoder block has three sub-layers. The first layer is a DeLighT transformation ~that learns representations in high-dimensional space. The second layer is a single-head attention that encodes contextual relationships. The third layer is a position-wise light-weight feed-forward network. Similar to \\citet{vaswani2017attention}, we employ a residual connections \\citep{he2016deep}. We use learnable look-up table to map tokens to vectors. Similar to the encoder, the decoder also stacks  $\\mathcal{B}$ blocks. Decoder blocks are identical to encoder blocks, except that they have an additional source-target single-head attention unit before the light-weight FFN. Keys and values in source-target single-head attention unit are projections over the encoder output. We use standard learnable look-up table to map tokens to vectors and linear classification layer to map vectors to tokens. \n\n\\section{Group linear transformation with Input-mixer connection}\n\\label{sec:glt_explain}\nGroup linear transformation (GLT) $\\mathcal{F}$ splits a $d_m$-dimensional input $\\mathbf{X}$ into $g$ non-overlapping groups such that $\\mathbf{X} = \\text{Concat}(\\mathbf{X}_1, \\cdots, \\mathbf{X}_g)$, where $\\mathbf{X}_i$ is a $\\frac{d_m}{g}$-dimensional vector. $\\mathbf{X}_i$'s are then simultaneously transformed using $g$ linear transforms $\\mathbf{W}_i \\in \\mathbf{R}^{\\frac{d_m}{g} \\times \\frac{d_o}{g}}$ to produce $g$ outputs $\\mathbf{Y}_i = \\mathbf{X}_i \\mathbf{W}_i$. $\\mathbf{Y}_i$'s are then concatenated to produce the final $d_o$-dimensional output $\\mathbf{Y} = \\text{Concat}(\\mathbf{Y}_1, \\cdots, \\mathbf{Y}_g)$. \n\nFigure \\ref{fig:glt_app} shows an example of GLT in the expansion phase of DeLighT transformation . For illustrative purposes, we have used the same dimensions in this example. Recall that as we go deeper in the expansion phase, the number of groups increases. In this example, the first layer has one group, the second layer has two groups and the third layer has four groups. GLTs learns group-specific representations and are local.  To allow GLT to learn global representations, we use feature shuffle. An example of GLT with feature shuffle is shown in Figure \\ref{fig:glt_shuff_app}. Furthermore, training deep neural networks by merely stacking linear or group linear (with or without feature shuffle) is challenging because of vanishing gradient problem. Residual connections introduced by \\citet{he2016deep} mitigates this problem and helps train deep neural networks. However, such connections cannot be employed when input and output dimensions are not the same (e.g., during the expansion and reduction phases in DeLighT transformation ). To stabilize the training and learn deeper representations, we use input-mixer connection of \\citet{mehta2020DeFINE}. Figure \\ref{fig:glt_shuff_mix_app} shows an example of GLT with feature shuffle and input mixer connection.\n\n\\begin{figure}[h!]\n    \\centering\n    \\begin{subfigure}[b]{0.3\\columnwidth}\n    \\includegraphics[height=200px]{glt_images/glt.pdf}\n    \\caption{GLT}\n    \\label{fig:glt_app}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[b]{0.3\\columnwidth}\n        \\includegraphics[height=200px]{glt_images/glt_shuffle.pdf}\n        \\caption{GLT with feature shuffle}\n        \\label{fig:glt_shuff_app}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[b]{0.33\\columnwidth}\n        \\includegraphics[width=\\columnwidth]{glt_images/glt_shuffle_inp.pdf}\n        \\caption{GLT with feature shuffle and input mixture connection}\n        \\label{fig:glt_shuff_mix_app}\n    \\end{subfigure}\n    \\caption{This figure visualizes different variants of group linear transformations that are used in the DeLighT transformation .}\n    \\label{fig:glt_vis}\n\\end{figure}\n\n\\section{Multiplication-Addition Operations in DeLighT }\n\\label{sec:appendix_mac}\nThe DeLighT ~block is built using linear transformations, GLTs, and scaled dot-product attention. Total number of multiplication-addition operations (MACs) in a network is an accumulation of these individual operations. \n\nLet $n$ denotes the number of source tokens, $m$ denotes the number of target tokens, $d_m$ denotes the input dimension, $d_o$ denotes the output dimension, and $g$ denotes the number of groups in GLT. The procedure for counting MACs for each of these operations is described below. \n\n\\paragraph{Group linear transformation (GLT):} GLT $\\mathcal{F}$ has $g$ learnable matrices $\\mathbf{W}_i \\in \\mathbf{R}^{\\frac{d_m}{g} \\times \\frac{d_o}{g}}$. Therefore, GLT learns \\colorbox{red!20}{$\\frac{d_md_o}{g}$} parameters and performs \\colorbox{red!20}{$\\frac{d_md_o}{g}$} MACs to transform $d_m$-dimensional input to $d_o$-dimensional output. Following a standard practice, e.g., ResNet of \\citet{he2016deep}, we count addition and multiplication as one operation instead of two because these operations can be fused in recent hardwares.\n\nImportantly, when $g=1$, the GLT is the same as linear transformation.\n\n\\paragraph{Self-attention in DeLighT :} The scaled dot-product self-attention in DeLighT ~is defined as:\n\\begin{equation}\n    \\text{Attention}(\\mathbf{K}, \\mathbf{Q}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_o}}\\right) \\mathbf{V}\n\\end{equation}\nwhere $\\mathbf{Q} \\in \\mathbb{R}^{n \\times d_o}$, $\\mathbf{K} \\in \\mathbb{R}^{n \\times d_o}$, $\\mathbf{V} \\in \\mathbb{R}^{n \\times d_o}$ denotes query, key, and value, respectively.\n\nThe attention operation involves two dot-products. The first dot product between $\\mathbf{Q}$ and $\\mathbf{K}$ while the second dot product is between the output of first dot product and $\\mathbf{V}$. Both dot products require $d_on^2$ MACs. Therefore, total number of MACs in computing scaled dot-product self-attention are \\colorbox{red!20}{$2d_on^2$}.\n\nIn case of a source-target attention (as in machine translation), $\\mathbf{K}$'s and $\\mathbf{V}$'s are from the source (encoder) and $\\mathbf{Q}$'s are incrementally decoded (one token at a time). Therefore, the number of MACs required to decode $m$ target tokens given $n$ source tokens are \\colorbox{red!20}{$\\sum\\limits_{k=1}^{m} 2knd_o$}.\n\n\\section{Ablations on the WikiText-103 dataset}\n\\label{sec:appendix_ablations}\nTable \\ref{tab:scaling_ablte} studies the impact of DeLighT ~block parameters on the WikiText-103 dataset, namely (1) minimum number of GLTs $N_{min}$, (2) maximum number of GLTs $N_{max}$, (3) width multiplier $w_m$, and (4) model dimension $d_m$ (see Figure \\ref{fig:redefine_transformer_sa}). Figure~\\ref{fig:dextra_define}, Figure~\\ref{fig:shuffling}, and Figure~\\ref{fig:reduction_fac} shows the impact of the DeLighT transformation , feature shuffling, and the light-weight FFN. Table \\ref{tab:dextra_position} shows the effect of position of DeLighT transformation ~in the DeLighT ~block while Figure \\ref{fig:appendix_delight_scaling} shows the effect of scaling DeLighT ~networks. We choose the WikiText-103 dataset for ablations because it has very large vocabulary compared to other datasets (267K vs. 30-40K), allowing us to test the ability under large vocabulary sizes. The performance is reported in terms of perplexity (lower is better) on the validation set. In our ablation studies, we used the same settings for training as in Section \\ref{ssec:language_mdoeling} except that we train only for 50K iterations.\n\n\\noindent{\\bf DeLighT block:} Overall, Table~\\ref{tab:scaling_ablte} shows that scaling depth and width using DeLighT transformation ~and block-wise scaling improves performance. We make following observations: \n\\vspace{-1mm}\n\\begin{enumerate}[leftmargin=*, label=\\alph*)]\n\\setlength\\itemsep{0em}\n\\item Block-wise scaling (R4, R5) delivers better performance compared to uniform scaling (R1-R3). For instance, DeLighT ~with $N_{min}=4$ and $N_{max}=8$ (R4) is $1.25\\times$ shallower than DeLighT ~with $N_{min}=8$ and $N_{max}=8$ (R2), but delivers better performance with a similar  number of parameters and operations. Scaling $w_m$ improves performance (R2 vs. R3), however, the improvement is significantly lower than for the model with block-wise scaling (R3 vs. R5). This suggests that non-uniform distribution of parameters across blocks allows the network to learn better representations.  \n\n\\item Different ratios between $N_{max}$ and $N_{min}$ yields different results. We observe significant performance improvements when the ratio is greater than or equal to two. For example, when we scale $\\frac{N_{max}}{N_{min}}$ from 2 to 3 (R6 vs. R8), the perplexity improves by ${\\sim}5$ points with only a moderate increase in network parameters. On the other hand, when the $\\frac{N_{max}}{N_{min}}$ is close to 1 (R6 vs. R7), performance does not change appreciably. This is likely because the allocation of parameters across blocks is close to uniform (Eq. \\ref{eq:mw}). This is consistent with our previous observation. \n\n\\item Learning shallower and narrower representations near the input and deeper and wider representations near the output achieves better performance. For example, when we scaled $N_{max}$ from 8 to 12 for $N_{min}=4$ (R6, R8), DeLighT ~delivered better performance with a similar number of parameters compared to a model with $N_{min}=6$ (R7, R9). This is likely because the ratio of $N_{max}$ and $N_{min}$ is higher when $N_{min}=4$, which helps allocate parameters per block more effectively. \n\\item Deeper and wider representations near the input and shallower and narrower representations near the output hurts performance (R13 vs. R16). \n\\item Scaling width using $w_m$ and $d_m$ improves performance (R10-R15), however, their impact is different. For example, when we scale $w_m$ and $d_m$ by two, the rate of increase in number of parameters and operations is more rapid with  $d_m$ compared to $w_m$. DeLighT 's~ability to learn wider representations in different ways may be useful in selecting application specific models.\n\\end{enumerate}\n\n\\begin{table}[t!]\n    \\centering\n        \\resizebox{0.8\\columnwidth}{!}{\n            \\begin{tabular}{l|cccc|cccc}\n              \\toprule[1.5pt]\n              \\rowcolor{white}\n              \\textbf{Row \\#} & $N_{min}$  & $N_{max}$ & $w_m$ & $d_m$ & \\textbf{Depth} & \\textbf{Parameters}  & \\textbf{MACs} & \\textbf{Perplexity} \\\\\n              \\toprule[1.25pt]\n              \\multicolumn{9}{c}{\\textbf{Uniform vs. block-wise scaling}} \\\\\n              \\midrule\n                R1 & 4 & 4 & 2 & 256 & 43 & 14.1 M & 2.96 B  & 56.19 \\\\\n                R2 & 8 & 8 & 2 & 256 & 115 & 16.6 M & 3.49 B & 48.58 \\\\\n                R3 & 8 & 8 & 4 & 256 & 115 & 22.1 M & 4.64 B & 45.10 \\\\\n                \\cmidrule[0.25pt]{2-9}\n                R4 & 4 & 8 & 2 & 256 & 92 & 16.7 M & 3.51 B & 46.30 \\\\\n                R5 & \\cellcolor{red!25} 4 & \\cellcolor{red!25} 12 & \\cellcolor{red!25} 2 & \\cellcolor{red!25} 256 & 158 & 21.0 M & 4.41 B & 41.18 \\\\\n              \\toprule[1pt]\n              \\multicolumn{9}{c}{\\textbf{Varying depth ($N_{min}$ and $N_{max}$ (Eq. \\ref{eq:mw})}} \\\\\n              \\midrule\n                R6 & 4 & 8 & 2 & 256 & 92 & 16.7 M & 3.51 B & 46.30 \\\\\n                R7 & 6 & 8 & 2 & 256 & 102 & 16.5 M & 3.46 B & 46.68 \\\\\n                R8 & \\cellcolor{red!25} 4 & \\cellcolor{red!25} 12 & \\cellcolor{red!25} 2 & \\cellcolor{red!25} 256 & 158 & 21.0 M & 4.41 B & 41.18 \\\\\n                R9 & 6 & 12 & 2 & 256 & 172 & 20.0 M & 4.20 B & 42.26  \\\\\n              \\toprule[1pt]\n              \\multicolumn{9}{c}{\\textbf{Varying DeLighT transformation 's width $w_m$ (Eq. \\ref{eq:mw})}} \\\\\n              \\midrule\n                R10 & \\cellcolor{red!25} 4 & \\cellcolor{red!25} 12 & \\cellcolor{red!25} 2 & \\cellcolor{red!25} 256 & 158 & 21.0 M & 4.41 B & 41.18 \\\\\n                R11 & 4 & 12 & 3 & 256 & 158 & 23.8 M & 4.99 B & 39.92  \\\\\n                R12 & 4 & 12 & 4 & 256 & 158 & 27.1 M & 5.69 B & 39.10 \\\\\n              \\toprule[1pt]\n              \\multicolumn{9}{c}{\\textbf{Varying model width $d_m$}} \\\\\n              \\midrule \n                R13 & \\cellcolor{red!25} 4 & \\cellcolor{red!25} 12 & \\cellcolor{red!25} 2 & \\cellcolor{red!25} 256 & 158 &  21.0 M & 4.41 B & 41.18 \\\\\n                R14 & 4 & 12 & 2 & 384 & 158 & 29.9 M & 6.28 B & 35.14 \\\\\n                R15 & 4 & 12 & 2 & 512 & 158 & 43.8 M & 9.20 B & 30.81 \\\\\n              \\toprule[1pt]\n              \\multicolumn{9}{c}{\\textbf{Deeper and wider near the Input}} \\\\\n              \\midrule \n               R16 & 12 &  4 & 2 &  256 & 158 &  21.0 M & 4.41 B & 43.10 \\\\\n            \\bottomrule[1.5pt]\n            \\end{tabular}\n        }\n    \\caption{\\textbf{Ablations on different aspects of the DeLighT ~block}, including uniform vs. block-wise scaling, depth scaling, and width scaling. Rows partially highlighted in color have the same configuration (repeated for illustrating results). Our experimental setup is similar to Section \\ref{sec:results}, except that we train our models for 50K iterations. Multiplication and addition operations (MACs) are computed for 20 time steps. }\n    \\label{tab:scaling_ablte}\n\\end{table}\n\n\\vspace{1mm}\n\\noindent{\\bf Impact of DeLighT transformation :} We replace DeLighT transformation ~in the DeLighT ~block (Figure \\ref{fig:redefine_transformer_sa}) with (1) the DeFINE transformation and (2) a stack of linear layers. Figure \\ref{fig:dextra_define} shows that DeLighT transformation ~delivers similar performance with significantly fewer parameters compared to the DeFINE unit and linear layers. In these experiments, the settings are the same as R13-R15 (Table \\ref{tab:scaling_ablte}), except, $N_{max}=8$, because models with a stack of linear layers learn too many parameters.\n\n\\begin{figure}[b!]\n    \\centering\n    \\begin{minipage}{0.45\\columnwidth}\n        \\includegraphics[width=0.9\\columnwidth]{images/define_dextra.pdf}\n        \\caption{\\textbf{Impact of different transformations.} DeLighT ~transformations are more parametric efficient than DeFINE and linear transformations. Lower perplexity value means better performance.}\n    \\label{fig:dextra_define}\n    \\end{minipage}\n    \\hfill\n    \\begin{minipage}{0.45\\columnwidth}\n        \\centering\n        \\includegraphics[width=0.9\\columnwidth]{images/shuffle.pdf}\n        \\caption{\\textbf{Impact of feature shuffling.} Feature shuffling allows us to learn representations from global information and improves performance. Lower perplexity value means better performance.}\n        \\label{fig:shuffling}\n    \\end{minipage}\n\\end{figure}\n\n\\vspace{1mm}\n\\noindent{\\bf Feature shuffling:} Figure \\ref{fig:shuffling} shows that feature shuffling improves the performance of DeLighT ~by 1-2 perplexity points. Here, we use the same settings as in R13-R15 (Table \\ref{tab:scaling_ablte}).\n\n\\vspace{1mm}\n\\noindent{\\bf Light-weight FFN:} Figure \\ref{fig:reduction_fac} shows the impact of varying the reduction factor $r$ in the light-weight FFN. We use the same settings as in R13 (Table \\ref{tab:scaling_ablte}). We did not observe any significant drop in performance until $r=4$. Beyond $r=4$, we see a drop in performance (perplexity increases by ${\\sim}2$ points). In such cases, the inner dimensions of the light-weight FFN are very small and hurt performance. Notably, the light-weight FFN with $r=2^2$ delivered the same performance as $r=2^{-2}$, but with $1.28\\times$ fewer network parameters. At $r=2^{-2}$, the light-weight FFN is the same as the FFN in \\cite{vaswani2017attention}. This suggests that the ability of DeLighT transformation ~to learn representations in high-dimensional spaces efficiently allows us to reduce the computational burden on the FFN. \n\nWe also tested removing the light-weight FFN and while it reduced parameters by $\\sim$0.5-1 M, performance dropped by about 2-3 perplexity points across different parametric settings.\n\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=0.45\\columnwidth]{images/reduction.pdf}\n    \\caption{\\textbf{Impact of reduction factor $r$ in light-weight FFN.} The ability of DeLighT transformation ~to learn representations in high-dimensional spaces efficiently allows us to reduce the computational burden on the FFN. Lower perplexity value means better performance.}\n    \\label{fig:reduction_fac}\n\\end{figure}\n\n\\vspace{1mm}\n\\noindent{\\bf Uniform vs. block-wise scaling:} Figure \\ref{fig:appendix_block_unifrom} compares the performance of DeLighT ~with uniform and block-wise scaling. For a given model dimension $d_m$, DeLighT ~models with block-wise scaling delivers better performance.\n\n\\begin{figure}[t!]\n    \\centering\n    \\begin{subfigure}[b]{0.5\\columnwidth}\n        \\centering\n        \\resizebox{!}{120px}{\n            \\input{tikz/scaling_new.tikz}\\scaling\n        }\n        \\caption{}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[b]{0.48\\columnwidth}\n        \\includegraphics[width=\\columnwidth]{images/layer_scaling.pdf}\n        \\caption{}\n    \\end{subfigure}\n    \\caption{\\textbf{Uniform vs. block-wise scaling.} (a) contrasts the uniform and block-wise scaling methods. (b) compares the results of DeLighT ~with uniform and block-wise scaling methods on the WikiText-103 dataset. DeLighT ~networks with block-wise scaling delivers better performance across different settings. Lower perplexity value means better performance.}\n    \\label{fig:appendix_block_unifrom}\n\\end{figure}\n\n\\paragraph{Position of DeLighT transformation :}\n%\\label{ssec:ablate_position}\nWe studied three configurations for the DeLighT transformation ~on the WikiText-103 validation set (Table \\ref{tab:dextra_position}): (1) DeLighT transformation ~followed by single-headed attention and light-weight FFN, (2) single-headed attention followed by DeLighT transformation , and (3) single-headed attention followed by DeLighT transformation ~and light-weight FFN. For similar number of parameters, we found that (2) and (3) drops the performance of (1) significantly across different parametric settings. This suggests that deeper and wider representations helps learn better contextual representations; allowing us to replace multi-headed attention with single-headed attention.\n\n\\begin{table}[t!]\n    \\centering\n    \\resizebox{0.9\\columnwidth}{!}{\n    \\begin{tabular}{lrr}\n        \\toprule[1.5pt]\n        \\textbf{Configuration}  & \\textbf{Parameters} & \\textbf{Perplexity} \\\\\n        \\midrule[1pt]\n        DeLighT transformation  + Single-head attention + Light-weight FFN & 31 M & \\textbf{34.20} \\\\ \n        Single-head attention + DeLighT transformation  & 30 M & 39.02 \\\\ \n        Single-head attention + DeLighT transformation  + Light-weight FFN & 31 M & 39.43 \\\\ \n        \\midrule\n        DeLighT transformation  + Single-head attention + Light-weight FFN & 99 M & \\textbf{23.16} \\\\ \n        Single-head attention + DeLighT transformation  & 96 M & 28.33 \\\\ \n        Single-head attention + DeLighT transformation  + Light-weight FFN & 99 M & 27.94 \\\\\n        \\bottomrule[1.5pt]\n    \\end{tabular}\n    }\n    \\caption{\\textbf{Effect of the position of DeLighT transformation }. Lower value of perplexity means better performance.}\n    \\label{tab:dextra_position}\n\\end{table}\n\n\\vspace{1mm}\n\\noindent{\\bf Scaling up DeLighT :} Figure \\ref{fig:appendix_delight_scaling} shows the results of DeLighT ~models obtained after varying configuration parameters of DeLighT ~transformations ($N_{min}$=\\{4, 6\\}, $N_{max}$=\\{8, 12\\}, $w_m$=\\{2, 3, 4\\}, and $d_m$=\\{256, 384, 512\\}). We can see that scaling one configuration parameter (e.g., $d_m$) while keeping other configuration parameters constant (e.g., $N_{min}$, $N_{max}$, and $w_m$) consistently improves performance.\n\nThis work investigates relationships between $N_{min}$, $N_{max}$, $w_m$, and $d_m$, manually. We believe that a more principled approach, such as compound scaling of \\citet{tan2019efficientnet}, that establishes relationships between these parameters would produce more efficient and accurate models. %We will explore such methods in the future.\n\n\\begin{figure}[t!]\n    \\centering\n        \\begin{subfigure}[b]{0.49\\columnwidth}\n            \\centering\n            \\includegraphics[width=0.9\\columnwidth]{images/plot_4_8.pdf}\n            \\caption{$N_{min}$=4, $N_{max}$=8}\n            \\label{fig:arch_4_8}\n        \\end{subfigure}\n        \\hfill\n         \\begin{subfigure}[b]{0.49\\columnwidth}\n            \\centering\n            \\includegraphics[width=0.9\\columnwidth]{images/plot_6_8.pdf}\n            \\caption{$N_{min}$=6, $N_{max}$=8}\n            \\label{fig:arch_6_8}\n        \\end{subfigure}\n        \\vfill\n         \\begin{subfigure}[b]{0.49\\columnwidth}\n            \\centering\n            \\includegraphics[width=0.9\\columnwidth]{images/plot_4_12.pdf}\n            \\caption{$N_{min}$=4, $N_{max}$=12}\n            \\label{fig:arch_4_12}\n        \\end{subfigure}\n        \\hfill\n         \\begin{subfigure}[b]{0.49\\columnwidth}\n            \\centering\n            \\includegraphics[width=0.9\\columnwidth]{images/plot_6_12.pdf}\n            \\caption{$N_{min}$=6, $N_{max}$=12}\n            \\label{fig:arch_6_12}\n        \\end{subfigure}\n        \\caption{\\textbf{Scaling up DeLighT }. Scaling one configuration parameter (e.g., $d_m$) while keeping other configuration parameters constant (e.g., $N_{min}$, $N_{max}$, and $w_m$) consistently improves performance. The numbers on top of each bar represents network parameters (in million). Lower value of perplexity means better performance.}\n        \\label{fig:appendix_delight_scaling}\n\\end{figure}\n\n\\section{Source code for group linear transformation}\n\\label{sec:append_source}\nThe source code for implementing group linear transformation (GLT) in PyTorch is shown in Listing \\ref{lst:navie_glt}. The source code for efficiently implementing the grouping function in GLT is shown in Listing \\ref{lst:group_kernel}. Since the ungrouping kernel is similar to grouping kernel, we have not shown it here.\n\nThe reshape and transpose operations in naive PyTorch implementation for grouping and ungrouping are replaced with a dedicated CUDA kernels, resulting in reduced memory footprint and faster training.\n\n\\definecolor{dkgreen}{rgb}{0,0.6,0}\n\\definecolor{gray}{rgb}{0.5,0.5,0.5}\n\\definecolor{mauve}{rgb}{0.58,0,0.82}\n\n\\lstset{frame=H,\n  language=Python,\n  aboveskip=3mm,\n  belowskip=3mm,\n  showstringspaces=false,\n  columns=flexible,\n  basicstyle={\\small\\ttfamily},\n  numbers=none,\n  numberstyle=\\tiny\\color{gray},\n  keywordstyle=\\color{blue},\n  commentstyle=\\color{dkgreen},\n  stringstyle=\\color{mauve},\n  breaklines=true,\n  breakatwhitespace=true,\n  tabsize=3\n}\n\n\n\\begin{minipage}{\\columnwidth}\n\\begin{lstlisting}[caption=\"Naive implementation of GLT in Pytorch\", label={lst:navie_glt}]\nimport torch\ndef glt_function(x, n_groups, weights, bias=None):\n    '''\n    :param x: Input tensor of size [B x N], where B is batch size and N is input dimension\n    :param n_groups: number of groups in GLT\n    :param weights: glt weights [g x N/g x M/g]\n    :param bias: GLT bias (optional) of size [g x 1 x M/g]\n    :return: output tensor of size [B x M]\n    '''\n    bsz = x.size(0)\n    \n    ## GROUPING FUNCTION: Converts [B x N] tensor to [g x B  x N/g] ##\n    # [B x N] --> [B x g  x N/g]\n    x = x.contiguous().view(bsz, n_groups, -1)\n    # [B x g x N/g] --> [g x B  x N/g]\n    x = x.transpose(0, 1)  # transpose so that group is first\n\n    ## TRANSFORMATION FUNCTION: Transforms from N/g-dimensional space to M/g-dimensional space ##\n    # [g x B  x N/g] x [g x N/g x M/g] --> [g x B x M/g]\n    x = torch.bmm(x, weights)  # multiply with Weights\n    # add bias\n    if bias is not None:\n        x = torch.add(x, bias)\n        \n    ## REGROUPING FUNCTION: Converts [g x B x M/g] tensor to [B x M] ##\n    # [g x B x M/g] --> [B x g x M/g]\n    x = x.transpose(0, 1)  # transpose so that batch is first\n    # [B x g x M/g] --> [B x M]\n    x = x.contiguous().view(bsz, -1)\n    return x\n\\end{lstlisting}\n\\end{minipage}\n\n\n\\lstset{language=C++}\n\\begin{minipage}{\\columnwidth}\n\\begin{lstlisting}[caption=\"Grouping kernel in CUDA\", label={lst:group_kernel}]\n\n/* Grouping Kernel: Transforms input from [B x N] to [g x B x N/g] */\ntemplate<typename scalar_t>\n__global__ void grouping_kernel_forward(const scalar_t* input,\n                            const int groups, const int total_elements,\n                            const int input_features, const int group_features,\n                            const int batch_size, scalar_t* output){\n    const int index = IMUL(blockIdx.x, blockDim.x) + threadIdx.x;\n    if (index >= total_elements){\n        return;\n    }\n    const int b_idx = index / group_features;\n    const int g_f_idx = (index % group_features);\n    int in_offset, out_offset;\n    #pragma unroll\n    for(int g=0; g <  groups; g++){\n        in_offset = (b_idx * input_features) + (g * group_features) + g_f_idx;\n        out_offset = ((g * batch_size + b_idx) * group_features) + g_f_idx;\n        output[out_offset] = input[in_offset];\n    }\n}\n\\end{lstlisting}\n\\end{minipage}\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Finetuned Language Models Are Zero-Shot Learners \\raggedright}\n\n\\begin{document}\n\n\\maketitle\n\\vspace{-2mm}\n\\begin{abstract}\n\nThis paper explores a simple method for improving the zero-shot learning abilities of language models. \nWe show that \\textit{instruction tuning}---finetuning language models on a collection of datasets described via instructions---substantially improves zero-shot performance on unseen tasks.\\vspace{1mm}\n\nWe take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates.\nWe evaluate this instruction-tuned model, which we call \\flan{}, on unseen task types.\n\\flan{} substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate.\n\\flan{} even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze.\nAblation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.\n\n\\end{abstract}\n\n\\vspace{-3mm}\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[height=82mm]{figs/figure-pull-example.pdf}\n    \\vspace{-2mm}\n    \\caption{\n    Top: overview of instruction tuning and \\flan. Instruction tuning finetunes a pretrained language model on a mixture of tasks phrased as instructions.\n    At inference time, we evaluate on an unseen task type; for instance, we could evaluate the model on natural language inference (NLI) when no NLI tasks were seen during instruction tuning.\n    Bottom: performance of zero-shot FLAN, compared with zero-shot and few-shot GPT-3, on three unseen task types where instruction tuning improved performance substantially out of ten we evaluate.\n    NLI datasets: ANLI R1--R3, CB, RTE. Reading comprehension datasets: BoolQ, MultiRC, OBQA. Closed-book QA datasets: ARC-easy, ARC-challenge, NQ, TriviaQA.\n    }\n    \\label{fig:flan-pull}\n\\end{figure}\n\n\\clearpage\n\n\\section{Introduction}\n\nLanguage models (LMs) at scale, such as GPT-3~\\citep{brown2020language}, have been shown to perform few-shot learning remarkably well. \nThey are less successful at zero-shot learning, however.\nFor example, GPT-3's zero-shot performance is much worse than few-shot performance on tasks such as reading comprehension, question answering, and natural language inference. \nOne potential reason is that, without few-shot exemplars, it is harder for models to perform well on prompts that are not similar to the format of the pretraining data.\n\nIn this paper, we explore a simple method to improve the zero-shot performance of large language models, which would expand their reach to a broader audience.\nWe leverage the intuition that NLP tasks can be described via natural language instructions, such as ``\\textit{Is the sentiment of this movie review positive or negative?}'' or ``\\textit{Translate `how are you' into Chinese.}''\nWe take a pretrained language model of 137B parameters and perform \\textit{instruction tuning}---finetuning the model on a mixture of more than 60 NLP datasets expressed via natural language instructions.\nWe refer to this resulting model as \\flan, for \\underline{F}inetuned \\underline{La}nguage \\underline{N}et.\n\nTo evaluate the zero-shot performance of \\flan\\ on unseen tasks, we group NLP datasets into clusters based on their task types and hold out each cluster for evaluation while instruction tuning \\flan{} on all other clusters.\nFor example, as shown in \\cref{fig:flan-pull}, to evaluate \\flan's ability to perform natural language inference, we instruction tune the model on a range of other NLP tasks such as commonsense reasoning, translation, and sentiment analysis.\nAs this setup ensures that \\flan{} has not seen any natural language inference tasks in instruction tuning, we then evaluate its ability to perform zero-shot natural language inference.\n\nOur evaluations show that \\flan{} substantially improves the zero-shot performance of the base 137B-parameter model.\n\\flan's zero-shot also outperforms 175B-parameter GPT-3's zero-shot on 20 of 25 datasets that we evaluate, and even outperforms GPT-3's few-shot by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. \nIn ablation studies, we find that increasing the number of task clusters in instruction tuning improves performance on unseen tasks and that the benefits of instruction tuning emerge only with sufficient model scale. \n\nInstruction tuning is a simple method that, as depicted in \\cref{fig:flan-paradigm}, combines appealing aspects of both the pretrain--finetune and prompting paradigms by using supervision via finetuning to improve language model's responses to inference-time text interactions.\nOur empirical results demonstrate promising abilities of language models to perform tasks described purely via instructions.\nSource code for loading the instruction tuning dataset used for \\flan{} is publicly available at \\url{https://github.com/google-research/flan}.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.93\\linewidth]{figs/figure-general.pdf}\n    \\vspace{-1mm}\n    \\caption{Comparing instruction tuning with pretrain--finetune and prompting.}\n    \\label{fig:flan-paradigm}\n\\end{figure}\n\n\\section{\\flan: Instruction Tuning Improves Zero-Shot Learning}\nThe motivation of instruction tuning is to improve the ability of language models to respond to NLP instructions.\nThe idea is that by using supervision to teach an LM to perform tasks described via instructions, the LM will learn to follow instructions and do so even for unseen tasks. \nTo evaluate performance on unseen tasks, we group datasets into clusters by task type and hold out each task cluster for evaluation while instruction tuning on all remaining clusters.\n\n\\subsection{Tasks \\& Templates}\\label{subsec:tasks_and_templates}\nAs creating an instruction tuning dataset with many tasks from scratch would be resource-intensive, we transform existing datasets from the research community into an instructional format. \nWe aggregate 62 text datasets that are publicly available on Tensorflow Datasets, including both language understanding and language generation tasks, into a single mixture.\n\\cref{fig:flan-clusters} shows these datasets---each dataset is categorized into one of twelve task clusters, for which datasets in a given cluster are of the same task type. \nDescriptions, sizes, and examples of each dataset are shown in \\cref{task_details}.\n\\vspace{-1mm}\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/all-tasks.pdf}\n    \\vspace{-7mm}\n    \\caption{Datasets and task clusters used in this paper (NLU tasks in blue; NLG tasks in teal).}\n    \\vspace{-2mm}\n    \\label{fig:flan-clusters}\n\\end{figure}\n\nFor each dataset, we manually compose ten unique templates that use natural language instructions to describe the task for that dataset.\nWhile most of the ten templates describe the original task, to increase diversity, for each dataset we also include up to three templates that ``turned the task around,'' (e.g., for sentiment classification we include templates asking to generate a movie review).\nWe then instruction tune a pretrained language model on the mixture of all datasets, with examples in each dataset formatted via a randomly selected instruction template for that dataset.\n\\cref{fig:flan-template-example} shows multiple instruction templates for a natural language inference dataset.\n\\vspace{-1mm}\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.93\\linewidth]{figs/template-example.pdf}\n    \\vspace{-3mm}\n    \\caption{Multiple instruction templates describing a natural language inference task.}\n    \\vspace{-1mm}\n    \\label{fig:flan-template-example}\n\\end{figure}\n\n\\subsection{Evaluation Splits}\\label{subsec:eval_splits}\nWe are interested in how \\flan{} performs on tasks not seen in instruction tuning, and so it is crucial to define what counts as an unseen task.\nWhereas some prior work defines unseen tasks by disallowing the same dataset to appear in training, we use a more conservative definition that leverages the task clusters from \\cref{fig:flan-clusters}.\nIn this work, we only consider dataset $\\mathcal{D}$ unseen at evaluation time if no datasets from any task clusters that $\\mathcal{D}$ belongs to were seen during instruction tuning.\nFor instance, if $\\mathcal{D}$ is an entailment task, then no entailment datasets appeared in instruction tuning, and we instruction-tuned on all other clusters.\\footnote{When evaluating on the read.\\ comp.\\ with commonsense cluster, both read.\\ comp.\\ and commonsense reasoning were dropped from instruction tuning.\nConversely, the read.\\ comp.\\ with commonsense cluster was not used for instruction tuning when evaluating on read.\\ comp.\\ or commonsense reasoning.\nWe also drop the paraphrase cluster from instruction tuning when evaluating on NLI tasks and vice-versa.}\nHence, to evaluate zero-shot \\flan{} on $c$ task clusters, we instruction tune $c$ models, where each model holds out a different task cluster for evaluation.\n\n\\subsection{Classification with Options}\\label{subsec:options}\n\\vspace{-1mm}\nThe output space for a given task is either one of several classes (classification) or free text (generation). \nAs \\flan{} is an instruction-tuned version of a decoder-only language model, it naturally responds in free text, and so no further modifications are needed for generation tasks.\n\nFor classification tasks, prior work \\citep{brown2020language} used a \\textit{rank classification} approach where, for example, only two outputs (``\\textit{yes}'' and ``\\textit{no}'') are considered and the higher probability one is taken as the model's prediction.\nThough this procedure is logically sound, it is imperfect in that the probability mass for answers may have an undesired distribution among ways of saying each answer (e.g., a large number of alternative ways of saying ``\\textit{yes}'' may lower the probability mass assigned to ``\\textit{yes}'').\nTherefore, we include an \\textit{options} suffix, in which we append the token \\textttsmall{OPTIONS} to the end of a classification task along with a list of the output classes for that task.\nThis makes the model aware of which choices are desired when responding to classification tasks.\nExample use of options is shown in the NLI and commonsense examples in \\cref{fig:flan-pull}.\n\n\\vspace{-1mm}\n\\subsection{Training Details}\n\\vspace{-1mm}\n\n\\textbf{Model architecture and pretraining.} \nIn our experiments, we use \\baselm{}, a dense left-to-right, decoder-only transformer language model of 137B parameters \\citep{thoppilan2022lamda}.\nThis model is pretrained on a collection of web documents (including those with computer code), dialog data, and Wikipedia, tokenized into 2.49T BPE tokens with a 32k vocabulary using the SentencePiece library \\citep{sentencepiece}. \nAround 10\\% of the pretraining data was non-English.\nNote that \\baselm{} only has language model pretraining (c.f. LaMDA, which was finetuned for dialog).\n\n\\textbf{Instruction tuning procedure.}\n\\flan{} is the instruction-tuned version of \\baselm. \nOur instruction tuning pipeline mixes all datasets and randomly samples from each dataset.\nTo balance the different sizes of datasets, we limit the number of training examples per dataset to 30k and follow the examples-proportional mixing scheme \\citep{raffel2019exploring} with a mixing rate maximum of 3k.\\footnote{In this mixing scheme, a mixing rate maximum of 3,000 means that a dataset does not receive additional sampling weight for examples in excess of 3,000.}\nWe finetune all models for 30k gradient steps with a batch size of 8,192 tokens using the Adafactor Optimizer \\citep{shazeer2018adafactor} with a learning rate of 3e-5. \nThe input and target sequence lengths used in finetuning are 1024 and 256, respectively. \nWe use packing \\citep{raffel2019exploring} to combine multiple training examples into a single sequence, separating inputs from targets using a special EOS token.\nThis instruction tuning takes around 60 hours on a TPUv3 with 128 cores.\nFor all evaluations, we report results on the final checkpoint trained for 30k steps.\n\n\\vspace{-1mm}\n\\section{Results}\\label{sec:results}\n\\vspace{-2mm}\nWe evaluate \\flan{} on natural language inference, reading comprehension, closed-book QA, translation, commonsense reasoning, coreference resolution, and struct-to-text. \nAs described in \\cref{subsec:eval_splits}, we evaluate on unseen tasks by grouping datasets into task clusters and holding out each cluster for evaluation while instruction tuning on all remaining clusters (i.e., each evaluation task cluster uses a different checkpoint).\nFor each dataset, we evaluate the mean of performance on all templates, which proxies the expected performance given a typical natural language instruction.\nAs a dev set is sometimes available for manual prompt engineering \\citep{brown2020language}, for each dataset we also obtain the test set performance using the template with the best dev set performance.\n\nFor comparison, we report zero and few-shot results for \\baselm\\ using the same prompts as GPT-3 (as \\baselm\\ is not suitable for natural instructions without instruction tuning).\nThis baseline provides the most direct ablation of how much instruction tuning helps.\nInstruction tuning significantly improves \\baselm\\ on most datasets.\n\nWe also show the zero-shot performances of GPT-3 175B \\citep{brown2020language} and GLaM 64B/64E \\citep{du2021glam}, as reported in their respective papers. \nWith the best dev template, zero-shot \\flan{} outperforms zero-shot GPT-3 on 20 of 25 datasets and even surpasses GPT-3's few-shot performance on 10 datasets. \nWith the best dev-template, zero-shot \\flan{} outperforms zero-shot GLaM on 13 of 19 available datasets and one-shot GLaM on 11 of 19 datasets.\n\nOverall, we observe that instruction tuning is very effective on tasks naturally verbalized as instructions (e.g., NLI, QA, translation, struct-to-text) and is less effective on tasks directly formulated as language modeling, where instructions would be largely redundant (e.g., commonsense reasoning and coreference resolution tasks that are formatted as finishing an incomplete sentence or paragraph).\nResults on natural language inference, reading comprehension, closed-book QA, and translation are summarized in \\cref{fig:flan-results-summary} and described below.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/summary-results.pdf}\n    \\vspace{-6mm}\n    \\caption{Zero-shot performance of \\flan{} compared to \\baselm{} 137B, GPT-3 175B, and GLaM 64B/64E on natural language inference, reading comprehension, closed-book QA, and translation. \n    Performance of \\flan{} is the mean of up to 10 instructional templates per task.\n    Supervised models were either T5, BERT, or translation models (specified in \\cref{tab:nlu_table} and \\cref{tab:nlg_table} in the Appendix).\n    }\n    \\vspace{-1mm}\n    \\label{fig:flan-results-summary}\n\\end{figure}\n\n\\textbf{Natural language inference (NLI).} \nOn five NLI datasets, where a model must determine whether a hypothesis is true given some premise, FLAN outperforms all baselines by a large margin. %\nAs noted by \\citet{brown2020language}, perhaps one reason why GPT-3 struggles with NLI is that NLI examples are unlikely to have appeared naturally in an unsupervised training set and are thus awkwardly phrased as a continuation of a sentence. \nFor \\flan{}, we phrase NLI as the more natural question ``\\texttt{\\small Does <premise> mean that <hypothesis>?}'', achieving much higher performance.\n\n\\textbf{Reading comprehension.} \nOn reading comprehension, where models are asked to answer a question about a provided passage, FLAN outperforms baselines for MultiRC \\citep{khashabi-etal-2018-looking} and OBQA \\citep{mihaylov-etal-2018-suit}.\nOn BoolQ \\citep{clark-etal-2019-boolq}, \\flan{} outperforms GPT-3 by a large margin, though \\baselm{} already achieves high performance on BoolQ.\n\n\\textbf{Closed-book QA.}\nFor closed-book QA, which asks models to answer questions about the world without access to specific information containing the answer, \\flan{} outperforms GPT-3 on all four datasets.\nCompared to GLaM, \\flan{} has better performance on ARC-e and ARC-c \\citep{clark2018think}, and slightly lower performance on NQ \\citep{orqa,kwiatkowski2019natural} and TQA \\citep{JoshiTriviaQA2017}.\n\n\\textbf{Translation.}\nSimilar to GPT-3, the training data for \\baselm\\ is around 90\\% English and includes some text in other languages that was not specifically used to train the model to perform machine translation. \nWe also evaluate \\flan's performance on machine translation for the three datasets evaluated in the GPT-3 paper: French--English from WMT'14 \\citep{wmt14}, and German--English and Romanian--English from WMT'16 \\citep{wmt16}. \nCompared with GPT-3, \\flan{} outperforms zero-shot GPT-3 for all six evaluations, though it underperforms few-shot GPT-3 in most cases.\nSimilar to GPT-3, \\flan{} shows strong results for translating into English and compares favorably against supervised translation baselines.\nTranslating from English into other languages, however, was relatively weaker, as might be expected given that \\flan{} uses an English sentencepiece tokenizer and that the majority of pretraining data is English.\n \n\n\\newcommand{\\flanvalspaced}[4]{\n\\makecell[l]{\\hspace{#4mm}#1\\vspace{-1.5mm}\\\\\n{\\hspace{#4mm}\\battleshipgrey{\\tiny std=#2}}\\vspace{0.3mm}\\\\\n\\hspace{#4mm}{#3}}}\n\\newcommand{\\flanval}[3]{\\flanvalspaced{#1}{#2}{#3}{0}}\n\n\\newcommand{\\gptvalspaced}[4]{\\makecell[l]{\\hspace{#4mm}#1\\vspace{-0.4mm}\\\\{\\hspace{#4mm}\\battleshipgrey{\\scriptsize #3}}}}\n\\newcommand{\\gptval}[3]{\\gptvalspaced{#1}{#2}{#3}{0}}\n\n\\newcommand{\\gptname}[0]{\\makecell[l]{GPT-3 175B zero-shot\\vspace{-0.6mm}\\\\{\\scriptsize \\hspace{3mm}$\\cdot$ few-shot}}}\n\\newcommand{\\glmname}[0]{\\makecell[l]{\\baselm{} 137B zero-shot \\vspace{-0.6mm}\\\\{\\scriptsize\\hspace{3mm}$\\cdot$ few-shot}}}\n\\newcommand{\\gptvalna}[0]{\\gptval{--}{--}{--}}\n\\newcommand{\\tasktype}[1]{\\multicolumn{6}{l}{\\textsc{\\underline{\\textbf{#1}}}}}\n\\newcommand{\\tfiveval}[1]{#1$^{a}$}\n\\newcommand{\\bertlargeval}[1]{#1$^{b}$}\n\\newcommand{\\flanname}[0]{\\makecell[l]{- average template{\\tiny \\vspace{2mm}} \\\\ {- best dev template}}}\n\\newcommand{\\datasetacc}[1]{\\makecell{#1 \\vspace{-0.7mm}\\\\{\\footnotesize acc.}\\vspace{-0.7mm}}}\n\\newcommand{\\datasetem}[1]{\\makecell{#1 \\vspace{-0.7mm}\\\\{\\scriptsize EM}\\vspace{-0.7mm}}}\n\\newcommand{\\datasetbleu}[1]{\\makecell{#1 \\vspace{-0.7mm}\\\\{\\scriptsize BLEU}\\vspace{-0.7mm}}}\n\\newcommand{\\datasetfone}[1]{\\makecell{#1 \\vspace{-0.7mm}\\\\{\\scriptsize F1}\\vspace{-0.7mm}}}\n\\newcommand{\\datasetcustom}[2]{\\makecell{#1 \\vspace{-0.7mm}\\\\{\\scriptsize #2}\\vspace{-0.7mm}}}\n\\newcommand{\\wewin}[1]{\\textcolor{jweigreen}{\\hspace{0.3mm}{\\scriptsize$\\blacktriangle$}{\\scriptsize#1}}\\hspace{1mm}}\n\\newcommand{\\welose}[1]{\\textcolor{americanrose}{\\hspace{0.7mm}{\\scriptsize -}{\\scriptsize#1}}\\hspace{1mm}}\n\\newcommand{\\wekindawin}[1]{\\textcolor{jweigreen}{\\scriptsize{\\hspace{0.6mm}$\\uparrow$\\hspace{0.3mm}#1}\\hspace{1mm}}}\n\\newcommand{\\explainflan}[0]{For \\flan, we report both the average of up to ten templates, as well as the best dev template.}\n\\newcommand{\\explainwewin}[0]{The triangle \\textcolor{jweigreen}{\\scriptsize$\\blacktriangle$} indicates improvement over few-shot GPT-3.}\n\\newcommand{\\explainwekindawin}[0]{The up-arrow \\textcolor{jweigreen}{\\scriptsize$\\uparrow$} indicates improvement only over zero-shot GPT-3.}\n\\newcommand{\\gptvala}[3]{& #1 & #2 {\\tiny #3}}\n\\newcommand{\\baselmvala}[3]{& #1 & #2 {\\tiny [#3]}}\n\\newcommand{\\flanvala}[3]{& #1{\\tiny $\\pm$#2} & #3}\n\\newcommand{\\na}[0]{\\makecell[c]{--}}\n\\newcommand{\\explainkt}[0]{{\\scriptsize $[k]$} indicates the number of few-shot exemplars. {\\scriptsize \\#$t$} indicates the number of templates that FLAN is evaluated on.}\n\\newcommand{\\fewk}[1]{{\\tiny [#1]}}\n \n \n\n\\textbf{Additional tasks.}\nAlthough we see strong results for the above task clusters, one limitation with instruction tuning is that it does not improve performance for many language modeling tasks (e.g., commonsense reasoning or coreference resolution tasks formulated as sentence completions).\nFor seven commonsense reasoning and coreference resolution tasks (see \\cref{tab:nlu_table} in the Appendix), \\flan{} only outperforms \\baselm{} on three of the seven tasks.\nThis negative result indicates that when the downstream task is the same as the original language modeling pre-training objective (i.e., in cases where instructions are largely redundant), instruction tuning is not useful.\nFinally, we report results for sentiment analysis, paraphrase detection, and struct-to-text, as well as additional datasets for which GPT-3 results are not available, in \\cref{tab:nlu_table} and \\cref{tab:nlg_table} in the Appendix.\nGenerally, zero-shot \\flan{} outperforms zero-shot \\baselm{} and is comparable with or better than few-shot \\baselm{}.\n\n\\section{Ablation Studies \\& Further Analysis}\n\n\\subsection{Number of instruction tuning clusters}\\label{subsec:finetuning_clusters}\nAs the core question of our paper asks how instruction tuning improves a model's zero-shot performance on unseen tasks, in this first ablation we examine how performance is affected by the number of clusters and tasks used in instruction tuning.\nFor this setup, we hold out NLI, closed-book QA, and commonsense reasoning as evaluation clusters, and use the seven remaining clusters for instruction tuning.\\footnote{We do not use the paraphrase or reading comprehension with commonsense clusters for instruction tuning in this ablation because they are too similar to NLI and commmonsense reasoning, respectively.}\nWe show results for one to seven instruction tuning clusters, where clusters are added in decreasing order of number of tasks per cluster.\n\n\\cref{fig:flan-ablation-numtasks} shows these results.\nAs expected, we observe that average performance across the three held-out clusters improves as we add additional clusters and tasks to instruction tuning (with the exception of the sentiment analysis cluster), confirming the benefits of our proposed instruction tuning approach on zero-shot performance on novel tasks.\nIt is further interesting to see that, for the seven clusters we test, the performance does not appear to saturate, implying that performance may further improve with even more clusters added to instruction tuning.\nOf note, this ablation does not allow us to draw conclusions about which instruction tuning cluster contributes the most to each evaluation cluster, although we see minimal added value from the sentiment analysis cluster.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.65\\linewidth]{figs/ablation-1-figure.pdf}\n    \\vspace{-2mm}\n    \\caption{Adding additional task clusters to instruction tuning improves zero-shot performance on held-out task clusters. \n    The evaluation tasks are the following. \n    Commonsense: CoPA, HellaSwag, PiQA, and StoryCloze. NLI: ANLI R1--R3, QNLI, RTE, SNLI, and WNLI. Closed-book QA: ARC easy, ARC challenge, Natural Questions, and TriviaQA.}\n    \\label{fig:flan-ablation-numtasks}\n\\end{figure}\n\\vspace{-2mm}\n\n\\begin{wrapfigure}{r}{0.53\\textwidth}\n    \\centering\n    \\vspace{-13mm}\n    \\includegraphics[width=\\linewidth]{figs/scale-ablation-held-out-only.pdf}\n    \\vspace{-7mm}\n    \\caption{\n    Whereas instruction tuning helps large models generalize to new tasks, for small models it actually hurts generalization to unseen tasks, potentially because all model capacity is used to learn the mixture of instruction tuning tasks. \n    }\n    \\vspace{-6mm}\n    \\label{fig:scale-ablation}\n\\end{wrapfigure}\n\n\\subsection{Scaling laws}\\label{subsec:scaling_laws}\n\nAs \\citet{brown2020language} shows that zero and few-shot capabilities of language models substantially improve for larger models, we next explore how the benefits of instruction tuning are affected by model scale. \nUsing the same cluster split as in the previous ablation study, we evaluate the effect of instruction tuning on models of size 422M, 2B, 8B, 68B, and 137B parameters. \n\n\\cref{fig:scale-ablation} shows these results. \nWe see that for the two models on the order of 100B parameters, instruction tuning substantially improves performance on held-out tasks, as is expected given the prior results in our paper.\nThe behavior on held-out tasks for the 8B and smaller models, however, is thought-provoking---instruction tuning actually hurts performance on held-out tasks. \nOne potential explanation for this result could be that for small-scale models, learning the $\\sim$40 tasks used during instruction tuning fills the entire model capacity, causing these models to perform worse on new tasks. \nUnder this potential explanation, for the larger scale models, instruction tuning fills up some model capacity but also teaches these models how to follow instructions, allowing them to generalize to new tasks with the remaining capacity. \n\n\\begin{wrapfigure}{r}{0.43\\textwidth}\n    \\centering\n    \\vspace{-6mm}\n    \\includegraphics[width=\\linewidth]{figs/instruction-ablation.pdf}\n    \\vspace{-7mm}\n    \\caption{\n    Ablation study result using models with instructions removed from finetuning (FT). \n    }\n    \\vspace{-3mm}\n    \\label{fig:role-of-insructions}\n\\end{wrapfigure}\n\n\\subsection{Role of instructions}\\label{subsec:role-of-instructions}\nIn a final ablation study, we explore the role of instructions during finetuning, as one possibility is that performance gains come entirely from multi-task finetuning and the model could perform just as well without instructions.\nWe hence consider two finetuning setups without instructions.\nIn a \\textit{no template} setup, only inputs and outputs were given to the model (e.g., for translation the input would be ``\\textit{The dog runs.}'' and the output would be ``\\textit{Le chien court.}'').\nIn a \\textit{dataset name} setup, each input is prepended with the name of the task and dataset (e.g., for translation to French, the input would be ``\\textit{[Translation: WMT'14 to French] The dog runs.}'').\n\nWe compare these two ablations to \\flan{}'s finetuning procedure, which used natural instructions (e.g., ``\\textit{Please translate this sentence to French: `The dog runs.'}'').\nWe perform evaluations for four held-out clusters from \\cref{fig:flan-results-summary}.\nFor the no template setup, we used the \\flan{} instructions during zero-shot inference (because if we used no template, the model would not know what task to perform).\nFor models finetuned on dataset name only, we report zero-shot performance for \\flan{} instructions as well as using the dataset name.\n\\cref{fig:role-of-insructions} shows the results---both ablation configurations performed substantially worse than \\flan{}, indicating that training with instructions is crucial for zero-shot performance on unseen tasks.\n\n\\subsection{Instructions with Few-Shot Exemplars}\\label{subsec:finetune}\n\n\\newcommand{\\instruct}{\\textrm{instruct}}\nSo far, we have focused on instruction tuning in the zero-shot setting. \nHere, we study how instruction tuning can be used when few-shot exemplars are available at inference time.\nThe format for the few-shot setting builds on the zero-shot format. \nFor some input $x$ and output $y$, let $\\instruct(x)$ denote the zero-shot instructions. \nThen, given $k$ few-shot exemplars ${(x_i, y_i)}_{i=1}^k$ and a new input $x$, the instruction format for the few-shot setting is ``$\\instruct(x_1) \\oplus y_1 \\oplus \\instruct(x_2) \\oplus y_2 \\oplus \\ldots \\oplus \\instruct(x_k) \\oplus y_k \\oplus \\instruct(x)$'', where $\\oplus$ denotes string concatenation with a delimiter token inserted in between.\nAt both training and inference time, exemplars are randomly drawn from the training set, and the number of exemplars is capped at 16 and such that the total sequence length is less than 960 tokens. \nOur experiment uses the same task splits and evaluation procedure as \\cref{sec:results}, such that few-shot exemplars for an unseen task are only used at inference time.\n\nAs shown in \\cref{fig:few-shot}, few-shot exemplars improve the performance on all task clusters, compared with zero-shot FLAN. \nExemplars are especially effective for tasks with large/complex output spaces, such as struct to text, translation, and closed-book QA, potentially because exemplars help the model better understand the output format. \nIn addition, for all task clusters, standard deviation among templates is lower for few-shot FLAN, indicating reduced sensitivity to prompt engineering. \n\n\\begin{figure}[h!]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/few-shot.pdf}\n    \\vspace{-4mm}\n    \\caption{\n    Adding few-shot exemplars to FLAN is a complementary method for improving the performance of instruction-tuned models.\n    The orange bars indicate standard deviation among templates, averaged at the dataset level for each task cluster.\n    }\n    \\label{fig:few-shot}\n\\end{figure}\n\n\\subsection{Instruction Tuning Facilitates Prompt Tuning}\\label{subsec:prompt_tuning}\n\n\\begin{wrapfigure}{r}{0.33\\textwidth}\n    \\centering\n    \\vspace{-10mm}\n    \\includegraphics[width=\\linewidth]{figs/prompt-tuning-figure.pdf}\n    \\vspace{-7mm}\n    \\caption{\n    Instruction-tuned models respond better to continuous inputs from prompt tuning.\n    When prompt tuning on a given dataset, no tasks from the same cluster as that dataset were seen during instruction tuning.\n    Performance shown is the average on the SuperGLUE dev set.\n    }\n    \\vspace{-3mm}\n    \\label{fig:prompt_tuning}\n\\end{wrapfigure}\n\nAs we've seen that instruction tuning improves the ability of a model to respond to instructions, it follows that, if \\flan{} is indeed more amenable to performing NLP tasks, then it should also achieve better performance when performing inference using soft prompts, represented by prepended continuous variables optimized via prompt tuning \\citep{li-liang-2021-prefix,lester-prompt-tuning}.\nAs further analysis, we train continuous prompts for each of the SuperGLUE \\citep{wang2019superglue} tasks in accordance with the cluster splits from \\cref{subsec:eval_splits} such that when prompt-tuning on task $\\mathcal{T}$, no tasks in the same cluster as $\\mathcal{T}$ were seen during instruction tuning.\nOur prompt tuning setup follows the procedure of \\citet{lester-prompt-tuning} except that we use a prompt length of 10, weight decay of 1e-4, and did not use dropout on the attention scores; we found in preliminary experiments that these changes improved the performance of \\baselm. \n\n\\cref{fig:prompt_tuning} shows the results of these prompt tuning experiments for both using a fully-supervised training set and in a low-resource setting with only 32 training examples.\nWe see that in all scenarios, prompt tuning works better with \\flan{} than \\baselm.\nIn many cases, especially for the low-resource setting, prompt tuning on \\flan{} even achieves more than 10\\% improvement over prompt tuning on the \\baselm. \nThis result exemplifies in another way how instruction tuning can result in a checkpoint that is more desirable for performing NLP tasks.\n\n\\section{Related Work}\nOur work relates to several broad research areas including zero-shot learning, prompting, multi-task learning, and language models for NLP applications \\citep[][\\textit{inter alia}]{radford2019language,raffel2019exploring,brown2020language,efrat2020turking,aghajanyan2021muppet,li-liang-2021-prefix}.\nWe describe prior work for these broad areas in an extended related work section (\\cref{sec:extended_related_work}), and here we describe two subareas narrower in scope that perhaps relate most closely to our work.\n\nThe way we ask a model to respond to instructions is similar to QA-based task formulation \\citep{kumar2016ask,mccann2018natural}, which aims to unify NLP tasks by casting them as QA over a context.\nThough these methods are very similar to ours, they mostly focus on multi-task learning instead of zero-shot learning, and---as noted by \\citet{liu2021survey}---they are generally not motivated by using existing knowledge in pretrained LMs.\nMoreover, our work supercedes recent work such as \\citet{chai2020description} and \\citet{zhong2021meta} in terms of both model scale and scope of tasks.\n\nThe success of language models has led to nascent research on the ability of models to follow instructions. \nMost recently, \\citet{mishra2021natural} finetune 140M parameter BART on instructions with few-shot exemplars, and evaluate its few-shot abilities on unseen tasks---this is similar to our few-shot instruction tuning result from \\cref{subsec:finetune}.\nThis promising result (as well as one from \\citet{ye2021crossfit}, which does not emphasize instructions as much) suggests that finetuning on a collection of tasks improves few-shot performance on unseen tasks, even at a smaller model scale.\n\\citet{sanh2021multitask} finetune T5 in a setup similar to ours, finding that zero-shot learning can be improved in a model of 11B parameters.\nAt a model scale similar to ours, OpenAI's InstructGPT models are trained via both finetuning and reinforcement learning to produce outputs that are more preferred by human raters \\citep{ouyang2022instructgpt}.\n\n\\section{Discussion}\n\nOur paper has explored a simple question in zero-shot prompting: does finetuning a model on a collection of tasks phrased as instructions improve its performance on unseen tasks?\nWe operationalize this question via instruction tuning, a simple method that combines appealing aspects of both the pretrain--finetune and prompting paradigms.\nOur instruction-tuned model, \\flan{}, improves performance against an untuned model and surpasses zero-shot GPT-3 on the majority of tasks that we evaluate on.\nAblation studies reveal that performance on unseen tasks improves with the number of instruction tuning task clusters, and, interestingly, that performance improvements from instruction tuning emerge only with sufficient model scale.\nMoreover, instruction tuning can be combined with other prompting methods such as few-shot prompting and prompt tuning.\n\nThe diverse capabilities of language models at scale have drawn attention to the tradeoffs between specialist models (one model per task) and generalist models \\citep[one model for many tasks;][]{arivazhagan2019massively,pratap2020massively}, for which our study has potential implications. \nAlthough one might expect labeled data to have the most natural role in improving specialist models, instruction tuning demonstrates how labeled data can be used to help large language models perform many, unseen tasks. \nIn other words, the positive effect of instruction tuning on cross-task generalization shows that task-specific training is complementary to general language modeling and motivates further research on generalist models. \n\nAs for limitations of our study, there is a degree of subjectivity in assigning tasks to clusters (though we try to use accepted categorizations in the literature), and we only explore the use of relatively short instructions of typically a single sentence (c.f. detailed instructions given to crowd-workers). \nA limitation for our evaluation is that individual examples might have appeared in the models’ pretraining data, which includes web documents, though in post-hoc analysis (\\cref{sec:data_contamination}) we do not find any evidence that data overlap substantially impacted the results.\nFinally, the scale of \\flan{} 137B makes it costly to serve.\nFuture work on instruction tuning could include gathering/generating even more task clusters for finetuning, \ncross-lingual experiments, \nusing \\flan{} to generate data for training downstream classifiers, \nand using finetuning to improve model behavior with respect to bias and fairness \\citep{solaiman2021process}. \n\n\\section{Conclusions}\nThis paper has explored a simple method for improving the ability of language models at scale to perform zero-shot tasks based purely on instructions.\nOur instruction-tuned model, \\flan{}, compares favorably against GPT-3 and signals the potential ability for language models at scale to follow instructions.\nWe hope that our paper will spur further research on instructions-based NLP, zero-shot learning, and using labeled data to improve large language models. \n\n\\clearpage\n\n\\section*{Ethical Considerations}\nThis work uses language models, for which the risks and potential harms are discussed in \\citet{bender-koller-2020-climbing}, \\citet{brown2020language}, \\citet{10.1145/3442188.3445922}, Patterson et al., (2021), and others. As our contribution in this paper is not a pretrained language model itself but rather an empirical study of how instruction tuning affects the zero-shot performance of a language model on unseen tasks, we additionally highlight two relevant ethical considerations. First, labeled datasets such as those we use for finetuning can contain undesirable biases, and these biases can be propagated into zero-shot applications of the model on downstream tasks.  And second, instruction-tuned models can potentially require less data and expertise to use; such lower barriers to access could increase both the benefits and associated risks of such models.\n\n\\section*{Environmental Considerations}\nWe use the same pretrained language models as \\citet{Austin2021ProgramSW}. %\nThe energy cost and carbon footprint for the pretrained models were 451 MWh and 26 tCO2e, respectively. \nThe additional instruction tuning gradient-steps for finetuning \\flan{} is less than 2\\% of the number of pretraining steps, and so the estimated additional energy cost is comparatively smaller.\n\n\\section*{Author Contributions}\\label{sec:contributions}\nMaarten Bosma conceived the original idea and implemented the first version of \\flan. \nVincent Zhao prototyped the training and evaluation pipelines, as well as rank classification. \nKelvin Guu proposed and implemented the idea of task clusters and evaluation using inter-cluster splits. \nJason Wei, Maarten Bosma, Vincent Zhao, and Adams Wei Yu implemented the NLP tasks. \nJason Wei, Vincent Zhao, and Adams Wei Yu conducted and managed most of the experiments. \nJason Wei designed and ran the ablation studies. \nJason Wei, Maarten Bosma, and Quoc V. Le wrote most of the paper. \nJason Wei, Maarten Bosma, and Nan Du obtained the zero and few-shot baselines. \nVincent Zhao and Kelvin Guu designed, implemented, and conducted the few-shot FLAN experiments.\nMaarten Bosma and Jason Wei ran the data contamination analysis.\nBrian Lester ran the prompt tuning experiments. \nQuoc V.~Le and Andrew M.~Dai advised, provided high-level guidance, and helped edit the paper.\n\n\\section*{Acknowledgements}\nWe thank Ed Chi, Slav Petrov, Dan Garrette, Ruibo Liu, and Clara Meister for providing feedback on our manuscript. \nWe thank Adam Roberts, Liam Fedus, Hyung Won Chung, and Noam Shazeer for helping debug some of our models. \nWe thank Ellie Pavlick for feedback on the study design during the middle stages of the project.  \nWe thank Daniel De Freitas Adiwardana for helping initiate the project, large language model advising, and giving us access to some computational resources.\nFinally, we thank the team involved in pretraining \\baselm{}: Daniel De Freitas Adiwardana, Noam Shazeer, Yanping Huang, Dmitry Lepikhin, Dehao Chen, Yuanzhong Xu and Zhifeng Chen.\n\n\\clearpage \n\n\\newpage \n\\appendix\n\n\\newcommand{\\trainsize}[0]{30,000}\n\\newcommand{\\devsize}[0]{200}\n\n\\section{Additional Results}\\label{sec:supp_all_results}\n\nThis section shows the full results for all datasets we evaluate.\nResults for translation and struct to text are shown in \\cref{tab:nlg_table}, and the results for eight NLU task clusters are shown in \\cref{tab:nlu_table}.\n\nWe show FLAN's performance using the best of up to ten instruction templates as well as the template with the best performance on the dev set. \nFor \\baselm{}, we use the templates from \\citet{brown2020language}, which were optimized for GPT-3, without performing any prompt engineering to optimize them on our model. \nFor simplicity, we use greedy search for all generative tasks (compared with beam search used in \\citet{brown2020language}). \nUnlike GPT-3, which chooses the number of few-shot exemplars $k$ via best dev set performance, for few-shot \\baselm{} we choose the highest $k$ that fits in the context length of 1024 tokens, from $k \\in \\{1, 3, 5, 10\\}$. \n\nFor DROP \\citep{Dua2019DROP} and SQuADv2 \\citep{rajpurkar-etal-2018-know}, based on email correspondence with \\citet{brown2020language}, their definition of zero-shot differs from ours in that they actually use exemplars, but only from the same passage as the inference question (each passage has more than one question). \nHence, GPT-3 zero-shot results are not directly comparable with ours for DROP and SQuADv2.\nWe mark these results using the $^{\\dagger}$ symbol.\nMoreover, it is unclear how to parse the end of an answer for these two datasets, and so we use curly bracket delimiters \\texttt{\\{} and \\texttt{\\}}, where we expect \\texttt{\\}} to indicate the end of the answer.\n\nFor struct to text, reported T5/mT5 results are from the GEM benchmark paper \\citep{gehrmann2021gem}, though we do not report their results for DART (through correspondence with authors, we confirmed that their results for DART were incorrect). \nThough we use a summarization task cluster during instruction tuning, we leave evaluation of summarization for future work, as the mean input of most summarization datasets exceeds \\flan's input length of 1024 tokens.\n\n\\begingroup\n\\setlength{\\tabcolsep}{1.5pt}\n\\newcommand{\\centerme}[1]{\\multicolumn{1}{c}{#1}}\n\\newcommand{\\centermewithrightbar}[1]{\\multicolumn{1}{c|}{#1}}\n\\begin{table}[h]\n    \\centering\n    \\small\n    \\begin{tabular}{l lc cl cl rc rc lc}\n    \\toprule\n     & & & & & & & \\multicolumn{6}{c}{FLAN 137B} \\\\\n     \\cmidrule(lr){8-13}\n     & & & \\multicolumn{2}{c}{\\baselm{}} &  \\multicolumn{2}{c}{GPT-3 175B} & \\multicolumn{2}{c}{zero-shot} & \\multicolumn{3}{c}{few-shot}\\\\\n     \\cmidrule(lr){4-5} \\cmidrule(lr){6-7} \\cmidrule(lr){8-9} \\cmidrule(lr){10-12}\n     & Metric & \\makecell[c]{\\scriptsize Supervised\\vspace{-0.6mm}\\\\\\scriptsize Model} & \\makecell[c]{zero-\\vspace{-0.6mm}\\\\ shot} & \\makecell[c]{few-\\vspace{-0.6mm}\\\\ shot {\\tiny [$k$]}} & \\makecell[c]{zero-\\vspace{-0.6mm}\\\\ shot} & \\makecell[c]{few-\\vspace{-0.6mm}\\\\ shot {\\tiny [$k$]}} & \\makecell[c]{\\scriptsize average \\vspace{-0.6mm}\\\\ \\scriptsize template}  & \\makecell[c]{\\scriptsize best dev \\vspace{-0.6mm}\\\\ \\scriptsize template} &  \\makecell[c]{\\scriptsize average \\vspace{-0.6mm}\\\\ \\scriptsize template}  & \\makecell[c]{\\scriptsize best dev \\vspace{-0.6mm}\\\\ \\scriptsize template} & {\\tiny [$k$]} & {\\scriptsize \\#$t$} \\\\\n    \\midrule\n    \\tasktype{Translation} \\\\\n    WMT '14 En$\\rightarrow$Fr & BLEU & 35.0$^d$     \\baselmvala{11.2}{31.5}{5} \\gptvala{25.2}{32.6}{[64]} \\flanvala{32.9}{1.1}{33.9} \\flanvala{33.9}{0.2}{33.8} & \\fewk{9} & \\tiny{5} \\\\\n    WMT '14 Fr$\\rightarrow$En & BLEU & 45.6$^c$     \\baselmvala{7.2}{34.7}{5} \\gptvala{21.2}{39.2}{[64]} \\flanvala{35.5}{1.3}{35.9} \\flanvala{38.0}{0.1}{37.9} & \\fewk{9} & \\tiny{3} \\\\\n    WMT '16 En$\\rightarrow$De & BLEU & 38.6$^f$     \\baselmvala{7.7}{26.7}{5} \\gptvala{24.6}{29.7}{[64]} \\flanvala{25.4}{1.8}{27.0} \\flanvala{26.8}{0.4}{26.1} & \\fewk{11} & \\tiny{5} \\\\\n    WMT '16 De$\\rightarrow$En & BLEU & 41.2$^e$     \\baselmvala{20.8}{36.8}{5} \\gptvala{27.2}{40.6}{[64]} \\flanvala{38.9}{0.3}{38.9} \\flanvala{40.6}{0.1}{40.7} & \\fewk{11} & \\tiny{3} \\\\\n    WMT '16 En$\\rightarrow$Ro & BLEU & 39.9$^g$     \\baselmvala{3.5}{22.9}{5} \\gptvala{14.1}{21.0}{[64]} \\flanvala{16.7}{1.6}{18.9} \\flanvala{20.5}{0.1}{20.5} & \\fewk{9} & \\tiny{5} \\\\\n    WMT '16 Ro$\\rightarrow$En & BLEU & 38.5$^g$     \\baselmvala{9.7}{37.5}{5} \\gptvala{19.9}{39.5}{[64]} \\flanvala{36.8}{0.5}{37.3} \\flanvala{38.2}{0.1}{38.1} & \\fewk{9} & \\tiny{3} \\\\\n    \\midrule\n    \\tasktype{Struct to Text} \\\\\n    CommonGen & Rouge-1 & \\tfiveval{64.0}   \\baselmvala{3.9}{56.7}{3} \\gptvala{\\na}{\\na}{} \\flanvala{54.6}{2.3}{56.3} \\flanvala{56.6}{0.3}{56.4} & \\fewk{16} & \\tiny{6} \\\\\n     & Rouge-2 & \\tfiveval{29.4}            \\baselmvala{1.5}{29.6}{3} \\gptvala{\\na}{\\na}{} \\flanvala{28.8}{2.4}{27.6} \\flanvala{30.9}{0.7}{29.9} & \\fewk{16} & \\tiny{6} \\\\\n     & Rouge-L & \\tfiveval{54.5}            \\baselmvala{3.2}{48.5}{3} \\gptvala{\\na}{\\na}{} \\flanvala{48.4}{1.9}{48.7} \\flanvala{50.7}{0.2}{51.0} & \\fewk{16} & \\tiny{6} \\\\\n    DART & Rouge-1 & \\na                    \\baselmvala{11.3}{56.0}{3} \\gptvala{\\na}{\\na}{} \\flanvala{45.5}{4.2}{48.9} \\flanvala{57.9}{1.6}{59.2} & \\fewk{11} & \\tiny{7} \\\\\n     & Rouge-2 & \\na                        \\baselmvala{1.5}{29.6}{3} \\gptvala{\\na}{\\na}{} \\flanvala{25.0}{3.7}{30.0} \\flanvala{35.8}{1.0}{36.2} & \\fewk{11} & \\tiny{7} \\\\\n     & Rouge-L & \\na                        \\baselmvala{3.2}{48.5}{3} \\gptvala{\\na}{\\na}{} \\flanvala{38.4}{3.8}{43.4} \\flanvala{48.5}{0.9}{48.2} & \\fewk{11} & \\tiny{7} \\\\\n    E2ENLG & Rouge-1 & \\tfiveval{72.6}      \\baselmvala{6.2}{56.7}{3} \\gptvala{\\na}{\\na}{} \\flanvala{44.8}{3.9}{51.4} \\flanvala{59.1}{1.3}{59.7} & \\fewk{12} & \\tiny{9} \\\\\n     & Rouge-2 & \\tfiveval{47.5}            \\baselmvala{2.5}{31.4}{3} \\gptvala{\\na}{\\na}{} \\flanvala{24.2}{3.6}{30.1} \\flanvala{33.2}{1.1}{33.6} & \\fewk{12} & \\tiny{9} \\\\\n     & Rouge-L & \\tfiveval{56.4}            \\baselmvala{4.9}{41.1}{3} \\gptvala{\\na}{\\na}{} \\flanvala{37.0}{3.5}{42.4} \\flanvala{44.9}{0.8}{45.1} & \\fewk{12} & \\tiny{9} \\\\\n    WebNLG & Rouge-1 & \\tfiveval{83.5}      \\baselmvala{13.9}{68.3}{3} \\gptvala{\\na}{\\na}{} \\flanvala{50.6}{4.7}{57.7} \\flanvala{68.5}{2.2}{71.2} & \\fewk{10} & \\tiny{8} \\\\\n     & Rouge-2 & \\tfiveval{63.6}            \\baselmvala{6.9}{46.0}{3} \\gptvala{\\na}{\\na}{} \\flanvala{29.8}{4.2}{35.4} \\flanvala{48.0}{1.5}{49.8} & \\fewk{10} & \\tiny{8} \\\\\n     & Rouge-L & \\tfiveval{71.0}            \\baselmvala{11.8}{56.5}{3} \\gptvala{\\na}{\\na}{} \\flanvala{43.4}{4.5}{49.7} \\flanvala{58.8}{1.1}{60.2} & \\fewk{10} & \\tiny{8} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\n    Results for translation and struct-to-text tasks.\n    \\explainkt\n    $^{a}$T5-11B,\n    $^c$\\citet{edunov-etal-2018-understanding},\n    $^d$\\citet{durrani-etal-2014-edinburghs},\n    $^e$\\citet{wang2019multi},\n    $^f$\\citet{sennrich-etal-2016-edinburgh},\n    $^g$\\citet{liu-etal-2020-multilingual-denoising}.\n    }\n    \\label{tab:nlg_table}\n\\end{table}\n\\endgroup\n\\begingroup\n\\setlength{\\tabcolsep}{0.8pt}\n\\newcommand{\\centerme}[1]{\\multicolumn{1}{c}{#1}}\n\\newcommand{\\centermewithrightbar}[1]{\\multicolumn{1}{c|}{#1}}\n\\begin{table}[t]\n    \\centering\n    \\small\n    \\begin{tabular}{l cc ll cl cl lc lcl r}\n    \\toprule\n     & & & & & & & & & \\multicolumn{6}{c}{FLAN 137B} \\\\\n     \\cmidrule(lr){10-15}\n     & & & \\multicolumn{2}{c}{GLaM} & \\multicolumn{2}{c}{\\baselm{}} & \\multicolumn{2}{c}{GPT-3 175B} & \\multicolumn{2}{c}{zero-shot} & \\multicolumn{3}{c}{few-shot} \\\\\n     \\cmidrule(lr){4-5} \\cmidrule(lr){6-7} \\cmidrule(lr){8-9} \\cmidrule(lr){10-11} \\cmidrule(lr){12-14}\n     & \\makecell[c]{\\scriptsize Random\\vspace{-0.6mm}\\\\ \\scriptsize Guess} & \\makecell[c]{\\scriptsize Supervised\\vspace{-0.6mm}\\\\\\scriptsize Model} & \\makecell[c]{zero-\\vspace{-0.6mm}\\\\ shot} & \\makecell[c]{one-\\vspace{-0.6mm}\\\\ shot} & \\makecell[c]{zero-\\vspace{-0.6mm}\\\\ shot} & \\makecell[c]{few-\\vspace{-0.6mm}\\\\ shot {\\tiny [$k$]}} & \\makecell[c]{zero-\\vspace{-0.6mm}\\\\ shot} & \\makecell[c]{few-\\vspace{-0.6mm}\\\\ shot {\\tiny [$k$]}} & \\makecell[c]{\\scriptsize average \\vspace{-0.6mm}\\\\ \\scriptsize template}  & \\makecell[c]{\\scriptsize best dev \\vspace{-0.6mm}\\\\ \\scriptsize template} &  \\makecell[c]{\\scriptsize average \\vspace{-0.6mm}\\\\ \\scriptsize template}  & \\makecell[c]{\\scriptsize best dev \\vspace{-0.6mm}\\\\ \\scriptsize template} & {\\tiny [$k$]} & {\\scriptsize \\#$t$} \\\\\n    \\midrule\n    \\tasktype{NLI} \\\\\n    ANLI R1 & 33.3 & \\bertlargeval{57.4} & 40.9 & 42.4       \\baselmvala{39.6}{39.0}{5} \\gptvala{34.6}{36.8}{[50]} \\flanvala{47.7}{1.4}{46.4} \\flanvala{44.2}{2.3}{47.9} & \\fewk{6} & \\tiny{8} \\\\\n    ANLI R2 & 33.3 & \\bertlargeval{48.3} & 38.2 & 40.0       \\baselmvala{39.9}{37.5}{5} \\gptvala{35.4}{34.0}{[50]} \\flanvala{43.9}{1.3}{44.0} \\flanvala{41.6}{1.4}{41.1} & \\fewk{6} & \\tiny{8} \\\\\n    ANLI R3 & 33.3 & \\bertlargeval{43.5} & 40.9 & 40.8       \\baselmvala{39.3}{40.7}{5} \\gptvala{34.5}{40.2}{[50]} \\flanvala{47.0}{1.3}{48.5} \\flanvala{42.8}{2.2}{46.8} & \\fewk{6} & \\tiny{8} \\\\\n    CB & 33.3 & \\tfiveval{93.6} & 33.9 & 73.2                 \\baselmvala{42.9}{34.4}{5} \\gptvala{46.4}{82.1}{[32]} \\flanvala{64.1}{14.7}{83.9} \\flanvala{82.6}{4.4}{82.1} & \\fewk{7} & \\tiny{10} \\\\\n    MNLI-m & 33.3 & \\tfiveval{92.2} & \\na & \\na             \\baselmvala{35.7}{43.7}{5} \\gptvala{\\na}{\\na}{} \\flanvala{51.1}{6.2}{61.2} \\flanvala{60.8}{3.7}{63.5} & \\fewk{10} & \\tiny{10} \\\\\n    MNLI-mm & 33.3 & \\tfiveval{91.9} & \\na & \\na           \\baselmvala{37.0}{43.8}{5} \\gptvala{\\na}{\\na}{} \\flanvala{51.0}{6.5}{62.4} \\flanvala{61.0}{3.5}{63.5} & \\fewk{10} & \\tiny{10} \\\\\n    QNLI & 50.0 & \\tfiveval{96.9} & \\na & \\na              \\baselmvala{50.6}{55.7}{5} \\gptvala{\\na}{\\na}{} \\flanvala{59.6}{4.9}{66.4} \\flanvala{62.0}{1.7}{63.3} & \\fewk{12} & \\tiny{9} \\\\\n    RTE & 50.0 & \\tfiveval{92.5} & 68.8 & 71.5              \\baselmvala{73.3}{70.8}{5} \\gptvala{63.5}{72.9}{[32]} \\flanvala{78.3}{7.9}{84.1} \\flanvala{79.9}{6.9}{84.5} & \\fewk{8} & \\tiny{10} \\\\\n    SNLI & 33.3 & \\bertlargeval{91.3} & \\na & \\na          \\baselmvala{33.3}{54.7}{5} \\gptvala{\\na}{\\na}{} \\flanvala{43.0}{7.4}{53.4} \\flanvala{62.3}{2.4}{65.6} & \\fewk{15} & \\tiny{9} \\\\\n    WNLI & 50.0 & \\tfiveval{94.5} & \\na & \\na              \\baselmvala{56.3}{64.8}{5} \\gptvala{\\na}{\\na}{} \\flanvala{61.0}{10.6}{74.6} \\flanvala{55.4}{11.0}{70.4} & \\fewk{14} & \\tiny{10} \\\\\n    \\midrule\n    \\tasktype{Reading Comp.} \\\\\n    BoolQ & 50.0 & \\tfiveval{91.2} & 83.0 & 82.8             \\baselmvala{81.0}{80.0}{1} \\gptvala{60.5}{77.5}{[32]} \\flanvala{80.2}{3.1}{82.9} \\flanvala{83.6}{0.8}{84.6} & \\fewk{4} & \\tiny{9} \\\\\n    DROP & \\na & \\bertlargeval{80.5} & 54.9 & 55.2      \\baselmvala{3.8}{10.3}{1} \\gptvala{23.6$^{\\dagger}$}{36.5}{[20]} \\flanvala{21.9}{0.9}{22.7} \\flanvala{22.3}{1.1}{23.9} & \\fewk{2} & \\tiny{7} \\\\\n    MultiRC & \\na & \\tfiveval{88.1} & 45.1 & 62.0        \\baselmvala{60.0}{59.6}{5} \\gptvala{72.9}{74.8}{[32]} \\flanvala{74.5}{3.7}{77.5} \\flanvala{69.2}{3.2}{72.1} & \\fewk{1} & \\tiny{8} \\\\\n    OBQA & 25.0 & \\tfiveval{85.4} & 53.0 & 55.2              \\baselmvala{41.8}{50.6}{10} \\gptvala{57.6}{65.4}{[100]} \\flanvala{77.4}{1.3}{78.4} \\flanvala{77.2}{1.3}{78.2} & \\fewk{16} & \\tiny{7} \\\\\n    SQuADv1 & \\na & \\tfiveval{96.2} & \\na & \\na        \\baselmvala{22.7}{50.2}{3} \\gptvala{\\na}{\\na}{} \\flanvala{79.5}{1.6}{80.1} \\flanvala{82.1}{0.5}{82.7} & \\fewk{4} & \\tiny{8} \\\\\n    SQuADv2 & \\na & \\bertlargeval{83.4} & 68.3 & 70.0   \\baselmvala{11.1}{34.9}{3} \\gptvala{59.5$^{\\dagger}$}{69.8}{[16]} \\flanvala{40.9}{1.8}{44.2} \\flanvala{40.8}{0.9}{43.1} & \\fewk{3} & \\tiny{10} \\\\\n    \\midrule \n    \\tasktype{Closed-Book QA} \\\\\n    ARC-c & 25.0 & \\tfiveval{81.1} & 48.2 & 50.3     \\baselmvala{42.0}{49.4}{10} \\gptvala{51.4}{51.5}{[50]} \\flanvala{61.7}{1.4}{63.1} \\flanvala{63.7}{0.6}{63.8} & \\fewk{13} & \\tiny{7} \\\\\n    ARC-e & 25.0 & \\tfiveval{92.6} & 71.9 & 76.6           \\baselmvala{76.4}{80.9}{10} \\gptvala{68.8}{70.1}{[50]} \\flanvala{79.5}{0.8}{79.6} \\flanvala{80.5}{0.5}{80.7} & \\fewk{14} & \\tiny{7} \\\\\n    NQ & \\na & \\tfiveval{36.6} & 21.5 & 23.9            \\baselmvala{3.2}{22.1}{5} \\gptvala{14.6}{29.9}{[64]} \\flanvala{18.6}{2.7}{20.7} \\flanvala{27.2}{0.5}{27.6} & \\fewk{16} & \\tiny{10} \\\\\n    TQA {\\tiny (wiki)} & \\na & \\tfiveval{60.5} & 68.8 & 71.5      \\baselmvala{21.9}{63.3}{10} \\gptvala{64.3}{71.2}{[64]} \\flanvala{66.5}{2.6}{68.1} \\flanvala{66.5}{1.0}{67.3} & \\fewk{16} & \\tiny{10} \\\\\n    TQA {\\tiny (tfds-dev)} & \\na & \\tfiveval{51.0} & \\na & \\na      \\baselmvala{18.4}{55.1}{10} \\gptvala{\\na}{\\na}{\\na} \\flanvala{55.0}{2.3}{56.7} \\flanvala{57.2}{0.6}{57.8} & \\fewk{16} & \\tiny{10} \\\\\n    \\midrule \n    \\tasktype{Commonsense} \\\\\n    COPA & 50.0 & \\tfiveval{94.8} & 90.0 & 92.0              \\baselmvala{90.0}{89.0}{10} \\gptvala{91.0}{92.0}{[32]} \\flanvala{90.6}{2.0}{91.0} \\flanvala{88.5}{3.8}{87.0} & \\fewk{16} & \\tiny{8} \\\\\n    HellaSwag & 25.0 & \\bertlargeval{47.3} & 77.1 & 76.8     \\baselmvala{57.0}{58.8}{10} \\gptvala{78.9}{79.3}{[20]} \\flanvala{56.4}{0.5}{56.7} \\flanvala{59.4}{0.2}{59.2} & \\fewk{3} & \\tiny{8} \\\\\n    PIQA & 50.0 & \\bertlargeval{66.8} & 80.4 & 81.4          \\baselmvala{80.3$^*$}{80.2$^*$}{10} \\gptvala{81.0}{82.3}{[50]} \\flanvala{80.9$^*$}{0.8}{80.5$^*$} \\flanvala{82.1$^*$}{0.3}{81.7$^*$} & \\fewk{10} & \\tiny{8} \\\\\n    StoryCloze & 50.0 & \\bertlargeval{89.2} & 82.5 & 84.0    \\baselmvala{79.5}{83.7}{10} \\gptvala{83.2}{87.7}{[70]} \\flanvala{92.2}{1.3}{93.4} \\flanvala{93.3}{0.9}{94.7} & \\fewk{10} & \\tiny{8} \\\\\n    \\midrule \n    \\tasktype{Sentiment} \\\\\n    IMDB & 50.0 & \\bertlargeval{95.5} & \\na & \\na          \\baselmvala{76.9}{83.3}{1} \\gptvala{\\na}{\\na}{} \\flanvala{94.1}{0.4}{94.3} \\flanvala{94.8}{0.3}{95.0} & \\fewk{2} & \\tiny{7} \\\\\n    Sent140 & 50.0 & \\bertlargeval{87.0} & \\na & \\na   \\baselmvala{41.4}{63.3}{5} \\gptvala{\\na}{\\na}{} \\flanvala{69.9}{2.5}{73.5} \\flanvala{68.7}{1.2}{69.3} & \\fewk{16} & \\tiny{6} \\\\\n    SST-2 & 50.0 & \\tfiveval{97.5} & \\na & \\na              \\baselmvala{51.0}{92.3}{5} \\gptvala{71.6}{95.6}{[8]} \\flanvala{92.6}{1.7}{94.6} \\flanvala{94.4}{0.8}{94.6} & \\fewk{16} & \\tiny{8} \\\\\n    Yelp & 50.0 & \\bertlargeval{98.1} & \\na & \\na           \\baselmvala{84.7}{89.6}{3} \\gptvala{\\na}{\\na}{} \\flanvala{97.8}{0.2}{98.1} \\flanvala{97.9}{0.1}{98.0} & \\fewk{4} & \\tiny{7} \\\\\n    \\midrule \n    \\tasktype{Paraphrase} \\\\\n    MRPC & 50.0 & \\tfiveval{90.4} & \\na & \\na               \\baselmvala{53.7}{64.0}{5} \\gptvala{\\na}{\\na}{} \\flanvala{69.1}{1.3}{69.1} \\flanvala{67.5}{1.7}{67.2} & \\fewk{10} & \\tiny{10} \\\\\n    QQP & 50.0 & \\tfiveval{90.6} & \\na & \\na                \\baselmvala{34.9}{58.9}{3} \\gptvala{\\na}{\\na}{} \\flanvala{72.1}{6.8}{75.9} \\flanvala{73.5}{2.9}{75.9} & \\fewk{16} & \\tiny{7} \\\\\n    PAWS Wiki & 50.0 & \\tfiveval{91.9} & \\na & \\na          \\baselmvala{45.5}{53.5}{5} \\gptvala{\\na}{\\na}{} \\flanvala{61.5}{6.5}{69.4} \\flanvala{66.2}{0.9}{70.2} & \\fewk{10} & \\tiny{10} \\\\\n    \\midrule \n    \\tasktype{Coreference} \\\\\n    DPR & 50.0 & \\bertlargeval{84.8} & \\na & \\na            \\baselmvala{54.6}{57.3}{5} \\gptvala{\\na}{\\na}{} \\flanvala{60.3}{3.5}{66.8} \\flanvala{62.4}{1.6}{63.3} & \\fewk{16} & \\tiny{10} \\\\\n    Winogrande & 50.0 & \\bertlargeval{65.8} & 73.4 & 73.0     \\baselmvala{68.3}{68.4}{10} \\gptvala{70.2}{77.7}{[50]} \\flanvala{67.3}{2.5}{71.2} \\flanvala{72.3}{0.9}{72.8} & \\fewk{16} & \\tiny{10} \\\\\n    WSC273 & 50.0 & \\bertlargeval{70.0} & 86.8 & 83.9        \\baselmvala{81.0}{61.5}{5} \\gptvala{88.3}{88.5}{[32]} \\flanvala{80.8}{3.7}{\\na} \\flanvala{\\na}{\\na}{\\na} & \\fewk{\\na} & \\tiny{10} \\\\\n    \\midrule \n    \\tasktype{Read. Comp. w/ Commonsense} \\\\\n    CosmosQA & 25.0 & \\bertlargeval{67.1} & \\na & \\na      \\baselmvala{34.1}{33.8}{5} \\gptvala{\\na}{\\na}{} \\flanvala{58.4}{1.3}{60.6} \\flanvala{56.7}{1.3}{56.0} & \\fewk{5} & \\tiny{8} \\\\\n    ReCoRD & \\na & \\tfiveval{93.4} & 90.3 & 90.3              \\baselmvala{87.8$^*$}{87.6$^*$}{1} \\gptvala{90.2}{89.0}{[32]} \\flanvala{67.8$^*$}{3.0}{72.5$^*$} \\flanvala{77.0$^*$}{2.0}{79.0$^*$} & \\fewk{1} & \\tiny{10} \\\\\n    \n    \n    \\bottomrule\n    \\end{tabular}\n    \\caption{\n    Results for eight NLU task clusters. \n    All values shown are for accuracy (or exact match) except DROP, MultiRC, and SQuAD v1 and v2, which are F1.\n    \\explainkt\n    $^{a}$T5-11B,\n    $^{b}$BERT-large.\n    $^*$see data contamination (\\cref{sec:data_contamination}).\n    WSC273 \\citep{levesque2012winograd} does not have training or validation sets, and so we do not compute few-shot results for FLAN.\n    For Trivia QA (TQA), we report exact match (EM) on both the wikipedia subset of the dev set to compare with GPT-3, as well as the full TFDS dev set.\n    }\n    \\label{tab:nlu_table}\n\\end{table}\n\\endgroup\n\n\\clearpage\n\\section{Further Ablation Studies and Analysis}\n\\subsection{Datasets per Task Cluster \\& Templates per Dataset}\nOur primary hypothesis is that instruction tuning on a diverse set of tasks improves performance on unseen tasks.\n\\cref{subsec:finetuning_clusters} showed that adding more task clusters improves performance; here, we further explore whether adding additional datasets improves performance when the number of task clusters is held constant.\nWe use the same split as in \\cref{subsec:finetuning_clusters}, where the NLI, commonsense reasoning, and closed-book QA clusters are held-out, and seven other task clusters remain for instruction tuning.\nFor these seven task clusters, we instruction tune models using just one dataset per task cluster and using four datasets per task cluster (for task clusters that did not have four tasks, we just used all available tasks). \nIn addition, we simultaneously explore the role of the number of instruction templates per dataset; as mentioned in \\cref{subsec:tasks_and_templates}, for each dataset we manually composed ten instructional templates for instruction tuning. \nHere, we instruction tune models using 1, 4, and 10 templates per dataset.\n\n\\cref{fig:ablation-templates} shows these results.\nUsing more datasets per cluster improved performance by almost 10\\% on average across the three held-out clusters. \nUsing more templates per dataset, however, had a comparatively negligible effect on performance when there was one task per cluster, which disappeared when there were four tasks per cluster.\nThe small effect of templates is striking given our original motivation that composing ten templates per task would mitigate overfitting to any particular template. \nThis results serves to underscore, however, the unpredictability of finetuning large language models, as one hypothesis is that models at such scale do not easily overfit to a finetuning single task.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.55\\linewidth]{figs/templates-per-dataset.png}\n    \\vspace{-2mm}\n    \\caption{Effect of datasets per task cluster and templates per dataset on performance on three held-out clusters: NLI, commonsense reasoning, and closed-book QA.\n    Adding more datasets per task cluster substantially improves performance.\n    Using more templates per dataset, however, only had a very small effect on performance, which disappeared when there were sufficient dataset per task cluster.\n    }\n    \\label{fig:ablation-templates}\n\\end{figure}\n\n\\subsection{Role of instructions during finetuning}\\label{subsec:role_instructions}\nThe per-cluster results for the ablation study from \\cref{subsec:role-of-instructions} are shown in \\cref{tab:no_instructions}.\n\n\\begingroup\n\\setlength{\\tabcolsep}{4.5pt}\n\\begin{table}[h]\n    \\centering\n    \\begin{tabular}{ll cccc c}\n    \\toprule\n     & & \\multicolumn{5}{c}{Zero-shot performance on unseen task cluster} \\\\\n     \\cmidrule(lr){3-7} \n     \n    \\makecell[l]{Finetuning prompt} & \\makecell[l]{Inference prompt} & \\makecell[c]{NLI} & \\makecell[c]{\\footnotesize{Read.}\\vspace{-0.5mm}\\\\\\footnotesize{Comp.}} &  \\makecell[c]{\\footnotesize{Closed-}\\vspace{-0.5mm}\\\\\\footnotesize{Book QA}} & \\makecell[c]{Translation} & \\makecell[c]{\\footnotesize{\\underline{Four-Task}}\\vspace{-0.2mm}\\\\\\footnotesize{\\underline{Average}}}  \\\\\n     \\midrule\n    \\makecell[l]{Natural instructions \\\\(= \\flan{})} & Natural instructions & 56.2 & 77.4 & 56.6 & 30.7 & 55.2 \\\\\n     \\midrule\n     No template & Natural instructions & 50.5 & 58.2 & 25.5 & 15.0 & 37.3 \\\\\n     Task/dataset name & Natural instructions & 52.8 & 63.0 & 44.8 & 25.9 & 46.6 \\\\\n     Task/dataset name & Task/dataset name & 60.2 & 64.9 & 40.8 & 21.9 & 47.0 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\n    Ablation study result using models where instructions are removed from the finetuning process. In ``no template,'' only inputs and outputs are given, which does not distinguish among tasks during multi-task finetuning. In ``task/dataset name'', inputs during multi-task finetuning are prepended with the name of the task and dataset (e.g., \\textit{``[Translation: WMT'14 to French] The dog runs''}) NLI datasets: ANLI R1--R3, CB, and RTE; reading comprehension datasets: BoolQ, MultiRC, and OpenbookQA; closed-book QA datasets: ARC-c, ARC-e, NQ, and TQA; translation datasets: WMT'14 Fr$\\leftrightarrow$En, WMT'16 De$\\leftrightarrow$En, and WMT'16 Ro$\\leftrightarrow$En. \n    Notably, training with task/dataset name achieved a high NLI score largely because it achieved a score of 83.9 on the CB dataset, for which the validation set only has 56 examples (\\flan{} also gets 83.9 with the best dev template, but the average template was only 64.1).\n    }\n    \\label{tab:no_instructions}\n\\end{table}\n\\endgroup\n\\subsection{Further Analysis: Instruction Tuning Facilitates Prompt Tuning}\n\nThe per-dataset results for the analysis in \\cref{subsec:prompt_tuning} are given in \\cref{tab:prompt_tuning}.\nAs the above tasks are all classification, further work in this direction might include tasks such as summarization or question answering, or try to finetune the model using the supervised datasets.\n\n\\begingroup\n\\setlength{\\tabcolsep}{3.5pt}\n\\begin{table}[h]\n    \\centering\n    \\begin{tabular}{ll cccc cccc cc}\n    \\toprule\n     & & \\multicolumn{8}{c}{\\textsc{Prompt Tuning Analysis}} \\\\\n     \\cmidrule(lr){3-10} \n     \n    & \\makecell[c]{\\scriptsize{Prompt tuning}\\vspace{-1.2mm}\\\\\\scriptsize{train.~examples}} & \\datasetacc{BoolQ} & \\datasetacc{CB} & \\datasetacc{CoPA} & \\datasetfone{MultiRC} & \\datasetacc{ReCoRD} & \\datasetacc{RTE} & \\datasetacc{WiC} & \\datasetacc{WSC}  \\\\\n     \\midrule\n     \\baselm{} & \\multirow{2}{*}{32} & 55.5 & 55.4 & 87.0 & 65.4 & 78.0 & 52.4 & 51.6 & 65.4 \\\\\n     \\flan{} & & 77.5 & 87.5 & 91.0 & 76.8 & 80.8 & 83.0 & 57.8 & 70.2 \\\\\n     \\midrule\n     \\baselm{} & \\multirow{2}{*}{\\makecell[l]{\\footnotesize{full}\\vspace{-0.5mm}\\\\\\footnotesize{dataset}}} & 82.8 & 87.5 & 90.0 & 78.6 & 84.8 & 82.0 & 54.9 & 72.7 \\\\\n     \\flan{} & & 86.3 & 98.2 & 94.0 & 83.4 & 85.1 & 91.7 & 74.0 & 86.5 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\n    \\flan{} (instruction tuning) responds better to continuous inputs attained via prompt tuning than \\baselm{} (no instruction tuning).\n    When prompt tuning on a given dataset, no tasks from the same cluster as that dataset were seen during instruction tuning.\n    }\n    \\label{tab:prompt_tuning}\n\\end{table}\n\\endgroup\n\\section{Data Contamination Analysis}\\label{sec:data_contamination}\n\nOne reasonable concern is that since the pretraining corpus of \\flan{} has more than 2 trillion tokens, it is possible that examples from a given evaluation dataset may have already been seen verbatim by the model during pre-training, hence inflating the performance of our purported zero-shot model.\nTo this end, like GPT-3 \\citep{brown2020language}, we perform post-hoc data contamination analysis to investigate whether the performance of the model is in fact inflated by evaluating on examples that occurred in the pretraining dataset.\n\nOur data contamination procedure follows the setup of \\citet{brown2020language}, which, for each benchmark, produces a ``clean'' version that removes all potentially leaked examples, defined as examples for which any $n$-gram ($n$ varies per dataset but is roughly 13) overlapped with anything in the pretraining corpus.\nWe use the same $n$ per dataset as \\citet{brown2020language} and also split on spaces.\nWe then evaluate our model on this clean subset, comparing against model performance on the original dataset (clean + dirty).\nLower performance on the clean subset would suggest that data contamination leads to inflated results. \n\n\\cref{fig:data-contamination} summarizes these results, with the exact numbers given in \\cref{tab:data_contamination}. \nWe see several trends very similar to those in the GPT-3 paper: \n(1) many datasets had a substantial number of examples that overlapped with the pretraining data, \n(2) across all datasets, we do not see a correlation that evaluating on clean data does worse than evaluating on the total dataset, and \n(3) as datasets had fewer clean examples, there was higher variance in the percent change in performance (likely due to a smaller number of clean examples).\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/data-contamination.pdf}\n    \\vspace{-2mm}\n    \\caption{Like GPT-3, we also measured performance on cleaned versions of our datasets, which had high confidence to be unseen in the pretraining data of FLAN. \n    We do not see a correlation that FLAN performed better on evaluation sets for which examples occurred more often in the pretraining data.\n    When the percent of clean data is very small, there are fewer examples for computing the clean performance, which leads to high variance. \n    }\n    \\label{fig:data-contamination}\n\\end{figure}\n\nLike GPT-3, we also found that DROP and SQuADv2 had almost total overlap with the pretraining data.\nWe follow their procedure of manually inspecting the data, and find that most overlapping $n$-grams were only in the contexts of examples (99.6\\% for DROP and 97.2\\% for SQuADv2).\nOverlaps never occurred in both the question and answer for DROP, and only occurred for both the question and answer for SQuADv2 in 5 of the 11,153 evaluation examples.\nHence, for these two datasets, the model gains only background information and cannot memorize the answer to any specific questions (aside from the five examples in SQuADv2).\n\nANLI R1 and R2 \\citep{anli} also had almost complete data contamination, to a much higher degree than GPT-3. \nUpon further inspection, we see that most overlaps occur in example contexts and not hypotheses (97.3\\% for ANLI R1 and 98.2\\% for ANLI R2). \nAs ANLI R1 and R2 are based entirely from Wikipedia examples (R3 is not), we posit that this higher degree of contamination in our pretraining dataset compared with GPT-3's is potentially due to using a more-recent version of Wikipedia that includes the contexts used in ANLI R1 and R2 (which were collected in 2019). \nBecause seeing a particular context in pretraining does not help with the NLI task given a new, unseen sentence, we think it is unlikely that these overlaps affected performance on the two datasets.\n\nOf the remaining datasets, only ReCoRD and PIQA had a clean subset performance that was lower than the overall evaluation set performance by more than 1\\%. \nThese two datasets are language modeling (i.e., ``what's the best continuation of this sentence?''), and so it is more likely compared with previous tasks that seeing a complete sentence in the pretraining data could help the model predict the right answer in downstream evaluations.\nFor PIQA, both the goal and solution had overlaps in 93 of the 1,838 evaluation examples, and for ReCoRD, the query had overlaps in 2,320 of the 10,000 training examples.\nWe hence mark these results with an asterisk $^*$ in \\cref{tab:nlu_table}.\n\\citet{brown2020language} also reported substantial contamination rates for these two datasets (61\\% dirty for ReCoRD and 29\\% for PIQA), and also mark PIQA with an asterisk.\n\nAs this overlap analysis follows that performed in \\citet{brown2020language}, we reiterate the same caveats: the conservative nature of our $n$-gram matching procedure likely introduces additional false positives; there are no guarantees that the clean subset is drawn from the same distribution as the overall subset; and, accurately detecting test contamination is a relatively new research area without established best practices.\nMoreover, as our pretraining corpus is almost five times larger than that used for GPT-3 (which was 500B tokens), it is possible that there are more false positives in detecting dirty data.\n\n\\begingroup\n\\setlength{\\tabcolsep}{3.5pt}\n    \\begin{table}[h]\n        \\centering\n        \\small\n        \\begin{tabular}{ll cc cc cc}\n        \\toprule\n        Dataset & Metric & \\makecell[c]{Total \\\\ count} & \\makecell[c]{Total \\\\ acc/F1/BLEU} &  \\makecell[c]{Clean \\\\ count} & \\makecell[c]{Clean \\\\ acc/F1/BLEU} & \\% clean & \\makecell[c]{\\% Diff \\\\ (clean $-$\\\\ overall)} \\\\\n        \\midrule\n        DROP                   & F1       & 9,536  & 22.4 & 61   & 33.0 & 0.6  & 47.4  \\\\\n        SQuADv2              & F1       & 11,873 & 41.3 & 106  & 38.7 & 0.9  & -6.2  \\\\\n        ANLI R1               & acc & 1,000  & 48.1 & 14   & 57.1 & 1.4  & 18.8  \\\\\n        ANLI R2               & acc & 1,000  & 42.9 & 21   & 38.1 & 2.1  & -11.2 \\\\\n        ReCoRD                 & acc & 10,000 & 4.6  & 3,203 & 4.5  & 32.0 & -2.7  \\\\\n        MultiRC                & acc & 4,848  & 75.4 & 1,972 & 75.7 & 40.7 & 0.5   \\\\\n        PIQA                   & acc & 1,838  & 23.7 & 896  & 23.3 & 48.7 & -1.7  \\\\\n        ANLI R3               & acc & 1,200  & 44.2 & 718  & 45.3 & 59.8 & 2.5   \\\\\n        HellaSwag              & acc & 10,042 & 28.5 & 6,578 & 28.7 & 65.5 & 0.7   \\\\\n        RTE                    & acc & 2,77   & 84.1 & 183  & 84.2 & 66.1 & 0.0   \\\\\n        WMT'14 En$\\rightarrow$Fr            & BLEU     & 3,003  & 31.3 & 2,243 & 31.5 & 74.7 & 0.9   \\\\\n        WMT'14 Fr$\\rightarrow$En            & BLEU     & 3,003  & 34.0 & 2,243 & 34.1 & 74.7 & 0.2   \\\\\n        BoolQ                & acc & 3,270  & 76.5 & 2,515 & 76.3 & 76.9 & -0.4  \\\\\n        TQA (tfds-dev)             & F1       & 11,313 & 62.2 & 8,731 & 62.0 & 77.2 & -0.2  \\\\\n        ARC Easy              & acc & 2,365  & 79.5 & 1,888 & 79.0 & 79.8 & -0.6  \\\\\n        ARC Challenge         & acc & 1,165  & 63.1 & 983  & 64.2 & 84.4 & 1.7   \\\\\n        OpenbookQA             & acc & 500   & 74.6 & 425  & 74.8 & 85.0 & 0.3   \\\\\n        WMT'16 En$\\rightarrow$De & BLEU     & 2,999  & 22.7 & 2,569 & 23.0 & 85.7 & 1.4   \\\\\n        WMT'16 De$\\rightarrow$En & BLEU     & 2,999  & 38.6 & 2,569 & 38.7 & 85.7 & 0.2   \\\\\n        WMT'16 En$\\rightarrow$Ro & BLEU     & 1,999  & 15.5 & 1,752 & 15.4 & 87.6 & -0.7  \\\\\n        WMT'16 Ro$\\rightarrow$En & BLEU     & 1,999  & 36.7 & 1,752 & 36.8 & 87.6 & 0.1   \\\\\n        COPA                   & acc & 100   & 88.0 & 91   & 87.9 & 91.0 & -0.1  \\\\\n        CB                     & acc & 56    & 41.1 & 53   & 41.5 & 94.6 & 1.1   \\\\\n        NQ     & F1       & 3,610  & 24.5 & 3,495 & 24.3 & 96.8 & -0.5  \\\\\n        StoryCloze           & acc & 1,871  & 92.1 & 1,864 & 92.1 & 99.6 & 0.0   \\\\\n        Winogrande             & acc & 1,267  & 39.4 & 1,265 & 39.4 & 99.8 & 0.2   \\\\\n        \\bottomrule\n        \\end{tabular}\n    \\caption{\n    Overlap statistics for the subset of datasets that are also used in GPT-3, sorted from dirtiest to cleanest. \n    An evaluation example was dirty if it had any $n$-gram collision with the pretraining corpus. \n    We computed these results for FLAN's performance using only a single template for each dataset, so they differ slightly compared with the results for average performance over all templates.\n    }\n    \\label{tab:data_contamination}\n    \\end{table}\n\\endgroup\n\\clearpage \n\\section{Extended Related Work}\\label{sec:extended_related_work}\n\n\\subsection{Language Models and Multi-task Learning}\nOur work is broadly inspired by a long line of prior work on language models for NLP applications \\citep[][\\textit{inter alia}]{dai2015semi,peters-etal-2018-deep,howard-ruder-2018-universal,radford2018improving,radford2019language}. \nInstruction tuning can be seen as a formulation of multitask learning (MTL), which is an established area within deep learning \\citep[][\\textit{inter alia}]{collobert2011natural,luong2015multi,ruder2017overview,velay2018seq2seq,clark2019bam,liu2019multi}---see \\citet{Worsham2020MultitaskLF} for a recent survey on MTL for NLP. \nDiffering from prior MTL work which focuses on performance improvements across training tasks \\citep{raffel2019exploring,aghajanyan2021muppet} or to new domains \\citep{axelrod-etal-2011-domain}, our work is motivated by improving zero-shot generalization to tasks that were not seen in training.\n\n\\subsection{Zero-Shot Learning and Meta-Learning}\nOur work also falls in the well-established category of zero-shot learning, which has historically been used to refer to classifying instances among a set of unseen categories \\citep[][\\textit{inter alia}]{lampert2009learning,romera2015embarrassingly,srivastava-etal-2018-zero,yin-etal-2019-benchmarking}.\nIn NLP, zero-shot learning work also includes translating between unseen language pairs \\citep{johnson-etal-2017-googles,pham-etal-2019-improving}, language modeling on unseen languages \\citep{lauscher-etal-2020-zero}, as well as various NLP applications \\citep{liu-etal-2019-reconstructing,corazza-etal-2020-hybrid,wang2021zl3}.\nMost recently, the emergent ability of language models \\citep{brown2020language} has led to increased interest in how models generalize to unseen tasks, the definition of zero-shot learning used in our paper.\nIn addition, meta-learning \\citep[][\\textit{inter alia}]{finn2017model,vanschoren2018meta} also broadly tries to train models that adapt quickly to unseen tasks, typically based on a few examples.\n\n\\subsection{Prompting}\nInstruction tuning leverages the intuition that language models at scale contain substantial world knowledge and can perform a range of NLP tasks \\citep[][see also \\cite{Bommasani2021OnTO}]{brown2020language}.\nAnother line of work that shares this goal prompts models with continuous inputs optimized via backpropagation to substantially improve performance \\citep{li-liang-2021-prefix,lester-prompt-tuning,qin-eisner-2021}, as well as work that prompts models to produce specialized outputs \\citep{wei2022chain}.\nAlthough the success of these approaches depends heavily on model scale \\citep{lester-prompt-tuning}, for which large models can be costly to serve, the ability of a single large model to perform many tasks slightly eases this burden.\nAs shown by our experiments in \\cref{subsec:prompt_tuning}, prompt tuning is an orthogonal method for which instruction tuning can additionally improve performance.\n\\citet{reif2021recipe} is similar to our work in that they also use related tasks to improve zero-shot learning, though they differ by only using related tasks in the context (and not finetuning), and focus on the application of text style transfer.\n\nOur work shares similar motivations with prompting in that we use inference-time text interactions to prompt a single model, without creating separate checkpoints for each task. \nWhereas prompting work such as GPT-3 uses prompt engineering to write prompts that intentionally mimic text that is likely to be seen during pretraining (e.g., for MultiRC GPT-3 tries a prompt that mimics a test with an answer key), we hope that finetuning models to respond to natural language instructions instead of completing a sentence will make such large models more accessible to non-technical users.\n\n\\subsection{Finetuning Large Language Models}\nFinetuning pretrained language models is a well-established method in NLP, with much of the work so far occurring on models in the range of 100M to 10B parameters \\cite[][\\textit{inter alia}]{dai2015semi,devlin-etal-2019-bert,raffel2019exploring,lewis-etal-2020-bart}.\nFor models of O(100B) parameters, recent work has finetuned task-specific models for program synthesis \\citep{Austin2021ProgramSW,chen2021evaluating}, summarization \\citep{wu2021recursively}, as well as improved bias and fairness behavior \\citep{solaiman2021process}.\nIn addition to the traditional ``dense'' models, sparse mixture of experts (MoE) models of up to more than 1T parameters have been trained and finetuned \\citep{lepikhin2020gshard,fedus2021switch}.  \nCompared with this prior work that finetunes and evaluates on the same downstream task, our setup studies the effect of instruction tuning on ability to perform unseen tasks.\n\n\\subsection{Multi-task Question Answering}\nThe instructions we use for instruction tuning are similar to QA-based task formulation research, which aims to unify NLP tasks by casting them as question-answering over a context.\nFor instance, \\citet{mccann2018natural} cast ten NLP tasks as QA and train a model on a collection of tasks formulated with natural language prompts;\nthey report transfer learning gains on finetuning tasks as well as zero-shot domain adaptation results on SNLI \\citep{bowman-etal-2015-large} and Amazon/Yelp Reviews \\citep{amazonyelp2015}.\nWhile \\citet{mccann2018natural} does not leverage unsupervised pre-training and only reports zero-shot transfer to unseen domains, our work uses a pretrained LM and focuses on zero-shot performance on unseen task clusters.\nUnifiedQA \\citep{khashabi-etal-2020-unifiedqa} shows similar transfer learning gains as \\citet{mccann2018natural} across 20 datasets and reports good generalization to unseen tasks across four types of QA. \nFocusing on binary text classification, \\citet{zhong2021meta} finetune T5-770M on 43 tasks phrased as yes/no questions and study the zero-shot performance on unseen tasks. \nIn comparison, our paper is much larger in scope, empirically demonstrating the idea on a wide range of tasks with a much larger model.\nOther work has used QA-based task formulation for more-targeted applications including semantic role labeling \\citep{he-etal-2015-question}, relation extraction \\citep{levy-etal-2017-zero}, coreference resolution \\citep{wu-etal-2020-corefqa} and named entity recognition \\citep{li-etal-2020-unified} as question answering.\n\n\\subsection{Instructions-Based NLP}\nRecent improvements in the capabilities of language models have led to increased interest in a nascent area of instructions-based NLP \\citep[][and see \\citet{mccarthy1960programs}]{goldwasser2014learning}.\n\\citet{schick-schutze-2021-exploiting} \\citep[also see][]{gao-etal-2021-making,tam2021adapet} use task descriptions in cloze-style phrases to help language models assign soft labels for few-shot and semi-supervised learning, though this line of work finetunes new checkpoints for each downstream task.\n\\citet{efrat2020turking} evaluated GPT-2 \\citep{radford2019language} on simple tasks ranging from retrieving the $n$th word of a sentence to generating examples for SQuAD, concluding that GPT-2 performs poorly across all tasks.\n\nIn terms of the setup of finetuning on a large number of tasks and evaluating on unseen tasks, two recent papers are similar to ours. \n\\citet{mishra2021natural} finetune BART \\citep{lewis-etal-2020-bart} using instructions and few-shot examples for tasks such as question answering, text classification, and text modification, and find that this few-shot finetuning with instructions improves performance on unseen tasks.\n\\citet{ye2021crossfit} introduce a setup for cross-task few-shot learning, finding that multi-task meta-learning using MAML \\citep{finn2017model} improves the few-shot capabilities of BART on unseen downstream tasks.\nOur work differs from these two papers in that we focus on zero-shot learning, for which we observe the crucial importance of model scale (FLAN is 1,000x larger than BART-base).\n\nPerhaps the papers most related to ours are the recent \\citet{sanh2021multitask} and \\citet{min2021metaicl}, which were released after our initial preprint. \\citet{min2021metaicl} finetunes GPT-2 Large (770M parameters) to be a few-shot learner, which is the same approach as our experiment in Section 4.3. Similar to our conclusions, they also observe that including few-shot exemplars and instruction tuning are complementary ways to improve performance. \\citet{sanh2021multitask} propose to finetune T5-11B to respond to prompts, and they also report performance improvements on zero-shot learning. These two papers and our work all study finetuning with instructions, but, as noted by \\citet{min2021metaicl}, it is hard to directly compare results, due to differing model sizes, model types (decoder-only vs encoder-decoder), pretraining data, task mixtures, and type of instructions (\\citet{sanh2021multitask} say that their instructions are more diverse).\n\nFinally, OpenAI has a model called InstructGPT \\citep{ouyang2022instructgpt}. \nInstructGPT uses human anntations to guide desired model behavior, both via finetuning and reinforcement learning, finding that InstructGPT is preferred by human rathers compared with unmodified GPT-3.\n\n\\clearpage\n\\section{Frequently Asked Questions}\\label{sec:faq}\n\n\\textbf{How do the \\flan{} instructions differ from GPT-3 or T5 prompts?}\n\nGPT-3 prompting is done in a way such that the prompt looks like data that the model has been pretrained on, and the model finishes the continuation. \nT5 prompts are mostly just a tag for the dataset, which would not work in the zero-shot setting. \nIn contrast, the prompts that we use for \\flan{} are similar to what would be used to ask a human to perform the task.\n\nFor instance, given an input for an NLI task, these would be the prompts.\n\n\\textit{T5 prompt:} \\\\\n\\textttsmall{cb hypothesis: At my age you will probably have learnt one lesson. premise: It's not certain how many lessons you'll learn by your thirties.}\n\n\\textit{GPT-3 prompt:} \\\\\n\\textttsmall{At my age you will probably have learnt one lesson.} \\\\\n\\textttsmall{question: It's not certain how many lessons you'll learn by your thirties. true, false, or neither?\nanswer:}\n\n\\textit{FLAN prompt:} \\\\\n\\textttsmall{Premise: At my age you will probably have learnt one  lesson.}\\\\\n\\textttsmall{Hypothesis: It's not certain how many lessons you'll learn by your thirties.}\\\\\n\\textttsmall{Does the premise entail the hypothesis?}\n\nSo because \\flan{} prompts are formulated as responding to an instruction, they do not work well for pretrained language models without finetuning. \nPerformance was near zero for most generation tasks. \nFor instance, given the input \\textit{```The dog runs.' Translate this sentence to French.''}, \\baselm{} continues with \\textit{''The dog runs after the cat''} instead of actually translating the sentence. Hence, we used the established GPT-3 prompts for our \\baselm{} baselines.\n\n\\vspace{4mm}\n\\textbf{What are some limitations/failure cases of \\flan{}?}\n\nWhile we qualitatively find that \\flan{} responds well to most tasks, it does fail on some simple tasks. \nFor instance, as shown in \\cref{fig:examples-failures}, \\flan{} fails at the very simple task of returning the second word in a sentence, and also incorrectly translates a question to Danish when asked to answer the question in Danish.\nAdditional limitations include a context length of only 1024 tokens (which is not enough for most summarization tasks), and that the model was mostly trained on English data.\n\n\\vspace{4mm}\n\\textbf{Can \\flan{} be used when large amounts of training data are available?}\n\nIn this work, we focus on cross-task generalization to zero-shot tasks, but we also believe that instruction tuning could result in positive task transfer among seen tasks, depending on the mixture of tasks (though we leave this for future work). \nIn \\cref{subsec:prompt_tuning}, where we apply prompt tuning to the \\flan{} checkpoint, we see promising results that indicate positive task transfer in a supervised setting.\n\n\\vspace{4mm}\n\\textbf{Are the ten unique templates per dataset or per task cluster?} \n\nThe ten unique templates are for each dataset and not for a task cluster. This is because datasets in the same task cluster often differed slightly (e.g., \\textit{``is this movie review positive''} vs \\textit{``is this yelp review positive''}).\n\n\\vspace{4mm}\n\\textbf{In \\cref{fig:scale-ablation}A, why does the untuned \\baselm{} model see worse performance with more parameters for reading comprehension and sentiment analysis?}\n\nFor context, \\cref{fig:scale-ablation}A is a check of correctness for \\cref{fig:scale-ablation}B. \\cref{fig:scale-ablation}A confirms that scale improves performance for tasks that were seen during instruction tuning, as expected. The untuned \\baselm{} model performance in \\cref{fig:scale-ablation}A is shown just for completeness.\n\nNonetheless, the fact that scale does not always improve zero-shot performance of untuned \\baselm{} is an interesting artifact. Initially, we were surprised, because \\citet{brown2020language} shows that scale improves performance across a large number of tasks in aggregate.\n\nIt turns out that scale does not improve performance for certain tasks. This is especially true for zero-shot learning, and we think that this happens to be the case for the reading comprehension and sentiment analysis tasks we evaluate. The GPT-3 paper itself similarly reports that zero-shot performance on BoolQ and DROP decreases from 13B to 175B parameters. The GPT-3 paper does not show results on sentiment analysis, but \\citet{holtzman-etal-2021-surface} find that zero-shot performance on SST-2 also gets worse from 13B to 175B parameters. Hence, this artifact is consistent across both GPT-3 and the models we use.\n\nThis artifact is certainly worth further study, but is outside the scope of instruction tuning. \nIdeally, we would have performed the \\cref{fig:scale-ablation} ablation with cross-validation instead of a single split, which likely would have smoothed out that artifact.\n\n\\section{Qualitative Examples}\\label{sec:qualitative}\n\nThis section shows qualitative examples of \\flan{} responding to various prompts.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.96\\linewidth]{figs/flan-examples-sentiment.pdf}\n    \\vspace{-3mm}\n    \\caption{For sentiment analysis, \\flan{} changes the answer appropriately when the question is flipped.}\n    \\label{fig:examples-sentiment}\n\\end{figure}\n\n\\begin{figure}[h]\n    \\centering\n    \\vspace{2mm}\n    \\includegraphics[width=0.96\\linewidth]{figs/flan-examples-qa.pdf}\n    \\vspace{-3mm}\n    \\caption{For question answering, \\flan{} can answer a question in another language when instructed to do so. \\samplingexplanation{}}\n    \\label{fig:examples-qa}\n\\end{figure}\n\n\\begin{figure}[h]\n    \\centering\n    \\vspace{2mm}\n    \\includegraphics[width=0.96\\linewidth]{figs/flan-examples-rules.pdf}\n    \\vspace{-3mm}\n    \\caption{\\flan{} can perform zero-shot rule-based manipulations.}\n    \\label{fig:examples-rules}\n\\end{figure}\n\n\\begin{figure}[h]\n    \\centering\n    \\vspace{2mm}\n    \\includegraphics[width=0.96\\linewidth]{figs/flan-examples-recommendations.pdf}\n    \\vspace{-3mm}\n    \\caption{\\flan{} can make zero-shot recommendations. \\samplingexplanation{}}\n    \\label{fig:examples-recommendations}\n\\end{figure}\n\n\\begin{figure}[h]\n    \\centering\n    \\vspace{2mm}\n    \\includegraphics[width=0.96\\linewidth]{figs/flan-examples-intents.pdf}\n    \\vspace{-4mm}\n    \\caption{\\flan{} can be used in a zero-shot fashion to generate data, such as utterances that are consistent with a given intent. \\samplingexplanation{}}\n    \\label{fig:examples-intents}\n\\end{figure}\n\n\\begin{figure}[h]\n    \\centering\n    \\vspace{2mm}\n    \\includegraphics[width=0.96\\linewidth]{figs/flan-examples-query.pdf}\n    \\vspace{-4mm}\n    \\caption{\\flan{} can be used for zero-shot query expansion. \\samplingexplanation{}}\n    \\label{fig:examples-query}\n\\end{figure}\n\n\\begin{figure}[h]\n    \\centering\n    \\vspace{2mm}\n    \\includegraphics[width=0.96\\linewidth]{figs/flan-examples-writing.pdf}\n    \\vspace{-4mm}\n    \\caption{\\flan{} can perform zero-shot tasks relevant to assisted-writing applications. \\samplingexplanation{}}\n    \\label{fig:examples-writing}\n\\end{figure}\n\n\\begin{figure}[h]\n    \\centering\n    \\vspace{2mm}\n    \\includegraphics[width=0.96\\linewidth]{figs/flan-examples-lexigenesis.pdf}\n    \\vspace{-3mm}\n    \\caption{\\flan{} can be used for zero-shot word formation. \\samplingexplanation{}}\n    \\label{fig:examples-lexigenesis}\n\\end{figure}\n\n\\begin{figure}[h]\n    \\centering\n    \\vspace{2mm}\n    \\includegraphics[width=0.96\\linewidth]{figs/flan-examples-open.pdf}\n    \\vspace{-3mm}\n    \\caption{Open-ended generation tasks by \\flan{}. The carrot story was from sampling sixteen outputs with a minimum length of 150 and choosing the highest probability output.}\n    \\label{fig:examples-open}\n\\end{figure}\n\n\\begin{figure}[h]\n    \\centering\n    \\vspace{2mm}\n    \\includegraphics[width=0.96\\linewidth]{figs/flan-examples-failures.pdf}\n    \\vspace{-3mm}\n    \\caption{Example failure cases for \\flan{}. Left: \\flan{} fails to perform a simple task of returning the $n$th word. Right: \\flan{} translates a question instead of answering it. \\samplingexplanation{}}\n    \\label{fig:examples-failures}\n\\end{figure}\n\n\\clearpage\n\\section*{Changes from V4 to V5}\n\\begin{itemize}\n    \\item Replaced the tables in the main figure with a figure, which takes up less space and focuses on zero-shot performance. \\item Added GLaM 64B/64E as a baseline.\n    \\item Moved the ablation about the role of instructions, as well as prompt tuning, into the main paper (and condensed the figures).\n\\end{itemize}\n\n\\section*{Changes to V4 from V3}\n\\begin{itemize}\n    \\item We added a Frequently Asked Questions section (\\cref{sec:faq}).\n    \\item We added a section with qualitative examples (\\cref{sec:qualitative}).\n    \\item We added an additional ablation study on the role of instructions during finetuning (\\cref{subsec:role_instructions}).\n    \\item We updated the related work (\\cref{sec:extended_related_work}) with manuscripts posted on arxiv since our initial preprint.\n\\end{itemize}\n\n\\section*{Changes to V3 from V2}\n\\begin{itemize}\n    \\item The number of tokens used in pretraining was corrected from 2.81T to 2.49T tokens.\n\\end{itemize}\n\n\\section*{Changes to V2 from V1}\n\\begin{itemize}\n    \\item We updated the terminology to ``datasets'' and ``task clusters.''\n    \\item We renamed the previous ``open-domain QA'' task cluster to ``closed-book QA.''\n    \\item We extended the related work section and moved it to the \\cref{sec:extended_related_work}, using a shorter version in the main body.\n    \\item We added \\flan{} and \\baselm{} results for additional datasets for which GPT-3 results were not reported.\n    \\item For TriviaQA, v1 reported results on the tfds dev set of 11,313 examples. GPT-3 actually evaluates on the wikipedia dev set of 7,993 examples, so we ran an additional evaluation on that dev set in order to compare with GPT-3's performance. Zero-shot \\flan{} now beats zero-shot GPT-3 on that task (and therefore on 20 of 25 tasks). We still show the original result in \\cref{tab:nlu_table}, though there is no GPT-3 result to compare with.\n    \\item We moved commonsense reasoning and coreference resolution from the main body to the Appendix.\n    \\item We moved prompt tuning from the main body to \\cref{subsec:prompt_tuning}.\n    \\item We added data contamination analysis (\\cref{sec:data_contamination}).\n    \\item We added few-shot instruction tuning (\\cref{subsec:finetune}).\n    \\item We cited additional datasets in \\cref{task_details}.\n    \\item The number of tokens used in pretraining was corrected from 2.81T to 2.49T tokens.\n\\end{itemize}\n\n\\clearpage \n\\section{Tasks and Datasets}\\label{task_details}\n\nThis appendix further details the datasets that we use in this paper.\nWe group datasets into one of the following task clusters:\n\n\\begin{itemize}[leftmargin=*]\n    \\itemsep0em \n    \\item \\textbf{Natural language inference} concerns how two sentences relate, typically asking, given a first sentence, whether a second sentence is true, false, or possibly true.\n    We use the following datasets:\n    \\begin{enumerate}\n        \\item ANLI \\citep{anli}\n        \\item CB \\citep{de2019commitmentbank}\n        \\item MNLI \\citep{N18-1101}\n        \\item QNLI \\citep{rajpurkar-etal-2018-know}\n        \\item SNLI \\citep{bowman-etal-2015-large}\n        \\item WNLI \\citep{levesque2012winograd}\n        \\item RTE \\citep{10.1007/11736790_9,haim2006second,giampiccolo-etal-2007-third,bentivogli2009fifth}\n    \\end{enumerate}\n    \\item \\textbf{Reading comprehension} tests the ability to answer a question when given a passage that contains the answer. \n    We use the following datasets:\n    \\begin{enumerate}\n        \\item BoolQ \\cite{clark-etal-2019-boolq}\n        \\item DROP \\citep{Dua2019DROP}\n        \\item MultiRC \\citep{khashabi-etal-2018-looking}\n        \\item OBQA \\citep{mihaylov-etal-2018-suit}\n        \\item SQuADv1 \\citep{rajpurkar-etal-2016-squad}\n        \\item SQuADv2 \\citep{rajpurkar-etal-2018-know}\n    \\end{enumerate}\n    \\item \\textbf{Commonsense reasoning} evaluates the ability to perform physical or scientific reasoning with an element of common sense. \n    We use the following datasets:\n    \\begin{enumerate}\n        \\item COPA \\citep{SSS112418}\n        \\item HellaSwag \\citep{zellers-etal-2019-hellaswag}\n        \\item PiQA \\citep{Bisk2020}\n        \\item StoryCloze \\citep{mostafazadeh-etal-2016-corpus}\n    \\end{enumerate}\n    \\item \\textbf{Sentiment analysis} is a classic NLP task aims to understand whether a piece of text is positive or negative. \n    We use the following datasets:\n    \\begin{enumerate}\n        \\item IMDB \\citep{maas-EtAl:2011:ACL-HLT2011}\n        \\item Sentiment140 \\citep{go2009twitter}\n        \\item SST-2 \\citep{socher-etal-2013-recursive}\n        \\item Yelp \\citep{yelpdataset}\n    \\end{enumerate}\n    \\item \\textbf{Closed-book QA} asks models to answer questions about the world without specific access to information that contains the answer. \n    We use the following datasets:\n    \\begin{enumerate}\n        \\item ARC \\citep{clark2018think}\n        \\item NQ \\citep{orqa,kwiatkowski2019natural}\n        \\item TriviaQA \\cite{JoshiTriviaQA2017}\n    \\end{enumerate}\n    \\item \\textbf{Paraphrase detection} asks a model to determine whether two sentences are semantically equivalent.\\footnote{Although paraphrasing can be seen as positive entailment in both directions, it has been distinct from NLI in the academic literature.}\n    We use the following datasets:\n    \\begin{enumerate}\n        \\item MRPC \\citep{dolan-brockett-2005-automatically}\n        \\item QQP \\citep[see]{wang-etal-2018-glue}\n        \\item Paws Wiki \\citep{zhang-etal-2019-paws}\n    \\end{enumerate}\n    \\item \\textbf{Coreference resolution} tests the ability to identify expressions of the same entity in some given text.\n    We use the following datasets:\n    \\begin{enumerate}\n        \\item DPR \\citep{rahman-ng-2012-resolving}\n        \\item Winogrande \\citep{sakaguchi2020winogrande}\n        \\item WSC273 \\citep{levesque2012winograd}\n    \\end{enumerate}\n    \\item \\textbf{Reading comprehension with commonsense} combines elements of both reading comprehension with commonsense.\n    We use the following datasets:\n    \\begin{enumerate}\n        \\item CosmosQA \\citep{huang-etal-2019-cosmos}\n        \\item ReCoRD \\citep{DBLP:journals/corr/abs-1810-12885}\n    \\end{enumerate}\n    \\item \\textbf{Struct to text} tests the ability to describe some structured data using natural language. \n    We use the following datasets:\n    \\begin{enumerate}\n        \\item CommonGen \\citep{lin-etal-2020-commongen}\n        \\item DART \\citep{nan-etal-2021-dart}\n        \\item E2ENLG \\citep{dusek-etal-2019-semantic}\n        \\item WebNLG \\citep{gardent-etal-2017-webnlg}\n    \\end{enumerate}\n    \\item \\textbf{Translation} is the task of translating text from one language into a different language. We use the following datasets:\n    \\begin{enumerate}\n        \\item En--Fr from WMT'14 \\citep{wmt14}\n        \\item En--De, En--Tr, En--Cs, En--Fi, En--Ro, and En--Ru from WMT'16 \\citep{wmt16}\n        \\item En--Es from Paracrawl \\citep{banon-etal-2020-paracrawl}\n    \\end{enumerate}\n    \\item \\textbf{Summarization} asks models to read a piece of text and generate an abbreviated summary of it.\n    We use the following datasets:\n    \\begin{enumerate}\n        \\item AESLC \\citep{zhang2019slg}\n        \\item CNN-DM \\citep{see-etal-2017-get}\n        \\item Gigaword \\citep{napoles-etal-2012-annotated}\n        \\item MultiNews \\citep{fabbri-etal-2019-multi}\n        \\item Newsroom \\citep{grusky-etal-2018-newsroom}\n        \\item Samsum \\citep{gliwa-etal-2019-samsum}\n        \\item XSum \\citep{narayan-etal-2018-dont}\n        \\item AG News \\citep{NIPS2015_250cf8b5}\n        \\item Opinion Abstracts - Rotten Tomatoes \\citep{wang-ling-2016-neural}\n        \\item Opinion Abstracts - iDebate \\citep{wang-ling-2016-neural}\n        \\item Wiki Lingua English \\citep{ladhak-etal-2020-wikilingua}\n    \\end{enumerate}\n    \\item Additional datasets that we assign to a miscellaneous task cluster include:\n    \\begin{enumerate}\n        \\item Conversational question-answering: QuAC \\citep{choi-etal-2018-quac} and CoQA  \\citep{reddy-etal-2019-coqa}\n        \\item Evaluating context-sentence word meanings: WiC \\citep{pilehvar-camacho-collados-2019-wic}\n        \\item Question classification: TREC \\citep{li-roth-2002-learning,hovy-etal-2001-toward}\n        \\item Linguistic acceptability: CoLA \\citep{warstadt2018neural}\n        \\item Math questions \\citep{saxton2019analysing}\n    \\end{enumerate}\n\\end{itemize}\n\nFor all tasks, our finetuning and evaluation code uses tensorflow datasets (TFDS) to load and process datasets.\nRegarding the number of training examples per dataset, we limited the training set size per dataset to 30,000 so that no dataset dominated the finetuning distribution.  \nWhen a test set with labels was available in TFDS, we used it; otherwise, we used the TFDS validation set as our test set, splitting the training set into a train and dev set.\n\nOn the following pages, we show inputs and outputs for evaluation tasks where we compared with GPT-3. See the attached supplementary material for the templates for all other datasets.\n\n\\newcommand{\\taskio}[3]{\n    \\begin{table}[h]\n        \\centering\n        \\begin{tabular}{ |p{0.95\\linewidth}| } \n             \\hline\n             \\vspace{-1mm} \\textsc{\\textbf{\\underline{Input}}}  \\\\ \n             #1 \\\\\n             \\hline\n             \\vspace{-1mm} \\textsc{\\textbf{\\underline{Target}}}  \\\\ \n             {#2} \\\\\n             \\hline\n        \\end{tabular}\n        \\caption{#3}\n    \\end{table}\n}\n\n\\newcommand{\\taskdescription}[8]{Example input and target for #1 (#2). #2 #3 #4. Of the #5, we use #6 for train and #7 for dev. We use #8 as our test set for reporting numbers.}\n\n\\clearpage \n\\subsection{Natural Language Inference}\\label{appen:b-nli}\n\n\\taskio\n{Joey Heindle (born 14 May 1993 in Munich) is a German singer. He is best known for winning the seventh season of the game show Ich bin ein Star – Holt mich hier raus! and finishing in 5th place in season 9 of Deutschland sucht den Superstar, despite universally negative reviews from the jury each week.\\\\\\\\Based on the paragraph above can we conclude that \"Joey Heindle was highly disliked by people on television.\"?\\\\\\\\OPTIONS:\\\\- Yes\\\\- It's impossible to say\\\\- No}\n{Yes}\n{   \\taskdescription\n    {Adversarial NLI} %\n    {ANLI} %\n    {\\citep{anli}} %\n    {is a large-scale NLI benchmark with adversarial examples collected iteratively with a human and model in the loop. The task is to determine whether a hypothesis is entailed by a premise (entailment, not entailment, or impossible to say). There are three rounds, R1--R3} %\n    {three training sets with 16,946, 45,460, and 100,459 examples} %\n    {16,946, 30,000, and 30,000} %\n    {200 from each of the three TFDS validation sets} %\n    {the TFDS ``test'' sets of 1,000, 1,000, and 1,200 examples} %\n}\n\n\\taskio\n{A: so I watch the fish, you know. Whatever I can do to keep myself occupied. I like to have the TV on, because that usually keeps me, um, more occupied. It kind of takes the time away and I don't realize, that's really the only time I ever watch TV, is when I'm on the bike. and then usually after I'm done riding the bike, just to cool myself down, I usually take a walk, you know, and that just kind of uh, gets me, you know, to where I'm not quite as tired I guess. But it's definitely a task. B: You think so? A: I can't say that I really enjoy it.\\\\\\\\Based on the paragraph above can we conclude that \"she really enjoys it\"?\\\\\\\\OPTIONS:\\\\- Yes\\\\- No\\\\- It's impossible to say}\n{No}\n{\n    \\taskdescription\n    {Commitment Bank} %\n    {CB} %\n    {\\citep{de2019commitmentbank}} %\n    {is a corpus of texts in which a hypothesis is extracted from a premise, and the task is to determine whether the hypothesis is entailed by the premise (entailment, not entailment, or impossible to say)} %\n    {training set with 250 examples} %\n    {200} %\n    {50} %\n    {the TFDS validation set of 56 examples} %\n}\n\n\\taskio\n{After years of study, the Vatican's doctrinal congregation has sent church leaders a confidential document concluding that \"sex-change\" procedures do not change a person's gender in the eyes of the church.\\\\\\\\Based on the paragraph above can we conclude that \"Sex-change operations become more common.\"?\\\\\\\\OPTIONS:\\\\- yes\\\\- no}\n{no}\n{\n    \\taskdescription\n    {Recognizing Textual Entailment} %\n    {RTE} %\n    {\\citep{10.1007/11736790_9,haim2006second,giampiccolo-etal-2007-third,bentivogli2009fifth}} %\n    {asks whether a second sentence is entailed by a first (binary, either entailed or not entailed)} %\n    {training set with 2490 examples} %\n    {2,290} %\n    {200} %\n    {the TFDS validation set of 277 examples} %\n}\n\n\\clearpage \n\\subsection{Reading Comprehension}\\label{appen:b-reading-comp}\n\n\\taskio\n{There are four ways an individual can acquire Canadian citizenship: by birth on Canadian soil; by descent (being born to a Canadian parent); by grant (naturalization); and by adoption. Among them, only citizenship by birth is granted automatically with limited exceptions, while citizenship by descent or adoption is acquired automatically if the specified conditions have been met. Citizenship by grant, on the other hand, must be approved by the Minister of Immigration, Refugees and Citizenship.\\\\\\\\Can we conclude that can i get canadian citizenship if my grandfather was canadian?\\\\\\\\OPTIONS:\\\\- no\\\\- yes}\n{no}\n{\n    \\taskdescription\n    {Boolean Questions} %\n    {BoolQ} %\n    {\\cite{clark-etal-2019-boolq}} %\n    {asks a yes/no question based on a passage and a question} %\n    {training set with 9,427 examples} %\n    {9,227} %\n    {200} %\n    {the TFDS validation set of 3,270 examples} %\n}\n\n\\taskio\n{Imagine you are standing in a farm field in central Illinois. The land is so flat you can see for miles and miles. On a clear day, you might see a grain silo 20 miles away. You might think to yourself, it sure is flat around here. If you drive one hundred miles to the south, the landscape changes. In southern Illinois, there are rolling hills. Why do you think this is? What could have caused these features? There are no big rivers that may have eroded and deposited this material. The ground is capable of supporting grass and trees, so wind erosion would not explain it. To answer the question, you need to go back 12,000 years. Around 12,000 years ago, a giant ice sheet covered much of the Midwest United States. Springfield, Illinois, was covered by over a mile of ice. Its hard to imagine a mile thick sheet of ice. The massive ice sheet, called a glacier, caused the features on the land you see today. Where did glaciers go? Where can you see them today? Glaciers are masses of flowing ice. \\\\\\\\Question: \"How big were the glaciers?\"\\\\\\\\Response: \"One mile\"\\\\\\\\Does the response correctly answer the question?\\\\\\\\OPTIONS:\\\\- no\\\\- yes}\n{yes}\n{\n    \\taskdescription\n    {Multi-Sentence Reading Comprehension} %\n    {MultiRC} %\n    {\\cite{khashabi-etal-2018-looking}} %\n    {asks an open-ended question given a paragraph that contains the answer} %\n    {training set with 27,243 examples} %\n    {27,043} %\n    {200} %\n    {the TFDS validation set of 4,848 examples} %\n}\n\n\\taskio\n{soil is a renewable resource for growing plants\\\\A plant that needs to expand will be able to have an endless resource in\\\\\\\\OPTIONS:\\\\- dirt\\\\- pesticides\\\\- pay\\\\- beans}\n{dirt}\n{\n    \\taskdescription\n    {Openbook Question Answering} %\n    {OBQA} %\n    {\\citep{mihaylov-etal-2018-suit}} %\n    {asks 4-way multiple choice questions based facts} %\n    {training set with 4,957 examples} %\n    {all} %\n    {200 in the TFDS validation set of 500 examples} %\n    {the TFDS test set of 500 examples} %\n}\n\n\\clearpage \n\\subsection{Commonsense Reasoning}\\label{appen:b-commonsense}\n\n\\taskio\n{I packed up my belongings. What is the cause?\\\\\\\\OPTIONS:\\\\- I was hunting for a new apartment.\\\\- I was moving out of my apartment.}\n{I was moving out of my apartment.}\n{\n    \\taskdescription\n    {Choice of Plausible Alternatives} %\n    {COPA} %\n    {\\citep{SSS112418}} %\n    {is a causal reasoning task that asks to infer either a cause of effect of a premise from two choices} %\n    {training set with 400 examples} %\n    {350} %\n    {50} %\n    {the TFDS validation set of 100 examples} %\n}\n\n\\taskio\n{What happens next in this paragraph?\\\\\\\\Once the rope is inside the hook, he begins moving up the wall but shortly after he stops and begins talking. The male then begins talking about the clip again and goes back up the wall. as he\\\\OPTIONS:\\\\- progresses, there are hooks everywhere on the wall and when he gets near them, he puts his rope inside of it for support and safety.\\\\- changes time, an instant replay of his initial move is shown a second time.\\\\- continues to talk, another male speaks about the move and shows another closeup of the plex by the male.\\\\- continues, other people start to arrive and begin to hang out with him as he makes a few parts of the rope.}\n{progresses, there are hooks everywhere on the wall and when he gets near them, he puts his rope inside of it for support and safety.}\n{\n\\taskdescription\n    {Commonsense Sentence Completion} %\n    {HellaSwag} %\n    {\\citep{zellers-etal-2019-hellaswag}} %\n    {tests for sentence completion that requires common sense, asking for the most probable ending given four contexts} %\n    {training set with 39,905 examples} %\n    {30,000} %\n    {200} %\n    {the TFDS validation set of 10,042 examples} %\n}\n\n\\taskio\n{Here is a goal: Remove smell from garbage disposal.\\\\\\\\How would you accomplish this goal?\\\\\\\\OPTIONS:\\\\- Create soda ice cubes and grind through disposal.\\\\- Create vinegar ice cubes and grind through disposal.}\n{Create vinegar ice cubes and grind through disposal.}\n{\n    \\taskdescription\n    {Physical Question Answering} %\n    {PiQA} %\n    {\\citep{Bisk2020}} %\n    {is a commonsense QA benchmark for naive physics reasoning, where a solution to a goal must be selected from two choices} %\n    {training set with 16,113 examples} %\n    {16,013} %\n    {100} %\n    {the TFDS validation set of 1,838 examples} %\n}\n\n\\taskio\n{Caroline never drinks carbonated beverages. Her friends pick on her because of it. One day they challenged her to drink a soda. Caroline wanted to win the challenge.\\\\\\\\Predict the next sentence.\\\\OPTIONS:\\\\- Caroline refused to open the soda.\\\\- Caroline opened the soda and drank it all in one gulp!}\n{Caroline opened the soda and drank it all in one gulp!}\n{\n    \\taskdescription\n    {The Story Cloze Test} %\n    {StoryCloze} %\n    {\\citep{mostafazadeh-etal-2016-corpus}} %\n    {is a commonsense reasoning framework for story generation, where a system chooses the correct ending to a four-sentence story. We use the 2016 version on TFDS} %\n    {validation set with 1,871 examples (no training set is available)} %\n    {1,671} %\n    {200} %\n    {the TFDS test set of 1,871 examples} %\n}\n\n\\clearpage \n\\subsection{Closed-Book QA}\\label{appen:b-open-qa}\n\n\\taskio\n{What season is the Northern Hemisphere experiencing when it is tilted directly toward the Sun?\\\\\\\\OPTIONS:\\\\- fall\\\\- winter\\\\- spring\\\\- summer}\n{summer}\n{\n\\taskdescription\n    {The AI2 Reasoning Challenge} %\n    {ARC} %\n    {\\citep{clark2018think}} %\n    {asks grade-school level 4-way multiple choice science questions. There is a challenge set and an easy set, where the challenge set questions were answered incorrectly by both a retrieval-based algorithm and a co-occurrence algorithm} %\n    {training sets with 1,119 examples (challenge) and 2,251 (easy)} %\n    {we use 919 and 2,051 respectively} %\n    {200 each} %\n    {the TFDS test sets of 1,172 and 2,376 examples respectively} %\n}\n\n\\taskio\n{Question: who is the girl in more than you know??\\\\Answer:}\n{Romi Van Renterghem.}\n{\n    \\taskdescription\n    {Natural Questions (Open)} %\n    {NQ} %\n    {\\citep{orqa,kwiatkowski2019natural}} %\n    {asks for an open-ended answer given a question, where all questions can be answered using the contents of Wikipedia} %\n    {training set of 87,925 examples} %\n    {30,000} %\n    {200} %\n    {the TFDS validation set of 3,610 examples} %\n}\n\n\\taskio\n{Please answer this question: Henry Croft, an orphan street sweeper who collected money for charity, is associated with what organised charitable tradition of working class culture in London, England?}\n{pearly kings and queens}\n{\n    \\taskdescription\n    {Trivia Question Answering} %\n    {TriviaQA} %\n    {\\cite{JoshiTriviaQA2017}} %\n    {includes question-answer pairs authored by trivia enthusiasts} %\n    {training set of 87,622 examples} %\n    {30,000} %\n    {200} %\n    {7,993 examples from Wikipedia of the 11,313 examples in the TFDS validation set, which is the same validation set used in \\citep{brown2020language}.} %\n}\n\n\\clearpage \n\\subsection{Coreference Resolution}\\label{appen:b-coreference}\n\n\\taskio\n{How does the sentence end?\\\\\\\\Elena wanted to move out of her parents fast but Victoria wanted to stay for a while, \\\\\\\\OPTIONS:\\\\- Elena went to school.\\\\- Victoria went to school.}\n{Victoria went to school.}\n{\n    \\taskdescription\n    {Adversarial Winograd Schema Challenge} %\n    {Winogrande} %\n    {\\citep{sakaguchi2020winogrande}} %\n    {tests for coreference resolution by asking a model to fill in a masked token in a sentence by choosing an entity from two options} %\n    {40.4k examples in the XL training set} %\n    {30,000} %\n    {200} %\n    {the TFDS validation set of 1,267} %\n}\n\n\\taskio\n{Jane knocked on Susan's door, but there was no answer.\\\\OPTIONS:\\\\- Jane was out.\\\\- Susan was out.}\n{Susan was out.}\n{\n    \\taskdescription\n    {Winograd Schema Challenge} %\n    {WSC273} %\n    {\\citep{levesque2012winograd}} %\n    {tests for coreference resolution by asking a model to complete the sentence in a fashion that requires understanding the entities in the sentence} %\n    {0 examples in the training set (WSC273 is test-set only)} %\n    {none} %\n    {none} %\n    {the TFDS test set} %\n}\n\n\\clearpage\n\\subsection{Reading Comprehension with Commonsense}\\label{appen:b-read-comp-commonsense}\n\n\\taskio\n{Complete the passage.\\\\\\\\(CNN) -- At first glance, \"The Flat\" might seem like an episode of \"Hoarders,\" Israeli-style. The documentary film opens after an elderly woman dies in Tel Aviv. Her grandchildren assemble to clean out her apartment, packed with dusty books, vintage clothing (dozens of pairs of fancy gloves, for instance), enough purses to stock a department store, jewelry, mementoes and closets full of knickknacks. But buried among the detritus they chance upon something remarkable -- mysterious papers linking the grandparents to an important Nazi figure. How could such ardent Zionists, who left their native Germany in the early 1930s, have been involved with an SS official like Leopold von Mildenstein?\\\\\\\\What I found out was this journey, the Nazi (\\\\\\\\OPTIONS:\\\\- Arnon Goldfinger) and his wife were accompanied by my grandparents,\" Goldfinger told CNN.\\\\- CNN) and his wife were accompanied by my grandparents,\" Goldfinger told CNN.\\\\- Germany) and his wife were accompanied by my grandparents,\" Goldfinger told CNN.\\\\- Israeli) and his wife were accompanied by my grandparents,\" Goldfinger told CNN.\\\\- Leopold von Mildenstein) and his wife were accompanied by my grandparents,\" Goldfinger told CNN.\\\\- Nazi) and his wife were accompanied by my grandparents,\" Goldfinger told CNN.\\\\- SS) and his wife were accompanied by my grandparents,\" Goldfinger told CNN.\\\\- Tel Aviv) and his wife were accompanied by my grandparents,\" Goldfinger told CNN.\\\\- The Flat) and his wife were accompanied by my grandparents,\" Goldfinger told CNN.\\\\- Zionists) and his wife were accompanied by my grandparents,\" Goldfinger told CNN.}\n{Leopold von Mildenstein) and his wife were accompanied by my grandparents,\" Goldfinger told CNN.}\n{\n    \\taskdescription\n    {Reading Comprehension with Commonsense Reasoning} %\n    {ReCoRD} %\n    {\\citep{DBLP:journals/corr/abs-1810-12885}} %\n    {asks for the answer to a cloze-style question where an entity is masked out} %\n    {the training set of 100,730 examples} %\n    {30,000} %\n    {200} %\n    {the TFDS validation set of 10,000 examples} %\n}\n\n \n\\subsection{Translation (7 languages)}\\label{appen:data-to-text}\n\n\\taskio\n{Here the largest town of the district is located: Nordenham , lying opposite to Bremerhaven at the Weser mouth.\\\\\\\\Translate to German}\n{An der B 211 befindet sich in Loyermoor der so genannte ``Geest-Abbruch'', der eine Höhendifferenz von gut 30 Meter überbrückt.}\n{\nExample input and output for translation. This example is from WMT'16 English--German; all languages use the same translation templates.\n}\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-1910.04209v3.tex",
        "arXiv-2008.00623v2.tex",
        "arXiv-2109.01652v5.tex"
    ],
    "group_id": "group_110",
    "response": "### Title: Optimization Techniques and Zero-Shot Learning in Large Language Models\n\n### Introduction\nThe field of deep learning, particularly in the context of natural language processing (NLP) and machine translation, has seen significant advancements with the advent of adaptive optimization algorithms and large-scale language models. Adaptive optimization algorithms like Adam have become standard in training deep neural networks due to their ability to adjust learning rates for individual parameters based on historical gradient information. However, these algorithms often require careful tuning of hyperparameters, including learning rate schedules, to ensure stable and efficient training. Similarly, large language models, such as GPT-3 and BERT, have demonstrated remarkable few-shot learning capabilities, but their zero-shot performance remains a challenge, especially for tasks that are not similar to the format of their pretraining data. \n\nIn recent years, researchers have proposed various methods to improve the zero-shot learning abilities of these models. One such method is instruction tuning, which involves finetuning a large language model on a diverse set of tasks described via natural language instructions. This approach aims to enhance the model's ability to follow instructions and perform unseen tasks without additional fine-tuning. Additionally, efforts have been made to design more efficient architectures for deep learning models, such as DeLighT, which aims to reduce the number of parameters and operations while maintaining or improving performance.\n\nThe history of this field is marked by the development of more sophisticated optimization algorithms and the scaling of models to handle larger datasets and more complex tasks. Adaptive optimization algorithms like Adam and RAdam have been crucial in stabilizing the training of deep neural networks, while large language models have been pivotal in demonstrating the potential of deep learning for a wide range of NLP tasks. However, challenges remain in the form of hyperparameter tuning and the need for extensive labeled data to achieve good zero-shot performance.\n\nCurrent progress includes the refinement of optimization techniques and the exploration of new architectures that can efficiently allocate parameters. Additionally, there is increasing interest in methods that can improve zero-shot learning capabilities, making large language models more accessible and versatile for a broader range of applications. This summary will delve into three recent papers that address these challenges, focusing on the optimization of Adam, the introduction of DeLighT, and the enhancement of zero-shot learning through instruction tuning.\n\n### Main Content of Each Paper\n\n#### Paper 1: On the Adequacy of Untuned Warmup for Adaptive Optimization\nThis paper investigates the necessity and effectiveness of learning rate warmup schedules in the context of adaptive optimization algorithms, specifically Adam. The authors challenge the claim made by \\citeauthor{liu2019radam} that the Rectified Adam (RAdam) algorithm, which automatically corrects for high variance in Adam's adaptive learning rate, surpasses the vanilla Adam and reduces the need for warmup tuning. Instead, the authors propose that the primary issue with Adam is the magnitude of the update steps during early training stages, which can be quite large and unstable.\n\nThe paper begins by defining the Adam algorithm and its adaptive learning rate, which scales the global learning rate for each parameter based on the first and second moment estimators of gradients. It then introduces the concept of warmup schedules, which gradually increase the learning rate from zero to its intended value, often used to mitigate early training instability. The authors argue that the variance-based motivation for warmup is insufficient because it overlooks the correlation between the first and second moment estimators, which is crucial for understanding the actual parameter updates.\n\nThe authors conduct a simulation-based analysis to demonstrate that even at a simulated local minimum of the objective function, Adam's parameter update magnitudes are highly non-regular during early training stages. This suggests that warmup is necessary to stabilize these large initial update magnitudes. They propose two simple warmup schedules for Adam: exponential warmup and linear warmup, both of which require no tuning. The authors recommend linear warmup over $2 \\cdot (1 - \\beta_2)^{-1}$ iterations as a sensible default for untuned warmup.\n\nTo empirically validate their findings, the authors evaluate these warmup schedules on various tasks, including image classification (ResNet-50 on ImageNet), language modeling (Transformer-based model on WikiText-103), and machine translation (Transformer on WMT16-EN-DE). The results show that untuned warmup schedules perform similarly to RAdam across different configurations of Adam's hyperparameters, indicating that RAdam's claimed benefits can be achieved with simpler, off-the-shelf methods.\n\n#### Paper 2: DeLighT: Deep and Light-weight Transformer\nThis paper introduces the DeLighT (Deep and Light-weight Transformer) architecture, which aims to deliver similar or better performance than standard transformer-based models with significantly fewer parameters. The DeLighT architecture leverages the DeLighT transformation, a deep and light-weight transformation, and block-wise scaling to efficiently allocate parameters both within and across transformer blocks. DeLighT models are designed to be 2.5 to 4 times deeper than standard transformer models, yet they have fewer parameters and operations.\n\nThe DeLighT transformation uses group linear transformations (GLTs) with an expand-reduce strategy to vary the depth and width of the DeLighT block efficiently. GLTs are local transformations that derive the output from specific parts of the input, and the DeLighT transformation uses feature shuffling to share information between different groups, similar to channel shuffling in convolutional networks. This allows for replacing multi-head attention and feed-forward layers with single-headed attention and light-weight feed-forward layers, reducing the total network parameters and operations.\n\nThe paper evaluates DeLighT on benchmark machine translation and language modeling tasks. For machine translation, DeLighT matches or improves the performance of baseline transformers with 2 to 3 times fewer parameters on average. On the WMT'16 En-Ro dataset, DeLighT achieves transformer performance with $2.8\\times$ fewer parameters, and on the WMT'14 En-De dataset, it delivers better performance (+0.4 BLEU score) with $1.8\\times$ fewer parameters. For language modeling, DeLighT matches the performance of Transformer-XL on the WikiText-103 dataset with $1.5\\times$ fewer parameters.\n\nThe authors also conduct ablation studies to understand the impact of different hyperparameters on DeLighT's performance. They find that deeper and wider representations near the output and shallower and narrower representations near the input are crucial for performance. Additionally, they explore the role of instructions during finetuning and show that training with natural language instructions significantly improves zero-shot performance on unseen tasks.\n\n#### Paper 3: Finetuned Language Models Are Zero-Shot Learners (FLAN)\nThis paper explores a method for improving the zero-shot learning abilities of large language models through instruction tuning. The authors take a 137B parameter pretrained language model and finetune it on over 60 NLP datasets described via natural language instructions. They refer to this finetuned model as FLAN (Finetuned Language Nete), and evaluate its zero-shot performance on unseen task types.\n\nThe authors group NLP datasets into task clusters and hold out each cluster for evaluation while finetuning FLAN on all other clusters. They find that FLAN substantially improves zero-shot performance on unseen tasks compared to its unmodified counterpart and even outperforms few-shot GPT-3 on several datasets, including ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. The paper also evaluates FLAN on natural language inference, reading comprehension, closed-book QA, and translation tasks, demonstrating its effectiveness across a wide range of NLP tasks.\n\nAblation studies reveal that the number of finetuning datasets, model scale, and natural language instructions are key factors in improving zero-shot performance. The authors also investigate the role of instructions during finetuning and find that removing instructions from the finetuning process significantly hurts zero-shot performance. They further explore how few-shot exemplars can complement instruction tuning and improve performance on unseen tasks.\n\n### Commonalities and Innovations\nAll three papers contribute to the advancement of deep learning models and their optimization techniques. Paper 1 focuses on the optimization of Adam by proposing untuned warmup schedules that perform similarly to RAdam, thereby reducing the complexity of hyperparameter tuning. Paper 2 introduces the DeLighT architecture, which efficiently allocates parameters both within and across transformer blocks, leading to a model that is deep and light-weight. Paper 3 explores the use of instruction tuning to improve the zero-shot learning capabilities of large language models, demonstrating that finetuning on a diverse set of tasks can enhance performance on unseen tasks.\n\nThe common theme across these papers is the pursuit of more efficient and effective training methods for deep learning models. Paper 1 addresses the practical challenges of tuning Adam's learning rate schedules, Paper 2 focuses on designing a more efficient transformer architecture, and Paper 3 investigates how to improve zero-shot learning through instruction tuning. Each paper proposes innovative solutions to these challenges, contributing to the broader goal of making deep learning models more accessible and versatile.\n\n### Comparison of Results and Discussion\nThe results from the three papers highlight the effectiveness of their respective methods in improving model performance. Paper 1 demonstrates that untuned warmup schedules for Adam can achieve similar performance to RAdam across various tasks, suggesting that the benefits of RAdam can be replicated without the need for additional tuning. Paper 2 shows that DeLighT models can achieve better performance with fewer parameters and operations, indicating that the architecture is more efficient than standard transformers. Paper 3 reveals that instruction tuning significantly improves zero-shot performance on unseen tasks, outperforming both zero-shot and few-shot GPT-3 on many datasets.\n\nDespite the common goal of improving model performance, the three papers differ in their approaches and the specific challenges they address. Paper 1 focuses on the practical aspects of Adam optimization, while Paper 2 introduces a novel architecture for transformers. Paper 3, on the other hand, explores a method for enhancing zero-shot learning capabilities in large language models. These differences reflect the diverse nature of challenges in deep learning and the various strategies employed to overcome them.\n\n### Conclusion\nThe main findings of these papers are that untuned warmup schedules can effectively stabilize Adam's training, DeLighT architecture can significantly reduce the number of parameters and operations while maintaining or improving performance, and instruction tuning can substantially enhance zero-shot learning capabilities in large language models. These contributions highlight the ongoing efforts to make deep learning models more efficient and versatile.\n\nFuture research directions could include further exploration of untuned warmup schedules for other adaptive optimization algorithms, development of more efficient transformer architectures that can handle a wider range of tasks, and investigation into the impact of instruction tuning on different types of tasks and model scales. Additionally, combining instruction tuning with other prompting methods could lead to even more significant improvements in zero-shot learning. Overall, these papers underscore the importance of continued research in optimization techniques and model architectures to advance the capabilities of deep learning models in NLP and related fields."
}