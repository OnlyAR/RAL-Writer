{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Learning to Retrieve Reasoning Paths over \\\\  Wikipedia Graph for  Question Answering}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nAnswering questions that require multi-hop reasoning at web-scale necessitates retrieving multiple evidence documents, one of which often has little lexical or semantic relationship to the question. This paper introduces a new graph-based recurrent retrieval approach that learns to retrieve reasoning paths over the Wikipedia graph to answer multi-hop open-domain questions. \nOur retriever model trains a recurrent neural network that learns to sequentially retrieve evidence paragraphs in the reasoning path by conditioning on the previously retrieved documents. \nOur reader model ranks the reasoning paths and extracts the answer span included in the best reasoning path.\nExperimental results show state-of-the-art results in three open-domain QA datasets, showcasing the effectiveness and robustness of our method. Notably, our method achieves significant improvement in HotpotQA, outperforming the previous best model by more than 14 points.\\footnote{Our code and data id available at \\url{https://github.com/AkariAsai/learning_to_retrieve_reasoning_paths}.}\n\n\\end{abstract}\n\n\\section{Introduction}\n\nOpen-domain Question Answering (QA) is the task of answering a question given a large collection of text documents (e.g., Wikipedia).\nMost state-of-the-art approaches for open-domain QA~\\citep{chen2017reading,wang2018r,lee2018ranking,yang-etal-2019-end-end} leverage non-parameterized models (e.g., TF-IDF or BM25) to retrieve a fixed set of documents, where an answer span is extracted by a neural reading comprehension model. \nDespite the success of these pipeline methods in single-hop QA, whose questions can be answered based on a single paragraph, they often fail to retrieve the required evidence for answering multi-hop questions, e.g., the question in Figure~\\ref{figure:examples_multi_hop_qa}.\nMulti-hop QA~\\citep{yang-etal-2018-hotpotqa} usually requires finding more than one evidence document, one of which often consists of little lexical overlap or semantic relationship to the original question.\nHowever, retrieving a fixed list of documents independently does not capture relationships between evidence documents through {\\it bridge entities} that are required for multi-hop reasoning. \n\nRecent open-domain QA methods learn end-to-end models to jointly retrieve and read documents~\\citep{denspi,lee-chang-toutanova:2019:ACL2019}.\nThese methods, however, face challenges for entity-centric questions since compressing the necessary information into an embedding space does not capture lexical information in entities. \nCognitive Graph~\\citep{cognitive_graph_2019} incorporates entity links between documents for multi-hop QA  to extend the list of retrieved documents.\nThis method, however, compiles a fixed list of documents independently and expects the reader to find the reasoning paths. \n\nIn this paper, we introduce a new recurrent graph-based retrieval method that learns to retrieve evidence documents as reasoning paths for answering complex questions.\nOur method sequentially retrieves each evidence document, given the history of previously retrieved  documents to form several reasoning paths in a graph of entities.\nOur method then leverages an existing reading comprehension model to answer questions by ranking the retrieved reasoning paths.\nThe strong interplay between the retriever model and reader model enables our entire method to answer complex questions by exploring more accurate reasoning paths compared to other methods. \n\nTo be more specific, our method (sketched in Figure~\\ref{figure:overview}) constructs the Wikipedia paragraph graph using Wikipedia hyperlinks and document structures to model the relationships between paragraphs.\nOur retriever trains a recurrent neural network to  score reasoning paths in this graph by maximizing the likelihood of selecting a correct evidence paragraph at each step and fine-tuning paragraph BERT encodings.\nOur reader model is a multi-task learner to score each reasoning path according to its likelihood of containing and extracting the correct answer phrase. We leverage data augmentation and negative example mining for robust training of both models. \n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth]{images/fig_1_mel.pdf}  % 0.95 originally\n  \\caption{An example of open-domain multi-hop question from HotpotQA. Paragraph 2 is unlikely to be retrieved using TF-IDF retrievers due to little lexical overlap to the given question. }\n  \\label{figure:examples_multi_hop_qa}\n\\end{figure}\n\nOur experimental results show that our  method achieves the state-of-the-art results on HotpotQA full wiki and HotpotQA distractor settings~\\citep{yang-etal-2018-hotpotqa}, outperforming the previous state-of-the-art methods by more than 14 points absolute  gain on the full wiki setting. \nWe also evaluate our approach on SQuAD Open~\\citep{chen2017reading} and Natural Questions Open~\\citep{lee-chang-toutanova:2019:ACL2019} without changing any architectural designs, achieving better or comparable to the state of the art, which suggests that our method is robust across different datasets.\nAdditionally, our framework provides interpretable insights into the underlying entity relationships used for multi-hop reasoning. \n\n\\vspace{-3.5mm}\n\\section{Related Work} \\vspace{-2mm}\n\\paragraph{Neural open-domain question answering}\nMost current open-domain QA methods use a pipeline approach that includes a {\\it retriever} and {\\it reader}. \n\\cite{chen2017reading} incorporate a TF-IDF-based retriever with a state-of-the-art neural reading comprehension model.\nThe subsequent work improves the heuristic retriever by re-ranking retrieved documents~\\citep{wang2018r,wang2018evidence,lee2018ranking,lin2018denoising}.\nThe performance of these methods is still bounded by the performance of the initial retrieval process. \nIn multi-hop QA, non-parameterized retrievers face the challenge of retrieving all the relevant documents, one or some of which are lexically distant from the question.\nRecently, \\cite{lee-chang-toutanova:2019:ACL2019} and \\cite{denspi} introduce fully trainable models that retrieve a few candidates directly from large-scale Wikipedia collections.\nAll these methods find evidence documents independently without the knowledge of previously selected documents or relationships between documents.\nThis would result in failing to conduct multi-hop retrieval.\n\n\\vspace{-2mm}\n\\paragraph{Retrievers guided by entity links}\nMost relevant to our work are recent studies that attempt to use entity links for multi-hop open-domain QA. \nCognitive Graph~\\citep{cognitive_graph_2019} retrieves evidence documents offline, and trains a reading comprehension model to jointly predict possible answer spans and next-hop spans to extend the reasoning chain. Instead, we train our retriever to find reasoning paths directly.\nConcurrent with our work, Entity-centric IR~\\citep{godbole2019entity_links} uses entity linking for multi-hop retrieval.\nUnlike our method, this method does not learn to retrieve reasoning paths sequentially, nor study the interplay between retriever and reader.\nMoreover, while the previous approaches require a system to encode all possible nodes, our beam search decoding process only encodes the nodes on the reasoning paths, which significantly reduces the computational costs.\nPullNet~\\citep{sun-etal-2019-pullnet} learns to retrieve question-aware sub-graphs from text corpora and knowledge bases (e.g., Freebase), while we focus on open-domain QA solely based on text.\n\n\\vspace{-2mm}\n\\paragraph{Multi-step (iterative) retrievers}\nSimilar to our recurrent retriever, multi-step retrievers explore multiple evidence documents iteratively. \nMulti-step reasoner~\\citep{das2019multi} repeats the retrieval process for a {\\it fixed} number of steps, interacting with a reading comprehension model by reformulating the query in a latent space to enhance retrieval performance.\n\\cite{muppet2019multihop} also propose a query reformulation mechanism with a focus on multi-hop open-domain QA.\nMost recently, \\cite{qi2019answering} introduce GoldEn Retriever, which reads and generates search queries for two steps to search documents for HotpotQA full wiki. \nThese methods do not use the graph structure of the documents during the iterative retrieval process.\nIn addition, all of these multi-step retrieval methods do not accommodate arbitrary steps of reasoning and the termination condition is hard-coded. In contrast, our method leverages the Wikipedia graph to retrieve documents that are lexically or semantically distant to questions, and is adaptive to any reasoning path lengths, which leads to significant improvement over the previous work in HotpotQA and SQuAD Open. \n\n\\section{Open-domain Question Answering over Wikipedia Graph}\n\\begin{figure}[t!]\n  \\centering\n  \\includegraphics[width=0.95\\textwidth]{images/fig1_v6.pdf}\n  \\caption{Overview of our framework. }\n  \\label{figure:overview}\n\\end{figure}\n\n\\vspace{-2mm}\n\\paragraph{Overview}\nThis paper introduces a new graph-based recurrent retrieval method (Section~\\ref{sec:retriever}) that learns to find evidence documents as reasoning paths for answering complex questions.\nWe then extend an existing reading comprehension model (Section~\\ref{sec:reader}) to answer questions given a collection of reasoning paths. \nOur method uses a strong interplay between retrieving and reading steps such that the retrieval method learns to retrieve a set of reasoning paths to narrow down the search space for our reader model, for robust pipeline process.\nFigure~\\ref{figure:overview} sketches the overview of our QA model. \n\n\\if0{\nWe are given large-scale documents containing millions of paragraphs $p_i$ (e.g., Wikipedia) in total, a question $q$, and its answer candidate string $a$. \nLet $E = [p_i, \\ldots, p_k]$ denote a set of multiple paragraphs necessary to answer the question (we call it a ``reasoning path''). \nWe formulate the open-domain QA task by decomposing the objective function into the retriever objective $S_\\mathrm{retr}(q, E)$ that sequentially selects evidence documents to form reasoning paths $E$, and the reader objective $S_\\mathrm{read}(q, E, a)$ that finds the answer $a$ in $E$: {\\bf [Potential alternative paragraph is below]}\n}\\fi\n\nWe use Wikipedia for open-domain QA, where each article is divided into paragraphs, resulting in millions of paragraphs in total.\nEach paragraph $p$ is considered as our retrieval target.\nGiven a question $q$, our framework aims at deriving its answer $a$ by retrieving and reading reasoning paths, each of which is represented with a sequence of paragraphs: $E = [p_i, \\ldots, p_k]$.\nWe formulate the task by decomposing the objective into the retriever objective $S_\\mathrm{retr}(q, E)$ that selects reasoning paths $E$ relevant to the question, and the reader objective $S_\\mathrm{read}(q, E, a)$ that finds the answer $a$ in $E$:\n\\begin{equation}\n\\label{eq:task_def}\n    \\argmax_{E, a}S(q, E, a) ~~~\\mathrm{s.t.}~~~ S(q, E, a) = S_\\mathrm{retr}(q, E) + S_\\mathrm{read}(q, E, a).\n\\end{equation}  \n\n\\subsection{Learning to Retrieve Reasoning Paths}\n\\label{sec:retriever}\nOur method learns to retrieve reasoning paths across a graph structure. \nEvidence paragraphs for a complex question do not necessarily have lexical overlaps with the question, but one of them is likely to be retrieved, and its entity mentions and the question often entail another paragraph (e.g., Figure~\\ref{figure:examples_multi_hop_qa}).\nTo perform such multi-hop reasoning, we first construct a graph of paragraphs, covering all the Wikipedia paragraphs.\nEach node of the Wikipedia graph $\\mathcal{G}$ represents a single paragraph $p_i$.\n\n\\vspace{-2mm}\\paragraph{Constructing the Wikipedia graph}\nHyperlinks are commonly used to construct relationships between articles on the web, usually maintained by article writers, and are thus useful knowledge resources.\nWikipedia consists of its internal hyperlinks to connect articles. \nWe use the hyperlinks to construct the direct edges in $\\mathcal{G}$.\nWe also consider symmetric within-document links, allowing a paragraph to hop to other paragraphs in the same article.\nThe Wikipedia graph $\\mathcal{G}$ is densely connected and covers a wide range of topics that provide useful evidence for open-domain questions.\nThis graph is constructed offline and is reused throughout training and inference for any question.\n\n\\subsubsection{The Graph-based Recurrent Retriever}\n\\label{sec:selector}\n\n\\vspace{-2mm}\\paragraph{General formulation with a recurrent retriever}\nWe use a Recurrent Neural Network (RNN) to model the reasoning paths for the question $q$.\nAt the $t$-th time step ($t\\geq1$) our model selects a paragraph $p_{i}$ among candidate paragraphs $\\mathbf{C}_t$ given the current hidden state $h_t$ of the RNN.\nThe initial hidden state $h_1$ is independent of any questions or paragraphs, and based on a parameterized vector.\nWe use BERT's [CLS] token representation~\\citep{devlin2018bert} to independently encode each candidate paragraph $p_i$ {\\it along with} $q$.\\footnote{Appendix~\\ref{subsec:appendix_qp-encoding} discusses the motivation, and Appendix~\\ref{subsec:appendix_query_independent_results} shows results with an alternative approach.}\nWe then compute the probability $P(p_i|h_t)$ that $p_i$ is selected. \nThe RNN selection procedure captures relationships between paragraphs in the reasoning path by conditioning on the selection history.\nThe process is terminated when [EOE], the end-of-evidence symbol, is selected, to allow it to capture reasoning paths with arbitrary length given each question.  \nMore specifically, the process of selecting $p_i$ at the $t$-th step is formulated as follows: \\begin{align}\nw_{i} &= \\mathrm{BERT}_\\mathrm{[CLS]}(q, p_i)\\in\\mathbb{R}^{d}, \\label{eq:selector_qp_encoding} \\\\\nP(p_i|h_t) &= \\sigma(w_i \\cdot h_t + b), \\label{eq:selector_prob} \\\\\nh_{t+1} &= \\mathrm{RNN}(h_t, w_i)\\in\\mathbb{R}^{d},\n\\label{eq:selector_rnn}\n\\end{align}\nwhere $b\\in\\mathbb{R}^{1}$ is a bias term.\nMotivated by~\\citet{NIPS2016_6114}, we normalize the RNN states to control the scale of logits in Equation~(\\ref{eq:selector_prob}) and allow the model to learn multiple reasoning paths.\nThe details of Equation~(\\ref{eq:selector_rnn}) are described in Appendix~\\ref{subsec:normalized_rnn}.\nThe next candidate set $\\mathbf{C}_{t+1}$ is constructed to include paragraphs that are linked from the selected paragraph $p_i$ in the graph.\nTo allow our model to flexibly retrieve multiple paragraphs within $\\mathbf{C}_{t}$, we also add $K$-best paragraphs other than $p_i$ (from $\\mathbf{C}_{t}$) to $\\mathbf{C}_{t+1}$, based on the probabilities.\nWe typically set $K=1$ in this paper.\n\n\\vspace{-2mm}\\paragraph{Beam search for candidate paragraphs}\nIt is computationally expensive to compute Equation~(\\ref{eq:selector_qp_encoding}) over millions of the possible paragraphs. Moreover, a fully trainable retriever often performs poorly for entity-centric questions such as SQuAD, since it does not explicitly maintain lexical information~\\citep{lee-chang-toutanova:2019:ACL2019}.\nTo navigate our retriever in the large-scale graph effectively, we initialize candidate paragraphs with a TF-IDF-based retrieval and guide the search over the Wikipedia graph.\nIn particular, the initial candidate set $\\mathbf{C}_1$ includes $F$ paragraphs with the highest TF-IDF scores with respect to the question.\nWe expand $\\mathbf{C}_t$ ($t\\geq 2$) by appending the [EOE] symbol.\nWe additionally use a beam search to explore paths in the directed graph.\nWe define the score of a reasoning path $E=[p_i, \\ldots, p_k]$ by multiplying the probabilities  of selecting the paragraphs: $P(p_i|h_1)\\ldots P(p_k|h_{|E|})$.\nThe beam search outputs the top $B$ reasoning paths $\\mathbf{E}=\\{E_1, \\ldots, E_B\\}$ with the highest scores to pass to the reader model i.e.,  $S(q, E, a) = S_\\mathrm{read}(q, E, a)$ for $E\\in\\mathbf{E}$.\n\nIn terms of the computational cost, the number of the paragraphs processed by Equation~(\\ref{eq:selector_qp_encoding}) is bounded by $\\mathcal{O}(|\\mathbf{C}_1|+B\\sum_{t\\geq2}\\overline{|\\mathbf{C}_t|})$, where $B$ is the beam size and $\\overline{|\\mathbf{C}_t|}$ is the average size of $\\mathbf{C}_t$ over the $B$ hypothesises.\n\n\\subsubsection{Training of the graph-based recurrent retriever}\n\\label{subsubsec:training_selector}\n\n\\vspace{-2mm}\\paragraph{Data augmentation}\nWe train our retriever in a supervised fashion using evidence paragraphs annotated for each question.\nFor multi-hop QA, we have multiple paragraphs for each question, and single paragraph for single-hop QA.\nWe first derive a ground-truth reasoning path $g=[p_1, \\ldots, p_{|g|}]$ using the available annotated data in each dataset.\n$p_{|g|}$ is set to [EOE] for the termination condition.\nTo relax and stabilize the training process, we augment the training data with additional  reasoning paths -- not necessarily the shortest paths -- that can derive the answer.\nIn particular, we add a new training path $g_r =[p_r, p_1, \\ldots, p_{|g|}]$ by adding a paragraph $p_r\\in \\mathbf{C_1}$ that has a high TF-IDF score and is linked to the first paragraph $p_1$ in the ground-truth path $g$.\nAdding these new training paths helps at the test time when the first paragraph in the reasoning path does not necessarily appear among the paragraphs that initialize the Wikipedia search using the heuristic TF-IDF retrieval. \n\n\\vspace{-2mm}\\paragraph{Negative examples for robustness}\nOur graph-based recurrent retriever needs to be trained to discriminate between relevant and irrelevant paragraphs at each step.\nWe therefore use negative examples along with the ground-truth paragraphs; to be more specific, we use two types of negative examples: (1) TF-IDF-based and (2) hyperlink-based ones.\nFor single-hop QA, we only use the type (1).\nFor multi-hop QA, we use both types, and the type (2) is especially important to prevent our retriever from being distracted by reasoning paths without correct answer spans.\nWe typically set the number of the negative examples to 50.\n\n\\vspace{-2mm}\\paragraph{Loss function}\nFor the sequential prediction task, we estimate $P(p_i|h_t)$ independently in Equation~(\\ref{eq:selector_prob}) and use the binary cross-entropy loss to maximize probability values of all the possible paths.\nNote that using the widely-used cross-entropy loss with the softmax normalization over $\\mathbf{C}_t$ is not desirable here; maximizing the probabilities of $g$ and $g_r$ contradict with each other.\nMore specifically, the loss function of $g$ at the $t$-th step is defined as follows:\n\\begin{equation}\n    L_\\mathrm{retr}(p_t, h_t) = -\\log{P(p_t|h_t)} - \\sum_{\\tilde{p}\\in\\tilde{\\mathbf{C}}_t} \\log{(1-P(\\tilde{p}|h_t))},\n\\end{equation}\nwhere $\\tilde{\\mathbf{C}}_t$ is a set of the negative examples described above, and includes [EOE] for $t < |g|$.\nWe exclude $p_r$ from $\\tilde{\\mathbf{C}_1}$ for the sake of our multi-path learning.\nThe loss is also defined with respect to $g_r$ in the same way.\nAll the model parameters, including those in BERT, are jointly optimized.\n\n\\subsection{Reading and Answering Given Reasoning Paths}\n\\label{sec:reader}\nOur reader model first verifies each reasoning path in $\\mathbf{E}$, and finally outputs an answer span $a$ from the most plausible reasoning path.\nThis interplay is effective in making our framework robust; this is further discussed in Appendix~\\ref{subsec:appendix_interplay}.\nWe model the reader as a multi-task learning of (1) {\\it reading comprehension,} that extracts an answer span from a reasoning path $E$ using a standard approach~\\citep{seo2016bidirectional,xiong2017dynamic,devlin2018bert}, and (2) {\\it reasoning path re-ranking,} that re-ranks the retrieved reasoning paths by computing the probability that the path includes the answer.\n\nFor the reading comprehension task, we use BERT~\\citep{devlin2018bert}, where the input is the concatenation of the question text and the text of all the paragraphs in $E$.\nThis lets our reader to fully leverage the self-attention mechanism across the {\\it concatenated} paragraphs in the retrieved reasoning paths; this paragraph interaction is crucial for multi-hop reasoning~\\citep{wang-etal-2019-multi-hop}.\n\nWe share the same model for re-ranking, and use the BERT's [CLS] representation to estimate the probability of selecting $E$ to answer the question:\n\\begin{equation}\n    \\label{eq:re-ranking}\n    P(E|q) = \\sigma(w_{n} \\cdot u_{E}) ~~~\\mathrm{s.t.}~~~ u_{E} = \\mathrm{BERT}_\\mathrm{[CLS]}(q, E) \\in\\mathbb{R}^{D},\n\\end{equation}\nwhere $w_{n} \\in \\mathbb{R}^{D}$ is a weight vector.\nAt the inference time, we select the best evidence $E_{best} \\in \\mathbf{E}$ by $P(E|q)$, and output the answer span by $S_\\mathrm{read}$:\n\\begin{equation}\nE_{best} = \\argmax_{E \\in \\mathbf{E}}P(E|q), ~~~S_\\mathrm{read} = \\argmax_{i,j,~i\\leq j}P^{start}_i P^{end}_j,\n\\end{equation}\nwhere $P^{start}_i, P^{end}_j$ denote the probability that the $i$-th and $j$-th tokens in $E_{best}$ are the start and end positions, respectively, of the answer span,\nand are calculated by following \\citet{devlin2018bert}.\n\n\\vspace{-2mm}\\paragraph{Training examples}\nTo train the multi-task reader model, we use the ground-truth evidence paragraphs used for training our retriever. \nIt is known to be effective in open-domain QA to use distantly supervised examples, which are not originally associated with the questions but include expected answer strings~\\citep{chen2017reading,wang2018r,hu2019retrieve}.\nThese distantly supervised examples are also effective to simulate the inference time process. \nTherefore, we combine distantly supervised examples from a TF-IDF retriever with the original supervised examples. \nFollowing the procedures in \\cite{chen2017reading}, we add up to one distantly supervised example for each supervised example. \nWe set the answer span as the string that matches $a$ and appears first.\n\nTo train our reader model to discriminate between relevant and irrelevant reasoning paths, we augment the original training data with additional negative examples to simulate incomplete evidence.\nIn particular, we add paragraphs that appear to be relevant to the given question but actually do not contain the answer.\nFor multi-hop QA, we select one ground-truth paragraph including the answer span, and swap it with one of the TF-IDF top ranked paragraphs.\nFor single-hop QA, we simply replace the single ground-truth paragraph with TF-IDF-based negative examples which do not include the expected answer string. \nFor the distorted evidence $\\tilde{E}$, we aim at minimizing $P(\\tilde{E}|q)$.\n\n\\vspace{-2mm}\\paragraph{Multi-task loss function}\nThe objective is the sum of cross entropy losses for the span prediction and re-ranking tasks.\nThe loss for the question $q$ and its evidence candidate $E$ is as follows:\n\\begin{equation}\n    L_\\mathrm{read} = L_\\mathrm{span} + L_\\mathrm{no\\textunderscore answer} = (- \\log{P^{start}_{y^{start}}} - \\log{P^{end}_{y^{end}}}) - \\log{P^r},\n\\end{equation}\nwhere $y^{start}$ and $y^{end}$ are the ground-truth start and end indices, respectively.\n$L_\\mathrm{no\\textunderscore answer}$ corresponds to the loss of the re-ranking model, to discriminate the distorted reasoning paths with no answers.\n$P^r$ is $P(E|q)$ if $E$ is the ground-truth evidence; otherwise $P^r=1-P(E|q)$.\nWe mask the span losses for negative examples, in order to avoid unexpected effects to the span predictions.\n\n\\section{Experiments}\n\\subsection{Experimental Setup}\nWe evaluate our method in three open-domain Wikipedia-sourced datasets: HotpotQA, SQuAD Open and Natural Questions Open.\nWe target all the English Wikipedia paragraphs for SQuAD Open and Natural Questions Open, and the first paragraph (introductory paragraph) of each article for HotpotQA following previous studies.\nMore details can be found in Appendix~\\ref{sec:appendix_experimental_details}.\n\n\\vspace{-2mm}\\paragraph{HotpotQA}\nHotpotQA~\\citep{yang-etal-2018-hotpotqa} is a human-annotated large-scale multi-hop QA dataset.\nEach answer can be extracted from a collection of 10 paragraphs in the {\\it distractor} setting, and from the entire Wikipedia in the {\\it full wiki} setting.\nTwo evidence paragraphs are associated with each question for training.\nOur primary target is the full wiki setting due to its open-domain scenario, and we use the distractor setting to evaluate how well our method works in a closed scenario where the two evidence paragraphs are always included.\nThe dataset also provides annotations to evaluate the prediction of supporting sentences, and we adapt our retriever to the supporting fact prediction.\nNote that this subtask is specific to HotpotQA.\nMore details are described in Appendix~\\ref{subsec:appendix_sp}.\n\n\\vspace{-2mm}\\paragraph{SQuAD Open}\nSQuAD Open~\\citep{chen2017reading} is composed of questions from the original SQuAD dataset~\\citep{rajpurkar2016squad}. \nThis is a single-hop QA task, and a single paragraph is associated with each question in the training data.\n\n\\vspace{-2mm}\\paragraph{Natural Questions Open}\nNatural Questions Open~\\citep{lee-chang-toutanova:2019:ACL2019} is composed of questions from the Natural Questions dataset~\\citep{kwiatkowski2019natural},\\footnote{We use train/dev/test splits provided by \\cite{min2019discrete}, which can be downloaded from \\url{https://drive.google.com/file/d/1qsN5Oyi_OtT2LyaFZFH26vT8Sqjb89-s/view}.} which is based on Google Search queries independently from the existing articles.\nA single paragraph is associated with each question, but our preliminary analysis showed that some questions benefit from multi-hop reasoning.\n\n\\vspace{-2mm}\\paragraph{Metrics} \nWe report standard {\\it F1} and {\\it EM} scores for HotpotQA and SQuAD Open, and EM score for Natural Questions Open to evaluate the overall QA accuracy to find the correct answers. For HotpotQA, we also report {\\it Supporting Fact F1 (SP F1)} and {\\it Supporting Fact EM (SP EM)} to evaluate the sentence-level supporting fact retrieval accuracy.\nTo evaluate the paragraph-level retrieval accuracy for the multi-hop reasoning, we use the following metrics: {\\it Answer Recall (AR),} which evaluates the recall of the answer string among top paragraphs~\\citep{wang2018r,das2019multi}, \n{\\it Paragraph Recall (PR),} which evaluates \nif at least one of the ground-truth paragraphs is included among the retrieved paragraphs, and \n{\\it Paragraph Exact Match (P EM)},\nwhich evaluates if both of the ground-truth paragraphs for multi-hop reasoning are included among the retrieved paragraphs.\n\n\\vspace{-2mm}\\paragraph{Evidence Corpus and the Wikipedia graph}\nWe use English Wikipedia as the evidence corpus and do not use other data such as Google search snippets or external structured knowledge bases. \nWe use the several versions of Wikipedia dumps for the three datasets (See Appendix~\\ref{subsec:appendix_wikipedia_dumps}). \nTo construct the Wikipedia graph, the hyperlinks are automatically extracted from the raw HTML source files. \nDirected edges are added between a paragraph $p_i$ and all of the paragraphs included in the target article. \nThe constructed graph consists of 32.7M nodes and 205.4M edges. \nFor HotpotQA we only use the introductory paragraphs in the graph that includes about 5.2M nodes and 23.4M edges. %, following previous \n\n\\vspace{-2mm}\\paragraph{Implementation details}\nWe use the pre-trained BERT models \\citep{devlin2018bert} using the uncased base configuration $(d = 768)$ for our retriever and the whole word masking uncased large (wwm) configuration $(d = 1024)$ for our readers.\nWe follow \\cite{chen2017reading} for the TF-IDF-based retrieval model and use the same hyper-parameters.\nWe tuned the most important hyper-parameters, $F$, the number of the initial TF-IDF-based paragraphs, and $B$, the beam size, by mainly using the HotpotQA development set (the effects of increasing $F$ are shown in Figure~\\ref{img:robustness} in Appendix~\\ref{subsec:appendix_robust} along with the results with $B=1$).\nIf not specified, we set $B=8$ for all the datasets, $F=500$ for HotpotQA full wiki and SQuAD Open, and $F=100$ for Natural Questions Open.\n\n\\subsection{Overall Results}\n\\label{sec:overall_results}\n\\begin{table}[!tb]\n    \\centering\n    \\small{\n\\begin{tabular}{ p{0.37\\linewidth} | c c |cc|cc | cc}\\toprule \n\n & \\multicolumn{4}{c}{full wiki}  & \\multicolumn{4}{c}{distractor}\\\\\n& \\multicolumn{2}{c}{QA}  & \\multicolumn{2}{c}{SP}&\\multicolumn{2}{c}{QA}  & \\multicolumn{2}{c}{SP}\\\\\n Models  & F1 & EM  & F1 & EM & F1 & EM & F1 & EM \\\\\n    \\midrule\n\nSemantic Retrieval \\citep{nie_pip_2019} & 58.8 & 46.5 &71.5 &39.9 &-- &  -- &-- & --\\\\  \nGoldEn Retriever~\\citep{qi2019answering} & 49.8& -- &64.6 & -- &-- &  -- &-- & --\\\\  \nCognitive Graph \\citep{cognitive_graph_2019} & 49.4 & 37.6 & 58.5& 23.1 & -- & -- &-- &-- \\\\\nDecompRC \\citep{min2019multi} & 43.3 & -- &  -- & -- &  70.6 & --　&  -- & --　\\\\\nMUPPET \\citep{muppet2019multihop} & 40.4 & 31.1 &47.7 & 17.0 & -- & -- & -- &-- \\\\\nDFGN \\citep{dynamic_multihop} & -- & -- & -- & -- &69.2 &  55.4 & --& -- \\\\ \nQFE \\citep{nishida2019qfe} & -- & -- & -- & -- &68.7& 53.7 & 84.7 & \\bf 58.8 \\\\ \nBaseline \\citep{yang-etal-2018-hotpotqa} & 34.4 & 24.7& 41.0 &5.3 & 58.3 & 44.4 & 66.7 &  22.0\\\\  \\hdashline\nTransformer-XH \\citep{zhao2020transformerxh}&  62.4 & 50.2 & 71.6 &42.2 & -- & -- & -- & -- \\\\  \\hline\nOurs (Reader: BERT wwm) & \\bf 73.3 & \\bf 60.5  & \\bf 76.1 & \\bf 49.3 & \\bf 81.2 &  \\bf 68.0 & \\bf 85.2 & 58.6 \\\\\nOurs (Reader: BERT base) &  65.8 & 52.7 & 75.0  & 47.9  & 73.3  & 59.4  & 84.6 & 57.4  \\\\\n\\bottomrule\n\\end{tabular}\n    \\caption{{\\bf HotpotQA development set results}: QA and SP (supporting fact prediction) results on HotpotQA's full wiki and distractor settings. ``--'' denotes no results are available.\n    }\n    \\label{table:dev_results_hotpot_qa}\n    }\n\\end{table} \n\nTable~\\ref{table:dev_results_hotpot_qa} compares our method with previous published methods on the HotpotQA development set.\nOur method significantly outperforms all the previous results across the evaluation metrics under both the full wiki and distractor settings. \nNotably, our method achieves 14.5 F1 and 14.0 EM gains compared to state-of-the-art Semantic Retrieval~\\citep{nie_pip_2019} and 10.9 F1 gains over the concurrent Transformer-XH model~\\citep{zhao2020transformerxh} on full wiki. \nWe can see that our method, even with the BERT base configuration for our reader, significantly outperforms all the previous QA scores.\nMoreover, our method shows significant improvement in predicting supporting facts in the full wiki setting.\nWe compare the performance of our approach to other models on the HotpotQA full wiki official hidden test set in Table~\\ref{table:test_results_hotpot_qa}.\nWe outperform all the published and unpublished models including up-to-date work (marked with $^\\clubsuit$) by large margins in terms of QA performance.\n\nOn SQuAD Open, our model outperforms the concurrent state-of-the-art model~\\citep{multipassage-bert-2019} by 2.9 F1 and 3.5 EM scores as shown in Table~\\ref{table:main_results_squad}. \nDue to the fewer lexical overlap between questions and paragraphs on Natural Questions, pipelined approaches using term-based retrievers often face difficulties finding associated articles.\nNevertheless, our approach matches the performance of the best end-to-end retriever (ORQA), as shown in Table~\\ref{tab:natural_questions_result}.\nIn addition to its competitive performance, our retriever can be handled on a single GPU machine, while a fully end-to-end retriever in general  requires industry-scale computational resources for training~\\citep{denspi}.\nMore results on these two datasets are discussed in Appendix~\\ref{sec:squad_nq_additional_result}.\n\n\\begin{table}[!t] %%%%% here to adjust the horizontal position\n\\hspace{-0.35cm}\n\\begin{minipage}[t]{0.50\\linewidth}\n    \\centering\n    \\small{\n\\vspace{-0.1cm}\n\\begin{tabular}{ p{0.40\\linewidth} | c c | cc  }\\toprule \n  Models &\\multicolumn{2}{c}{QA} & \\multicolumn{2}{c}{SP}  \\\\\n  (*: anonymous) & F1 & EM & F1 & EM \\\\\n  \\midrule\nSemantic Retrieval & 57.3 & 45.3 & 70.8 & 38.7\\\\  \nGoldEn Retriever & 48.6 & 37.9 & 64.2 & 30.7\\\\\nCognitive Graph  & 48.9 & 37.1 &57.7  & 22.8\\\\\nEntity-centric IR &46.3 & 35.4 & 43.2 & 0.06\\\\\nMUPPET  & 40.3 & 30.6  &47.3 & 16.7\\\\\nDecompRC  & 40.7 & 30.0 & -- & -- \\\\\nQFE & 38.1 & 28.7  &44.4 & 14.2\\\\ \nBaseline & 32.9 &  24.0 & 37.7& 3.9\\\\  \\hdashline\nHGN*$^\\clubsuit$ & 69.2 & 56.7 & \\bf 76.4 & \\bf 50.0 \\\\\nMIR+EPS+BERT*$^\\clubsuit$ & 64.8 & 52.9 & 72.0 & 42.8\\\\\nTransformer-XH* & 60.8 & 49.0 & 70.0 & 41.7\\\\\n\\hline\n  Ours & \\bf 73.0 & \\bf 60.0 & \\bf 76.4\t& 49.1\\\\\n\\bottomrule\n\\end{tabular}\n    \\caption{{\\bf HotpotQA full wiki test set results}: official leaderboard results (on November 6, 2019) on the hidden test set of the HotpotQA full wiki setting. Work marked with $^\\clubsuit$ appeared after September 25.}\\label{table:test_results_hotpot_qa}\n    }\n\\end{minipage}\n\\hspace{0.2cm}\n\\begin{minipage}[t]{.5\\linewidth} %0.45\n\\vspace{-0.1cm}\n    \\centering\n    \\small{\n\\begin{tabular}{ p{0.7\\linewidth} | c c }\\toprule \n Models  & F1 & EM  \\\\\n  \\midrule\nmulti-passage ~\\citep{multipassage-bert-2019} & 60.9 & 53.0 \\\\\nORQA~\\citep{lee-chang-toutanova:2019:ACL2019} & --  & 20.2  \\\\\nBM25+BERT~\\citep{lee-chang-toutanova:2019:ACL2019} & --  & 33.2  \\\\\nWeaver~\\citep{raison2018weaver}& -- & 42.3 \\\\\nRE$^3$~\\citep{hu2019retrieve} & 50.2  &　41.9 \\\\\nMUPPET~\\citep{muppet2019multihop} & 46.2 & 39.3 \\\\\nBERTserini~\\citep{yang-etal-2019-end-end}& 46.1  & 38.6 \\\\\nDENSPI-hybrid~\\citep{denspi}  & 44.4  & 36.2 \\\\\nMINIMAL~\\citep{min2018efficient} &42.5 & 34.7 \\\\\nMulti-step Reasoner~\\citep{das2019multi} & 39.2   & 31.9\\\\\nParagraph Ranker~\\citep{lee2018ranking} & -- & 30.2 \\\\\nR$^3$~\\citep{wang2018r} & 37.5 & 29.1 \\\\\nDrQA~\\citep{chen2017reading}& -- & 29.3 \\\\\n\\hline\nOurs &  \\bf 63.8 &  \\bf 56.5  \\\\\n  \\bottomrule\n\\end{tabular}\n    \\caption{{\\bf SQuAD Open results}: we report F1 and EM scores on the test set of SQuAD Open, following previous work.\n    }\n    \\label{table:main_results_squad}\n    }\n\\end{minipage}\n\\end{table}\n\n\\begin{table}[!tb]\n\\begin{minipage}{.48\\linewidth}\n\\vspace{-0.7cm} % -0.7\n    \\centering\n    \\small{\n\\begin{tabular}{ l | c | c }\\toprule \n & \\multicolumn{2}{c}{EM}\\\\\n  Models & Dev & Test\\\\\n  \\midrule\nORQA~\\citep{lee-chang-toutanova:2019:ACL2019} &  31.3 & \\bf 33.3 \\\\\nHard EM~\\citep{min2019discrete} &  28.8  & 28.1\\\\\nBERT + BM 25~\\citep{lee-chang-toutanova:2019:ACL2019}&  24.8 &  26.5\\\\\n\\hline\nOurs   & \\bf 31.7 & 32.6 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{{\\bf Natural Questions Open results}: we report EM scores on the test and development sets of Natural Questions Open, following previous work.\n}\\label{tab:natural_questions_result}\n    }\n\\end{minipage}\n\\hspace{0.2cm}\n\\begin{minipage}{.48\\linewidth}\n    \\centering\n    \\small{\n\\begin{tabular}{ l | c c c c }\\toprule \n Models  & AR & PR & P EM & EM \\\\\n  \\midrule\n  Ours ($F=20$)  & 87.0 &　93.3 & 72.7 & 56.8 \\\\\\hline\n  TF-IDF & 39.7 & 66.9  &10.0 & 18.2 \\\\\n  Re-rank & 55.1 & 85.9 & 29.6 & 35.7 \\\\\n  Re-rank 2hop  & 56.0 & 70.1 & 26.1 & 38.8 \\\\\\hdashline\n  Entity-centric IR  & 63.4 & 87.3 &  34.9 & 42.0 \\\\\n  Cognitive Graph & 76.0& 87.6 & 57.8 & 37.6 \\\\\n　　Semantic Retrieval & 77.9 & 93.2 & 63.9 & 46.5 \\\\\n  \\bottomrule\n\\end{tabular}\n    \\caption{{\\bf Retrieval evaluation}: Comparing our retrieval method with other methods across Answer Recall, Paragraph Recall, Paragraph EM, and QA EM metrics.\n    }\\label{table:retrieval_results}\n    }\n\\end{minipage}\n\\end{table}\n\n\\subsection{Performance of Reasoning Path Retrieval}\n\\label{sec:retrieval_performance_evaluation}\nWe compare our retriever with competitive retrieval methods for HotpotQA full wiki, with $F=20$.\n\n{\\bf TF-IDF} ~\\citep{chen2017reading}, the widely used retrieval method that scores paragraphs according to the TF-IDF scores of the question-paragraph pairs.\nWe simply select the top-2 paragraphs.\n\\newline\n{\\bf Re-rank}~\\citep{nogueira2019passage} that learns to retrieve paragraphs by fine-tuning BERT to re-rank the top $F$ TF-IDF paragraphs.\nWe select the top-2 paragraphs after re-ranking.\n\\newline\n{\\bf  Re-rank 2hop} which extends Re-rank to accommodate two-hop reasoning.\nIt first adds paragraphs linked from the top TF-IDF paragraphs.\nIt then uses the same BERT model to select the paragraphs.\n\\newline\n{\\bf Entity-centric IR} is our re-implementation of \\cite{godbole2019entity_links} that is related to Re-rank 2hop, but instead of simply selecting the top two paragraphs, they re-rank the possible combinations of the paragraphs that are linked to each other.\n\\newline\n{\\bf Cognitive Graph}~\\citep{cognitive_graph_2019} that uses the provided prediction results of the Cognitive Graph model on the HotpotQA development dataset.\\newline\n{\\bf Semantic Retrieval}~\\citep{nie_pip_2019} that uses the provided prediction results of the state-of-the-art Semantic Retrieval model on the HotpotQA development dataset.\n\n\\vspace{-2mm} \\paragraph{Retrieval results}\nTable~\\ref{table:retrieval_results} shows that our recurrent retriever yields 8.8 P EM and 9.1 AR, leading to the improvement of 10.3 QA EM over Semantic Retrieval.\nThe significant improvement from Re-rank2hop to Entity-centric IR demonstrates that exploring entity links from the initially retrieved documents helps to retrieve the paragraphs with fewer lexical overlaps.\nOn the other hand, comparing our retriever with Entity-centric IR and Semantic Retrieval shows the importance of learning to sequentially retrieve reasoning paths in the Wikipedia graph. \nIt should be noted that our method with $F=20$ outperforms all the QA EM scores in Table~\\ref{table:dev_results_hotpot_qa}.\n\n\\subsection{Analysis}\nWe conduct detailed analysis of our framework on the HotpotQA full wiki development set.\n\\label{sec:analysis}\n\n\\vspace{-2mm}\n\\paragraph{Ablation study of our framework}\nTo study the effectiveness of our modeling choices, we compare the performance of variants of our framework.\nWe ablate the retriever with 1) {\\it  No recurrent module}, which removes the recurrence from our retriever,\nand computes the probability of each paragraph to be included in reasoning paths independently and selects the path with the highest joint probability path on the graph;\n2) {\\it No beam search}, which uses a greedy search ($B=1$) in our recurrent retriever;\n3) {\\it No  link-based negative examples,} which trains the retriever model without adding hyperlink-based negative examples besides TF-IDF-based negative examples.\n\nWe ablate the reader model with 1) {\\it No reasoning path re-ranking}, which outputs the answer only with the best reasoning path from the retriever model, and \n2) {\\it No negative examples}, which trains the model only with the gold paragraphs, removing  $L_\\mathrm{no\\textunderscore answer}$ from $L_\\mathrm{read}$. \nDuring inference,``No negative examples'' reads all the paths and outputs an answer with the highest answer probability.\n\n\\vspace{-3mm}\\paragraph{Ablation results}\nTable~\\ref{tab:ablation_study} shows that removing any of the listed components gives notable performance drop.\nThe most critical component in our retriever model is the recurrent module, dropping the EM by 17.4 points. \nAs shown in Figure~\\ref{figure:examples_multi_hop_qa}, multi-step retrieval often relies on information mentioned in another paragraph. \nTherefore, without conditioning on the previous time steps, the model fails to retrieve the complete evidence.\nTraining without hyperlink-based negative examples results in the second largest performance drop, indicating that the model can be easily distracted by reasoning paths without a correct answer and the importance of negative sampling for training.\nReplacing the beam search with the greedy search gives a performance drop of about 4 points on EM, which demonstrates that being aware of the graph structure is helpful in finding the best reasoning paths.\n\nPerformance drop by removing the reasoning path re-ranking indicates the importance of verifying the reasoning paths in our reader.\nNot using negative examples to train the reader degrades EM more than 16 points, due to the over-confident predictions as discussed in \\cite{clark-gardner-2018-simple}.\n\n\\begin{table}[!tb]\n\\begin{minipage}{.50\\linewidth}\n\\vspace{-0.3cm}\n    \\centering\n    \\small{\n\\begin{tabular}{ l | c | c }\\toprule \n  Settings ($F=100$) & F1 & EM \\\\\n  \\midrule\nfull & 72.4 & 59.5 \\\\\\hdashline\nretriever, no recurrent module & 52.5 & 42.1 \\\\\nretriever, no beam search &  68.7 & 56.2 \\\\\nretriever, no link-based negatives  & 64.1  &  52.6 \\\\ \n\\hline\nreader, no reasoning path re-ranking  & 70.1 & 57.4 \\\\\nreader, no negative examples  & 53.7 & 43.3  \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{{\\bf Ablation study}: evaluating different variants of our model on HotpotQA full wiki.\n}\\label{tab:ablation_study}\n    }\n\\end{minipage}\n\\hspace{2mm}\n\\begin{minipage}{.45\\linewidth}\n\\vspace{-0.6cm}\n    \\centering\n    \\small{\n\\begin{tabular}{ l | c | c }\\toprule \n  Settings ($F=100$) & F1 & EM \\\\\n  \\midrule\nwith hyperlinks & 72.4 & 59.5 \\\\\\hdashline\nwith entity linking system & 70.1 & 57.3  \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{{\\bf Performance with different link structures}: comparing our results on the Hotpot QA full wiki development set when we use an off-the-shelf entity linking system instead of the Wikipedia hyperlinks.\n}\\label{tab:entity_link}\n    }\n\\end{minipage}\n\\end{table}\n\n\\begin{table}[!tb]\n\\begin{minipage}{.46\\linewidth}\n    \\centering\n    \\small{\n\\begin{tabular}{ l | l | c | c }\\toprule \n  \\multicolumn{2}{c}{Settings ($F=100$)} & F1 & EM \\\\\n  \\midrule\n\\multicolumn{2}{c|}{Adaptive retrieval} & 72.4 & 59.5 \\\\\\hline\n\\multirow{4}{*}{$L$-step retrieval } & $L=1$ &  45.8 & 35.5  \\\\\n  & $L=2$ &  71.4 &  58.5 \\\\\n& $L=3$ &  70.1 &  57.7 \\\\\n & $L=4$ &  66.3 &  53.9 \\\\\n\\bottomrule\n\\end{tabular}\n\\hspace{0.3cm}\n\\caption{{\\bf Performance with different reasoning path length}: comparing the performance with different path length on HotpotQA full wiki. $L$-step retrieval sets the number of the reasoning steps to a fixed number.\n}\\label{tab:fixed_length}\n    }\n\\end{minipage}\n\\hspace{2mm}\n\\begin{minipage}{.49\\linewidth}\n\\vspace{-0.1cm}\n    \\centering\n    \\small{\n\\begin{tabular}{c| c |c ||c }\\toprule \n($F=100$) & Retriever & Reader &  EM \\\\\n\\cmidrule{1-3}\nAvg. \\# of $L$ & 1.96 & 2.21 & with $L$\\\\\n\\midrule\n$1$ & ~~~539 & ~~~403 & 31.2 \\\\\n$2$ & 6,639 & 5,655 & 60.0\\\\\n$3$  & ~~~227 & 1,347 & 63.0 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{{\\bf Statistics of the reasoning paths}: the average length and the distribution of length of the reasoning paths selected by our retriever and reader for HotpotQA full wiki. Avg. EM represents QA EM performance.\n}\\label{tab:comparison_num_retrieval}\n    }\n\\end{minipage}\n\\end{table}\n\n\\vspace{-2mm}\\paragraph{The performance with an off-the-shelf entity linking system}\nAlthough the existence of the hyperlinks is not special on the web, one question is how well our method works without the Wikipedia hyperlinks.\nWe evaluate our method on the development set of HotpotQA full wiki with an off-the-shelf entity linking system~\\citep{ferragina2011fast} to construct the document graph in our method.\nMore details about this experimental setup can be found in Appendix~\\ref{subsec:appendix_entity_linking}. \nTable~\\ref{tab:entity_link} shows that our approach with the entity linking system shows only 2.3 F1 and 2.2 EM lower scores than those with the hyperlinks, still achieving the state of the art.\nThis suggests that our approach is not restricted to the existence of the hyperlink information, and using hyperlinks is promising.\n\n\\vspace{-2mm}\\paragraph{The effectiveness of arbitrary-step retrieval}\nThe existing iterative retrieval methods fix the number of reasoning steps~\\citep{qi2019answering,das2019multi,godbole2019entity_links,muppet2019multihop}, while our approach accommodates arbitrary steps of reasoning.\nWe also evaluate our method by fixing the length of the reasoning path ($L = \\{1,2,3,4\\}$).\nTable~\\ref{tab:fixed_length} shows that out adaptive retrieval performs the best, although the length of all the annotated reasoning paths in HotpotQA is two.\nAs discussed in \\cite{min2019compositional}, we also observe that some questions are answerable based on a single paragraph, where our model flexibly selects a single paragraph and then terminates retrieval.\n\n\\vspace{-2mm}\n\\paragraph{The effectiveness of the interplay between retriever and reader}\nTable~\\ref{tab:ablation_study} shows that the interplay between our retriever and reader models is effective.\nTo understand this, we investigate the length of reasoning paths selected by our retriever and reader, and their final QA performance.\nTable~\\ref{tab:comparison_num_retrieval} shows that the average length selected by our reader is notably longer than that by our retriever.\nTable~\\ref{tab:comparison_num_retrieval} also presents the EM scores averaged over the questions with certain length of reasoning paths ($L=\\{1, 2,3\\}$). \nWe observe that our framework performs the best when it selects the reasoning paths with $L=3$, showing 63.0 EM score.\nBased on these observations, we expect the retriever favors a shorter path, while the reader tends to select a longer and more convincing multi-hop reasoning path to derive an answer string. \n\n\\vspace{-2mm}\\paragraph{Qualitative examples of retrieved  reasoning paths}\n\\begin{table}[tb!]\n\\begin{minipage}{.51\\linewidth}\n    \\makeatletter          %%%\n    \\def\\@captype{figure}  %%% to change the caption title\n    \\makeatother           %%%\n    \\includegraphics[width=1.0\\textwidth]{images/fig3_new.pdf}\n      \\caption{Reasoning examples by our model (two paragraphs connected by a dotted line) and Re-rank (the bottom two paragraphs). Highlighted text denotes a bridge entity, and blue-underlined text represents hyperlinks.\n      }\\label{img:example_retrieved_path}\n\\end{minipage}\n\\hspace{0.2cm}\n\\begin{minipage}{.45\\linewidth}\n    \\makeatletter          %%%\n    \\def\\@captype{figure}  %%% to change the caption title\n    \\makeatother           %%%\n    \\includegraphics[width=0.95\\textwidth]{images/fig4_v2.pdf}\n      \\caption{Reasoning examples by our retriever (the bottom paragraph) and our reader (two paragraphs connected by a dotted line). Highlighted text denotes a bridge entity, and blue-underlined text represents hyperlinks.\n      }\\label{img:example_second}\n\\end{minipage}\n\\end{table}\nFinally, we show two examples from HotpotQA full wiki, and Appendix~\\ref{subsec:appendix_qualitative_examples} presents more qualitative examples.\nIn Figure~\\ref{img:example_retrieved_path}, our approach successfully retrieves the correct reasoning path and answers correctly, while Re-rank fails. \nThe top two paragraphs next to the graph are the introductory paragraphs of the two entities on the reasoning path, and the paragraph at the bottom shows the wrong paragraph selected by Re-rank.\nThe ``Millwall F.C.'' has fewer lexical overlaps and the bridge entity ``Millwall'' is not stated in the given question. Thus, Re-rank chooses a wrong paragraph with high lexical overlaps to the given question.\nIn Figure~\\ref{img:example_second}, we compare the reasoning paths ranked highest by our retriever and reader. Although the gold path is included among the top 8 paths selected by the beam search, our retriever model selects a wrong paragraph as the best reasoning path. \nBy re-ranking the reasoning paths, the reader eventually selects the correct reasoning path (``2017-18 Wigan Athletic F.C. season'' $\\rightarrow$ ``EFL Cup'').\nThis example shows the effectiveness of the strong interplay of our retriever and reader.\n\n\\section{Conclusion}\nThis paper introduces a new graph-based recurrent retrieval approach, which retrieves reasoning paths over the Wikipedia graph to answer multi-hop open-domain questions. \nOur retriever model learns to sequentially retrieve evidence paragraphs to form the reasoning path.\nSubsequently, our reader model re-ranks the reasoning paths, and it determines the final answer as the one extracted from the best reasoning path. \nOur experimental results significantly advance the state of the art on HotpotQA by more than 14 points absolute gain on the full wiki setting.\nOur approach also achieves the state-of-the-art performance on SQuAD Open and Natural Questions Open without any architectural changes, demonstrating the robustness of our method. \nOur method provides insights into the underlying entity relationships, and the discrete reasoning paths are helpful in interpreting our framework's reasoning process. % <-- compared with what? maybe we can say \"the discrete reasoning paths are interpretable\", \"the discrete reasoning paths are helpful in interpreting our framework's reasoning process\" or something like this.\nFuture work involves end-to-end training of our graph-based recurrent retriever and reader for improving upon our current two-stage training. \n\n\\subsubsection*{Acknowledgments}\nWe acknowledge grants from ONR N00014-18-1-2826, DARPA\nN66001-19-2-403, NSF (IIS1616112, IIS1252835), and Samsung GRO. \nWe thank Sewon Min, David Wadden, Yizhong Wang, Akhilesh Gotmare, Tong Niu, and UW NLP group and Salesforce research members for their insightful discussions.\nWe would also like to show our gratitude to Melvin Gruesbeck for providing us with the artistic figures presented in this paper. \nWe thank the anonymous reviewers for their helpful and thoughtful comments. \nAkari Asai is supported by The Nakajima Foundation Fellowship. \n\n\\appendix\n\n\\section*{Appendix}\n\n\\section{Details about Modeling}\n\n\\subsection{A Normalized RNN}\n\\label{subsec:normalized_rnn}\nWe  decompose Equation~(\\ref{eq:selector_rnn}) as follows:\n\\begin{equation}\na_{t+1} = W_r [h_t; w_i] + b_r,~~~~h_{t+1} = \\frac{\\alpha}{\\|a_{t+1}\\|} a_{t+1},\n\\end{equation}\nwhere $W_r\\in\\mathbb{R}^{d\\times 2d}$ is a weight matrix, $b_r\\in\\mathbb{R}^{d}$ is a bias vector, and $\\alpha\\in\\mathbb{R}^{1}$ is a scalar parameter (initialized with 1.0).\nWe set the global initial state $a_1$ to a parameterized vector $s\\in\\mathbb{R}^{d}$, and we also parameterize an [EOE] vector $w_\\mathrm{[EOE]}\\in\\mathbb{R}^{d}$ for the [EOE] symbol.\nThe use of $w_i$ for both the input and output layers is inspired by \\citet{tai_softmax,tai_softmax_2}.\nIn addition, we align the norm of $w_\\mathrm{[EOE]}$ with those of $w_i$, by applying layer normalization~\\citep{Ba2016LayerN} of the last layer in BERT because $w_\\mathrm{[EOE]}$ is used along with the BERT outputs.\nWithout the layer normalization, the $L2$-norms of $w_i$ and $w_\\mathrm{[EOE]}$ can be quite different, and the model can easily discriminate between them by the difference of the norms.\n\n\\subsection{Question-Paragraph Encoding in Our Retriever Component}\n\\label{subsec:appendix_qp-encoding}\n\nEquation~(\\ref{eq:selector_qp_encoding}) shows that we compute each paragraph representation $w_i$ conditioned on the question $q$.\nAn alternative approach is separately encoding the paragraphs and the question, to directly retrieve paragraphs~\\citep{lee-chang-toutanova:2019:ACL2019,denspi,das2019multi}. \nHowever,\ndue to the lack of explicit interactions between the paragraphs and the question, such a neural retriever using question-independent paragraph encodings suffers from compressing the necessary information into fixed-dimensional vectors, resulting in low performance on entity-centric questions~\\citep{lee-chang-toutanova:2019:ACL2019}.\nIt has been shown that attention-based paragraph-question interactions improve the retrieval accuracy if the retrieval scale is tractable~\\citep{wang2018r,lee2018ranking}.\nThere is a trade-off between the scalability and the accuracy, and this work aims at striking the balance by jointly using the lexical matching retrieval and the graphs, followed by the rich question-paragraph encodings.\n\n\\paragraph{A question-independent variant}\nWe can also formulate our retriever model by using a question-independent approach.\nThere are only two simple modifications.\nFirst, we reformulate Equation~(\\ref{eq:selector_qp_encoding}) as follows:\n\\begin{equation}\n\\label{eq:q-independent}\nw_{i} = \\mathrm{BERT}_\\mathrm{[CLS]}(p_i),\n\\end{equation}\nwhere we no longer input the question $q$ together with the paragraphs.\nNext, we condition the initial RNN state $h_1$ on the question information.\nMore specifically, we compute $h_1$ by using Equation~(\\ref{eq:selector_rnn}) as follows:\n\\begin{align}\nw_q   &= \\mathrm{BERT}_\\mathrm{[CLS]}(q), \\\\\nh_{1} &= \\mathrm{RNN}(h'_1, w_q),\n\\end{align}\nwhere $w_q$ is computed by using the same BERT encoder as in Equation~(\\ref{eq:q-independent}), and $h'_1$ is the original $h_1$ used in our question-dependent approach as described in Appendix~\\ref{subsec:normalized_rnn}.\nThe remaining parts are exactly the same, and we can perform the reasoning path retrieval in the same manner.\n\n\\subsection{Why is the interplay important?}\n\\label{subsec:appendix_interplay}\nOur retriever model learns to predict plausibility of the reasoning paths by capturing the paragraph interactions through the BERT's [CLS] representations, after {\\it independently} encoding the paragraphs along with the question; this makes our retriever scalable to the open-domain scenario. \nBy contrast, our reader jointly learns to predict the plausibility and answer the question, and moreover, fully leverages the self-attention mechanism across the {\\it concatenated} paragraphs in the retrieved reasoning paths; this paragraph interaction is crucial for multi-hop reasoning~\\citep{wang-etal-2019-multi-hop}.\nIn summary, our retriever is scalable, but the top-1 prediction is not always enough to fully capture multi-hop reasoning to answer the question.\nTherefore, the additional re-ranking process mitigates the uncertainty and makes our framework more robust.\n\n\\subsection{Handling Yes-No Questions in Our Reader Component}\n\\label{subsec:appendix_modeling}\nIn the HotpotQA dataset, we need to handle yes-no questions as well as extracting answer spans from the paragraphs.\nWe treat the two special types of the answers, yes and no, by extending the re-ranking model in Equation~(\\ref{eq:re-ranking}).\nIn particular, we extend the binary classification to a multi-class classification task, where the positive ``answerable'' class is decomposed into the following three classes: span, yes, and no.\nIf the probability of ``yes'' or ``no'' is the largest among the three classes, our reader directly outputs the label as the answer, without any span extraction.\nOtherwise, our reader uses the span extraction model to output the answer.\n\n\\subsection{Supporting Fact Prediction in HotpotQA}\n\\label{subsec:appendix_sp}\n\nWe adapt our recurrent retriever to the subtask of the supporting fact prediction in HotpotQA~\\citep{yang-etal-2018-hotpotqa}.\nThe task is outputting sentences which support to answer the question.\nSuch supporting sentences are annotated for the two ground-truth paragraphs in the training data.\nSince our framework outputs the most plausible reasoning path $E$ along with the answer, we can add an additional step to select supporting facts (sentences) from the paragraphs in $E$.\nWe train our recurrent retriever by using the training examples for the supporting fact prediction task, where the model parameters are not shared with those of our paragraph retriever.\nWe replace the question-paragraph encoding in Equation~(\\ref{eq:selector_qp_encoding}) with question-answer-sentence encoding for the task, where a question string is concatenated with its answer string.\nThe answer string is the ground-truth one during the training time.\nWe then maximize the probability of selecting the ground-truth sequence of the supporting fact sentences, while setting the other sentences as negative examples.\nAt test time, we use the best reasoning path and its predicted answer string from our retriever and reader models to finally output the supporting facts for each question.\nThe supporting fact prediction task is performed after finalizing the reasoning path and the answer for each question, and hence this additional task does not affect the QA accuracy.\n\n\\section{Details about experiments}\n\\label{sec:appendix_experimental_details}\n\n\\subsection{Dataset Details of HotpotQA, SQuAD Open and Natural Questions Open}\n\n\\paragraph{HotpotQA}\nThe HotpotQA training, development, and test datasets contain 90,564, 7,405 and 7,405 questions, respectively.\nTo train our retriever model for the distractor setting, we use the distractor training data, where only the original ten paragraphs are associated with each question.\nThe retriever model trained with this setting is also used in our ablation study as ``retriever, no link-based negatives'' in Table~\\ref{tab:ablation_study}.\nFor the full wiki setting, we train our retriever model with the data augmentation technique and the additional negative examples described in Section~\\ref{subsubsec:training_selector}.\nWe use the same reader model, for both the settings, trained with the augmented additional references and the negative examples described in Section~\\ref{sec:reader}.\n\n\\paragraph{SQuAD Open and Natural Questions Open}\nFor SQuAD Open, we use the original training set (containing 78,713 questions) as our training data, and the original development set (containing 10,570 questions) as our test data.\nFor Natural Questions Open, we follow the dataset splits provided by \\cite{min2019discrete}, and the training, development and test datasets contain 79,168, 8,757 and 3,610, respectively.\nFor both the SQuAD Open and Natural Questions Open, we train our reader on the original examples with the augmented additional negative examples and the distantly supervised examples described in Section~\\ref{sec:reader}.\n\n\\subsection{Deriving ground-truth reasoning paths}\nSection~\\ref{subsubsec:training_selector} describes our training strategy for our recurrent retriever.\nWe apply the data augmentation technique to HotpotQA and Natural Questions to consider multi-hop reasoning.\nTo derive the ground-truth reasoning path $g$, we use the ground-truth evidence paragraphs associated with the questions in the training data for each dataset.\nFor SQuAD and Natural Questions Open, each training example has only single paragraph $p$, and thus it is trivial to derive $g$ as $[p, [\\mathrm{EOE}]]$.\nFor the multi-hop case, HotpotQA, we have two ground-truth paragraphs $p_1, p_2$ for each question.\nAssuming that $p_2$ includes the answer string, we set $g=[p_1, p_2, [\\mathrm{EOE}]]$.\n\n\\subsection{Details about negative examples for our reader model in SQuAD Open and Natural Questions Open}\n\nTo train our reader model for SQuAD Open, in addition to the TF-IDF top-ranked paragraphs, we add two types of additional negative examples: (i) paragraphs, which do not include the answer string, from the originally annotated articles, and (ii) ``unanswerable'' questions from SQuAD 2.0~\\citep{rajpurkar-etal-2018-know}. \nFor Natural Questions Open, we add negative examples of the type (i). \n\n\\subsection{Training settings}\nTo use the pre-trained BERT models, we used the public code base, pytorch-transformers,\\footnote{\\url{https://github.com/huggingface/pytorch-transformers}.} written in PyTorch.\\footnote{\\url{https://pytorch.org/}.}\nFor optimization, we used the code base's implementation of the Adam optimizer~\\citep{adam}, with a weight-decay coefficient of $0.01$ for non-bias parameters.\nA warm-up strategy in the code base was also used, with a warm-up rate of $0.1$.\nMost of the settings follow the default settings.\nTo train our recurrent retriever, we set the learning rate to $3\\cdot 10^{-5}$, and the maximum number of the training epochs to three.\nThe mini-batch size is four; a mini-batch example consists of a question with its corresponding paragraphs.\nTo train our reader model, we set the learning rate to $3\\cdot 10^{-5}$, and the maximum number of training epochs to two.\nEmpirically we observe better performance with a larger batch size as discussed in previous work~\\citep{liu2019roberta,ott-etal-2018-scaling}, and thus we set the mini-batch size to 120.\nA mini-batch example consists of a question with its evidence paragraphs.\nWe will release our code to follow our experiments.\n\n\\subsection{The Wikipedia dumps for each dataset}\n\\label{subsec:appendix_wikipedia_dumps}\n\nFor HotpotQA full wiki, we use the pre-processed English Wikipedia dump from October, 2017, provided by the HotpotQA authors.\\footnote{\\url{https://hotpotqa.github.io/wiki-readme.html}.}\nFor Natural Questions Open, we use the English Wikipedia dump from December 20, 2018, following \\citet{lee-chang-toutanova:2019:ACL2019} and \\citet{min2019discrete}.\nFor SQuAD Open, we use the Wikipedia dump provided by \\cite{chen2017reading}.\n\nAlthough using a single dump for different open-domain QA datasets is a common practice~\\citep{chen2017reading,wang2018r,lee2018ranking}, this potentially causes inconsistent or even unfair evaluation across different experimental settings, due to the temporal inconsistency of the Wikipedia articles.\nMore concretely, every Wikipedia article is editable and and as a result, a fact can be re-phrased or could be removed.\nFor instance, a question from the SQuAD development set, ``Where does Kenya rank on the CPI scale?'' is originally paired with a paragraph from the article of Kenya.\nBased on a single sentence ``Kenya ranks low on Transparency International's Corruption Perception Index (CPI)'' from the paragraph, an annotated answer span is ``low.''\nHowever, this sentence has been rewritten as ``Kenya has a high degree of corruption according to Transparency International's Corruption Perception Index (CPI)'' in a later version of the same article.\\footnote{\\url{https://en.wikipedia.org/wiki/Kenya} on October 25, 2019}\nThis is problematic considering the major evaluation metrics based on string matching.\n\nAnother problem exists especially in Natural Questions Open.\nThe dataset contains real Google search queries, and some of them reflect temporal trends at the time when the queries were executed.\nIf a query is related to a TV show broadcasted in 2018, we can hardly expect to extract the answer from a dump in 2017.\n\nLike this, although Wikipedia is a useful knowledge source for open-domain QA research, its rapidly evolving nature should be considered more carefully for the reproducibility. We will make all of the data including pre-processed Wikipedia articles for each experiment available for future research. \n\n\\subsection{Details about Initial Candidates $C_1$ selection}\n\\label{sec:retireval_details}\nTo retrieve the initial candidates $C_1$ for each question, we use a TF-IDF based retriever with the bi-gram hashing~\\citep{chen2017reading}.\nFor HotpotQA full wiki, we retrieve top $F$ introductory paragraphs, for each question, from a corpus including all the introductory paragraphs.\nFor SQuAD Open and Natural Questions Open, we first retrieve 50 Wikipedia articles through the same TF-IDF retriever, and further run another TF-IDF-based paragraph retriever~\\citep{clark-gardner-2018-simple,min2019discrete} to retrieve $F$ paragraphs in total.\n\n\\subsection{Details about entity linking experiment}\n\\label{subsec:appendix_entity_linking}\n\nWe experiment with a variant of our approach, where we incorporate an entity linking system with our framework, in place of the Wikipedia hyperlinks. \nIn this experiment, we first retrieve seed paragraphs using TF-IDF ($F = 100$), and run an off-the-shelf entity linker (TagMe by \\cite{ferragina2011fast}) over the paragraphs. If the entity linker detects some entities, we retrieve their corresponding Wikipedia articles, and add edges from the seed paragraphs to the entity-linked paragraphs.\nOnce we build the graph, then we re-run all of the experiments while the other components are exactly the same. \nWe use the TagMe official Python wrapper.\\footnote{\\url{https://github.com/marcocor/tagme-python}}\n\n\\section{Additional Results on HotpotQA}\n\\subsection{Upper-bound of our retrieval module}\nFor scalability and computational efficiency, we bootstrap our retrieval module with TF-IDF retrieval; we first retrieval $F$ paragraphs using TF-IDF with the method described in Section \\ref{sec:retireval_details} and initialize $C_1$ with these TF-IDF paragraphs.\nAlthough we expand our candidate paragraphs at each time step using the Wikipedia graph, if our method failed to retrieve paragraphs a few-hops away from the answer paragraphs, it is likely to fail to reach the answer paragraphs. \nTo estimate the paragraph EM upper-bound, we have checked if two gold paragraphs are included in the top 20 TF-IDF paragraphs and their hyperlinked paragraphs in the HotpotQA full wiki setting.\nWe found that for 75.4\\% of the questions, all of the gold paragraphs are included in the collections of the TF-IDF paragraphs and the hyperlinked paragraphs. \nAlso, it should be noted when we only consider the TF-IDF retrieval results, the upper-bound drops to 35.1\\%, which suggests that the TF-IDF-based retrieval cannot effectively discover the paragraphs multi-hop away due to the few lexical overlap. \nWhen we increase the number of $F$ to 100 and 500, the upper-bound reaches 84.1\\% and 89.2\\%, respectively. \n\n\\subsection{Per-Category Question Answering and Retrieval Performance on HotpotQA full wiki}\nIn HotpotQA, there are two types of questions, {\\it bridge} and {\\it comparison}. \nWhile comparison-type questions explicitly mention the two entities related to the given questions, in bridge-type questions, the bridge entities are rarely explicitly stated. \nThis makes it hard for a retrieval system to discover the paragraphs entailed by the bridge entities only. \n\nWe evaluate the question answering and paragraph retrieval performance for each of the two question types. \nWe compare the PR, P EM and QA EM for each of the two categories with two state-of-the-art models, Cognitive Graph~\\citep{cognitive_graph_2019} and Semantic Retrieval~\\citep{nie_pip_2019}. \nHere, we set our initial TF-IDF number $F$ to 500. \nTable~\\ref{table:retrieval_results_per_category} shows that our retriever yields 16.5 P EM gain and 15.1 EM gain over Semantic Retrieval for the challenging bridge-type questions. \nFor the comparison-type questions, our method achieves almost 10 point higher QA EM than Semantic Retrieval. \nWe observed that some of the comparison-type questions can be answered based on single paragraph, and thus our model selects only one paragraph for some of these comparison-type questions, resulting in lower P EM scores on the comparison-type questions. \nWe show several examples of the questions where we can answer based on single paragraph in Section \\ref{subsec:appendix_qualitative_examples}.\n\n\\begin{table}[!tb]\n    \\centering\n    \\small{\n\\begin{tabular}{ l | c c c | c c c | cc c}\\toprule \n& \\multicolumn{3}{c}{Total (7,405)} & \\multicolumn{3}{|c|}{Bridge (5,918)} &  \\multicolumn{3}{c}{Comp (1,487)}  \\\\\n Models  & PR & P EM & EM & PR & P EM & EM & PR & P EM & EM  \\\\\n  \\midrule\n  Ours  & \\bf 94.3& \\bf 75.7 & \\bf 60.5& \\bf 93.9 & \\bf 73.7 & \\bf 57.8& 98.7 & 83.5 & \\bf 70.5\\\\\\hdashline\n  Cognitive Graph~\\citep{cognitive_graph_2019} & 87.6& 57.8 & 37.5 & 84.8 & 51.8 & 36.1&  98.6 & 81.6 & 53.7\\\\\n  Semantic Retrieval~\\citep{nie_pip_2019} & 93.2 & 63.9 & 46.5 & 91.6 & 57.2 & 42.7& \\bf 99.7 & \\bf 90.6 & 61.7\\\\\n  \\bottomrule\n\\end{tabular}\n    \\caption{{\\bf Retrieval evaluation}: Comparing our retrieval method with other methods across Answer Recall, Paragraph Recall, Paragraph EM, and QA EM metrics.\n    }\\label{table:retrieval_results_per_category}\n    }\n\\end{table}\n\n\\subsection{On the robustness to the increase of the paragraphs} \n\\label{subsec:appendix_robust}\n\n\\begin{figure}[t!]\n  \\centering\n  \\includegraphics[width=0.5\\textwidth]{images/scalability_new.png}\n  \\caption{{\\bf Robustness to the increase of $F$.} We compare the F1 scores of our model, our model without a beam search and Re-rank with different number of $F$. }\n  \\label{img:robustness}\n\\end{figure}\n\nAs we discussed in \\ref{sec:selector}, we aim at significantly reducing the search space and thus scaling the number of initial TF-IDF candidates.\nIncreasing the number of the initial retrieved paragraphs often improves the recall of the evidence paragraphs of the datasets.\nOn the other hand, increasing the candidate paragraphs introduces additional noises, may distract models, and eventually hurt the performance~\\citep{kratzwald-feuerriegel-2018-adaptive}.\nWe compare the performance of three different approaches: (i)  {\\it ours}, (ii) {\\it ours (greedy, without reasoning path re-ranking)}, and (iii) {\\it Re-rank}. \nWe increase the number of the TF-IDF-based retrieved paragraphs from 10 to 500 (For Re-rank, we compare the performance up to 200 paragraphs). \nFigure~\\ref{img:robustness} clearly shows that our approach is robust towards the increase of the initial candidate paragraphs, and thus can constantly yield performance gains with more candidate paragraphs.\nOur approach with the greedy search also shows performance improvements; however, after a certain number, the greedy approach stops improving the performance.\nRe-rank starts suffering from the noises caused by many distracting paragraphs included in the initial candidate paragraphs at $F=200$.\n\n\\subsection{Results of Question-Independent Paragraph Encoding for our Retriever}\n\\label{subsec:appendix_query_independent_results}\n\n\\begin{table}[t]\n\\begin{center}\n\\begin{tabular}{ l | c c | c c }\\toprule \n & \\multicolumn{2}{|c}{full wiki ($F=100$)} & \\multicolumn{2}{|c}{distractor} \\\\\n Encoding method  & QA F1 & QA EM & QA F1 & QA EM \\\\\n  \\midrule\n  Question-dependent (our main model)  & 64.1 & 52.6 & 81.2 & 68.0 \\\\\n  Question-independent                 & 47.3 & 37.8 & 80.0 & 66.4 \\\\\n  \\bottomrule\n\\end{tabular}\n    \\caption{{\\bf Effects of the question-dependent paragraph encoding}: Comparing our retriever model with and without the query-dependent encoding. For our question-dependent approach, the full wiki results correspond to ``retriever, no link-based negatives'' in Table~\\ref{tab:ablation_study}, and the distractor results correspond to ``Ours (Reader: BERT wwm)'' Table~\\ref{table:dev_results_hotpot_qa}, to make the results comparable.\n    }\\label{table:query_independent_results}\n\\end{center}\n\\end{table}\n\nTo show the importance of the question-paragraph encoding in our retriever model, we conduct an experiment on the development set of HotpotQA, by replacing it with the question-independent encoding described in Appendix~\\ref{subsec:appendix_qp-encoding}.\nFor a fair comparison, we use the same initial TF-IDF-based retrieval (only for the full wiki setting), hyperlink-based Wikipedia graph, beam search, and reader model (BERT wwm).\nWe train the alternative model without using the data augmentation technique (described in Section~\\ref{subsubsec:training_selector}) for quick experiments.\n\nTable~\\ref{table:query_independent_results} shows the results in both the full wiki and distractor settings.\nAs seen in this table, the QA F1 and EM performance significantly deteriorates on the full wiki setting, which demonstrates the importance of the question-dependent encoding for complex and entity-centric open-domain question answering.\n\nWe can also see that the performance drop on the distractor setting is much smaller than that on the full wiki setting.\nThis is due to its closed nature; for each question, we are given only ten paragraphs and the two gold paragraphs are always included, which significantly narrows the searching space down and makes the retrieval task much easier than that in the full wiki setting.\nTherefore, our recurrent retriever model is likely to discover the gold reasoning paths by the beam search, and our reader model can select the gold paths by the robust re-ranking approach.\nTo verify this hypothesis, we checked the P EM score as a retrieval accuracy in the distractor setting.\nIf we only consider the top-1 path from the beam search, the P EM score of the question-independent model is 12\\% lower than that of our question-dependent model.\nHowever, if we consider all the reasoning paths produced by the beam search, the coverage of the gold paths is almost the same.\nAs a result, our reader model can perform similarly with both the question-dependent/independent approaches.\nThis additionally shows the robustness of our re-ranking approach.\n\n\\subsection{More qualitative analysis on the reasoning path on HotpotQA full wiki}\n\\label{subsec:appendix_qualitative_examples}\n\nIn this section, we conduct more qualitative analysis on the reasoning paths predicted by our model. \nExplicitly retrieving plausible reasoning paths and re-ranking the paths provide us interpretable insights into the underlying entity relationships used for multi-hop reasoning.\n\nAs shown in Table~\\ref{tab:comparison_num_retrieval}, our model flexibly selects one or more paragraphs for each question. \nTo understand these behaviors, we conduct qualitative analysis on these examples whose reasoning paths are shorter or longer than the original gold reasoning paths. \n\n\\paragraph{Reasoning path only with single paragraph}\nFirst, we show two examples (one is a bridge-type question and the other is a comparison-type question), where our retriever selects single paragraph and terminates without selecting any additional paragraphs.\n\nThe bridge-type question in Table~\\ref{table:single_path} shows that, while originally this question requires a system to read two paragraphs, {\\bf Before I Go to Sleep (film)} and {\\bf Nicole Kidman}, our retriever and reader eventually choose {\\bf Nicole Kidman} only.\nThe second paragraph has a lot of lexical overlaps to the given question, and thus, a system may not need to read both of the paragraphs to answer.\n\nThe comparison-type question in Table~\\ref{table:single_path} also shows that even comparison-type questions do not always require two paragraphs to answer the questions, and our model only selects one paragraph necessary to answer the given example question.\nIn this example, the question has large lexical overlap with one of the ground-truth paragraph ({\\bf The Bears and I}), resulting in allowing our model to answer the question based on the single paragraph.\n\n\\cite{min2019compositional} also observed that some of the questions do not necessarily require multi-hop reasoning, while HotpotQA is designed to require multi-hop reasoning~\\citep{yang-etal-2018-hotpotqa}.\nIn that sense, we can say that our method automatical detects potentially single-hop questions.\n\n\\begin{table}[ht!]\n\\center\n\\begin{tabular}{p{0.95\\linewidth}}\n\\toprule \n\\multirow{2}{\\linewidth}{{\\bf Q [bridge]}:~~Before I Go to Sleep stars an Australian actress, producer and occasional what?}\n\\\\\n\\\\\\hline\n\\multirow{6}{\\linewidth}{ {\\bf Before I Go to Sleep (film)}:~~Before I Go to Sleep is a 2014 mystery psychological thriller film written and directed by Rowan Joffé and based on the 2011 novel of the same name by S. J. Watson. An international co-production between the United Kingdom, the United States, France, and Sweden, the film stars \\hl{Nicole Kidman}, Mark Strong, Colin Firth, and Anne-Marie Duff.}\\\\\n\\\\\n\\\\\n\\\\\n\\\\\n\\\\\\hdashline\n\\multirow{5}{\\linewidth}{{\\bf Nicole Kidman}:~~\\hl{Nicole Mary Kidman}, is an Australian actress, producer and occasional \\textcolor{red}{singer}. She is the recipient of several awards, including an Academy Award, two Primetime Emmy Awards, a BAFTA Award, three Golden Globe Awards, and the Silver Bear for Best Actress.}\n\\\\\n\\\\\n\\\\\n\\\\\n\\\\\n\\hdashline\nAnnotated reasoning path {\\bf Before I Go to Sleep (film)} $\\rightarrow$ {\\bf Nicole Kidman}\\\\\nPredicted reasoning path:  {\\bf Nicole Kidman}\n\\\\\\bottomrule\n\\\\\n\\toprule \n\\multirow{2}{\\linewidth}{{\\bf Q [comparison]}:~~In between The Bears and I and Oceans which was released on July 31, 1974, by Buena Vista Distribution?}\n\\\\\n\\\\\\hline\n\\multirow{5}{\\linewidth}{ {\\bf The Bears and I}:~~\\textcolor{red}{The Bears and I} is a 1974 American drama film directed by Bernard McEveety and written by John Whedon. The film stars Patrick Wayne, Chief Dan George, Andrew Duggan, Michael Ansara and Robert Pine. {\\it The film was released on July 31, 1974, by Buena Vista Distribution.}}\\\\\n\\\\\n\\\\\n\\\\\n\\\\\\hdashline\n\\multirow{3}{\\linewidth}{{\\bf Oceans (film)}:~~Oceans is a 2009 French nature documentary film directed, produced, co-written, and narrated by Jacques Perrin, with Jacques Cluzaud as co-director.}\n\\\\\n\\\\\n\\\\\n\\hdashline\nAnnotated reasoning path: {\\bf The Bears and I}, {\\bf Oceans (film)}\\\\\nPredicted reasoning path:  {\\bf The Bears and I}\n\\\\\\bottomrule\n\\end{tabular}\\caption{Two examples of the questions that our model retrieves a reasoning path with only one paragraph. We partly remove sentences irrelevant to the questions. Words in red correspond to the answer strings.}\\label{table:single_path}\n\\end{table}\n\\begin{table}[ht!]\n\\center\n\\begin{tabular}{p{0.95\\linewidth}}\n\\toprule \n\\multirow{2}{\\linewidth}{{\\bf Q}:~~Yoann Lemoine, a French video director, has created music videos for Lana Del Rey, Katy Perry, and an orchestral country pop ballad by which top pop artist?}\n\\\\\n\\\\\\hline\n\\multirow{5}{\\linewidth}{ {\\bf Yoann Lemoine}:~~Yoann Lemoine (born 16 March 1983) is a French music video director, graphic designer and singer-songwriter. His most notable works include his music video direction for Katy Perry's \"Teenage Dream\", Taylor Swift's single ``\\hl{Back to December}'', Lana Del Rey's ``Born to Die'' and Mystery Jets' ``Dreaming of Another World''.}\\\\\n\\\\\n\\\\\n\\\\\n\\\\\\hdashline\n\\multirow{5}{\\linewidth}{ {\\bf Back to December}:~~``\\hl{Back to December}'' is a song written and recorded by American singer/songwriter \\textcolor{red}{Taylor Swift} for her third studio album ``Speak Now'' (2010). ``Back to December'' is considered an orchestral country pop ballad and its lyrics are a remorseful plea for forgiveness for breaking up with a former lover.}\\\\\n\\\\\n\\\\\n\\\\\n\\\\\\hdashline\n\\multirow{6}{\\linewidth}{{\\bf Blue Jeans (Lana Del Rey song)}:~~``Blue Jeans'' is a song by American singer-songwriter Lana Del Rey for her second studio album ``Born to Die'' (2012). Produced by Emile Haynie, the song was written by Del Rey, Haynie, and Dan Heath. Charting across Europe and Asia, ``Blue Jeans'' reached the top 10 in Belgium, Poland, and Israel. The second was shot and directed by Yoann Lemoine, featuring film noir elements and crocodiles.}\n\\\\\n\\\\\n\\\\\n\\\\\n\\\\\n\\\\\n\\hdashline\nAnnotated reasoning path: {\\bf Yoann Lemoin} $\\rightarrow$ {\\bf Back to December}\\\\\nPredicted reasoning path:  {\\bf Blue Jeans (Lana Del Rey song)} $\\rightarrow$ {\\bf Yoann Lemoin} $\\rightarrow$ {\\bf Back to December}\n\\\\\\bottomrule\n\\end{tabular}\\caption{An example question where our model predicts reasoning paths of the length of three. Our model expects that the question is answerable based on the last paragraph of the annotated path. }\\label{table:three_paragraphs_path}\n\\end{table}\n\n\\begin{table}[t!]\n\\center\n\\begin{tabular}{p{0.97\\linewidth}}\n\\toprule \n\\multirow{2}{\\linewidth}{{\\bf Q}:~~Which songwriting duo composed music for \"La La Land\", and created lyrics for \"A Christmas Story: The Musica\"?}\n\\\\\n\\\\\n\\end{tabular}\n\\begin{tabular}{p{0.76\\linewidth} | c | c |c }\n\\hline\n\\small\n\\multirow{3}{*}{\\parbox{0.98\\linewidth}{{\\bf P1}: A Christmas Story: The Musical is a musical version of the film \"A Christmas Story ... The musical has music and lyrics written by \\hl{Pasek \\& Paul} and the book by Joseph Robinette.}} & \\multirow{2}{*}{0.98} & \\multirow{2}{*}{0.00}& \\multirow{2}{*}{0.00}\\\\\n&\\multirow{2}{*}{\\textcolor{red}{\\cmark}}&& \\\\\n&&& \\\\\\hdashline\n\\small\n\\multirow{4}{*}{\n\\parbox{0.98\\linewidth}{{\\bf P2}: \\textcolor{red}{Benj Pasek and Justin Paul}, known together as \\hl{Pasek and Paul}, are an American songwriting duo and composing team for musical theater, films, and television. ... they won both the Golden Globe and Academy Award for Best Original Song for the song \"City of Stars\".}} & \\multirow{2}{*}{0.08} & \\multirow{2}{*}{0.89}& \\multirow{2}{*}{0.00}\\\\\n&&& \\\\\n&&\\multirow{2}{*}{\\textcolor{red}{\\cmark}}& \\\\\n&&& \\\\\\hdashline\n\\small\n\\multirow{4}{*}{\n\\parbox{0.98\\linewidth}{{\\bf P3}: La La Land\" is a song recorded by American singer Demi Lovato. It was written by Lovato, Joe Jonas, Nick Jonas and Kevin Jonas and produced by the Jonas Brothers alongside John Fields, for Lovato's debut studio album, \"Don\\'t Forget\" (2008).}} & \\multirow{2}{*}{0.12} & \\multirow{2}{*}{0.00}& \\multirow{2}{*}{0.00}\\\\\n&&& \\\\\n&&& \\\\\n&&& \\\\\\bottomrule\n\\end{tabular}\n\\begin{tabular}{p{0.97\\linewidth}}\n\\multirow{2}{\\linewidth}{{\\bf Q}:~~Alexander Kerensky was defeated and destroyed by the Bolsheviks in the course of a civil war that ended when ?} \n\\\\\n\\\\\n\\end{tabular}\n\\\\\n\\begin{tabular}{p{0.76\\linewidth} | c | c |c }\n\\hline\n\\small\n\\multirow{6}{*}{\\parbox{0.98\\linewidth}{{\\bf P1}: The Socialist Revolutionary Party, or Party of Socialists-Revolutionaries sery\") was a major political party in early 20th century Russia and a key player in the Russian Revolution. ... The anti-Bolshevik faction of this party, known as the Right SRs, which remained loyal to the Provisional Government leader Alexander Kerensky was defeated and destroyed by the Bolsheviks in the course of \\hl{the Russian Civil War} and subsequent persecution.}} & \\multirow{4}{*}{0.95} & \\multirow{4}{*}{0.00}& \\multirow{4}{*}{0.00}\\\\\n&&& \\\\\n&&& \\\\\n&&& \\\\\n&\\multirow{2}{*}{\\textcolor{red}{\\cmark}}&& \\\\\n&&& \\\\\\hdashline\n\\small\n\\multirow{3}{*}{\n\\parbox{0.98\\linewidth}{{\\bf P2}: \\hl{The Russian Civil} War (November 1917 – \\textcolor{red}{October 1922}) was a multi-party war in the former Russian Empire immediately after the Russian Revolutions of 1917, as many factions vied to determine Russia\\'s political future.}} & \\multirow{2}{*}{0.00} & \\multirow{2}{*}{0.87}& \\multirow{2}{*}{0.00}\\\\\n&&\\multirow{2}{*}{\\textcolor{red}{\\cmark}}& \\\\\n&&& \\\\\\hdashline\n\\small\n\\multirow{3}{*}{\n\\parbox{0.98\\linewidth}{{\\bf P3}: Alexander Fyodorovich Kerensky was a Russian lawyer and key political figure in the Russian Revolution of 1917. }} & \\multirow{2}{*}{0.08} & \\multirow{2}{*}{0.09}& \\multirow{2}{*}{0.00}\\\\\n&&& \\\\\n&&& \\\\ \\bottomrule\n\\end{tabular}\n\\caption{Two examples from the HotpotQA distractor development set. Highlighted text shows the bridge entities for multi-hop reasoning, and also the words in red denote the predicted answer.}\\label{tab:example_distractor}\n\\end{table}\n\n\\paragraph{Reasoning path with three paragraphs}\nAll of the HotpotQA questions are authored by annotators who are shown two relevant paragraphs, and thus, originally the length of ground-truth reasoning paths is always two. \nOn the other hand, as our model accommodates arbitrary steps of reasoning, it often selects reasoning paths longer than the original annotations as shown in Table~\\ref{tab:comparison_num_retrieval}. \n{\\it When our model selects a longer reasoning path for a HotpotQA question, does it contain paragraphs that provide additional evidence?\n}\nWe show an example in Table~\\ref{table:three_paragraphs_path}, so as to answer this question.\nOur model selects an additional paragraph, {\\bf Blue Jeans (Lana Del Rey song)} at the first step, and then selects the two annotated gold paragraphs. \nThis first paragraph is strongly relevant to the given question, but does not contain the answer. This additional evidence might help the reader to find the correct bridge entity (``Back to December'').\n\n\\subsection{Qualitative analysis on the reasoning path on HotpotQA distractor}\nAlthough the main focus in this paper is on open-domain QA, we show the state-of-the-art performance on the HotpotQA distractor setting as well with the exactly same architecture. \nWe conduct qualitative analysis to understand our model's behavior in the closed setting.\nIn this setting, the two ground-truth paragraphs are always given for each question.\n\nTable~\\ref{tab:example_distractor} shows two examples from the HotpotQA distractor setting. \nIn the first example, {\\bf P1} and {\\bf P2} are its corresponding ground-truth paragraphs.\nAt the first time step, our retriever does not expect that {\\bf P2} is related to the evidence to answer the question, as the retriever is not aware of the bridge entity, ``Pasek \\& Paul''.\nIf we simply adopt the Re-rank strategy, {\\bf P3} with the second highest probability is selected, resulting in a wrong paragraph selection. \nIn our framework, our retriever is conditioned on the previous retrieval history and thus, at the second time step, it chooses the correct paragraph, {\\bf P2}, lowering the probability of {\\bf P3}.\nThis clearly shows the effectiveness of our multi-step retrieval method in the closed setting as well.\nAt the third step, our model stops the prediction by outputting [EOS].\nIn 588 examples (7.9\\%) of the entire distractor development dataset, the paragraph selection by our graph-based recurrent retriever differs from the top-2 strategy. \n\nWe present another example, where only the graph-based recurrent retrieval model succeeds in finding the correct paragraph pair, ({\\bf P1}, {\\bf P2}).\nThe second question in Table~\\ref{tab:example_distractor} shows that at the first time step our retriever successfully selects {\\bf P1}, but does not pay attention to {\\bf P2} at all, as the retriever is not aware of the bridge entity, ``the Russian Civil War''.\nAgain, once it is conditioned on {\\bf P1}, which includes the bridge entity, it can select {\\bf P2} at the second time step. \nLike this, we can see how our model successfully learns to model relationships between paragraphs for multi-hop reasoning.\n\n\\section{Additional Results on SQuAD Open and Natural Questions Open}\n\\label{sec:squad_nq_additional_result}\n\n\\begin{table}[t]\n\n\\begin{center}\n\\begin{tabular}{c| c |c || c | c}\\toprule\n & \\multicolumn{2}{c}{SQuAD Open} & \\multicolumn{2}{c}{Natural Questions Open} \\\\\n & Retriever & Reader & Retriever & Reader \\\\\nAvg. \\# of $L$ & 1.00 & 1.08  & 1.23 & 1.54 \\\\\n\\midrule\n$1$ &     10,570 &    9,759 &    6,719 &    4,047 \\\\\n$2$ & ~~~~~~~~~0 &   ~~~811 &    2,038 &    4,702 \\\\\n$3$ & ~~~~~~~~~0 & ~~~~~~~0 & ~~~~~~~0 & ~~~~~~~8 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{{\\bf Statistics of the reasoning paths for SQuAD Open and Natural Questions Open:} the average length and the distribution of length of the reasoning paths selected by our retriever and reader for SQuAD Open and Natural Questions Open.\n}\\label{tab:comparison_num_retrieval_squad_nq}\n\\end{center}\n\n\\end{table}\n\nAlthough the main focus of this work is on multi-hop open-domain QA, our framework shows competitive performance on the two open-domain QA datasets, SQuAD Open and Natural Questions Open.\nBoth of the two dataets are originally created by assigning a single ground-truth paragraph for each question, and in that sense, our framework is not specific to multi-hop reasoning tasks.\nIn this section, we further analyze our experimental results on the two datasets.\n\n\\paragraph{SQuAD Open}\n\nTable~\\ref{tab:comparison_num_retrieval_squad_nq} shows statistics of the lengths of the selected reasoning paths on our SQuAD Open experiment.\nThis table is analogous to Table~\\ref{tab:comparison_num_retrieval} on our HotpotQA experiments.\nWe can clearly see that our recurrent retriever always outputs a single paragraph for each question, if we only use the top-1 predictions.\nThis is because our retriever model for this dataset is trained with the single-paragraph annotations.\nOur beam search can find longer reasoning paths, and as a result, the re-ranking process in our reader model somtimes selects the reasoning paths including two paragraphs.\nThe trend is consistent with that in Table~\\ref{tab:comparison_num_retrieval}.\nHowever, the effects of selecting more than one paragraph do not have a big impact; we observed only 0.1\\% F1/EM improvement over our method with restricting the path length to one (based on the same experiment with $L=1$ in Table~\\ref{tab:fixed_length}).\nConsidering that SQuAD is a single-hop QA dataset, the result matches our intuition.\n\n\\paragraph{Natural Questions Open}\n\nTable~\\ref{tab:comparison_num_retrieval_squad_nq} also shows the results on Natural Questions Open, where we see the same trend again.\nThanks to the ground-truth path augmentation technique, our recurrent retriever model prefers longer reasoning paths than those on SQuAD Open.\nWe observed 1\\% EM improvement over the $L=1$ baseline on Natural Questions Open, and next we show an example to discuss why our reasoning path approach can be effective on this dataset.\n\nTable~\\ref{tab:example_text_nq} shows one example where our model finds a multi-hop reasoning path effectively in Natural Questions Open (development set).\nThe question ``who sang the original version of killing me so'' has relatively fewer lexical overlap with the originally annotated paragraph ({\\bf Killing Me Softly with His Song (V)} in Table~\\ref{tab:example_text_nq}).\nMoreover, there are several entities named as ``killing me softly'' in Wikipedia, because many artists cover the song. \nTo answer this question correctly, our retriever first selects {\\bf Roberta Flack (I)}, and then hops to the originally annotated paragraph, {\\bf Killing Me Softly with His Song (V)}. \nOur reader further verifies this reasoning path and extracts the correct answer from  {\\bf Killing Me Softly with His Song (V)}.\nThis example shows that even without gold reasoning paths annotations, our model trained on the augmented examples learns to retrieve multi-hop reasoning paths from the entire Wikipedia.\n\nThese detailed experimental results on the two other open-domain QA datasets demonstrate that our framework learns to retrieve reasoning paths flexibly with evidence sufficient to answer a given question, according to each dataset's nature.\n\n\\begin{table}[t]\n\\center\n\\begin{tabular}{p{0.96\\linewidth}}\n\\toprule \n\\multirow{2}{\\linewidth}{{\\bf Q}:~~who sang the original version of killing me softly }\n\\\\\n\\\\\\hline\n\\multirow{4}{\\linewidth}{ {\\bf Roberta Flack (I)}:~~Roberta Cleopatra Flack (born February 10, 1937) is an American singer. She is known for her No. 1 singles \"The First Time Ever I Saw Your Face\", \"\\hl{Killing Me Softly with His Song}\"...}\n\\\\\n\\\\\n\\\\\n\\\\\\hdashline\n\\multirow{4}{\\linewidth}{{\\bf Killing Me Softly with His Song (V)}, The song was written in collaboration with Lori Lieberman, who recorded the song in late 1971. In 1973 it became a number - one hit in the US and Canada for \\textcolor{red}{Roberta Flack}, Many artists have covered the song....}\n\\\\\n\\\\\n\\\\\n\\\\\\hdashline\nAnnotated reasoning path: {\\bf Killing Me Softly with His Song (V)}\\\\\nPredicted reasoning Path: {\\bf Roberta Flack (I)} $\\rightarrow$ {\\bf Killing Me Softly with His Song (V)}\n\\\\\\bottomrule\n\\end{tabular}\n\\caption{An example from Natural Questions Open. The bold text represents titles and paragraph indices (e.g., (I) denotes that the paragraph is an introductory paragraph). \nThe highlighted phrase represents a bridge entity and the text in red represents an answer span.\n}\\label{tab:example_text_nq}\n\\end{table}\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nWe propose a simple and efficient multi-hop dense retrieval approach for answering complex open-domain questions, which achieves state-of-the-art performance on two multi-hop datasets, HotpotQA and multi-evidence FEVER. Contrary to previous work, our method does not require access to any corpus-specific information, such as inter-document hyperlinks or human-annotated entity markers, and can be applied to any unstructured text corpus. Our system also yields a much better efficiency-accuracy trade-off, matching the best published accuracy on HotpotQA while being 10 times faster at inference time.\\footnote{\\url{https://github.com/facebookresearch/multihop_dense_retrieval}.}\n\n\\end{abstract}\n\n\\section{Introduction}\n\n\\emph{Open domain question answering} is a challenging task where the answer to a given question needs to be extracted from a large pool %of millions \nof documents.  \nThe prevailing approach~\\citep{drqa} tackles the problem in two stages. Given a question, a \\emph{retriever} first produces a %short \nlist of $k$ candidate documents, and a \\emph{reader} then extracts the answer from this %reduced \nset.\nUntil recently, retrieval models were dependent on traditional term-based information retrieval (IR) methods, which fail to capture the semantics of the question beyond lexical matching and remain a major performance bottleneck for the task. Recent work on dense retrieval methods instead uses pretrained encoders to cast the question and documents into dense representations in a vector space and relies on fast maximum inner-product search (MIPS) to complete the retrieval. These approaches~\\citep{ORQA,REALM,DPR} have demonstrated significant retrieval improvements over traditional IR baselines.  \n\nHowever, such methods remain limited to \\emph{simple} questions, where the answer to the question is explicit in a single piece of text evidence. In contrast, \\emph{complex} questions typically involve aggregating information from multiple documents, requiring logical reasoning or sequential (multi-hop) processing in order to infer the answer (see Figure \\ref{fig:example} for an example). %Due to the potentially sequential nature of such questions,\nSince the process for answering such questions might be sequential in nature, single-shot approaches to retrieval are insufficient. Instead, iterative methods are needed to recursively retrieve new information at each step, conditioned on the information already at hand.\nBeyond further expanding the scope of existing textual open-domain QA systems, answering more complex questions usually involves \\textit{multi-hop reasoning}, which poses unique challenges for existing neural-based AI systems. With its practical and research values,\nmulti-hop QA has been extensively studied recently~\\citep{ComplexWebQ,HotpotQA,Wikihop} and remains an active research area in NLP~\\citep{GoldEn,SMR,DecomQA,Transformer-XH,GraphRecurrentRetriever,UQD}.\n\nThe main problem in answering multi-hop open-domain questions is that the search space grows exponentially with each retrieval hop. Most recent work tackles this issue by constructing a document graph utilizing either entity linking or existing hyperlink structure in the underlying Wikipedia corpus~\\citep{SMR,GraphRecurrentRetriever}. The problem then becomes finding the best path in this graph, where the search space is bounded by the number of hyperlinks in each passage. However, such methods may not generalize to new domains, where entity linking might perform poorly, or where hyperlinks might not be as abundant as in Wikipedia. Moreover, efficiency remains a challenge despite using these data-dependent pruning heuristics, with the best model~\\citep{GraphRecurrentRetriever} needing hundreds of calls to large pretrained models to produce a single answer.  \n\nIn contrast, we propose to employ dense retrieval to the multi-hop setting with a simple recursive framework.\nOur method iteratively encodes the question and previously retrieved documents as a query vector and retrieves the next relevant documents using efficient MIPS methods. \nWith high-quality, dense representations derived from strong pretrained encoders, our work first demonstrates that the sequence of documents that provide sufficient information to answer the multi-hop question can be \naccurately discovered from unstructured text, \\emph{without} the help of corpus-specific hyperlinks. \nWhen evaluated on two multi-hop benchmarks, HotpotQA~\\citep{HotpotQA} and a multi-evidence subset of FEVER~\\citep{FEVER}, our approach improves greatly over the traditional linking-based retrieval methods. %(e.g., 28\\% relative gain in Recall@20 for HotpotQA). \nMore importantly, the better retrieval results also lead to state-of-the-art downstream results on both datasets. On HotpotQA, we demonstrate a vastly improved efficiency-accuracy trade-off achieved by our system: by limiting the amount of retrieved contexts fed into downstream models, our system can match the best published result while being 10x faster.\n\n\\begin{figure}[t]\n\\vspace{-0.2in}\n\\includegraphics[width=\\linewidth]{overview.pdf}\n\\vspace{-0.2in}\n \\caption{An overview of the multi-hop dense retrieval approach.}\n\\vspace{-0.2in}\n\\label{fig:example}\n\\end{figure}\n\n\\section{Method}\n\n\\subsection{Problem Definition} \n\nThe retrieval task considered in this work can be described as follows~(see also Figure~\\ref{fig:example}). \nGiven a multi-hop question~$q$ and a large text corpus $\\mathcal{C}$, the retrieval module needs to retrieve a sequence of passages $\\mathcal{P}_{seq} :\\{p_1, p_2, ..., p_{n}\\}$ that provide \\emph{sufficient} information for answering $q$.  \nPractically, the retriever returns the $k$ best-scoring sequence candidates, $\\{\\mathcal{P}_{seq}^1, \\mathcal{P}_{seq}^2, ..., \\mathcal{P}_{seq}^{k}\\}$ ($k \\ll |\\mathcal{C}|$), with the hope that at least one of them has the desired qualities. \n$k$ should be small enough for downstream modules to process in a reasonable time while maintaining adequate recall.\nIn general, retrieval also needs to be efficient enough to handle real-world corpora containing millions of documents.\n\n\\subsection{Multi-Hop Dense Retrieval} \n\\label{sec:model}\n\n\\paragraph{Model} Based on the sequential nature of the multi-hop retrieval problem, our system solves it in an iterative fashion. We model the probability of selecting a certain passage sequence as follows:\n\\begin{align*}\n    P(\\mathcal{P}_{seq}|q) = \\prod_{t=1}^{n} P(p_t|q,p_1, ..., p_{t-1}),\n\\end{align*}\nwhere for $t=1$, we only condition on the original question for retrieval. At each retrieval step, we construct a new query representation based on previous results and the retrieval is implemented as maximum inner product search over the dense representations of the whole corpus:\n\\begin{align*}\n    P(p_t|q,p_1, ..., p_{t-1}) = \\frac{\\exp{(\\langle \\vp_t, \\vq_t \\rangle})}{\\sum_{p \\in \\mathcal{C}} \\exp{(\\langle \\vp, \\vq_t \\rangle)}},  \\textrm{ where } \\vq_t = g(q, p_1, ..., p_{t-1}) \\textrm{ and } \\vp_t = h(p_t).\n\\end{align*}\nHere $\\langle\\cdot,\\cdot\\rangle$ is the inner product between the query and passage vectors. $h(\\cdot)$ and and $g(\\cdot)$ are passage and query encoders that produce the dense representations. In order to reformulate the query representation to account for previous retrieval results at time step $t$, \nwe simply concatenate the question and the retrieved passages as the inputs to $g(\\cdot)$. Note that our formulation for each retrieval step is similar to existing single-hop dense retrieval methods~\\citep{ORQA,REALM,DPR} except that we add the query reformulation process conditioned on previous retrieval results. Additionally, instead of using a bi-encoder architecture with separately parameterized encoders for queries and passages, we use a shared  RoBERTa-base~\\citep{RoBERTa} encoder for both $h(\\cdot)$ and $g(\\cdot)$. In~\\cref{sec:ablation}, we show this simple modification yields considerable improvements.\nSpecifically, we apply layer normalization over the start token's representations from RoBERTa to get the final dense query/passage vectors. \n\n\\paragraph{Training and Inference} The retriever model is trained as in~\\citet{DPR}, where each input query (which at each step consists of a question and previously retrieved passages) is paired with a positive passage and $m$ negative passages to approximate the softmax over all passages.  The positive passage is the gold annotated evidence at step $t$.  Negative passages are a combination of passages in the current batch which correspond to other questions (\\emph{in-batch}), and \\emph{hard} negatives which are false adversarial passages.  In our experiments, we obtain hard negatives from TF-IDF retrieved passages and their linked pages in Wikipedia.  We note that using hyperlinked pages as additional negatives is neither necessary nor critical for our approach.  In fact we observe only a very small degradation in performance if we remove them from training (\\cref{sec:ablation}). In addition to in-batch negatives, we use a memory bank ($\\mathcal{M}$) mechanism~\\citep{MemoryBank} to further increase the number of negative examples for each question. The memory bank stores a large number of dense passage vectors. As we block the gradient back-propagation in the memory bank, its size ($|\\mathcal{M}| \\gg$ batch size) is less restricted by the GPU memory size. Specifically, after training to convergence with the shared encoder, we freeze a copy of the encoder as the new passage encoder and collect a bank of passage representations across multiple batches to serve as the set of negative passages. This simple extension results in further improvement in retrieval. (\\cref{sec:ablation}).\n\nFor inference, we first encode the whole corpus into an index of passage vectors. Given a question, we use beam search to obtain top-$k$ passage sequence candidates, where the candidates to beam search at each step are generated by MIPS using the query encoder at step $t$, and \nthe beams are scored by the sum of inner products\nas suggested by the probabilistic formulation discussed above.  Such inference relies only on the dense passage index and the query representations, and does not need explicit graph construction using hyperlinks or entity linking. The top-$k$ sequences will then be fed into task-specific downstream modules to produce the desired outputs. \n\n\\section{Experiments} \n\\label{sec:task}\n\n\\paragraph{Datasets} \nOur experiments focus on two datasets: \\emph{HotpotQA} and \\emph{Multi-evidence FEVER}.\nHotpotQA~\\citep{HotpotQA} includes 113k multi-hop questions.\nUnlike other multi-hop QA datasets~\\citep{MetaQA,ComplexWebQ,Wikihop}, where the information sources of the answers are knowledge bases, \nHotpotQA uses documents in Wikipedia. Thus, its questions are not restricted by the fixed KB schema and can cover more diverse topics. Each question in HotpotQA is also provided with ground truth support passages, which enables us to evaluate the intermediate retrieval performance.\nMulti-evidence FEVER includes 20k claims from the FEVER~\\citep{FEVER} fact verification dataset, where the claims can only be verified using multiple documents. \nWe use this dataset to validate the general applicability of our method.\n\n\\paragraph{Implementation Details} \nAll the experiments are conducted on a machine with 8 32GB V100 GPUs. Our code is based on Huggingface Transformers~\\citep{wolf2019huggingface}. Our best retrieval results are predicted using the exact inner product search index (IndexFlatIP) in FAISS~\\citep{FAISS}. Both datasets assume 2 hops, so we fix $n=2$ for all experiments. Since HotpotQA does not provide the order of the passage sequences, as a heuristic, we consider the passage that includes the answer span as the final passage.\n\\footnote{If the answer span is in both, the one that has its title mentioned in the other passage is treated as the second.}\nIn \\cref{sec:ablation}, we show that the order of the passages is important for effective retriever training. The hyperparameters can be found in Appendix~\\ref{appendix:hyperparameter}. \n\n\\subsection{Experiments: Retrieval}\n\\label{sec:exp}\n\nWe evaluate our multi-hop dense retriever (\\method) in two different use cases: \\emph{direct} and \\emph{reranking}, where the former outputs the top-$k$ results directly using the retriever scores and the latter applies a task-specific reranking model to the initial results from \\method.\n\n\\subsubsection{Direct}\n\nWe first compare \\method with several efficient retrieval methods that can directly find the top-$k$ passage sequences from a large corpus, including TF-IDF, TF-IDF + Linked, DrKIT and Entity Linking. \\textbf{TF-IDF} is the standard term-matching baseline, while \\textbf{TF-IDF + Linked} is a straightforward extension that also extracts the hyperlinked passages from TF-IDF passages, and then reranks both TF-IDF and hyperlinked passages with BM25~\\footnote{https://pypi.org/project/rank-bm25} scores. \\textbf{DrKIT}~\\citep{DrKIT} is a recently proposed dense retrieval approach, which builds a entity-level (mentions of entities) dense index for retrieval. It relies on hyperlinks to extract entity mentions and prunes the search space with a binary mask that restricts the next hop to using hyperlinked entities. \nOn FEVER, we additionally consider an entity linking baseline~\\citep{UKP} that is commonly used in existing fact verification pipelines. This baseline first uses a constituency parser to extract potential entity mentions in the fact claim and then uses the MediaWiki API to search documents with titles that match the mentions. \n\nTable~\\ref{tab:efficient_baselines} shows the performance of different retrieval methods. On HotpotQA the metric is recall at the top $k$ paragraphs\\footnote{As the sequence length is 2 for HotpotQA, we pick the top $k$/2 sequences predicted by \\method.}, while on FEVER the metrics are precision, recall and F$_1$ in order to be consistent with previous results. On both datasets, \\method substantially outperforms all baselines.\n\n\\begin{table}\n    \\centering\n    \\small\n    \\vspace{-0.4in}\n    \\caption{Retrieval performance in recall at $k$ retrieved passages and precision/recall/F$_1$.}\n    \\begin{tabular}{c|ccc|ccc}\n    \\toprule\n        \\multirow{2}{*}{Method} & \\multicolumn{3}{c|}{HotpotQA} & \\multicolumn{3}{c}{FEVER} \\\\\n        & R@2 & R@10 & R@20 & Precision & Recall & F$_1$ \\\\ \n        \\midrule\n    TF-IDF & 10.3 & 29.1 & 36.8 & 14.9 & 28.2 & 19.5 \\\\\n    TF-IDF + Linked  & 17.3 & 50.0 & 62.7 & 18.6 & 35.8 & 24.5 \\\\\n    DrKIT & 38.3 &  67.2 & 71.0 & - & - & - \\\\\n    Entity Linking & - & - & - & 30.6 & 53.8 & 39.0\\\\\n    \\midrule\n    \\method & \\textbf{65.9} & \\textbf{77.5} & \\textbf{80.2} & \\textbf{45.7} & \\textbf{69.1} & \\textbf{55.0}\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\vspace{-0.2in}\n    \\label{tab:efficient_baselines}\n\\end{table}\n\n\\subsubsection{Reranking}\n\\label{sec:reranking}\n\n\\emph{Reranking} documents returned by efficient retrieval methods with a more sophisticated model is a common strategy for improving retrieval quality.\nFor instance, state-of-the-art multi-hop QA systems usually augment traditional IR techniques with large pretrained language models to select a more compact but precise passage set.\nOn HotpotQA, we test the effectiveness of \\method after a simple cross-attention reranking: each of the top $k$ passage sequences from \\method is first prepended with the original question and then fed into a pretrained Transformer encoder, i.e., ELECTRA-large~\\citep{ELECTRA}, that predicts relevant scores. We train this reranking model with a binary cross-entropy loss, with the target being whether the passage sequence cover both groundtruth passages.\nWe empirically compare our approach with two other existing reranking-based retrieval methods:\n\\textbf{Semantic Retrieval}~\\citep{SMR} uses BERT at both passage-level and sentence-level to select context from the initial TF-IDF and hyperlinked passages; \\textbf{Graph Recurrent Retriever}~\\citep{GraphRecurrentRetriever} learns to recursively select the best passage sequence on top of a hyperlinked passage graph, where each passage node is encoded with BERT.\n\n\\begin{table}[h]\n    \\vspace{-0.20in}\n    \\begin{minipage}[t]{.45\\textwidth}\n    \\centering\n    \\small\n    \\caption{HotpotQA reranked retrieval results (input passages for final answer prediction).}\n    \\begin{tabular}{lcc}\n    \\toprule\n    Method & SP EM & Ans Recall \\\\\n    \\midrule\n    Semantic Retrieval  & 63.9 & 77.9 \\\\\n    Graph Rec Retriever & 75.7 & 87.5 \\\\\n    \\midrule \n    \\method (direct)     & 65.9 & 75.4 \\\\\n    \\method (reranking) & \\textbf{81.2} & \\textbf{88.2} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\label{tab:retrieval_rerank}\n    \\end{minipage}%\n    \\hfill\n    \\begin{minipage}[t]{.5\\textwidth}\n    \\centering\n    \\small\n    \\caption{Retriever Model Ablation on HotpotQA retrieval. \\emph{Single-hop} here is equivalent to the DPR method~\\citep{DPR}.}\n    \\begin{tabular}{l|ccc}\n    \\toprule\n    Retriever variants & R@2 & R@10 & R@20 \\\\\n    \\midrule\n    Full Retrieval Model & 65.9 & 77.5 & 80.2 \\\\\n    - w/o linked negatives & 64.6 & 76.8 & 79.6\\\\\n    - w/o memory bank & 63.7 & 74.2 & 77.2\\\\\n    - w/o shared encoder & 59.9 & 70.6 & 73.1 \\\\\n    - w/o order & 17.6 & 55.6 & 62.3\\\\\n    Single-hop & 25.2 & 45.4 & 52.1\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\label{tab:ablation}\n    \\end{minipage}\n\\end{table}\n\nTable~\\ref{tab:retrieval_rerank} shows the reranking results. \nFollowing \\citet{GraphRecurrentRetriever}, we use \\emph{Answer Recall} and \\emph{Support Passage Exact Match (SP EM)} \\footnote{Whether the final predicted sequence covers both gold passages.} as the evaluation metrics.\nEven without reranking, \\method is already better than Semantic Retrieval, which requires around 50 BERT encoding (where each encoding involves cross-attention over a concatenated question-passage pair). After we rerank the top-100 sequences from the dense retriever, our passage recall is better than the state-of-the-art Graph Recurrent Retriever, which uses BERT to process more than 500 passages. \nWe do not compare the reranked results on FEVER, as most FEVER systems directly use BERT encoder to select the top evidence \\textit{sentences} from the retrieved documents, instead of the reranking the documents.\n\n\\subsubsection{Analysis}\n\\label{sec:retrieval_analysis}\n\\label{sec:error_analysis}\n\nTo understand the strengths and weaknesses of \\method, we conduct further analysis on HotpotQA dev. %the HotpotQA dev set\n\n\\paragraph{Retrieval Error Analysis}\n\n\\begin{wrapfigure}{r}{0.45\\textwidth}\n\\includegraphics[width=\\linewidth]{retrieva_types.pdf}\n\\vspace{-20pt}\n\\caption{The retrieval performance gap between comparison and bridge questions. Left: recall of groundtruth passage sequences without reranking. Right: Top-1 chain exact match after reranking.}\n\\vspace{-10pt}\n\\label{fig:typed_retrieval}\n\\end{wrapfigure}\n\nHotpotQA contains two question categories: \\emph{bridge} questions in which an intermediate entity is missing and needs to be retrieved before inferring the answer; and \\emph{comparison} questions where two entities are mentioned simultaneously and compared in some way. In Figure~\\ref{fig:typed_retrieval}, we show the retrieval performance of both question types.  The case of \\textit{comparison} questions proves easier, since both entities needed for retrieval are present in the question. \n\nThis case appears almost solved, confirming recent work demonstrating that dense retrieval is very effective at entity linking~\\citep{blink}.\n\nFor the case of \\textit{bridge} questions, we manually inspect 50 randomly sampled erroneous examples after reranking. We find that in half of these cases, our retrieval model predicts an alternative passage sequence that is also valid (see Appendix~\\ref{sec:false_errors} for examples). This gives an estimated top-1 passage sequence accuracy of about 90\\%. \nOther remaining errors are due to the dense method's inability to capture the exact n-gram match between the question and passages. This is a known issue~\\citep{ORQA, DPR} of dense retrieval methods when dealing with questions that have high lexical overlap with the passages. To this end, a hybrid multi-hop retrieval method with both term and dense index might be used to further improve the performance on \\textit{bridge} questions. \n\n\\paragraph{Retriever Ablation Study} % If this part is not removed, then the goal is to test the impact of different retriever to the end task.\n\\label{sec:ablation}\nIn Table~\\ref{tab:ablation}, we examine our model with different variations on HotpotQA to show the effectiveness of each proposed component. We see that further training with a memory bank results in modest gains, while using a shared encoder is crucial for the best performance. Respecting the ordering of passages in two hops is essential - training in an order-agnostic manner hardly works at all, and underperforms even the single-hop baseline. Finally, not using hyperlinked paragraphs from TF-IDF passages as additional negatives has only a minor impact on performance.\n\n\\paragraph{Question Decomposition for Retrieval}\n\\label{sec:decomposition}\nAs multi-hop questions have more complex structures than simple questions, recent studies~\\citep{DecomQA,UQD} propose to use explicit question decomposition to simplify the problem. \n\\citet{Break} shows that with TF-IDF, using decomposed questions improves the retrieval results. \nWe investigate whether the conclusion still holds with stronger dense retrieval methods. \nWe use the human-annotated question decomposition from the QDMR dataset~\\citep{Break} for analysis. For a question like \\texttt{Q:Mick Carter is the landlord of a public house located at what address?}, QDMR provides two subquestions, \\texttt{SubQ1: What is the public house that Mick Carter is the landlord of?} and \\texttt{SubQ2: What is the address that \\#1 is located at?}.\nWe sample 100 bridge questions and replace \\texttt{\\#1} in \\texttt{SubQ2} with the correct answer (The Queen Victoria) to \\texttt{SubQ1}. \nNote that this gives advantages to the decomposed method as we ignore any intermediate errors. \nWe estimate the performance of potential decomposed methods with the state-of-the-art single-hop dense retrieval model~\\citep{DPR}.\n\n\\begin{wraptable}{r}{7.5cm}\n    \\centering\n    \\small\n    \\vspace{-0.1in}\n    \\caption{Comparison with decomposed dense retrieval which uses oracle question decomposition (test on 100 bridge questions). See text for details about the decomposed settings.}\n    \\begin{tabular}{lccc}\n    \\toprule\n      Method & R@2 & R@10 & R@20 \\\\\n    \\midrule\n     \\method & 54.9 & 63.7 & 70.6\\\\\n     Decomp (SubQ1;SubQ2) & 50.0 & 64.7 & 67.6 \\\\\n     Decomp (Q;SubQ2) & 51.0 & 64.7 & 68.6 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\label{tab:decomposition}\n\\end{wraptable}\n\nAs shown in Table~\\ref{tab:decomposition}, we did not observe any strong improvements from explicit question decomposition, which is contrary to the findings by \\citet{Break} when using term-based IR methods. Moreover, as shown in the third row of the table, when the 1st hop of the decomposed retrieval (i.e., \\texttt{SubQ1}) is replaced with the original question, no performance degradation is observed. This suggests that strong pretrained encoders can effectively learn to select necessary information from the multi-hop question at each retrieval step. Regarding the performance drop when using explicit compositions, we hypothesize that it is because some information in one decomposed subquestion could be useful for the other retrieval hop. Examples supporting this hypothesis can be found in Appendix \\ref{appendix:decomposed_exmaples}. While this could potentially be addressed by a different style of decomposition, our analysis suggests that decomposition approaches might be sub-optimal in the context of dense retrieval with strong pretrained encoders.\n\n\\subsection{Experiments: HotpotQA}\n\\label{sec:exp-hotpotqa}\n\nWe evaluate how the better retrieval results of \\method improve multi-hop question answering in this section. As our retriever system is agnostic to downstream models, we test two categories of answer prediction architectures: \nthe \\emph{extractive} span prediction models based on pretrained masked language models, such as BERT~\\citep{BERT} and ELECTRA~\\citep{ELECTRA},\nand the retrieval-augmented \\emph{generative} reader models~\\citep{RAG,FiD}, which are based on pretrained sequence-to-sequence (seq2seq) models such as BART~\\citep{BART} and T5~\\citep{t5}. \nNote that compared to more complicated graph reasoning models~\\citep{HGN,Transformer-XH}, these two classes of models do not rely on hyperlinks and can be applied to any text.\n\n\\textbf{Extractive} reader models learn to predict an answer span from the concatenation of the question and passage sequence ([$q$, $p_1$, ..., $p_n$]). On top of the token representations produced by pretrained models, we add two prediction heads to predict the start and end position of the answer span.\\footnote{To account for yes/no questions, we prepend \\emph{yes} and \\emph{no} tokens to the context.} To predict the supporting sentences, we add another prediction head and predict a binary label at each sentence start. For simplicity, the same encoder is also responsible for reranking the top $k$ passage sequences. The reranking detail has been discussed in \\cref{sec:reranking}. Our best reader model is based on ELECTRA~\\citep{ELECTRA}, which has achieved the best single-model performance on the standard SQuAD~\\citep{SQuAD} benchmark. Additionally, we also report the performance of BERT-large with whole word masking (BERT-wwm) to fairly compare with~\\citet{GraphRecurrentRetriever}. \n\n\\textbf{Generative} models, such as RAG~\\citep{RAG} and FiD~\\citep{FiD}, are based on pretrained seq2seq models. These methods finetune pretrained models with the concatenated questions and retrieved documents as inputs, and answer tokens as outputs. This generative paradigm has shown state-of-the-art performance on single-hop open-domain QA tasks. Specifically, FiD first uses the T5 encoder to process each retrieved passage sequence independently and then uses the decoder to perform attention over the representations of all input tokens while generating answers. RAG is built on the smaller BART model. Instead of only tuning the seq2seq model, it also jointly train the question encoder of the dense retriever. We modified it to allow multi-hop retrieval. \n\nMore details about these two classes of reader models are described in Appendix~\\ref{appendix:multihop_rag}.\n\n\\subsubsection{Results}\n\\begin{table*}[h]\n    \\centering\n    \\small\n    \\vspace{-0.3in}\n    \\caption{HotpotQA-fullwiki test results.}\n    \\begin{tabular}{lcccccc}\n    \\toprule\n       \\multirow{2}{*}{Methods} &  \\multicolumn{2}{c}{Answer} & \\multicolumn{2}{c}{Support} & \\multicolumn{2}{c}{Joint}\\\\ & EM & F1 & EM & F1 & EM & F1 \\\\\n    \\midrule\n    GoldEn Retriever~\\citep{GoldEn} & 37.9 & 48.6 & 30.7 & 64,2 & 18.9 & 39.1\\\\\n    Semantic Retrieval~\\citep{SMR} & 46.5 & 58.8 & 39.9 & 71.5 & 26.6 & 49.2\\\\\n    Transformer-XH~\\citep{Transformer-XH} & 51.6 & 64.1 & 40.9 & 71.4 & 26.1 & 51.3 \\\\\n    HGN~\\citep{HGN} & 56.7 & 69.2 & 50.0 & 76.4 & 35.6 & 59.9\\\\\n    DrKIT~\\citep{DrKIT} & 42.1 &  51.7 & 37.1 & 59.8 & 24.7 & 42.9\\\\\n    Graph Recurrent Retriever \\citep{GraphRecurrentRetriever} & 60.0 & 73.0 & 49.1 & 76.4 & 35.4 & 61.2 \\\\\n\n    \\midrule \n    \\method (ELECTRA Reader) & \\textbf{62.3} & \\textbf{75.3} & \\textbf{57.5} & \\textbf{80.9} & \\textbf{41.8} & \\textbf{66.6}\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\label{tab:QA}\n    \\vspace{-0.2in}\n\\end{table*}\\paragraph{Comparison with Existing Systems} Table~\\ref{tab:QA} compares the HotpotQA test performance of our best ELECTRA reader with recently published systems, using the numbers from the official leaderboard, which measure answer and supporting sentence exact match (EM)/F1 and joint EM/F1. \nAmong these methods, only GoldEn Retriever~\\citep{GoldEn} does not exploit hyperlinks. In particular, Graph Recurrent Retriever trains a graph traversal model for chain retrieval; TransformerXH~\\citep{Transformer-XH} and HGN~\\citep{HGN} explicitly encode the hyperlink graph structure within their answer prediction models. In fact, this particular inductive bias provides a perhaps unreasonably strong advantage in the specific context of HotpotQA, which by construction guarantees ground-truth passage sequences to follow hyperlinks.  \nDespite not using such prior knowledge, our model outperforms all previous systems by large margins, especially on supporting fact prediction, which benefits more directly from better retrieval. \n\n\\paragraph{Reader Model Variants}\n\\label{sec:reader_comparison}\n    \n \n\n\\begin{wraptable}{r}{7.5cm}\n    \\centering\n    \\small\n    \\vspace{-0.1in}\n    \\caption{Reader comparison on HotpotQA dev set.}\n    \\begin{tabular}{l|l|l|cc}\n    \\toprule\n    \\textbf{}                   & Model & Top k & EM    & F1    \\\\ \\midrule\n        \\multirow{3}{*}\n    {Extractive}\n    & ELECTRA & Top 50 &  61.7 & 74.3 \\\\\n    & ELECTRA & Top 250 &  63.4 & 76.2 \\\\ & BERT-wwm & Top 250 & 61.5 & 74.7 \\\\\n    \\midrule\n    \\multirow{2}{*}{Generative} & Multi-hop RAG  &  Top 4*4 & 51.2 & 63.9 \\\\\n                                & FiD   & Top 50  & 61.7 &  73.1 \\\\ \n \n     \\bottomrule\n    \\end{tabular}\n    \\label{tab:reader}\n\\end{wraptable}\n\nResults for reader model variants are shown in Table~\\ref{tab:reader}.\\footnote{For the compute-heavy generative models, we feed in as many passages as possible without running into memory issues (Muli-hop RAG takes top 4 passages from hop1, and for each of those, takes another top 4 from hop2. They are not necessarily the same as the top 16 passages sequences.). As extractive models encode each passage sequence separately, we can use arbitrary number of input sequences. However, the performance mostly plateaus as we use over 200 input sequences.}\nFirst, we see that the BERT-wwm reader is 1-2\\% worse than the ELECTRA reader when using enough passages. However, it still outperforms the results in \\citep{GraphRecurrentRetriever} which also uses BERT-wwm for answer prediction.\nWhile RAG and FiD have shown strong improvements over extractive models on single-hop datasets such as NaturalQuestions~\\citep{NQ}, they do not show an advantage in the multi-hop case. Despite having twice as many parameters as ELECTRA, FiD fails to outperform it using the same amount of context (top 50). \nIn contrast, on NaturalQuestions, FiD is 4 points better than a similar extractive reader when using the top 100 passages in both.\\footnote{We implemented NQ extractive readers with both RoBERTa-large and ELECTRA-large, and RoBERTa-large yielded a better answer EM of 47.3, which is much lower than the 51.4 answer EM achieved by FiD.}\nWe hypothesize that the improved performance on single-hop questions is due to the ability of larger pretrained models to more effectively memorize single-hop knowledge about real-world entities.\\footnote{As shown by \\citet{t5close}, a large pretrained seq2seq model can be finetuned to directly decode answers with questions as the only inputs. However, we find that this retrieval-free approach performs poorly on multi-hop questions. See Appendix~\\ref{appendix:retrieval_free} for the exact numbers.}\nCompared to multi-hop questions that involve multiple relations and missing entities, simple questions usually only ask about a certain property of an entity. It is likely that such simple entity-centric information is explicitly mentioned by a single text piece in the pretraining corpus, while the evidence for multihop questions is typically dispersed, making the complete reasoning chain nontrivial to memorize. More analysis on RAG can be found in Appendix \\ref{appendix: reader}.\n\n    \n\n\\paragraph{Inference Efficiency}\n\\label{sec:qa_analysis}\n\n\\begin{wrapfigure}{r}{0.5\\textwidth}\n\\vspace{-10pt}\n\\includegraphics[width=\\linewidth]{figures/efficiency.pdf}\n\\vspace{-20pt}\n\\caption{Efficiency-performance trade-off comparison with published HotpotQA systems. The curve is plotted with different number of top $k$ ($k$=1,5,10,20,50,100,200) passage sequences we feed into the reader model. seq/Q denotes the time required for each query.}\n\\vspace{-5pt}\n\\label{fig:efficiency}\n\\end{wrapfigure}\n\nTo compare with existing multi-hop QA systems in terms of efficiency, we follow \\citet{DrKIT} and measure the inference time with 16 CPU cores and batch size 1. We implement our system with a fast approximate nearest neighbor search method, \\emph{i.e.}, HNSW~\\citep{HNSW}, which achieves nearly the same performance as exact search. With an in-memory index, we observe that the retrieval time is negligible compared to the forward pass of large pretrained models. Similarly, for systems that use term-based indices, the BERT calls for passage reranking cause the main efficiency bottleneck. Thus, for systems that do not release the end-to-end code, we estimate the running time based on the number of BERT cross-attention forward passes (the same estimation strategy used by \\citet{DrKIT}), and ignore the overhead caused by additional processing such as TF-IDF or linking graph construction. As shown in Figure~\\ref{fig:efficiency}, our method is about 10 times faster than current state-of-the-art systems while achieving a similar level of performance. Compared to two efficient systems (DrKIT and GoldEn), we achieve over 10 points improvement while only using the top-1 retrieval result for answer and supporting sentence prediction.\n\n\\subsection{Experiments: Multi-Evidence FEVER}\n\n\\begin{table}[t]\n    \\centering\n    \\small\n    \\vspace{-0.2in}\n    \\caption{Multi-Evidence FEVER Fact Verification Results. \\textbf{Loose-Multi} represents the subset that requires multiple evidence \\textit{sentences}. \\textbf{Strict-Multi} is a subset of \\textbf{Loose-Multi} that require multiple evidence sentences from different \\textit{documents}.}\n    \\begin{tabular}{l|cccc}\n        \\toprule\n      \\multirow{2}{*}{Method} &  \\multicolumn{2}{c}{Loose-Multi (1,960)} &  \\multicolumn{2}{c}{Strict-Multi (1,059)} \\\\\n      & LA & FEVER & LA & FEVER \\\\\n        \\midrule\n       GEAR & 66.4 & 38.0 & - & - \\\\\n       GAT & 66.1 & 38.2 & - & -\\\\\n       KGAT with ESIM rerank & 65.9 & 39.2 & 51.5 & 7.7\\\\\n       KGAT with BERT rerank & 65.9 & 40.1 & 51.0 & 6.2 \\\\\n       \\midrule\n       Ours + KGAT with BERT rerank  & \\textbf{77.9} & \\textbf{42.0} & \\textbf{72.1} & \\textbf{16.2}\\\\\n        \\bottomrule\n    \\end{tabular}\n    \\label{tab:fever_final}\n    \\vspace{-0.15in}\n\\end{table}\nFor FEVER claim verification, we reuse the best open-sourced verification system, \\emph{i.e.}, KGAT~\\citep{KGAT}, to show the benefit of our retrieval approach over existing retrieval methods. \nWe report the results in verification \\emph{label accuracy} (LA) and the \\emph{FEVER score}\\footnote{FEVER scores takes into account both support sentence accuracy and label accuracy, similar as the joint metrics in HotpotQA.} in Table~\\ref{tab:fever_final}, where the numbers of competitive baselines, GEAR~\\citep{GEAR}, graph attention network (GAT)~\\citep{GAT} and variants of KGAT\nare from the KGAT~\\citep{KGAT} paper.\nAll these baselines use entity linking for document retrieval, then rerank the sentences of the retrieved documents, and finally use different graph attention mechanisms over the fully-connected sentence graph to predict verification labels.\nSince some instances in the multi-evidence subset used by previous studies only needs multiple evidence \\textit{sentences} from the same document, we additionally test on a strict multi-hop subset with instances that need multiple \\textit{documents}.\nAs shown by the results, even without finetuning the downstream modules, simply replacing the retrieval component with \\method leads to significant improvements, especially on the strict multi-evidence subset.\n\n\\section{Related Work}\n\\label{sec:related_work}\n\n\\paragraph{Open-domain QA with Dense Retrieval}\n\nIn contrast to sparse term-index IR methods that are widely used by existing open-domain QA systems~\\citep{drqa,wang2017r,yang2019end}, recent systems ~\\citep{ORQA,REALM,DPR} typically uses dense passage retrieval techniques that better capture the semantic matching beyond simple n-gram overlaps. To generate powerful dense question and passage representations, these methods either conduct large-scale pretraining with self-supervised tasks that are close to the underlying question-passage matching in retrieval, or directly use the human-labeled question-passage pairs to finetune pretrained masked language models. On single-hop information-seeking QA datasets such as NaturalQuestions~\\citep{NQ} or WebQuestions~\\citep{WebQ}, these dense methods have achieved significant improvements over traditional IR methods. Prior to these methods based on pretrained models, \\citet{das2019multi} use RNN encoder to get dense representations of questions and passages. They also consider an iterative retrieval process and reformulate the query representation based on reader model's hidden states. However, their method requires an initial round of TF-IDF/BM25 retrieval and a sophisticated RL-based training paradigm to work well. Finally, like the aforementioned methods, only single-hop datasets are considered in their experiments. More akin to our approach, \\citet{feldman-el-yaniv-2019-multi} use a similar recursive dense retrieval formulation for multi-hop QA. In contrast to their biattenional reformulation component, which is applied on top of the token-level representations of the query and passages, we adopt a more straightforward query reformulation strategy, by simply concatenating the original query and previous retrieval as the inputs to the query encoder. Together with stronger pretrained encoders and more effective training methods (in-batch + memory bank negative sampling  vs their binary ranking loss), \\method is able to double the accuracy of their system.  \n\n\\paragraph{Query Expansion Techniques in IR} As our dense encoder augments the original question with the initial retrieved results to form the updated query representation, our work is also relevant to query expansion techniques~\\citep{rocchio1971relevance,voorhees1994query,ruthven2003survey} that are widely used in traditional IR systems. In particular, our system is similar in spirit to pseudo-relevance feedback techniques~\\citep{croft1979using,cao2008selecting,lv2010positional}, where no additional user interaction is required at the query reformulation stage. Existing studies mainly focus on alleviating the uncertainty of the user query~\\citep{collins2007estimation} by adding relevant terms from the first round of retrieval, where the retrieval target remains the same throughout the iterative process. In contrast, the query reformulation in our approach aims to follow the multi-hop reasoning chain and effectively retrieves different targets at each step. Furthermore, instead of explicitly selecting terms to expand the query, we simply concatenate the whole passage and rely on the pretrained encoder to choose useful information from the last retrieved passage.\n\n\\paragraph{Other Multi-hop QA Work} Apart from HotpotQA, other multi-hop QA datasets~\\citep{Wikihop,ComplexWebQ,MetaQA} are mostly built from knowledge bases (KBs). Compared to questions in HotpotQA, questions in these datasets are rather synthetic and less diverse. As multi-hop relations in KBs could be mentioned together in a single text piece, these datasets are not designed for an open-domain setting which necessitates multi-hop retrieval. Existing methods on these datasets either retrieve passages from a small passage pool pruned based on the the specific dataset~\\citep{sun2019pullnet,DrKIT}, or focus on a non-retrieval setting where a compact documents set is already given~\\citep{de2018question,zhong2019coarse,tu2019multi,beltagy2020longformer}. Compared to these research, our work aims at building an efficient multi-hop retrieval model that easily scales to large real-world corpora that include millions of open-domain documents.\n\n\\section{Conclusion}\nIn this work, we generalized the recently proposed successful dense retrieval methods by extending them to the multi-hop setting.  This allowed us to handle complex multi-hop queries with much better accuracy and efficiency than the previous best methods.  We demonstrated the versatility of our approach by applying it to two different tasks, using a variety of downstream modules.  In addition, the simplicity of the framework and the fact that it does not depend on a corpus-dependent graph structure opens the possibility of applying such multi-hop retrieval methods more easily and broadly cross different domains and settings.\n\n\\clearpage\n\n\\clearpage\n\n\\appendix\n\\section{Qualitative Analysis}\n\n\\subsection{False Bridge Question Error Cases}\n\\label{sec:false_errors}\nAs mentioned in \\cref{sec:error_analysis}, half of the errors of bridge questions are not real errors. In Table~\\ref{tab:bridge_errors}, we can see that the model predicts alternative passage sequences that could also be used to answer the questions.\n\n\\begin{table*}[h]\n    \\centering\n    \\caption{Error cases where our model predicts a passage sequence that is also correct. Important clues are marked in \\textcolor{blue}{blue}.}\n    \\begin{tabular}{p{0.95\\columnwidth}}\n    \\toprule\n    \\textbf{Q:} What languages did the son of Sacagawea speak? \\\\\n    \\textbf{Ground-truth SP Passage Titles:}  \\underline{Charbonneau, Oregon}; \\underline{Jean Baptiste Charbonneau}\\\\\n    \\textbf{Predicted:} \\\\\n    1. \\underline{Museum of Human Beings}: Museum of Human Beings, included in the National American Indian Heritage Month Booklist, November 2012 and 2013 is a novel written by Colin Sargent, which delves into the heart-rending life of \\textcolor{blue}{Jean-Baptiste Charbonneau, the son of Sacagawea}. Sacagawea was the Native American guide, who at 16 led the Lewis and Clark expedition. \\\\\n    2. \\underline{Jean Baptiste Charbonneau}: Jean Baptiste Charbonneau (February 11, 1805 – May 16, 1866) was an American Indian explorer, guide, fur trapper trader, military scout during the Mexican-American War, \"alcalde\" (mayor) of Mission San Luis Rey de Francia and a gold prospector and hotel operator in Northern California. He spoke French and English, and learned German and Spanish during his six years in Europe from 1823 to 1829. He spoke Shoshone, his mother tongue, and other western American Indian languages...\\\\\n    \\midrule\n    \\textbf{Q:} Altnahinch is located in a county that has a population density of how many per square mile? \\\\\n    \\textbf{Ground-truth SP Passage Titles:} \\underline{Altnahinch Dam}; \\underline{County Antrim}\\\\\n    \\textbf{Predicted:}\\\\\n    1. \\underline{Altnahinch}: Altnahinch is a townland \\textcolor{blue}{in County Antrim}, Northern Ireland.\\\\\n    2. \\underline{County Antrim}: County Antrim (named after the town of Antrim, from Irish: \"Aontroim\" , meaning \"lone ridge\" , )) is one of six counties that form Northern Ireland. Adjoined to the north-east shore of Lough Neagh, the county covers an area of 3046 km2 and has a population of about 618,000. County Antrim has a population density of 203 people per square kilometer / 526 people per square mile...\\\\\n    \\midrule\n    \\textbf{Q:} What foundation do scholars give for the likelihood of collaboration on a William Shakespeare Play written between 1588 and 1593? \\\\\n    \\textbf{Ground-truth SP Passage Titles:}\\\\ \\underline{Authorship of Titus Andronicus}, \\underline{William Shakespeare's collaborations}\\\\\n    \\textbf{Predicted:}\\\\\n    1. \\underline{Titus Andronicus}: Titus Andronicus is a tragedy by William Shakespeare, \\textcolor{blue}{believed to have been written between 1588 and 1593}, probably in collaboration with George Peele. It is thought to be Shakespeare's first tragedy, and is often seen as his attempt to emulate the violent and bloody revenge plays of his contemporaries, which were extremely popular with audiences throughout the 16th century.\\\\\n    2. \\underline{William Shakespeare's collaborations}:  Like most playwrights of his period, William Shakespeare did not always write alone... Some of the following attributions, such as \"The Two Noble Kinsmen\", have well-attested contemporary documentation; others, such as \"Titus Andronicus\", are dependent on linguistic analysis by modern scholars...\\\\\n    \\midrule \n    \\textbf{Q:} Zach Parise's father played in which league? \\\\\n    \\textbf{Ground-truth SP Passage Titles:} \\underline{Jordan Parise}; \\underline{Zach Parise} \\\\\n    \\textbf{Predicted:}\\\\\n    1. \\underline{Zach Parise}: Zachary Justin Parise (born July 28, 1984) is an American professional ice hockey left winger who is currently serving as an alternate captain for the Minnesota Wild in the National Hockey League (NHL). He has also played for the New Jersey Devils, where he served as team captain and led the team to the 2012 Stanley Cup Finals. Parise's father, J. P. Parisé... \\\\\n    2.\\underline{ J. P. Parisé}: Jean-Paul Joseph-Louis Parisé (December 11, 1941 – January 7, 2015) was a Canadian professional ice hockey coach and player. \\textcolor{blue}{Parise played in the National Hockey League (NHL)}, most notably for the Minnesota North Stars and the New York Islanders. \\\\\n    \\bottomrule\n    \\end{tabular}\n    \n    \\label{tab:bridge_errors}\n\\end{table*}\n\n\\clearpage\n\n\\subsection{Examples from the question decomposition analysis}\n\\label{appendix:decomposed_exmaples}\n\\begin{table*}[h]\n    \\centering\n    \\caption{Sampled retrieval errors (marked in \\textcolor{red}{red}) \\textit{only} made by the decomposed system. These errors could be potentially avoided if the model has access to the full information in the original question or previous hop results. The important clue for correctly retrieving the documents or avoiding errors is marked in \\textcolor{blue}{blue}. Once decomposed, the marked information are not longer available in one of the decomposed retrieval hop.}\n    \\begin{tabular}{p{0.95\\columnwidth}}\n    \\toprule\n     \\textbf{Multi-hop Question}:  What is the birthday of the \\textcolor{blue}{author} of \"She Walks These Hills\"?\\\\\n     \\textbf{Decomposed Questions}:\\\\\n     1. Who is the author of She Walks These Hills?\\\\\n     2. What is the birthday of Sharyn McCrumb?\\\\\n     \\textbf{Ground-truth SP Passages}: \\\\\n     \\underline{She Walks These Hills}: She Walks These Hills is a book written by Sharyn McCrumb and published by Charles Scribner's Sons in 1994, which later went on to win the Anthony Award for Best Novel in 1995.\\\\\n     \\underline{Sharyn McCrumb}: Sharyn McCrumb (born February 26, 1948) is an American writer whose books celebrate the history and folklore of Appalachia. McCrumb is the winner of numerous literary awards... \\\\\n     \\textbf{Decomposed Error Case:} \\\\\n     1. \\underline{She Walks These Hills} ($\\checkmark$)\\\\\n     2. \\textcolor{red}{\\underline{Tané McClure}}: Tané M. McClure (born June 8, 1958) is an American \\textcolor{blue}{singer and actress}. \\\\\n     \\midrule\n    \\textbf{Multi-hop Question:} When was the album with the song Unbelievable by \\textcolor{blue}{American rapper The Notorious B.I.G} released? \\\\\n    \\textbf{Decomposed Questions:} \\\\\n    1. What is the album with the song Unbelievable by American rapper The Notorious B.I.G?\\\\\n    2. When was the album Ready to Die released? \\\\\n    \\textbf{Ground-truth SP Passages:} \\\\\n    \\underline{Unbelievable (The Notorious B.I.G. song)}: Unbelievable is a song by American rapper The Notorious B.I.G., recorded for his debut studio album Ready to Die... \\\\\n    \\underline{Ready to Die}: Ready to Die is the debut studio album by American rapper The Notorious B.I.G.; it was released on September 13, 1994, by Bad Boy Records and Arista Records... \\\\\n    \\textbf{Decomposed Error Case:} \\\\\n    1. \\underline{Unbelievable (The Notorious B.I.G. song)} ($\t\\checkmark$)\\\\\n    2.\\textcolor{red}{\\underline{ Ready to Die (The Stooges album)}}: Ready to Die is the fifth and final studio album by \\textcolor{blue}{American rock band Iggy and the Stooges}. The album was released on April 30, 2013...\\\\\n    \\midrule\n    \\textbf{Multi-hop Question:} Whose death \\textcolor{blue}{dramatized} in a stage play helped end the death penalty in Australia? \\\\\n    \\textbf{Decomposed Questions:} \\\\\n    1. What is the stage play that helped end the death penalty in Australia?\\\\\n    2. Whose death was dramatized in Remember Ronald Ryan?\\\\\n    \\textbf{Ground-truth SP Passages}:\\\\\n    \\underline{Barry Dickins}: Barry Dickins (born 1949) is a prolific Australian playwright, author, artist, actor, educator and journalist... His most well-known work is the award winning stage play \"Remember Ronald Ryan\", a \\textcolor{blue}{dramatization} of the life and subsequent death of Ronald Ryan, the last man executed in Australia...\\\\\n    \\underline{Ronald Ryan}: Ronald Joseph Ryan (21 February 1925 – 3 February 1967) was the last person to be legally executed in Australia. Ryan was found guilty of shooting and killing warder George Hodson during an escape from Pentridge Prison, Victoria, in 1965...\\\\\n    \\textbf{Decomposed Error Case:}\\\\\n    1. \\textcolor{red}{\\underline{Capital punishment in Australia}}: Capital punishment in Australia has been abolished in all jurisdictions. Queensland abolished the death penalty in 1922. Tasmania did the same in 1968, the federal government abolished the death penalty in 1973, with application also in the Australian Capital Territory and the Northern Territory...\\\\\n    2.\\underline{Ronald Ryan}($\\checkmark$)\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\label{tab:decomposed_errors}\n\\end{table*}\n\n\\subsection{Extractive \\& Generative Reader Model}\n\\begin{wraptable}{r}{6cm}\n    \\centering\n    \\small\n    \\vspace{-0.3in}\n    \\caption{Answer EM using top 50 retrieved passage chains}\n    \\begin{tabular}{l|ccc}\n    \\toprule\n    Model   & Overall & \\begin{tabular}[c]{@{}l@{}}Comp\\\\ (20\\%)\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}Bridge\\\\ (80\\%)\\end{tabular} \\\\\\midrule\n    ELECTRA & 61.7    & 79.0        & 57.4          \\\\\n    FiD     & 61.7    & 75.3        & 58.3  \\\\ \\bottomrule\n    \\end{tabular}\n    \\vspace{-0.2in}\n    \\label{tab:fid_vs_electra}\n\n\\end{wraptable}\n\\label{appendix: reader}\nTable~\\ref{tab:reader} demonstrates the answer prediction performance for four different reader models. The extractive models predict answers given the top 250 retrieved passage sequences (pairs of passage from hop1 and hop2). Since generative models are generally heavier on the computation side, we can only use fewer passages. Besides the observations alredy discussed in \\cref{sec:reader_comparison}, we hypothesize the worse performance of multi-hop RAG compared to FiD is partially due to the smaller pretrained model used in RAG, i.e., BART is only half the size of T5-large. Also, as RAG back-propagate the gradients to the query encoder, it needs more memory footprint and can only take in fewer retrieved contexts. Our RAG implementation largely follows the implementation of the original paper and we did not use the PyTorch checkpoint (as used by FiD) to trade computation for memory. We conjecture the multi-hop RAG performance will also improve if we augment the current implementation with memory-saving tricks. However, given the same amount of context and read model size, the multi-hop RAG is still worse than the extractive ELECTRA reader, i.e., with only the top 1 retrieved passage sequence, our ELECTRA reader gets 53.8 EM compared to the 51.2 answer EM achieved by multi-hop RAG when using more context.\n\n \nGiven the same number of retrieved passage sequences (top 50) as shown in table \\ref{tab:fid_vs_electra}, FiD obtains similar performance to ELECTRA, despite that the generative model can generate arbitrary answers for the given input. (We tried constrained decoding for the generative model. However, no significant performance improvements were observed, indicating that the errors from the generative model are not due to the free-form generation task.) Further question type analysis in HotpotQA showed that the main difference comes from the comparison type of question, while for bridge question, FiD performs slightly better than ELECTRA. This finding might indicate that for generation models, numerical comparison is still a bigger issue compared to extractive models.\n\n\\section{Model Details}\n\n\\subsection{Best Model Hyperparameters}\n\\label{appendix:hyperparameter}\n\\begin{table}[h]\n    \\centering\n    \\caption{Hyperparameters of Retriever}\n    \\begin{tabular}{lc}\n    \\toprule\n        learning rate & 2e-5\\\\\n        batch size & 150 \\\\\n        maximum passage length & 300 \\\\\n        maximum query length at initial hop & 70 \\\\\n        maximum query length at 2nd hop & 350 \\\\\n        warmup ratio & 0.1 \\\\\n        gradient clipping norm & 2.0 \\\\\n        traininig epoch & 50 \\\\\n        weight decay & 0 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\label{tab:my_label}\n\\end{table}\n\n\\begin{table}[h]\n    \\centering\n    \\caption{Hyperparameters of Extractive Reader (ELECTRA)}\n    \\begin{tabular}{lc}\n    \\toprule\n        learning rate & 5e-5\\\\\n        batch size & 128 \\\\\n        maximum sequence length & 512 \\\\\n        maximum answer length & 30 \\\\\n        warmup ratio & 0.1 \\\\\n        gradient clipping norm & 2.0 \\\\\n        traininig epoch & 7 \\\\\n        weight decay & 0 \\\\\n        \\# of negative context per question & 5 \\\\\n        weight of SP sentence prediction loss & 0.025 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\label{tab:my_label}\n\\end{table}\n\n\\subsection{Further Details about Reader Models}\n\\label{appendix:multihop_rag}\n\n\\subsubsection{Extractive Reader}\nThe extractive reader is trained with four loss functions. With the \\texttt{[CLS]} token, we predict a reranking score based on whether the passage sequence match the groundtruth supporting passages. On top of the representation of each token, we predict a answer start score and answer end score. Finally, we prepend each sentence with the \\texttt{[unused0]} special token and predict whether the sentence is one of the supporting sentences using the representations of the special token. At training time, we pair each question with 1 groundtruth passage sequence and 5 negative passage sequence which do not contain the answer. At inference time, we feed in the top 250 passage sequences from \\method. We rank the predicted answer for each sequence with a linear combination of the reranking score and the answer span score. The combination weight is selected based on the dev results.\n\n\\subsubsection{Fusion-in-Decoder}\nThe FiD model uses T5-large as the underlying seq2seq model. It is twice as large as the extractive models and has 770M parameters. We reuse the hyperparameters as described in \\citet{FiD}. The original FiD uses the top 100 passages for NaturalQuestions. In our case, we use the top 50 retrieved passage sequences and concatenate the passages in each sequence before feeding into T5. In order to fit this model into GPU, we make use of PyTorch checkpoint~\\footnote{https://pytorch.org/docs/stable/checkpoint.html} for training. \n\n\\subsubsection{Multi-Hop RAG}\n\nThe RAG model aims to generate answer $y$ given question $x$ and the retrieved documents $z$. Similarly, the goal of multi-hop RAG can be expressed as: generate answer $y$ given question $x$ and retrieved documents in hop one $z_1$ and hop two $z_2$ (Limiting to two hops for HotpotQA). The model has three components: \n\\begin{itemize}\n    \\item Hop-one retriever $p_{\\eta_1}(z_1|x)$ with parameter $\\eta_1$ to represent the retrieved top-k passage distribution (top-k truncated distribution) given the input question $x$. \n    \\item Hop-two retriever $p_{\\eta_2}(z_2|x, z_1)$ with parameter $\\eta_2$ to represent the hop-two retrieved top-k passage distribution given not only the question $x$ but also the retrieved document $z_1$ from hop-one. \n    \\item A generator $p_\\theta(y_i|x, z_1, z_2, , y_{1:i-1})$ to represent the next token distribution given input question $x$, hop-one retrieved document $z_1$, hop-two retrieved document $z_2$ and previous predicted token $y_{1:i-1}$ parametrized by $\\theta$\n\\end{itemize}\n \\paragraph{Multi-Hop RAG Sequence Model}\n As the RAG Sequence model, this model generates the answer sequence given the fixed set of documents from hop-one retriever and hop-two retriever. In order to the get the probability of the generated sequence, we marginalize through the two latent variables corresponding to the two retrieval hops:\n\\begin{align*}\n& p_{\\textit{sequence}}(y|x) = \\\\\n& \\sum_{z_1}  p_{\\eta_1}(z_1|x)\\sum_{z_2} p_{\\eta_2}(z_2|x, z_1) \\prod_{i}^{N} p_\\theta(y_i|x, z_1, z_2, y_{1:i-1}) \\\\\n& \\sum_{z_1} \\sum_{z_2}  p_{\\eta_1}(z_1|x) p_{\\eta_2}(z_2|x, z_1) \\prod_{i}^{N} p_\\theta(y_i|x, z_1, z_2, y_{1:i-1})\n\\end{align*}\nwhere $z_1$ and $z_2$ are top k document from the respective retrieval modules.\n\n \\paragraph{Multi-Hop RAG Token Model}\n Moreover, the model can make predictions based on different passage extracted at each token. \n\\begin{align*}\n& p_{\\textit{token}}(y|x) = \\\\\n& \\prod_{i}^{N}  \\sum_{z_1}  \\sum_{z_2} p_{\\eta_1}(z_1|x) p_{\\eta_2}(z_2|x, z_1) p_\\theta(y_i|x, z_1, z_2, y_{1:i-1})\n\\end{align*}\n\nThe predicted probability for each token is the following\n\\begin{align*}\n& p_{\\textit{token}}(y_{i}|(x, y_{j})) = \\\\\n& \\sum_{z_1}  \\sum_{z_2} p_{\\eta_1}(z_1|x) p_{\\eta_2}(z_2|x, z_1) p_\\theta(y_i|x, z_1, z_2, y_{1:i-1})\n\\end{align*}\n\n\\section{Retrieval-free Approaches}\n\\label{appendix:retrieval_free}\nInspired by a recent work~\\citep{t5close} that trains the T5 seq2seq model to directly decode answers from  questions (\\textit{retrieval-free}), we conduct similar experiments on HotpotQA using BART~\\citep{BART}. As shown in Figure~\\ref{fig:retrieval_free}, the performance gap between retrieval-based methods and retrieval-free methods on multi-hop QA is much larger than the gap in the case of simple single-hop questions. \n\n\\begin{figure*}[h]\n\\centering\n\\includegraphics[width=.7\\linewidth]{retrieval_free.pdf}\n\\vspace{-0.2in}\n\\caption{Performance gap between retrieval-free and retrieval-based methods on different QA datasets.}\n\\label{fig:retrieval_free}\n\\end{figure*}\n\n\\section{A Unified QA Retrieval System}\n\nIn practice, when a fixed text corpus is given for open-domain systems, we do not know beforehand whether the incoming questions require single or multiple text evidence. Thus, it is essential to build a unified system that adaptively retrieves for multiple hops. Due to the simplicity of the approach, our method can easily be extended in the unified setup. To the best of our knowledge, only \\citep{GraphRecurrentRetriever} test the same retrieval method on both single and multi-hop questions but with separate trained models. Here we take a further step and explore the possibility of using a single retrieval model for both types of questions.\n \nTo enable adaptive retrieval, we add a binary prediction head on top of the question encoder. Once the retriever finishes the 1-hop retrieval, it encodes concatenation of $q$ and $p_1$ and predicts whether to stop retrieval using the final hidden state of the first token. We construct this unified setting with NaturalQuestions-Open~\\citep{ORQA} (NQ) as single-hop and HotpotQA as multi-hop. As the two datasets use different corpora, we merge the two\\footnote{The Wikipedia corpus of NQ is taken from DPR~\\citep{DPR}.} for easy comparison. As baselines, we use the retrieval models trained only on the respective dataset. For HotpotQA, the baseline is the best multi-hop retrieval model discussed in the main text. For NQ, we follow the training method in DPR~\\citep{DPR}, but with a shared question and passage encoder, which achieves stronger results. As the NQ corpus includes multiple passages of the same document and the HotpotQA corpus only uses the introduction passage, we are not able to compute the strict title-based support passage recall for HotpotQA as in \\cref{sec:exp-hotpotqa}. Thus, we only evaluate answer recall. Results are in Table~\\ref{tab:unified}. In contrast to existing studies that train different models for each dataset, we show that a unified dense retrieval model can maintain competitive performance on both, despite the vastly different nature of both datasets. Note that the information-seeking questions in NQ is usually noisier and more ambiguous, while HotpotQA questions are more complicated and contains more lexical overlaps with the evidence passages. Specifically, for NQ, the unified retrieval model achieves very similar performance as the single-dataset DPR model, while the performance on HotpotQA decreases more. We conjecture that this is because the information-seeking questions in NQ cover more diverse patterns, and the added HotpotQA training questions do not cause a dramatic distribution shift from the NQ test data. We leave the development of a more general retrieval system that handles different styles of questions to future work.\n\n\\begin{table}[h]\n    \\centering\n    \\small\n    \\caption{Comparing the unified retrieval model with models specifically trained for each task. We test the retrieval performance with a single merged corpus. For easy comparison, all three models are based on BERT-base encoder which we find achieves stronger performance than RoBERTa-base on NQ. AR@K denotes answer recall at top-K retrieved passage sequences.}\n    \\begin{tabular}{c|cccc}\n    \\toprule\n    \\multirow{2}{*}{Model} &  \\multicolumn{2}{c}{NQ} & \\multicolumn{2}{c}{HotpotQA} \\\\\n              & AR@20 & AR@100 & AR@20 & AR@100\\\\\n        \\midrule\n        single-hop only & 80.7 & 87.3 & - & -\\\\\n        multi-hop only & - & - & 83.4 & 89.4  \\\\\n        unified & 79.5 & 86.1 & 78.1 & 83.0 \\\\\n        \\bottomrule\n    \\end{tabular}\n    \n    \\label{tab:unified}\n\\end{table}\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Answering Open-Domain Questions of Varying Reasoning Steps from Text}\n\n\\begin{document}\n\n\\maketitle\n\n\\newif\\ifaclfinal\n\\aclfinaltrue\n\\ifaclfinal\n\\renewcommand*{\\thefootnote}{\\fnsymbol{footnote}}\n\\setcounter{footnote}{1}\n\\footnotetext{These authors contributed equally.}\n\\renewcommand*{\\thefootnote}{\\arabic{footnote}}\n\\setcounter{footnote}{0}\n\\fi\n\n\\begin{abstract}\n\n    We develop a unified system to answer directly from text open-domain questions that may require a varying number of retrieval steps.\n    We employ a single multi-task transformer model to perform all the necessary subtasks---retrieving supporting facts, reranking them, and predicting the answer from all retrieved documents---in an iterative fashion.\n    We avoid crucial assumptions of previous work that do not transfer well to real-world settings, including exploiting knowledge of the fixed number of retrieval steps required to answer each question or using structured metadata like knowledge bases or web links that have limited availability.\n    Instead, we design a system that can answer open-domain questions  on any text collection without prior knowledge of reasoning complexity.\n    To emulate this setting, we construct a new benchmark, called \\beerqa{}, by combining existing one- and two-step datasets with a new collection of 530 questions that require three Wikipedia pages to answer, unifying Wikipedia corpora versions in the process.\n    We show that our model demonstrates competitive performance on both existing benchmarks and this new benchmark.\n    We make the new benchmark available at \\url{https://beerqa.github.io/}.\n\n     \n \n\\end{abstract}\n\\section{Introduction}\n\nUsing knowledge to solve problems is a hallmark of intelligence.\nSince human knowledge is often containned in large text collections, open-domain question answering (QA) is an important means for intelligent systems to make use of the knowledge in large text collections.\nWith the help of large-scale datasets based on Wikipedia \\citep{rajpurkar2016squad, rajpurkar2018know} and other large corpora \\citep{trischler2016newsqa, dunn2017searchqa, talmor2018web}, the research community has made substantial progress on tackling this problem in recent years, including in the direction of complex reasoning over multiple pieces of evidence, or \\emph{multi-hop} reasoning \\citep{yang2018hotpotqa, welbl2018constructing, chen-etal-2020-hybridqa}. \n\nDespite this success, most previous systems are developed with, and evaluated on, datasets that contain exclusively \\emph{single-hop} questions (ones that require a single document or paragraph to answer) or \\emph{two-hop} ones.\nAs a result, their design is often tailored exclusively to single-hop \\citep[\\eg,][]{chen2017reading, wang2018evidence} or multi-hop questions \\citep[\\eg,][]{nie2019revealing, min-etal-2019-multi, feldman2019multi, zhao2020complex, xiong2021answering}. \nEven when the model is designed to work with both, it is often trained and evaluated on exclusively single-hop or multi-hop settings \\citep[\\eg,][]{asai2020learning}.\nIn practice, not only can we not expect open-domain QA systems to receive exclusively single- or multi-hop questions from users, but it is also non-trivial to judge reliably whether a question requires one or multiple pieces of evidence to answer \\emph{a priori}.\nFor instance, \\emph{``In which U.S. state was Facebook founded?''} appears to be single-hop, but its answer cannot be found in the main text of a single English Wikipedia page.\n\nBesides the impractical assumption about reasoning hops, previous work often also assumes access to non-textual metadata such as knowledge bases, entity linking, and Wikipedia hyperlinks when retrieving supporting facts, especially in answering complex questions \\citep{nie2019revealing,\nfeldman2019multi, zhao2019transformer, asai2020learning, dhingra2020differentiable, zhao2020complex}.\nWhile this information is helpful, it is not always available in text collections we might be interested in getting answers from, such as news or academic research articles, besides being labor-intensive and time-consuming to collect and maintain.\nIt is therefore desirable to design a system that is capable of extracting knowledge from text without using such metadata, to maximally emphasize using knowledge available to us in the form of text.\n\n\\begin{figure*}\n\\centering\n\\includegraphics[width=0.98\\textwidth]{image/pipeline} \n\\caption{The \\irrr{} question answering pipeline answers a complex question in the \\hotpotqa{} dataset by iteratively retrieving, reading, and reranking paragraphs from Wikipedia.\nIn this example, the question is answered in five steps: 1. the retriever model selects the words ``Ingerophrynus gollum'' from the question as an initial search query; 2. the question answering model attempts to answer the question by combining the question with each of the retrieved paragraphs and fails to find an answer; 3. the reranker picks the paragraph about the \\emph{Ingerophrynus gollum} toad to extend the reasoning path; 4. the retriever generates an updated query ``Lord of the Rings'' to retrieve new paragraphs; 5. the reader correctly predicts the answer ``150 million copies'' by combining the reasoning path (question + ``Ingerophrynus gollum'') with the newly retrieved paragraph about ``The Lord of the Rings''.\n}\\label{fig:pipeline}\n\\end{figure*}\n\nTo address these limitations, we propose Iterative Retriever, Reader, and Reranker (\\irrr{}), which features a single neural network model that performs all of the subtasks required to answer questions from a large collection of text (see Figure \\ref{fig:pipeline}).\n\\irrr{} is designed to leverage off-the-shelf information retrieval systems by generating natural language search queries, which allows it to easily adapt to arbitrary collections of text without requiring well-tuned neural retrieval systems or extra metadata.\nThis further allows users to understand and control \\irrr{}, if necessary, to facilitate trust.\nMoreover, \\irrr{} iteratively retrieves more context to answer the question, which allows it to easily accommodate questions of different number of reasoning steps.\n\nTo evaluate the performance of open-domain QA systems in a more realistic setting, we construct a new benchmark called \\beerqa{}\\footnote{\\url{https://beerqa.github.io/}} by combining the questions from the single-hop \\squadopen{} \\citep{rajpurkar2016squad, chen2017reading} and the two-hop \\hotpotqa{} \\citep{yang2018hotpotqa} with a new collection of 530 human-annotated questions that require information from at least three Wikipedia pages to answer.\nWe map all questions to a unified version of the English Wikipedia to reduce stylistic differences that might provide statistical shortcuts to models.\nAs a result, \\beerqa{} provides a more realistic evaluation of open-ended question answering systems in their ability to answer questions without knowledge of the number of reasoning steps required ahead of time.\nWe show that \\irrr{} not only achieves competitive performance with state-of-the-art models on the original \\squadopen{} and \\hotpotqa{} datasets, but also establishes a strong baseline for this new dataset.\n\nTo recap, our contributions in this paper are: (1)\na new open-domain QA benchmark, \\beerqa, that features questions requiring variable steps of reasoning to answer on a unified Wikipedia corpus.\n(2) A single unified neural network model that performs all essential subtasks in open-domain QA purely from text (retrieval, reranking, and reading comprehension), which not only achieves strong results on \\squad{} and \\hotpotqa{}, but also establishes a strong baseline on this new benchmark.%\n\\footnote{Our code for the model can be found at: \\url{https://github.com/beerqa/IRRR}.}\n\n\\section{Open-Domain Question Answering}\n\nThe task of open-domain question answering is concerned with finding the answer $a$ to a question $q$ from a large text collection $\\mathcal D$.\nSuccessful solutions to this task usually involve two crucial components: an \\emph{information retrieval} system that finds a small set of relevant documents $\\mathcal D_r$ from $\\mathcal D$, and a \\emph{reading comprehension} system that extracts the answer from it.%\n\\footnote{Some recent work breaks away from this mold, and use large pretrained language models \\citep[\\eg, T5; ][]{raffel2020t5} to directly generate answers from knowledge stored in model parameters.}\n\\citet{chen2017reading} presented the first neural-network-based approach to this problem, which was later extended by \\citet{wang2018reinforced} with a \\emph{reranking} system to further reduce the amount of context the reading comprehension component has to consider to improve answer accuracy.\n\nMore recently, \\citet{yang2018hotpotqa} showed that this single-step retrieve-and-read approach to open-domain question answering is inadequate for more complex questions that require multiple pieces of evidence to answer (\\eg, \\emph{``What is the population of Mark Twain's hometown?''}).\nWhile later work approaches these by extending supporting fact retrieval beyond one step, most assumes that all questions are either exclusively single-hop or multi-hop during training and evaluation.\nWe propose \\irrr{}, a system that performs variable-hop retrieval for open-domain QA to address these issues, and present a new benchmark, \\beerqa{}, to evaluate systems in a more realistic setting.\n\n\\section{\\irrr{}: Iterative Retriever, Reader, and Reranker}\n\nIn this section, we present a unified model to perform all of the subtasks necessary for open-domain question answering---Iterative Retriever, Reader, and Reranker (\\irrr{}), which performs the subtasks involved in an iterative manner to accommodate questions with a varying number of steps.\n\\irrr{} aims at building a reasoning path $p$ from the question $q$, through all the necessary supporting documents or paragraphs $d\\in \\mathcal{D}_{\\text{gold}}$ to the answer $a$ (where $\\mathcal{D}_{\\text{gold}}$ is the set of gold supporting facts).%\n\\footnote{For simplicity, we assume that there is a single set of relevant supporting facts that helps answer each question.}\nAs shown in \\figref{fig:pipeline}, \\irrr{} operates in a loop of retrieval, reading, and reranking to expand the reasoning path $p$ with new documents from $d\\in \\mathcal{D}$.\n\nSpecifically, given a question $q$, we initialize the reasoning path with the question itself, \\ie, $p_0 = [q]$, and generate from it a search query with \\irrr{}'s retriever.\nOnce a set of relevant documents $\\mathcal{D}_1 \\subset \\mathcal{D}$ is retrieved, they might either help answer the question, or reveal clues about the next piece of evidence we need to answer $q$.\nThe reader model then attempts to read each of the documents in $\\mathcal{D}_1$ to answer the question combined with the current reasoning path $p$.\nIf more than one answer can be found from these candidate reasoning paths, we predict the answer with the highest \\emph{answerability} score, which we will detail in \\secref{sec:reader}.\nIf no answer can be found, then \\irrr{}'s reranker scores each retrieved paragraph against the current reasoning path, and appends the top-ranked paragraph to the current reasoning path, \\ie, $p_{i+1} = p_i + [\\arg\\max_{d \\in D_1} \\mathrm{reranker}(p_i, d)]$, before the updated reasoning path is presented to the retriever to generate new search queries.\nThis iterative process is repeated until an answer is predicted from one of the reasoning paths, or until the reasoning path has reached a cap of $K$ documents.%\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=.48\\textwidth]{image/irrr_arch} \n\\caption{The overall architecture of our IRRR model, which uses a shared Transformer encoder to perform all subtasks of open-domain question answering. }\\label{fig:irrr_arch}\n\\end{figure}\n\nTo reduce computational cost and improve model representations of reasoning paths from shared statistical learning, %\n\\irrr{} is implemented as a multi-task model built on a pretrained Transformer model that performs all three subtasks.\nAt a high level, it consists of a Transformer encoder \\citep{vaswani2017attention} which takes the reasoning path $p$ (the question and all retrieved paragraphs so far) as input, and one set of task-specific parameters for each task of retrieval, reranking, and reading comprehension (see \\figref{fig:irrr_arch}).\nThe retriever generates natural language search queries by selecting words from the reasoning path, the reader extracts answers from the reasoning path and abstains if its confidence is not high enough, and the reranker assigns a scalar score for each retrieved paragraph as a potential continuation of the current reasoning path.\n\nThe input to our Transformer encoder is formatted similarly to that of the BERT model \\citep{devlin2019bert}. \nFor a reasoning path $p$ that consists of the question and $t$ retrieved paragraphs, the input is formatted as ``\\clstoken{} question \\septoken{} title\\textsubscript{1} \\conttoken{} para\\textsubscript{1}%\n\\septoken{} \\ldots\\ title\\textsubscript{$t$} \\conttoken{} para\\textsubscript{$t$} \\septoken{}'', where \\clstoken{}, \\septoken{}, and \\conttoken{} are special tokens to separate different components of the input.\nThe \\conttoken{} embedding is randomly initialized with a truncated normal distribution with a standard deviation of 0.02, and finetuned with other model parameters during training.\n\nWe will detail each of the task-specific components in the following subsections.\n\n\\subsection{Retriever}\n\nThe goal of the retriever is to generate natural language queries to retrieve relevant documents from an off-the-shelf text-based retrieval engine.%\n\\footnote{We employ Elasticsearch \\citep{gormley2015elasticsearch} as our text-based search engine, and follow previous work to process Wikipedia and search results, which we detail in Appendix \\ref{sec:search_engine}.}\nThis allows \\irrr{} to perform open-domain QA in an explainable and controllable manner, where a user can easily understand the model's behavior and intervene if necessary.\nWe extract search queries from the current reasoning path, \\ie, the original question and all of the paragraphs that we have already retrieved, similar to \\golden{} Retriever's approach \\citep{qi2019answering}.\nThis is based on the observation that there is usually a strong semantic overlap between the reasoning path and the next paragraph to retrieve, which helps reduce the search space of potential queries.\nWe note, though, that \\irrr{} differs from \\golden{} Retriever in two important ways:\n(1) we allow search queries to be any subsequence of the reasoning path instead of limiting it to substrings to allow for more flexible combinations of search phrases; (2) more importantly, we employ the same retriever model across reasoning steps to generate queries instead of training separate ones for each reasoning step, which is crucial for \\irrr{} to generalize to arbitrary reasoning steps.\n\nTo predict these search queries from the reasoning path, we apply a token-wise binary classifier on top of the shared Transformer encoder model, to decide whether each token is included in the final query.\nAt training time, we derive supervision signal to train these classifiers with a binary cross entropy loss (which we detail in Section \\ref{sec:query_oracle}); at test time, we select a cutoff threshold for query words to be included from the reasoning path.\nIn practice, we find that boosting the model to predict more query terms is beneficial to increase the recall of the target paragraphs in retrieval.\n\n\\subsection{Reader} \\label{sec:reader}\n\nThe reader model attempts to find the answer given a reasoning path comprised of the question and retrieved paragraphs. \nTo support unanswerable questions and the special non-extractive answers  \\emph{yes} and \\emph{no} from \\hotpotqa{}, we train a classifier conditioned on the Transformer encoder representation of the \\clstoken{} token to predict one of the 4 classes \\texttt{SPAN}\\slash \\texttt{YES}\\slash\\texttt{NO}\\slash\\texttt{NOANSWER}. The classifier thus simultaneously assigns an \\emph{answerability} score to this reasoning path to assess the likelihood of the document having the answer to the original question on this reasoning path.\nSpan answers are predicted from the context using a span start classifier and a span end classifier, following \\citet{devlin2019bert}.\n\nWe define answerability as the log likelihood ratio between the most likely positive answer and the \\texttt{NOANSWER} prediction, and use it to pick the best answer from all the candidate reasoning paths to stop \\irrr{}'s iterative process, if found. \nWe find that this likelihood ratio formulation is less affected by sequence length compared to prediction probability, thus making it easier to assign a global threshold across reasoning paths of different lengths to stop further retrieval.\nWe include further details about answerability calculation in Appendix \\ref{sec:further_training_details}.\n\n\\subsection{Reranker}\n\nWhen the reader fails to find an answer from the reasoning path, the reranker selects one of the retrieved paragraphs to expand it, so that the retriever can generate new search queries to retrieve new context to answer the question.\nTo achieve this, we assign each potential extended reasoning path a score by linearly transforming the hidden representation of the \\clstoken{} token, and picking the extension that has the highest score.\nAt training time, we normalize the reranker scores across top retrieved paragraphs with softmax, and maximize the log likelihood of selecting gold supporting paragraphs from retrieved ones, which is a noise contrastive estimation \\citep[NCE;][]{mnih2013learning, jean2015using} of the reranker likelihood over all retrieved paragraphs.\n\n\\subsection{Training IRRR}\n\n\\subsubsection{Dynamic Oracle for Query Generation} \\label{sec:query_oracle}\n\nSince existing open-domain QA datasets do not include human-annotated search queries, we need to derive supervision signal to train the retriever with a dynamic oracle.\nSimilar to \\golden{} Retriever, %\nwe derive search queries from overlapping terms between the reasoning path and the target paragraph with the goal of maximizing retrieval performance.\n\nTo reduce computational cost, we limit our attention to overlapping spans of text between the reasoning path and the target document when generating oracle queries.\nFor instance, when ``David'' is part of the overlapping span ``David Dunn'', the entire span is either included or excluded from the oracle query to reduce the search space.\nOnce $N$ overlapping spans are found, we approximate the importance of each with the following ``importance'' metric to avoid enumerating all $2^N$ combinations to generate the oracle query\n\\begin{align}\n    \\mathrm{Imp}(s_i) &= \\mathrm{Rank}(t, \\{s_j\\}_{j=1, j\\ne i}^N) - \\mathrm{Rank}(t, \\{s_i\\}), \\nonumber\n\\end{align}\nwhere $s_j$ are overlapping spans, and $\\mathrm{Rank}(t, S)$ is the rank of target document $t$ in the search result when spans $S$ are used as search queries (the smaller, the closer $t$ is to the top).\nIntuitively, the second term captures the importance of the search term when used alone, and the first captures its importance when combined with all other overlapping spans, which helps us capture query terms that are only effective when combined.\nAfter estimating importance of each overlapping span, we determine the final oracle query by first sorting all spans by descending importance, then including each in the final oracle query until the search rank of $t$ stops improving. \nThe resulting time complexity for generating these oracle queries is thus $O(N)$, \\ie, linear in the number of overlapping spans between the reasoning path and the target paragraph.\n\nFigure \\ref{fig:oracle_recall} shows that the added flexibility of non-span queries in \\irrr{} significantly improves retrieval performance compared to that of \\golden{} Retriever, which is only able to extract contiguous spans from the reasoning path as queries.\n\n\\begin{figure}\n    \\centering\n    \\resizebox{\\linewidth}{!}{\n\\pgfplotstableread[row sep=\\\\,col sep=&]{\n\tk & recall \\\\\n\t1 & 85.36 \\\\\n\t2 & 90.52 \\\\\n\t5 & 94.53 \\\\\n\t10 & 95.87 \\\\\n\t20 & 97.12 \\\\\n\t50 & 98.15 \\\\\n}\\goldenpone\n\n\\pgfplotstableread[row sep=\\\\,col sep=&]{\n\tk & recall \\\\\n\t1 & 75.11 \\\\\n\t2 & 79.91 \\\\\n\t5 & 86.47 \\\\\n\t10 & 89.47 \\\\\n\t20 & 91.82 \\\\\n\t50 & 93.99 \\\\\n}\\goldenptwo\n\n\\pgfplotstableread[row sep=\\\\,col sep=&]{\n\tk & recall \\\\\n\t1 & 92.64 \\\\\n\t2 & 95.21 \\\\\n\t5 & 97.14 \\\\\n\t10 & 98.06 \\\\\n\t20 & 98.77 \\\\\n\t50 & 99.32 \\\\\n}\\irrrpone\n\n\\pgfplotstableread[row sep=\\\\,col sep=&]{\n\tk & recall \\\\\n\t1 & 84.94 \\\\\n\t2 & 90.94 \\\\\n\t5 & 94.23 \\\\\n\t10 & 95.87 \\\\\n\t20 & 96.96 \\\\\n\t50 & 97.76 \\\\\n}\\irrrptwo\n\n\\begin{tikzpicture}[every plot/.append style={thick},font=\\footnotesize]\n\\begin{axis}[\nxlabel={Number of Retrieved Documents},\nxtick={1, 2, 5, 10, 20, 50},\nxticklabels={1,2,5,10,20,50},\nxmode=log,\nylabel={Dev Recall (\\%)},\nwidth=7cm,\nheight=4cm,\nymin=70,\nymax=100,\nlegend style={at={(1.1,0.5)}, anchor=west},\nlegend columns=1,\nymajorgrids=true,\ngrid style=dashed,\n]\n\\addplot[dashed, color=red, mark=square, mark options={solid}] plot table[x=k, y=recall]{\\goldenpone};\n\\addplot[dashed, color=red, mark=o, mark options={solid}] plot table[x=k,y=recall]{\\goldenptwo};\n\\addplot[color=blue, mark=square, mark options={solid, fill=blue}] plot table[x=k, y=recall]{\\irrrpone};\n\\addplot[color=blue, mark=o, mark options={solid, fill=blue}] plot table[x=k,y=recall]{\\irrrptwo};\n\\legend{\\golden{} Doc 1, \\golden{} Doc 2, \\irrr{} Doc 1, \\irrr{} Doc 2}\n\n\\end{axis}\n\\end{tikzpicture}    }\n    \\caption{Recall of the two gold supporting documents by the oracle queries of \\golden{} Retriever and \\irrr{} on the \\hotpotqa{} dataset, where each question corresponds to two supporting documents.}\n    \\label{fig:oracle_recall}\n\\end{figure}\n\n\\subsubsection{Reducing Exposure Bias with Data Augmentation}\\label{sec:data_aug}\n\nWith the dynamic oracle, we are able to generate target queries to train the retriever model, retrieve documents to train the reranker model, and expand reasoning paths in the training set by always choosing a gold paragraph, following \\citet{qi2019answering}.\nHowever, this might prevent the model from generalizing to cases where model behavior deviates from the oracle.\nTo address this, we augment the training data by occasionally selecting non-gold paragraphs to expand reasoning paths, and use the dynamic oracle to generate queries for the model to ``recover'' from these synthesized retrieval mistakes.\nWe found that this data augmentation significantly improves the performance of \\irrr{} in preliminary experiments, and thus report main results with augmented training data.\n\\section{Experiments}\n\n\\paragraph{Standard Benchmarks.} We test \\irrr{} on two standard benchmarks, \\squadopen{} and \\hotpotqa{}.\n\\squadopen{} \\cite{chen2017reading} designates the development set of the original \\squad{} dataset as its test set, which features more than 10,000 questions, each based on a single paragraph in a Wikipedia article.\nFor this dataset, we follow previous work and use the 2016 English Wikipedia as the corpus for evaluation.\nSince the authors did not present a standard development set, we further split part of the training set to construct a development set roughly as large as the test set.\n\\hotpotqa{} \\citep{yang2018hotpotqa} features more than 100,000 questions that require the introductory paragraphs of two Wikipedia articles to answer, and we focus on its open-domain ``fullwiki'' setting in this work.\nFor \\hotpotqa{}, we use the introductory paragraphs provided by the authors for training and evaluation, which is based on a 2017 Wikipedia dump.\n\n\\begin{figure}\n    \\centering\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{p{28em}}\n    \\toprule\n    \\textbf{Question:} How many counties are on the island that is home to the fictional setting of the novel in which Daisy Buchanan is a supporting character?\\\\\n    \\midrule\n    \\textbf{Wikipedia Page 1: \\textit{Daisy Buchanan}}\\\\\n    Daisy Fay Buchanan is a fictional character in F. Scott Fitzgerald's magnum opus ``The Great Gatsby'' (1925)...\\\\\n    \\midrule\n    \\textbf{Wikipedia Page 2: \\textit{The Great Gatsby}}\\\\\n    The Great Gatsby is a 1925 novel ... that follows a cast of characters living in the fictional town of West Egg on prosperous Long Island ...\\\\\n    \\midrule\n    \\textbf{Wikipedia Page 3: \\textit{Long Island}}\\\\\n    The Long Island ... comprises four counties in the U.S. state of New York: Kings and Queens ... to the west; and Nassau and Suffolk to the east...\\\\\n    \\midrule\n    \\textbf{Answer:} four\\\\ \n    \\bottomrule\n    \\end{tabular}\n    }\n    \\caption{An example of the newly collected challenge questions. This particular question requires three pieces of evidence to answer.\n    }\n    \\label{fig:three_hop_example}\n\\end{figure}\n\n\\paragraph{New Benchmark.} To evaluate the performance of \\irrr{} as well as future QA systems in a more realistic open-domain setting without a pre-specified number of reasoning steps for each question, we further combine \\squadopen{} and \\hotpotqa{} with 530 newly collected challenge questions (see Figure \\ref{fig:three_hop_example} for an example, and Appendix \\ref{sec:three_hop} for more details) to construct a new benchmark.\nNote that naively combining the datasets by merging the questions and the underlying corpora is problematic, as the corpora not only feature repeated and sometimes contradicting information, but also make them available in two distinct forms (full Wikipedia pages in one and just the introductory paragraphs in the other).\nThis could result in models taking corpus style as a shortcut to determine question complexity, or even result in plausible false answers due to corpus inconsistency.\n\nTo construct a high-quality unified benchmark, we begin by mapping the paragraphs each question is based on to a more recent version of Wikipedia.\\footnote{In this work, we used the English Wikipedia dump from August 1st, 2020.}\nWe discarded examples where the Wikipedia pages have either been removed or significantly edited such that the answer can no longer be found from paragraphs that are similar enough to the original contexts the questions are based on.\\footnote{We refer the reader to Appendix \\ref{sec:data_processing} for further details about these Wikipedia corpora and how we process and map between them.}\nAs a result, we filtered out 22,328 examples from \\squad{} Open, and 18,649 examples from \\hotpotqa{}'s fullwiki setting.%\n\\ifaclfinal\\else\\footnote{We obtained access to the \\hotpotqa{} fullwiki test set through personal communications with the authors.}\\fi~ We add newly annotated challenge questions to the test set of the new benchmark, which require at least three steps of reasoning to answer.\nThis allows us to test the generalization capabilities of QA models to this unseen scenario.\nThe statistics of the final dataset, which we name \\beerqa{}, can be found in Table \\ref{tab:new_benchmark_stats}.\nFor all benchmark datasets, we report standard answer exact match (EM) and unigram \\fone{} metrics.\n\n\\begin{table}\n    \\centering\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{lcccc}\n        \\toprule\n         & \\squad{} Open & \\hotpotqa{} & 3+ Hop\n         & Total  \\\\\n        \\midrule\n        Train & 59,285 & 74,758 & \\phantom{00}0 & 134,043 \\\\\n        Dev & \\phantom{0}8,132 & \\phantom{0}5,989 & \\phantom{00}0 & \\phantom{0}14,121\\\\\n        Test & \\phantom{0}8,424 & \\phantom{0}5,978 & 530 & \\phantom{0}14,932 \\\\\n        \\midrule\n        Total & 75,841 & 86,725 & 530 & 163,096\\\\\n        \\bottomrule\n    \\end{tabular}\n    }\n    \\caption{Counts of QA examples in the new unified benchmark, \\beerqa.} \n    \\label{tab:new_benchmark_stats}\n\\end{table}\n\n\\paragraph{Training details.} We use ELECTRA\\textsubscript{LARGE} \\citep{clark2020electra} as the pre-trained initialization for our Transformer encoder. \nWe train the model on a combined dataset of \\squad{} Open and \\hotpotqa{} questions where we optimize the joint loss of the retriever, reader, and reranker components simultaneously in an multi-task learning fashion.\nTraining data for the retriever and reranker components is derived from the dynamic oracle on the training set of these datasets, where reasoning paths are expanded with oracle queries and by picking the gold paragraphs as they are retrieved for the reader component.\nWe augment the training data with the technique in Section \\ref{sec:data_aug} and expand reasoning paths up to 3 reasoning steps on \\hotpotqa{} and 2 on \\squadopen{}, and find that this results in a more robust model.\nAfter an initial model is finetuned on this expanded training set, we apply our iterative training technique to further reduce exposure bias of the model by generating more data with the trained model and the dynamic oracle.\n\n\\section{Results}\n\nIn this section, we present the performance of \\irrr{} when evaluated against previous systems on standard benchmarks, and demonstrate its efficacy on our new, unified benchmark, especially with the help of iterative training.\n\n\\subsection{Performance on Standard Benchmarks}\n\nWe first compare \\irrr{} against previous systems on \\squad{} Open and the fullwiki setting of \\hotpotqa{}. %\nOn each dataset, we compare the performance of \\irrr{} against best previously published systems, as well as unpublished ones on public leaderboards.\nFor a fairer comparison to previous work, we make use of their respective Wikipedia corpora, and limit the retriever to retrieve 150 paragraphs of text from Wikipedia at each step of reasoning.\nWe also compare \\irrr{} against the Graph Recurrent Retriever \\citep[GRR;][]{asai2020learning} on our newly collected 3-hop question challenge test set, using the author's released code and models trained on \\hotpotqa{}.\nIn these experiments, we report \\irrr{} performance both from training on the dataset it is evaluated on, and from combining the training data we derived from both \\squadopen{} and \\hotpotqa{}.\n\n\\begin{table}\n\\small\n\\centering\n    \\begin{tabular}{lcc}\n        \\toprule\n        \\multirow{2}{5em}{System} & \\multicolumn{2}{c}{\\squad{} Open} \\\\\n        \\cline{2-3}\n        & EM & \\fone{}\\\\ \n         \\midrule\n         DrQA \\citep{chen2017reading} & 27.1 & ---\\\\\n         DensePR \\citep{karpukhin2020dense}  & 38.1 & --- \\\\\n         BERTserini \\citep{yang2019end} & 38.6 & 46.1 \\\\\n         MUPPET \\citep{feldman2019multi} & 39.3 & 46.2 \\\\\n         RE\\textsuperscript{3} \\citep{hu2019retrieve} & 41.9 & 50.2 \\\\\n         Knowledge-aided \\citep{zhou2020knowledge} & 43.6 & 53.4 \\\\\n         Multi-passage BERT \\citep{wang2019multi}& 53.0 & 60.9 \\\\\n         GRR \\citep{asai2020learning}& 56.5 & 63.8\\ \\\\\n         FiD \\citep{izacard2020leveraging}& 56.7 & --- \\\\\n         SPARTA \\citep{zhao2020sparta} & 59.3 & 66.5\\\\\n         \\midrule\n         \\irrr{} (\\squad) & 56.8 & 63.2 \\\\\n         \\irrr{} (\\squad+\\hotpotqa) & \\textbf{61.8} & \\textbf{68.9} \\\\\n         \\bottomrule\n    \\end{tabular}\n\\caption{End-to-end question answering performance on \\squad{} Open, evaluated on the same set of documents as \\citet{chen2017reading}. }\n\\label{tab:benchmark_results_squad}\n\\end{table}\n\n\\begin{table}[t]\n\\centering\n\\resizebox{.48\\textwidth}{!}{%\n    \\begin{tabular}{lccp{0em}cc}\n        \\toprule\n        \\multirow{2}{5em}{System} & \\multicolumn{2}{c}{\\hotpotqa{}} && \\multicolumn{2}{c}{3+ hop} \\\\\n       \n        \\cline{2-3} \\cline{5-6}\n        & EM & \\fone{} & & EM & \\fone{}\\\\ \n         \\midrule\n         GRR  \\citep{asai2020learning} & 60.0 & 73.0 && 27.2$^{\\dagger}$ & 31.9$^{\\dagger}$ \\\\\n         Step-by-step$^\\otimes$ & 63.0 & 75.4 && --- & ---\\\\\n         DDRQA \\citep{zhang2020ddrqa} & 62.3 & 75.3 && --- & --- \\\\\n         MDR \\citep{xiong2021answering} & 62.3 & 75.3&& --- & ---\\\\\n         EBS-SH $^\\otimes$ & 65.5 & 78.6 && --- & ---\\\\\n         TPRR $^\\otimes$ & 67.0 & 79.5 && --- & --- \\\\\n         HopRetriever \\citep{li2020hopretriever} & \\textbf{67.1} & \\textbf{79.9} && --- & --- \\\\\n         \\midrule\n         \\irrr{} (\\hotpotqa) & 65.2 & 78.0 && 29.2\\phantom{$^{\\dagger}$} & 34.2\\phantom{$^{\\dagger}$} \\\\\n         \\irrr{} (\\squad{} + \\hotpotqa) & 65.7 & 78.2 && \\textbf{32.5}\\phantom{$^{\\dagger}$} & \\textbf{36.7}\\phantom{$^{\\dagger}$} \\\\\n         \\bottomrule\n    \\end{tabular}\n}\n\n\\caption{End-to-end question answering performance on \\hotpotqa{} and the new 3+ hop challenge questions, evaluated on the official \\hotpotqa{} Wikipedia paragraphs. $\\otimes$ denotes anonymous/preprint unavailable at the time of writing of this paper. $\\dagger$ indicates results we obtained using the publicly available code and pretrained models.}\n\\vspace{-.5em}\n\\label{tab:benchmark_results_hotpot_3hop}\n\\end{table}\n\nAs can be seen in Tables \\ref{tab:benchmark_results_squad} and \\ref{tab:benchmark_results_hotpot_3hop}, \\irrr{} achieves competitive performance with previous work, and further outperforms previously published work on \\squadopen{} by a large margin when trained on combined data.\nIt also outperforms systems that were submitted after \\irrr{} was initially submitted to the \\hotpotqa{} leaderboard.\nOn the 3+ hop challenge set, we similarly notice a large performance margin between \\irrr{} and GRR, although neither is trained with questions requiring three or more hops, demonstrating that \\irrr{} generalizes well to questions that require more retrieval steps than the ones seen during training.\nWe note that the systems that outperform \\irrr{} on these datasets typically make use of trainable neural retrieval components, which \\irrr{} can potentially benefit from adopting as well.\nSpecifically, SPARTA \\citep{zhao2020sparta} introduces a neural sparse retrieval system that potentially works well with \\irrr{}'s oracle query generation procedure to further improve retrieval performance, thanks to its use of natural language queries.\nHopRetriever \\citep{li2020hopretriever} introduces a novel representation of documents for retrieval that is particularly suitable for discovering documents connected by the same entity to answer multi-hop questions, which \\irrr{} could benefit from as well.\nWe leave exploration of these directions to future work.\n\n\\begin{figure}[t]\n\n\\centering\n\\footnotesize\n\\resizebox{.46\\textwidth}{!}{\n\\hskip-1em\n\\begin{tikzpicture}\n\\begin{axis}[\n    ybar,\n    height=3.5cm,\n    bar width=7pt,\n    title={\\squadopen{}},\n    xlabel={Retrieval Steps/Question},\n    ylabel={Percentage},\n    xtick={1,2,3,4,5},\n    xmin=.5,xmax=5.5,\n    legend pos=north east,\n    ymajorgrids=true,\n    grid style=dashed,\n]\n\n\\addplot+[color=blue,fill=blue,postaction={pattern=crosshatch dots,pattern color=blue!40!white}] coordinates {(1,70.338)(2,2.53)(3,12.899)(4,8.837)(5,5.3938)};\n\n\\addplot+[color=green,fill=green,postaction={pattern=horizontal lines,pattern color=green!40!white}] coordinates {(1,73.544)(2,2.968)(3,11.891)(4,7.011)(5,4.585)};\n\n\\addplot+[color=red,fill=red,postaction={pattern=vertical lines,pattern color=red!40!white}] coordinates {(1,71.46)(2,3.158)(3,12.224)(4,8.904)(5,4.804)};\n\n\\legend{\\phantom{0}50 docs/step,100 docs/step,150 docs/step}\n\n\\end{axis}\n\\end{tikzpicture}\n\\hskip 1pt\n\\begin{tikzpicture}\n\\begin{axis}[\n    ybar,\n    height=3.5cm,\n    bar width=7pt,\n    title={\\hotpotqa{}},\n    xlabel={Retrieval Steps/Question},\n    xtick={1,2,3,4,5},\n    xmin=.5,xmax=5.5,\n    legend pos=north east,\n    ymajorgrids=true,\n    grid style=dashed,\n]\n\n\\addplot+[color=blue,fill=blue,postaction={pattern=crosshatch dots,pattern color=blue!40!white}] coordinates {(1,0.351)(2,84.56)(3,12.586)(4,1.958)(5,0.540)};\n\n\\addplot+[color=green,fill=green,postaction={pattern=horizontal lines,pattern color=green!40!white}] coordinates {(1,0.27)(2,85.82)(3,12.07)(4,1.404)(5,0.432)};\n\n\\addplot+[color=red,fill=red,postaction={pattern=vertical lines,pattern color=red!40!white}] coordinates {(1,0.64)(2,86.75)(3,8.90)(4,2.876)(5,0.8237)};\n\n\\legend{\\phantom{0}50 docs/step,100 docs/step,150 docs/step}\n\n\\end{axis}\n\\end{tikzpicture}\n}\n\\\\\n\\resizebox{.48\\textwidth}{!}{\n\\begin{tikzpicture}\n\\begin{axis}[\n    xlabel={Total Paragraphs Retrieved/Question},\n    ylabel={Answer \\fone{}},\n    height=5cm,\n    xmin=-50, xmax=550,\n    ymin=57.4, ymax=62.6,\n    xtick={0,100,200,300,400,500},\n    ytick={58,59,60,61,62},\n    legend pos=south east,\n    ymajorgrids=true,\n    grid style=dashed,\n]\n\n\\addplot[color=blue,mark=square,]\ncoordinates  {(50,58.49)(80,58.80)(109,58.64)(132,58.18)(151,58.00)}\nnode[pos=0,left]{(1)} node[pos=1,right]{(5)};\n\\addplot[color=green,mark=star,]\ncoordinates {(100,60.81)(155,61.09)(208,60.72)(250,60.34)(286,60.20)} \nnode[pos=0,left]{(1)} node[pos=1,right]{(5)};\n\\addplot[color=red,mark=triangle,]\ncoordinates {(150,61.74)(229,61.94)(305,61.63)(364,61.05)(412,60.91)}\nnode[pos=0,left]{(1)} node[pos=1,right]{(5)};\n\n\\legend{\\phantom{0}50 docs/step,100 docs/step,150 docs/step}\n\n\\end{axis}\n\\end{tikzpicture}\n\\hskip 1pt\n\\begin{tikzpicture}\n\\begin{axis}[\n    xlabel={Total Paragraphs Retrieved/Question},\n    height=5cm,\n    xmin=50, xmax=400,\n    ymin=73.9, ymax=79.1,\n    xtick={0,100,200,300,400},\n    ytick={74,75,76,77,78,79},\n    legend pos=south east,\n    ymajorgrids=true,\n    grid style=dashed,\n]\n\n\\addplot[color=blue,mark=square,]\ncoordinates  {(100,75.42)(107,76.94)(109,77.10)(110,77.11)}\nnode[pos=0,left]{(2)} node[pos=1,right]{(5)};\n    \n\\addplot[color=green,mark=star,]\ncoordinates {(200,76.17)(214,77.62)(216,77.76)(218,77.72)}\nnode[pos=0,left]{(2)} node[pos=1,right]{(5)};\n\n\\addplot[color=red,mark=triangle,]\ncoordinates {(300,76.40)(320,78.18)(328,78.40)(333,78.41)}\nnode[pos=0,left]{(2)} node[pos=1,right]{(5)};\n\n\\legend{\\phantom{0}50 docs/step,100 docs/step,150 docs/step}\n\n\\end{axis}\n\\end{tikzpicture}\n}\n\\caption{The retrieval behavior of \\irrr{} and its relation to the performance of end-to-end question answering.\nTop: The distribution of reasoning path lengths as determined by \\irrr{}\\@. Bottom: Total number of paragraphs retrieved by \\irrr{} vs.\\ the end-to-end question answering performance as measured by answer \\fone{}.\n} \\label{fig:retrieval}\n\\end{figure}\n\nTo better understand the behavior of \\irrr{} on these benchmarks, we analyze the number of paragraphs retrieved by the model when varying the number of paragraphs retrieved at each reasoning step among $\\{50, 100, 150\\}$.\nAs can be seen in Figure \\ref{fig:retrieval}, \\irrr{} stops its iterative process as soon as all necessary paragraphs to answer the question have been retrieved, effectively reducing the total number of paragraphs retrieved and read by the model compared to always retrieving a fixed number of paragraphs for each question.\nFurther, we note that the optimal cap for the number of reasoning steps is larger than the number of gold paragraphs necessary to answer the question on each benchmark, which we find is due to \\irrr{}'s ability to recover from retrieving and selecting non-gold paragraphs (see the example in Figure~\\ref{fig:example_query_prediction}).\nFinally, we note that increasing the number of paragraphs retrieved at each reasoning step remains an effective, if computationally expensive, strategy, to improve the end-to-end performance of \\irrr{}.\nHowever, the tradeoff between retrieval budget and model performance is more effective than that of previous work (\\eg, GRR), and we note that the queries generated by \\irrr{} are explainable to humans and can help humans easily control its behavior.\n\n\\subsection{Performance on the Unified Benchmark}\n\n\\begin{table}\n\\small\n\\centering\n    \\resizebox{.45\\textwidth}{!}{\n    \\begin{tabular}{lcccccc}\n        \\toprule\n        \\multirow{2}{2cm}{} & \\multicolumn{2}{c}{Dev} && \\multicolumn{2}{c}{Test} \\\\\n         \\cline{2-3} \\cline{5-6}  \n         & EM & \\fone{} && EM & \\fone{} \\\\ \n         \\midrule\n         \\squad{} Open  & 50.65 & 60.99 && 60.59 & 67.51  \\\\\n         \\hotpotqa{}    & 59.01 & 70.33 && 58.61 & 69.86  \\\\\n         3+ hop & --- & --- && 33.02 & 39.59  \\\\\n         \\midrule\n         Micro-averaged & 54.20 & 64.95 && 58.82 & 67.46 \\\\\n         Macro-averaged & 54.83 & 65.66 && 50.74 & 58.99 \\\\\n         \\bottomrule\n    \\end{tabular}\n    }\n    \\caption{End-to-end question answering performance of \\irrr{} on the unified benchmark, evaluated on the 2020 copy of Wikipedia.\n    These results are not directly comparable with those in Tables \\ref{tab:benchmark_results_squad} and \\ref{tab:benchmark_results_hotpot_3hop} because the set of questions and Wikipedia documents differ.}\n    \\label{tab:main_results}\n\\end{table}\n\n\\begin{table}\n    \\small\n    \\centering\n    \\resizebox{.48\\textwidth}{!}{%\n    \\begin{tabular}{lccc}\n    \\toprule\n    System & \\squad{} & \\hotpotqa{}\\\\\n    \\midrule\n    Ours (joint dataset) & %\n    58.69 & %\n    68.74  \\\\\n    \\quad vs. fixed retrieval steps ($K=3$) & %\n    31.70 & %\n    66.60 \\\\\n    \\quad vs. remove \\hotpotqa{} / \\squad{} data & 54.35 & %\n    66.91 \\\\\n    \\quad replace ELECTRA w/ BERT\\textsubscript{LARGE-WWM} & 57.19 & %\n    63.86 \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\caption{Ablation study of different design choices in \\irrr{}, as evaluated by Answer \\fone{} on the dev set of the unified benchmark.\n    Results differ from those in Table \\ref{tab:main_results} because fewer reasoning steps are used (3 vs.\\ 5) and fewer paragraphs retrieved at each step (50 vs.\\ 150).}\n    \\label{tab:ablations}\n\\end{table}\n\nTo demonstrate the performance of \\irrr{} in a more realistic setting of open-domain QA, we evaluate it on the new, unified benchmark.\nAs is shown in Table \\ref{tab:main_results}, \\irrr{}'s performance remains competitive on all questions from different origins in the unified benchmark, despite the difference in reasoning complexity when answering these questions.\nThe model also generalizes to the 3-hop questions despite having never been trained on them.\nWe note that the large performance gap between the development and test settings for \\squadopen{} questions is due to the fact that test set questions (the original \\squad{} dev set) are annotated with multiple human answers, while the dev set ones (originally from the \\squad{} training set) are not.\n\n\\begin{figure}\n    \\centering\n    \\resizebox{0.48\\textwidth}{!}{\n    \\small\n    \\begin{tabular}{p{4.5em}p{25em}}\n    \\toprule\n    Question & The \\textbf{\\textcolor{blue}{\\underline{Ingerophrynus gollum}}} is \\textbf{\\textcolor{blue}{\\underline{named}}} after a \\textbf{\\textcolor{blue}{\\underline{character}}} in a \\textbf{\\textcolor{blue}{\\underline{book}}} that \\textbf{\\textcolor{blue}{\\underline{sold}}} how many \\textbf{\\textcolor{blue}{\\underline{copies}}}? \\\\\n    \\midrule\n     Step 1\\newline \\emph{(Non-Gold)} & Ingerophrynus is a genus of true toads with 12 species. ... In 2007 a new species, ``Ingerophrynus gollum'', was added to this genus. This species is named after the character Gollum created by \\textbf{\\textcolor{green}{\\underline{J. R. R. Tolkien}}}.\" \\\\\n    Query & \\textcolor{blue}{Ingerophrynus gollum book sold copies} \\textbf{\\textcolor{green}{\\underline{J. R. R. Tolkien}}} \\\\\n    \\midrule\n     Step 2 \\newline \\emph{(Gold)} & Ingerophrynus gollum (Gollum's toad) is a species of \\textbf{\\textcolor{purple}{\\underline{true}}} toad. ... It is called ``gollum'' with reference of the eponymous character of The \\textbf{\\textcolor{purple}{\\underline{Lord of the Rings}}} by J. R. R. Tolkien. \\\\\n    Query & \\textcolor{blue}{Ingerophrynus gollum character book sold copies} \\textcolor{green}{J. R. R. Tolkien} \\textbf{\\textcolor{purple}{\\underline{true}} \\textcolor{purple}{\\underline{Lord of the Rings}}} \\\\\n    \\midrule\n     Step 3 \\newline \\emph{(Gold)} & The Lord of the Rings is an epic high fantasy novel written by English author and scholar J. R. R. Tolkien. ...  is one of the best-selling novels ever written, with \\textbf{\\textcolor{red}{150 million copies}} sold.   \\\\\n     Answer/GT & \\textbf{\\textcolor{red}{150 million copies}} \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\caption{An example of \\irrr{} answering a question from \\hotpotqa{} by generating natural language queries to retrieve paragraphs, then rerank them to compose reasoning paths and read them to predict the answer.\n    Here, \\irrr{} recovers from an initial retrieval/reranking mistake by retrieving more paragraphs, before arriving at the gold supporting facts and the correct answer.}\n    \\label{fig:example_query_prediction}\n\\end{figure}\n\nTo better understand the contribution of the various components and techniques we proposed for \\irrr{}, we performed ablation studies on the model iterating up to 3 reasoning steps with 50 paragraphs for each step, and present the results in Table \\ref{tab:ablations}. %\nFirst of all, we find it is important to allow \\irrr{} to dynamically stop retrieving paragraphs to answer the question.\nCompared to its fixed-step retrieval counterpart, dynamically stopping \\irrr{} improves \\fone{} on both \\squad{} and \\hotpotqa{} questions by 27.0 and 2.1 points respectively (we include further analyses for dynamic stopping in Appendix~\\ref{failure_cases}). \nWe also find combining SQuAD and HotpotQA datasets beneficial for both datasets in an open-domain setting, and that ELECTRA is an effective alternative to BERT for this task. \n\n\\section{Related Work}\n\nThe availability of large-scale question answering (QA) datasets has greatly contributed to the research progress on open-domain QA.\n\\squad{} \\citep{rajpurkar2016squad, rajpurkar2018know} is among the first question answering datasets adopted for this purpose by \\citet{chen2017reading} to build QA systems over Wikipedia articles.\nSimilarly, \\triviaqa{} \\citep{joshi-etal-2017-triviaqa} and Natural Questions \\citep{kwiatkowski-etal-2019-natural} feature Wikipedia-based questions that are written by trivia enthusiasts and extracted from Google search queries, respectively.\nMore recently, \\citet{petroni-etal-2021-kilt} presented, KILT, a new benchmark based on Wikipedia where many knowledge-based tasks are evaluated in a unified version of Wikipedia, including open-domain question answering, entity linking, dialogue, etc.\nUnlike \\beerqa{}, however, single-hop and multi-hop QA are held completely separate during evaluation in KILT, which makes the evaluation of open-domain QA less realistic.\nAside from Wikipedia, researchers have also used news articles \\citep{trischler2016newsqa} and search results from the web \\citep{dunn2017searchqa, talmor2018web} as the corpus for open-domain QA.\n\nInspired by the TREC QA challenge,\\footnote{\\url{https://trec.nist.gov/data/qamain.html}} \\citet{chen2017reading} were the first to combine information retrieval systems with accurate neural network-based reading comprehension models for open-domain QA.\nRecent work has improved open-domain QA performance by enhancing various components in this retrieve-and-read approach.\nWhile much research focused on improving the reading comprehension model \\citep{seo2017bidirectional, clark-gardner-2018-simple}, especially with pretrained langauge models like BERT \\citep{devlin2019bert}, researchers have also demonstrated that neural network-based information retrieval systems achieve competitive, if not better, performance compared to traditional IR engines \\citep{lee2019latent, khattab2020relevance, guu2020realm, xiong2021answering}.\nAside from the reading comprehension and retrieval components, researchers have also found value from reranking search results \\citep{wang2018reinforced} or answer candidates \\citep{wang2018evidence, hu2019retrieve}.\n\nWhile most work focuses on questions that require only a local context of supporting facts to answer, \\citet{yang2018hotpotqa} presented \\hotpotqa{}, which tests whether open-domain QA systems can generalize to more complex questions that require evidence from multiple documents to answer.\nResearchers have explored various techniques to extend retrieve-and-read systems to this problem, including making use of hyperlinks between Wikipedia articles \\citep{nie2019revealing, feldman2019multi, zhao2019transformer, asai2020learning, dhingra2020differentiable, zhao2019transformer} and iterative retrieval \\citep{talmor2018web, das2019multi, qi2019answering}.\nWhile most previous work on iterative retrieval makes use of neural retrieval systems that directly accept real vectors as input, our work is similar to that of \\citet{qi2019answering} in using natural language search queries.\nA crucial distinction between our work and previous work on multi-hop open-domain QA, however, is that we don't train models to exclusively answer single-hop or multi-hop questions, but demonstrate that one single set of parameters performs well on both tasks.\n\\section{Conclusion}\n\nIn this paper, we presented Iterative Retriever, Reader, and Reranker (\\irrr{}), a system that uses a single model to perform subtasks to answer open-domain questions of arbitrary reasoning steps.\n\\irrr{} achieves competitive results on standard open-domain QA benchmarks, and establishes a strong baseline on \\beerqa{}, the new unified benchmark we present, which features questions with mixed levels of complexity.\n\n\\section*{Acknowledgments}\n\nThe authors would like to thank the anonymous reviewers for discussions and comments on earlier versions of this paper. \nThis research is funded in part by Samsung Electronics Co., Ltd.~and in part by the SAIL-JD Research Initiative.\n\n\\clearpage\n\\appendix\n\\section{Data processing} \\label{sec:data_processing}\n\nIn this section, we describe how we process the English Wikipedia and the \\squad{} dataset for training and evaluating \\irrr{}.\n\nFor the standard benchmarks (\\squadopen{} and \\hotpotqa{} fullwiki), we use the Wikipedia corpora prepared by \\citet{chen2017reading} and \\citet{yang2018hotpotqa}, respectively, so that our results are comparable with previous work on these benchmarks.\nSpecifically, for \\squadopen, we use the processed English Wikipedia released by \\citet{chen2017reading} which was accessed in 2016, and contains 5,075,182 documents.\\footnote{\\url{https://github.com/facebookresearch/DrQA}}\nFor \\hotpotqa{}, \\citet{yang2018hotpotqa} released a processed set of Wikipedia introductory paragraphs from the English Wikipedia originally accessed in October 2017.\\footnote{\\url{https://hotpotqa.github.io/wiki-readme.html}}\n\nWhile it is established that the \\squad{} dev set is repurposed as the test set for \\squadopen{} for ease of evaluation, most previous work make use of the entire training set during training, and as a result a proper development set for \\squadopen{} does not exist.%\n\\footnote{Thus, if any hyperparameter tuning has been performed, it is usually done to directly maximize the performance on this held-out test set, inflating the performance on this set as a result.}\nWe therefore resplit the \\squad{} training set into a proper development set that is not used during training, and a reduced training set that we use for all of our experiments.\nAs a result, although \\irrr{} is evaluated on the same test set as previous systems, it is likely disadvantaged due to the reduced amount of training data and hyperparameter tuning on this new dev set.\nWe split the training set by first grouping questions and paragraphs by the Wikipedia entity/title they belong to, then randomly selecting entities to add to the dev set until the dev set contains roughly as many questions as the test set (original \\squad{} dev set).\nThe statistics of our resplit of \\squad{} can be found in Table \\ref{tab:squad_resplit}.\nWe make our resplit publicly available to the community at \\url{https://beerqa.github.io/}.\n\nFor the unified benchmark, we started by processing the English Wikipedia\\footnote{Accessed on August 1st, 2020, which contains 6,133,150 articles in total.} with the WikiExtractor \\citep{attardi2015wikiextractor}.\nWe then tokenized this dump and the supporting context used in \\squad{} and \\hotpotqa{} with Stanford CoreNLP 4.0.0 \\citep{manning-EtAl:2014:P14-5} to look for paragraphs in the 2020 Wikipedia dump that might correspond to the context paragraphs in these datasets.\nSince many Wikipedia articles have been renamed or removed since, we begin by following Wikipedia redirect links to locate the current title of the corresponding Wikipedia page (\\eg, the page \\emph{``Madonna (entertainer)''} has been renamed \\emph{``Madonna''}).\nAfter the correct Wikipedia article is located, we look for combinations of one to two consecutive paragraphs in the 2020 Wikipedia dump that have high overlap with context paragraphs in these datasets.\nWe calculate the recall of words and phrases in the original context paragraph (because Wikipedia paragraphs are often expanded with more details), and pick the best combination of paragraphs from the article.\nIf the best candidate has either more than 66\\% unigrams in the original context, or if there is a common subsequence between the two that covers more than 50\\% of the original context, we consider the matching successful, and map the answers to the new context paragraphs.\nThe main causes of mismatches are a) Wikipedia pages that have been permanently removed (due to copyright issues, unable to meet notability standards, etc.); b) significantly edited to improve presentation (see Figure \\ref{fig:wiki_changes:madonna}); c) significantly edited because the world has changed (see Figure \\ref{fig:wiki_changes:peter_madsen}).\n\n\\begin{figure*}\n    \\small\n    \\centering\n    \\subfigure[The Wikipedia page about Madonna, on December 20, 2016 (on the left, which is in the version \\squadopen{} used) versus July 31, 2020 (on the right, which is in the version \\beerqa{} used).]{\n    \\parbox{0.47\\textwidth}{%\n    \\begin{framed}\n    Madonna Louise Ciccone (born August 16, 1958) is an American singer, songwriter, actress, and \\deleted{businesswoman}. \\modified{She achieved popularity by pushing the boundaries of lyrical content in mainstream popular music and imagery in her music videos}, \\deleted{which became a fixture on MTV.} \\modified{Madonna is known for reinventing both her music and image, and for maintaining her autonomy within the recording industry.} Music critics have acclaimed her musical productions, which have generated some controversy. Referred to as the ``Queen of Pop'', Madonna is often cited as an influence by other artists.\n    \\end{framed}}\\ \n    \\parbox{0.53\\textwidth}{%\n    \\begin{framed}\n    Madonna Louise Ciccone (born August 16, 1958) is an American singer-songwriter, author, actress and \\added{record executive}. She has been referred to as the ``Queen of Pop'' \\added{since the 1980s}. \\modified{Madonna is noted for her continual reinvention and versatility in music production, songwriting, and visual presentation. She has pushed the boundaries of artistic expression in popular culture, while remaining completely in charge of every aspect of her career.} Her works, \\added{which incorporate social, political, sexual, and religious themes,} have made a cultural impact which has generated both critical acclaim and controversy. Madonna is often cited as an influence by other artists.\n    \\end{framed}}\n    \\label{fig:wiki_changes:madonna}\n    }\\\\\n    \\subfigure[The Wikipedia page about Peter Madsen, on September 27, 2017 (on the left, which is in the version \\hotpotqa{} used) versus July 26, 2020 (on the right, which is in the version \\beerqa{} used).]{\n    \\parbox{0.55\\textwidth}{%\n    \\begin{framed}\n    Peter Langkj\\ae{}r Madsen (born 12 January 1971) is a Danish \\deleted{aerospace engineering enthusiast, ``art engineer'', submarine builder, entrepreneur, co-founder of the non-profit organization Copenhagen Suborbitals, and founder and CEO of RML Spacelab ApS. He was arrested in August 2017 for involvement} in the death of Swedish journalist Kim Wall; \\deleted{the investigation is ongoing.}\n    \\end{framed}}\n    \\parbox{0.45\\textwidth}{%\n    \\begin{framed}\n    Peter Langkj\\ae{}r Madsen (]; born 12 January 1971) is a Danish \\added{convicted murderer. In April 2018 he was convicted of the 2017} murder of Swedish journalist Kim Wall \\added{on board his submarine, UC3 Nautilus, and sentenced to life imprisonment. He had previously been an engineer and entrepreneur.}\n    \\end{framed}}\n    \\label{fig:wiki_changes:peter_madsen}\n    }\n    \\caption{Changes in Wikipedia that present challenges in matching them across years. We highlight portions of the text that have been deleted in \\deleted{red underlined text}, that have been added in \\added{green boldface text}, and that have been significantly paraphrased in \\modified{orange italics}, and leave near-verbatim text in the normal font and color.}\n    \\label{fig:wiki_changes}\n\\end{figure*}\n\n\\begin{table}\n    \\centering\n    \\begin{tabular}{cccc}\n    \\toprule\n         Split & Origin & \\# Entities & \\#QAs \\\\\n    \\midrule\n        train & train & 387 & 77,087 \\\\\n        dev & train & \\phantom{0}55 & 10,512 \\\\\n        test & dev & \\phantom{0}48 & 10,570 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Statistics of the resplit \\squad{} dataset for proper training and evaluation on the \\squadopen{} setting.}\n    \\label{tab:squad_resplit}\n\\end{table}\n\nAs a result, 20,182/2,146 \\squad{} train/dev examples (that is, 17,802/2,380/2,146 train/dev/test examples after data resplit) and 15,806/1,416/1,427 \\hotpotqa{} train/dev/fullwiki test examples have been excluded from the unified benchmark.\nTo understand the data quality after converting \\squadopen{} and \\hotpotqa{} to the newer version of Wikipedia, we sampled 100 examples from the training split of each dataset.\nWe find that 6\\% of \\squad{} questions and 10\\% of \\hotpotqa{} questions are no longer answerable from their context paragraphs due to edits in Wikipedia or changes in the world, despite the presence of the answer span.\nWe also find that 43\\% of \\hotpotqa{} examples contain more than the minimal set of necessary paragraphs to answer the question as a result of the mapping process.\n\n\\section{Elasticsearch Setup} \\label{sec:search_engine}\n\nWe set up Elasticsearch in standard benchmark settings (\\squadopen{} and \\hotpotqa{} fullwiki) following practices in previous work \\citep{chen2017reading, qi2019answering}, with minor modifications to unify these approaches.\n\nSpecifically, to reduce the context size for the Transformer encoder in \\irrr{} to avoid unnecessary computational cost, we primarily index the individual paragraphs in the English Wikipedia.\nTo incorporate the broader context from the entire article, as was done by \\citet{chen2017reading}, we also index the full text for each Wikipedia article to help with scoring candidate paragraphs.\nEach paragraph is associated with the full text of the Wikipedia article it originated from, and the search score is calculated as the summation of two parts: the similarity between query terms and the paragraph text, and the similarity between the query terms and the full text of the article.\n\nFor query-paragraph similarity, we use the standard BM25 similarity function \\citep{robertson1995okapi} with default hyperparameters ($k_1=1.2, b=0.75$).\nFor query-article similarity, we find BM25 to be less effective, since the length of these articles overwhelm the similarity score stemming from important rare query terms, which has also been reported in the information retrieval literature \\citep{lv2011documents}.\nInstead of boosting the term frequenty score as considered by \\citet{lv2011documents}, we extend BM25 by taking the square of the IDF term and setting the TF normalization term to zero ($b=0$), which is similar to the TF-IDF implementation by \\citet{chen2017reading} that is shown effective for \\squadopen{}.\n\nSpecifically, given a document $D$ and query $Q$, the score is calculated as\n\\begin{align}\n    \\mathrm{score}(D, Q) &= \\sum_{i=1}^n \\mathrm{IDF}_+^2(q_i) \\cdot \\frac{f(D, q_i) \\cdot (1 + k_1)}{f(D, q_i) + k_1},\n\\end{align}\nwhere $\\mathrm{IDF}_+(q_i) = \\max(0, \\log((N-n(q_i) + 0.5) / (n(q_i) + 0.5))$, with $N$ denoting the total numberr of documents and $n(q_i)$ the document frequency of query term $q_i$, and $f(q_i, D)$ is the term frequency of query term $q_i$ in document $D$.\nWe set $k_1 = 1.2$ in all of our experiments.\nIntuitively, compared to the standard BM25, this scoring function puts more emphasis on important, rare term overlaps while it is less dampened by document length, making it ideal for an initial sift to find relevant documents for open-domain question answering.\n\n\\section{Further Training and Prediction Details} \\label{sec:further_training_details}\n\nWe include the hyperparameters used to train the \\irrr{} model in Table \\ref{tab:hyperparams} for reproducibility.\n\nFor our experiments using \\squad{} for training, we also follow the practice of \\citet{asai2020learning} to include the data for \\squad{} 2.0 \\citep{rajpurkar2018know} as negative examples for the reader component.\nHyperparameters like the prediction threshold of binary classifiers in the query generator are chosen on the development set to optimize end-to-end QA performance.\n\nWe also include how we use the reader model's prediction to stop the \\irrr{} pipeline for completeness.\nSpecifically, when the most likely answer is yes or no, the answerability of the reasoning path is the difference between the yes/no logit and the \\texttt{NOANSWER} logit.\nFor reasoning paths that are not answerable, we further train the span classifiers to predict the \\clstoken{} token as the ``output span'', and thus we also include the likelihood ratio between the best span and the \\clstoken{} span if the positive answer is a span.\nTherefore, when the best predicted answer is a span, its answerability score is computed by including the score of the ``\\clstoken{} span'' as well, \\ie,\n\\begin{align}\n    \\mathrm{Answerability}_{\\text{span}}(p) &= \\mathrm{logit}_{\\text{span}} - \\mathrm{logit}_{\\texttt{NOANSWER}} \\nonumber \\\\\n    &+ \\frac{\\mathrm{logit}_{s}^{\\text{start}} - \\mathrm{logit}_{\\clstoken}^{\\text{start}}}{2} \\nonumber \\\\\n    &+ \\frac{\\mathrm{logit}_{e}^{\\text{end}} - \\mathrm{logit}_{\\clstoken}^{\\text{end}}}{2},\n\\end{align}\nwhere $\\mathrm{logit}_{\\text{span}}$ is the logit of predicting span answers from the 4-way classifier, while $\\mathrm{logit}^{\\text{start}}$ and $\\mathrm{logit}^{\\text{end}}$ are logits from the span classifiers for selecting the predicted span from the reasoning path.\n\n\\begin{table}\n    \\centering\n    \\small\n    \\begin{tabular}{lc}\n        \\toprule\n        Parameter & Value \\\\\n        \\midrule\n        Learning rate & $3\\times 10^{-5}$ \\\\\n        Batch size & 320 \\\\\n        Iteration & 10,000 \\\\\n        Warming-up & 1,000 \\\\\n        Training tokens & $1.638\\times 10^9$\\\\\n        Reranker Candidates & 5 \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{Hyperparameter setting for \\irrr{} training.} \n    \\label{tab:hyperparams}\n\\end{table}\n\n\\section{Further Analyses of Model Behavior}\n\nIn this section, we perform further analyses and introduce further case studies to demonstrate the behavior of the \\irrr{} system.\nWe start by analyzing the effect of the dynamic stopping criterion for reasoning path retrieval, then move on to the end-to-end performance and leakages in the pipeline, and end with a few examples to demonstrate typical failure modes we have identified that might point to limitations with the data.\n\n\\begin{figure}\n    \\centering\n    \\resizebox{0.48\\textwidth}{!}{\n    \\small\n    \\begin{tabular}{p{6em}p{24em}}\n    \\toprule\n    Question & What \\textbf{\\textcolor{blue}{\\underline{team}}} was the \\textbf{\\textcolor{blue}{\\underline{AFC champion}}}? \\\\\n    \\midrule\n     Step1\\newline (Non-Gold) & However, the eventual-AFC Champion \\textbf{\\textcolor{green}{\\underline{Cincinnati Bengals}}}, playing in their first AFC Championship Game, defeated the Chargers 27-7 in what became known as the Freezer Bowl. ... \\\\\n    \\midrule\n     Step2 \\newline (Non-Gold) & Super Bowl XXVII was an American football game between the American Football Conference (AFC) champion \\textbf{\\textcolor{purple}{\\underline{Buffalo Bills}}} and the National Football Conference (NFC) champion Dallas Cowboys to decide the National Football League (NFL) champion for the 1992 season. ... \\\\\n    \\midrule\n     Gold & Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion \\textbf{\\textcolor{red}{\\underline{Denver Broncos}}} defeated the National Football Conference (NFC) champion Carolina Panthers 24-10 to earn their third Super Bowl title. ... \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\caption{An example where there are false negative answers in Wikipedia for the question from \\squadopen{}.}\n    \\label{fig:squad_failure_case}\n\\end{figure}\n\n\\paragraph{Effect of Dynamic Stopping.}\nWe begin by studying the effect of using the answerability score as a criterion to stop the iterative retrieval, reading, and reranking process within \\irrr{}.\nWe compare the performance of a model with dynamic stopping to one that is forced to stop at exactly $K$ steps of reasoning, neither more nor fewer, where $K=1, 2, \\ldots, 5$.\nAs can be seen in Table \\ref{tab:squad_hotpotqa_adaptive}, \\irrr{}'s dynamic stopping criterion based on the answerability score is very effective in achieving good end-to-end question answering performance for questions of arbitrary complexity without having to specify the complexity of questions ahead of time.\nOn both \\squadopen{} and \\hotpotqa{}, it achieves competitive, if not superior question answering performance, even without knowing the true number of gold paragraphs necessary to answer each question.\n\n\\begin{table}\n    \\centering\n    \\small\n    \\begin{tabular}{lccp{.1em}cc}\n    \\toprule\n    \\multirow{2}{3em}{Steps} & \\multicolumn{2}{c}{\\squadopen{}} && \\multicolumn{2}{c}{\\hotpotqa{}}\\\\\n    \\cline{2-3}\\cline{5-6}\n    & EM & \\fone && EM & \\fone\n    \\\\\n    \\midrule\n    Dynamic &  49.92 & 60.91 && \\textbf{65.74} & \\textbf{78.41} \\\\\n    1 step & \\textbf{51.07} & \\textbf{61.74} && 13.75 & 18.95 \\\\\n    2 step & 38.74 & 48.61 && 65.12 & 77.75 \\\\\n    3 step & 32.14 & 41.66 && 65.37 & 78.16 \\\\\n    4 step & 29.06 & 38.33 && 63.89 & 76.72 \\\\\n    5 step & 19.53 & 25.86 && 59.86 & 72.79 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\\squad{} and \\hotpotqa{} performance using adaptive vs.\\ fixed-length reasoning paths, as measured by answer exact match (EM) and \\fone{}.\n    The dynamic stopping criterion employed by \\irrr{} achieves comparable performance to its fixed-step counterparts, without knowledge of the true number of gold paragraphs.}\n    \\label{tab:squad_hotpotqa_adaptive}\n\\end{table}\n\nAside from this, we note four interesting findings: (1) the performance of \\hotpotqa{} does not peak at two steps of reasoning, but instead is helped by performing a third step of retrieval for the average question; (2) for both datasets, forcing the model to retrieve more paragraphs after a point consistently hurt QA performance; (3) dynamic stopping slightly hurts QA performance on \\squadopen{} compared to a fixed number of reasoning steps ($K=1$); (4) when \\irrr{} is allowed to select a dynamic stopping criterion for each example independently, the resulting question answering performance is better than a one-size-fits-all solution of applying the same number of reasoning steps to all examples.\nWhile the last confirms the effectiveness of our answerability-based stopping criterion, the cause behind the first three warrants further investigation.\nWe will present further analyses to shed light on potential causes of these in the remainder of this section.\n\n\\paragraph{Case Study for Failure Cases.} \\label{failure_cases}\n\nBesides model inaccuracy, one common reason for \\irrr{} to fail at finding the correct answer provided with the datasets is the existence of false negatives (see Figure \\ref{fig:squad_failure_case} for an example from \\squadopen{}).\nWe estimate that there are about 9\\% such cases in the \\hotpotqa{} part of the training set, and 26\\% in the \\squad{} part of the training set.\n\nThese false negatives hurt the quality of data generation as well, especially when generating the \\squad{} part of the training set. We investigate randomly selected question-context pairs in the training set and find 24\\% of our \\squad{} training set and 13\\% of GRR's \\squad{} training set are false negatives. This means our methods find better candidate documents but true answers in those documents become false positives. That results in worse performance for our model when it is trained with only the \\squad{} part of training set as shown in Table~\\ref{tab:benchmark_results_squad}. \n\n\\section{Three+ Hop Challenge Set Analysis} \\label{sec:three_hop}\nAlthough \\squadopen{} and \\hotpotqa{} probe our model's ability on single and two-hop questions, we lacked insight into the ability of our model to generalize to questions that require three or more reasoning steps/hops, which is more than what our model is trained on. \nTherefore, we built a challenge set comprised of questions that require at least three hops of reasoning to answer (see Table \\ref{tab:hops-stats} for a breakdown of the number of documents required to answer each question in the challenge set). While the vast majority of challenge set questions require three documents, questions that require four or more documents are also present, hence the ``Three+ Hop Challenge Set'' name.\nAlthough we intend to use the challenge set for testing only, we will share a few key insights into the question sourcing process, the reasoning types required, and the answer types present.\n\n\\begin{table}\n\\small\n\\begin{tabular}{p{2.5cm}cccccc}\n\\toprule\n\\# of Documents to answer the question & 3 & 4 & 5 & 6 & 7 & 8\\\\\n\\midrule\n\\# of questions & 495 & 17 & 8 & 0 & 9 & 1\\\\\n\\bottomrule\n\\end{tabular}\n\n\\caption{Distribution of reasoning steps for questions in Three+ Hop Challenge Set.} \\label{tab:hops-stats}\n\\end{table}\n\n\\paragraph{Question Sourcing Process.}\nWe annotated 530 examples that require three or more paragraphs to be answered on the 2020 Wikipedia dump. \nWe developed roughly 50--100 question templates that cover a diverse set of topics, including science, literature, film, music, history, sports, technology, politics, and geography. \nWe then annotated approximately ten to twenty examples from each of these question templates to ensure that the resulting challenge set contained a diverse set of topics and questions.\n\n\\begin{table}\n    \\centering\n\\begin{tabular}{lcc}\n\\toprule\nReasoning Type    & \\%   \\\\ \n\\midrule\nComparison        & 25.6 \\\\\nBridge-Comparison &  25.3 \\\\\nBridge            & 49.1 \\\\\n\\bottomrule\n\\end{tabular}\n\n\\caption{Reasoning types required for Three+ Hop Challenge Set.}\n\\label{tab:three_hop_reasoning_types}\n\\end{table}\n\n\\paragraph{Reasoning Types.}\nDuring the annotation process for the challenge set, we recorded the types of reasoning required to answer each question (Table \\ref{tab:three_hop_reasoning_types}). \nRoughly half of the questions require chain reasoning (Bridge), where the reader must identify bridge entities that link the question to the first context paragraph, the first context paragraph to the second, and finally the second to the third where the answer can be found. In the case that four or more hops of reasoning are required, this chain of reasoning will extend past the third paragraph to the $n$-th paragraph where the answer can be found.\nAdditionally, approximately 25\\% of the questions require the comparison of three or more entities (Comparison). \nFor these questions, the reader needs to retrieve three or more context paragraphs identified in the question that are not directly connected to each other and then compare them on certain aspects specified in the question, similar to the comparison questions in \\hotpotqa{}. \nThe remaining 25\\% of the questions require both chain reasoning and the comparison of two or more entities (Bridge-Comparison). \nFor these questions, the reader must first identify a bridge entity that links the question to the first context paragraph. They then must identify two or more entities to compare within the first context paragraph. \nAfterwards, they retrieve context paragraphs for each of the aforementioned entities and compare them on certain aspects specified in the question.\n\n\\begin{table}\n    \\centering\n    \\resizebox{\\linewidth}{!}{%\n    \\begin{tabular}{lcl}\n    \\toprule\nAnswer Type   & \\%   & Example(s)                                            \\\\ \n\\midrule\nPerson        & 29 & Kate Elizabeth Winslet                                \\\\\nNumber        & 20 & 388,072, 5.5 million                                  \\\\\nYes / No      & 15 & ---                                                     \\\\\nGroup / Org   & 11 & CNN                                                   \\\\\nDate          & 8 & March 28, 1930                                        \\\\\nOther Proper Noun & 7 & Boeing 747-400                                        \\\\\nCreative Work & 5  & ``California Dreams''                                   \\\\\nLocation      & 4  & New York City                                         \\\\\nCommon Noun   & 1    & comedy-drama                                         \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Types of answers in Three+ Hop Challenge Set. \nThese statistics are based on 100 randomly sampled examples.}\n\\label{tab:three_hop_answer_types}\n\\end{table} \n\n\\paragraph{Answer Types.}\nWe also analyze the types of answers present in the challenge set.\nAs shown in Table \\ref{tab:three_hop_answer_types}, the challenge set features a diverse set of answers. \nWe find that roughly half of the questions ask about people (29\\%) and numeric quantities (20\\%).  \nAdditionally, we find a considerable number of questions that require a yes or no answer (15\\%), ask about groups or organizations (11\\%), dates (8\\%), and other proper nouns (7\\%). The challenge set also contains a non-negligible amount of questions that ask about creative works (5\\%), locations (4\\%), and common nouns (1\\%).\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-1911.10470v2.tex",
        "arXiv-2009.12756v2.tex",
        "arXiv-2010.12527v4.tex"
    ],
    "group_id": "group_103",
    "response": "### Title: Advances in Multi-Hop Open-Domain Question Answering: Graph-Based Retrieval, Dense Retrieval, and Iterative Retrieval\n\n### Introduction\n\nOpen-domain question answering (QA) is a critical area of natural language processing (NLP) that aims to extract answers from a large, unstructured text corpus, such as Wikipedia, in response to complex questions. This task has seen significant advancements in recent years, with the introduction of datasets like SQuAD, Natural Questions, and HotpotQA, which challenge models to retrieve and reason over multiple pieces of evidence to answer questions. However, answering multi-hop questions, which require information from more than one document, remains a formidable challenge. Traditional term-based information retrieval (IR) methods often fail to capture the necessary semantic relationships between question and evidence, leading to poor performance. Recent approaches have attempted to address this issue by leveraging neural networks and graph structures, but they have their own limitations, such as the need for corpus-specific metadata or the inability to generalize across different domains.\n\nThe field of open-domain QA has made substantial progress, particularly with the advent of end-to-end models that integrate retrieval and reading comprehension. These models have shown promise in handling complex questions, but they still face significant hurdles, especially in the context of multi-hop reasoning. The primary challenge lies in the scalability and efficiency of these models, as well as their ability to generalize to unseen question types and corpora. This summary explores three recent papers that tackle these challenges using different methodologies: a graph-based recurrent retrieval approach, a multi-hop dense retrieval method, and an iterative retriever, reader, and reranker (IRRR) system.\n\n### Paper 1: Graph-Based Recurrent Retrieval for Multi-Hop QA\n\nThe first paper introduces a graph-based recurrent retrieval approach to answer multi-hop open-domain questions. The authors construct a graph of Wikipedia paragraphs using hyperlinks and document structures to model relationships between paragraphs. This graph is used to sequentially retrieve evidence paragraphs that form reasoning paths, which are then ranked by a reading comprehension model to extract the answer span. The paper's primary contribution is the introduction of a recurrent neural network (RNN) that learns to retrieve evidence paragraphs by conditioning on the history of previously retrieved documents. This method is particularly effective in handling entity-centric questions, where the evidence paragraphs may have little lexical or semantic overlap with the question.\n\nThe authors propose a two-stage training process: the retriever is trained to score reasoning paths by maximizing the likelihood of selecting correct evidence paragraphs, while the reader is trained to rank these paths and extract the answer. The retriever uses a combination of TF-IDF retrieval and beam search over the Wikipedia graph to ensure scalability and robustness. Data augmentation and negative example mining are employed to stabilize the training process and improve performance. The paper demonstrates significant improvements over existing methods, particularly on the HotpotQA dataset, achieving an absolute gain of more than 14 points in F1 and EM scores.\n\n#### Key Innovations:\n- **Graph Construction**: Utilizes Wikipedia hyperlinks to construct a densely connected graph of paragraphs.\n- **Recurrent Retrieval**: Employs an RNN to sequentially retrieve evidence paragraphs, forming reasoning paths.\n- **Beam Search**: Uses beam search to explore paths in the graph, ensuring computational efficiency.\n\n### Paper 2: Multi-Hop Dense Retrieval for Open-Domain QA\n\nThe second paper presents a simple and efficient multi-hop dense retrieval approach for answering complex open-domain questions. This method leverages dense representations derived from strong pretrained encoders, such as RoBERTa, to cast the question and documents into a vector space. The retrieval process is implemented as a maximum inner product search (MIPS) over the dense representations of the whole corpus. The authors demonstrate that their approach can accurately discover the sequence of documents that provide sufficient information to answer multi-hop questions without the help of corpus-specific hyperlinks or entity markers.\n\nThe model iteratively encodes the question and previously retrieved documents as a query vector and retrieves the next relevant documents using MIPS. The authors show that their method outperforms traditional linking-based retrieval methods on two multi-hop benchmarks, HotpotQA and a multi-evidence subset of FEVER. They also highlight the efficiency of their approach, achieving state-of-the-art performance while being 10 times faster at inference time compared to existing methods.\n\n#### Key Innovations:\n- **Dense Retrieval**: Uses dense representations from pretrained encoders to perform retrieval.\n- **Iterative Retrieval**: Iteratively retrieves documents by reformulating the query based on previous retrieval results.\n- **Efficiency**: Achieves state-of-the-art performance while being significantly faster at inference time.\n\n### Paper 3: Iterative Retriever, Reader, and Reranker for Open-Domain QA\n\nThe third paper introduces Iterative Retriever, Reader, and Reranker (IRRR), a unified system designed to answer open-domain questions that may require a varying number of retrieval steps. The IRRR system employs a single multi-task transformer model to perform all necessary subtasks—retrieving supporting facts, reranking them, and predicting the answer—iteratively. The authors construct a new benchmark, \\beerqa{}, by combining questions from SQuAD Open and HotpotQA with a new collection of 530 human-annotated questions that require information from at least three Wikipedia pages to answer.\n\nThe IRRR system generates natural language search queries from the reasoning path to retrieve paragraphs, then reranks them to compose reasoning paths, and finally predicts the answer from all retrieved documents. The authors demonstrate that their model can achieve competitive performance on standard open-domain QA benchmarks and establish a strong baseline on the new unified benchmark, \\beerqa{}. They also show that the model can generalize to questions requiring more than two hops of reasoning, despite being trained on datasets that primarily contain one- and two-hop questions.\n\n#### Key Innovations:\n- **Unified Benchmark**: Constructs a new benchmark, \\beerqa{}, that features questions requiring variable reasoning steps.\n- **Iterative Process**: Iteratively retrieves, reads, and reranks paragraphs to form reasoning paths.\n- **Dynamic Stopping**: Employs a dynamic stopping criterion based on answerability scores to stop the iterative process.\n\n### Commonalities and Innovations\n\nAll three papers address the challenge of multi-hop open-domain QA by proposing new retrieval methods. However, they differ in their approaches and underlying assumptions:\n- **Graph-Based Retrieval (Paper 1)**: Relies on the Wikipedia graph structure to retrieve evidence paragraphs, which is corpus-specific.\n- **Dense Retrieval (Paper 2)**: Uses dense representations from pretrained models to perform retrieval, which is more generalizable.\n- **Iterative Retrieval (Paper 3)**: Employs an iterative process to retrieve paragraphs, allowing for a varying number of reasoning steps.\n\nThe commonality among these papers is the use of iterative retrieval to handle multi-hop questions. Paper 1 and Paper 3 both utilize a graph structure to model relationships between paragraphs, but Paper 1's graph is constructed using Wikipedia hyperlinks, whereas Paper 3 uses natural language search queries. Paper 2, on the other hand, leverages dense representations and MIPS to perform retrieval, which is more efficient and generalizable.\n\n### Experimental Results and Comparisons\n\nThe authors of each paper evaluate their methods on different datasets, including HotpotQA, SQuAD Open, and Natural Questions Open. Table \\ref{tab:comparison_results} summarizes the performance of the proposed methods on these datasets.\n\n\\begin{table}[h]\n    \\centering\n    \\small\n    \\caption{Comparison of performance across different datasets.}\n    \\begin{tabular}{l|ccc|cc}\n    \\toprule\n    \\textbf{Paper} & \\multicolumn{3}{c}{HotpotQA} & \\multicolumn{2}{c}{SQuAD Open} \\\\\n    & \\textbf{EM} & \\textbf{F1} & \\textbf{SP EM} & \\textbf{EM} & \\textbf{F1} \\\\\n    \\midrule\n    Paper 1 & 60.5 & 73.3 & 49.3 & 56.5 & 63.8 \\\\\n    Paper 2 & 58.61 & 67.1 & 80.9 & 61.8 & 68.9 \\\\\n    Paper 3 & 65.74 & 78.41 & 78.2 & 50.65 & 60.91 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\label{tab:comparison_results}\n\\end{table}\n\nPaper 1 achieves the highest performance on HotpotQA, with an EM score of 60.5 and an F1 score of 73.3. Paper 2, while achieving lower performance on HotpotQA, demonstrates superior efficiency, being 10 times faster at inference time. Paper 3, with its unified approach, achieves competitive performance on both HotpotQA and SQuAD Open, with an EM score of 65.74 and an F1 score of 78.41 on HotpotQA, and an EM score of 50.65 and an F1 score of 60.91 on SQuAD Open.\n\n### Discussion of Results\n\nThe performance differences among the papers can be attributed to several factors:\n- **Graph-Based Retrieval (Paper 1)**: The use of Wikipedia hyperlinks and entity links provides a strong foundation for multi-hop reasoning, but it is corpus-specific and less generalizable to other text corpora.\n- **Dense Retrieval (Paper 2)**: The reliance on dense representations and MIPS allows for efficient and scalable retrieval, but it may struggle with questions that have high lexical overlap with the passages.\n- **Iterative Retrieval (Paper 3)**: The iterative process and dynamic stopping criterion enable the model to handle questions of varying complexity, but it may be less efficient than the dense retrieval approach.\n\nPaper 1's method is particularly effective for complex questions that require multi-hop reasoning, as it explicitly models the relationships between paragraphs using the Wikipedia graph. Paper 2's method, while less accurate on HotpotQA, is more efficient and generalizable, as it does not rely on corpus-specific metadata. Paper 3's IRRR system demonstrates a strong ability to generalize to questions requiring more than two hops of reasoning, despite being trained on datasets that primarily contain one- and two-hop questions.\n\n### Conclusion\n\nThe three papers collectively advance the state-of-the-art in multi-hop open-domain QA by proposing novel retrieval methods that improve upon existing pipeline approaches. Paper 1's graph-based recurrent retrieval method achieves the highest performance on HotpotQA, while Paper 2's dense retrieval method offers significant efficiency improvements. Paper 3's IRRR system introduces a unified approach that can handle questions of varying complexity, establishing a strong baseline on a new benchmark, \\beerqa{}.\n\nFuture research could focus on integrating these methods to leverage their strengths. For instance, combining the graph-based retrieval of Paper 1 with the dense representations of Paper 2 could improve both accuracy and efficiency. Additionally, exploring ways to enhance the dynamic stopping criterion of Paper 3 could further improve its performance on complex questions. The development of more generalizable and efficient retrieval methods remains an active area of research, with potential applications in various domains beyond Wikipedia.\n\n### Acknowledgments\n\nThe authors of these papers acknowledge various grants and funding sources, including ONR, DARPA, NSF, and Samsung GRO, for their research. They also thank numerous collaborators and reviewers for their insightful discussions and feedback."
}