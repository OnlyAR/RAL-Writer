{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{\\emph{Did Aristotle Use a Laptop?}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nA key limitation in current datasets for \\emph{multi-hop reasoning} is that the required steps for answering the question are mentioned in it \\emph{explicitly}.\nIn this work, we introduce \\strategyqa{}, a question answering (QA) benchmark where the required reasoning steps are \\emph{implicit} in the question, and should be inferred using a \\emph{strategy}.\nA fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts.\nMoreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and \n(2) Wikipedia paragraphs that contain the answers to each step.\nOverall, \\strategyqa{} includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs.\nAnalysis shows that questions in \\strategyqa{} are short, topic-diverse, and cover a wide range of strategies.\nEmpirically, we show that humans perform well ($87$\\%) on this task, while our best baseline reaches an accuracy of $\\sim 66\\%$. \n\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:introduction}\n\nDeveloping models that successfully reason over multiple parts of their input has attracted substantial attention recently, leading to the creation of many multi-step reasoning Question Answering (QA) benchmarks\n~\\cite{welbl2018constructing,talmor2018web,khashabi2018looking,yang2018hotpotqa,dua2019drop,suhr2019corpus}.\n\nCommonly, the language of questions in such benchmarks \\emph{explicitly} describes the process for deriving the answer. For instance (Figure~\\ref{figure:intro}, Q2), the question \\nl{Was Aristotle alive when the laptop was invented?} explicitly specifies the required reasoning steps. However, in real-life questions, reasoning is often \\emph{implicit}. For example, the question \\nl{Did Aristotle use a laptop?} (Q1) can be answered using the same steps, but the model must infer the \\emph{strategy} for answering the question -- \\emph{temporal} comparison, in this case.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[scale=0.72,trim=0cm 0.5cm 0cm 0cm]{figures/intro.pdf}\n    \\caption{\n        Questions in \\strategyqa{} (Q1) require \\emph{implicit} decomposition into reasoning steps (D), for which we annotate supporting evidence from Wikipedia (E). This is in contrast to multi-step questions that \\emph{explicitly} specify the reasoning process (Q2). \n    }\n    \\label{figure:intro}\n\\end{figure}\n\nAnswering implicit questions poses several challenges compared to answering their explicit counterparts. \nFirst, retrieving the context is difficult as there is little overlap between the question and its context (Figure~\\ref{figure:intro}, Q1 and `E').  Moreover, questions tend to be short,  lowering the possibility of the model exploiting shortcuts in the language of the question.\nIn this work, we introduce \\strategyqa{}, a boolean QA benchmark focusing on implicit multi-hop reasoning for \\emph{strategy questions}, where a \\emph{strategy} is the ability to infer from a question its atomic sub-questions. \nIn contrast to previous benchmarks \\cite{khot2020qasc, yang2018hotpotqa}, questions in \\strategyqa{} are not limited to predefined decomposition patterns and cover a wide range of strategies that humans apply when answering questions.\n\nEliciting strategy questions using crowdsourcing is non-trivial.\nFirst, authoring such questions requires \\emph{creativity}. Past work often collected multi-hop questions by showing workers an entire context, which led to limited creativity and high lexical overlap between questions and contexts and consequently to reasoning shortcuts \\cite{khot2020qasc, yang2018hotpotqa}. An alternative approach, applied in Natural Questions \\cite{kwiatkowski2019natural} and MS-MARCO \\cite{nguyen2016ms}, overcomes this by collecting real user questions. However, can we elicit creative questions independently of the context and without access to users?\n\nSecond, an important property in \\strategyqa{} is that questions entail \\emph{diverse} strategies. While the example in Figure~\\ref{figure:intro} necessitates temporal reasoning, there are many possible strategies for answering questions (Table~\\ref{table:example_strategy_questions}). We want a benchmark that exposes a broad range of strategies. But crowdsourcing workers often use repetitive patterns, which may limit question diversity. \n\nTo overcome these difficulties, we use the following techniques in our pipeline for eliciting strategy questions: (a) we prime crowd workers with random Wikipedia terms that serve as a minimal context to inspire their imagination and increase their creativity; (b) we use a large set of annotators to increase question diversity, limiting the number of questions a single annotator can write; and (c) we continuously train adversarial models during data collection, slowly increasing the difficulty in question writing and preventing recurring patterns~\\cite{bartolo2020beat}.%\\tushar{I actually wanted to contrast to AFLite-style approaches (also used by ~\\cite{khot2020qasc}). Such apost-hoc pproaches filter out strategies whereas your approach promotes new strategies as models learn to model them better.} \\jb{so maybe the correct ref is adversarial NLI? and I think this is a good point of contrast but maybe should be written in the text later and not intro?}\n\nBeyond the questions, as part of \\strategyqa{}, we annotated: (a) \\emph{question decompositions}: \na sequence of steps sufficient for answering the question\n(`D' in Figure~\\ref{figure:intro}), and (b) \\emph{evidence} paragraphs: Wikipedia paragraphs that contain the answer to \\emph{each} decomposition step (`E' in Figure~\\ref{figure:intro}). \\strategyqa{} is the first QA dataset to provide decompositions and evidence annotations for each individual step of the reasoning process.\n\nOur analysis shows that \\strategyqa{} necessitates reasoning on a wide variety of knowledge domains (physics, geography, etc.) and logical operations (e.g. number comparison). Moreover, experiments show that \\strategyqa{} poses a combined challenge of retrieval and QA, and while humans perform well on these questions, even strong systems struggle to answer them.\n\nIn summary, the contributions of this work are: \n\\begin{enumerate}[noitemsep,nolistsep]\n    \\item Defining \\emph{strategy} questions -- a class of question requiring \\emph{implicit} multi-step reasoning.\n    \\item \\strategyqa{}, the first benchmark for implicit multi-step QA, that covers a diverse set of reasoning skills. \\strategyqa{} consists of 2,780 questions, annotated with their decomposition and per-step evidence.\n    \\item A novel annotation pipeline designed to elicit quality strategy questions, with minimal context for priming workers.\n\\end{enumerate}\nThe dataset and codebase are publicly available at \\url{https://allenai.org/data/strategyqa}.\n\n\\section{Strategy Questions}\n\\label{sec:strategy_questions}\n\n\\subsection{Desiderata}\n\\label{sec:strategy_questions_desiderata}\n\n\\begin{table*}[t]\n    \\centering\n    \\footnotesize\n    {\n    \\begin{tabular}{p{7cm}|p{8cm}}\n         Question & Implicit facts \\\\ \\toprule\n         Can one spot helium? (\\answer{No}) & Helium is a gas, Helium is odorless, Helium is tasteless, Helium has no color \\\\ \\midrule\n         Would Hades and Osiris hypothetically compete for real estate in the Underworld? (\\answer{Yes}) & Hades was the Greek god of death and the Underworld. Osiris was the Egyptian god of the Underworld. \\\\ \\midrule\n        Would a monocle be appropriate for a cyclop? (\\answer{Yes}) & Cyclops have one eye. A monocle helps one eye at a time. \\\\ \\midrule\n        Should a finished website have lorem ipsum paragraphs? (\\answer{No}) & Lorem Ipsum paragraphs are meant to be temporary. Web designers always remove lorem ipsum paragraphs before launch. \\\\ \\midrule\n        Is it normal to find parsley in multiple sections of the grocery store? (\\answer{Yes}) & Parsley is available in both fresh and dry forms. Fresh parsley must be kept cool. Dry parsley is a shelf stable product. \\\\ \\hline\n    \\end{tabular}\n    }\n    \\caption{Example strategy questions and the implicit facts needed for answering them. }\n    \\label{table:example_strategy_questions}\n\\end{table*}\n\n\\begin{table*}[t]\n    \\centering\n    \\footnotesize\n    {\n    \\begin{tabular}{p{4.4cm}|c|c|p{9.0cm}}\n         Question & MS & IM & Explanation\\\\ \\toprule\n         Was Barack Obama born in the United States?  (\\answer{Yes}) & & & The question explicitly states the required information for the answer -- the birth place of Barack Obama. The answer is likely to be found in a single text fragment in Wikipedia. \\\\ \\hline\n         Do cars use drinking water to power their engine? (\\answer{No}) & & & The question explicitly states the required information for the answer -- the liquid used to power car engines. The answer is likely to be found in a single text fragment in Wikipedia.  \\\\ \\hline\n         Are sharks faster than crabs? (\\answer{Yes}) & \\checkmark & & The question explicitly states the required reasoning steps: 1) How fast are sharks? 2) How fast are crabs? 3) Is \\#1 faster than \\#2? \\\\ \\hline\n        Was Tom Cruise married to the female star of Inland Empire? (\\answer{No}) & \\checkmark & & The question explicitly states the required reasoning steps: 1) Who is the female star of Inland Empire? 2) Was Tom Cruise married to \\#2? \\\\  \\hline\n         Are more watermelons grown in Texas than in Antarctica? (\\answer{Yes}) & \\checkmark & \\checkmark & The answer can be derived through geographical/botanical reasoning that the climate in Antarctica does not support growth of watermelons. \\\\ \\hline\n         Would someone with a nosebleed benefit from Coca? (\\answer{Yes}) & \\checkmark & \\checkmark & The answer can be derived through biological reasoning that Coca constricts blood vessels, and therefore, serves to stop bleeding. \\\\ \\hline\n    \\end{tabular}\n    }\n    \\caption{Example questions demonstrating the multi-step (MS) and implicit (IM) properties of strategy questions.}\n    \\label{table:example_question_properties}\n\\end{table*}\n\nWe define strategy questions by characterizing their desired properties. \nSome properties, such as whether the question is answerable, also depend on the context used for answering the question. \nIn this work, we assume this context is a corpus of documents, specifically, Wikipedia, which we assume provides correct content.\n\n\\paragraph{Multi-step} \nStrategy questions are multi-step questions, that is, they comprise a sequence of \\emph{single-step questions}. A single-step question is either (a) a question that can be answered from a short text fragment in the corpus (e.g. steps 1 and 2 in Figure~\\ref{figure:intro}), or (b) a logical operation over answers from previous steps (e.g. step 3 in Figure~\\ref{figure:intro}). A strategy question should have at least two steps for deriving the answer. Example multi- and single- step questions are provided in Table~\\ref{table:example_question_properties}.\nWe define the reasoning process structure in \\S\\ref{sec:strategy_questions_supervision}.\n\n\\paragraph{Feasible}\nQuestions should be answerable from paragraphs in the corpus. Specifically, for each reasoning step in the sequence, there should be sufficient evidence from the corpus to answer the question.\nFor example, the answer to the question \\nl{Would a monocle be appropriate for a cyclop?} can be derived from paragraphs stating that cyclops have one eye and that a monocle is used by one eye at the time. This information is found in our corpus, Wikipedia, and thus the question is feasible.\nIn contrast, the question \\nl{Does Justin Beiber own a Zune?} is \\emph{not} feasible, because answering it requires going through Beiber's belongings, and this information is unlikely to be found in Wikipedia.\n\n\\paragraph{Implicit} A key property distinguishing strategy questions from prior multi-hop questions is their implicit nature.\nIn explicit questions, each step in the reasoning process can be inferred from the \\emph{language} of the question directly. For example, in Figure~\\ref{figure:intro}, the first two questions are explicitly stated, one in the main clause and one in the adverbial clause. Conversely, reasoning steps in strategy questions require going beyond the language of the question. Due to language variability, a precise definition of implicit questions based on lexical overlap is elusive, but a good rule-of-thumb is the following: if the question decomposition can be written with a vocabulary limited to words from the questions, their inflections, and function words, then it is an explicit question. If new content words must be introduced to describe the reasoning process, the question is implicit.\nExamples for implicit and explicit questions are in Table~\\ref{table:example_question_properties}.\n\n\\commentout{\n\\paragraph{Feasible}\nQuestions should be answerable from paragraphs in the corpus. Specifically, there should exist a reasoning process for which there is sufficient evidence from the corpus to answer the question.\nFor example, to answer the question \\nl{Would a monocle be appropriate for a cyclop?} could be derived from paragraphs stating that cyclops have one eye and that a monocle helps one eye at the time. Paragraphs providing this information can be found in Wikipedia, thus, the question is feasible.\nIn contrast, the question \\nl{Does Justin Beiber own a Zune?} is \\emph{not} feasible, because the only way to answer it would be to go through the list of Beiber's belongings, which is unlikely to appear in Wikipedia.\n\n}\n\\paragraph{Definite} \nA type of questions we wish to avoid are \\emph{non-definitive} questions, such as \\nl{Are hamburgers considered a sandwich?} and \\nl{Does chocolate taste better than vanilla?} for which there is no clear answer.\nWe would like to collect questions where the answer is definitive or, at least, very likely, based on the corpus.\nE.g., consider the question \\nl{Does wood conduct electricity?}. Although it is possible that a damp wood will conduct electricity, the answer is generally \\emph{no}.\n\nTo summarize, strategy questions are multi-step questions with implicit reasoning (a strategy) and a definitive answer \nthat can be reached given a corpus.\nWe limit ourselves to Boolean\nyes/no questions,\nwhich limits the output space, but lets us focus on the complexity of the questions, which is the key contribution. \nExample strategy questions are in Table~\\ref{table:example_strategy_questions}, and examples that demonstrate the mentioned properties are in Table~\\ref{table:example_question_properties}.\nNext (\\S\\ref{sec:strategy_questions_supervision}), we describe additional structures annotated during data collection.\n\n\\subsection{Decomposing Strategy Questions}\n\\label{sec:strategy_questions_supervision}\n\nStrategy questions involve complex reasoning that leads to a yes/no answer.\nTo guide and evaluate the QA process, we annotate every example with a description of the expected reasoning process. \n\nPrior work used \\emph{rationales} or \\emph{supporting facts}, i.e., text snippets extracted from the context \\cite{deyoung2020eraser,yang2018hotpotqa, kwiatkowski2019natural,khot2020qasc} as evidence for an answer. \nHowever, reasoning can rely on elements that are not explicitly expressed in the context.\nMoreover, answering a question based on relevant context does not imply that the model performs reasoning properly \\cite{jiang2019avoiding}. \n\nInspired by recent work \\cite{wolfson2020break}, we associate every question-answer pair with a \\emph{strategy question decomposition}.\nA decomposition of a question $q$ is a sequence of $n$ steps $\\langle s^{(1)}, s^{(2)}, ..., s^{(n)} \\rangle$ required for computing the answer to $q$. \nEach step $s^{(i)}$ corresponds to a single-step question and may include special \\emph{references}, which are placeholders referring to the result of a previous step $s^{(j)}$. The last decomposition step (i.e. $s^{(n)}$) returns the final answer to the question. Table~\\ref{table:decomposition_examples} shows decomposition examples. \n\n\\newcite{wolfson2020break} targeted explicit multi-step questions (first row in Table~\\ref{table:decomposition_examples}), where the decomposition is restricted to a small vocabulary derived almost entirely from the original question.\nConversely, decomposing strategy questions requires using implicit knowledge, and thus decompositions can include any token that is needed for describing the implicit reasoning (rows 2-4 in Table~\\ref{table:decomposition_examples}). This makes the decomposition task significantly harder for strategy questions.\n\nIn this work, we distinguish between two types of required actions for executing a step. \\textit{Retrieval}: a step that requires retrieval from the corpus, and  \\textit{operation}, a logical function over answers to previous steps. \nIn the second row of Table~\\ref{table:decomposition_examples}, the first two steps are retrieval steps, and the last step is an operation. \nA decomposition step can require both retrieval and an operation (see last row in Table~\\ref{table:decomposition_examples}).\n\n\\begin{table}[t]\n    \\centering\n    \\footnotesize\n    {\n    \\begin{tabular}{p{1.9cm}|p{4.8cm}}\n        Question & Decomposition \\\\ \\toprule\n         \\multirow{3}{=}{Did the Battle of Peleliu or the Seven Days Battles last longer?} & (1) How long did \\emph{the Battle of Peleliu last}? \\\\\n         & (2) How long did \\emph{the Seven Days Battle last}? \\\\\n         & (3) Which is \\emph{longer} of \\#1 , \\#2? \\\\ \\midrule \\midrule \n         \\multirow{3}{=}{Can the President of Mexico vote in New Mexico primaries?} & (1) What is \\textbf{the citizenship requirement} for \\emph{voting in} \\emph{New Mexico}? \\\\\n         & (2) What is \\textbf{the citizenship requirement} of any \\emph{President of Mexico}? \\\\\n         & (3) Is \\#2 the same as \\#1?  \\\\ \\midrule\n         \\multirow{4}{=}{Can a microwave melt a Toyota Prius battery?} & (1) What \\textbf{kind of} \\emph{battery} does a \\emph{Toyota Prius} use? \\\\\n         & (2) What \\textbf{type of material} is \\#1 made out of? \\\\\n         & (3) What is the \\textbf{melting point} of \\#2?  \\\\\n         & (4) Can a \\emph{microwave's} \\textbf{temperature} reach at least \\#3?    \\\\ \\midrule\n        \\multirow{3}{=}{Would it be common to find a penguin in Miami?} & (1) Where is a typical \\emph{penguin's} \\textbf{natural habitat}? \\\\\n         & (2) What \\textbf{conditions} make \\#1 suitable for \\emph{penguins}? \\\\\n         & (3) Are all of \\#2 present in \\emph{Miami}?\n    \\end{tabular}\n    }\n    \\caption{Explicit (row 1) and strategy (rows 2-4) question decompositions. We mark words that are explicit (italic) or implicit in the input (bold).\n    }\n    \\label{table:decomposition_examples}\n\\end{table}\n\nTo verify that steps are valid single-step questions that can be answered using the corpus (Wikipedia), we collect\n\\textit{supporting evidence} for each retrieval step and annotate operation steps.\nA supporting evidence is one or more paragraphs that provide an answer to the retrieval step. \n\nIn summary, each example in our dataset contains a) a strategy question, b) the strategy question decomposition, and c) supporting evidence per decomposition step. %\\tushar{retrieval?}\nCollecting strategy questions and their annotations is the main challenge of this work, and we turn to this next.\n\n\\section{Data Collection Pipeline}\n\\label{sec:data_collection}\n\nOur goal is to establish a procedure\nfor collecting strategy questions and their annotations at scale. To this end, we build a multi-step crowdsourcing\\footnote{We use Amazon Mechanical Turk as our framework.} pipeline designed for encouraging worker creativity, while preventing biases in the data.\n\nWe break the data collection into three tasks: question writing (\\S\\ref{subsection:cqw}), question decomposition (\\S\\ref{subsection:sqd}), and evidence matching (\\S\\ref{subsection:evm}). In addition, we implement mechanisms for quality assurance (\\S\\ref{subsection:verification}).\nAn overview of the data collection pipeline is in Figure~\\ref{figure:pipeline}. \n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[scale=0.8]{figures/pipeline_example.pdf}\n    \\vspace*{-2mm}\n    \\caption{Overview of the data collection pipeline. First (CQW, \\S\\ref{subsection:cqw}), a worker is presented with a term (T) and an expected answer (A) and writes a question (Q) and the facts (F1,F2) required to answer it. Next, the question is decomposed (SQD, \\S\\ref{subsection:sqd}) into steps (S1, S2) along with Wikipedia page titles (P1,P2) that the worker expects to find the answer in. Last (EVM, \\S\\ref{subsection:evm}), decomposition steps are matched with evidence from Wikipedia (E1, E2).\n    }\n    \\label{figure:pipeline}\n\\end{figure*}\n\n\\subsection{Creative Question Writing (CQW)}\n\\label{subsection:cqw}\n\nGenerating natural language annotations  through crowdsourcing (e.g.,  question generation) is known to suffer from several shortcomings. First, when annotators generate many instances, they use recurring patterns that lead to biases in the data. \\cite{gururangan2018annotation,geva2019modeling}.\nSecond, when language is generated conditioned on a long context, such as a paragraph, annotators use similar language \\cite{kwiatkowski2019natural}, leading to high lexical overlap and hence, inadvertently, to an easier problem.    \nMoreover, a unique property of our setup is that we wish to cover a \\emph{broad and diverse} set of strategies. Thus, we must discourage repeated use of the same strategy. \n\nWe tackle these challenges on multiple fronts. First, rather than using a long paragraph as context,  we prime workers to write questions given single terms from Wikipedia, reducing the overlap with the context to a minimum.\nSecond, to encourage diversity, we control the population of annotators, making sure a large number of annotators contribute to the dataset. Third, we use \\emph{model-in-the-loop} adversarial annotations \\cite{dua2019drop,khot2020qasc,bartolo2020beat} to filter our questions, and only accept questions that fool our models. While some model-in-the-loop approaches use fixed pre-trained models to eliminate ``easy'' questions, we continuously update the models during data collection to combat the use of repeated patterns or strategies.\n\nWe now provide a description of the task, and elaborate on these methods (Figure~\\ref{figure:pipeline}, upper row). \n\n\\paragraph{Task description} Given a term (e.g., \\emph{silk}), a description of the term, and an expected answer (yes or no), the task is to write a strategy question about the term with the expected answer, and the facts required to answer the question.\n\n\\paragraph{Priming with Wikipedia terms}\nWriting strategy questions from scratch is difficult. To inspire worker creativity, we ask to write questions about terms they are familiar with or can easily understand.\nThe terms are titles of ``popular''\\footnote{We filter pages based on the number of contributors and the number of backward links from other pages.} Wikipedia pages.\nWe provide workers only with a short description of the given term.\nThen, workers use their background knowledge and web search skills to form a strategy question.\n\n\\paragraph{Controlling the answer distribution} We ask workers to write questions where the answer is set to be `yes' or `no'. To balance the answer distribution, the expected answer is dynamically sampled inversely proportional to the ratio of `yes' and `no' questions collected until that point.\n\n\\paragraph{Model-in-the-loop filtering}\nTo ensure questions are challenging and reduce recurring language and reasoning patterns, questions are only accepted when verified by two sets of online \\emph{solvers}. We deploy a set of 5 pre-trained models (termed \\ckptzero{}) that check if the question is too easy. If at least 4 out of 5 answer the question correctly, it is rejected. \nSecond, we use a set of 3 models (called \\fntd{}) that are continuously fine-tuned on our collected data and are meant to detect biases in the current question set. \nA question is rejected if all 3 solvers answer it correctly. \nThe solvers are \\textsc{RoBERTa} \\cite{liu2019roberta} models fine-tuned on different auxiliary datasets; details in \\S\\ref{subsec:baselines}.\n\n\\paragraph{Auxiliary sub-task} We ask workers to provide the facts required to answer the question they have written,  \nfor several reasons: 1) it helps workers frame the question writing task and describe the reasoning process they have in mind, 2) it helps reviewing their work, and 3) it provides useful information for the decomposition step (\\S\\ref{subsection:sqd}).\n\n\\subsection{Strategy Question Decomposition (SQD)}\n\\label{subsection:sqd}\n\nOnce a question and the corresponding facts are written, we generate the strategy question decomposition (Figure~\\ref{figure:pipeline}, middle row). \nWe annotate decompositions \\emph{before} matching evidence in order to avoid biases stemming from seeing the context.\n\nThe decomposition strategy for a question is not always obvious, which can lead to undesirable explicit decompositions. For example, a possible explicit decomposition for Q1 (Figure~\\ref{figure:intro}) might be (1) \\textit{What items did Aristotle use?} (2) \\textit{Is laptop in \\#1?}; but the first step is not feasible.\nTo guide the decomposition, we provide workers with the facts written in the CQW task to show the strategy of the question author. Evidently, there can be many valid strategies and the same strategy can be phrased in multiple ways -- the facts only serve as a soft guidance. \n\n\\paragraph{Task description} Given a strategy question, a yes/no answer, and a set of facts, the task is to write the steps needed to answer the question.\n\n\\paragraph{Auxiliary sub-task} \nWe observe that in some cases, annotators write explicit decompositions, which often lead to infeasible steps that cannot be answered from the corpus. \nTo help workers avoid explicit decompositions, we ask them to specify, for each decomposition step, a Wikipedia page they expect to find the answer in. This encourages workers to write decomposition steps for which it is possible to find answers in Wikipedia, and leads to feasible strategy decompositions, with only a small overhead (the workers are not required to read the proposed Wikipedia page).\n\n\\subsection{Evidence Matching (EVM)}\n\\label{subsection:evm}\nWe now have a question and its decomposition. To ground them in context, we add a third task of evidence matching (Figure~\\ref{figure:pipeline}, bottom row).\n\n\\paragraph{Task description} Given a question and its decomposition (a list of single-step questions), the task is to find evidence paragraphs on Wikipedia for each retrieval step. Operation steps that do not require retrieval (\\S\\ref{sec:strategy_questions_supervision}) are marked as  \\emph{operation}.\n\n\\paragraph{Controlling the matched context} Workers search for evidence on Wikipedia. \nWe index Wikipedia\\footnote{We use the Wikipedia Cirrus dump from 11/05/2020. \n} and provide a search interface where workers can drag-and-drop paragraphs from the results shown on the search interface. \nThis guarantees that annotators choose paragraphs we included in our index, at a pre-determined paragraph-level granularity. \n\n\\subsection{Data Verification Mechanisms}\n\\label{subsection:verification}\n\n\\paragraph{Task qualifications} For each task, we hold qualifications that test understanding of the task, and manually review several examples. Workers who follow the requirements are granted access to our tasks. Our qualifications are open to workers from English speaking countries who have high reputation scores. %\\footnote{The total number and percentage of submitted assignments that have been and approved.}\nAdditionally, the authors regularly review annotations to give feedback and prevent noisy annotations.\n\n\\paragraph{Real-time automatic checks} \nFor CQW, we use heuristics to check question validity, e.g., whether it ends with a question mark, and that it doesn't use language that characterizes explicit multi-hop questions (for instance, having multiple verbs). \nFor SQD, we check that the decomposition structure forms a directed acyclic graph, i.e. (i) each decomposition step is referenced by (at least) one of the following steps, such that all steps are reachable from the last step; and (ii) steps don't form a cycle. \nIn the EVM task, a warning message is shown when the worker marks an intermediate step as an operation (an unlikely scenario).\n\n\\paragraph{Inter-task feedback} At each step of the pipeline, we collect feedback about previous steps.\nTo verify results from the CQW task, we ask workers to indicate whether the given answer is incorrect (in the SQD, EVM tasks), or if the question is not definitive (in the SQD task) (\\S\\ref{sec:strategy_questions_desiderata}).\nSimilarly, to identify non-feasible questions or decompositions, we ask workers to indicate if there is no evidence for a decomposition step (in the EVM task).\n\n\\paragraph{Evidence verification task} \nAfter the EVM step, each example comprises a question, its answer, decomposition and supporting evidence.\nTo verify that a question can be answered by executing the decomposition steps against the matched evidence paragraphs, we construct an additional evidence verification task (EVV). In this task, workers are given a question, its decomposition and matched paragraphs, and are asked to answer the question in each decomposition step purely based on the provided paragraphs. Running EVV on a subset of examples during data collection, helps identify issues in the pipeline and in worker performance. \n\n\\section{The \\strategyqa{} Dataset}\n\\label{sec:strategyqa}\n\n\\begin{table}[t]\n    \\centering\n    \\footnotesize\n    {\n    \\begin{tabular}{p{4.6cm}|c|c}\n         & Train & Test \\\\ \\toprule\n        \\# of questions  & 2290 & 490 \\\\\n        \\% ``yes'' questions & 46.8\\% & 46.1\\% \\\\  \n        \\# of unique terms  & 1333 & 442 \\\\\n        \\# of unique decomposition steps  & 6050 & 1347 \\\\ \n        \\# of unique evidence paragraphs  & 9251 & 2136 \\\\\n        \\# of occurrences of the top trigram   & 31 & 5 \\\\ \\hline\n        \\# of question writers  &  23 & 6 \\\\\n        \\# of filtered questions  & 2821 & 484 \\\\ \\hline\n        Avg. question length (words)  & 9.6 & 9.8 \\\\\n        Avg. decomposition length (steps) & 2.93 & 2.92 \\\\\n        Avg. \\# of paragraphs per question  & 2.33 & 2.29 \\\\\n    \\end{tabular}\n    }\n    \\caption{\\strategyqa{} statistics. Filtered questions were rejected by the solvers \n    (\\S\\ref{subsection:cqw}). The train and test sets of question writers are disjoint. \n    The ``top trigram'' is the most common trigram.\n    }\n    \\label{table:dataset_statistics}\n\\end{table}\n\nWe run our pipeline on 1,799 Wikipedia terms, allowing a maximum of 5 questions per term. We update our online fine-tuned solvers (\\textsc{Fntd}) every 1K questions. Every question is decomposed once, and evidence is matched for each decomposition by 3 different workers. The cost of annotating a full example is \\$4.\n\nTo encourage diversity in strategies used in the questions, we recruited new workers throughout data collection. \nMoreover, periodic updates of the online solvers prevent workers from exploiting shortcuts, since the solvers adapt to the training distribution.\nOverall, there were 29 question writers, 19 decomposers, and 54 evidence matchers participating in the data collection.\n\nWe collected 2,835 questions, out of which 55 were marked as having an incorrect answer during SQD (\\S\\ref{subsection:sqd}).\nThis results in a collection of 2,780 verified strategy questions, for which we create an annotator-based data split \\cite{geva2019modeling}. \nWe now describe the dataset statistics (\\S\\ref{subsec:dataset_statistics}), analyze the quality of the examples, (\\S\\ref{subsec:data_quality}) and explore the reasoning skills in \\strategyqa{} (\\S\\ref{subsec:data_diversity}).\n\n\\subsection{Dataset Statistics}\n\\label{subsec:dataset_statistics}\n\nWe observe (Table~\\ref{table:dataset_statistics}) that the answer distribution is roughly balanced (yes/no). Moreover, questions are short ($<10$ words), and the most common trigram occurs in roughly $1\\%$ of the examples. This indicates that the language of the questions is both simple and diverse. For comparison, the average question length in the multi-hop datasets \\textsc{HotpotQA} \\cite{yang2018hotpotqa} and \\textsc{ComplexWebQuestions} \\cite{talmor2018web} is $13.7$ words and $15.8$ words, respectively. Likewise, the top trigram in these datasets occurs in 9.2\\% and 4.8\\% of their examples, respectively.\n\nMore than half of the generated questions are filtered by our solvers, pointing to the difficulty of generating good strategy questions. We release all 3,305 filtered questions as well.\n\nTo characterize the \\emph{reasoning complexity} required to answer questions in \\strategyqa{}, we examine the decomposition length and the number of evidence paragraphs. Figure~\\ref{figure:reasoning_process_complexity} and Table~\\ref{table:dataset_statistics} (bottom) show the distributions of these properties\nare centered around 3-step decompositions and 2 evidence paragraphs, but a considerable\nportion of the dataset requires more steps and paragraphs.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[scale=0.42, trim=0cm 0cm 0cm 0cm]{figures/reasoning_process_complexity.pdf}\n    \\vspace*{-2mm}\n    \\caption{\n        The distributions of decomposition length (left) and the number of evidence paragraphs (right).\n        The majority of the questions in \\strategyqa{} require a reasoning process comprised of $\\geq 3$ steps, of which about 2 steps involve retrieving external knowledge.\n    }\n    \\label{figure:reasoning_process_complexity}\n\\end{figure}\n\n\\begin{table}[t]\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{l|c|c|c}\n        & multi-step & single-step &  \\\\ \\hline\n      implicit  & 81 & 1 & 82 \\\\ \\hline\n      explicit  & 14.5 & 3.5 & 18 \\\\ \\hline\n       & 95.5 & 4.5 & 100\n    \\end{tabular}\n    \\caption{Distribution over the implicit and multi-step properties (\\S\\ref{sec:strategy_questions}) in a sample of 100 \\strategyqa{} questions, annotated by two experts (we average the expert decisions).\n    Most questions are \\emph{multi-step} and \\emph{implicit}. Annotator agreement is substantial for both the implicit ($\\kappa = 0.73$) and multi-step ($\\kappa = 0.65$) properties. \n    }\n    \\label{table:quality_stats}\n\\end{table}\n\n\\subsection{Data Quality}\n\\label{subsec:data_quality}\n\n\\paragraph{Do questions in \\strategyqa{} require multi-step implicit reasoning?}\nTo assess the quality of questions, we sampled 100 random examples from the training set, and had two experts (authors) independently annotate whether the questions satisfy the desired properties of strategy questions (\\S\\ref{sec:strategy_questions_desiderata}).\nWe find that most of the examples (81\\%) are valid multi-step implicit questions, 82\\% of questions are implicit, and 95.5\\% are multi-step (Table~\\ref{table:quality_stats}).\n\n\\paragraph{Do questions in \\strategyqa{} have a definitive answer?}\nWe let experts review the answers to 100 random questions, allowing access to the Web. We then ask them to state for every question whether they agree or disagree with the provided answer. We find that the experts agree with the answer in 94\\% of the cases, and disagree only in 2\\%. For the remaining 4\\%, either the question was ambiguous, or the annotators could not find a definite answer on the Web. Overall,\nthis suggests that questions in \\strategyqa{} have clear answers.\n\n\\paragraph{What is the quality of the decompositions?}\nWe randomly sampled 100 decompositions and asked experts to judge their quality.\nExperts judged if the decomposition is explicit or utilizes a strategy. We find that 83\\% of the decompositions validly use a strategy to break down the question. The remaining 17\\% decompositions are explicit, however, in 14\\% of the cases the original question is already explicit. Second, experts checked if the phrasing of the decomposition is ``natural'', i.e., it reflects the decomposition of a person that does not already know the answer. We find that 89\\% of the decompositions express a ``natural'' reasoning process, while 11\\% may depend on the answer. Last, we asked experts to indicate any potential logical flaws in the decomposition, but no such cases occurred in the sample.\n\n\\commentout{\nA decomposition should express an implicit yet natural reasoning process, one would employ to derive an answer to the question.\nTo verify the correctness and coherence of decompositions in \\strategyqa{}, we sample 100 decompositions, and ask experts (authors) to label every decomposition with two labels: a) strategic/explicit for whether the decomposition uses a strategy or is an explicit breakdown of the question, b) natural/artificial for whether the decomposition sounds like a natural breakdown of the question \\jb{this sounds highly subjective...}. In addition, we ask experts to indicate any potential logical flaws in the decomposition.\n\nWe find that 83\\% of the decompositions validly use a strategy to break down the question. The rest 17\\% decompositions are explicit, however, in 14\\% of the cases the question is by itself explicit. Moreover, 89\\% of the decompositions express a natural reasoning process, while 11\\% assume additional knowledge which makes them sound artificial. Lastly, there were no decompositions with logical flaws, that for example, could lead to the opposite answer.\n}\n\n\\paragraph{Would different annotators use the same decomposition strategy?}\nWe sample 50 examples, and let two different workers decompose the questions. \nComparing the decomposition pairs, we find that a) for all pairs, the last step returns the same answer, b) in 44 out of 50 pairs, the decomposition pairs follow the same reasoning path , and c) in the other 6 pairs, the decompositions either follow a different reasoning process (5 pairs) or one of the decompositions is explicit (1 pair). \nThis shows that different workers usually use the same strategy when decomposing questions. \n\n\\paragraph{Is the evidence for strategy questions in Wikipedia?}\nAnother important property is whether questions in \\strategyqa{} can be answered based on context from our corpus, Wikipedia, given that questions are written independently of the context. \nTo measure evidence coverage, in the EVM task (\\S\\ref{subsection:evm}), we provide workers with a checkbox for every decomposition step, indicating whether only partial or no evidence could be found for that step. Recall that three different workers match evidence for each decomposition step.\nWe find that 88.3\\% of the questions are fully covered: evidence was matched for each step by some worker.\nMoreover, in 86.9\\% of the questions, at least one worker found evidence  for \\emph{all} steps. Last, in only 0.5\\% of the examples, all three annotators could not match evidence for \\emph{any} of the steps. \nThis suggests that overall, Wikipedia is a good corpus for questions in \\strategyqa{}, that were written independently of the context.\n\n\\paragraph{Do matched paragraphs provide evidence?}\nWe assess the quality of matched paragraphs by analyzing both example-level and step-level annotations.\nFirst, we sample 217 decomposition steps with their corresponding paragraphs matched by one of the three workers. We let 3 different crowdworkers decide whether the paragraphs provide evidence for the answer to that step. We find that in 93\\% of the cases, the majority vote is that the evidence is valid.\\footnote{With moderate annotator agreement of $\\kappa = 0.42$.}\n\nNext, we analyze annotations of the verification task (\\S\\ref{subsection:verification}), where workers are asked to answer all decomposition steps based only on the matched paragraphs. We find that the workers could answer sub-questions and derive the correct answer in 82 out of 100 annotations. Moreover, in 6 questions indeed there was an error in evidence matching, but another worker that annotated the example was able to compensate for the error, leading to 88\\% of the questions where evidence matching succeeds. In the last 12 cases indeed evidence is missing, and is possibly absent from Wikipedia.\n\nLastly, we let experts review the paragraphs matched by one of the three workers to all the decomposition steps of a question, for 100 random questions. We find that for 79 of the questions the matched paragraphs provide sufficient evidence for answering the question. \nFor 12 of the 21 questions without sufficient evidence, the experts indicated they would expect to find evidence in Wikipedia, and the worker probably could not find it. For the remaining 9 questions, they estimated that evidence is probably absent from Wikipedia.\n\nIn conclusion, 93\\% of the paragraphs matched at the step-level were found to be valid. Moreover, when considering single-worker annotations, $\\sim$80\\% of the questions are matched with paragraphs that provide sufficient evidence for all retrieval steps. This number increases to 88\\% when aggregating the annotations of three workers.\n\n\\paragraph{Do different annotators match the same evidence paragraphs?}\nTo compare the evidence paragraphs matched by different workers, we check whether for a given decomposition step, the same paragraph IDs are retrieved by different annotators.\nGiven two non-empty sets of paragraph IDs $\\mathcal{P}_1, \\mathcal{P}_2$, annotated by two workers, we compute the Jaccard coefficient $J(\\mathcal{P}_1, \\mathcal{P}_2) = \\frac{|\\mathcal{P}_1 \\cap \\mathcal{P}_2|}{|\\mathcal{P}_1 \\cup \\mathcal{P}_2|}$. In addition, we take the sets of corresponding Wikipedia page IDs $\\mathcal{T}_1, \\mathcal{T}_2$ for the matched paragraphs, and compute $J(\\mathcal{T}_1, \\mathcal{T}_2)$. \nNote that a score of 1 is given to two identical sets, while a score of 0 corresponds to sets that are disjoint.\nThe average similarity score is 0.43 for paragraphs and 0.69 for pages. This suggests that evidence for a decomposition step can be found in more than one paragraph in the same page, or in different pages.\n\n\\begin{table}[t]\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{lp{4.3cm}c}\n      Strategy & Example & \\% \\\\ \\toprule\n      Physical & \\nl{Can human nails carve a statue out of quartz?} & 13 \\\\ \\hline\n      Biological & \\nl{Is a platypus immune from cholera?} & 11 \\\\ \\hline\n      Historical & \\nl{Were mollusks an ingredient in the color purple?} & 10 \\\\ \\hline\n      Temporal & \\nl{Did the 40th president of the United States forward lolcats to his friends?} & 10 \\\\ \\hline\n      Definition & \\nl{Are quadrupeds represented on Chinese calendar?} & 8 \\\\ \\hline\n      Cultural & \\nl{Would a compass attuned to Earth's magnetic field be a bad gift for a Christmas elf?} & 5 \\\\ \\hline\n      Religious & \\nl{Was Hillary Clinton's deputy chief of staff in 2009 baptised?} & 5 \\\\ \\hline\n      Entertainment & \\nl{Would Garfield enjoy a trip to Italy?} & 4 \\\\ \\hline\n      Sports & \\nl{Can Larry King's ex-wives form a water polo team?} & 4 \\\\ \\hline\n    \\end{tabular}\n    \\caption{Top strategies in \\strategyqa{} and their frequency in a 100 example subset (accounting for 70\\% of the analyzed examples).}\n    \\label{table:top_strategies}\n\\end{table}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[scale=0.33, trim=0cm 1cm 0cm 1cm]{figures/skill_bubbles.pdf}\n    \\caption{Reasoning skills in \\strategyqa{}; each skill is associated with the proportion of examples it is required for. Domain-related and logical reasoning skills are marked in blue and orange (italic), respectively.}\n    \\label{figure:reasoning_skills}\n\\end{figure}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[scale=0.4, trim=0cm 1cm 0cm 0cm]{figures/term_category_distribution.pdf}\n    \\caption{The top 15 categories of terms used to prime workers for question writing and their proportion.}\n    \\label{figure:term_categories}\n\\end{figure}\n\n\\subsection{Data Diversity}\n\\label{subsec:data_diversity}\nWe aim to generate creative and diverse questions. We now analyze diversity in terms of the required reasoning skills and question topic.\n\n\\paragraph{Reasoning skills}\nTo explore the required reasoning skills in \\strategyqa{}, we sampled 100 examples and let two experts (authors) discuss and annotate each example with a) the type of strategy for decomposing the question, and b) the required reasoning and knowledge skills per decomposition step. We then aggregate similar labels (e.g. botanical $\\rightarrow$ biological) and compute the proportion of examples each strategy/reasoning skill is required for (an example can have multiple strategy labels).\n\nTable~\\ref{table:top_strategies} demonstrates the top strategies, showing that \\strategyqa{} contains a broad set of strategies.\nMoreover,  diversity is apparent (Figure~\\ref{figure:reasoning_skills}) in terms of both domain-related reasoning (e.g. biological and technological) and logical functions (e.g. set inclusion and ``is member of''). \nWhile the reasoning skills sampled from questions in \\strategyqa{} do not necessarily reflect their prevalence in a ``natural'' distribution, we argue that promoting research on methods for inferring strategies is an important research direction.\n\n\\paragraph{Question topics}\nAs questions in \\strategyqa{} were triggered by Wikipedia terms, we use the ``instance of'' Wikipedia property to characterize the topics of questions.\\footnote{It is usually a 1-to-1 mapping from a term to a Wikipedia category. In cases of 1-to-many, we take the first category. \n} \nFigure~\\ref{figure:term_categories} shows the distribution of topic categories in \\strategyqa{}. The distribution shows \\strategyqa{} is very diverse, with the top two categories (``human'' and ``taxon'', i.e. a group of organisms) covering only a quarter of the data, and a total of 609 topic categories.\n\nWe further compare the diversity of \\strategyqa{} to \\textsc{HotpotQA}, a multi-hop QA dataset over Wikipedia paragraphs. To this end, we sample 739 pairs of evidence paragraphs associated with a single question in both datasets, and map the pair of paragraphs to a pair of Wikipedia categories using the ``instance of'' property.\nWe find that there are 571 unique category pairs in \\strategyqa{}, but only 356 unique category pairs in \\textsc{HotpotQA}.\nMoreover, the top two category pairs in both of the datasets (``human-human'', ``taxon-taxon'') constitute 8\\% and 27\\% of the cases in \\strategyqa{} and \\textsc{HotpotQA}, respectively. This demonstrates the creativity and breadth of category combinations in \\strategyqa{}.\n\n\\subsection{Human Performance}\n\\label{subsec:human_performance}\n\nTo see how well humans answer strategy questions, \nwe sample a subset of 100 questions from \\strategyqa{} and have experts (authors) answer questions, given access to Wikipedia articles and an option to reveal the decomposition for every question. In addition, we ask them to provide a short explanation for the answer, the number of searches they conducted to derive the answer, and to indicate whether they have used the decomposition. \nWe expect humans to excel at coming up with strategies for answering questions. Yet, humans are not necessarily an upper bound because finding the relevant paragraphs is difficult and could potentially be performed better by machines.\n\nTable~\\ref{table:human_performance} summarizes the results. Overall, humans infer the required strategy and answer the questions with high accuracy. \nMoreover, the low number of searches shows that humans leverage background knowledge, as they can answer some of the intermediate steps without search.  \nAn error analysis shows that the main reason for failure (10\\%) is difficulty to find evidence, and the rest of the cases (3\\%) are due to ambiguity in the question that could lead to the opposite answer.\n\n\\begin{table}\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{l|c}\n         Answer accuracy & 87\\%  \\\\\n         Strategy match & 86\\% \\\\\n         Decomposition usage & 14\\% \\\\\n         Average \\# searches & 1.25\n    \\end{tabular}\n    \\caption{Human performance in answering questions. Strategy match is computed by comparing the explanation provided by the expert with the decomposition. Decomposition usage and the number of searches are computed based on information provided by the expert.}\n    \\label{table:human_performance}\n\\end{table}\n\n\\section{Experimental Evaluation}\n\\label{sec:experiments}\n\nIn this section, we conduct experiments to answer the following questions: a) How well do pre-trained language models (LMs) answer strategy questions? b) Is retrieval of relevant context helpful? and c) Are decompositions useful for answering questions that require implicit knowledge?\n\n\\subsection{Baseline Models}\n\\label{subsec:baselines}\n\nAnswering strategy questions requires external knowledge that cannot be obtained by training on \\strategyqa{} alone. Therefore, our models and online solvers (\\S\\ref{subsection:cqw}) are based on pre-trained LMs, fine-tuned on auxiliary datasets that require reasoning. Specifically, in all models we fine-tune \\textsc{RoBERTa} \\cite{liu2019roberta} on a subset of:\n\\begin{itemize}[leftmargin=*,topsep=5pt,itemsep=0pt,parsep=0pt]\n    \\item \\textsc{BoolQ} \\cite{clark2019boolq}: A dataset for boolean question answering.\n    \\item \\textsc{MNLI} \\cite{williams2018broad}: A large natural language inference (NLI) dataset. The task is to predict if a textual premise entails, contradicts or is neutral with respect to the hypothesis.\n    \\item \\textsc{Twenty Questions} (\\textsc{20Q}): A collection of 50K short commonsense boolean questions.\\footnote{\\url{https://github.com/allenai/twentyquestions}}\n    \\item \\textsc{DROP} \\cite{dua2019drop}: A large dataset for numerical reasoning over paragraphs.\n\\end{itemize}\n\nModels are trained in two configurations:\n\\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt,parsep=0pt]\n    \\item \\textbf{No context} \n    : The model is fed with the question only, and outputs a binary prediction using the special \\texttt{CLS} token.\n\n    \\item \\textbf{With context} \n    : We use \\textsc{BM25} \\cite{robertson1995okapi} to retrieve context from our corpus, while removing stop words from all queries. We examine two retrieval methods: a) question-based retrieval: by using the question as a query and taking the top $k=10$ results, and b) decomposition-based retrieval: by initiating a separate query for each (gold or predicted) decomposition step and concatenating the top $k=10$ results of all steps (sorted by retrieval score).\n    In both cases, the model is fed with the question concatenated to the retrieved context, truncated to\n    512 tokens (the maximum input length of \\textsc{RoBERTa}), and outputs a binary prediction.\n\\end{itemize}\n\n\\paragraph{Predicting decompositions}\nWe train a seq-to-seq model, termed  \\textsc{BART$_{\\text{decomp}}$}, that given a question, generates its decomposition token-by-token. Specifically, we fine-tune \\textsc{BART} \\cite{lewis2020bart} on \\strategyqa{} decompositions.\n\n\\paragraph{Baseline models}\nAs our base model, we train a model as follows: \nWe take a \\textsc{RoBERTa} \\cite{liu2019roberta} model and fine-tune it on \\textsc{DROP}, \\textsc{20Q} and \\textsc{BoolQ} (in this order). The model is trained on \\textsc{DROP} with multiple output heads, as in \\citet{segal2020simple}, which are then replaced with a single Boolean output.\\footnote{For brevity, exact details on model training and hyper-parameters will be released as part of our codebase.} We call this model \\textsc{RoBERTa*}.\n\nWe use \\textsc{RoBERTa*} and \\textsc{RoBERTa} to train the following models on \\strategyqa{}: without context (\\textsc{RoBERTa*$_\\varnothing$}), with question-based retrieval (\\textsc{RoBERTa*$_\\text{IR-Q}$}, \\textsc{RoBERTa$_\\text{IR-Q}$}), and with predicted decomposition-based retrieval (\\textsc{RoBERTa*$_\\text{IR-D}$}). \n\nWe also present four oracle models: \n\\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0pt,parsep=0pt]\n    \n    \\item \\textsc{RoBERTa*$_\\text{ORA-P}$}: uses the gold paragraphs (no retrieval).\n    \n    \\item \\textsc{RoBERTa*$_\\text{IR-ORA-D}$}: performs retrieval with the gold decomposition.\n    \n    \\item \\textsc{RoBERTa*}$^{\\text{last-step}}_\\text{ORA-P-D}$: exploits both the gold decomposition and the gold paragraphs. We fine-tune \\textsc{RoBERTa} on \\textsc{BoolQ} and \\textsc{SQuAD} \\cite{rajpurkar2016squad} to obtain a model that can answer single-step questions. We then run this model on \\strategyqa{} to obtain answers for all decomposition sub-questions, and replace all placeholder references with the predicted answers. Last, we fine-tune \\textsc{RoBERTa*} to answer the last decomposition step of \\strategyqa{}, for which we have supervision. \n    \n    \\item \\textsc{RoBERTa*}$^{\\text{last-step-raw}}_\\text{ORA-P-D}$: \\textsc{RoBERTa*} that is fine-tuned to predict the answer from the gold paragraphs and the last step of the gold decomposition, \\emph{without} replacing placeholder references.\n\n\\end{itemize} \n\n\\paragraph{Online solvers}\nFor the solvers integrated in the data collection process (\\S\\ref{subsection:cqw}), we use three no-context models and two question-based retrieval models. The solvers are listed in Table~\\ref{table:baseline_models}.\n\n\\begin{table}[t]\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{p{4.4cm}c}\n      Model  & Solver group(s) \\\\ \\hline\n      \\textsc{RoBERTa$_\\varnothing$(20Q)}  &  \\ckptzero{}, \\fntd{} \\\\\n      \\textsc{RoBERTa$_\\varnothing$(20Q+BoolQ)}  & \\ckptzero{}, \\fntd{} \\\\\n      \\textsc{RoBERTa$_\\varnothing$(BoolQ)}  &  \\ckptzero{}, \\fntd{} \\\\\n      \\textsc{RoBERTa$_{\\text{IR-Q}}$(BoolQ)}  & \\ckptzero{} \\\\\n      \\textsc{RoBERTa$_{\\text{IR-Q}}$(MNLI+BoolQ)}  & \\ckptzero{} \n    \\end{tabular}\n    \\caption{QA models used as online solvers during data collection (\\S\\ref{subsection:cqw}).\n    Each model was fine-tuned on the datasets mentioned in its name.\n    }\n    \\label{table:baseline_models}\n\\end{table}\n\n\\subsection{Results}\n\\label{subsection:experiment_results}\n\n\\begin{table}[t]\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{l|cc}\n      Model & Accuracy & Recall@10 \\\\ \\toprule\n      \\textsc{Majority} & 53.9 & - \\\\\n      \\textsc{RoBERTa*$_\\varnothing$} & 63.6 $\\pm$ 1.3 & - \\\\\n      \\textsc{RoBERTa$_{\\text{IR-Q}}$} & 53.6 $\\pm$ 1.0 & 0.174 \\\\\n      \\textsc{RoBERTa*$_{\\text{IR-Q}}$} & 63.6 $\\pm$ 1.0 & 0.174  \\\\\n      \\textsc{RoBERTa*$_{\\text{IR-D}}$} & 61.7 $\\pm$ 2.2 & 0.195 \\\\  \\hline \\hline\n      \\textsc{RoBERTa*$_{\\text{IR-ORA-D}}$} & 62.0 $\\pm$ 1.3 & 0.282 \\\\\n      \\textsc{RoBERTa*$_{\\text{ORA-P}}$} & 70.7 $\\pm$ 0.6 & - \\\\\n       \\textsc{RoBERTa*}$^{\\text{last-step-raw}}_\\text{ORA-P-D}$ & 65.2 $\\pm$ 1.4 & -\\\\\n       \\textsc{RoBERTa*}$^{\\text{last-step}}_\\text{ORA-P-D}$ & 72.0 $\\pm$ 1.0 & -\\\\\n    \\end{tabular}\n    \\caption{QA accuracy (with standard deviation across 7 experiments), and retrieval performance, measured by Recall@10, of baseline models on the test set.}\n    \\label{table:baseline_results}\n\\end{table}\n\n\\paragraph{Strategy QA performance}\nTable~\\ref{table:baseline_results} summarizes the results of all models (\\S\\ref{subsec:baselines}). \\textsc{RoBERTa*$_{\\text{IR-Q}}$} substantially outperforms \\textsc{RoBERTa$_{\\text{IR-Q}}$}, indicating that fine-tuning on related auxiliary datasets before \\strategyqa{} is crucial. Hence, we focus on \\textsc{RoBERTa*} for all other results and analysis.\n\nStrategy questions pose a combined challenge of retrieving the relevant context, and deriving the answer based on that context. Training without context shows a large accuracy gain of $53.9 \\rightarrow 63.6$ over the majority baseline. This is far from human performance, but shows that some questions can be answered by a large LM fine-tuned on related datasets without retrieval.\nOn the other end, training with \\emph{gold} paragraphs raises performance to $70.7$. This shows that high-quality retrieval lets the model effectively reason over the given paragraphs. Last, using both gold decompositions and retrieval further increases performance to $72.0$, showing the utility of decompositions. \n\nFocusing on retrieval-based methods, we observe that question-based retrieval reaches an accuracy of $63.6$ and retrieval with gold decompositions results in an accuracy of $62.0$. This shows that the quality of retrieval even with gold decompositions is not high enough to improve the $63.6$ accuracy obtained by \\textsc{RoBERTA*$_\\varnothing$}, a model that uses no context. Retrieval with predicted decompositions results in an even lower accuracy of $61.7$. We also analyze predicted decompositions below.\n\n\\paragraph{Retrieval evaluation}\nA question decomposition describes the reasoning steps for answering the question. Therefore, using the decomposition for retrieval may help obtain the relevant context and improve performance. \nTo test this, we directly compare performance of question- and decomposition-based retrieval with respect to the annotated gold paragraphs. \nWe compute Recall@10, i.e., the fraction of the gold paragraphs retrieved in the top-10 results of each method. Since there are 3 annotations per question, we compute Recall@10 for each annotation and take the maximum as the final score. For a fair comparison, in decomposition-based retrieval, we use the top-10 results across \\emph{all} steps.\n\nResults (Table~\\ref{table:baseline_results}) show that retrieval performance is low, partially explaining why retrieval models do not improve performance compared to \\textsc{RoBERTa*$_\\varnothing$},\nand demonstrating the retrieval challenge in our setup. Gold decomposition-based retrieval substantially outperforms question-based retrieval, showing that using the decomposition for retrieval is a promising direction for answering multi-step questions. Still, predicted decomposition-based retrieval does not improve retrieval compared to question-based retrieval, showing better decomposition models are needed.\n\nTo understand the low retrieval scores, we analyzed the query results of 50 random decomposition steps. Most failure cases are due to the shallow pattern matching done by BM25, e.g., failure to match synonyms. This shows that indeed there is little word overlap between decomposition steps and the evidence, as intended by our pipeline design. In other examples, either a key question entity was missing because it was represented by a reference token, or the decomposition step had complex language, leading to failed retrieval.\nThis analysis suggests that \nadvances in neural retrieval might be beneficial for \\strategyqa{}.\n\n\\paragraph{Human retrieval performance}\nTo quantify human performance in finding gold paragraphs, we ask experts to find evidence paragraphs for 100 random questions. For half of the questions we also provide decomposition. \nWe observe average Recall@10 of $0.586$ and $0.513$ with and without the decomposition, respectively. This shows that humans significantly outperform our IR baselines. However, humans are still far from covering the gold paragraphs, since there are multiple valid evidence paragraphs (\\S\\ref{subsec:data_quality}), and retrieval can be difficult even for humans.\nLastly, using decompositions improves human retrieval, showing decompositions indeed are useful for finding evidence.\n\n\\paragraph{Predicted decompositions}\nAnalysis shows that \\textsc{BART$_{\\text{DECOMP}}$}'s decompositions are grammatical and well-structured. Interestingly, the model generates strategies, but often applies them to questions incorrectly.\nE.g., the question \\nl{Can a lifeboat rescue people in the Hooke Sea?} is decomposed to \\nl{1) What is the maximum depth of the Hooke Sea? 2) How deep can a lifeboat dive? 3) Is \\#2 greater than or equal to \\#1?}. While the decomposition is  well-structured, it uses a wrong strategy (lifeboats do not dive).\n\n\\section{Related Work}\n\\label{sec:related_work}\n\n\\begin{comment}\n\\begin{itemize}\n    \\item Multi-step reasoning datasets (DROP, HotpotQA, CWQ, MultiQA, etc.)\n    \\item OpenBookQA and QASC - implicit reasoning, but limited to a specific domain (science) and only a single composition type.\n    \\item Datasets for common sense reasoning (CSQA, SWAG, etc.)\n    \\item Datasets for open-domain/natural questions (Natural Questions, BoolQ, WebQuestions, TriviaQA) - were not built for testing multi-step reasoning and therefore the questions either do not require reasoning or they can be decomposed based on the language of the question only. \n    \\item Similar priming method? tydi-qa\n\\end{itemize}\n\\end{comment}\n\nPrior work has typically let annotators write questions based on an entire context \\cite{khot2020qasc, yang2018hotpotqa, dua2019drop, mihaylovetal2018, khashabi2018looking}. \nIn this work, we prime annotators with minimal information (few tokens) and let them use their imagination and own wording to create questions. \nA related priming method was recently proposed by \\citet{clark2020tydi}, who used the first 100 characters of a Wikipedia page. \n\nAmong multi-hop reasoning datasets, our dataset stands out in that it requires \\emph{implicit} decompositions.\nTwo recent datasets \\cite{khot2020qasc, mihaylovetal2018} have considered questions requiring implicit facts. However, they are limited to specific domain strategies, while in our work we seek diversity in this aspect.\n\nMost multi-hop reasoning datasets do not fully annotate question decomposition \\cite{yang2018hotpotqa, khot2020qasc, mihaylovetal2018}. \nThis issue has prompted recent work to create question decompositions for existing datasets~\\cite{wolfson2020break}, and to train models that generate question decompositions \\cite{perez2020unsupervised, khot2020text, min2019multi}.\nIn this work, we annotate question decompositions as part of the data collection. \n\n\\section{Conclusion}\n\\label{sec:conclusion}\n\nWe present \\strategyqa{}, the first dataset of \\emph{implicit} multi-step questions requiring a wide-range of reasoning skills.\nTo build \\strategyqa{}, we introduced a novel annotation pipeline for eliciting creative questions that use simple language, but cover a challenging range of diverse strategies.\nQuestions in \\strategyqa{} are annotated with decomposition into reasoning steps and evidence paragraphs, to guide the ongoing research towards addressing implicit multi-hop reasoning. \n\n\\section*{Acknowledgement}\nWe thank Tomer Wolfson for helpful feedback and the REVIZ team at Allen Institute for AI, particularly Michal Guerquin and Sam Skjonsberg.\nThis research was supported in part by the Yandex Initiative for Machine Learning, and the European Research Council (ERC) under the European Union Horizons 2020 research and innovation programme (grant ERC DELPHI 802800). Dan Roth is partly supported by ONR contract N00014-19-1-2620 and DARPA contract FA8750-19-2-1004, under the Kairos program.  \nThis work was completed in partial fulfillment for the Ph.D degree of Mor Geva.\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nMost benchmark datasets targeting commonsense reasoning focus on everyday scenarios: physical knowledge like knowing that you could fill a cup under a waterfall \\citep{Alon_Talmor_2019}, social knowledge like bumping into someone is awkward \\citep{Maarten_Sap_20219}, and other generic situations. However, there is a rich space of commonsense inferences anchored to knowledge about specific entities: for example, deciding the truthfulness of a claim \\emph{Harry Potter can teach classes on how to fly on a broomstick}. Can models learn to combine entity knowledge with commonsense reasoning in this fashion? We introduce \\oursno, a testbed for commonsense reasoning about entity knowledge, bridging fact-checking about entities (Harry Potter is a wizard and is skilled at riding a broomstick) with commonsense inferences (if you're good at a skill you can teach others how to do it). Our dataset consists of 13k human-authored English claims about entities that are either true or false, in addition to a small contrast set.\nCrowdworkers can easily come up with these statements and human performance on the dataset is high (high 90s); we argue that models should be able to blend entity knowledge and commonsense reasoning to do well here. In our experiments, we focus on the closed-book setting and observe that a baseline model finetuned on existing fact verification benchmark struggles on \\oursno. Training a model on \\ours improves accuracy by a substantial margin, but still falls short of human performance. Our benchmark provides a unique probe into natural language understanding models, testing both its ability to retrieve facts (e.g., who teaches at the University of Chicago?) and unstated commonsense knowledge (e.g., butlers do not yell at guests).  \n\\end{abstract}\n\n\\section{Introduction}\n\nTo understand text, humans use rich background knowledge about the world. Despite the impressive ability of large-scale pretrained models, models often generate sentences that violate a reader's expectations, particularly in terms of common sense.  \nAs these models are increasingly employed in settings like generative question answering \\citep{Angela_Fan_2019,Patrick_Lewis_2020} and fact verification \\citep{Andreas_Vlachos_2014, William_Wang_2017, fever}, they should exhibit not just commonsense about everyday scenarios (physical, social, etc.), but factual knowledge about entities as well. These concepts overlap in a set of inferences involving entities that we call \\emph{entity commonsense}. For example, to recognize that \\emph{``Many business owners rely on WordPress to create their websites.''} is true requires both knowledge about the entity  (WordPress is a website hosting service) and a more nebulous piece of commonsense information (famous products like WordPress are widely used).\n\\begin{figure}\n    \\centering\n    \\begin{minipage}{\\textwidth}\n        \\centering\n        \\includegraphics[width=0.9\\textwidth]{fig1.pdf}\n        \\caption{\\ours claims with different reasoning types. Datasets like FEVER focus on retrieval (as in the last case); our dataset also features many claims that involve both retrieval and also commonsense reasoning (54\\% of the data according  our manual study in Section~\\ref{sec:quality-verification}).}\n        \\label{fig:intro}\n    \\end{minipage}\n    \\vspace{-4pt}\n\\end{figure}\n\nWe present \\oursno, a dataset aiming to evaluate two major desiderata of NLP models: entity understanding and commonsense inference. Figure~\\ref{fig:intro} shows how these concepts interact in examples from \\oursno. Building LMs with a stronger ability to perform this type of inference can help make NLP systems more effective and reliable.\n\nOur dataset consists of 13k English claims covering 2.7k entities, each labeled as true or false. Each claim is generated by a crowdworker based on a Wikipedia entity, which can be named entities (e.g., John Dewey), common nouns (e.g., penguins), and abstract concepts (e.g., freedom of speech). Our lightweight task design provides annotators with a set of popular entity topics, and by not including explicit evidence documents to copy text from, annotators are encouraged to create examples fully from scratch. This results in sentences where annotators combine their knowledge about entities with common sense to generate claims. Even without resorting to adversarial filtering, which artificially biases a dataset against existing model checkpoints~\\citep{Sam_Bowman_2021}, we find our annotation protocol leads to challenging claims for existing models. We provide in-depth analysis on what makes our dataset uniquely challenging: for example, 18\\% of claims in \\ours contain quantifiers (e.g., enough, always, rarely etc.) that necessitate subtle commonse reasoning, compared to existing fact verification datasets~\\citep{fever} where only 5\\% of claims contain the quantifiers.\n\nAsking crowdworkers to generate free-form sentences can introduce dataset artifacts \\citep{Suchin_Gururangan_2018, Mor_Geva_2019}. We carefully examine such artifacts in our datasets using quantitative tools \\citep{Swabha_Swayamdipta_2020,Matt_Gardner_2020} as well as qualitative inspection. We also provide a small set of expert-written contrast examples \\citep{Divyansh_Kaushik_2019,Matt_Gardner_2020} which pair true and false claims sharing almost identical context.  \n\nTo establish an initial performance level on \\oursno, we evaluate state-of-the-art pre-trained language models \\citep{Yinhan_Liu_19,T5}. Our experiments shows that \\ours is challenging even for a large model, with a gap between model and human accuracy of 10 points on the development set and about 27 points on the contrast set for the largest model. Moreover, the model trained on \\ours outperforms the model trained on other claim verification datasets~\\citep{fever,Julian_Eisenschlos_2021,Jungsoo_Park_2021}, suggesting that \\ours tests different reasoning capabilities compared to existing datasets. We further characterize the performance based on model size, entity type, and the whether external knowledge is used. Our analysis supports that to achieve high performance on our dataset, models should possess not only entity knowledge but also complex reasoning skills. The code and data are publicly available at \\url{https://www.cs.utexas.edu/~yasumasa/creak}.\n\n\\section{Related Work}\\label{sec:related-work} \n\n\\paragraph{Claim Verification}\nOur task is formulated as claim verification, which has seen increasing work in recent years. The largest claim verification dataset, FEVER~\\citep{fever}, has claims designed to be verifiable with a passage from English Wikipedia and typically covers simple facts such as attributes and records (e.g., \\emph{``Benjamin Franklin was a person,''} and \\emph{``Spider-Man 2 was released in 2004.''}). In fact, 58\\% of FEVER claims contain a simple copular verb ({\\it is, are, was, were}) and many claims contain a definition. Prior work~\\citep{Julian_Eisenschlos_2021} observed high lexical overlap between claims and corresponding entity definitions in Wikipedia, and collected more complex claims using a human-in-the-loop adversarial approach. Similarly, recent work \\citep{Jungsoo_Park_2021} derives a challenging verification dataset from ambiguous questions~\\citep{Sewon_Min_2020} and their different interpretations. However, both datasets focus on a retrieval setting where there is a single paragraph in Wikipedia from which the claim can be easily verified. In contrast, our dataset contains claims where it is not easy to find a single paragraph that can verify them, testing models' intrinsic abilities. \n\n\\paragraph{Question Answering} Question answering and claim verification are closely related, particularly when it comes to binary questions \\citep{Clark-etal-2019-boolq}. Our dataset is purposely constructed to go beyond basic factoid information like that tested in open QA benchmarks like NaturalQuestions \\citep{Tom_Kwiatkowski_2019} and focuses on information that is less likely to have textual support on the web. The recently proposed StrategyQA dataset \\citep{Mor_Geva_2021} which contains binary questions requiring implicit reasoning that goes beyond evidence retrieval (e.g., \\emph{``would it be common to find a penguin in Miami?''}) captures a similar type of reasoning and knowledge as in our work. However, our annotation process does not require authoring strategies, allowing us to scale to a larger dataset (13K vs. 2.8K) while capturing a wide range of inference types. Finally, some QA datasets have been adapted for evaluating differentiable commonsense reasoning models \\citep{lin-etal-2021-differentiable}, but these benchmarks still test very different knowledge from ours.\n\n\\paragraph{Commonsense Reasoning}\nCommonsense reasoning tasks \\citep[inter alia]{Hector_Levesque_2011, Rowan_Zellers_2018, Alon_Talmor_2019,Lourie_etal_2021_Unicorn} evaluate models' reasoning skills in the physical world, with reporting bias being a principal challenge \\citep{Gordon2013ReportingBA}. Yet, most datasets assume hypothetical environments and do not address real-world entities. Our work relates to judging plausibility of events \\citep{forbes-choi-2017-verb,wang-etal-2018-modeling}, closely tied to inferences accessible from feature norms \\citep{McRae}, but again these past efforts do not focus on judgments around specific entities.\n\n\\paragraph{Knowledge Probing} The LAMA benchmark~\\citep{Petroni-etal-2019-language} was proposed to query factual knowledge covered in language models. Our dataset also covers such factual knowledge but also requires commonsense reasoning capabilities. Our work also creates a moderately sized training dataset. Other datasets in the KILT benchmark \\citep{Fabio_Petroni_2020}, an aggregate suite focusing on knowledge intensive tasks, are more focused on recognizing entities and relations, ``low-level'' factual knowledge which does not require the kinds of commonsense inferences in our dataset. Another recent commonsense-focused dataset~\\citep{lin-etal-2020-birds}, focuses on probing numeric claims.\n\n\\section{\\textsc{\\ours}}\\label{sec:dataset}\n\n\\subsection{Task Definition}\n\\paragraph{Problem Scope} Our benchmark covers claims that are typically quite easy for humans to verify but challenging for language models. We focus on factual claims about real-world entities, but our claims are more complex than existing fact verification examples which tend to state relatively simple facts (i.e., definitive sentences, \\emph{X is a Y}, or sentences expressing simple relations, like \\emph{X is CEO of Z}). To the extent possible, we avoid information that is obscure or requires computation, such as asking about the time between two arbitrary events or how many copies of an album were sold, which test either retrieval or memorization rather than commonsense reasoning. We found that our claims can often be verified with minimum knowledge of the entities combined with common sense (i.e., you can guess the answer accurately even if you do not know the entity very well).\\footnote{During our validation, we could confidently judge about 30-50\\% of claims without searching the web.} We argue that this knowledge is what pre-trained LMs should possess about moderately well-known entities after seeing a few occurrences of them during pre-training. Therefore, our claims should be solvable in the closed-book setting where we can purely evaluate LMs' commonsense reasoning skills, isolated from the performance of retrieval models.    \n\nWe formally define the \\textsc{\\ours} task as follows. Given a single sentence claim $c$ containing at least one entity mention, the task is to assign a binary label $y \\in \\{{\\tt TRUE, FALSE}\\}$ indicating whether the claim is true or false. Dataset statistics can be found in Table~\\ref{tab:data}.\n\n\\paragraph{Dataset Properties} Our dataset has following key properties. The claims are \\textbf{diverse} covering various types of entities: they are written by 684 distinct crowdworkers\\footnote{We limit the number of claims that a single crowdworker can generate (no more than 7\\% of any split.} only based on the entity names and their minimal information. We rarely find lexical overlap between the claims and publicly available knowledge sources (e.g., the first paragraph of English Wikipedia). As a result, the claims contain a variety of reasoning types, but nevertheless are \\textbf{typically not subjective} and \\textbf{easily verifiable}. As discussed in Section~\\ref{sec:quality-verification}, a majority of our examples do \\textbf{involve a combination of commonsense reasoning and knowledge}. Finally, Sections~\\ref{sec:quality-verification} and \\ref{sec:experiments} show that the dataset is \\textbf{relatively robust to spurious correlations}.\n\n\\renewcommand{\\arraystretch}{1}\n\\begin{table*}[t]\n\t\\centering\n\t\\small\n\t\t\\caption{Data statistics of \\oursno.}\\label{tab:data}\n\t\\setlength{\\tabcolsep}{4pt}\n    \\centering\n\t\\begin{tabular}{l c c c c c c c c c c}\n\t\t\\toprule\n\t\t\\multicolumn{1}{c}{\\multirow{3}{*}{Split}} & \\multicolumn{1}{c}{} & \\multicolumn{3}{c}{\\# Claims}& \\multicolumn{1}{c}{}  & \\multicolumn{1}{c}{Average Length}& \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{\\multirow{3}{*}{\\# Unique Entities}} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{\\multirow{3}{*}{Vocab Size}}\\\\\n\t\t\\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{Total} & \\multicolumn{1}{c}{True} & \\multicolumn{1}{c}{False} & \\multicolumn{1}{c}{}  & \\multicolumn{1}{c}{(\\# tokens)}  & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{}\\\\\n\t\t\\midrule\n\t\tTrain && 10,176 & 5,088 & 5,088 && 10.8  && 2,096 && 19,006 \\\\\n\t\tDev && \\:\\:\\:1,371 & \\:\\:\\:691 & \\:\\:\\:680 &&  \\:\\:9.7  && \\:\\:\\:531 && \\:\\:\\:4,520\\\\\n\t\tTest && \\:\\:\\:1,371 & \\:\\:\\:707 & \\:\\:\\:664 && \\:\\:9.9  && \\:\\:\\:538 && \\:\\:\\:4,620\\\\\n\t\tTest (Contrast) && \\:\\:\\:\\:\\:\\:200 & \\:\\:\\:100 & \\:\\:\\:100 && 11.0  && \\:\\:\\:100 && \\:\\:\\:\\:\\:\\:800\\\\\n\t\t\\bottomrule \n\t\\end{tabular}\n\n\\end{table*}\n\n\\subsection{Data Collection}\\label{sec:data-collection}\nWe collect our data on the Amazon Mechanical Turk platform. Open-ended text generation is challenging to crowdsource, so we take several steps in our task design to ensure quality. First, we ask crowdworkers to write down the reason why the generated claim is true or false; although past work observes that this does not improve example quality in isolation \\citep{Nikita_Nangia_2021}, we found it helpful for our task, and it additionally helped us spot workers who misunderstood the task. To keep the sentences natural, we use a minimal set of requirements and encourage crowdworkers to produce creative and diverse sentences. One key requirement is to use action verbs instead of copula, which prevent crowdworkers from writing simple definitive sentences. See Appendix~\\ref{app:interface} for more details about the annotation instructions. \n\nWe do not take a model-in-the-loop approach \\citep{Rowan_Zellers_2018,Rown_Zeller_2019,Yixin_Nie_2020,Ronan_Le_Bras_2020} during data collection in order to keep our dataset organic, meaning that sentences preserve the original data distribution given by annotators. Therefore, this benchmark does not favor or disfavor particular LM checkpoints, providing a fair and comparable testbed \\citep{Sam_Bowman_2021}.\n\n\\paragraph{Seed Entities Curation} Entity selection plays a crucial role in this task, since authoring sentences is a much easier task if a crowdworker is familiar with the entity. We take two steps to enable crowdworkers to focus on known entities. First, we use the entity list created by \\citet{Mor_Geva_2021} as part of StrategyQA, which aligns with our needs; the authors select entities based on some popularity measures such as the number of contributors and the number of backward links from other pages. Second, we present five entities to each annotator and let them pick from that set of five when authoring their sentences. %\\todo{histogram over how much sentences from each entity in the supp mat} \nWe manually inspect the seed entities to maintain the diversity of the types of entities so that the generated claims cover diverse topics (e.g., we want to avoid too many location entities that occur in English Wikipedia frequently). We finally obtain 6.4k entities after this process. \n\nWe split the seed entity list into two parts; one for the training instances and one for the development and test instances. In both sets, roughly 80\\% of entities are named entities. The 5 most popular entities in the train set are \\emph{Sloth}, \\emph{Giraffe}, \\emph{George Orwell}, \\emph{50 Cent}, \\emph{Mattel}. In the development and test sets, \\emph{Butterfly}, \\emph{Ray Charles}, \\emph{Whole Foods Market}, \\emph{Internet troll}, and \\emph{Bigfoot} are the top 5 popular entities. As can be seen, crowdworkers prefer to select relatively common entities.\n\n\\paragraph{Quality Control} \nWe split the data curation into two separate tasks such that no annotator contributed to both training and evaluation datasets. This mitigates the issue of learning to model the behavior of specific annotators~\\citep{Mor_Geva_2019} and annotation artifacts from annotator developing a template (e.g., ENTITY created ENTITY) across many instances. In total, \\ours is created by a large number of annotators: 153 crowdworkers annotated the development and test instances, and 531 crowdworkers worked on the training instances. We also use disjoint sets of entities between training and dev/test data so a model trained on the dataset is not simply learning properties of the entities under discussion here. We discuss more in Section~\\ref{sec:quality-verification}.\n\nDuring the annotation process, we monitored the sentence quality and barred crowdworkers who repeatedly produced low-quality sentences or examples following a single pattern. We then inspected the examples included in our evaluation dataset. During the inspection, we found some claims that are subjective, ambiguous, or non-factual (see Appendix~\\ref{app:examples}). These errors potentially lower the human performance on the development and test sets. Since automatically detecting these errors is non-trivial, the authors manually filtered all claims in the evaluation dataset. This process removed roughly 18\\% of crowdsourced claims. This process was crucial for very high human performance (99\\% majority human performance), as we will see in the experiments. \n\n\\paragraph{Contrast Set}\nThe authors of the paper created a small subset of contrastive examples \\citep{Divyansh_Kaushik_2019, Matt_Gardner_2020}. We select 100 seed claims from the evaluation set, then annotate true and false claims based on the seed claims by applying minimal modification (e.g., replacing a word with a similar one that changes the truth of the claim). Examples can be found in Appendix~\\ref{app:examples}.\n\n\\subsection{Dataset Analysis}\\label{sec:quality-verification}\nIn this section, we examine the quality of our dataset. We first manually examine what types of reasoning are required to verify our claims. Then, we study potential lexical and syntactic artifacts in human-generated claims through statistical tests and training dynamics to identify word-level artifacts and learnability of the training instances. \n\\paragraph{Manual Analysis of Reasoning Types}\n\nWe manually validate whether the \\ours claims truly require both knowledge and commonsense. We classify reasoning types into three categories: 1) retrieval, 2) common sense, and 3) a mix of retrieval and common sense. These distinctions are somewhat subjective based on the background knowledge of the reader (i.e., is it common sense that NYC is a major city?); we use our own judgments as authors. The first category, retrieval, asks simple facts about entities which can be found in some knowledge sources such as English Wikipedia (e.g., \\textit{The Harry Potter series originally began with the books.}). The second category, common sense, requires more complex reasoning but are verifiable with the basic knowledge of the entities (e.g., \\textit{Only trumpet players can perform a solo.}). The third category is a mix of retrieval and common sense, meaning that it involves some degree of retrieval and commonsense reasoning. For example, the claim \\textit{One can drive from La Jolla to New York City in less than two hours.} requires knowing the locations of La Jolla and New York City (retrieval) and reasoning about driving times (common sense). We randomly sample 100 claims from the evaluation instances and classify them into the three categories. The proportion of the retrieval, common sense, and a mix of the two categories is 18\\%, 28\\%, and 54\\% respectively. You can find examples for each reasoning type in Appendix~\\ref{app:examples}.\n\n\\paragraph{Dataset Artifacts} Past work on natural language inference has noted that ``artifacts,'' or spurious correlations with surface properties of text, may arise during the annotation process \\citep{Suchin_Gururangan_2018,poliak-etal-2018-hypothesis}. The low performance of a bag-of-words model in our setting (see Table~\\ref{tab:main_result}) gives some confidence that such correlations are not a dominant factor in performance on our data, but we undertake quantitative analysis to explore this further.\n\nWe identify the word-level artifacts in \\ours by computing the artifact statistics described in \\citet{Matt_Gardner_2021}. These statistics tell us, given a balanced dataset, if some words are highly correlated with either true or false claims in a way that a model can exploit. This boils down to a one-side binomial hypothesis test with the null hypothesis $p (y | x_i) = 0.5$, where $y \\in \\{\\texttt{TRUE}, \\texttt{FALSE}\\}$ is a label and $x_i$ is a word in the vocabulary. We first count the occurrence of all words\\footnote{We drop punctuation and lower all words.} in \\textsc{\\ours}. For each word $x_i$ that appears in the $n_i$ claims, we count the number of the target label $y$ in the $n_i$ claims. We estimate $p (y | x_i)$ with the observed probability $\\hat p (y | x_i)$, which is given by a fraction of the count of $y$ over $n_i$. Following \\cite{Matt_Gardner_2021}, we then compute a $z$-statistic and reject/accept the null hypothesis using $\\alpha = 0.01$ with the Bonferroni correction.\n\nFigure~\\ref{fig:competency} plots the word counts $n_i$ ($x$-axis) against the observed probability $\\hat p (y | x_i)$ ($y$-axis) for \\ours and FEVER dataset. We additionally draw the curve that represents the corresponding probability of $\\alpha = 0.01$/13k (for \\oursno) and $\\alpha = 0.01$/10k (for FEVER) at each $n_i$. Any words above this line are considered to be artifacts in the dataset. We find 14 words (out of 13k words in the vocabulary) sit above the line. We label the most frequent words in the plot. Surprisingly, \\emph{and} ($n=1973$) is the most frequent artifact that signals the true label, followed by some quantifiers (\\emph{many}, $n=483$, and \\emph{several}, $n=119$). \\emph{not} ($n=274$) and \\emph{only} ($n=186$) suggest the false label in both datasets. Overall, \\textsc{\\ours} contains relatively few artifacts, and they do not impact the data quality significantly since their frequency is not very high. We observe fewer artifacts compared to FEVER dataset (14 words vs. 28 words above the threshold).\n\n\\begin{figure}\n     \\centering\n     \\begin{minipage}{0.48\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{artifacts_train_2.pdf}\n         \\caption*{(a) \\ours}\n         \\label{fig1:dev}\n     \\end{minipage}\n     \\hspace{10pt}\n     \\begin{minipage}{0.48\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{artifacts_fever_train_3.pdf}\n         \\caption*{(b) FEVER}\n         \\label{fig:contrast}\n     \\end{minipage}\n     \\caption{Artifact statistics of \\ours and FEVER train sets. Words (colored dots) above the green line have detectable correlation with class labels. \\ours contains relatively fewer artifacts, with low severity and frequency.}\n     \\label{fig:competency}\n\\end{figure}\n\n\\paragraph{Training Dynamics} We analyze training dynamics using the framework proposed by \\citet{Swabha_Swayamdipta_2020}. The training dynamics of a training instance are defined by \\emph{confidence} and \\emph{variability},the mean and the standard deviation of model predictions (probability) on the gold label over training epochs. Additionally, \\emph{correctness} is computed by the number of times a training instance is correctly predicted over the number of epochs. Figure~\\ref{fig:trainin-dynamics} shows the histograms of those measurements\\footnote{All values are normalized between 0 and 1 then bucketed into subgroups.} for \\ours (10k instances) and FEVER (105k instances). We use $\\textsc{RoBERTa}_{\\:\\text{Large}}$\\footnote{Following the suggestions by \\citet{Swabha_Swayamdipta_2020}, we train models with early stopping with the patience $= 3$, resulting in 7 epochs for \\ours and  6 epochs for FEVER.} for all experiments. In the \\emph{confidence} plots, \\ours has a fatter distribution (i.e., certain instances get low probability on their gold labels) compared to FEVER's skewed distribution where the majority of instances get very high probability (e.g., > 0.9) on the gold labels. \\oursno's \\emph{variability} histogram is nearly bell-shaped while FEVER's histogram skews towards zero. As can be seen in the \\emph{correctness} plots, some training instances in \\ours are not always predicted correctly during training, as its distribution suggests. However, the most of training instances of FEVER are correctly predicted consistently through the training epochs. By aggregating these observations, we hypothesize that \\ours contains training instances with different difficulty levels compared to FEVER. \n\n\\begin{figure}\n     \\centering\n     \\begin{minipage}{0.48\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{ENTITY_hist.pdf}\n         \\caption*{(a) \\ours}\n         \\label{fig1:dev}\n     \\end{minipage}\n     \\hspace{10pt}\n     \\begin{minipage}{0.48\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{FEVER_hist.pdf}\n         \\caption*{(b) FEVER}\n         \\label{fig:contrast}\n     \\end{minipage}\n     \\caption{Training dynamics for \\ours and FEVER train sets. This figure shows histograms of \\ours or FEVER  training instances bucketed by \\textit{confidence} (mean), \\textit{variability} (std.), or \\textit{correctness}. On all three measures, \\ours shows fatter distributions compared to FEVER, implying that \\ours consists of instances with different difficulties.}\n     \\label{fig:trainin-dynamics}\n\\end{figure}\n\n\\section{Experiments}\n\\label{sec:experiments}\n\nWe focus on the closed-book setting where models are ask to make decisions based solely on claims without any additional retrieved evidence. To see if existing claim verification datasets provide entity commonsense supervision, we train claim-only baseline models on \\textsc{FEVER}~\\citep{fever}, \\textsc{FaVIQ}~\\citep{Jungsoo_Park_2021}, and \\textsc{FoolMeTwice (FM2)}~\\citep{Julian_Eisenschlos_2021} and then evaluate them on \\ours. Next, we train models on the \\ours training set and measure the improvements over the baselines. We also investigate the impacts of model sizes and external knowledge.\n\n\\subsection{Experimental Setup} \n\nWe investigate three training data settings. In the \\emph{Zero-Shot} setting, we train models on the train sets of $\\textsc{FEVER}_{\\:\\text{KILT}}$, \\textsc{FaVIQ-a}, \\textsc{FaVIQ-r},\\footnote{The \\textsc{FaVIQ} benchmark consists of two datasets based on the same source QA dataset.} and \\textsc{FoolMeTwice (FM2)}. In the \\emph{In-Domain} setting, we train models on the \\ours train set in a standard fashion. The \\emph{Finetuning} setting means that we train models on \\textsc{FEVER} and then further finetune on \\oursno. \n\nWe evaluate all models on the \\ours balanced development, test, and contrast sets and report accuracy. As we discussed in Section~\\ref{sec:dataset}, these evaluation sets use distinct entities from the train set and are authored by a different set of crowdworkers. \n\n\\subsection{Comparison Systems}\n\n\\paragraph{Closed-book (Claim-only) Models} In what we consider our standard setting, these models take a claim as input and predict if the claim is true or false. We use a RoBERTa encoder \\citep{Yinhan_Liu_19} with a MLP classifier for baseline models: $\\textsc{RoBERTa}_{\\:\\text{Large}}$ and $\\textsc{RoBERTa}_{\\:\\text{Base}}$. We also train SVM with TF-IDF, which gives a linear baseline using far fewer parameters than the LM-based models. We further employ $\\textsc{T5-3b}$ to see if more parameters help to learn the complex reasoning in \\oursno. \n\n\\paragraph{Retrieval-based Models} These models are augmented with knowledge retrieved from Wikipedia. We feed a claim and $k$ retrieved passages to a model, which can use the information in the passages to influence the decision. We use Dense Passage Retrieval (DPR) \\citep{dpr}\\footnote{DPR is licensed under CC BY-NC 4.0}, a dual-encoder based model, as a retriever and English Wikipedia as a knowledge base. Specifically, we use the DPR model trained on the KILT benchmark, which includes FEVER. We use this configuration for the open-book experiments, where we finetune models on our training set as well as on FEVER. For a claim classifier, we use the $\\textsc{RoBERTa}_{\\:\\text{Large}}$ model and denote this retrieval-based model as $\\textsc{RoBERTa}_{\\:\\text{Large-DPR}}$. We retrieve $k=3$ passages for all experiments.\n\n\\paragraph{Human Performance} To estimate human performance on the development set, we sample 100 examples and ask the authors of this paper to predict the corresponding labels. For the contrast set, three of the authors predict labels for claims that they did not annotate. We report the averaged human accuracy and the ensemble accuracy which we use the majority label as the final prediction to computer human performance. \n\n\\renewcommand{\\arraystretch}{1}\n\\begin{table*}[t]\n\t\\centering\n\t\\small\n\t\t\\caption{Performance of closed-book approaches on \\oursno. Transfer results from prior datasets show that our dataset is distnct from these. Larger models trained with in-domain data perform the best out of all models we consider, but still lag behind human performance.}\n\t\\setlength{\\tabcolsep}{4pt}\n\t\\begin{tabular}{l l c c c c c c c}\n\t\t\\toprule\n\t\t\\multicolumn{1}{c}{\\multirow{2}{*}{}} & \\multicolumn{1}{c}{\\multirow{2}{*}{\\textbf{Model}}}  & \\multicolumn{1}{c}{\\multirow{2}{*}{\\textbf{\\#Params}}} & \\multicolumn{2}{c}{\\textbf{Training Data}} &&  \\multicolumn{3}{c}{\\multirow{1}{*}{\\textbf{Accuracy}}}\\\\\n\t\t& \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{Type} &  \\multicolumn{1}{c}{Size} && \\multicolumn{1}{c}{Dev}  & \\multicolumn{1}{c}{Test} & \\multicolumn{1}{c}{Contrast}\\\\\n\t\t\\midrule\n\t\t& Majority Label & -- & -- & -- && 51.6 & 51.6  & 50.0 \\\\ \n\t\t\\midrule\n\t\t\\multirow{5}{*}{\\textit{Zero-Shot}} & $\\textsc{RoBERTa}_{\\:\\text{Large}}$  & 355M & \\textsc{FaVIQ-r} & 141k && 49.6 & 48.4 & 50.0  \\\\\n\t\t& $\\textsc{RoBERTa}_{\\:\\text{Large}}$ & 355M & \\textsc{FaVIQ-a} & 17k && 52.3 & 52.6 & 52.0 \\\\\n\t\t& $\\textsc{RoBERTa}_{\\:\\text{Large}}$ & 355M & \\textsc{FM2} & 10k && 59.2  & 58.2 & 52.0 \\\\\n        & $\\textsc{RoBERTa}_{\\:\\text{Large}}$ & 355M & $\\textsc{FEVER}_{\\:\\text{KILT}}$ & 105k && 69.6 & 70.2  & 59.0  \\\\\n\t\t& $\\textsc{T5-3b}$ & 3B & $\\textsc{FEVER}_{\\:\\text{KILT}}$ & 105k && 72.9 & 76.7 & 61.5  \\\\\n\t\t\\midrule\n\t\t\\multirow{4}{*}{\\textit{In-Domain}} & $\\textsc{SVM + TF-IDF}$ & 13k & \\textsc{\\ours} & 10k && 60.2 & 60.3  & 52.0 \\\\\n\t\t& $\\textsc{RoBERTa}_{\\:\\text{Base}}$ & 125M & \\textsc{\\ours} & 10k && 72.2 & 71.6 & 56.0 \\\\\n\t\t& $\\textsc{RoBERTa}_{\\:\\text{Large}}$ & 355M & \\textsc{\\ours} & 10k && 80.6 & 80.3 & 61.5 \\\\\n\t\t& $\\textsc{T5-3b}$ & 3B & \\textsc{\\ours} & 10k && \\textbf{85.6} & \\textbf{85.1} & \\textbf{70.0} \\\\\n\t\t\\midrule\n\t\t\\multirow{1}{*}{\\textit{Finetuning}} & $\\textsc{RoBERTa}_{\\:\\text{Large}}$ & 355M & \n\t\t$\\textsc{FEV} \\rightarrow$ \\textsc{\\ours} & 115k && 80.5 & 81.1 &  64.0  \\\\\n\t\t\\midrule\n\t\t& Human (averaged)  & -- & -- & -- && 96.3 & --  & 92.2 \\\\  \n\t\t& Human (ensemble)  & -- & -- & -- && 99.0 & --  & 99.0 \\\\  \n\t\t\\bottomrule \n\t\\end{tabular} \\label{tab:main_result}\n\\end{table*}\n\n\\section{Results and Discussion}\n\nTable~\\ref{tab:main_result} presents our main experimental results for closed book systems, and Table~\\ref{tab:open-book} presents results for retrieval augmented approaches. We observe that all baseline models fall behind our estimated human performance by a substantial margin. \n\n\\paragraph{Transfer from existing datasets} The \\emph{zero-shot} block of Table~\\ref{tab:main_result} compares performance of RoBERTa models trained on four prior claim verification datasets. The models trained on \\textsc{FaVIQ-r} and \\textsc{FaVIQ-a} perform similarly with the majority label baseline. The model trained on \\textsc{FM2} shows better performance than the \\textsc{FaVIQ} models, but the accuracy is still very low. We see much improved transfer from $\\textsc{FEVER}_{\\:\\text{KILT}}$ dataset, reaching an accuracy of 70\\%. Although designed to be more challenging than FEVER, \\textsc{FaVIQ} and \\textsc{FM2} may result in models that transfer less well because these datasets are more dependent on retrieving specific passages to judge claims, containing fewer claims resolvable with commonsense reasoning. Additionally,  $\\textsc{T5-3b}$ trained on $\\textsc{FEVER}_{\\:\\text{KILT}}$ is only better than $\\textsc{RoBERTa}_{\\:\\text{Large}}$ by 3 points on the development set and 6.5 points on the test set although it is 8 times larger, suggesting that $\\textsc{FEVER}_{\\:\\text{KILT}}$ is bounded in terms of how useful it can be for \\ours. \n\nIn the \\emph{Finetuning} block of Table~\\ref{tab:main_result}, we report the performance of $\\textsc{RoBERTa}_{\\:\\text{Large}}$ first trained on  $\\textsc{FEVER}_{\\:\\text{KILT}}$ and then on \\oursno. Compared to $\\textsc{RoBERTa}_{\\:\\text{Large}}$ trained only \\oursno, additional pre-training does not bring meaningful gains. \n \n\\paragraph{Are larger models better?} The \\emph{In-Domain} block of Table~\\ref{tab:main_result} lists performance by models with different sizes ranging from 13k to 3B parameters. All models are trained on \\oursno. $\\textsc{RoBERTa}_{\\:\\text{Base}}$, outperforms SVM with TF-IDF by 11 points on the test set, suggesting that a larger, knowledge-rich model can do better. But its advantage shrinks on the contrast set, only gaining 4 points over SVM with TF-IDF. A larger model $\\textsc{RoBERTa}_{\\:\\text{Large}}$, 355M parameters, further improves the performance, and this trend continues to an even larger model $\\textsc{T5-3b}$, which outperforms $\\textsc{RoBERTa}_{\\:\\text{Large}}$ by 5 points on the test set and 8.5 points on the contrast set. $\\textsc{T5-3b}$ achieves the highest accuracy in the closed-book setting. Given how the contrast set was constructed, the fact that higher-capacity models work better suggests that having more entity knowledge is a key to doing better on this task.\n\n\\begin{figure}\n     \\centering\n     \\begin{minipage}{0.45\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{entity_pageview_plot.pdf}\n         \\caption*{(a) Accuracy by entity popularity}\n         \\label{fig1:eval_entity_pop}\n     \\end{minipage}\n     \\begin{minipage}{0.45\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{entity_type_plot.pdf}\n         \\caption*{(b) Accuracy by entity type}\n         \\label{fig:eval_entity_type}\n     \\end{minipage}\n     \\caption{Performance breakdown on partitions of the development set, split by entity popularity and type, using the four most common entity types, which comprise 86\\% of examples (includng ``NONE'' for non-named entities).}\n     \\label{fig:eval_breakdown}\n     \\vspace{-8pt}\n\\end{figure}\n\\paragraph{Performance breakdown by entity types}\nWe examine whether models are better equipped at verify claims about different entities depending on their popularity and type, as given by an NER tagger. We use $\\textsc{RoBERTa}_{\\:\\text{Large}}$ as an in-domain baseline model for this analysis. To compare entity popularity, we partition our dataset into equally sized quartiles based on total number of views the entity's Wikipedia page has received since Jan.~1, 2016. For entity types, we use an off-the-shelf NER tagger from spaCy \\citep{spacy} to group examples by the entity type. In Figure~\\ref{fig:eval_breakdown}, we plot the performance on each partition of our dataset. We observe that the model performs comparably regardless of entity popularity, partially because we sampled from popular entities, and that entity type has a greater affect on accuracy.\n\\paragraph{Retrieval-based models with external knowledge}\nTo investigate the importance of entity knowledge in \\textsc{\\ours}, we experiment in two additional settings. First, to confirm that entities are important, we experiment with the closed-book setting where all entities are dropped from the claims; this data is denoted as \\textsc{Creak}$_\\textsc{ent\\_less}$. Second, we explore the retrieval setting, where we append three English Wikipedia passages retrieved by DPR to the claims. Similar to the main experiments, we use three data settings: \\emph{Zero-Shot}, \\emph{In-Domain}, and \\emph{Finetuning}. \n\nTable~\\ref{tab:open-book} shows the results of all models. $\\textsc{RoBERTa}_{\\:\\text{Large}}$ trained on \\textsc{Creak}$_\\textsc{ent\\_less}$ loses 10 points compared to the model trained on the standard \\textsc{\\ours} training set (\\emph{In-Domain} $\\textsc{RoBERTa}_{\\:\\text{Large}}$ in Table~\\ref{tab:main_result}). This shows that seeing the entity mention in the claim is important. For open-book models, we again see that \\emph{In-Domain} models are better than \\emph{Zero-Shot} models. One distinction from the closed-book setting is that the additional finetuning on $\\textsc{FEVER}_{\\:\\text{KILT}}$ improves performance. If we compare the \\emph{In-Domain} model from the closed and retrieval settings, the additional passages bring 4 points of improvement. Although adding more entity knowledge improves performance on \\textsc{\\oursno}, there is still a gap from the human performance, particularly on the contrast set. This shows that there are some facts immediately retrievable from Wikipedia; however, our analysis of the dataset also shows that significant additional reasoning is required as well. Moreover, we believe that this kind of knowledge should be accessible to models in a closed-book way, as annotators were able to create these examples without consulting Wikipedia or other knowledge sources. \n\n\\renewcommand{\\arraystretch}{1}\n\\begin{table*}[t]\n\t\\centering\n\t\\small\n\t\t\\caption{Performance of retrieval-augmented approaches on \\oursno. Large models retrieving from Wikipedia can do better, although the performance on the contrast set is still low.}\n\t\\setlength{\\tabcolsep}{4pt}\n    \\centering\n\t\\begin{tabular}{ll c c c c c c c}\n\t\t\\toprule\n\t\t&\\multicolumn{1}{c}{\\multirow{2}{*}{\\textbf{Model}}}  & \\multicolumn{1}{c}{\\multirow{2}{*}{\\textbf{\\#Params}}} & \\multicolumn{2}{c}{\\textbf{Training Data}} &&  \\multicolumn{3}{c}{\\multirow{1}{*}{\\textbf{Accuracy}}}\\\\\n\t\t&& \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{Type} &  \\multicolumn{1}{c}{Size} && \\multicolumn{1}{c}{Dev}  & \\multicolumn{1}{c}{Test} & \\multicolumn{1}{c}{Contrast}\\\\\n\t\t\\midrule\n\t\t&Majority Label & -- & -- & -- && 51.6 & 51.6  & 50.0 \\\\ \n\t\t\\midrule\n\t\t\\textit{Zero-Shot}&$\\textsc{RoBERTa}_{\\:\\text{Large+DPR}}$ & 575M & $\\textsc{FEVER}_{\\:\\text{KILT}}$ & 105k && 79.9 & 80.7 & 68.0  \\\\\n\t\t\\midrule\n\t\t\\multirow{2}{*}{\\textit{In-Domain}} & $\\textsc{RoBERTa}_{\\:\\text{Large}}$ & 355M & \\textsc{Creak}$_\\textsc{ent\\_less}$ & 10k && 69.2 & 67.3 & 52.0 \\\\\n\t\t&$\\textsc{RoBERTa}_{\\:\\text{Large+DPR}}$ & 575M & \\textsc{\\ours} & 10k && 84.0 & 84.3 & 70.0 \\\\\n\t\t\\midrule\n\t\t\\textit{Finetuning}&$\\textsc{RoBERTa}_{\\:\\text{Large+DPR}}$ & 575M & $\\textsc{FEV} \\rightarrow$ \\textsc{\\ours} & 115k && \\textbf{88.7} & \\textbf{86.8} & \\textbf{72.0} \\\\\n\t\t\\midrule\n\t\t&Human (averaged)  & -- & -- & -- && 96.3 & --  & 92.2 \\\\  \n\t\t&Human (ensemble)  & -- & -- & -- && 99.0 & --  & 99.0 \\\\  \n\t\t\\bottomrule \n\t\\end{tabular} \\label{tab:open-book}\n\t\\vspace{-4pt}\n\\end{table*}\n\n\\section{Conclusion}\n\nWe have presented a dataset \\ours of binary claims involving ``entity commonsense,'' a combination of entity knowledge and commonsense reasoning. This dataset is useful both as a training set for instilling this kind of reasoning into models as well as a test set for probing whether models can recognize factually incorrect statements about entities. \nWe believe this can be a useful proving ground for models infused with entity knowledge (e.g., entities-as-experts \\citep{fevry-etal-2020-entities} or interpretable entity embeddings \\citep{onoe-durrett-2020-interpretable}) and contribute to development of these techniques.\n\n\\paragraph{Limitations and Ethical Concerns} We emphasize that our dataset is not intended for training general fact-checking models; we do not support large-scale deployment of models trained on \\ours for this purpose. Furthermore, while we have tried to measure artifacts in this dataset and found them to be minimal, our claims are artificially generated and the nature of the dataset can differ significantly from claims naturally occurring in social media or web. \nLarge language models fine-tuned on our dataset may preserve biases learned from the web text during pre-training or biases of our annotators and make biased judgments as a result. See the datasheet in the Supplementary Material for more information about specific harms that could arise from this dataset.\n\n\\section*{Acknowledgments}\n\nThis work was partially supported by NSF Grant IIS-1814522; this support included funding for the data annotation. The authors would like to thank Mor Geva for providing the raw list of entities used in StrategyQA, as well as the Mechanical Turk annotators who participated in our task.\n\n\\medskip\n\n{\n\\small\n}\n\n\\newpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{The Flan Collection: Designing Data and Methods for Effective Instruction Tuning}\n\n\\begin{document}\n\n\\doparttoc % Tell to minitoc to generate a toc for the parts\n\\faketableofcontents % Run a fake tableofcontents command for the partocs\n\n\\maketitle\n\n\\begin{abstract}\n\\noindent \n\nWe study the design decisions of publicly available instruction tuning methods, and break down the development of \\flantwo{} models \\citep{chung2022scaling}.\nThrough careful ablation studies on the Flan Collection \\emph{of instruction tuning tasks and methods}, we tease apart the effect of design decisions that enable Flan-T5 to outperform prior work by 3-17\\%+ across evaluation settings. \nWe find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings (zero-shot, few-shot, and chain-of-thought) actually yields stronger (2\\%+) performance in \\emph{all} settings.\nIn further experiments, we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks---motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks.\nFinally, to accelerate research on instruction tuning, we make the \\flantwo{} collection of datasets, templates, and methods publicly available.\\footnote{Data generation code available at: \\url{https://github.com/google-research/FLAN/tree/main/flan/v2}. Generation code allows users to vary mixtures rates, templates, prompt types and data augmentations techniques, for faster public research.}\n\n\\end{abstract}\n\n\\makeatletter\n\\newenvironment{customlegend}[1][]{%\n    \\begingroup\n    \\pgfplots@init@cleared@structures\n    \\pgfplotsset{#1}%\n}{%\n    \\pgfplots@createlegend\n    \\endgroup\n}%\n\n\\def\\addlegendimage{\\csname pgfplots@addlegendimage\\endcsname}\n\n\\begin{figure}[ht]\n\\pgfplotsset{width=4.3cm, height=6.0cm,\n    /pgfplots/ybar legend/.style={\n    /pgfplots/legend image code/.code={%\n       \\draw[##1,/tikz/.cd,yshift=-0.25em]\n        (0cm,0cm) rectangle (7pt,0.8em);},\n   },}\n    \\centering\n    \\begin{tikzpicture}  \n    \\begin{groupplot}[\n          group style={\n          group name=plot,\n          horizontal sep=6pt,\n          vertical sep=0pt,\n          group size=5 by 1},]\n      \\nextgroupplot[\n            ybar,\n            ymin=0, ymax=80,\n            ytick={0, 20, 40, 60, 80},\n            major x tick style = transparent,\n            bar width=12pt,\n            enlarge x limits=0.25,\n            ylabel={Avg. Accuracy (\\%)},\n            symbolic x coords={Zero-Shot Held-In},  \n            xtick=data,  \n            axis x line*=bottom,\n            axis y line*=left,\n            y label style={at={(axis description cs:-0.22,0.5)},anchor=south},\n            ]  \n        \\addplot[ybar, fill=frenchblue,  postaction={}] coordinates {\n            (Zero-Shot Held-In, 73.8)\n        };\n        \\addplot[ybar, fill=babyblue,  postaction={}] coordinates {\n            (Zero-Shot Held-In, 68.4)\n        };  \n        \\addplot[ybar, fill=classicrose,  postaction={}] coordinates {\n            (Zero-Shot Held-In, 70.6)\n        };  \n        \\addplot[ybar, fill=beige,  postaction={}] coordinates {\n            (Zero-Shot Held-In, 50.3)\n        };  \n \n      \\nextgroupplot[\n            ybar,\n            ymin=0, ymax=80,\n            ytick={0, 10, 20, 30, 40, 50, 60, 70, 80},\n            major x tick style = transparent,\n            bar width=12pt,\n            enlarge x limits=0.25,\n            symbolic x coords={Zero-Shot CoT},  \n            xtick=data,  \n            axis x line*=bottom,\n            axis y line*=left,\n            y axis line style={opacity=0},\n            ytick=\\empty,\n            ]  \n        \\addplot[ybar, fill=frenchblue,  postaction={}] coordinates {\n            (Zero-Shot CoT, 34.2)\n        };\n        \\addplot[ybar, fill=babyblue,  postaction={}] coordinates {\n            (Zero-Shot CoT, 24.6)\n        };  \n        \\addplot[ybar, fill=classicrose,  postaction={}] coordinates {\n            (Zero-Shot CoT, 25.6)\n        };  \n        \\addplot[ybar, fill=beige,  postaction={}] coordinates {\n            (Zero-Shot CoT, 13.8)\n        };  \n        \n      \\nextgroupplot[\n            ybar,\n            ymin=0, ymax=80,\n            ytick={0, 10, 20, 30, 40, 50, 60, 70, 80},\n            major x tick style = transparent,\n            bar width=12pt,\n            enlarge x limits=0.25,\n            symbolic x coords={Few-Shot BBH},  \n            xtick=data,  \n            axis x line*=bottom,\n            axis y line*=left,\n            y axis line style={opacity=0},\n            ytick=\\empty,\n            ]  \n        \\addplot[ybar, fill=frenchblue,  postaction={}] coordinates {\n            (Few-Shot BBH, 39.3)\n        };\n        \\addplot[ybar, fill=babyblue,  postaction={}] coordinates {\n            (Few-Shot BBH, 28.3)\n        };  \n        \\addplot[ybar, fill=classicrose,  postaction={}] coordinates {\n            (Few-Shot BBH, 30.8)\n        };  \n        \\addplot[ybar, fill=beige,  postaction={}] coordinates {\n            (Few-Shot BBH, 15.6)\n        };  \n        \\addplot[ybar, fill=gray!35,  postaction={}] coordinates {\n            (Few-Shot BBH, 35.7)\n        };  \n\n      \\nextgroupplot[\n            ybar,\n            ymin=0, ymax=80,\n            ytick={0, 10, 20, 30, 40, 50, 60, 70, 80},\n            major x tick style = transparent,\n            bar width=12pt,\n            enlarge x limits=0.25,\n            symbolic x coords={Zero-Shot MMLU},  \n            xtick=data,  \n            axis x line*=bottom,\n            axis y line*=left,\n            y axis line style={opacity=0},\n            ytick=\\empty,\n            ]  \n        \\addplot[ybar, fill=frenchblue,  postaction={}] coordinates {\n            (Zero-Shot MMLU, 50.3)\n        };\n        \\addplot[ybar, fill=babyblue,  postaction={}] coordinates {\n            (Zero-Shot MMLU, 41.4)\n        };  \n        \\addplot[ybar, fill=classicrose,  postaction={}] coordinates {\n            (Zero-Shot MMLU, 46.1)\n        };  \n        \\addplot[ybar, fill=beige,  postaction={}] coordinates {\n            (Zero-Shot MMLU, 35.6)\n        };  \n        \\addplot[ybar, fill=gray!35,  postaction={}] coordinates {\n            (Zero-Shot MMLU, 49.1)\n        };  \n\n        ]\n      \\nextgroupplot[\n            ybar,\n            ymin=0, ymax=80,\n            ytick={0, 10, 20, 30, 40, 50, 60, 70, 80},\n            major x tick style = transparent,\n            bar width=12pt,\n            enlarge x limits=0.25,\n            symbolic x coords={Few-Shot MMLU},  \n            xtick=data,  \n            axis x line*=bottom,\n            axis y line*=left,\n            y axis line style={opacity=0},\n            ytick=\\empty,\n                legend style={\n                        at={(2.2,0.25)},\n                        draw=none,\n                        anchor=south,\n                        column sep=1ex,\n                        font=\\small,\n                }\n            ]  \n        \\addplot[ybar, fill=frenchblue,  postaction={}] coordinates {\n            (Few-Shot MMLU, 52.4)\n        };\n        \\addplot[ybar, fill=babyblue,  postaction={}] coordinates {\n            (Few-Shot MMLU, 34.8)\n        };  \n        \\addplot[ybar, fill=classicrose,  postaction={}] coordinates {\n            (Few-Shot MMLU, 34.1)\n        };  \n        \\addplot[ybar, fill=beige,  postaction={}] coordinates {\n            (Few-Shot MMLU, 31.1)\n        };\n        \\addplot[ybar, fill=gray!35,  postaction={}] coordinates {\n            (Few-Shot MMLU, 47.1)\n        }; \n        ]\n        ]\n    \\end{groupplot}\n    \\node[below,color=forestgreen,font=\\small] at (0.55,4.65) {\\textbf{+3.3}};\n    \\node[below,color=forestgreen,font=\\small] at (3.45,2.48) {\\textbf{+10.2}};\n    \\node[below,color=forestgreen,font=\\small] at (6.14,2.75) {\\textbf{+8.5}};\n    \\node[below,color=forestgreen,font=\\small] at (9.1,3.38) {\\textbf{+4.2}};\n    \\node[below,color=forestgreen,font=\\small] at (12.03,3.51) {\\textbf{+17.6}};\n    \n    \\begin{customlegend}[legend columns=-1,legend style={at={(14.2,-1.0)},draw=none,column sep=1ex},legend entries={T5-XL Flan 2022,T5-XL Flan 2021,T5-XL P3++,T5-XL SNI,OPT-IML-Max 175B}]\n    \\addlegendimage{black,fill=frenchblue,ybar,ybar legend}\n    \\addlegendimage{black,fill=babyblue,ybar,ybar legend}\n    \\addlegendimage{black,fill=classicrose,ybar,ybar legend}\n    \\addlegendimage{black,fill=beige,ybar,ybar legend}\n    \\addlegendimage{black,fill=gray!35,ybar,ybar legend}\n    \\end{customlegend}\n    \n    \\end{tikzpicture} \n    \\caption{\n    \\textbf{Comparing public instruction tuning collections} on Held-In, Held-Out (BIG-Bench Hard \\citep{suzgun2022challenging} and MMLU \\citep{hendrycks2020measuring}), and Chain-of-Thought evaluation suites, detailed in \\cref{app:eval}.\n    All models except OPT-IML-Max (175B) are T5-XL with 3B parameters. \\textcolor{forestgreen}{Green text} indicates absolute improvement over the next best comparable T5-XL (3B) model.\n    }\n    \\label{fig:flan-vs-competitors}\n\\end{figure}% \\vspace{-2mm}\n\n\\section{Introduction}\n\nLarge language models such as PaLM \\citep{chowdhery2022palm}, Chinchilla \\citep{hoffmann2022training}, and ChatGPT among others \\citep{brown2020language,ouyang2022training} have unlocked new capabilities in performing natural language processing (NLP) tasks from reading instructive prompts.\nPrior art has shown that instruction tuning---finetuning language models on a collection of NLP tasks formatted with instructions---further enhances the ability of language models to perform an unseen task from an instruction \\citep{wei2021finetuned, sanh2021multitask, min-etal-2022-metaicl}.\n\nIn this work, we evaluate the methods and results of \\emph{open sourced} instruction generalization efforts, comparing their finetuning techniques and methods.\nAnd in particular, we identify and evaluate the critical methodological improvements in the ``\\flantwo{} Collection'', which is the term we use for the collection \\emph{of data and methods for data augmentation and instruction tuning}, first implemented and used in \\citet{chung2022scaling}.\nWhere \\citet{chung2022scaling} focuses on the emergent and state-of-the-art results of combining Flan 2022 with PaLM 540B, this work focuses in on the details of the instruction tuning methods themselves, ablating individual factors, and comparing them directly to prior work by keeping the pretrained model size and checkpoint consistent.\n\nThe \\flantwo{} Collection offers the most extensive publicly available set of tasks and methods for instruction tuning, which we have compiled in one place.\nWe have also supplemented this with hundreds more of our own high-quality templates, richer formatting patterns, and data augmentations.\nWe show that a model trained on this collection outperforms other public collections on all tested evaluation benchmarks, including the original Flan 2021 \\citep{wei2021finetuned}, T0++ \\citep{sanh2021multitask}, Super-Natural Instructions \\citep{wang2022benchmarking}, and the concurrent work on OPT-IML \\citep{iyer2022optiml}.\nAs shown in \\cref{fig:flan-vs-competitors}, this includes 4.2\\%+ and 8.5\\% improvements on the MMLU \\citep{hendrycks2020measuring} and BIG-Bench Hard \\citep{suzgun2022challenging} evaluation benchmarks respectively, for equally sized models.\n\nAnalysis of the \\flantwo{} method suggests the strong results stem both from the larger and more diverse set of tasks, but also from a set of simple finetuning and data augmentation techniques.\nIn particular, training on a mix of examples templatized with zero-shot, few-shot, and chain-of-thought prompts improves performance in every one of these settings, together.\nFor instance, adding just 10\\% few-shot prompts improves zero-shot prompting results by 2\\%+.\nAdditionally, enriching task diversity by inverting input-output pairs, as used in \\citep{sanh2021multitask,min-etal-2022-metaicl}, along with balancing task sources, are both shown to be critical to performance.\nThe resulting Flan-T5 model converges faster and at a higher performance than T5 models in single-task finetuning---suggesting instruction-tuned models offer a more computationally-efficient starting checkpoint for downstream applications, corroborating \\citet{aribandi2021ext5} and \\citet{tfew2022}.\n\nWe hope making these findings and resources publicly available will unify resources around instruction tuning and accelerate research into more general-purpose language models.\nWe summarize this work's core contributions as follows:\n\\begin{itemize}\\itemsep0em\n    \\vspace{-2 mm}\n    \\item Methodological: Show that training with mixed zero- and few-shot prompts yields much better performance in \\textbf{both} settings (\\cref{sec:mtft-zs-fs}).\n    \\item Methodological: Measure and demonstrate the critical techniques to effective instruction tuning: scaling \\cref{sec:mtft-scaling}, enriching task variety with input inversion (\\cref{sec:mtft-input-inversion}), adding chain-of-thought training data, and balancing different data sources (\\cref{sec:mtft-mix-balance}).\n    \\item Results: Demonstrate these technical choices yield 3-17\\% Held-Out task improvements over existing open source instruction tuning collections (\\cref{fig:flan-vs-competitors}).\n    \\item Results: Demonstrate Flan-T5 serves as a stronger and more computationally-efficient starting checkpoint for single-task finetuning (\\cref{sec:single-target-ft}).\n    \\item Open source the new \\flantwo{} task collection, templates, and methods for public research.\n\\end{itemize}\n\n\\section{Public Instruction Tuning Collections}\n\\label{sec:public-collections}\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.99\\linewidth]{fables/instruction-tuning-collection-5.pdf}\n    \\caption{\n    \\small\n    A \\textbf{Timeline of Public Instruction Tuning Collections} specifies the collection release date, detailed information on the finetuned models (the base model, their size, and whether the model itself is Public (\\textcolor{forestgreen}{P}) or Not Public (\\textcolor{red}{NP})), what prompt specification they were trained for (zero-shot, few-shot, or Chain-of-Thought), the number of tasks contained in the Flan 2022 Collection (released with this work), and core methodological contributions in each work.\\\\\n    Note that the number of tasks and of examples vary under different assumptions and so are estimates. For instance, the definition of ``task'' and ''task category'' vary by work, and are not easily simplified to one ontology. The reported counts for the number of tasks are reported using task definitions from the respective works.\\\\\n    \\textsuperscript{\\textdagger} indicates concurrent work.\\\\\n    }\n    \\vspace{-3mm}\n    \\label{fig:instruction-tuning-collections}\n\\end{figure}\n\n\\paragraph{Large Language Models} Instruction tuning has emerged as a tool to make large language models (LLMs) and their abilities more useful for interactive dialog and functional tasks.\nPrevious work \\citep{raffel2020exploring, liu2019multi,aghajanyan-etal-2021-muppet, aribandi2021ext5} experimented with large scale multi-task finetuning, to improve downstream single target finetuning, but without instruction prompts.\nUnifiedQA and others \\citep{khashabi-etal-2020-unifiedqa, mccann2018natural, keskar2019unifying} unified a wide range of NLP tasks into a single generative question answering format, using prompt instructions for multi-task finetuning and evaluation.\n\n\\vspace{-2mm}\n\\paragraph{The First Wave}\nSince 2020, several instruction tuning task collections have been released in rapid succession, outlined in \\cref{fig:instruction-tuning-collections}.\nNatural Instructions \\citep{mishra2021cross}, Flan 2021 \\citep{wei2021finetuned}, P3 (the Public Pool of Prompts, \\citealp{bach-etal-2022-promptsource}) aggregated large NLP task collections and templatized them with instructions (\\emph{zero-shot prompting}), specifically for finetuning models to generalize to unseen instructions. % To discuss in a meeting with Shayne: We also started from CrossFit and UnifiedQA and TaskEmbed. Flan started from GPT.\nMetaICL \\citep{min-etal-2022-metaicl} also consolidated other task collections \\citep{ye2021crossfit,khashabi-etal-2020-unifiedqa} to train models to learn tasks ``in-context'' -- from several input-output examples, known as \\emph{few-shot prompting}, but in this case without instructions.\nEach of these works affirmed the scaling benefits of task and template diversity, and some reported strong benefits from inverting the inputs and outputs in templates to produce new tasks (``noisy channel'' in \\citealp{min-etal-2022-metaicl}).\n\n\\vspace{-2mm}\n\\paragraph{The Second Wave}\nA second wave of instruction tuning collections expanded prior resources: combining more datasets and tasks into one resource, like Super-Natural Instructions \\citep{wang2022benchmarking} or OPT-IML \\citep{iyer2022optiml}, adding multilingual instruction tuning in xP3 \\citep{muennighoff2022crosslingual}, and Chain-of-Thought training prompts in \\flantwo{} \\citep{chung2022scaling}.\nBoth the Flan Collection and OPT-IML contain most tasks represented in prior collections.\\footnote{Note that each work defines datasets, tasks, and task categories differently. For simplicity, we use their own definitions in \\cref{sec:public-collections}.}\nOur work is positioned here, coalescing most of these collections (of collections) and their methods, as the strongest starting point for future open source work.\n\n\\vspace{-2mm}\n\\paragraph{New Directions}\nConcurrent and future work is beginning to explore two new directions: (a) expanding task diversity even more aggressively with synthetic data generation, particularly in creative, and open-ended dialogue \\citep{selfinstruct2022, honovich2022unnatural, ye2022guess, gupta2022improving}, and (b) offering human feedback signals on model responses \\citep{ouyang2022training, glaese2022improving, bai2022training, nakano2021webgpt, bai2022constitutional}.\nWe view most of these new directions as likely additive to a foundation of instruction tuning methods.\n\n\\vspace{-2mm}\n\\paragraph{Tuning with Human Feedback}\nInstruction tuning on human feedback has demonstrated strong results on open-ended tasks, but at the expense of performance on a wide array of more traditional NLP tasks \\citep{ouyang2022training, glaese2022improving, bai2022training, nakano2021webgpt}.\n(See \\citet{ouyang2022training}'s discussion of the ``alignment tax''.)\nOur work focuses specifically on instruction generalization, without human feedback, for two reasons.\nFirst, human feedback datasets are far less publicly available than instruction tuning datasets (and may be model-specific).\nSecond, by itself, instruction generalization shows great promise in enhancing human preferred responses on open-ended tasks, as well as improving traditional NLP metrics \\citep{chung2022scaling}.\nThe extent of obtainable progress \\emph{without} expensive human response demonstrations or ratings remains an open question, and an important pursuit to narrow the gap between public and non-public research.\n\n\\paragraph{The Importance of Open Source}\n\nHigh profile research is increasingly driven by non-public data, as in the case of GPT-3 and others \\citep{ouyang2022training, glaese2022improving}.\nThe inaccessibility of these resources inhibits the research community's ability to analyze and improve these methods in the public domain.\nWe narrow our purview to open source and accessible data collections, motivated by the goal of democratizing accessibility to research.\n\n\\section{Flan 2022 Instruction Tuning Experiments}\n\\label{sec:flan-it-exps}\n\nRecent research has yet to coalesce around a unified set of techniques, with different tasks, model sizes, and target input formats all represented. \nWe open source a new collection, first introduced in \\citet{chung2022scaling}, denoted ``\\flantwo{}'', which combines Flan 2021, P3++\\footnote{``P3++'' is our notation for all datasets in the Public Pool of Prompts (P3): \\url{https://huggingface.co/datasets/bigscience/P3}}, Super-Natural Instructions, with some additional reasoning, dialog, and program synthesis datasets.\nWe defer to \\citet{chung2022scaling} for details of templatization and collection; and in this work we take a deeper look at key methodological improvements and compare the collection on equivalent model sizes to existing collections.\n\nIn this section, we evaluate the design decisions in Flan and discuss four in particular that yield strong improvements to the instruction tuning recipe.\nThese design components, outlined in \\cref{sec:public-collections}, are: \\textbf{(I)} using mixed zero-shot, few-shot, and Chain-of-Thought templates at training (\\cref{sec:mtft-zs-fs}), \\textbf{(II)} scaling T5-sized models to 1800+ tasks (\\cref{sec:mtft-scaling}), \\textbf{(III)} enriching tasks with input inversion (\\cref{sec:mtft-input-inversion}), and \\textbf{(IV)} balancing these task mixtures (\\cref{sec:mtft-mix-balance}).\nIn \\cref{sec:ablations}, we begin by measuring the value of each component and compare the final model against alternative instruction tuning collections (and their methods).\n\n\\vspace{-3mm}\n\\paragraph{Experimental Setup}\nWe finetune on the prefix language model adapted T5-LM \\citep{lester-etal-2021-power}, using the XL (3B) size for all models for consistency, unless otherwise stated.\nWhile other sizes of Flan-T5 are available, we felt XL was appropriately sized to run large-scale systematic ablations, while being sufficiently large to draw general conclusions.\nWe evaluate on (a) a suite of 8 ``Held-In'' tasks represented within the 1800+ training task collection (4 question answering and 4 natural language inference validation sets), (b) Chain-of-Thought (CoT) tasks (5 validation sets), and (c) the MMLU \\citep{hendrycks2020measuring} and BBH \\citep{suzgun2022challenging} benchmarks as our set of ``Held-Out'' tasks, as they are not included as part of Flan 2022 finetuning.\nThe Massivley Multitask Language Understanding benchmark (MMLU) broadly tests reasoning and knowledge capacity across 57 tasks in the sciences, social sciences, humanities, business, health, among other subjects.\nBIG-Bench Hard (BBH) includes 23 challenging tasks from BIG-Bench \\citep{bigbench} where \\palm{} under-performs human raters.\nIn our ablations, we also evaluate BBH with Chain-of-Thought inputs, following \\citet{chung2022scaling}.\nAdditional finetuning and evaluation details are provided in \\cref{sec:app-setup}.\n\n\\subsection{Ablation Studies}\n\\label{sec:ablations}\n\n\\cref{tab:ablations} summarizes the mean contribution to Held-in, Held-out, and Chain-of-thought tasks, by individually deducting methods: mixture weight balancing (``- Mixture Balancing\"), Chain-of-thought tasks (``- CoT\"), mixed prompt settings (``- Few Shot Templates\"), and Input Inversion (``- Input Inversion\").\nFlan-T5 XL leverages all four of these methods together.\nWe also finetune T5-XL-LM on other collections, including Flan 2021, P3++, Super-Natural Instructions for comparison.\n\n\\begingroup\n\\setlength{\\tabcolsep}{4pt}\n\\begin{table*}[ht]\n    \\centering\n    \\small\n    \\begin{tabular}{l | ccccc}\n    \\toprule\n    \\textsc{Model} & \\textsc{Held-In} & \\textsc{CoT} & \\textsc{MMLU} & \\textsc{BBH} & \\textsc{BBH-CoT} \\\\\n    \\midrule\n    T5-XL \\flantwo{} & \\textbf{73.8 / 74.8} & 35.8 / \\textbf{34.1} & \\textbf{50.3 / 52.4} & 26.2 / \\textbf{39.3}  & \\textbf{33.9 / 35.2} \\\\\n    \\midrule\n    - CoT & 73.3 / 73.2 & 28.8 / 24.6 & 47.5 / 46.9 & 18.2 / 30.0 & 18.2 / 12.0 \\\\\n    - Input Inversion & \\textbf{73.8} / 74.1 & 32.2 / 23.5 & 41.7 / 41.2 & 18.4 / 24.2\t& 15.7 / 13.0 \\\\\n    - Mixture Balancing & 71.2 / 73.1 & 32.3 / 30.5 & 45.4 / 45.8 & 15.1 / 24.3 & 13.8 / 15.4 \\\\\n    - Few Shot Templates & 72.5 / 62.2 & \\textbf{38.9}\t/ 28.6 & 47.3 / 38.7 & 27.6 / 30.8 & 18.6 / 23.3 \\\\\n    \\midrule\n    T5-XL Flan 2021 & 68.4 / 56.3 & 24.6 / 22.7 &\t41.4 / 34.8 & \\textbf{28.1} / 28.3 & 26.0 / 26.9 \\\\\n    T5-XL P3++ & 70.5 / 62.8 & 25.6 / 25.6 & 46.1 / 34.1 & 26.0 / 30.8 & 23.4 / 26.1 \\\\\n    T5-XL Super-Natural Inst. & 50.3 / 42.2&\t13.8 / 14.3 & 35.6 / 31.1 & 10.4 / 15.6 & 8.0 / 12.5 \\\\\n    GLM-130B\\textsuperscript{\\textdagger} & - &\t- & -- / 44.8 & - & - \\\\\n    OPT-IML-Max 30B\\textsuperscript{\\textdagger} & - &\t- & 46.3 / 43.2 &  -- / 30.9 & - \\\\\n    OPT-IML-Max 175B\\textsuperscript{\\textdagger} & - &\t- & 49.1 / 47.1 &  -- / 35.7 & - \\\\\n    \\midrule\n    \\flantwo{} - Next Best T5-XL & \\textcolor{forestgreen}{+3.3} / \\textcolor{forestgreen}{+12} & \\textcolor{forestgreen}{+10.2} / \\textcolor{forestgreen}{+8.5} & \\textcolor{forestgreen}{+4.2} / \\textcolor{forestgreen}{+17.6} & \\textcolor{color3}{-1.9} / \\textcolor{forestgreen}{+8.5} & \\textcolor{forestgreen}{+7.9} / \\textcolor{forestgreen}{+8.3}\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\n    \\textbf{Method Ablations (top)} show the importance of each method for Flan-T5 XL.\n    \\textbf{Collection Ablations (bottom)} evaluate Flan-T5 XL against T5-XL finetuned on other instruction tuning collections: FLAN 2021, P3++, and Super-Natural Instructions.\n    \\textbf{\\flantwo{} - Next Best T5-XL} shows the improvement of Flan-T5 XL over the next best T5-XL (comparatively sized) finetuned on another collection.\n    Metrics are reported in both zero-shot / few-shot settings across Held-In, Chain-of-Thought, and Held-Out (MMLU, BBH) tasks. \\\\\n    \\textsuperscript{\\textdagger} We also inlcude the results reported by OPT-IML \\citep{iyer2022optiml} and GLM-130B \\citep{zeng2022glm}.\n    }\n    \\label{tab:ablations}\n\\end{table*}\n\\endgroup\n\nEach of the ablated components of Flan contributes improvements to different metrics: Chain-of-Thought training to Chain-of-Thought evaluation, input inversion to Held-Out evaluations (MMLU and BBH), few-shot prompt training to few-shot evaluations, and mixture balancing to all metrics.\n\nAs compared to T5-XL models trained on alternative instruction tuning collections (and their methods), Flan outperforms in almost every setting.\nWhile previous collections are tuned specifically to zero-shot prompts, Flan-T5 XL is tuned for either zero- or few-shot prompts.\nThis yields performance margins of +3-10\\% for most of the zero-shot settings, and margins of 8-17\\% for the few-shot settings.\nMost impressively, \\flantwo{} outperforms OPT-IML-Max's much larger (10x) 30B and (58x) 175B models.\nNext, we isolate some of \\flantwo{}'s ablated methods individually, to examine the benefits of each.\n\n\\subsection{Training with Mixed Prompt Settings}\n\\label{sec:mtft-zs-fs}\nPrior work has shown a wide variety of input templates per task can improve performance.\nHowever, separate from the wording of the instruction template, these prior LLMs mostly tune with template sets \\emph{targeted to a single prompt setting}: for zero-shot prompting \\citep{wei2021finetuned, sanh2021multitask, aghajanyan-etal-2021-muppet,aribandi2021ext5} or for few-shot prompting \\citep{min-etal-2022-metaicl, wang2022benchmarking}.\n\nAn underappreciated design decision in InstructGPT \\citep{ouyang2022training} was to mix training templates for each of these prompt settings, rather than target a single setting.\nHowever, since \\citet{ouyang2022training} do not examine this choice, we expected a performance trade-off in finetuning for zero-shot or few-shot prompting performance -- particularly for smaller models.\nInstead, we find training with mixed zero- and few-shot prompts significantly improves performance in \\textbf{both} settings -- most surprisingly, even for models with only 3B parameters.\n\n\\makeatletter\n\\newenvironment{customlegend2}[1][]{%\n    \\begingroup\n    \\pgfplots@init@cleared@structures\n    \\pgfplotsset{#1}%\n}{%\n    \\pgfplots@createlegend\n    \\endgroup\n}%\n\n\\def\\addlegendimage{\\csname pgfplots@addlegendimage\\endcsname}\n\n\\begin{figure}[ht]\n    \\begin{centering}\n    \\begin{tikzpicture}\n        \\pgfplotsset{footnotesize,samples=10}\n        \\begin{groupplot}[\n            group style = {group size = 2 by 1, horizontal sep = 42pt},\n            width = 8cm, \n            height = 6.5cm]\n            \\nextgroupplot[\n                align = center,\n                title = {\\textsc{Held-In Task Performance}},\n                legend style={at={(-0.12,1.4)},anchor=south},\n                xmin=-0.01, xmax=1.0,\n                ymin=60, ymax=74,\n                xtick={0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0},\n                axis x line*=bottom,\n                axis y line*=left,\n                xticklabels={0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100},\n                xlabel={Percent (\\%) Few Shot Templates at Training},\n                ylabel={Accuracy (\\%)},\n                ytick={60, 62, 64, 66, 68, 70, 72, 74},\n                grid style=dashed,\n                x label style={at={(axis description cs:0.5,-0.1)},anchor=north},\n                y label style={at={(axis description cs:-0.12,0.5)},anchor=south},\n                xtick pos=bottom,\n                ytick pos=left,\n                grid=both,\n                ]\n                \\addplot[ % Seen ZS\n                    color=color4,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (0.005, 68.1)\n                    (0.05, 68.4)\n                    (0.1, 69.6)\n                    (0.25, 70.2)\n                    (0.5, 69.2)\n                    (0.75, 69.6)\n                    (0.9, 67.9)\n                    (0.95, 68.1)\n                    (0.99, 63.1)\n                    };\n                \\addplot[ % Seen FS\n                    color=frenchlilac,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (0.005, 56.3)\n                    (0.05, 67.6)\n                    (0.1, 67.7)\n                    (0.25, 70)\n                    (0.5, 69.5)\n                    (0.75, 69.6)\n                    (0.9, 69.3)\n                    (0.95, 69.7)\n                    (0.99, 65.4)\n                    };\n\n                \\addplot[ % Peak Seen ZS\n                    color=color4,\n                    mark=star,\n                    mark size=5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (0.25, 70.2)\n                    };\n                \\addplot[ % Peak Seen FS\n                    color=frenchlilac,\n                    mark=star,\n                    mark size=5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (0.25, 70)\n                    };\n\n            \\nextgroupplot[\n                align = center,\n                title = {\\textsc{Held-Out MMLU Performance}},\n                legend style={at={(-0.12,1.4)},anchor=south},\n                xmin=-0.01, xmax=1.0,\n                ymin=34, ymax=48,\n                xtick={0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0},\n                axis x line*=bottom,\n                axis y line*=left,\n                xticklabels={0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100},\n                xlabel={Percent (\\%) Few Shot Templates at Training},\n                ylabel={Accuracy (\\%)},\n                ytick={34, 36, 38, 40, 42, 44, 46, 48, 50},\n                grid style=dashed,\n                x label style={at={(axis description cs:0.5,-0.1)},anchor=north},\n                y label style={at={(axis description cs:-0.12,0.5)},anchor=south},\n                xtick pos=bottom,\n                ytick pos=left,\n                grid=both,\n                ]\n                \n                \\addplot[ % MMLU ZS\n                    color=color4,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (0.005, 41.4)\n                    (0.05, 41.9)\n                    (0.1, 43.2)\n                    (0.25, 43.0)\n                    (0.5, 45.1)\n                    (0.75, 44.6)\n                    (0.9, 43.2)\n                    (0.95, 42.1)\n                    (0.99, 41.3)\n                    };\n                \\addplot[ % MMLU FS\n                    color=frenchlilac,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (0.005, 34.8)\n                    (0.05, 41.2)\n                    (0.1, 41.8)\n                    (0.25, 42)\n                    (0.5, 43.1)\n                    (0.75, 43.9)\n                    (0.9, 44)\n                    (0.95, 43.4)\n                    (0.99, 41.8)\n                    };\n\n                \\addplot[ % Peak MMLU ZS\n                    color=color4,\n                    mark=star,\n                    mark size=5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (0.5, 45.1)\n                    };\n                \\addplot[ % Peak MMLU FS\n                    color=frenchlilac,\n                    mark=star,\n                    mark size=5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (0.9, 44)\n                    };\n        \\end{groupplot}\n        \n    \\begin{customlegend2}[legend columns=-1,legend style={at={(9.5,-1.2)},draw=none,column sep=1ex},legend entries={Zero-Shot Eval, Few-Shot Eval}]\n    \\addlegendimage{color4,fill=color4,mark=*,sharp plot}\n    \\addlegendimage{frenchlilac,fill=frenchlilac,mark=*,sharp plot}\n    \\end{customlegend2}\n    \n    \\end{tikzpicture}\n    \\caption{\n    \\textbf{Training jointly with zero-shot and few-shot prompt templates improves performance} on both Held-In and Held-Out tasks.\n    The stars indicate the peak performance in each setting.\n    } \n    \\label{fig:zero-few-shot}\n    \\end{centering}\n\\end{figure}\n\n\\cref{fig:zero-few-shot} shows (1) adding as little as 5\\% few-shot training templates can dramatically improve zero-shot performance, and (2) adding 10\\%+ of zero-shot data improves few-shot performance too.\nBoth Held-In and Held-Out tasks peak anywhere between 10-90\\% of few-shot data, but this range is consistently higher than training with only one prompt setting.\n\n\\subsection{Scaling Small Models to 1.8k+ Tasks}\n\\label{sec:mtft-scaling}\n\nThe most recent and concurrent publicly available instruction tuning efforts, like \\flantwo{}, train on thousands of tasks \\citep{wang2022benchmarking,iyer2022optiml}, but operate on different task compositions and underlying training methods.\nTo measure the impact of scaling model sizes and tasks for the  \\flantwo{} collection, we finetune T5-LM adapted models (Small, Base, Large, XL, XXL) on randomly selected task subsets (8, 25, 50, 100, 200, 400, 800, all 1873). \nEvery finetuning run is guaranteed to include the Held-In tasks, so we can estimate how task scaling impacts the model capacity to maintain performance on a given task its already seen.\n\n\\begin{figure}[ht]\n    \\begin{centering}\n    \\begin{tikzpicture}\n        \\pgfplotsset{footnotesize,samples=10}\n        \\begin{groupplot}[\n            group style = {group size = 2 by 1, horizontal sep = 42pt},\n            width = 8cm, \n            height = 6.5cm]\n            \\nextgroupplot[\n                align = center,\n                title = {\\textsc{Held-In Tasks Performance}},\n                legend style={at={(-0.12,1.4)},anchor=south},\n                xmode=log,\n                xmin=0, xmax=2000,\n                ymin=40, ymax=95,\n                xtick={10, 50, 100, 200, 400, 800, 1600},\n                axis x line*=bottom,\n                axis y line*=left,\n                xticklabels={10, 50, 100, 200, 400, 800, 1600},\n                xlabel={Number of Tasks},\n                ylabel={Accuracy (\\%)},\n                ytick={0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100},\n                grid style=dashed,\n                x label style={at={(axis description cs:0.5,-0.1)},anchor=north},\n                y label style={at={(axis description cs:-0.12,0.5)},anchor=south},\n                xtick pos=bottom,\n                ytick pos=left,\n                ]\n                \\addplot[ % T5 Small\n                    color=color1,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (8, 51)\n                    (25, 48.5)\n                    (50, 49.8)\n                    (100, 51.9)\n                    (200, 52.3)\n                    (400, 49.1)\n                    (800, 50.1)\n                    (1873, 46.6)\n                    };\n                \\addplot[ % T5 Base\n                    color=color2,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (8, 55.1)\n                    (25, 54.6)\n                    (50, 55.2)\n                    (100, 61.4)\n                    (200, 62.6)\n                    (400, 61.2)\n                    (800, 60)\n                    (1873, 50.7)\n                    };\n                \\addplot[ % T5 Large\n                    color=color3,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (8, 64.7)\n                    (25, 65.2)\n                    (50, 68.7)\n                    (100, 70.5)\n                    (200, 73.1)\n                    (400, 71.3)\n                    (800, 70)\n                    (1873, 67.9)\n                    };\n                \\addplot[ % T5 XL\n                    color=color4,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (8, 69.7)\n                    (25, 69.2)\n                    (50, 72.2)\n                    (100, 73.8)\n                    (200, 74.6)\n                    (400, 74.7)\n                    (800, 74)\n                    (1873, 72.7)\n                    };\n                \\addplot[ % T5 XXL\n                    color=color5,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (8, 80.7)\n                    (25, 80.3)\n                    (50, 81)\n                    (100, 82.3)\n                    (200, 83)\n                    (400, 83.1)\n                    (800, 81.7)\n                    (1873, 81.1)\n                    };\n                \\addplot[ % Peak T5-Small\n                    color=color11,\n                    mark=star,\n                    mark size=4pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (200, 52.3)\n                    };\n                \\addplot[ % Peak T5-Base\n                    color=color11,\n                    mark=star,\n                    mark size=4pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (200, 62.6)\n                    };\n                \\addplot[ % Peak T5-Large\n                    color=color11,\n                    mark=star,\n                    mark size=4pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (200, 73.1)\n                    };\n                \\addplot[ % Peak T5-XL\n                    color=color11,\n                    mark=star,\n                    mark size=4pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (400, 74.7)\n                    };\n                \\addplot[ % Peak T5-XXL\n                    color=color11,\n                    mark=star,\n                    mark size=4pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (400, 83.1)\n                    };\n                    \\node[color=gray,font=\\scriptsize] at (axis cs: 1550,84) {XXL};\n                    \\node[color=gray,font=\\scriptsize] at (axis cs: 1550,76) {XL};\n                    \\node[color=gray,font=\\scriptsize] at (axis cs: 1550,65) {Large};\n                    \\node[color=gray,font=\\scriptsize] at (axis cs: 1550,57) {Base};\n                    \\node[color=gray,font=\\scriptsize] at (axis cs: 1550,43) {Small};\n\n            \\nextgroupplot[\n                align = center,\n                title = {\\textsc{Held-Out MMLU Performance}},\n                legend style={at={(-0.12,1.4)},anchor=south},\n                xmode=log,\n                xmin=0, xmax=2000,\n                ymin=19, ymax=56,\n                xtick={10, 50, 100, 200, 400, 800, 1600},\n                axis x line*=bottom,\n                axis y line*=left,\n                xticklabels={10, 50, 100, 200, 400, 800, 1600},\n                xlabel={Number of Tasks},\n                ylabel={Accuracy (\\%)},\n                ytick={0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100},\n                grid style=dashed,\n                x label style={at={(axis description cs:0.5,-0.1)},anchor=north},\n                y label style={at={(axis description cs:-0.12,0.5)},anchor=south},\n                xtick pos=bottom,\n                ytick pos=left,\n                ]\n                \\addplot[ % T5 Small\n                    color=color6,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (8, 26.9)\n                    (25, 24.1)\n                    (50, 25.1)\n                    (100, 28.8)\n                    (200, 31.2)\n                    (400, 29)\n                    (800, 28.3)\n                    (1873, 29.8)\n                    };\n                \\addplot[ % T5 Base\n                    color=color7,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (8, 23.6)\n                    (25, 23.4)\n                    (50, 24.4)\n                    (100, 33)\n                    (200, 32)\n                    (400, 32.7)\n                    (800, 31.5)\n                    (1873, 33.6)\n                    };\n                \\addplot[ % T5 Large\n                    color=color8,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (8, 25.5)\n                    (25, 26.8)\n                    (50, 29.7)\n                    (100, 35.7)\n                    (200, 40.9)\n                    (400, 41)\n                    (800, 40.7)\n                    (1873, 41.4)\n                    };\n                \\addplot[ % T5 XL\n                    color=color9,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (8, 32.6)\n                    (25, 35.7)\n                    (50, 39.4)\n                    (100, 44.4)\n                    (200, 44.2)\n                    (400, 45.8)\n                    (800, 47.2)\n                    (1873, 47.9)\n                    };\n                \\addplot[ % T5 XL\n                    color=color10,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (8, 45.6)\n                    (25, 47.9)\n                    (50, 51.5)\n                    (100, 52)\n                    (200, 52.8)\n                    (400, 53.7)\n                    (800, 52.4)\n                    (1873, 53.8)\n                    };\n                \\addplot[ % Peak T5-Small\n                    color=color11,\n                    mark=star,\n                    mark size=4pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (200, 31.2)\n                    };\n                \\addplot[ % Peak T5-Base\n                    color=color11,\n                    mark=star,\n                    mark size=4pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (1873, 33.6)\n                    };\n                \\addplot[ % Peak T5-Large\n                    color=color11,\n                    mark=star,\n                    mark size=4pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (1873, 41.4)\n                    };\n                \\addplot[ % Peak T5-XL\n                    color=color11,\n                    mark=star,\n                    mark size=4pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (1873, 47.9)\n                    };\n                \\addplot[ % Peak T5-XXL\n                    color=color11,\n                    mark=star,\n                    mark size=4pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (1873, 53.8)\n                    };\n                    \\node[color=gray,font=\\scriptsize] at (axis cs: 1550,52) {XXL};\n                    \\node[color=gray,font=\\scriptsize] at (axis cs: 1550,46) {XL};\n                    \\node[color=gray,font=\\scriptsize] at (axis cs: 1550,39) {Large};\n                    \\node[color=gray,font=\\scriptsize] at (axis cs: 1550,35.5) {Base};\n                    \\node[color=gray,font=\\scriptsize] at (axis cs: 1550,28) {Small}; \n        \\end{groupplot}\n    \\end{tikzpicture}\n    \\caption{\n    \\textbf{Performance Scaling Laws for the number of finetuning tasks and model sizes}. Held-In performance (left) and Held-Out MMLU performance (right) are shown. The gold star indicates the peak performance for that model size.\n    } \n    \\label{fig:scaling-laws}\n    \\end{centering}\n\\end{figure}\n\n\\cref{fig:scaling-laws} demonstrates that both Held-In and Held-Out tasks appear to benefit from adding hundreds of finetuning tasks.\nHeld-in task evaluations peak around 200 total tasks, and diminish in performance as more tasks are added, though larger models peak later and diminish less.\nHeld-out task performance increases log-linearly with the number of tasks, achieving the highest performances with all 1836 tasks.\nSurprisingly, only T5-Small appears to exceed its Held-Out task performance before 1836 tasks, while larger model sizes continue to improve.\nThese results suggest (a) even T5-Base may not have exhausted its capacity with thousands of tasks, and (b) the largest LMs could benefit from thousands more tasks for Held-In and Held-Out task performance.\n\nOne necessary assumption of this analysis is that all tasks are defined and counted equally.\n\\cref{sec:mtft-mix-balance} demonstrates how not all task sources are equally beneficial to training, and the model performance may saturate from too many tasks from one source (e.g. Super-Natural Instructions).\nWe would caution conclusions that task scaling beyond 1800 would translate to increased returns without also paying attention to task diversity and quality.\n\n\\subsection{Task Enrichment with Input Inversion}\n\\label{sec:mtft-input-inversion}\n\nPrior instruction tuning work has enriched their diversity of tasks by inverting the ($x$, $y$) input-output pairs in supervised tasks---referred to as ``prompts not intended for the original task'' in P3 \\citep{bach-etal-2022-promptsource} or the ``noisy channel'' in MetaICL \\citep{min-etal-2022-metaicl}.\nFor example, a dataset may be originally designed for, given a question $x$, evaluate if a model can answer $y$. Input inversion instead gives a model the answer $y$ and trains it to generate the question $x$.    \nThis is an easy method to enrich the task variety given a limited set of data sources.\nHowever, it isn't clear that this method remains helpful when 100s of unique data sources and 1000s of tasks are already available.\n\nTo assess this, we enrich our mixtures with input inverted tasks (details and examples in \\cref{sec:app-input-inversion}) and measure the effect.\nIn \\cref{tab:ablations} we find this is not beneficial for Held-In performance, but strongly beneficial for Held-Out performance.\nThese benefits invigorate the prospect of data augmentation techniques for LLM finetuning, which had previously been shown to have diminishing returns the longer models are pretrained \\citep{longpre2020effective}.\n\n\\begin{table}[ht]\n    \\centering\n    \\begin{tabular}{l | ccc}\n    \\toprule\n    \\textsc{Train Mixtures} &  & \\textsc{Metrics} & \\\\\n    & Held-In & CoT & MMLU \\\\\n    \\midrule\n    All (Equal) & 64.9 & 41.4 & 47.3 \\\\\n    \\midrule\n    All - Flan 2021 & 55.3 & 38.6 & 45.7 \\\\\n    All - T0-SF & 63.2 & \\textbf{43.4} & 44.7 \\\\\n    All - Super-Nat. Inst. & 65.9 & 42.2 & 46.8 \\\\\n    All - CoT & 65.6 & 29.1 & 46.8 \\\\\n    All - Prog. Synth. & 66.9 & 42.3 & 46.8  \\\\\n    All - Dialog & 65.4 & 40.3 & 47.1 \\\\\n    \\midrule\n    All (Weighted) & \\textbf{66.4} & 40.1 & \\textbf{48.1} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\n    Subsets of tasks are left out from an equally weighted mixture to measure their importance.\n    \\textbf{T0-SF and Flan 2021 finetuning are most important for MMLU, while Chain-of-Thought (CoT) finetuning is most important for Chain-of-Thought evaluation.}}\n    \\label{tab:mixture-ranking}\n\\end{table}\n\n\\subsection{Balancing Data Sources}\n\\label{sec:mtft-mix-balance}\n\nScaling architecture size and the number of tasks are effective, but our results suggest the mixture weighting deserves as much attention to optimize results. \nTo converge on a balanced weighting, we omit different sets of task sources, one at a time (Flan 2021, \\tzeromixture{}, Super-Natural Instructions, Chain-of-Thought, Dialog, and Program Synthesis), and rank their contributions on the MMLU benchmark.\\footnote{Following \\citet{chung2022scaling} we refer to the subset of P3++ that is not in Flan 2021 as T0-SF (SF stands for sans Flan).}.\n\nAs shown in \\cref{tab:mixture-ranking}, Flan 2021 and \\tzeromixture{} are among the most beneficial mixtures, followed by Super-Natural Instructions and Chain-of-Thought, with Dialog and Program Synthesis last.\nThese findings are corroborated by \\citet{iyer2022optiml} who extensively test data mixing proportions, and also determine their Flan 2021, \\tzeromixture{}, and T5 mixtures are the most broadly beneficial.\nAdditionally, they find Super-Natural Instructions has limited scaling benefits on Held-Out task performance, which they relate to its unique input format and instruction design.\nNotably, Chain-of-thought finetuning appears beneficial across all our evaluation settings, especially considering they contain far fewer tasks than Flan 2021, \\tzeromixture{} or Natural Instructions.\n\n\\begin{figure}[ht]\n\\pgfplotsset{width=3.5cm, height=5.5cm,\n    /pgfplots/ybar legend/.style={\n    /pgfplots/legend image code/.code={%\n       \\draw[##1,/tikz/.cd,yshift=-0.25em]\n        (0cm,0cm) rectangle (7pt,0.8em);},\n   },}\n    \\centering\n    \\begin{tikzpicture}  \n    \\begin{groupplot}[\n          group style={\n          group name=plot,\n          horizontal sep=5pt,\n          vertical sep=40pt,\n          group size=7 by 2},]\n      \\nextgroupplot[\n            ybar,\n            ymin=45, ymax=100,\n            ytick={50, 60, 70, 80, 90, 100},\n            major x tick style = transparent,\n            bar width=10pt,\n            enlarge x limits=0.25,\n            ylabel={Eval Metrics (\\%)},\n            symbolic x coords={ANLI},  \n            xtick=data,  \n            axis x line*=bottom,\n            axis y line*=left,\n            y label style={at={(axis description cs:-0.35,0.5)},anchor=south},\n            legend cell align=left,\n                legend style={\n                        at={(1.7,1.05)},\n                        anchor=south,\n                        column sep=1ex,\n                        font=\\small,\n                }\n            ]  \n        \\addplot[ybar, fill=color1,  postaction={}] coordinates {\n            (ANLI, 67.1)\n        };  \n        \\addplot[ybar, fill=babyblue,  postaction={}] coordinates {\n            (ANLI, 63.1)\n        };\n        \\addplot[ybar, fill=blanchedalmond,  postaction={}] coordinates {\n            (ANLI, 53.3)\n        };  \n      \\nextgroupplot[\n            ybar,\n            ymin=45, ymax=100,\n            ytick={50, 60, 70, 80, 90, 100},\n            major x tick style = transparent,\n            bar width=10pt,\n            enlarge x limits=0.25,\n            symbolic x coords={ARC},  \n            xtick=data,  \n            axis x line*=bottom,\n            axis y line*=left,\n            y axis line style={opacity=0},\n            ytick=\\empty,\n            legend cell align=left,\n                legend style={\n                        at={(0.0,1.05)},\n                        draw=none,\n                        anchor=south,\n                        column sep=1ex,\n                        font=\\small,\n                }\n            ]  \n        \\addplot[ybar, fill=color1,  postaction={}] coordinates {\n            (ARC, 71.8)\n        };\n        \\addplot[ybar, fill=babyblue,  postaction={}] coordinates {\n            (ARC, 63.1)\n        };\n        \\addplot[ybar, fill=blanchedalmond,  postaction={}] coordinates {\n            (ARC, 69.4)\n        };  \n\n      \\nextgroupplot[\n            ybar,\n            ymin=45, ymax=100,\n            ytick={50, 60, 70, 80, 90, 100},\n            major x tick style = transparent,\n            bar width=10pt,\n            enlarge x limits=0.25,\n            symbolic x coords={BoolQ},  \n            xtick=data,  \n            axis x line*=bottom,\n            axis y line*=left,\n            y axis line style={opacity=0},\n            ytick=\\empty,\n            legend cell align=left,\n                legend style={\n                        at={(0.0,1.05)},\n                        draw=none,\n                        anchor=south,\n                        column sep=1ex,\n                        font=\\small,\n                }\n            ]  \n        \\addplot[ybar, fill=color1,  postaction={}] coordinates {\n            (BoolQ, 95.5)\n        };  \n        \\addplot[ybar, fill=babyblue,  postaction={}] coordinates {\n            (BoolQ, 92.8)\n        };\n        \\addplot[ybar, fill=blanchedalmond,  postaction={}] coordinates {\n            (BoolQ, 92.2)\n        };  \n\n      \\nextgroupplot[\n            ybar,\n            ymin=45, ymax=100,\n            ytick={50, 60, 70, 80, 90, 100},\n            major x tick style = transparent,\n            bar width=10pt,\n            enlarge x limits=0.25,\n            symbolic x coords={CosmosQA},  \n            xtick=data,  \n            axis x line*=bottom,\n            axis y line*=left,\n            y axis line style={opacity=0},\n            ytick=\\empty,\n            legend cell align=left,\n                legend style={\n                        at={(0.0,1.05)},\n                        draw=none,\n                        anchor=south,\n                        column sep=1ex,\n                        font=\\small,\n                }\n            ]  \n        \\addplot[ybar, fill=color1,  postaction={}] coordinates {\n            (CosmosQA, 94)\n        };\n        \\addplot[ybar, fill=babyblue,  postaction={}] coordinates {\n            (CosmosQA, 93)\n        };\n        \\addplot[ybar, fill=blanchedalmond,  postaction={}] coordinates {\n            (CosmosQA, 89.5)\n        };  \n\n      \\nextgroupplot[\n            ybar,\n            ymin=45, ymax=100,\n            ytick={50, 60, 70, 80, 90, 100},\n            major x tick style = transparent,\n            bar width=10pt,\n            enlarge x limits=0.25,\n            symbolic x coords={RTE},  \n            xtick=data,  \n            axis x line*=bottom,\n            axis y line*=left,\n            y axis line style={opacity=0},\n            ytick=\\empty,\n            legend cell align=left,\n                legend style={\n                        at={(0.0,1.05)},\n                        draw=none,\n                        anchor=south,\n                        column sep=1ex,\n                        font=\\small,\n                }\n            ]  \n        \\addplot[ybar, fill=color1,  postaction={}] coordinates {\n            (RTE, 97.5)\n        };\n        \\addplot[ybar, fill=babyblue,  postaction={}] coordinates {\n            (RTE, 89.7)\n        };\n        \\addplot[ybar, fill=blanchedalmond,  postaction={}] coordinates {\n            (RTE, 95.6)\n        };  \n\n      \\nextgroupplot[\n            ybar,\n            ymin=45, ymax=100,\n            ytick={50, 60, 70, 80, 90, 100},\n            major x tick style = transparent,\n            bar width=10pt,\n            enlarge x limits=0.25,\n            symbolic x coords={SQuAD v2},  \n            xtick=data,  \n            axis x line*=bottom,\n            axis y line*=left,\n            y axis line style={opacity=0},\n            ytick=\\empty,\n            legend cell align=left,\n                legend style={\n                        at={(0.0,1.05)},\n                        draw=none,\n                        anchor=south,\n                        column sep=1ex,\n                        font=\\small,\n                }\n            ]  \n        \\addplot[ybar, fill=color1,  postaction={}] coordinates {\n            (SQuAD v2, 89.8)\n        };\n        \\addplot[ybar, fill=babyblue,  postaction={}] coordinates {\n            (SQuAD v2, 87.4)\n        };\n        \\addplot[ybar, fill=blanchedalmond,  postaction={}] coordinates {\n            (SQuAD v2, 84.8)\n        };  \n\n      \\nextgroupplot[\n            ybar,\n            ymin=45, ymax=100,\n            ytick={50, 60, 70, 80, 90, 100},\n            major x tick style = transparent,\n            bar width=10pt,\n            enlarge x limits=0.25,\n            symbolic x coords={AI2 Science},  \n            xtick=data,  \n            axis x line*=bottom,\n            axis y line*=left,\n            y axis line style={opacity=0},\n            ytick=\\empty,\n            legend cell align=left,\n                legend style={\n                        at={(0.0,1.05)},\n                        draw=none,\n                        anchor=south,\n                        column sep=1ex,\n                        font=\\small,\n                }\n            ]  \n        \\addplot[ybar, fill=color1,  postaction={}] coordinates {\n            (AI2 Science, 78.7)\n        };\n        \\addplot[ybar, fill=babyblue,  postaction={}] coordinates {\n            (AI2 Science, 62)\n        };\n        \\addplot[ybar, fill=blanchedalmond,  postaction={}] coordinates {\n            (AI2 Science, 78)\n        };  \n\n      \\nextgroupplot[\n            ybar,\n            ymin=45, ymax=100,\n            ytick={50, 60, 70, 80, 90, 100},\n            major x tick style = transparent,\n            bar width=10pt,\n            enlarge x limits=0.25,\n            ylabel={Eval Metrics (\\%)},\n            symbolic x coords={CondaQA},  \n            xtick=data,  \n            axis x line*=bottom,\n            axis y line*=left,\n            y label style={at={(axis description cs:-0.35,0.5)},anchor=south},\n            legend cell align=left,\n                legend style={\n                        at={(1.7,1.05)},\n                        anchor=south,\n                        column sep=1ex,\n                        font=\\small,\n                }\n            ]  \n        \\addplot[ybar, fill=color1,  postaction={}] coordinates {\n            (CondaQA, 74.7)\n        };  \n        \\addplot[ybar, fill=babyblue,  postaction={}] coordinates {\n            (CondaQA, 72.1)\n        };\n        \\addplot[ybar, fill=blanchedalmond,  postaction={}] coordinates {\n            (CondaQA, 64.5)\n        };  \n\n      \\nextgroupplot[\n            ybar,\n            ymin=45, ymax=100,\n            ytick={50, 60, 70, 80, 90, 100},\n            major x tick style = transparent,\n            bar width=10pt,\n            enlarge x limits=0.25,\n            symbolic x coords={CxC},  \n            xtick=data,  \n            axis x line*=bottom,\n            axis y line*=left,\n            y axis line style={opacity=0},\n            ytick=\\empty,\n            legend cell align=left,\n                legend style={\n                        at={(0.0,1.05)},\n                        draw=none,\n                        anchor=south,\n                        column sep=1ex,\n                        font=\\small,\n                }\n            ]  \n        \\addplot[ybar, fill=color1,  postaction={}] coordinates {\n            (CxC, 84.5)\n        };\n        \\addplot[ybar, fill=babyblue,  postaction={}] coordinates {\n            (CxC, 84.5)\n        };\n        \\addplot[ybar, fill=blanchedalmond,  postaction={}] coordinates {\n            (CxC, 78.3)\n        };  \n\n      \\nextgroupplot[\n            ybar,\n            ymin=45, ymax=100,\n            ytick={50, 60, 70, 80, 90, 100},\n            major x tick style = transparent,\n            bar width=10pt,\n            enlarge x limits=0.25,\n            symbolic x coords={MedNLI},  \n            xtick=data,  \n            axis x line*=bottom,\n            axis y line*=left,\n            y axis line style={opacity=0},\n            ytick=\\empty,\n            legend cell align=left,\n                legend style={\n                        at={(0.0,1.05)},\n                        draw=none,\n                        anchor=south,\n                        column sep=1ex,\n                        font=\\small,\n                }\n            ]  \n        \\addplot[ybar, fill=color1,  postaction={}] coordinates {\n            (MedNLI, 87.2)\n        };  \n        \\addplot[ybar, fill=babyblue,  postaction={}] coordinates {\n            (MedNLI, 87.1)\n        };\n        \\addplot[ybar, fill=blanchedalmond,  postaction={}] coordinates {\n            (MedNLI, 81.8)\n        };  \n\n      \\nextgroupplot[\n            ybar,\n            ymin=45, ymax=100,\n            ytick={50, 60, 70, 80, 90, 100},\n            major x tick style = transparent,\n            bar width=10pt,\n            enlarge x limits=0.25,\n            symbolic x coords={PubmedQA},  \n            xtick=data,  \n            axis x line*=bottom,\n            axis y line*=left,\n            y axis line style={opacity=0},\n            ytick=\\empty,\n            legend cell align=left,\n                legend style={\n                        at={(0.0,1.05)},\n                        draw=none,\n                        anchor=south,\n                        column sep=1ex,\n                        font=\\small,\n                }\n            ]  \n        \\addplot[ybar, fill=color1,  postaction={}] coordinates {\n            (PubmedQA, 52.3)\n        };\n        \\addplot[ybar, fill=babyblue,  postaction={}] coordinates {\n            (PubmedQA, 50)\n        };\n        \\addplot[ybar, fill=blanchedalmond,  postaction={}] coordinates {\n            (PubmedQA, 51.9)\n        };  \n\n      \\nextgroupplot[\n            ybar,\n            ymin=45, ymax=100,\n            ytick={50, 60, 70, 80, 90, 100},\n            major x tick style = transparent,\n            bar width=10pt,\n            enlarge x limits=0.25,\n            symbolic x coords={WANLI},  \n            xtick=data,  \n            axis x line*=bottom,\n            axis y line*=left,\n            y axis line style={opacity=0},\n            ytick=\\empty,\n            legend cell align=left,\n                legend style={\n                        at={(2.2,0.34)},\n                        draw=none,\n                        anchor=south,\n                        column sep=1ex,\n                        font=\\small,\n                }\n            ] \n        \\addplot[ybar, fill=color1,  postaction={}] coordinates {\n            (WANLI, 78)\n        };\n        \\addplot[ybar, fill=babyblue,  postaction={}] coordinates {\n            (WANLI, 76.4)\n        };\n        \\addplot[ybar, fill=blanchedalmond,  postaction={}] coordinates {\n            (WANLI, 60.6)\n        };  \n\n        \\legend{\n            T5 $\\rightarrow$ Flan $\\rightarrow$ FT,\n            T5 $\\rightarrow$ FT,\n            T5 $\\rightarrow$ Flan,\n            }\n        ]\n        ]\n    \\end{groupplot}\n    \\node (title) at ($(group c1r1.center)!0.5!(group c2r1.center)+(0,2.5cm)$) {\\textsc{Flan Held-In Tasks}};\n    \\node (title) at ($(group c1r1.center)!0.5!(group c2r1.center)-(0,3.6cm)$) {\\textsc{Flan Held-Out Tasks}};\n    \n    \\node[below,color=forestgreen,font=\\small] at (0.5,2.2) {\\textbf{+4.0}};\n    \\node[below,color=forestgreen,font=\\small] at (2.53,2.55) {\\textbf{+8.7}};\n    \\node[below,color=forestgreen,font=\\small] at (4.62,4.25) {\\textbf{+2.7}};\n    \\node[below,color=forestgreen,font=\\small] at (6.7,4.05) {\\textbf{+1.0}};\n    \\node[below,color=forestgreen,font=\\small] at (8.8,4.3) {\\textbf{+7.8}};\n    \\node[below,color=forestgreen,font=\\small] at (10.92,3.8) {\\textbf{+2.4}};\n    \\node[below,color=forestgreen,font=\\small] at (12.99,3.05) {\\textbf{+16.7}};\n    \n    \\node[below,color=forestgreen,font=\\small] at (0.5,-2.65) {\\textbf{+2.6}};\n    \\node[below,color=forestgreen,font=\\small] at (2.53,-2.0) {\\textbf{+0.0}};\n    \\node[below,color=forestgreen,font=\\small] at (4.62,-1.8) {\\textbf{+0.1}};\n    \\node[below,color=forestgreen,font=\\small] at (6.7,-4.3) {\\textbf{+2.3}};\n    \\node[below,color=forestgreen,font=\\small] at (8.8,-2.4) {\\textbf{+1.6}};\n    \n    \\end{tikzpicture} \n    \\caption{\n    \\textbf{Flan-T5 Outperforms T5 on Single-Task Finetuning.} We compare single-task finetuned T5, single-task finetuned Flan-T5, and Flan-T5 without any further finetuning.\n    }\n    \\label{fig:single-target}\n\\end{figure}\nWe used these findings to significantly narrow the mixture weights search space, and used our practitioner's intuition from there.\nThis strategy is simple but effective, as shown in \\cref{tab:ablations}, but leaves ample room for more sophisticated future work.\n\n\\makeatletter\n\\newenvironment{customlegend3}[1][]{%\n    \\begingroup\n    \\pgfplots@init@cleared@structures\n    \\pgfplotsset{#1}%\n}{%\n    \\pgfplots@createlegend\n    \\endgroup\n}%\n\n\\def\\addlegendimage{\\csname pgfplots@addlegendimage\\endcsname}\n\n\\begin{figure}[ht]\n    \\begin{centering}\n    \\begin{tikzpicture}\n        \\pgfplotsset{footnotesize,samples=10}\n        \\begin{groupplot}[\n            group style = {group size = 5 by 1, horizontal sep = 9pt},\n            width = 4.3cm, \n            height = 4.3cm]\n            \\nextgroupplot[\n                align = center,\n                title = {\\textsc{WANLI}},\n                legend style={at={(-0.12,1.4)},anchor=south},\n                xmin=0, xmax=200,\n                ymin=30, ymax=90,\n                xtick={0, 50, 100, 150, 200},\n                axis x line*=bottom,\n                axis y line*=left,\n                ylabel={Accuracy (\\%)},\n                ytick={30, 40, 50, 60, 70, 80, 90},\n                grid style=dashed,\n                x label style={at={(axis description cs:0.5,-0.1)},anchor=north},\n                y label style={at={(axis description cs:-0.18,0.5)},anchor=south},\n                xtick pos=bottom,\n                ytick pos=left,\n                grid=both,\n                ]\n                \\addplot[ % WANLI T5\n                    color=color3,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    dashed,\n                    ]\n                    coordinates {\n                    (20, 37.16)\n                    (40, 48.18)\n                    (60, 48.16)\n                    (80, 52.68)\n                    (100, 61.16)\n                    (120, 64.08)\n                    (140, 68.8)\n                    (160, 69.9)\n                    (180, 72.64)\n                    (200, 72.6)\n                    };\n                \\addplot[ % WANLI FLAN-T5\n                    color=color3,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (20, 48.76)\t\n                    (40, 52.28)\n                    (60, 59.64)\n                    (80, 61.52)\n                    (100, 69.38)\n                    (120, 74.4)\n                    (140, 75.08)\n                    (160, 75.4)\n                    (180, 75.22)\n                    (200, 75.64)\n                    };\n\n            \\nextgroupplot[\n                align = center,\n                title = {\\textsc{MedNLI}},\n                legend style={at={(-0.12,1.4)},anchor=south},\n                xmin=0, xmax=200,\n                ymin=30, ymax=90,\n                xtick={0, 50, 100, 150, 200},\n                axis x line*=bottom,\n                axis y line*=left,\n                ytick={30, 40, 50, 60, 70, 80, 90},\n                yticklabels=\\empty,\n                grid style=dashed,\n                x label style={at={(axis description cs:0.5,-0.1)},anchor=north},\n                y label style={at={(axis description cs:-0.12,0.5)},anchor=south},\n                xtick pos=bottom,\n                ytick pos=left,\n                grid=both,\n                ]\n                    \n                \\addplot[ % MedNLI T5\n                    color=frenchlilac,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    dashed,\n                    ]\n                    coordinates {\n                    (20, 30.52)\n                    (40, 33.33)\n                    (60, 63.29)\n                    (80, 82.21)\n                    (100, 84.53)\n                    (120, 85.79)\n                    (140, 85.94)\n                    (160, 84.88)\n                    (180, 86.43)\n                    (200, 85.94)\n                    };\n                \\addplot[ % MedNLI FLAN-T5\n                    color=frenchlilac,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (20, 58.86)\n                    (40, 81.29)\n                    (60, 83.97)\n                    (80, 86.08)\n                    (100, 86.43)\n                    (120, 88.26)\n                    (140, 87.06)\n                    (160, 88.47)\n                    (180, 87.83)\n                    (200, 87.55)\n                    };\n                    \n            \\nextgroupplot[\n                align = center,\n                title = {\\textsc{CondaQA}},\n                legend style={at={(-0.12,1.4)},anchor=south},\n                xmin=0, xmax=200,\n                ymin=30, ymax=90,\n                xtick={0, 50, 100, 150, 200},\n                axis x line*=bottom,\n                axis y line*=left,\n                xlabel={Number of Finetuning Steps},\n                ytick={30, 40, 50, 60, 70, 80, 90},\n                yticklabels=\\empty,\n                grid style=dashed,\n                x label style={at={(axis description cs:0.5,-0.2)},anchor=north},\n                y label style={at={(axis description cs:-0.12,0.5)},anchor=south},\n                xtick pos=bottom,\n                ytick pos=left,\n                grid=both,\n                ]\n                \\addplot[ % CondaQA T5\n                    color=camel,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    dashed,\n                    ]\n                    coordinates {\n                    (20, 42.54)\n                    (40, 47.75)\n                    (60, 47.71)\n                    (80, 47.5)\n                    (100, 47.4)\n                    (120, 47.73)\n                    (140, 49.1)\n                    (160, 57.33)\n                    (180, 70.03)\n                    (200, 69.05)\n                    };\n                \\addplot[ % CondaQA FLAN-T5\n                    color=camel,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (20, 55.19)\n                    (40, 65.72)\n                    (60, 65.41)\n                    (80, 70.5)\n                    (100, 72.68)\n                    (120, 74.01)\n                    (140, 70.87)\n                    (160, 72.51)\n                    (180, 74.56)\n                    (200, 69.71)\n                    };\n                    \n            \\nextgroupplot[\n                align = center,\n                title = {\\textsc{PubmedQA}},\n                legend style={at={(-0.12,1.4)},anchor=south},\n                xmin=0, xmax=200,\n                ymin=30, ymax=90,\n                xtick={0, 50, 100, 150, 200},\n                axis x line*=bottom,\n                axis y line*=left,\n                ytick={30, 40, 50, 60, 70, 80, 90},\n                yticklabels=\\empty,\n                grid style=dashed,\n                x label style={at={(axis description cs:0.5,-0.1)},anchor=north},\n                y label style={at={(axis description cs:-0.12,0.5)},anchor=south},\n                xtick pos=bottom,\n                ytick pos=left,\n                grid=both,\n                ]\n                    \n                \\addplot[ % PubmedQA T5\n                    color=color8,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    dashed,\n                    ]\n                    coordinates {\n                    (20, 33.8)\n                    (40, 58.4)\n                    (60, 56.2)\n                    (80, 67.2)\n                    (100, 63.4)\n                    (120, 68.4)\n                    (140, 64.8)\n                    (160, 67)\n                    (180, 68.4)\n                    (200, 68.8)\n                    };\n                \\addplot[ % PubmedQA FLAN-T5\n                    color=color8,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (20, 66.6)\n                    (40, 65.8)\n                    (60, 69.4)\n                    (80, 71.6)\n                    (100, 73.2)\n                    (120, 72.8)\n                    (140, 73.4)\n                    (160, 71.4)\n                    (180, 71)\n                    (200, 71.6)\n                    };\n                    \n            \\nextgroupplot[\n                align = center,\n                title = {\\textsc{CxC}},\n                legend style={at={(-0.12,1.4)},anchor=south},\n                xmin=0, xmax=200,\n                ymin=30, ymax=90,\n                xtick={0, 50, 100, 150, 200},\n                axis x line*=bottom,\n                axis y line*=left,\n                ytick={30, 40, 50, 60, 70, 80, 90},\n                yticklabels=\\empty,\n                grid style=dashed,\n                x label style={at={(axis description cs:0.5,-0.1)},anchor=north},\n                y label style={at={(axis description cs:-0.12,0.5)},anchor=south},\n                xtick pos=bottom,\n                ytick pos=left,\n                grid=both,\n                ]\n\n                \\addplot[ % CxC T5\n                    color=frenchblue,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    dashed,\n                    ]\n                    coordinates {\n                    (20, 15.57)\n                    (40, 51.54)\n                    (60, 55.01)\n                    (80, 56.67)\n                    (100, 57.24)\n                    (120, 55.66)\n                    (140, 57.97)\n                    (160, 59.38)\n                    (180, 56.86)\n                    (200, 61.83)\n                    };\n                \\addplot[ % CxC FLAN-T5\n                    color=frenchblue,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (20, 65.99)\n                    (40, 69.43)\n                    (60, 65.17)\n                    (80, 65.17)\n                    (100, 65.56)\n                    (120, 67.8)\n                    (140, 66.51)\n                    (160, 66.57)\n                    (180, 68.5)\n                    (200, 67.82)\n                    };\n        \\end{groupplot}\n        \n    \\begin{customlegend3}[legend columns=-1,legend style={at={(14.5,-0.5)},column sep=1ex},legend entries={Flan-T5 XL, T5-XL}]\n    \\addlegendimage{black,fill=black,sharp plot}\n    \\addlegendimage{black,fill=black,dashed}\n    \\end{customlegend3}\n    \n    \n    \\end{tikzpicture}\n    \\caption{\n    \\textbf{Flan-T5 convergences faster than T5 on single-task finetuning} for each of 5 Held-Out tasks from Flan finetuning.\n    } \n    \\label{fig:single-target-convergence}\n    \\end{centering}\n\\end{figure}\n\n\\subsection{Discussion}\n\\label{sec:discussion}\n\nOPT-IML \\citep{iyer2022optiml} presents the closest comparison to this work, including a similar collection of tasks, examples and techniques.\nHowever, while their used tasks are all publicly sourced, their collection, with templates, processing, and example mixing, is not released, and as a result cannot be easily compared.\n\\citet{iyer2022optiml} report that Flan-T5-XL (3B) and XXL (11B) outperforms OPT-IML-Max 175B on both MMLU and BBH.\nAs they discuss, these differences may arise from any combination of pre-training, model architecture, and instruction tuning.\nModel architecture and pretraining before instruction tuning can play a significant role \\citep{wang2022language}.\nBut there are many other details in instruction tuning that may vary between \\flantwo{} and OPT-IML.\nLikely candidates are are: example templatization, how the mixed input prompting procedures are used at training, and task composition.\n\nHow significant are each of these difference?\nWhile OPT-IML contains more tasks than \\flantwo{}, we estimate approximately $94\\% (2067 / 2207)$ are also used in the \\flantwo{} collection\\footnote{This is calculated using their definition of ``task'' (reported in \\citet{iyer2022optiml}'s Table 1), which does not deduplicate across collections.}, and very few tasks in \\flantwo{} are not contained in some format in OPT-IML.\nThis suggests the overall difference in task diversity is not significant when using a shared definition of ``task''.\nTask mixture rates also emphasize similar sources, including Flan 2021 (46\\% vs 20\\%), PromptSource/P3 (28\\% vs 45\\%), and Super-Natural Instructions (25\\% vs 25\\%), for \\flantwo{} and OPT-IML respectively.\\footnote{Note that 46\\% weight for \\flantwo{} is actually on Muffin from \\citet{chung2022scaling} which combines Flan 2021 with new dialog and program synthesis tasks.}\nOPT-IML's other collections (Crossfit, ExMix, T5, U-SKG) are not weighted significantly: 4\\%, 2\\%, 2\\%, 2\\% respectively.\n\nWe believe example templatization and the mixed prompt formats may pose the largest differences with OPT-IMLs instruction tuning.\nOur template repository was significantly updated from Flan 2021, adding variety not just in instructions, but also along dimensions.\nFor instance, the templatization procedure varies where the instruction is placed (before or after few-shot prompts), the spacing and separators between few-shot and Chain-of-Thought prompts, and the formatting permutations of answer options (and their targets) for multiple-choice examples, which sometimes includes and sometimes excludes answer options in the inputs or exemplars.\nWhile we do not have dedicated experiments comparing many iterations of development, we found these procedures dramatically augment input variety and showed repeated performance improvements.\nOur example templatizing procedure is open sourced for inspection and future work.\n\n\\section{Instruction Tuning Enhances Single-Task Finetuning}\n\\label{sec:single-target-ft}\n\nIn applied settings, machine learning practitioners deploy NLP models finetuned (FT) specifically for a single target task, usually where finetuning data is already available. While prior work has shown the benefits of intermediate finetuning~\\citep{pruksachatkun2020intermediate,vu-etal-2020-exploring} or multi-task finetuning~\\citep{aghajanyan-etal-2021-muppet,aribandi2021ext5} %(without instruction prompts) \nfor downstream tasks, this has not been studied extensively for instruction-tuned models.\n\nWe evaluate \\flantwo{} instruction tuning as an intermediary step before single target finetuning, to understand if Flan-T5 would serve as a better starting checkpoint for applied practitioners.\nWe evaluate three settings in \\cref{fig:single-target}: finetuning T5 directly on the target task as the conventional baseline (blue bars), using Flan-T5 without further finetuning (beige bars), and finetuning Flan-T5 further on the target task (red bars).\n\n\\vspace{-3mm}\n\\paragraph{Pareto Improvements to Single Task Finetuning}\nFor both sets of Held-In and Held-Out tasks examined, finetuning Flan-T5 offers a pareto improvement over finetuning T5 directly. In some instances, usually where finetuning data is limited for a task, Flan-T5 without further finetuning outperforms T5 with task finetuning.\n\n\\vspace{-3mm}\n\\paragraph{Faster Convergence \\& Computational Benefits}\nUsing Flan-T5 as a starting checkpoint has an added benefit in training efficiency.\nAs demonstrated in \\cref{fig:single-target-convergence}, Flan-T5 converges much more quickly than T5 during single target finetuning, as well as peaking at higher accuracies.\nThese convergence results also suggest there are strong green-AI incentives for the NLP community to adopt instruction-tuned models, like Flan-T5 for single-task finetuning, rather than conventional non-instruction-tuned models.\nWhile instruction tuning is more computationally-expensive than single-task finetuning, it is a one-time cost.\nOn the contrary, pretrained models that require extensive finetuning become more costly when aggregating over many millions of additional training steps \\citep{wu2022sustainable, bommasani2021opportunities}. Instruction-tuned models offer a promising solution to significantly reduce the amount of finetuning steps across a wide swathe of tasks, if they are adopted as a new standard starting point for single-task finetuning.\n\n\\section{Related Work}\n\\label{sec:rw}\n\n\\paragraph{Large Language Models} % General-Purpose Representations that Power Instruction-Tuned Models\nAs the foundation of instruction tuning, the practice of pretraining one general-purpose language representation that is useful for multiple downstream tasks has a long tradition that goes back at least \\citet{word2vec} and \\citet{pretrainedLSTM}. % Colbert & Weston 2008?\nIn 2018, \\citet{peters-etal-2018-deep} and \\citet{devlin-etal-2019-bert} cemented the paradigm of pretraining a large model on a large unsupervised corpus, and the field of NLP quickly converged to using these models which substantially outperform the prior art of non-pretrained task-specific LSTM models on all tasks. \nHowever, the dominate way to access that high-quality syntactic and semantic knowledge encoded in pretrained models was not to prompt them with instructions,\nbut to train an additional task-specific linear layer that maps the model activations into numerical class labels.\nA short year later, \\citet{radford2019language}, \\citet{raffel2020exploring}, and \\citet{lewis-etal-2020-bart} popularized the notion that downstream tasksand multiple taskscan be jointly learned by directly using the pretrained LM head to generate the answers in natural language (cf. task-specific numerical class labels), % Note GPT-2 was zero/few-shot with primitive instructions, while T5 was multitask trained but without semantically meaningful instruction prompts.\nthe task-general nature of these generative models became the precursor to many multitask transfer learning studies \\citep{mccann2018natural,khashabi-etal-2020-unifiedqa,ye2021crossfit,vu-etal-2020-exploring}, which in turn led to the first wave of instruction tuning as described in \\cref{sec:public-collections}.\n\nThe continuing advancement in research on the pretraining corpora, architectures and pretraining objectives of LMs also has a large impact on instruction tuning. \nAs of 2022, decoder-only left-to-right causal Transformers dominate the market of models larger than 100B \\citep{brown2020language,thoppilan2022lamda,rae2021scaling,chowdhery2022palm,hoffmann2022training}, \nand all models of such size class with fully public model parameters are decoder-only \\citep{gpt-j,scao2022bloom,zhang2022opt},\nthe decision of which are often due to better hardware and software framework support. % DeepSpeed only supported decoder-only.\nHowever, \\citet{raffel2020exploring}, \\citet{lewis-etal-2020-bart}, and \\citet{tay2022unifying} have consistently found that left-to-right causal language modeling is a suboptimal objective, while \\citet{tay2022transcending} and \\citet{wang2022language} particularly showed that a mixture of non-sequential objectives is much superior for downstream tasks with zero-shot and few-shot prompting.\nAn additional factor which remains under-explored is the relationship between pretraining corpora, instruction tuning, and downstream abilities.\nTypically, public models are all trained on one of a few public corpora: C4 \\citep{raffel2020exploring}, The Pile \\citep{gao2020pile}, or ROOTs \\citep{laurencconbigscience}.\n\n\\vspace{-2mm}\n\\paragraph{Instruction Tuning} In \\cref{sec:public-collections} we outline major developments in instruction tuning. \nOther important developments include the prospect of complimenting or replacing few-shot in-context learning-the currently predominate method of evaluating pretrained and instruction-tuned modelswith parameter-efficient tuning.\nAs standard finetuning of models larger than 100B requires a high number of accelerators with the right interconnects often too expensive even for many industry labs, parameter-efficient tuning (a.k.a. continuous or soft prompt tuning) shows that only updating a small subset of model parameters can reach comparable performance as fully tuning all model parameters (\\citealp{lester-etal-2021-power,vu-etal-2022-spot,lora}; see \\citealp{he2022towards} for a detailed analysis).\nNotably, \\citet{tfew2022} show that, due to the long sequence length of few-shot ICL and that the few-shot exemplars need to be repeatedly inferenced for evaluating every example, parameter-efficient tuning can be computationally cheaper and higher performing than in-context learning.\nFurther, \\citet{tfew2022}, \\citet{vu-etal-2022-spot}, \\citet{wei2021finetuned}, and \\citet{med-palm} collectively show that both single-task and multi-task parameter-efficient tuning can be productively combined with instruction tuning, either before or after regular full-model instruction tuning.\nThis line of work makes it easy for other researchers to build on top of a general-domain instruction-tuned model, \nand collect a custom instruction-tuning mixture for their use,\ne.g., with multiple modalities \\citep{2022_palm_saycan,huang2022inner,multimodal-inst-tuning} or special domains such as science and medicine \\citep{minerva,med-palm}. \n\n\\vspace{-2mm}\n\\paragraph{Problems Addressed by Instruction Tuning \\& Alignment Techniques}\nInstruction tuning is part of a line of work designed to ``align'' language models with more useful objectives and human preferences.\nIn the absence of such methods, language models are known to demonstrate toxic/harmful behaviour \\citep{sheng-etal-2019-woman,liang2021towards,wallace-etal-2019-universal}, generate non-factual information \\citep{maynez-etal-2020-faithfulness,longpre2021entity,devaraj-etal-2022-evaluating}, and other challenges in deployment and evaluation \\citep{zellers2019defending,mcguffie2020radicalization,talat2022you}.\nAnalyzing, evaluating and mitigating these problems pose a promising direction for future work \\citep{gao2022attributed,ganguli2022red}.\nInstruction tuning warrants greater investigation, as it has already demonstrated itself an encouraging remedy in reducing NLP bias metrics, as shown in \\citet{chung2022scaling}.\n\n\\section{Conclusions}\nThe new \\flantwo{} instruction tuning collection unifies the most popular prior public collections and their methods, while adding new templates and simple improvements like training with mixed prompt settings.\nThe resulting collection outperforms Flan 2021, P3++, Super-Natural Instructions, and OPT-IML-Max 175B on Held-In QA, NLI, and Chain-of-Thought tasks, and Held-Out MMLU and BBH, often by large margins.\nResults suggest this new collection serves as a more competitive starting point for researchers and practitioners interested in both generalizing to new instructions, or finetuning on a single new task.\n\n\\section*{Acknowledgements}\n\nWe would like to thank Ed H Chi, Xinyun Chen, and Colin Raffel for their advice and feedback on the paper.\n\n\\clearpage\n\n\\clearpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2101.02235v1.tex",
        "arXiv-2109.01653v1.tex",
        "arXiv-2301.13688v2.tex"
    ],
    "group_id": "group_30",
    "response": "### Title: Advances in Multi-Step Reasoning and Commonsense Reasoning in NLP: A Comparative Analysis of StrategyQA, CREAK, and Flan-T5\n\n### Introduction\n\nThe field of Natural Language Processing (NLP) has seen significant advancements in the past decade, with a particular focus on developing models that can perform complex reasoning tasks. These tasks include multi-step reasoning, where a model must infer a series of logical steps to derive an answer from a given context, and commonsense reasoning, where a model must understand and apply everyday knowledge to answer questions. The history of NLP research in these areas is marked by the development of numerous datasets and benchmarks designed to test and improve the reasoning capabilities of language models. However, these benchmarks often have limitations, such as explicit reasoning steps, limited diversity, or a focus on specific domains, which can hinder the true evaluation of a model's reasoning abilities.\n\nCurrent progress in NLP has led to the creation of large language models that can perform a wide range of tasks, from question answering to fact verification. These models, such as PaLM, Chinchilla, and Flan-T5, have shown impressive performance on various benchmarks, but they still struggle with tasks that require implicit reasoning and a broader range of commonsense skills. The challenge lies in designing datasets that can effectively probe these capabilities without introducing biases or simplifying the reasoning process.\n\nThis summary focuses on three recent research papers that introduce new datasets and methods for evaluating and improving multi-step reasoning and commonsense reasoning in NLP models. The first paper, StrategyQA, introduces a benchmark for implicit multi-step reasoning, the second paper, CREAK, presents a dataset for commonsense reasoning over entity knowledge, and the third paper, Flan-T5, studies the design decisions for instruction tuning methods. Each paper addresses a unique aspect of reasoning in NLP, contributing to the broader understanding of how language models can be trained and evaluated for more complex and diverse reasoning tasks.\n\n### Main Content of Each Paper\n\n#### StrategyQA: A Benchmark for Implicit Multi-Step Reasoning\n\n\\textbf{StrategyQA} is a novel benchmark designed to evaluate the ability of NLP models to perform implicit multi-step reasoning. The dataset consists of 2,780 questions, each requiring a reasoning process that is not explicitly stated in the question itself. Instead, the model must infer the necessary steps to answer the question, which is referred to as a \\emph{strategy}. For example, the question \"Did Aristotle use a laptop?\" requires the model to infer that the question is about temporal comparison, rather than being explicitly stated. Each question in StrategyQA is annotated with a decomposition into reasoning steps and evidence paragraphs from Wikipedia, which are necessary to answer each step of the decomposition. This annotation process is crucial for guiding and evaluating the QA process, as it allows researchers to assess whether a model is performing the correct reasoning steps.\n\nThe authors of StrategyQA faced several challenges in collecting these questions, including ensuring the questions were creative, diverse, and required implicit reasoning. To address these challenges, they developed a multi-step crowdsourcing pipeline that includes creative question writing (CQW), strategy question decomposition (SQD), and evidence matching (EVM). The CQW task primes workers with single terms from Wikipedia to write questions, which helps to reduce the overlap between questions and context and encourages creativity. The SQD task ensures that the questions are decomposed into valid reasoning steps, and the EVM task matches evidence paragraphs from Wikipedia to each step of the decomposition. This pipeline is designed to elicit high-quality questions that cover a broad range of reasoning skills and strategies.\n\nThe dataset includes a diverse set of reasoning skills, such as physical, biological, historical, temporal, and cultural reasoning. The authors also provide a detailed analysis of the dataset's quality, including the accuracy of human performance and the effectiveness of different retrieval methods. Human performance on the dataset is high, with experts able to answer 87\\% of the questions correctly, while the best baseline model reaches an accuracy of only 66\\%. This gap suggests that there is still room for improvement in developing models that can effectively perform implicit multi-step reasoning.\n\n#### CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge\n\n\\textbf{CREAK} is another benchmark that focuses on commonsense reasoning about specific entities, such as historical figures, animals, or fictional characters. This dataset consists of 13,000 claims about entities, each labeled as true or false, and is designed to test the ability of NLP models to combine entity knowledge with commonsense reasoning. For example, the claim \"Harry Potter can teach classes on how to fly on a broomstick\" requires knowledge about Harry Potter being a wizard and skilled at riding a broomstick, as well as the commonsense reasoning that if you're good at a skill, you can teach others how to do it.\n\nThe authors of CREAK collected the data on Amazon Mechanical Turk, where they asked crowdworkers to generate claims based on a set of popular Wikipedia entities. To ensure the claims were diverse and challenging, they limited the number of claims that a single worker could generate and used a minimal set of requirements for the claims. The claims were not generated based on explicit evidence from Wikipedia, but rather on the workers' background knowledge and commonsense reasoning. This approach helps to avoid the creation of claims that are too easy to verify or that rely on specific passages from Wikipedia.\n\nThe dataset is analyzed for its quality, including the types of reasoning required to verify the claims and the presence of artifacts or spurious correlations. The authors found that the dataset contains relatively few artifacts and that the claims are typically not subjective or ambiguous. They also evaluated the performance of several pre-trained language models on the dataset, finding that even large models struggle to achieve human-level performance. The best model, T5-3B, reaches an accuracy of 85.6\\% on the development set, but still falls short of the human performance of 99\\%. This suggests that there is a need for further research into models that can effectively combine entity knowledge with commonsense reasoning.\n\n#### Flan-T5: Enhancing Instruction Tuning for General-Purpose Language Models\n\n\\textbf{Flan-T5} is a study that evaluates the effectiveness of instruction tuning methods for large language models. The authors focus on the design decisions that enable Flan-T5 to outperform other instruction tuning collections, such as T0++ and Super-Natural Instructions. They conduct ablation studies to measure the impact of individual factors, such as task balancing, input inversion, and the use of mixed prompt settings (zero-shot, few-shot, and chain-of-thought). The results show that Flan-T5 converges faster and reaches higher performance than T5 models in single-task finetuning, suggesting that instruction-tuned models can serve as more efficient starting points for downstream applications.\n\nThe Flan-T5 collection includes a wide range of tasks and methods for instruction tuning, such as question answering, natural language inference, and chain-of-thought reasoning. The authors also provide a detailed analysis of the performance of Flan-T5 on various evaluation benchmarks, including MMLU and BIG-Bench Hard. They find that Flan-T5 outperforms other collections on most benchmarks, with improvements ranging from 3\\% to 17\\%. This suggests that the design decisions and methods used in Flan-T5 are effective for enhancing the generalization capabilities of language models.\n\n### Commonalities and Innovations\n\nAll three papers contribute to the field of NLP by introducing new datasets and methods for evaluating and improving reasoning capabilities in language models. However, each paper addresses a different aspect of reasoning and has unique innovations.\n\n\\textbf{StrategyQA} introduces a benchmark for implicit multi-step reasoning, which is a challenging task for language models. The dataset is annotated with decompositions and evidence paragraphs, which are necessary for answering the questions. This annotation process is innovative, as it allows researchers to assess whether a model is performing the correct reasoning steps. The authors also developed a multi-step crowdsourcing pipeline to elicit high-quality questions that cover a broad range of reasoning skills and strategies.\n\n\\textbf{CREAK} focuses on commonsense reasoning over entity knowledge, which is a combination of factual knowledge about specific entities and commonsense reasoning. The dataset is collected on Amazon Mechanical Turk, where crowdworkers generate claims based on their background knowledge and commonsense reasoning. This approach helps to avoid the creation of claims that are too easy to verify or that rely on specific passages from Wikipedia. The authors also provide a detailed analysis of the dataset's quality and evaluate the performance of several pre-trained language models on the dataset.\n\n\\textbf{Flan-T5} studies the design decisions that enable instruction-tuned models to outperform other collections. The authors conduct ablation studies to measure the impact of individual factors, such as task balancing, input inversion, and the use of mixed prompt settings. They find that these factors are critical for enhancing the generalization capabilities of language models. The Flan-T5 collection includes a wide range of tasks and methods for instruction tuning, and the authors provide a detailed analysis of the performance of Flan-T5 on various evaluation benchmarks.\n\n### Comparison of Results and Discussion\n\nThe results of the three papers highlight the challenges and opportunities in developing NLP models that can perform complex reasoning tasks. \\textbf{StrategyQA} shows that human performance on the dataset is high, with experts able to answer 87\\% of the questions correctly, while the best baseline model reaches an accuracy of only 66\\%. This gap suggests that there is still room for improvement in developing models that can effectively perform implicit multi-step reasoning.\n\n\\textbf{CREAK} demonstrates that even large language models struggle to achieve human-level performance on the dataset, with the best model, T5-3B, reaching an accuracy of 85.6\\% on the development set, but still falling short of the human performance of 99\\%. This suggests that there is a need for further research into models that can effectively combine entity knowledge with commonsense reasoning.\n\n\\textbf{Flan-T5} shows that instruction-tuned models can outperform other collections on various evaluation benchmarks, with improvements ranging from 3\\% to 17\\%. This suggests that the design decisions and methods used in Flan-T5 are effective for enhancing the generalization capabilities of language models.\n\nThe differences in the results of the three papers can be attributed to the unique challenges and design decisions of each dataset. \\textbf{StrategyQA} focuses on implicit multi-step reasoning, which is a challenging task for language models. \\textbf{CREAK} focuses on commonsense reasoning over entity knowledge, which requires models to combine factual knowledge with commonsense reasoning. \\textbf{Flan-T5} focuses on instruction tuning, which is designed to enhance the generalization capabilities of language models.\n\n### Conclusion\n\nThe three papers discussed in this summary contribute to the broader understanding of how language models can be trained and evaluated for more complex and diverse reasoning tasks. \\textbf{StrategyQA} introduces a benchmark for implicit multi-step reasoning, which is a challenging task for language models. \\textbf{CREAK} presents a dataset for commonsense reasoning over entity knowledge, which requires models to combine factual knowledge with commonsense reasoning. \\textbf{Flan-T5} studies the design decisions that enable instruction-tuned models to outperform other collections, suggesting that these methods can enhance the generalization capabilities of language models.\n\nThe main findings of the papers suggest that there is still room for improvement in developing models that can effectively perform implicit multi-step reasoning and combine entity knowledge with commonsense reasoning. The authors of \\textbf{StrategyQA} and \\textbf{CREAK} provide detailed analyses of their datasets' quality and evaluate the performance of several pre-trained language models on the datasets. The authors of \\textbf{Flan-T5} conduct ablation studies to measure the impact of individual factors on the performance of instruction-tuned models.\n\nFuture research directions include developing more effective methods for instruction tuning, improving the ability of language models to perform implicit multi-step reasoning, and combining entity knowledge with commonsense reasoning. The papers also highlight the importance of careful data collection and annotation in designing effective benchmarks for evaluating the reasoning capabilities of language models. By addressing these challenges, researchers can develop more robust and versatile language models that can perform a wide range of reasoning tasks.\n\n### Acknowledgements\n\nThe authors of the papers would like to thank the REVIZ team at Allen Institute for AI, the NSF Grant IIS-1814522, and the Mechanical Turk annotators who participated in their tasks. They also acknowledge the contributions of other researchers and practitioners in the field of NLP, who have helped to advance the understanding of reasoning capabilities in language models.\n\n### Tables and Figures\n\n\\textbf{Table 1:} Performance of Baseline Models on StrategyQA\n\\begin{table}[ht]\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{l|cc}\n         Model & Accuracy & Recall@10 \\\\ \\toprule\n         Majority Label & 53.9 & - \\\\\n         RoBERTa*$_\\varnothing$ & 63.6 $\\pm$ 1.3 & - \\\\\n         RoBERTa$_{\\text{IR-Q}}$ & 53.6 $\\pm$ 1.0 & 0.174 \\\\\n         RoBERTa*$_{\\text{IR-Q}}$ & 63.6 $\\pm$ 1.0 & 0.174  \\\\\n         RoBERTa*$_{\\text{IR-D}}$ & 61.7 $\\pm$ 2.2 & 0.195 \\\\  \\hline \\hline\n         RoBERTa*$_{\\text{IR-ORA-D}}$ & 62.0 $\\pm$ 1.3 & 0.282 \\\\\n         RoBERTa*$_{\\text{ORA-P}}$ & 70.7 $\\pm$ 0.6 & - \\\\\n         RoBERTa*}$^{\\text{last-step-raw}}_\\text{ORA-P-D}$ & 65.2 $\\pm$ 1.4 & -\\\\\n         RoBERTa*}$^{\\text{last-step}}_\\text{ORA-P-D}$ & 72.0 $\\pm$ 1.0 & -\\\\\n    \\end{tabular}\n    \\caption{Performance of baseline models on the StrategyQA dataset.}\n    \\label{tab:baseline_results}\n\\end{table}\n\n\\textbf{Table 2:} Performance of Closed-Book Approaches on CREAK\n\\begin{table}[ht]\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{l | ccc}\n         Model & Dev & Test & Contrast \\\\ \\toprule\n         Majority Label & 51.6 & 51.6  & 50.0 \\\\ \n         RoBERTa$_\\varnothing$(20Q) & 49.6 & 48.4 & 50.0 \\\\\n         RoBERTa$_\\varnothing$(20Q+BoolQ) & 63.6 & 63.6 & 61.5 \\\\\n         RoBERTa$_\\varnothing$(BoolQ) & 63.6 & 63.6 & 61.5 \\\\\n         RoBERTa$_{\\text{IR-Q}}$(BoolQ) & 53.6 & 53.6 & 50.0 \\\\\n         T5-3b & 72.9 & 76.7 & 61.5 \\\\\n         Human (averaged) & 96.3 & -- & 92.2 \\\\\n         Human (ensemble) & 99.0 & -- & 99.0 \\\\\n    \\end{tabular}\n    \\caption{Performance of closed-book approaches on the CREAK dataset.}\n    \\label{tab:creak_results}\n\\end{table}\n\n\\textbf{Table 3:} Ablation Studies on Flan-T5\n\\begin{table}[ht]\n    \\centering\n    \\footnotesize\n    \\begin{tabular}{l | ccccc}\n         Model & Held-In & CoT & MMLU & BBH & BBH-CoT \\\\\n         \\midrule\n         T5-XL \\flantwo{} & \\textbf{73.8 / 74.8} & 35.8 / \\textbf{34.1} & \\textbf{50.3 / 52.4} & 26.2 / \\textbf{39.3} & \\textbf{33.9 / 35.2} \\\\\n         - CoT & 73.3 / 73.2 & 28.8 / 24.6 & 47.5 / 46.9 & 18.2 / 30.0 & 18.2 / 12.0 \\\\\n         - Input Inversion & \\textbf{73.8} / 74.1 & 32.2 / 23.5 & 41.7 / 41.2 & 18.4 / 24.2 & 15.7 / 13.0 \\\\\n         - Mixture Balancing & 71.2 / 73.1 & 32.3 / 30.5 & 45.4 / 45.8 & 15.1 / 24.3 & 13.8 / 15.4 \\\\\n         - Few Shot Templates & 72.5 / 62.2 & \\textbf{38.9} / 28.6 & 47.3 / 38.7 & 27.6 / 30.8 & 18.6 / 23.3 \\\\\n         \\midrule\n         T5-XL Flan 2021 & 68.4 / 56.3 & 24.6 / 22.7 & 41.4 / 34.8 & \\textbf{28.1} / 28.3 & 26.0 / 26.9 \\\\\n         T5-XL P3++ & 70.5 / 62.8 & 25.6 / 25.6 & 46.1 / 34.1 & 26.0 / 30.8 & 23.4 / 26.1 \\\\\n         T5-XL Super-Natural Inst. & 50.3 / 42.2 & 13.8 / 14.3 & 35.6 / 31.1 & 10.4 / 15.6 & 8.0 / 12.5 \\\\\n         OPT-IML-Max 30B & - & - & 46.3 / 43.2 & -- / 30.9 & - \\\\\n         OPT-IML-Max 175B & - & - & 49.1 / 47.1 & -- / 35.7 & - \\\\\n         \\midrule\n         \\flantwo{} - Next Best T5-XL & \\textcolor{forestgreen}{+3.3} / \\textcolor{forestgreen}{+12} & \\textcolor{forestgreen}{+10.2} / \\textcolor{forestgreen}{+8.5} & \\textcolor{forestgreen}{+4.2} / \\textcolor{forestgreen}{+17.6} & \\textcolor{color3}{-1.9} / \\textcolor{forestgreen}{+8.5} & \\textcolor{forestgreen}{+7.9} / \\textcolor{forestgreen}{+8.3}\\\\\n    \\end{tabular}\n    \\caption{Ablation studies on the Flan-T5 dataset.}\n    \\label{tab:flan_ablations}\n\\end{table}\n\n\\textbf{Figure 1:} Artifact Statistics of StrategyQA and CREAK\n\\begin{figure}[ht]\n     \\centering\n     \\begin{minipage}{0.48\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{artifacts_train_2.pdf}\n         \\caption*{(a) StrategyQA}\n         \\label{fig1:dev}\n     \\end{minipage}\n     \\hspace{10pt}\n     \\begin{minipage}{0.48\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{artifacts_fever_train_3.pdf}\n         \\caption*{(b) CREAK}\n         \\label{fig:contrast}\n     \\end{minipage}\n     \\caption{Artifact statistics of StrategyQA and CREAK datasets.}\n     \\label{fig:competency}\n\\end{figure}\n\n\\textbf{Figure 2:} Performance Scaling Laws for Flan-T5\n\\begin{figure}[ht]\n    \\begin{tikzpicture}\n        \\pgfplotsset{footnotesize,samples=10}\n        \\begin{groupplot}[\n            group style = {group size = 2 by 1, horizontal sep = 42pt},\n            width = 8cm, \n            height = 6.5cm]\n            \\nextgroupplot[\n                align = center,\n                title = {\\textsc{Held-In Task Performance}},\n                legend style={at={(-0.12,1.4)},anchor=south},\n                xmin=-0.01, xmax=2000,\n                ymin=40, ymax=95,\n                xtick={10, 50, 100, 200, 400, 800, 1600},\n                axis x line*=bottom,\n                axis y line*=left,\n                xlabel={Number of Tasks},\n                ylabel={Accuracy (\\%)},\n                ytick={0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100},\n                grid style=dashed,\n                x label style={at={(axis description cs:0.5,-0.1)},anchor=north},\n                y label style={at={(axis description cs:-0.12,0.5)},anchor=south},\n                xtick pos=bottom,\n                ytick pos=left,\n                grid=both,\n                ]\n                \\addplot[ % T5 Small\n                    color=color1,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (8, 51)\n                    (25, 48.5)\n                    (50, 49.8)\n                    (100, 51.9)\n                    (200, 52.3)\n                    (400, 49.1)\n                    (800, 50.1)\n                    (1873, 46.6)\n                    };\n                \\addplot[ % T5 Base\n                    color=color2,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (8, 55.1)\n                    (25, 54.6)\n                    (50, 55.2)\n                    (100, 61.4)\n                    (200, 62.6)\n                    (400, 61.2)\n                    (800, 60)\n                    (1873, 50.7)\n                    };\n                \\addplot[ % T5 Large\n                    color=color3,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (8, 64.7)\n                    (25, 65.2)\n                    (50, 68.7)\n                    (100, 70.5)\n                    (200, 73.1)\n                    (400, 71.3)\n                    (800, 70)\n                    (1873, 67.9)\n                    };\n                \\addplot[ % T5 XL\n                    color=color4,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (8, 69.7)\n                    (25, 69.2)\n                    (50, 72.2)\n                    (100, 73.8)\n                    (200, 74.6)\n                    (400, 74.7)\n                    (800, 74)\n                    (1873, 72.7)\n                    };\n                \\addplot[ % T5 XXL\n                    color=color5,\n                    mark=*,\n                    mark size=1.5pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (8, 80.7)\n                    (25, 80.3)\n                    (50, 81)\n                    (100, 82.3)\n                    (200, 83)\n                    (400, 83.1)\n                    (800, 81.7)\n                    (1873, 81.1)\n                    };\n                \\addplot[ % Peak T5-Small\n                    color=color11,\n                    mark=star,\n                    mark size=4pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (200, 52.3)\n                    };\n                \\addplot[ % Peak T5-Base\n                    color=color11,\n                    mark=star,\n                    mark size=4pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (200, 62.6)\n                    };\n                \\addplot[ % Peak T5-Large\n                    color=color11,\n                    mark=star,\n                    mark size=4pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (200, 73.1)\n                    };\n                \\addplot[ % Peak T5-XL\n                    color=color11,\n                    mark=star,\n                    mark size=4pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (400, 74.7)\n                    };\n                \\addplot[ % Peak T5-XXL\n                    color=color11,\n                    mark=star,\n                    mark size=4pt,\n                    line width=1pt,\n                    ]\n                    coordinates {\n                    (400, 83.1)\n                    };\n                    \\node[color=gray,font=\\scriptsize] at (axis cs: 1550,84) {XXL};\n                    \\node[color=gray,font=\\scriptsize] at (axis cs: 1550,76) {XL};\n                    \\node[color=gray,font=\\scriptsize] at (axis cs: 1550,65) {Large};\n                    \\node[color=gray,font=\\scriptsize] at (axis cs: 1550,57) {Base};\n                    \\node[color=gray,font=\\scriptsize] at (axis cs: 1550,43) {Small};\n        \\end{groupplot}\n    \\end{tikzpicture}\n    \\caption{Performance scaling laws for Flan-T5 on Held-In tasks.}\n    \\label{fig:scaling-laws}\n\\end{figure}\n\n### Future Research Directions\n\nThe papers discussed in this summary highlight several future research directions in the field of NLP. One direction is to develop more effective methods for instruction tuning, such as task balancing, input inversion, and the use of mixed prompt settings. Another direction is to improve the ability of language models to perform implicit multi-step reasoning and combine entity knowledge with commonsense reasoning. Additionally, researchers can explore the relationship between pretraining corpora, instruction tuning, and downstream abilities, as well as the impact of model architecture and pretraining objectives on reasoning performance.\n\nBy addressing these challenges, researchers can develop more robust and versatile language models that can perform a wide range of reasoning tasks. This can help to narrow the gap between human and machine performance on complex reasoning tasks, and make language models more useful for real-world applications."
}