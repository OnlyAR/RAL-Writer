{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{ShapeNet: An Information-Rich 3D Model Repository}\n\n\\begin{document}\n\n\\title{ShapeNet: An Information-Rich 3D Model Repository}\n\n\\author[1]{Angel X. Chang}\n\\author[2]{Thomas Funkhouser}\n\\author[1]{Leonidas Guibas}\n\\author[1]{Pat Hanrahan}\n\\author[3]{Qixing Huang}\n\\author[3]{Zimo Li}\n\\author[1]{Silvio Savarese}\n\\author[1]{Manolis Savva\\thanks{Contact authors: \\texttt{\\{msavva,haosu\\}@cs.stanford.edu}}}\n\\author[2]{Shuran Song}\n\\author[1]{Hao Su\\protect\\contactAuthorMark}\n\\author[2]{Jianxiong Xiao}\n\\author[1]{Li Yi}\n\\author[2]{Fisher Yu}\n\\affil[1]{Stanford University}\n\\affil[2]{Princeton University}\n\\affil[3]{Toyota Technological Institute at Chicago}\n\n\\maketitle\n\n\\begin{abstract}\nWe present \\emph{ShapeNet}: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects.  ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy.  It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations.  Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.\n\\end{abstract}\n\n\\section{Introduction}\nRecent technological developments have led to an explosion in the amount of 3D data that we can generate and store.  Repositories of 3D CAD models are expanding continuously, predominantly through aggregation of 3D content on the web.  RGB-D sensors and other technology for scanning and reconstruction are providing increasingly higher fidelity geometric representations of objects and real environments that can eventually become CAD-quality models.\n\nAt the same time, there are many open research problems due to fundamental challenges in using 3D content.  Computing segmentations of 3D shapes, and establishing correspondences between them are two basic problems in geometric shape analysis.  Recognition of shapes from partial scans is a research goal shared by computer graphics and vision.  Scene understanding from 2D images is a grand challenge in vision that has recently benefited tremendously from 3D CAD models~\\cite{Song:2014:slidingwindow,wu:2014:3d}. Navigation of autonomous robots and planning of grasping manipulations are two large areas in robotics that benefit from an understanding of 3D shapes.  At the root of all these research problems lies the need for attaching semantics to representations of 3D shapes, and doing so at large scale.\n\nRecently, data-driven methods from the machine learning community have been exploited by researchers in vision and NLP (natural language processing). ``Big data'' in the visual and textual domains has led to tremendous progress towards associating semantics with content in both fields.  Mirroring this pattern, recent work in computer graphics has also applied similar approaches to specific problems in the synthesis of new shape variations~\\cite{kalogerakis:2012:probabilistic} and new arrangements of shapes~\\cite{fisher:2012:example}.  However, a critical bottleneck facing the adoption of data-driven methods for 3D content is the lack of large-scale, curated datasets of 3D models that are available to the community.\n\nMotivated by the far-reaching impact of dataset efforts such as the Penn Treebank~\\cite{marcus:1993:penntreebank}, WordNet~\\cite{miller:1995:wordnet} and ImageNet~\\cite{deng:2009:imagenet}, which collectively have tens of thousands of citations, we propose establishing \\emph{ShapeNet}: a large-scale 3D model dataset.  Making a comprehensive, semantically enriched shape dataset available to the community can have immense impact, enabling many avenues of future research.\n\nIn constructing ShapeNet we aim to fulfill several goals:\n\\begin{itemize}\\denselist\n \\item Collect and centralize 3D model datasets, helping to organize effort in the research community.\n \\item Support data-driven methods requiring 3D model data.\n \\item Enable evaluation and comparison of algorithms for fundamental tasks involving geometry (e.g., segmentation, alignment, correspondence).\n \\item Serve as a knowledge base for representing real-world objects and their semantics.\n\\end{itemize}\n\nThese goals imply several desiderata for ShapeNet:\n\\begin{itemize}\\denselist\n \\item Broad and deep coverage of objects observed in the real world, with thousands of object categories and millions of total instances.\n \\item Categorization scheme connected to other modalities of knowledge such as 2D images and language.\n \\item Annotation of salient physical attributes on models, such as canonical orientations, planes of symmetry, and part decompositions.\n \\item Web-based interfaces for searching, viewing and retrieving models in the dataset through several modalities: textual keywords, taxonomy traversal, image and shape similarity search.\n\\end{itemize}\n\nAchieving these goals and providing the resulting dataset to the community will enable many advances and applications in computer graphics and vision.\n\nIn this report, we first situate ShapeNet, explaining the overall goals of the effort and the types of data it is intended to contain, as well as motivating the long-term vision and infrastructural design decisions (\\Cref{sec:shapenet}).  We then describe the acquisition and validation of annotations collected so far (\\Cref{sec:acquisition}), summarize the current state of all available ShapeNet datasets, and provide basic statistics on the collected annotations (\\Cref{sec:statistics}).  We end with a discussion of ShapeNet's future trajectory and connect it with several research directions (\\Cref{sec:conclusion}).\n\n\\section{Background and Related Work}\nThere has been substantial growth in the number of of 3D models available online over the last decade, with repositories like the Trimble 3D Warehouse providing millions of 3D polygonal models covering thousands of object and scene categories.  Yet, there are few collections of 3D models that provide useful organization and annotations.  Meaningful textual descriptions are rarely provided for individual models, and online repositories are usually either unorganized or grouped into gross categories (e.g., furniture, architecture, etc.~\\cite{GAMMA}).  As a result, they have been poorly utilized in research and applications.\n\nThere have been previous efforts to build organized collections of 3D models (e.g.,~\\cite{AIM@SHAPE,GAMMA}). However, they have provided quite small datasets, covered only a small number of semantic categories, and included few structural and semantic annotations. Most of these previous collections have been developed for evaluating shape retrieval and classification algorithms. For example, datasets are created annually for the Shape Retrieval Contest (SHREC) that commonly contains sets of models organized in object categories. However, those datasets are very small --- the most recent SHREC iteration in 2014~\\cite{li:2014:shrec} contains a ``large'' dataset with around 9,000 models consisting of models from a variety of sources organized into 171 categories (Table \\ref{fig:shrec2014datasets}).\n\nThe Princeton Shape Benchmark is probably the most well-known and frequently used 3D shape collection to date (with over 1000 citations)~\\cite{shilane:2004:princeton}.  It contains around 1,800 3D models grouped into 90 categories, but has no annotations beyond category labels.  Other commonly-used datasets contain segmentations~\\cite{Chen:2009:segmentationbenchmark}, correspondences~\\cite{Kim:2012:fuzzycorres, Kim:2013:partbasedtemplates}, hierarchies~\\cite{Liu:2014:creating}, symmetries~\\cite{Kim:2010:mobius}, salient features~\\cite{Chen:2012:schelling}, semantic segmentations and labels~\\cite{Xiao:2013:sun3d}, alignments of 3D models with images~\\cite{xiang:2014:pascal3d}, semantic ontologies~\\cite{AIM@SHAPE}, and other functional annotations --- but again only for small size datasets.  For example, the Benchmark for 3D Mesh Segmentation contains just 380 models in 19 object classes~\\cite{Chen:2009:segmentationbenchmark}.\n\nIn contrast, there has been a flurry of activity on collecting, organizing, and labeling large datasets in computer vision and related fields.  For example, ImageNet \\cite{deng:2009:imagenet} provides a set of 14M images organized into 20K categories associated with ``synsets'' of WordNet \\cite{miller:1995:wordnet}.  LabelMe provides segmentations and label annotations of hundreds of thousands of objects in tens of thousands of images \\cite{russell:2009:labelme}.  The SUN dataset provides 3M annotations of objects in 4K categories appearing in 131K images of 900 types of scenes. Recent work demonstrated the benefit of a large dataset of 120K 3D CAD models in training a convolutional neural network for object recognition and next-best view prediction in RGB-D data~\\cite{wu:2014:3d}. Large datasets such as this and others (e.g.,~\\cite{krause:2013:finegrain, liebelt:2010:multi}) have revitalized data-driven algorithms for recognition, detection, and editing of images, which have revolutionized computer vision.\n\nSimilarly, large collections of annotated 3D data have had great influence on progress in other disciplines.  For example, the Protein Data Bank \\cite{berman:2000:pdb} provides a database with 100K protein 3D structures, each labeled with its source and links to structural and functional annotations \\cite{laskowski:1997:pdbsum}.  This database is a common repository of {\\em all} 3D protein structures solved to date and provides a shared infrastructure for the collection and transfer of knowledge about each entry. It has accelerated the development of data-driven algorithms, facilitated the creation of benchmarks, and linked researchers and industry from around the world. We aim to provide a similar resource for 3D models of everyday objects.\n\n\\begin{table*}\n\\centering{\\small\n\\begin{tabular}{lcccc}\nBenchmarks   &          Types           &      \\# models       &       \\# classes        & Avg \\# models per class \\\\ \\midrule\nSHREC14LSGTB &         Generic          &        8,987         &           171           &           53            \\\\ \\midrule\nPSB          &         Generic          & 907+907 (train+test) &   90+92 (train+test)    &   10+10 (train+test)    \\\\\nSHREC12GTB   &         Generic          &         1200         &           60            &           20            \\\\\nTSB          &         Generic          &        10,000        &           352           &           28            \\\\\nCCCC         &         Generic          &         473          &           55            &            9            \\\\\nWMB          & Watertight (articulated) &         400          &           20            &           20            \\\\\nMSB          &       Articulated        &         457          &           19            &           24            \\\\\nBAB          &       Architecture       &         2257         & 183+180 (function+form) &  12+13 (function+form)  \\\\\nESB          &           CAD            &         867          &           45            &           19\n\\end{tabular}\n}\n\\caption{\\small{Source datasets from SHREC 2014: \\emph{Princeton Shape Benchmark (PSB)}~\\cite{shilane:2004:princeton}, \\emph{SHREC 2012 generic Shape Benchmark (SHREC12GTB)}~\\cite{li:2012:shrec}, \\emph{Toyohashi Shape Benchmark (TSB)}~\\cite{tatsuma:2012:large}, \\emph{Konstanz 3D Model Benchmark (CCCC)}~\\cite{vranic:2004:3d}, \\emph{Watertight Model Benchmark (WMB)}~\\cite{veltkamp:2007:shrec}, \\emph{McGill 3D Shape Benchmark (MSB)}~\\cite{zhang:2005:retrieving}, \\emph{Bonn Architecture Benchmark (BAB)}~\\cite{wessel:2009:3d}, \\emph{Purdue Engineering Shape Benchmark (ESB)}~\\cite{jayanti:2006:developing}.}}\n\\label{fig:shrec2014datasets}\n\\end{table*}\n\n\\section{ShapeNet: An Information-Rich 3D Model Repository}\n\\label{sec:shapenet}\nShapeNet is a large, information-rich repository of 3D models. It contains models spanning a multitude of semantic categories. Unlike previous 3D model repositories, it provides extensive sets of annotations for every model and links between models in the repository and other multimedia data outside the repository.\n\nLike ImageNet, ShapeNet provides a view of the contained data in a hierarchical categorization according to WordNet synsets (Figure~\\ref{fig:taxonomy-ui}). Unlike other model repositories, ShapeNet also provides a rich set of annotations for each shape and correspondences between shapes. The annotations include geometric attributes such as upright and front orientation vectors, parts and keypoints, shape symmetries (reflection plane, other rotational symmetries), and scale of object in real world units. These attributes provide valuable resources for processing, understanding and visualizing 3D shapes in a way that is aware of the semantics of the shape.\n\nWe have currently collected approximately 3 million shapes from online 3D model repositories, and categorized 300 thousand of them against the WordNet taxonomy.  We have also annotated a subset of these models with shape properties such as upright and front orientations, symmetries, and hierarchical part decompositions. We are continuing the process of expanding the annotated set of models and also collecting new models from new data sources.\n\nIn the following sections, we discuss how 3D models are collected for ShapeNet, what annotations will be added, how those annotations will be generated, how annotations will be updated as the dataset evolves over time, and what tools will be provided for the community to search, browse, and utilize existing data, as well as contribute new data.\n\n\\begin{figure}\n\\includegraphics[width=\\columnwidth]{fig/taxonomy-ui}\n\\caption{Screenshot of the online ShapeNet taxonomy view, organizing contained 3D models under WordNet synsets.}\n\\label{fig:taxonomy-ui}\n\\end{figure}\n\n\\subsection{Data Collection}\n\nThe raw 3D model data for ShapeNet comes from public online repositories or existing research datasets. ShapeNet is intended to be an evolving repository with regular updates as more and more 3D models become available, as more people contribute annotations, and as the data captured with new 3D sensors become prevalent.\n\nWe have collected 3D polygonal models from two popular public repositories: Trimble 3D Warehouse\\footnote{https://3dwarehouse.sketchup.com/} and Yobi3D\\footnote{https://yobi3d.com}. The Trimble 3D Warehouse contains 2.4M user-designed 3D models and scenes.  Yobi3D contains 350K additional models collected from a wide range of other online repositories. Together, they provide a diverse set of shapes from a broad set of object and scene categories --- e.g., many organic shape categories (e.g., humans and mammals), which are rare in Warehouse3D, are plentiful in Yobi3D.  For more detailed statistics on the currently available ShapeNet models refer to \\Cref{sec:statistics}.\n\nThough the tools developed for this project will be general-purpose, we intend to include only 3D models of objects encountered by people in the everyday world.  That is, it will not include CAD mechanical parts, molecular structures, or other domain-specific objects.  However, we will include scenes (e.g., office), objects (e.g., laptop computer), and parts of objects (e.g., keyboard).  Models are organized under WordNet~\\cite{miller:1995:wordnet} noun ``synsets'' (synonym sets).  WordNet provides a broad and deep taxonomy with over 80K distinct synsets representing distinct noun concepts arranged as a DAG network of hyponym relationships (e.g., ``canary'' is a hyponym of ``bird'').  This taxonomy has been used by ImageNet to describe categories of objects at multiple scales~\\cite{deng:2009:imagenet}.  Though we first use WordNet due to its popularity, the ShapeNet UI is designed to allow multiple views into the collection of shapes that it contains, including different taxonomy views and faceted navigation.\n\n\\subsection{Annotation Types}\n\\label{sec:annotation-types}\n\n\\begin{figure*}\n  \\centering\n  \\includegraphics[width=\\textwidth]{fig/shapenet-view}\n  \\caption{\\small{ShapeNet annotations illustrated for an example chair model. \\emph{Left:} links to the WordNet taxonomy provide definitions of objects, \\texttt{is-a} and \\texttt{has-a} relations, and a connection to images from ImageNet. \\emph{Middle-left:} shape is aligned to a consistent upright and front orientation, and symmetries are computed \\emph{Middle-right:} hierarchical decomposition of shape into parts on which various attributes are defined: names, symmetries, dimensions, materials, and masses. \\emph{Right:} part-to-part and point-to-point connections are established at all levels within ShapeNet producing a dense and semantically rich network of correspondences. The gray background indicates annotations that are currently ongoing and not yet available for release.}}\n  \\label{fig:shapenet-view}\n\\end{figure*}\n\nWe envision ShapeNet as far more than a collection of 3D models. ShapeNet will include a rich set of annotations that provide semantic information about those models, establish links between them, and links to other modalities of data (e.g., images). These annotations are exactly what make ShapeNet uniquely valuable.  \\Cref{fig:shapenet-view} illustrates the value of this dense network of interlinked attributes on shapes, which we describe below.\n\n\\mypara{Language-related Annotations:} Naming objects by their basic category is useful for indexing, grouping, and linking to related sources of data.  As described in the previous section, we organize ShapeNet based on the WordNet~\\cite{miller:1995:wordnet} taxonomy.  Synsets are interlinked with various relations, such as hyper and hyponym, and part-whole relations.  Due to the popularity of WordNet, we can leverage other resources linked to WordNet such as ImageNet, ConceptNet, Freebase, and Wikipedia. In particular, linking to ImageNet~\\cite{deng:2009:imagenet} will help transport information between images and shapes.  We assign each 3D model in ShapeNet to one or more synsets in the WordNet taxonomy (i.e., we populate each synset with a collection of shapes). Please refer to \\Cref{sec:acquisition:categoryannotation} for details on the acquisition and validation of basic category annotations.  Future planned annotations include natural language descriptions of objects and object part-part relation descriptions.\n\n\\mypara{Geometric Annotations:} A critical property that distinguishes ShapeNet from image and video datasets is the fidelity with which 3D geometry represents real-world structures. We combine algorithmic predictions and manual annotations to organize shapes by category-level geometric properties and further derive rich geometric annotations from the raw 3D model geometry.\n\\begin{itemize}\n\\item \\textbf{Rigid Alignments:}\nEstablishing a consistent canonical orientation (e.g., upright and front) for every model is important for various tasks such as visualizing shapes~\\cite{Kim:2012:fuzzycorres}, shape classification~\\cite{huang:2013:fineGrained} and shape recognition~\\cite{wu:2014:3d}. Fortunately, most raw 3D model data is by default placed in an upright orientation, and the front orientations are typically aligned with an axis.  This allows us to use a hierarchical clustering and alignment approach to ensure consistent rigid alignments within each category (see \\Cref{sec:alignment}).\n\\item \\textbf{Parts and Keypoints:} Many shapes contain or have natural decompositions into important parts, as well as significant keypoints related to both their geometry and their semantics. For example, often different materials are associated with different parts. We intend to capture as much of that as possible into ShapeNet.\n\\item \\textbf{Symmetry:} Bilateral symmetry planes and rotational symmetries are prevalent in artificial and natural objects, and deeply connected with the alignment and functionality of shapes. We refer to \\Cref{sec:symmetry} for more details on how we compute symmetries for the shapes in ShapeNet.\n\\item \\textbf{Object Size:} Object size is useful for many applications, such as reducing the hypothesis space in object recognition. Size annotations are discussed in \\Cref{sec:shapenetsem}.\n\\end{itemize}\n\n\\mypara{Functional Annotations:} Many objects, especially man-made artifacts such as furniture and appliances, can be used by humans. Functional annotations describe these usage patterns. Such annotations are often highly correlated with specific regions of an object. In addition, it is often related with the specific type of human action. ShapeNet aims to store functional annotations at the global shape level and at the object part level.\n\\begin{itemize}\n\\item \\textbf{Functional Parts:} Parts are critical for understanding object structure, human activities involving a 3D shape, and ergonomic product design. We plan to annotate parts according to their function --- in fact the very definition of parts has to be based on both geometric and functional criteria.\n\\item \\textbf{Affordances:} We are interested in affordance annotations that are function and activity specific. Examples of such annotations include supporting plane annotations, and graspable region annotations for various object manipulations.\n\\end{itemize}\n\n\\mypara{Physical Annotations:} Real objects exist in the physical world and typically have fixed physical properties such as dimensions and densities.  Thus, it is important to store physical attribute annotations for 3D shapes.\n\\begin{itemize}\n\\item \\textbf{Surface Material:} We are especially interested in the optical properties and semantic names of surface materials. They are important for applications such as rendering and structural strength estimation.\n\\item \\textbf{Weight:} A basic property of objects which is very useful for physical simulations, and reasoning about stability and static support.\n\\end{itemize}\n\nIn general, the issue of compact and informative representations for all the above attributes over shapes raises many interesting questions that we will need to address as part of the ShapeNet effort.  Many annotations are currently ongoing projects and involve interesting open research problems.\n\n\\subsection{Annotation Methodology}\n\nThough at first glance it might seem reasonable to collect the annotations we describe purely through manual human effort, we will in general take a hybrid approach.  For annotation types where it is possible, we will first algorithmically predict the annotation for each model instance (e.g., global symmetry planes, consistent rigid alignments).  We will then verify these predictions through crowd-sourcing pipelines and inspection by human experts.  This hybrid strategy is sensible in the context of 3D shape data as there are already various algorithms we can leverage, and collecting corresponding annotations entirely through manual effort can be extremely labor intensive.  In particular, since objects in a 3D representation are both more pure and more complete than objects in images, we can expect better and easier to establish correspondences between 3D shapes, enabling algorithmic transport of semantic annotations.  In many cases, the design of the human annotation interfaces themselves is an open question --- which stands in contrast to largely manual image labeling efforts such as ImageNet.  As a concrete example, shape part annotation can be presented and performed in various ways with different trade-offs in the type of obtained part annotation, the accuracy and the efficiency of the annotation process.\n\nCoupled with this hybrid annotation strategy, we also take particular care to preserve the provenance and confidence of each algorithmic and human annotation.  The annotation source (whether an algorithm, or human effort), and a measure of the trust we can place in each annotation are critical pieces of information especially when we have to combine, aggregate, and reconcile several annotations.\n\n\\subsection{Annotation Schema and Web API}\n\nTo provide convenient access to all of the model and annotation data contained within ShapeNet, we construct an index over all the 3D models and their associated annotations using the Apache Solr framework.\\footnote{\\url{http://lucene.apache.org/solr/}}  Each stored annotation for a given 3D model is contained within the index as a separate attribute that can be easily queried and filtered through a simple web-based UI.  In addition, to make the dataset conveniently accessible to researchers, we provide a batched download capability.\n\n\\section{Annotation Acquisition and Validation}\n\\label{sec:acquisition}\nA key challenge in constructing ShapeNet is the methodology for acquiring and validating annotations. Our goal is to provide all annotations with high accuracy. In cases where full verification is not yet available, we aim to estimate a confidence metric for each annotation, as well as record its provenance. This will enable others to properly estimate the trustworthiness of the information we provide and use it for different applications.\n\n\\subsection{Category Annotation}\n\\label{sec:acquisition:categoryannotation}\nAs described in Section~\\ref{sec:annotation-types}, we assign each 3D model to one or more synsets in the WordNet taxonomy.\n\n\\mypara{Annotation}\nModels are retrieved by textual query into the online repositories that we collected, and the initial category annotation is set to the used textual query for each retrieved model.  After we retrieve these models we use the popularity score of each model on the repository to sort models and ask human workers to verify the assigned category annotation.  This is sensible since the more popular models tend to be high quality and correctly retrieved through the category keyword textual query.  We stop verifying category annotations with people once the positive ratio is lower than a 2\\% threshold.\n\n\\mypara{Clean-up}\nIn order for the dataset to be easily usable by researchers it should contain clean and high quality 3D models.  Through inspection, we identify and group 3D models into the following categories: \\emph{single 3D models}, \\emph{3D scenes}, \\emph{billboards}, and \\emph{big ground plane}.\n\n\\begin{itemize}\\denselist\n  \\item Single 3D models: semantically distinct objects; focus of our \\emph{ShapeNetCore} annotation effort.\n  \\item 3D scenes: detected by counting the number of connected components in a voxelized representation. We manually verify these detections and mark scenes for future analysis.\n  \\item Billboards: planes with a painted texture. Often used to represent people and trees. These models are generally not useful for geometric analysis. They can be detected by checking whether a single plane can fit all vertices.\n  \\item Big ground plane: object of interest placed on a large horizontal plane or in front of large vertical plane. Although we do not currently use these models, the plane can easily be identified and removed through simple geometric analysis.\n\\end{itemize}\n\nWe currently include the single 3D models in the ShapeNetCore subset of ShapeNet.\n\n\\subsection{Hierarchical Rigid Alignment}\n\\label{sec:alignment}\nThe goal of this step is to establish a consistent canonical orientation for models within each category. Such alignment is important for various tasks such as visualizing shapes, shape classification and shape recognition. \\Cref{fig:rigid_alignment} shows several categories in ShapeNet that have been consistently aligned.\n\nThough the concept of consistent orientation seems natural, one issue has to be addressed. We explain by an example. ``armchair'', ``chair'' and ``seat'' are three categories in our taxonomy, each being a subcategory of its successor. Consistent orientation can be well defined for shapes in the ``armchair'' category, by checking arms, legs and backs. Yet, it becomes difficult to define for the ``chair'' category. For example, ``side chair'' and ``swivel chair'' are both subcategories of ``chair'', however, swivel chairs have a very different leg structure than most side chairs. It becomes even more ambiguous to define for ``seat'', which has subcategories such as ``stool'', ``couch'', and ``chair''. However, the concept of an upright orientation still applies throughout most levels of the taxonomy.\n\nFollowing the above discussion, it is natural for us to propose a hierarchical alignment method, with a small amount of human supervision.  The basic idea is to hierarchically align models following the taxonomy of ShapeNet in a bottom-up manner, i.e., we start from aligning shapes in low-level categories and then gradually elevate to higher level categories. When we proceed to the higher level, the self-consistent orientation within a subcategory should be maintained. For the alignment at each level, we first use a geometric algorithm described in the \\Cref{sec:appendix:alignment}, and then ask human experts to check and correct possible misalignments.  With this strategy, we efficiently obtain consistent orientations.  In practice, most shapes in the same low-level categories can be well aligned algorithmically, requiring limited manual correction.  Though the proportion of manual corrections increases for aligning higher-level categories, the number of categories at each level becomes logarithmically smaller.\n\n\\begin{figure}[t]\n\t\\includegraphics[width=\\linewidth]{fig/aligned-models}\n\t\\caption{Examples of aligned models in the chair, laptop, bench, and airplane synsets.}\n\t\\label{fig:rigid_alignment}\n\\end{figure}\n\n\\subsection{Parts and Keypoints}\nTo obtain part and keypoint annotations we start from some curated part annotations within each category. For parts, this acquisition can be speeded up by having  algorithmically generated segmentations and then having users accept or modify parts from these. We intend to experiment with both 2D and 3D interfaces for this task. We then exploit a number of different algorithmic techniques to propagate this information to other nearby shapes. Such methods can rely on rigid alignments in 3D, feature descriptor alignments in an appropriately defined feature space, or general shape correspondences. We iterate this pipeline, using active learning to estimate the 3D models and regions of these models where further human annotation would be most informative, generate a new set of crowd-sourced annotation tasks, algorithmically propagate their results, and so on. In the end we have users verify all proposed parts and keypoints, as verification is much faster than direct annotation.\\label{sec:parts}\n\n\\subsection{Symmetry Estimation}\nWe provide bilateral symmetry plane detections for all 3D models in ShapeNetCore. Our method is a modified version of~\\cite{mitra:2013:symmetry}. The basic idea is to use hough transform to vote on the parameters of the symmetry plane. More specifically, we generate all combinations of pairs of vertices from the mesh. Each pair casts a vote of a possible symmetry plane in the discretized space of plane parameters partitioned evenly. We then pick the parameter with the most votes as the symmetry plane candidate. As a final step, this candidate is verified to ensure that every vertex has a symmetric counterpart.\\label{sec:symmetry}\n\n\\subsection{Physical Property Estimation}\nBefore computing physical attribute annotations, the dimensions of the models need to be correspond to the real world.  We estimate the absolute dimensions of models using prior work in size estimation~\\cite{savva:2014:sizes}, followed by manual verification.  With the given absolute dimensions, we now compute the total solid volume of each model through filled-in voxelization.  We use the space carving approach implemented by Binvox~\\cite{nooruddin:2003:simplification}.  Categories of objects that are known to be container-like (i.e., bottles, microwaves) are annotated as such and only the surface voxelization volume is used instead.  We then estimate the proportional material composition of each object category and use a table of material densities along with each model instance volume to compute a rough total weight estimate for that instance.  More details about the acquisition of these physical attribute annotations are available separately~\\cite{savva:2015:semgeo}. \\label{sec:physical}\n\n\\section{Current Statistics}\n\\label{sec:statistics}\nAt the time of this technical report, ShapeNet has indexed roughly 3,000,000 models. 220,000 models of these models are classified into 3,135 categories (WordNet synsets). Below we provide detailed statistics for the currently annotated models in ShapeNet as a whole, as well as details of the available publicly released subsets of ShapeNet.\n\n\\begin{figure*}[t]\n\t\\includegraphics[width=\\linewidth]{fig/category-distributions}\n\t\\caption{Plots of the distribution of ShapeNet models over WordNet synsets at multiple levels of the taxonomy (only the top few children synsets are shown at each level). The highest level (root) is at the top and the taxonomy levels become lower downwards and to the right.  Note the bias towards rigid man-made artifacts at the top and the broad coverage of many low level categories towards the bottom.}\n\t\\label{fig:category-distributions}\n\\end{figure*}\n\n\\mypara{Category Distribution} \\Cref{fig:category-distributions} shows the distributions of the number of shapes per synset at various taxonomy levels for the current ShapeNetCore corpus. To the best of our knowledge, ShapeNet is the largest clean shape dataset available in terms of total number of shapes, average number of shapes per category, as well as the number of categories.\n\nWe observe that ShapeNet as a whole is strongly biased towards categories of rigid man-made artifacts, due to the bias of the source 3D model repositories.  This is in contrast to common image database statistics that contain more natural objects such as plants and animals~\\cite{torralba:2010:labelme}. This distribution bias is probably due to a combination of factors: 1) meshes of natural objects are more difficult to design using common CAD software; 2) 3D model consumers are typically more interested in artificial objects such as those observed in modern urban lifestyles. The former factor can be mitigated in the near future by using the rapidly improving depth sensing and 3D scanning technology.\n\n\\subsection{ShapeNetCore}\n\n\\emph{ShapeNetCore} is a subset of the full ShapeNet dataset with single clean 3D models and manually verified category and alignment annotations. It covers 55 common object categories with about 51,300 unique 3D models. The 12 object categories of PASCAL 3D+\\cite{xiang:2014:pascal3d}, a popular computer vision 3D benchmark dataset, are all covered by ShapeNetCore.  The category distribution of ShapeNetCore is shown in \\Cref{tab:shapenetcore}.\n\n\\begin{table*}\n\\centering{\n\t\\begin{tabular}{ccr|ccr|ccr}\n\t\\hline\n\tID&Name&Num&ID&Name&Num&ID&Name&Num\\\\\n\t\\hline\n\t04379243&table&8443\t&\t03593526&jar&597\t&04225987&skateboard&152\\\\\n\t02958343&car&7497\t&\t02876657&bottle&498\t&04460130&tower&133\\\\\n\t03001627&chair&6778\t&\t02871439&bookshelf&466\t&02942699&camera&113\\\\\n\t02691156&airplane&4045\t&\t03642806&laptop&460\t&02801938&basket&113\\\\\n\t04256520&sofa&3173\t&\t03624134&knife&424\t&02946921&can&108\\\\\n\t04090263&rifle&2373\t&\t04468005&train&389\t&03938244&pillow&96\\\\\n\t03636649&lamp&2318\t&\t02747177&trash bin&343\t&03710193&mailbox&94\\\\\n\t04530566&watercraft&1939\t&\t03790512&motorbike&337\t&03207941&dishwasher&93\\\\\n\t02828884&bench&1816\t&\t03948459&pistol&307\t&04099429&rocket&85\\\\\n\t03691459&loudspeaker&1618\t&\t03337140&file cabinet&298\t&02773838&bag&83\\\\\n\t02933112&cabinet&1572\t&\t02818832&bed&254\t&02843684&birdhouse&73\\\\\n\t03211117&display&1095\t&\t03928116&piano&239\t&03261776&earphone&73\\\\\n\t04401088&telephone&1052\t&\t04330267&stove&218\t&03759954&microphone&67\\\\\n\t02924116&bus&939\t&\t03797390&mug&214\t&04074963&remote&67\\\\\n\t02808440&bathtub&857\t&\t02880940&bowl&186\t&03085013&keyboard&65\\\\\n\t03467517&guitar&797\t&\t04554684&washer&169\t&02834778&bicycle&59\\\\\n\t03325088&faucet&744\t&\t04004475&printer&166\t&02954340&cap&56\\\\\t\n\t03046257&clock&655\t&\t03513137&helmet&162\t& & &\\\\\n\t03991062&flowerpot&602\t&\t03761084&microwaves&152 & &\\bf{Total}&57386 \\\\\n\t\\hline\\\\\n\t\\end{tabular}\n\t\\caption{\\small{Statistics of ShapeNetCore synsets. ID corresponds to WordNet synset offset, which is aligned with ImageNet.}}\n\t\\label{tab:shapenetcore}\n}\n\\end{table*}\n\n\\subsection{ShapeNetSem}\n\\label{sec:shapenetsem}\n\n\\begin{table*}\n\\centering\\footnotesize\n\\begin{tabular}{cr|cr|cr|cr|cr}\n\t\\hline\n\\textbf{Category}&\\textbf{Num}&\\textbf{Category}&\\textbf{Num}&\\textbf{Category}&\\textbf{Num}&\\textbf{Category}&\\textbf{Num}&\\textbf{Category}&\\textbf{Num}\\\\\nChair&696&Monitor&127&WallLamp&78&Gun&54&FlagPole&38\\\\\nLamp&663&RoundTable&120&SideChair&77&Nightstand&53&TvStand&38\\\\\nChestOfDrawers&511&TrashBin&117&VideoGameConsole&75&Mug&51&Fireplace&37\\\\\nTable&427&DrinkingUtensil&112&MediaStorage&73&AccentChair&50&Rack&37\\\\\nCouch&413&DeskLamp&110&Painting&73&ChessBoard&49&LightSwitch&36\\\\\nComputer&244&Clock&101&Desktop&71&Rug&49&Oven&36\\\\\nDresser&234&ToyFigure&101&AccentTable&70&WallUnit&46&Airplane&35\\\\\nTV&233&Plant&98&Camera&70&Mirror&45&DresserWithMirror&35\\\\\nWallArt&222&Armoire&95&Picture&69&Bowl&44&Calculator&34\\\\\nBed&221&QueenBed&94&Refrigerator&68&SodaCan&44&TableClock&34\\\\\nCabinet&221&Stool&92&Speaker&68&VideoGameController&44&Toilet&34\\\\\nFloorLamp&201&EndTable&91&Sideboard&67&WallClock&43&Cup&33\\\\\nDesk&189&Bottle&88&Barstool&66&Printer&42&Stapler&33\\\\\nPottedPlant&188&DiningTable&88&Guitar&65&Sword&40&PaperBox&32\\\\\nFoodItem&180&Bookcase&87&MediaPlayer&62&USBStick&40&SpaceShip&32\\\\\nLaptop&173&CeilingLamp&86&Ipod&59&Chaise&39&Toy&32\\\\\nVase&163&Bench&84&PersonStanding&57&OfficeSideChair&39&ToiletPaper&31\\\\\nTableLamp&142&Book&84&Piano&56&Poster&39&Knife&30\\\\\nOfficeChair&137&CoffeeTable&81&Curtain&55&Sink&39&PictureFrame&30\\\\\nCellPhone&130&Pencil&80&Candle&54&Telephone&39&Recliner&30\\\\\n\t\\hline\\\\\n\\end{tabular}\n\\caption{\\small{Total number of models for the top 100 ShapeNetSem categories (out of 270 categories). Each category is also linked to the corresponding WordNet synset, establishing the same linkage to WordNet and ImageNet as with ShapeNetCore.}}\n\\label{tab:shapenetsem}\n\\end{table*}\n\n\\emph{ShapeNetSem} is a smaller, more densely annotated subset consisting of 12,000 models spread over a broader set of 270 categories.  In addition to manually verified category labels and consistent alignments, these models are annotated with real-world dimensions, estimates of their material composition at the category level, and estimates of their total volume and weight.  The total numbers of models for the top 100 categories in this subset are given in \\Cref{tab:shapenetsem}.\n\n\\section{Discussion and Future Work}\n\\label{sec:discussion}\nThe construction of ShapeNet is a continuous, ongoing effort.  Here we have just described the initial steps we have taken in defining ShapeNet and populating a core subset of model annotations that we hope will prove useful to the community.  We plan to grow ShapeNet in four distinct directions:\n\n\\mypara{Additional annotation types} We will introduce several additional types of annotations that have strong connections to the semantics and functionality of objects. Firstly, hierarchical part decompositions of objects will provide a useful finer granularity description of object structure that can be leveraged for part segmentation and shape synthesis. Secondly, physical object property annotations such as materials and their attributes will allow higher fidelity physics and appearance simulation, adding another layer of understanding to methods in vision and graphics.\n\n\\mypara{Correspondences} One of the most important goals of ShapeNet is to provide a dense network of correspondences between 3D models and their parts.  This will be invaluable for enabling much shape analysis research and helping to improve and evaluate methods for many traditional tasks such as alignment and segmentation.  Additionally, we plan to provide correspondences between 3D model parts and image patches in ImageNet --- a link that will be critical for propagating information between image space and 3D models.\n\n\\mypara{RGB-D data} The rapid proliferation of commodity \\mbox{RGB-D} sensors is already making the process of capturing real-world environments better and more efficient.  Expanding ShapeNet to include shapes reconstructed from scanned \\mbox{RGB-D} data is a critical goal.  We foresee that over time, the amount of available reconstructed shape data will overshadow the existing designed 3D model data and as such this is a natural growth direction for ShapeNet.  A related effort that we are currently undertaking is to align 3D models to objects observed in RGB-D frames.  This will establish a powerful connection between real world observations and 3D models.\n\n\\mypara{Annotation coverage} We will continue to expand the set of annotated models to cover a bigger subset of the entirety of ShapeNet. We will explore combinations of algorithmic propagation methods and crowd-sourcing for verification of the algorithmic results.\n\n\\vfill\n\n\\section{Conclusion}\n\\label{sec:conclusion}\nWe firmly believe that ShapeNet will prove to be an immensely useful resource to several research communities in several ways:\n\n\\mypara{Data-driven research} By establishing ShapeNet as the first large-scale 3D shape dataset of its kind we can help to move computer graphics research toward a data-driven direction following recent developments in vision and NLP. Additionally, we can help to enable larger-scale quantitative analysis of proposed systems that can clarify the benefits of particular methodologies against a broader and more representative variety of 3D model data.\n\n\\mypara{Training resource} By providing a large-scale, richly annotated dataset we can also promote a broad class of recently resurgent machine learning and neural network methods for applications dealing with geometric data. Much like research in computer vision and natural language understanding, computational geometry and graphics stand to benefit immensely from these data-driven learning approaches.\n\n\\mypara{Benchmark dataset} We hope that ShapeNet will grow to become a canonical benchmark dataset for several evaluation tasks and challenges.  In this way, we would like to engage the broader research community in helping us define and grow ShapeNet to be a pivotal dataset with long-lasting impact.\n\n\\begin{small}\n\\end{small}\n\n\\newpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Objaverse: A Universe of Annotated 3D Objects}\n\n\\begin{document}\n\n\\title{Objaverse: A Universe of Annotated 3D Objects}\n\n\\author{\\\\[-0.3in]\\textbf{Matt Deitke$^{\\dagger\\psi}$, Dustin Schwenk$^\\dagger$, Jordi Salvador$^\\dagger$, Luca Weihs$^\\dagger$, Oscar Michel$^\\dagger$}\\\\\\textbf{Eli VanderBilt$^\\dagger$, Ludwig Schmidt$^\\psi$, Kiana Ehsani$^\\dagger$, Aniruddha Kembhavi$^{\\dagger\\psi}$, Ali Farhadi$^\\psi$}\\\\\n$^\\dagger$PRIOR @ Allen Institute for AI, $^\\psi$University of Washington, Seattle\\\\\n\\href{https://objaverse.allenai.org/}{\\texttt{objaverse.allenai.org}}\n}\n\n\\newcommand{\\w}{1.329in}\n\\newcommand{\\h}{1in}\n\n\\twocolumn[{\n\\renewcommand\\twocolumn[1][]{#1}\n\\maketitle\n\\vspace*{-0.25in}\n\\centering\n\\captionsetup{type=figure}\\includegraphics[width=\\textwidth]{assets/annotations.png}\n\\captionof{figure}{Example instances from our large-scale 3D asset dataset \\data. \\data 3D assets are semantically diverse, high-quality, and paired with natural-language descriptions.\\\\[0.15in]}\n\\label{fig:teaser}\n\\vspace{-1em}\n}]\n\n\\maketitle\n\n\\blfootnote{Correspondence to $<$mattd@allenai.org$>$.}\n\n\\vspace*{-0.10in}\n\n\\begin{abstract}\n\\vspace{-0.125in}\nMassive data corpora like WebText, Wikipedia, Conceptual Captions, WebImageText, and LAION have propelled recent dramatic progress in AI. Large neural models trained on such datasets produce impressive results and top many of today's benchmarks. A notable omission within this family of large-scale datasets is 3D data. Despite considerable interest and potential applications in 3D vision, datasets of high-fidelity 3D models continue to be mid-sized with limited diversity of object categories. Addressing this gap, we present Objaverse 1.0, a large dataset of objects with 800K+ (and growing) 3D models with descriptive captions, tags, and animations. Objaverse improves upon present day 3D repositories in terms of scale, number of categories, and in the visual diversity of instances within a category. We demonstrate the large potential of Objaverse via four diverse applications: training generative 3D models, improving tail category segmentation on the LVIS benchmark, training open-vocabulary object-navigation models for Embodied AI, and creating a new benchmark for robustness analysis of vision models. Objaverse can open new directions for research and enable new applications across the field of AI.\n\n\\end{abstract}\n\n\\vspace{-0.25in}\n\\section{Introduction}\n\\label{sec:intro}\n\n\\newcommand{\\s}{0.05in}\n\nMassive datasets have enabled and driven rapid progress in AI. Language corpora on the web led to large language models like GPT-3~\\cite{brown2020language}; paired image and text datasets like Conceptual Captions~\\cite{sharma2018conceptual} led to vision-and-language pretrained models like VilBERT~\\cite{lu2019vilbert}; YouTube video datasets led to video capable models like Merlot-Reserve~\\cite{zellers2022merlot}; and massive multimodal datasets like WebImageText~\\cite{srinivasan2021wit} and LAION~\\cite{schuhmann2021laion,schuhmann2022laion} led to models like CLIP~\\cite{radford2021learning} and StableDiffusion~\\cite{rombach2022high}. These leaps in dataset scale and diversity were triggered by moving from manually curated datasets to harnessing the power of the web and its creative content.\n\nIn contrast to the datasets described above, the size of the datasets we are feeding to our, data-hungry, deep learning models in many other areas of research is simply not comparable. For instance, the number of 3D assets used in training generative 3D models is, maximally, on the order of thousands~\\cite{gao2022get3d} and the simulators used to train embodied AI models typically have only between a few dozen to a thousand unique scenes~\\cite{kolve2017ai2thor,szot2021habitat,ramrakhya2022habitat,li2021igibson}. The startling advances brought about by developing large-scale datasets for images, videos, and natural language, demand that an equivalent dataset be built for 3D assets.\n\nWe present \\data 1.0, a large scale corpus of high-quality, richly annotated, 3D objects; see Fig.~\\ref{fig:teaser}. Objects in our dataset are free to use\\footnote{Creative Commons license} and sourced from Sketchfab, a leading online platform for managing, viewing, and distributing 3D models. In total, \\data contains over \\textbf{800K} 3D assets designed by over \\textbf{100K} artists which makes this data large and diversely sourced.\nAssets not only belong to varied categories like animals, humans, and vehicles, but also include interiors and exteriors of large spaces that can be used, \\eg, to train embodied agents. \\data is a universe of rich 3D data with detailed metadata that can support many different annotations to enable new applications. \nWith this remarkable increase in scale, we see an incredible opportunity for \\data to impact research progress across domains. In this work, we provide promising results to answer three questions.\n\n\\textbf{Can 3D vision benefit from a large-scale dataset?}\nFirst, as a 3D asset resource, \\data can support the exciting field of 3D generative modeling. We use data extracted from \\data to train generative models for single and multiple categories using GET3D~\\cite{gao2022get3d} and find that we are able to generate high-quality objects and, moreover, that our generated objects are found by human annotators to be more diverse than those generated by a model trained on ShapeNet objects in 91\\% of cases.\n\n\\textbf{Can the diversity of 3D models help improve classical 2D vision task performance?}\nTo answer this question, we use the diversity of \\data\nto improve the performance of long tail instance segmentation models. Instance segmentation data can be expensive to obtain owing to the cost of annotating contours around objects. The recent LVIS dataset contains annotations for 1{,}230 categories but the task remains very challenging for present day models, particularly on tail categories that have few examples. We show that increasing the volume of data by leveraging a simple Copy{+}Paste augmentation method with \\data assets can improve the performance of state-of-the-art segmentation methods.\n\nWe also use \\data to build a benchmark for evaluating the robustness of state-of-the-art visual classification models to perspective shifts. We render objects in \\data from random orientations, which is how one might expect to see them in the real world and test the ability of CLIP-style visual backbones to correctly classify these images. Our experiments show that current state-of-the-art models' performance degrades dramatically in this setting when viewing objects from arbitrary views.\n\\data allows us to build benchmarks to test (and potentially train) for orientation robustness for a long tail distribution of asset categories. Building such benchmarks is made uniquely possible by the scale and diversity of 3D assets in \\data. This would simply not be feasible to create in the real world nor can they be generated from existing 2D images.\n\n\\begin{figure*}[t]\n    \\begin{subfigure}[t]{0.725\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{assets/shapenet14.png}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[t]{0.25\\textwidth}\n        \\vspace{-1.85in}\n        \\centering\n        \\resizebox{\\textwidth}{!}{%\n        \\begin{tabular}{lccccccc}\n            \\toprule\n            Dataset & \\# Objects & \\# Classes \\\\\n            \\midrule\n            YCB~\\cite{calli2015benchmarking} & 77 & 5 \\\\[\\s]\n            BigBIRD~\\cite{Singh2014BigBIRDAL} & 125 & -- \\\\[\\s]\n            KIT~\\cite{kasper2012kit} & 219 & 145 \\\\[\\s]\n            IKEA~\\cite{lim2013parsing} & 219 & 11 \\\\[\\s]\n            Pix3D~\\cite{sun2018pix3d} & 395 & 9 \\\\[\\s]\n            GSO~\\cite{downs2022google} & 1K & 17 \\\\[\\s]\n            EGAD~\\cite{morrison2020egad} & 2K & -- \\\\[\\s]\n            PhotoShape~\\cite{park2018photoshape} & 5K & 1 \\\\[\\s]\n            ABO~\\cite{collins2022abo} & 8K & 63 \\\\[\\s]\n            3D-Future~\\cite{fu20213d} & 10K & 34 \\\\[\\s]\n            ShapeNet~\\cite{chang2015shapenet} & 51K & 55 \\\\[\\s]\n            \\textbf{Objaverse} & \\textbf{818K} & \\textbf{21K} \\\\\n            \\bottomrule\n        \\end{tabular}\n        }\n    \\end{subfigure}\n    \\caption{Comparison between \\data and existing 3D object datasets. (\\textit{Left:}) Visual comparison of instances from \\data and ShapeNet for the categories of \\textsc{Car}, \\textsc{Bed}, \\textsc{Vase}, and \\textsc{Bag}. \\data instances are substantially more diverse since objects can come from many 3D content creation platforms, whereas ShapeNet models look more similar and all come from SketchUp, a 3D modeling platform built for simple architectural modeling. (\\textit{Right:}) Scale comparison table between existing 3D object datasets.}\n    \\label{fig:shapenet-comparison}\n    \\vspace{-1em}\n\\end{figure*}\n\n\\textbf{Can a large-scale 3D dataset help us train embodied agents performant embodied agents?\n}\nWe use assets in \\data to populate procedurally generated simulated environments in ProcTHOR~\\cite{deitke2022procthor} that are used to train Embodied AI agents.\nThis results in an orders of magnitude increase in the number of unique assets available for use in ProcTHOR scenes (previously limited to AI2-THOR's~\\cite{kolve2017ai2thor} asset library of a few thousand unique instances each assigned to one of 108 object categories).\nUsing \\data populated scenes enables open vocabulary object navigation from any text description. In this paper, we provide quantitative results for navigating to 1.1K semantic object categories, roughly a 50x increase.  %\n\nThese findings represent just a small fraction of what can be accomplished using \\data. We are excited to see how the research community will leverage \\data to enable fast and exciting progress in 2D and 3D computer vision applications and beyond.\n\n\\vspace{-0.05in}\n\n\\section{Related Work}\n\\label{sec:related-work}\n\n\\textbf{Large scale datasets.} \nScaling the size and scope of training datasets has widely been demonstrated to be an effective avenue of improvement for model performance. In computer vision, the adoption of early large scale datasets such as Imagenet\\cite{russakovsky2015imagenet,deng2009imagenet} and MS-COCO\\cite{lin2014microsoft} has dramatically accelerated progress on a variety of tasks including classification, object detection, captioning, and more. Ever since, the diversity and scale of datasets have continued to grow. YFCC100M is a dataset of 99.2M images and 800K videos\\cite{thomee2016yfcc100m}. OpenImages\\cite{kuznetsova2020open} is a large scale dataset of 9M images that contains labeled subsets bounding boxes, visual relationships, segmentation masks, localized narratives, and categorical annotations. Massive web-scraped datasets containing image-text pairs such as Conceptual Captions\\cite{sharma2018conceptual}, WIT\\cite{srinivasan2021wit}, and LAION\\cite{schuhmann2021laion,schuhmann2022laion} have seen increased popularity recently as they have been used to train impressive models for vision-language representation learning\\cite{radford2021learning,ilharco_gabriel_2021_5143773,jia2021scaling}, text-to-image generation\\cite{ramesh2021zero,ramesh2022hierarchical,rombach2022high,jia2021scaling}, and vision-language multitasking\\cite{cho2021unifying,tan2019lxmert,wang2022image,chen2022pali}.\n\n\\textbf{3D datasets.}\nCurrent large-scale 2D image datasets offer three crucial components that benefit learning: scale, diversity, and realism. Ideally, models that reason about 3D objects should have access to datasets that meet these same criteria. However, of the numerous 3D object datasets that currently exist, none are able to excel in all three categories to the same degree as their 2D counterparts. \nDatasets such as KIT\\cite{kasper2012kit}, YCB\\cite{calli2015benchmarking}, BigBIRD\\cite{Singh2014BigBIRDAL}, IKEA\\cite{lim2013parsing}, and Pix3D\\cite{sun2018pix3d} provide image-calibrated models over a diverse set of household objects, but severely lack in scale with only a few hundred objects at most. EGAD\\cite{morrison2020egad} procedurally generates 2K objects for grasping, but produces objects that are not that realistic or diverse. Slightly larger datasets of photo-realistic objects include GSO\\cite{downs2022google}, PhotoShape\\cite{park2018photoshape}, ABO\\cite{collins2022abo} and 3D-Future\\cite{fu20213d}, and ShapeNet\\cite{chang2015shapenet} with object counts in the tens of thousands, see Fig.~\\ref{fig:shapenet-comparison} for comparisons between \\data and these datasets. Datasets for CAD models, such as ModelNet\\cite{wu20153d} and DeepCAD\\cite{wu2021deepcad}, and ABC\\cite{koch2019abc} do not include textures or materials, which limits their ability to represent objects that could plausibly be found in the real world. Datasets of scanned 3D objects and environments are valuable for real-world understanding\\cite{couprie2013indoor,dai2017scannet,choi2016large,levoy2000digital}, but are quite small and limited. In addition to containing numerous artist designed objects, \\data contains many scanned assets, making it a useful source of data for learning from real-world distributions.\n\nWhile rapid progress has been made in developing datasets that combine image and text, in contrast, only a few datasets that pair language and 3D data exist. Text2Shape\\cite{chen2018text2shape} released a dataset of 15,038 chairs and tables from ShapeNet each with around 5 text captions, giving 75,344 total text-shape pairs. ShapeGlot\\cite{achlioptas2019shapeglot} released the CiC (Chairs in Context) dataset which contains 4,511 chairs from ShapeNet along with 78,789 descriptive utterances generated from a referential game. Due to the small scale and limited diversity of these datasets, current SoTA text-to-3D models\\cite{michel2022text2mesh,poole2022dreamfusion,jain2022zero} forgo the use of 3D datasets entirely and instead rely on 2D image-text supervision.\n\n\\vspace{-0.075in}\n\n\\section{Objaverse}\n\\label{sec:objaverse}\n\n\\data is a massive annotated 3D dataset that can be used to enable research in a wide range of areas across computer vision.\nThe objects are sourced from Sketchfab, an online 3D marketplace where users can upload and share models for both free and commercial use. Objects selected for \\data have a distributable Creative Commons license and were obtained using Sketchfabs public API. Aside from licensing consideration, models marked as restricted due to objectionable or adult thematic content were excluded from the dataset.\n\n\\begin{figure*}[t!]\n     \\centering\n          \\begin{subfigure}[b]{0.48\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{assets/categories.pdf}\n         \\caption{Breakdown of high-level \\data categories}\n         \\label{figures:category_donut} \n     \\end{subfigure}\n     \\hfill\n     \\begin{subfigure}[b]{0.48\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{assets/tag_cloud.pdf}\n         \\caption{Word cloud of \\data metadata tags.}\n         \\label{figures:tag_cloud} \n     \\end{subfigure}\n     \\\\[0.15in]\n     \\begin{subfigure}[b]{0.48\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{assets/objaverse_cat_dist.pdf}\n         \\caption{\\datalvis category occurrence distribution.}\n         \\label{figures:lvis_cat_dist} \n     \\end{subfigure}\n     \\hfill\n     \\begin{subfigure}[b]{0.495\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{assets/objaverse_tag_dist.pdf}\n         \\caption{\\data tag occurrence distribution.}\n         \\label{figures:tag_dist} \n     \\end{subfigure}\n     \\caption{\\textbf{\\data statistics.} (a) All 18 high-level categories present in \\data's metadata with their corresponding number of occurrences. The relative share of most popular categories are evenly split, with a small number of less frequently categories. (b) A sample of several thousand popular object tags found in \\data log-scaled by their frequency. (c) A histogram of fine-grained \\datalvis categories with representative members from several bins highlighted.\n     (d) A histogram of \\data tags with representative members from several bins highlighted (note y-axis log scale). Tags from the low-occurrence side of the distribution correspond to unique objects that, taken individually, are rarely seen in the world. Frequently used tags like \"furniture\" and \"car\" reflect their real-world normalcy, but the high frequency of assets like \"sword\" diverge from their real-world counterparts.} \n     \\label{fig:metadata vis}\n     \\vspace{-1em}\n\\end{figure*}\n\n\\textbf{Model metadata.}\n\\data objects inherit a set of foundational annotations supplied by their creator when uploaded to Sketchfab. Figure~\\ref{fig:sf-metadata} shows an example of the metadata available for each model. The metadata includes a name, assignments to a set of fixed categories, a set of unrestricted tags, and a natural language description.\n\n\\textbf{\\datalvis.}\nWhile \\data metadata contains a great deal of information about objects, Sketchfab's existing categorization scheme covers only 18 categories, too coarse for most applications. Object names, categories, and tags provide multiple potential categorizations at varying levels of specificity and with some inherent noise. However, for many existing computer vision tasks, it is useful to assign objects to a single category drawn from a predetermined set of the right size and level of semantic granularity.\n\nWe choose the categories from the LVIS dataset~\\cite{gupta2019lvis} for categorizing a long-tail subset of objects in \\data. We construct a 47K LVIS categorized object subset, called \\datalvis, comprised of objects uniquely assigned to one of 1156 LVIS categories. We perform these assignments by first selecting 500 candidate objects per category using a combination of predictions from a CLIP classification model and candidates suggested by terms in their metadata. This combined pool contains objects visually resembling the target category (from the CLIP features of their thumbnail images) that might have missing metadata, as well as visually unusual instances of a category that are accurately named or tagged. These 250k candidate objects were then manually filtered and their assigned categories verified by crowdworkers. Since we only presented 500 object candidates per class, many popular categories, such as chair or car, have substantially more objects that could be included in \\datalvis with future annotations.\n\n\\textbf{Animated objects and rigged characters.} \\data includes 44K animated objects and over 63K objects self-categorized as characters. Examples of animations include fridge doors opening, animals running, and the hands on a clock moving. Rigged characters can be set up for animation and rendering, and may often come annotated with bone mappings. The vast scale of animations available in \\data can support a wide range of research in temporal 3D learning, such as building text-based animation generative models~\\cite{tevet2022human}, representing object changes over time with NERFs~\\cite{pumarola2021d,park2021nerfies}, and temporal self-supervised learning via. future frame prediction~\\cite{zellers2022merlot,jabri2020space}.\n\n\\begin{figure*}[t!]\n    \\centering\n    \\includegraphics[width=1\\textwidth]{assets/metadata-3.png}\n    \\caption{An example of metadata available for each object in \\data. Each uploaded object has a 3D model, user-selected rendered thumbnail image, name, description, tags, category, and stats, among additional metadata.}\n    \\label{fig:sf-metadata}\n    \\vspace{-1em}\n\\end{figure*}\n\n\\begin{figure*}[b!]\n    \\includegraphics[width=\\textwidth]{assets/fig2.png}\n    \\caption{Highlights of the visual diversity of objects that appear in \\data, including animated objects, rigged (body-part annotated) characters, models separatable into parts, exterior environments, interior environments, and a wide range visual styles.}\n    \\label{fig:visDiv}\n    \\vspace{-1em}\n\\end{figure*}\n\n\\textbf{Articulated objects.} Decomposing 3D objects into parts has led to a flurry of research in the past few years, including work in learning robotic grasping policies~\\cite{xu2021adagrasp,xiang2020sapien}, 3D semantic segmentation~\\cite{mo2019partnet}, and shape generation~\\cite{mo2019structurenet}. Since many objects in \\data were uploaded by artists, the objects often come separated into parts. Figure~\\ref{fig:visDiv} shows an example, where a chair is separated by its backrest, wheels, and legs, among many smaller parts.\n\n\\textbf{Exteriors.} Photogrammetry and NERF advances have enabled the commercialization of capturing high-quality 3D objects of large exteriors by taking pictures ~\\cite{tancik2022block,xiangli2022bungeenerf}. In \\data, there are a large number of scanned buildings, cities, and stadiums. Figure~\\ref{fig:visDiv} shows an example of a 3D object of NYC's skyline captured through a scan.\n\n\\textbf{\\data-Interiors.} There are 16K+ interior scenes in \\data, including houses, classrooms, and offices. The scenes often have multiple floors, many types of rooms, and are densely populated with objects from human input. Objects in the scenes are separable into parts, which allows them to be usable for interactive robotics, embodied AI, and scene synthesis. To put the scale of \\data-Interiors in perspective, the number of scenes in \\data-Interiors is significantly larger than the 400 or so existing hand-built interactive embodied AI scenes~\\cite{kolve2017ai2thor,gan2020threedworld,li2021igibson,szot2021habitat}.\n\n\\textbf{Visual styles.} Objects in the world can be constructed in many styles and often differ in style based on the time-period, geographic location, and artist's style. \\data objects cover a vast set of visual styles, including 3D scans, 3D modeled objects from virtually any platform, point clouds, and photo-realism via physically based rendering (PBR)~\\cite{pharr2016physically}.\nMoreover, instances of objects often appear with many styles, which is critical for training and evaluating robust computer vision models~\\cite{radford2021learning}. Figure~\\ref{fig:visDiv} shows examples of chairs in \\data in many different styles, including Gothic, modern, Victorian, cartoon, and abstract.\n\n\\textbf{Statistics.} \\data 1.0 includes 818K 3D objects, designed by 160K artists. There are ${>}$2.35M tags on the objects, with ${>}$170K of them being unique. We estimate that the objects have coverage for nearly 21K WordNet entities~\\cite{miller1995wordnet} (see appendix for details). Objects were uploaded between 2012 and 2022, with over 200K objects uploaded uploaded just in 2021. Figure~\\ref{fig:metadata vis} visualizes several statistics of the dataset, including the breakdown of objects into their self-assigned Sketchfab categories, a word cloud over the tags, a frequency plot of the tags, and the number of objects in \\datalvis categories. \n\n\\section{Applications}\n\\label{sec:applications}\n\nIn this section, we present 4 initial distinct applications of \\data, including 3D generative modeling, instance segmentation with CP3D, open-vocabulary ObjectNav, and analyze robustness in computer vision models. %\n\n\\subsection{3D Generative Modeling}\\label{sec:3d-gen-modeling}\n\n\\begin{figure}[t!]\n    \\begin{subfigure}[t]{\\columnwidth}\n        \\vspace{-0.225in}\n        \\makebox[\\textwidth][c]{\n        \\includegraphics[width=1.075\\textwidth]{assets/shapenet-single-col.png}\n        }\n        \\vspace{-0.31in}\n        \\caption{Comparison of \\emph{Bag}s generated with \\data and ShapeNet.}\n    \\end{subfigure}\n    \\begin{subfigure}[t]{\\columnwidth}\n        \\vspace{-0.025in}\n        \\makebox[\\textwidth][c]{\n            \\includegraphics[width=1.075\\textwidth]{assets/fv2.png}\n        }\n        \\vspace{-0.31in}\n        \\caption{\\emph{Shoe} and \\emph{Fruit\\&Veg.} generations.}\n    \\end{subfigure}\n    \\begin{subfigure}[t]{\\columnwidth}\n        \\vspace{-0.025in}\n        \\makebox[\\textwidth][c]{\n            \\includegraphics[width=1.075\\textwidth]{assets/fv1.png}\n        }\n        \\vspace{-0.31in}\n        \\caption{\\emph{Fruit\\&Veg.} interpolation.}\n    \\end{subfigure}\n    \\caption{(a) Example GET3D \\emph{Bag} object generations using \\data and ShapeNet models for training. (b) Additional \\emph{Shoe} and \\emph{Fruit\\&Veg} generations from \\data models. (c) models generated when interpolating between two, randomly sampled, latent encodings with our trained Fruit\\&Veg{.} model; what appears to be a pumpkin smoothly transforms into a mushroom.}\n    \\vspace{-1em}\n    \\label{fig:text-to-3d}\n\\end{figure}\n\n3D generative modeling has shown much improvement recently with models such as GET3D~\\cite{gao2022get3d} delivering impressive high quality results with rich geometric details. GET3D is trained to generate 3D textured meshes for a category and produces impressive 3D objects for categories like \\emph{Car}, \\emph{Chair}, and \\emph{Motorcycle} using data from ShapeNet~\\cite{chang2015shapenet}. \\data contains 3D models for many diverse categories including tail categories which are not represented in other datasets. It also contains diverse and realistic object instances per category. This scale and diversity can be used to train large vocabulary and high quality 3D generative models. In this work, we showcase the potential of this data as follows. We choose three categories of objects, \\emph{Shoe}, \\emph{Bag}, and \\emph{Fruit\\&Veg}, and subsample objects from \\data to create three separate datasets containing, respectively, 143 shoes, 816 bags, and 571 fruits \\& vegetables (116 apples, 112 gourds, 92 mushrooms, 68 bananas, 52 oranges, 52 pears, 31 potatoes 24 lemons, and 24 pineapples). For comparison, we also train a GET3D model on the set of 83 bags from the ShapeNet dataset. Fig.~\\ref{fig:text-to-3d} shows a collection of 3D objects generated by our trained GET3D models. Qualitatively, the 3D-meshes generated by the \\data-trained models are high-quality and diverse, especially when compared to the generations from the ShapeNet-trained model. To quantify this observation, we asked crowdworkers to rate the diversity of \\emph{Bag} generations produced by the \\data and ShapeNet trained models. When shown collections of nine randomly sampled generations from both models, workers rated the collection generated from the \\data trained model as more diverse in appearance 91\\% of the time.\n\nOur fruits and vegetables, composed of 9 varieties produces perhaps the highest quality output, a promising signal that can inspire future work in text-to-3D generation.\n\n\\subsection{Instance Segmentation with CP3D}\n\n\\begin{figure}\n    \\vspace{-0.215in}\n    \\centering\n    \\makebox[\\columnwidth][c]{\n        \\includegraphics[width=1.085\\columnwidth]{assets/cp3d.png}\n    }\n    \\vspace{-0.31in}\n    \\caption{An illustration of 3DCP (3D copy-paste) for segmentation augmentation. We render 3D objects from multiple views and paste them over LVIS training images.}\n    \\label{fig:copy-paste-seg}\n    \\vspace{-1em}\n\\end{figure}\n\n\\begin{table}[b!]\n\\centering\n\\begin{tabular}{l ll ll ll ll}\n\\toprule\nMethod & AP & APr & APc & APf \\\\\n\\midrule\nRFS~\\cite{gupta2019lvis} & 23.7 & 13.3 & 23.0 & 29.0\\\\\nEQLv2~\\cite{tan2021equalization} & 25.5 & 17.7 & 24.3 & 30.2\\\\\nLOCE~\\cite{feng2021exploring} & 26.6 & 18.5 & 26.2 & 30.7\\\\\nNorCal with RFS~\\cite{pan2021model} & 25.2 & 19.3 & 24.2 & 29.0\\\\\nSeesaw~\\cite{wang2021seesaw} & 26.4 & 19.5 & 26.1 & 29.7\\\\\nGOL~\\cite{alexandridis2022long} & 27.7 & 21.4 & 27.7 & 30.4\\\\\nGOL + 3DCP & \\textbf{28.3} & \\textbf{21.8} & \\textbf{28.3} & \\textbf{31.1} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Comparison of our approach (GOL+3DCP) against SoTA Mask-RCNN ResNet-50 models on LVIS. We report results for APr, APc, and APf which measure AP for categories that are rare (appear in 1-10 images), common (appear in 11-100 images), and frequent (appear in $>$100 images), respectively}\n\\label{seg_table}\n\\vspace{-1em}\n\\end{table}\n\n\\begin{figure*}[t!]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{assets/pt-obj.png}\n    \\vspace{-0.1in}\n    \\caption{An existing ProcTHOR scene (left) and a semantically similar ProcTHOR generatable scene with \\data objects (right).}\n    \\vspace{-1.8em}\n\\end{figure*}\n\nA key advantage of using simulated data for computer vision is that it is much cheaper to obtain expert annotations. Annotated \\data objects can be rendered into images, allowing them to serve as a rich source of additional data that can be used to enhance model performance on 2D computer vision tasks. As a proof-of-concept demonstrating the effectiveness of this approach, we use segmented data from \\data objects as auxiliary labels for training models on the LVIS dataset for Large Scale Instance Segmentation \\cite{gupta2019lvis}. The LVIS dataset contains instance segmentation masks for 1200 object categories that occur throughout a set of 164k images. Recognition is especially challenging in this task due to the long tail of the object category distribution in this dataset. LVIS categories only contain an average 9 instances across the dataset, so training on simulated data is a promising approach for overcoming the challenges of learning in this low-sample regime. \n\nUsing the LVIS-annotated subset of \\data, we introduce 3DCP: an enhancement to the simple, but effective, copy-and-paste technique of\\cite{ghiasi2021simple}. Figure~\\ref{fig:copy-paste-seg} shows an example of the setup for 3DCP. Here, we render different views of 3D objects and paste them on-top of existing LVIS images. We render 5 distinct views of each object and cache them for use throughout training. During training, an image is selected for the copy-paste augmentation with 0.5 probability, and once selected, 1-3 images of randomly chosen LVIS-annotated \\data objects are pasted onto the selected training image. The segmentation masks of the selected objects are added to the training image's annotation as well. Object images and masks are randomly scaled and translated before being pasted. We use this strategy to finetune the pretrained ResNet-50 Mask-RCNN \\cite{he2017mask,he2016deep} of \\cite{alexandridis2022long}. As shown in Tab.\\ref{seg_table}, simply finetuning this model for 24 epochs yields performance gains across several metrics.\n\n\\vspace{-0.05in}\n\n\\subsection{Open-Vocabulary ObjectNav}\n\n\\vspace{-0.05in}\n\nIn this section, we introduce open-vocabulary ObjectNav, a new task propelled by the vast diversity of objects that appear in \\data. Here, an agent is placed at a random starting location inside of a home and tasked to navigate to a target object provided from a text description (\\eg \\emph{``Raspberry Pi Pico''}). To facilitate this task, we procedurally generate 10K new homes in ProcTHOR~\\cite{deitke2022procthor} fully populated with objects from \\data-LVIS. Until now, ObjectNav tasks have focused on training agents to navigate to 20 or so target objects provided their category label~\\cite{deitke2022retrospectives,ramrakhya2022habitat,deitke2022procthor}, and existing interactive embodied AI simulations, including ProcTHOR, only include around 2K total objects across around 100 object types~\\cite{li2021igibson,deitke2022procthor,szot2021habitat}. In this work, we take a large step to massively scale the number of target objects used in ObjectNav (\\textbf{20 $\\to$ OpenVocab}), the number of objects available in simulation (\\textbf{2K $\\to$ 36K}), and the number of object types of the objects (\\textbf{100 $\\to$ 1.1K}).\n\n\\textbf{Object placement.} To make the placement of objects in the houses more natural, we use the \\datalvis subset and annotate placement constraints for each object category. Specifically, we annotate if objects of a given category typically appears on the floor, on-top of a surface, or on a wall. If instances of the object category may appear on the floor, we also annotate whether it may appear in the middle of the scene (\\eg a clutter object like a basketball) or on the edge of the scene (\\eg a toilet or a fridge). For objects placed on the floor, we also to automatically detect flat regions on top of the object's mesh to place surface object types. The annotations are used by ProcTHOR for sampling objects to place in a scene. We also filter out \\datalvis objects that do not appear inside of homes, such as a jet plane. Structural objects, like doors and windows, are inherited from ProcTHOR as they would require additional cleanup.\n\n\\textbf{Object size correction.} Objects in Sketchfab may be uploaded at unnatural scales (\\eg a plant being as large as a tower). We therefore scale the objects to be of a reasonable size for them to look natural in a house. Here, for each object category, we annotate the maximum bounding box dimension length that every instance of the object category should be scaled to. For example, we annotate the maximum bounding box dimension for bookcase to be 2 meters \nand fork to be 0.18 meters. If a 3D modeled bookcase then has a bounding box of 20m$\\times$6m$\\times$3m, we shrink each side by a factor of $\\max(20, 6, 3)/2 = 5$. \n\n\\textbf{Preprocessing for AI2-THOR.} We add support to AI2-THOR for loading objects on the fly at runtime. Previously, all objects had to be stored in a Unity build, but such an approach is impractical when working with orders of magnitude more object data. For each object, we compress it with Blender~\\cite{blender} by joining all of its meshes together, decimate the joined mesh such that it has at most 5K vertices, and bake all the UV texture maps into a single texture map. We then generate colliders using V-HACD~\\cite{mamou2016volumetric} to support rigid-body interactions.\n\n\\textbf{Approach.}\nGiven procedural houses populated with \\datalvis, the task is to navigate to the proximity of a chosen target object and invoke a task-completion action when the target object is in sight, given an open-vocabulary description formed with the template ``\\texttt{a \\{name\\} \\{category\\}}\". The \\texttt{name} is the object name given by its creator, which is often descriptive. We filter each by whether it is detected as being written in English by a language detector \\cite{joulin2016bag, joulin2016fasttext}, and fall back to a class-only description for non-English \\texttt{name}. Examples of the possible expressions include \\emph{``a victorian-monobike motorcycle\"}, \\emph{``a unicorn pony\"}, or \\emph{``a dino ghost lizard\"}. The agent, similar to the ones in \\cite{Khandelwal2022SimpleBE}, observes an RGB egocentric view of the environment, pre-processed by the visual branch of a frozen ResNet-50 CLIP model \\cite{radford2021learning} -- the target description is pre-processed by the corresponding text branch. We train the agent with DD-PPO \\cite{Wijmans2020DDPPOLN} and evaluate on houses with floor plans, objects, and descriptions unseen in training. We use the AllenAct~\\cite{AllenAct} framework to train our agent. Our trained agent achieves a success rate of 19.9\\%, for a random policy success of 5.1\\%. For more details about the experiment refer to the appendix.%\n\n\\vspace{-0.05in}\n\n\\subsection{Analyzing Robustness}\n\n\\vspace{-0.05in}\n\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{assets/robustness4.png}\n    \\caption{Examples of objects rendered from random orientations and their 0-shot classification categories with the CLIP ViT-B/32.}\n    \\label{fig:robustness}\n    \\vspace{-1em}\n\\end{figure}\n\nA persistent bias present in many image datasets, \\eg ImageNet~\\cite{russakovsky2015imagenet}, is that the subjects of interest are generally photographed from a forward-facing, canonical, orientation. When, for example, taking a photograph of a television, few would choose to take this photograph crouched on the floor behind the television. This impact of this bias was studied by Alcorn~\\etal~\\cite{Alcorn2019StrikeAPose} who find that modern computer vision systems are highly susceptible to deviations from canonical poses. This is more than a theoretical problem: computer vision systems deployed in the real world will frequently encounter objects in non-canonical orientations and in many applications, \\eg autonomous driving, it will be safety critical that they behave well.\n\nGiven the above, we adopt the experimental design of Alcorn~\\etal and design, using \\data assets, a benchmark for evaluating the robustness of state-of-the-art computer vision classification models to orientation shifts. In particular, for each object in our \\datalvis subset, we render 12 images of the object from random orientations rendered upon a background with RGB values equalling the mean RGB values from ImageNet; see Fig.~\\ref{fig:robustness} for examples. This ability to, at scale, render objects from random viewpoints is a practical impossibility in the real world but is made trivial when using 3D assets. We then evaluate several modern open-domain image-classification networks (constrained to the ${\\approx}1{,}200$ \\textsc{LVIS} categories) on these images and report 4 metrics for each model. These metrics include:\n\n\\begin{table}[t!]\n    \\centering\n    \\resizebox{\\columnwidth}{!}{\n    \\begin{tabular}{l|cc|cc|cc}\n        \\toprule\n        & \\multicolumn{2}{c}{Random Rotation} & \\multicolumn{2}{c}{Any Rotation}\\\\\\cmidrule(lr){2-3}\\cmidrule(lr){4-5}\n        Model & Top-1 & Top-5 & Top-1 & Top-5 & $\\Delta$Top-1 \\\\\n        \\midrule\\\\[-0.15in]\n        OpenAI-400M~\\cite{radford2021learning}\\\\\n        RN50 & 21.4\\% & 45.0\\% & 43.9\\% & 70.8\\% & 22.5\\% \\\\\n        ViT-L/14 & 29.1\\% & 54.5\\% & \\textbf{52.3\\%} & 77.2\\% & 23.2\\%\n        \\\\[0.05in]\n        \\midrule\\\\[-0.15in]\n        LAION-400M~\\cite{schuhmann2021laion}\\\\\n        ViT-B/32 & 24.1\\% & 48.5\\% & 46.9\\% & 74.2\\% & 22.8\\% \\\\\n        ViT-L/14 & 30.6\\% & 56.8\\% & 50.5\\% & 77.0\\% & 19.9\\% \\\\[0.05in]\n        \\midrule\\\\[-0.15in]\n        LAION-2B~\\cite{schuhmann2022laion}\\\\\n        ViT-B/32 & 27.0\\% & 51.8\\% & 50.3\\% & 76.1\\% & 23.3\\% \\\\\n        ViT-L/14 & \\textbf{32.9\\%} & \\textbf{59.2\\%} & 52.1\\% & \\textbf{78.0\\% } & 19.2\\% \\\\\n        ViT-H/14 & 32.3\\% & 58.8\\% & 50.1\\% & 77.3\\% & \\textbf{17.8\\%} \\\\[0.05in]\n        \\bottomrule\n    \\end{tabular}\n    }\n    \\caption{Evaluating 0-shot CLIP classification models on our rotational robustness benchmark. $\\Delta$Top-1 denotes the difference between \\emph{Top-1 Any Rotation} and \\emph{Top-1 Random Rotation}. Models are strongly overfit to standard views of objects.}\n    \\label{tab:robustness}\n    \\vspace{-1em}\n\\end{table}\n\n\\noindent$\\bullet$ \\emph{Top-1 Random Rotation} -- the frequency with which a model correctly classifies an image as belonging to the respective \\textsc{LVIS} category.\n\n\\noindent$\\bullet$ \\emph{Top-1 Any Rotation} -- the frequency with which a model classifies an image correctly from at least one of the 12 random orientations.\n\nThis second metric is diagnostic and serves to represent a model's performance when shown an object from a canonical pose. We also have \\emph{Top-5} variants of the above metric where the correct category need only be in the top 5 predictions from the model.\nWe report our results in Tab.~\\ref{tab:robustness} in which we evaluate a variety of performant pretrained models. Comparing the gap in performance between the \\emph{Top-$k$ Random Rotation} and \\emph{Top-$k$ Any Rotation} metrics we find that model performance dramatically degrades when viewing objects from unusual orientations.\n\n\\vspace{-0.05in}\n\n\\section{Conclusion}\n\\label{sec:conclusion}\n\\vspace{-0.05in}\n\nWe present \\data, a next-generation 3D asset library containing 818K high-quality, diverse, 3D models with paired text descriptions, titles, and tags. As a small glimpse of the potential uses of \\data, we present four experimental studies showing how \\data can be used to power (1) generative 3D models with clear future applications to text-to-3D generation, (2) improvements to classical computer vision tasks such as instance segmentation, (3) the creation of novel embodied AI tasks like Open Vocabulary Object Navigation, and (4) quantifying the rotational robustness of vision models on renderings of objects. We hope to see \\data enable a new universe of new applications for computer vision.\n\n{\\small\n}\n\n\\newpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{\\dataset: A Universe of 10M+ 3D Objects}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{figure}[h!]\n  \\vspace{-0.45in}\n  \\centering\n  \\includegraphics[width=\\textwidth]{images/cover-23.jpg}\n  \\caption{Objaverse-XL includes a ginormous collection of diverse 3D objects from a variety of sources. Here, we show examples of objects in Objaverse-XL rendered in a scene.}\n  \\vspace{-0.05in}\n  \\label{fig:teaser}\n\\end{figure}\n\n\\begin{abstract}\nNatural language processing and 2D vision models have attained remarkable proficiency on many tasks primarily by escalating the scale of training data. However, 3D vision tasks have not seen the same progress, in part due to the challenges of acquiring high-quality 3D data. In this work, we present Objaverse-XL, a dataset of over 10 million 3D objects. Our dataset comprises deduplicated 3D objects from a diverse set of sources, including manually designed objects, photogrammetry scans of landmarks and everyday items, and professional scans of historic and antique artifacts. Representing the largest scale and diversity in the realm of 3D datasets, Objaverse-XL enables significant new possibilities for 3D vision. Our experiments demonstrate the improvements enabled with the scale provided by Objaverse-XL. We show that by training Zero123 on novel view synthesis, utilizing over 100 million multi-view rendered images, we achieve strong zero-shot generalization abilities. We hope that releasing Objaverse-XL will enable further innovations in the field of 3D vision at scale.\n\\end{abstract}\n\n\\section{Introduction}\n\nScale has been paramount to recent advances in AI. Large models have produced breakthroughs in language comprehension and generation~\\cite{brown2020language,openai2023gpt}, representation learning~\\cite{radford2021learning}, multimodal task completion~\\cite{alayrac2022flamingo,Lu2022UnifiedIOAU}, image generation~\\cite{ramesh2022hierarchical,rombach2022high}, and more. With an increasing number of learnable parameters, modern neural networks consume increasingly large volumes of data. As data has scaled up, the capabilities exhibited by models has dramatically increased.\n\nJust a few years ago, GPT-2~\\cite{radford2019language} broke data barriers by consuming roughly 30 billion language tokens and demonstrated promising zero shot results on NLP benchmarks. Now, models like Chinchilla~\\cite{hoffmann2022training} and LLaMA~\\cite{touvron2023llama} consume trillions of web crawled tokens and easily surpass GPT-2 at benchmarks and capabilities. In computer vision, ImageNet~\\cite{deng2009imagenet}, with 1 million images, was the gold standard for representation learning until scaling to billions of images, via web crawled datasets like LAION-5B~\\cite{schuhmann2022laion}, produced powerful visual representations like CLIP~\\cite{radford2021learning}. Key to scaling up from millions of data points to billions and beyond has been the shift from assembling datasets manually to assembling them from diverse sources via the web.\n\nAs language and image data has scaled up, applications that require other forms of data have been left behind. Notable are applications in 3D computer vision, with tasks like 3D object generation and reconstruction, continue to consume small handcrafted datasets. 3D datasets such as ShapeNet~\\cite{chang2015shapenet} rely on professional 3D designers using expensive software to create assets, making the process tremendously difficult to crowdsource and scale. The resulting data scarcity has become a bottleneck for learning-driven methods in 3D computer vision. For instance, 3D object generation currently lags far behind 2D image generation, and current 3D generation approaches often still leverage models trained on large 2D datasets instead of being trained on 3D data from scratch. As demand and interest in AR and VR technologies goes up, scaling up 3D data is going to be increasingly crucial.\n\nWe introduce Objaverse-XL, a large-scale, web-crawled dataset of 3D assets. Advances in 3D authoring tools, demand, and photogrammetry, have substantially increased the amount of 3D data on the Internet. This data is spread across numerous locations including software hosting services like Github, specialized sites for 3D assets like Sketchfab, 3D printing asset sources like Thingiverse, 3D scanning platforms like Polycam, and specialized sites like the Smithsonian Institute. Objaverse-XL crawls such sources for 3D objects, providing a significantly richer variety and quality of 3D data than previously available, see Figure~\\ref{fig:teaser}. Overall, Objaverse-XL comprises of over 10 million 3D objects, representing an order of magnitude more data than the recently proposed Objaverse 1.0~\\cite{deitke2022objaverse} and is two orders of magnitude larger than ShapeNet.\n\nThe scale and diversity of assets in Objaverse-XL significantly expands the performance of state-of-the-art 3D models. The recently proposed Zero123~\\cite{liu2023zero1to3} model for novel view synthesis, when pre-trained with Objaverse-XL, shows significantly better zero-shot generalization to challenging and complex modalities including photorealistic assets, cartoons, drawings and sketches. Similar improvements are also seen with PixelNerf which is trained to synthesize novel views given a small set of images. On each of these tasks, scaling pre-training data continues to show improvements from a thousand assets all the way up to 10 million, with few signs of slowing down, showing the promise and opportunities enabled with web scale data.\n\n\\section{Related Work}\n\n\\paragraph{Pre-training Datasets.}\nMassive datasets have a prevalent role in modern, data-driven AI as they have produced powerful and general representations when paired with large-scale training. \nIn computer vision, ImageNet~\\cite{deng2009imagenet}, introduced nearly 14 years ago, has become the standard pre-training dataset of state-of-the-art visual models in object detection~\\cite{ren2015faster,carion2020end}, instance segmentation~\\cite{he2017mask,cheng2021mask2former} and more.\nMore recently, large image datasets, such as LAION-5B~\\cite{schuhmann2022laion}, have powered exciting advances in generative AI, such as Stable Diffusion~\\cite{rombach2022high}, and have given rise to new general-purpose vision and language representations with models like CLIP~\\cite{radford2021learning} and Flamingo~\\cite{alayrac2022flamingo}. This year, SAM~\\cite{kirillov2023segment}\n\n\\begin{minipage}{\\textwidth}\n  \\begin{minipage}[b]{0.7\\textwidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/distribution8.jpg}\n    \\captionof{figure}{t-SNE projection of CLIP L/14 embeddings on a subset of rendered objects. Compared to \\datasetone (orange), \\dataset more densely captures the distribution of 3D assets.}\n    \\label{fig:related}\n  \\end{minipage}\n  \\hfill\n  \\begin{minipage}[b]{0.275\\textwidth}\n    \\centering\n    \\begin{adjustbox}{width=0.975\\textwidth,center}\n        \\begin{tabular}{lr}\n            \\toprule\n            Source & \\# Objects\\\\\n            \\midrule\n            IKEA~\\cite{lim2013parsing} & 219\\\\\n            GSO~\\cite{downs2022google} & 1K\\\\\n            EGAD~\\cite{morrison2020egad} & 2K\\\\\n            OmniObject3D~\\cite{wu2023omniobject3d} & 6K\\\\\n            PhotoShape~\\cite{park2018photoshape} & 5K\\\\\n            ABO~\\cite{collins2022abo} & 8K\\\\\n            Thingi10K~\\cite{zhou2016thingi10k} & 10K\\\\\n            3d-Future~\\cite{fu20213d} & 10K\\\\\n            ShapeNet~\\cite{chang2015shapenet} & 51K\\\\\\midrule\n            Objaverse 1.0~\\cite{deitke2022objaverse} & 800K\\\\[0.025in]\n            \\textbf{Objaverse-XL} & \\textbf{10.2M}\\\\\n            \\bottomrule\n        \\end{tabular}\n    \\end{adjustbox}\n    \\vspace{1mm}\n      \\captionof{table}{Number of 3D models in common datasets. \\dataset is over an order of magnitude larger than prior datasets.}\n      \\label{tab:related}\n    \\end{minipage}\n\\end{minipage}\\vspace*{5mm}\n\nintroduced a dataset of one billion object masks used to train a model capable of segmenting any object from an image. \nIn language understanding, datasets like Common Crawl~\\cite{commoncrawl} have culminated in unprecedented capabilities of large language models such as GPT-4~\\cite{openai2023gpt}, which in turn power mainstream applications like ChatGPT.\nThe impact of large datasets is undeniable. \nHowever, current efforts to collect massive datasets focus on image and language modalities. \nIn this work we introduce and release publically a massive dataset of 3D objects, called \\dataset. \nGiven the promise of large datasets for 2D vision and language, we believe \\dataset will accelerate research in large-scale training for 3D understanding.\n\n\\paragraph{3D Datasets.}\nExisting 3D datasets have been instrumental in yielding significant findings in 3D over the years. \nShapeNet~\\cite{chang2015shapenet} has served as the testbed for modeling, representing and predicting 3D shapes in the era of deep learning. \nShapeNet provides a collection of 3D shapes, in the form of textured CAD models labeled with semantic categories from WordNet~\\cite{miller1995wordnet}. \nIn theory, it contains 3M CAD models with textures. In practice, a small subset of 51K models is used after filtering by mesh and texture quality. \nNotwithstanding its impact, ShapeNet objects are of low resolution and textures are often overly simplistic.\nOther datasets such as ABO~\\cite{collins2022abo}, GSO~\\cite{downs2022google}, and OmniObjects3D~\\cite{wu2023omniobject3d} improve on the texture quality of their CAD models but are significantly smaller in size with the largest constituting 15K CAD models.\nRecently, \\datasetone~\\cite{deitke2022objaverse} introduced a 3D dataset of 800K 3D models with high quality and diverse textures, geometry and object types, making it 15$\\times$ larger than prior 3D datasets. \nWhile impressive and a step toward a large-scale 3D dataset, \\datasetone remains several magnitudes smaller than dominant datasets in vision and language. \nAs seen in Figure~\\ref{fig:related} and Table~\\ref{tab:related}, \\dataset extends \\datasetone to an even larger 3D dataset of $10.2$M unique objects from a diverse set of sources, object shapes, and categories. \nWe discuss \\dataset and its properties in Section~\\ref{sec:obaverse}.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\textwidth]{images/models.jpg}\n    \\caption{\\textbf{Examples of 3D objects from various sources of \\dataset} spanning GitHub, Thingiverse, Polycam, the Smithsonian Institution, and Sketchfab. Objects from Thingiverse do not include color information, so each object's primary color is randomized during rendering.}\n\\end{figure}\n\n\\paragraph{3D Applications.}\nThe potential of a massive 3D dataset like \\dataset promises exciting novel applications in computer vision, graphics, augmented reality and generative AI. \nReconstructing 3D objects from images is a longstanding problem in computer vision and graphics. \nHere, several methods explore novel representations~\\cite{choy20163d,wang2018pixel2mesh,mescheder2019occupancy,mildenhall2020nerf}, network architectures~\\cite{gkioxari2019mesh,yu2021pixelnerf} and differentiable rendering techniques~\\cite{kato2018neural,chen2019learning,ravi2020accelerating,liu2023humans,liu2022shadows} to predict the 3D shapes and textures of objects from images with or without 3D supervision. \nAll of the aforementioned projects experiment on the small scale ShapeNet. \nThe significantly larger \\dataset could pave the way to new levels of performance, and increase generalization to new domains in a zero-shot fashion. \nOver the past year, generative AI has made its foray into 3D. MCC~\\cite{wu2023multiview} learns a generalizable representation with self-supervised learning for 3D reconstruction.\nDreamFusion~\\cite{poole2022dreamfusion} and later on Magic3D~\\cite{lin2023magic3d} demonstrated that 3D shapes could be generated from language prompts with the help of text-to-image models. \nPoint-E~\\cite{nichol2022point} and Shape-E~\\cite{jun2023shap} also train for text-to-3D with the help of 3D models from an undisclosed source. \nRecently, Zero123~\\cite{liu2023zero1to3} introduced an image-conditioned diffusion model which generates novel object views and is trained on \\datasetone. \nStable Dreamfusion~\\cite{stable-dreamfusion} replaces the text-to-image model in DreamFusion with the 3D-informed Zero123 and shows improved 3D generations. \nRecent findings in AI and scaling laws~\\cite{kaplan2020scaling,hoffmann2022training} suggest that both generative and predictive models benefit from larger models and larger pre-training datasets. \nFor 3D, \\dataset is by far the largest 3D dataset to date and has the potential to facilitate large-scale training for new applications in 3D.\n\\section{Objaverse-XL}\n\\label{sec:obaverse}\n\n\\dataset is a web scale 3D object dataset composed of a highly diverse set of 3D data sources on the internet. In this section, we discuss the sources, metadata of the objects, and provide an analysis of the objects.\n\n\\subsection{Composition}\n\n\\dataset is composed of 3D objects coming from several sources, including GitHub, Thingiverse, Sketchfab, Polycam, and the Smithsonian Institution. We detail each source below.\n\n\\begin{figure}\n    \\begin{subfigure}{0.43\\textwidth}\n        \\centering\n        \\includegraphics[width=0.9\\textwidth]{images/us-density.pdf}\n        \\caption{Object location density in the United States} %\n        \\label{fig:usdensity} %\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}{0.575\\textwidth}\n        \\centering\n        \\includegraphics[width=\\textwidth]{images/geotag6.pdf}\n        \\caption{Choropleth map of objects per country (log scale)} %\n        \\label{fig:geotag5} %\n    \\end{subfigure}\n    \\\\[0.10in]\n    \\begin{subfigure}{0.43\\textwidth}\n        \\centering\n        \\includegraphics[width=\\textwidth]{images/sankey-4.pdf}\n        \\caption{Sankey diagram of Objaverse-XL sources} %\n        \\label{fig:geotag5} %\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}{0.575\\textwidth}\n        \\centering\n        \\includegraphics[width=0.95\\textwidth]{images/aesthetic-wordcloud-3.jpg}\n        \\caption{Word cloud of file names for GitHub} %\n    \\end{subfigure}\n    \\\\[0.10in]\n    \\begin{subfigure}{\\textwidth}\n        \\includegraphics[width=0.245\\textwidth]{images/plot_1.pdf}%\n        \\hfill%\n        \\includegraphics[width=0.245\\textwidth]{images/plot_2.pdf}%\n        \\hfill%\n        \\includegraphics[width=0.245\\textwidth]{images/plot_3.pdf}%\n        \\hfill%\n        \\includegraphics[width=0.245\\textwidth]{images/anim-9.pdf}\n        \\caption{Statistics of the metadata extracted from the 3D objects.}\n        \\label{fig:statMeta}\n    \\end{subfigure}\n    \\caption{\\textbf{Analysis of metadata from Objaverse-XL.} Locations of geotagged objects in (a) the United States and (b) around the world. (c) Various sources and their contribution to \\dataset. (d) Frequency of filenames of GitHub objects. (e) Further statistics of collected 3D objects.}\n    \\label{fig:stats} %\n\\end{figure}\n\n\\textbf{GitHub} is a popular online platform for hosting code. We index 37M public files that contain common 3D object extensions; in particular, \\texttt{.obj}, \\texttt{.glb}, \\texttt{.gltf}, \\texttt{.usdz}, \\texttt{.usd}, \\texttt{.usda}, \\texttt{.fbx}, \\texttt{.stl}, \\texttt{.dae}, \\texttt{.ply}, \\texttt{.abc}, and \\texttt{.blend}. These extensions were chosen as they are best supported in Blender, which we use to render 2D images of the 3D objects. We only index objects that come from ``base'' GitHub repositories (\\textit{i.e.} non-forked repos, excluding forks that had more stars than the original repo). In total, the files come from over 500K repositories.\n\nAcross all of \\dataset, objects are deduplicated by file content hash, which removes approximately 23 million files. Among the remaining files, we were able to import and successfully render 5.5 million of those files. Files that were not successfully rendered were either caused by import compatibility issues (\\textit{i.e.} FBX ASCII files are not natively importable to Blender), no meshes are in the files, or the file is not a valid 3D file (\\textit{e.g.} an \\texttt{.obj} file may be a C compiler file instead of a Wavefront Object file). Moving forward, we expect a solution for converting 3D file formats into a consolidated representation may yield several million more unique 3D objects.\n\n\\textbf{Thingiverse} is a platform for sharing objects most commonly used for 3D printing. We index and download around 3.5 million objects from the platform, which are predominantly released under Creative Commons licenses. The vast majority of the objects are STL files, which are often watertight meshes that are untextured, and serve as useful data for learning a shape prior. During rendering, we randomize the colors to broaden the distribution of the images.\n\n\\textbf{Sketchfab} is an online platform where users can publish and share 3D models, encompassing a broad variety of categories. The data sourced from Sketchfab for our project is specifically from Objaverse 1.0, a dataset of 800K objects consisting of Creative Commons-licensed 3D models. Each model is distributed as a standardized GLB file. The 3D models are freely usable and modifiable, covering an array of object types, from real-world 3D scans to intricate designs created in 3D software.\n\n\\textbf{Polycam} is a 3D scanning mobile application designed to facilitate the acquisition and sharing of 3D data. One of its salient features is the \\textit{explore} functionality, which enables members of the user community to contribute their 3D scans to a publicly accessible database. In the context of our dataset, we focus specifically on the subset of objects within the explore page that are designated as savable. These savable objects are governed by a Creative Commons Attribution 4.0 International License (CC-BY 4.0). We indexed 72K objects that were marked as savable and licensed under a CC-BY 4.0 license. Following deduplication, we obtain 71K unique objects.\n\n\\textbf{Smithsonian 3D Digitization} is a project by the Smithsonian Institution dedicated to digitizing their vast collection of historical and cultural artifacts. The project has provided us with a set of 2.4K models, all licensed under a CC0 license, which signifies that these works are fully in the public domain and free for use without any restrictions. The objects in this collection are primarily scans of real-world artifacts. Each model is distributed in a standardized compressed GLB format.\n\n\\subsection{Metadata}\n\\label{sec:metadata}\n\nEach object comes with metadata from its source, and we also extract metadata from it in Blender and from its CLIP ViT-L/14 features. We describe the metadata acquisition process below.\n\n\\paragraph{Source Metadata.} From the source, we often get metadata such as its popularity, license, and some textual description. For example, on GitHub, the popularity is represented by the stars of the object's repository and the file name serves as the object's textual pair.\n\n\\paragraph{Blender Metadata.} For each object that we render, we obtain the following metadata for it: \\texttt{sha256}, \\texttt{file-size}, \\texttt{polygon-count}, \\texttt{vertex-count}, \\texttt{edge-count}, \\texttt{material-count}, \\texttt{texture-count}, \\texttt{object-count}, \\texttt{animation-count},\n\\texttt{linked-files},\n\\texttt{scene-dimensions}, and\n\\texttt{missing-textures}.\nDuring rendering, for objects that have a missing texture, we randomize the color of that texture. Figure~\\ref{fig:stats} shows some charts extracted from the metadata, including density plots over the number of polygons, vertex counts, and edge counts.%\n\n\\paragraph{Animated Objects.}\nFrom the Blender metadata, we find that the number of animated objects and those with armature (a digital skeleton used to animate 3D models) significantly increases from Objaverse 1.0 to Objaverse-XL. Figure~\\ref{fig:statMeta} (right) shows a bar chart of the increase, specifically from 41K to 459K animated objects and from 34K to 438K objects with armature.\n\n\\paragraph{Model Metadata.}\nFor each object, we extract its CLIP ViT-L/14~\\cite{radford2021learning} image embedding by averaging the CLIP embedding from 12 different renders of the object at random camera positions inside of a hollow sphere. We use the CLIP embeddings to predict different metadata properties, including aesthetic scores, NSFW predictions, face detection, and for detecting holes in the photogrammetry renderings. Section~\\ref{sec:analysis} provides more details on the analysis.\n\n\\begin{figure*}[t!]\n    \\centering\n    \\begin{adjustbox}{center}\n        \\includegraphics[width=1.225\\textwidth]{images/conditioning-3.jpg}\n    \\end{adjustbox}\\\\[-0.10in]\n    \\begin{adjustbox}{center}\n        \\includegraphics[width=1.225\\textwidth]{images/context-4.pdf}\n    \\end{adjustbox}\\\\[0.10in]\n    \\caption{\\textbf{Novel view synthesis on in-the-wild images.} Comparison between Zero123-XL trained on Objaverse-XL and Zero123 trained on Objaverse. Starting from the input view, the task is to generate an image of the object under a specific camera pose transformation. The camera poses are shown beside each example. Significant improvement can be found by training with more data, especially for categories including people (\\textbf{1\\textsuperscript{st} row)}, anime (\\textbf{2\\textsuperscript{nd} row)}, cartoon (\\textbf{3\\textsuperscript{rd} row)}, furniture (\\textbf{4\\textsuperscript{th} row)}, and sketches (\\textbf{5\\textsuperscript{th} row)}. Additionally, viewpoint control is significantly improved (see \\textbf{2\\textsuperscript{nd} row)}.\n    }\n    \\label{fig:zero123}\n\\end{figure*}\n\n\\subsection{Analysis}\n\\label{sec:analysis}\n\n\\paragraph{NSFW annotations.} Most data sources used for the creation of \\dataset already have either a strict NSFW policy or strong self-filtering. However, owing to the web scale of \\dataset we performed NSFW filtering using the rendered images of the objects. Each 3D object is rendered in 12 random views and each rendered image is passed through an NSFW classifier trained on the NSFW dataset introduced in LAION-5B~\\citep{schuhmann2022laion} by~\\citet{gadre2023datacomp} using the CLIP ViT-L/14~\\citep{radford2021learning} features. After careful analysis and manual inspection, we marked a rendered image as NSFW if it has an NSFW score above $0.9$ and a 3D object is marked as NSFW if at least $3$ rendered images are deemed to be NSFW. Overall, only $815$ objects out of the 10M are filtered out as NSFW objects. Note that the high threshold and multi-view consistency are needed due to the distribution shift between LAION-5B and \\dataset along with NSFW classification of certain viewpoint renders of harmless 3D objects.\n\n\\paragraph{Face detection.} We analyze the presence of faces in \\dataset using a detector trained by~\\citet{gadre2023datacomp}. Like NSFW filtering, we count the objects where at least $3$ images contain a detected face. Out of 10M assets, we estimate $266$K objects include faces. However, unlike most web-scale datasets, the faces present in \\dataset often come from the scans of dolls, historical sculptures, and anthropomorphic animations. Hence, there are less privacy concerns with most of these objects.\n\n\\paragraph{Photogrammetry hole detection.} When scanning 3D objects, if the back or bottom of the object is not scanned, rendering from various viewpoints may contain holes, leading to a ``bad'' render image. For example, a non-trivial number of Polycam 3D objects lack the information from the ``back side''. In most cases, images that are rendered from back-side viewpoints are noisy, low-fidelity, or contain holes. To analyze ``bad rendering'' at scale, we manually annotated 1.2K Polycam renders as ``good'' (label $1$) or ``bad'' (label $0$). We trained a ``bad render'' classifier (2-layer MLP) on top of the CLIP ViT-L/14 features of the rendered images; this classifier achieves a cross-validation accuracy of over $90\\%$ with a ``render score'' threshold of $0.5$. Overall, out of $71$K Polycam objects with 12 renders each, we found that $38.20\\%$ renders are ``bad'', with  $58$K objects having at least 2 bad renders.\n\n\\section{Experiments}\n\\label{sec:experiments}\n\n\\subsection{Novel View Synthesis with Zero123-XL}\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.95\\textwidth]{images/scaling_plot5.pdf}\n    \\captionof{figure}{\\textbf{Novel view synthesis at scale.} \\textbf{Left}: PixelNeRF~\\cite{yu2021pixelnerf} trained on varying scales of data and evaluated on a held-out subset of Objavserse-XL. \\textbf{Right}: Zero123~\\cite{liu2023zero1to3} trained on varying scales of data and evaluated on a zero-shot dataset.  Note that the 800K datapoint is  Zero123 and the 10M datapoint is Zero123-XL. The synthesis quality consistently improves with scale. LPIPS is scaled up 10 times for visualization.}\n    \\label{fig:scale}\n    \\vspace*{-5mm}\n\\end{figure*}\n\n\\begin{table}[b!]\n\\centering\n\\vspace*{-4mm}\n\\begin{tabular}{@{}ccccc@{}}\n\\toprule\nZero123-XL                 & PSNR ($\\uparrow$)    & SSIM ($\\uparrow$) & LPIPS ($\\downarrow$) & FID ($\\downarrow$) \\\\ \\midrule\n{Base} & 18.225             & 0.877           & 0.088              & 0.070                        \\\\\n{w/ Alignment Finetuning}               & \\textbf{19.876}             & \\textbf{0.888 }          & \\textbf{0.075}              & \\textbf{0.056}                        \\\\\n \\bottomrule\n\\end{tabular}\n\\vspace{0.05in}\n\\caption{\\textbf{Effect of high-quality data finetuning on Zero123-XL.} When evaluated zero-shot on Google Scanned Objects~\\cite{downs2022google}, a model finetuned on a high-quality alignment subset of \\dataset significantly outperforms the base model trained only on \\dataset.\n}\n\n\\label{tab:alignment}\n\\end{table}\n\nGenerating 3D assets conditioned on in-the-wild 2D images has remained a challenging problem in computer vision. A crucial lesson learned from large language models is that pretraining on simple and easily scalable tasks, such as next word prediction, leads to consistently improved performance and the emergence of zero-shot abilities. An analogous approach in 3D vision is to predict a novel view of an object from an input view. Zero123~\\cite{liu2023zero1to3} recently proposed a view-conditioned diffusion model to perform this task, where the weights of the diffusion model are initialized from Stable Diffusion to leverage its powerful zero-shot image generation abilities. Zero123 used objects in Objaverse 1.0 to render input and novel view pairs as the training dataset.\nWe use this framework to create \\textit{Zero123-XL}, which is the same approach except trained on the much larger Objaverse-XL instead. As shown in~\\cite{liu2023zero1to3}, the pretrained view-conditioned diffusion model can also be plugged into a score distillation framework such as DreamFusion~\\cite{poole2022dreamfusion} or SJC~\\cite{wang2023score} to obtain a 3D assets.\n\n\\paragraph{Zero-shot Generalization.} We found that training Zero123 on Objaverse-XL achieves significantly better zero-shot generalization performance than using Objaverse 1.0. Figure~\\ref{fig:zero123} shows examples from categories of data commonly known to be challenging for baseline systems, including people, cartoons, paintings, and sketches. For example, in both of the examples shown in 2nd and 3rd rows of the first column, Zero123 interprets the input image as a 2D plane and performs a simple transformation similar to a homography transformation. In comparison, Zero123-XL is able to generate novel views that are more consistent with the input view. Additionally, Zero123-XL is able to generate novel views from sketches of objects while keeping the original style as well as object geometric details. These examples show the effectiveness of dataset scaling for zero-shot generalization in 3D.\n\n\\paragraph{Improvement with Scale.} We further quantitatively evaluate the novel view synthesis performance on Google Scanned Objects dataset~\\cite{downs2022google}. As shown in Figure~\\ref{fig:scale}, the rvisual similarity score~\\cite{zhang2018unreasonable} between the predicted novel view and the ground truth view continues to improve as the dataset size increases.\n\n\\paragraph{Alignment Finetuning.}\nInstructGPT~\\cite{ouyang2022training} shows that large-scale pretraining does not directly lead to a model aligned with human preferences. More recently, LIMA~\\cite{zhou2023lima} shows that finetuning a pretrained model on a curated subset with high-quality data can achieve impressive alignment results. We adopted a similar approach here by selecting a high-quality subset of Objaverse-XL that contains 1.3 million objects. Selection is done by defining proxy estimation of human preference based on heuristics including vertex count, face count, popularity on the source website, and source of data, among other metrics. After pretraining the base model on the entire Objaverse-XL, we finetune Zero123-XL on the alignment subset with a reduced learning rate and performed an ablation study to evaluate the effect of alignment finetuning. Table~\\ref{tab:alignment} shows that alignment finetuning leads to significant improvement in zero-shot generalization performance. Please refer to Appendix~\\ref{sec:appendixImpl} for more implementation details regarding our model and experiments.\n\n\\subsection{Novel View Synthesis with PixelNeRF}\nSynthesizing objects and scenes from novel views is a long-standing challenge. Notably, neural radiance fields \\cite{mildenhall2020nerf} have shown impressive capabilities in rendering specific scenes from novel views. However, these methods require dozens of views of an individual scene, and can only synthesize views from the particular scene they were trained for.  More recent methods \\cite{deng2022depth, jain2021putting, wang2021ibrnet, yu2021pixelnerf} have been proposed for constructing NeRF models that generalize across scenes with few input images. Due to the challenging nature of obtaining the necessary camera parameters for training, such methods have traditionally been trained on small scale data sets. With the Objaverse-XL data, we train a PixelNeRF model on over two million objects, magnitudes of more data than has previously been used. We find that PixelNeRF generalizes to novel scenes and objects significantly better and performance improves consistently with scale (Figure \\ref{fig:scale} and Table \\ref{tab:fine-tune}). \n\n\\paragraph{Improvement with Scale.} We train PixelNeRF models conditioned on a single input image at varying scales of data (Figure \\ref{fig:scale}) and evaluate on a held out set of Objaverse-XL objects. We find that novel view synthesis quality consistently improves with more objects even at the scale of 2 million objects and 24 million rendered images. \n\n\\setlength{\\textfloatsep}{5pt}\n\\begin{wrapfigure}{r}{0.5\\columnwidth}\\vspace{-4mm}\n\\centering\n\\begin{tabular}{@{}ccc@{}}\n\\toprule\n                   PixelNeRF                            & {DTU~\\citep{aanaes2016large}} & ShapeNet~\\citep{chang2015shapenet} \\\\ \\midrule\n{Base}                 & 15.32                   & 22.71                        \\\\\n{w/ \\dataset} & \\textbf{17.53 $\\pm .37$}                   & \\textbf{24.22 $\\pm .55$ }                       \\\\ \\bottomrule\n\\end{tabular}\n\n\\captionof{table}{\\small\\textbf{Comparison (PSNR ($\\uparrow$)) of PixelNeRF trained from scratch vs. fine-tuned from Objaverse-XL.} Performance significantly improves from pretraining on the large-scale corpus.}\n\\label{tab:fine-tune}\n  \\vspace{-4mm}\n\\end{wrapfigure}\\paragraph{Generalization to Downstream Datasets.} \nSimilar to pretraining in 2D vision and language, we observe that pretraining on Objaverse-XL with PixelNeRF improves performance when fine-tuning to other datasets such as DTU \\cite{aanaes2016large} and ShapeNet \\cite{chang2015shapenet} (Table \\ref{tab:fine-tune}). We pretrain and fine-tune the model conditioned on a single input view and report the peak signal-to-noise ratio (PSNR).\n \n\n\\vspace*{-2mm}\n\\section{Limitations and Conclusion}\n\\label{sec:conclusion}\n\n\\paragraph{Limitations.}\nWhile \\dataset is more than an order of magnitude larger than its predecessor, \\datasetone, it is still orders of magnitude smaller than modern billion-scale image-text datasets. Future work may consider how to continue scaling 3D datasets and make 3D content easier to capture and create. Additionally, it may not be the case that all samples in \\dataset are necessary to train high performance models. Future work may also consider how to choose datapoints to train on. Finally, while we focus on generative tasks, future work may consider how \\dataset can benefit discriminative tasks such as 3D segmentation and detection.\n\n\\vspace*{-2mm}\n\\paragraph{Conclusion.}\nWe introduce \\dataset, which is comprised of 10.2M 3D assets.\nIn addition to documenting \\dataset's unprecedented scale and sample diversity, we demonstrate the potential of \\dataset for downstream applications.\nOn the task of zero-shot novel view synthesis, we establish empirically promising trends of scaling dataset size, while keeping the model architecture constant.\nWe hope \\dataset will provide a foundation for future work in 3D.\n\\clearpage\n\n\\section*{Acknowledgements}\n\nWe would like to thank Stability AI for compute used to train the experiments and LAION for their support. We would also like to thank Luca Weihs, Mitchell Wortsman, Romain Beaumont, and Vaishaal Shankar, Rose Hendrix, Adam Letts, Sami Kama, Andreas Blattmann, Kunal Pratap Singh, and Kuo-Hao Zeng for their helpful guidance and conversations with the project. Finally, we would like to thank the teams behind several open-source packages used throughout this project, including Blender~\\cite{blender}, PyTorch~\\cite{paszke2019pytorch}, PyTorch Lightning~\\cite{Falcon_PyTorch_Lightning_2019}, D3~\\cite{bostock2011d3}, Matplotlib~\\cite{Hunter:2007}, NumPy~\\cite{harris2020array}, Pandas~\\cite{reback2020pandas}, Wandb~\\cite{wandb}, and Seaborn~\\cite{Waskom2021}. Following the NeurIPS guidelines, we would also like to acknowledge the use of LLMs for helping revise some text and general coding assistance. Finally, we would also like to thank and acknowledge the content creators who contributed to the dataset.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-1512.03012v1.tex",
        "arXiv-2212.08051v1.tex",
        "arXiv-2307.05663v1.tex"
    ],
    "group_id": "group_64",
    "response": "### Summary of 3D Model Datasets: ShapeNet, Objaverse, and Objaverse-XL\n\n#### Title\n3D Model Datasets: An Overview of ShapeNet, Objaverse, and Objaverse-XL\n\n#### Introduction\nThe field of 3D computer vision and graphics has seen significant advancements in recent years, driven largely by the availability of large-scale, annotated 3D datasets. These datasets serve as foundational resources for training and evaluating machine learning models, particularly in areas such as 3D object recognition, scene understanding, and generative modeling. Historically, the development of such datasets has been limited by the challenges of acquiring high-quality 3D data and the labor-intensive process of annotating these models. However, recent technological advancements, including the proliferation of 3D scanning devices and online repositories, have enabled the creation of larger and more diverse datasets.\n\nCurrent progress in 3D datasets is characterized by an increasing scale and diversity, which has led to improvements in the performance of machine learning models across various tasks. However, challenges remain in terms of the quality and semantic richness of the models, as well as the difficulty of obtaining annotations for a wide range of object categories. This summary focuses on three datasets: ShapeNet, Objaverse, and Objaverse-XL, which have been pivotal in advancing the field of 3D computer vision and graphics.\n\n#### Introduce the Main Content of Each Paper Separately\n\n**ShapeNet: An Information-Rich 3D Model Repository**\n\nShapeNet, introduced by Chang et al. (2015), is a large-scale repository of 3D CAD models designed to provide a comprehensive, semantically enriched dataset for the research community. The dataset contains over 3 million 3D models, with approximately 220,000 models categorized into 3,135 categories based on the WordNet taxonomy. ShapeNet's primary goal is to support data-driven methods in computer graphics and vision by providing a rich set of annotations for each model. These annotations include geometric attributes (such as canonical orientations, parts, and symmetries), physical properties (like dimensions and material compositions), and functional annotations (like affordances).\n\nThe dataset is structured to allow for multiple views into the collection of shapes, including taxonomy traversal and image and shape similarity search. The annotations are generated through a combination of algorithmic predictions and manual verification, ensuring high accuracy and consistency. ShapeNetCore, a subset of ShapeNet, contains approximately 51,300 unique 3D models across 55 common object categories. The dataset is intended to be an evolving repository, with regular updates as more models and annotations are added.\n\n**Objaverse: A Universe of Annotated 3D Objects**\n\nObjaverse, introduced by Deitke et al. (2022), is a large-scale dataset of 3D models sourced from Sketchfab, a leading online platform for managing and sharing 3D models. The dataset contains over 800,000 3D models, designed by over 100,000 artists, and includes a variety of categories such as animals, humans, vehicles, and interiors. Objaverse is not only a repository of 3D models but also includes natural language descriptions, tags, and animations for each model. This dataset is designed to support applications in 3D generative modeling, instance segmentation, embodied AI, and robustness analysis of vision models.\n\nThe dataset's scale and diversity enable significant improvements in model performance. For instance, when used to train generative models like GET3D, Objaverse yields higher quality and more diverse object generations compared to ShapeNet. Additionally, Objaverse can be used to augment datasets like LVIS for instance segmentation, where the augmentation technique 3DCP (3D copy-and-paste) improves the performance of state-of-the-art Mask-RCNN models. Objaverse also supports the creation of new embodied AI tasks, such as open-vocabulary object navigation, and provides a benchmark for evaluating the robustness of vision models to orientation shifts.\n\n**Objaverse-XL: A Universe of 10M+ 3D Objects**\n\nObjaverse-XL, introduced by Deitke et al. (2023), is an even larger dataset of 3D objects, containing over 10 million unique models. The dataset is sourced from a variety of online platforms, including GitHub, Thingiverse, Sketchfab, Polycam, and the Smithsonian Institution. Each model is paired with metadata such as its popularity, license, and textual descriptions, and is rendered using Blender to extract CLIP ViT-L/14 features. These features are used to predict different metadata properties, including aesthetic scores and NSFW (Not Safe For Work) predictions.\n\nObjaverse-XL is used to train and evaluate models for novel view synthesis, a task that requires predicting a novel view of an object from an input view. The dataset's scale and diversity enable significant improvements in zero-shot generalization performance, particularly for categories known to be challenging for baseline systems, such as people, cartoons, and sketches. Additionally, pretraining on Objaverse-XL improves performance when fine-tuning to other datasets, such as DTU and ShapeNet, demonstrating the potential of large-scale 3D datasets for downstream applications.\n\n#### Summarize Their Commonalities and Innovations\n\n**Commonalities**\n\nAll three datasets share the goal of providing large-scale, high-quality 3D models to the research community. They are designed to support data-driven methods in computer graphics and vision, enabling tasks such as 3D object recognition, scene understanding, and generative modeling. Each dataset includes a variety of object categories, ranging from man-made artifacts to natural objects, and is structured to allow for multiple views into the collection of shapes, including taxonomy traversal and image and shape similarity search.\n\n**Innovations**\n\n- **ShapeNet** introduces a hierarchical categorization scheme based on WordNet synsets, which enables a broad and deep coverage of object categories. It also includes a rich set of annotations for each shape, such as rigid alignments, parts, and keypoints, which provide valuable resources for processing, understanding, and visualizing 3D shapes.\n- **Objaverse** leverages the scale and diversity of 3D models available on the web to provide a large-scale, richly annotated dataset. It includes natural language descriptions, tags, and animations for each model, which can be used to support a wide range of applications in 3D computer vision and graphics.\n- **Objaverse-XL** extends Objaverse by providing an even larger dataset of over 10 million unique models. It includes metadata extracted from the models, such as CLIP ViT-L/14 features, which can be used to predict different metadata properties and improve the performance of models for novel view synthesis.\n\n#### Compare the Results of the Papers and Discuss Differences\n\n**Comparison of Dataset Sizes**\n\n| Dataset        | Number of Models | Number of Categories | Annotations |\n|----------------|------------------|---------------------|-------------|\n| ShapeNet       | >3M              | 3,135               | Geometric, Physical, Functional |\n| Objaverse      | >800K            | 21K                 | Textual, Tags, Animations |\n| Objaverse-XL   | >10M             | 21K                 | CLIP Features, NSFW, Face Detection |\n\nShapeNet is the largest of the three datasets in terms of total number of models, but Objaverse and Objaverse-XL provide a broader and deeper coverage of object categories. Objaverse-XL is significantly larger than both ShapeNet and Objaverse, providing an order of magnitude more data for training and evaluating machine learning models.\n\n**Comparison of Annotations**\n\n- **ShapeNet** includes a rich set of annotations for each model, such as rigid alignments, parts, and keypoints, which provide valuable resources for processing, understanding, and visualizing 3D shapes. These annotations are generated through a combination of algorithmic predictions and manual verification, ensuring high accuracy and consistency.\n- **Objaverse** includes natural language descriptions, tags, and animations for each model, which can be used to support a wide range of applications in 3D computer vision and graphics. The dataset's scale and diversity enable significant improvements in model performance, particularly for tasks like 3D generative modeling and instance segmentation.\n- **Objaverse-XL** extends Objaverse by providing an even larger dataset of over 10 million unique models. It includes metadata extracted from the models, such as CLIP ViT-L/14 features, which can be used to predict different metadata properties and improve the performance of models for novel view synthesis. The dataset's scale and diversity enable significant improvements in zero-shot generalization performance, particularly for categories known to be challenging for baseline systems.\n\n**Comparison of Applications**\n\n- **ShapeNet** is primarily used for tasks like 3D object recognition, scene understanding, and generative modeling. The dataset's annotations enable the evaluation and comparison of algorithms for fundamental tasks involving geometry, such as segmentation, alignment, and correspondence.\n- **Objaverse** is used for a wide range of applications, including 3D generative modeling, instance segmentation, embodied AI, and robustness analysis of vision models. The dataset's scale and diversity enable significant improvements in model performance, particularly for tasks like 3D generative modeling and instance segmentation.\n- **Objaverse-XL** is used for novel view synthesis, a task that requires predicting a novel view of an object from an input view. The dataset's scale and diversity enable significant improvements in zero-shot generalization performance, particularly for categories known to be challenging for baseline systems.\n\n**Differences in Results**\n\n- **ShapeNet** provides a comprehensive, semantically enriched dataset that enables the evaluation and comparison of algorithms for fundamental tasks involving geometry. However, the dataset's annotations are limited to geometric and physical properties, and do not include natural language descriptions or animations.\n- **Objaverse** provides a large-scale, richly annotated dataset that enables significant improvements in model performance, particularly for tasks like 3D generative modeling and instance segmentation. However, the dataset's annotations are limited to natural language descriptions, tags, and animations, and do not include geometric and physical properties.\n- **Objaverse-XL** provides an even larger dataset of over 10 million unique models that enables significant improvements in zero-shot generalization performance, particularly for categories known to be challenging for baseline systems. However, the dataset's annotations are limited to CLIP ViT-L/14 features, and do not include natural language descriptions or animations.\n\n#### Conclusion\n\n**Main Findings**\n\nThe three datasets, ShapeNet, Objaverse, and Objaverse-XL, have significantly advanced the field of 3D computer vision and graphics by providing large-scale, high-quality 3D models with rich annotations. ShapeNet provides a comprehensive, semantically enriched dataset that enables the evaluation and comparison of algorithms for fundamental tasks involving geometry. Objaverse provides a large-scale, richly annotated dataset that enables significant improvements in model performance, particularly for tasks like 3D generative modeling and instance segmentation. Objaverse-XL provides an even larger dataset of over 10 million unique models that enables significant improvements in zero-shot generalization performance, particularly for categories known to be challenging for baseline systems.\n\n**Future Research Directions**\n\nFuture research directions include the continued expansion of these datasets to include more models and annotations, as well as the development of new applications that can leverage the scale and diversity of 3D data. Additionally, future work may consider how to choose datapoints to train on, particularly for discriminative tasks such as 3D segmentation and detection. Finally, future research may consider how to make 3D content easier to capture and create, particularly for tasks that require high-quality 3D data."
}