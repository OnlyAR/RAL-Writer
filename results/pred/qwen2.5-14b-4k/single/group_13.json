{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{LoTa-Bench: Benchmarking Language-orien-ted Task Planners for Embodied Agents}\n\n\\begin{document}\n\n\\maketitle\n\\def\\thefootnote{*}\\footnotetext{Equal contribution.}\\def\\thefootnote{\\arabic{footnote}}\n\n\\begin{abstract}\nLarge language models (LLMs) have recently received considerable attention as alternative solutions for task planning. However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and prompt construction. To address this, we propose a benchmark system for automatically quantifying performance of task planning for home-service embodied agents. Task planners are tested on two pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of Watch-And-Help and VirtualHome. Using the proposed benchmark system, we perform extensive experiments with LLMs and prompts, and explore several enhancements of the baseline planner. We expect that the proposed benchmark tool would accelerate the development of language-oriented task planners.\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe ability of embodied agents to comprehend natural language instructions and perform the desired tasks has been a long-standing goal in the field of AI and robotics. When the agent has a sufficiently diverse skill set, decomposing high-level tasks into sequences of executable skills becomes particularly important. Conventional approaches have addressed this challenge through symbolic planning in predefined domains \\citep{fikes1971strips, garrett2020pddlstream} or through learning-based task and motion planning \\citep{silver2023learning, shah2022value, li2022pre}. Recently, large language models (LLMs) have emerged as a promising alternative. These models, pre-trained on extensive corpora, seem to have semantic knowledge about the world \\citep{brown2020language, chowdhery2022palm, thoppilan2022lamda, zhang2022opt}. This knowledge can be effectively leveraged for high-level task planning through in-context learning without any additional training \\citep{huang2022language, singh2023progprompt, liang2023code, ahn2023do, huang2023inner, yao2023react}.\n\nHowever, the evaluation frameworks for LLM-based task planning remain underdeveloped. Most existing studies rely on human evaluation, which is not only time-consuming but also expensive. These evaluations often occur in custom environments, which also makes them difficult to reproduce. Although some research \\citep{huang2023inner, liang2023code} has utilized simulators for automated evaluation, these efforts are typically confined to simple tabletop manipulation tasks. Furthermore, there is a noticeable absence of in-depth investigation into various influential factors, such as the type and size of pre-trained model, the number and select strategy of in-context examples, the capability for replanning based on natural language feedback, and the impact of fine-tuning.\n\nTo address the limitations, we introduce LoTa-Bench, a benchmark for language-oriented task planning for embodied agents. Our system aims to automatically quantify planning performance, enabling easier, fair, and reproducible comparison between systems. The framework consists of a baseline task planner, a dataset, and a simulator, as illustrated in Figure \\ref{fig:fig1}. The baseline task planner capitalizes on the in-context learning ability of LLMs. It constructs a prompt using a prefix, in-context examples (comprising pairs of natural language instructions and corresponding skill sequences to accomplish the instruction), and a user-provided natural language instruction. With this prompt, the LLM calculates the probabilities of all executable skills to complete a task. The skill with the highest probability is selected and appended to the prompt for the next step in an autoregressive manner. \nIn the proposed benchmark suite, we evaluate the planner on two dataset-simulator pairs: 1) ALFRED dataset \\citep{shridhar2020alfred} with AI2-THOR simulator \\citep{kolve2017ai2}, and 2) our extension of Watch-And-Help (WAH) dataset \\citep{puig2021watchandhelp}, WAH-NL, paired with VirtualHome simulator \\citep{puig2018virtualhome}. Each dataset sample furnishes the planner with both a natural language instruction and an environment context. The simulator executes the planned actions, and the performance of task planning is automatically assessed by comparing the final state of the simulator with predefined goal conditions.\n\nIn addition to the introduction of the benchmark suite, we provide extensive experimental results to further understand LLM-based task planning. Our baseline experiments explore the influence of various pre-trained models and their sizes. Given the critical impact of in-context examples on the performance of LLM-based task planners, we investigate the effect of number of examples and selection strategies. Additionally, we probe into other influential factors such as replanning according to the failure of a previous step and the effectiveness of model fine-tuning in the task planning domain.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1.0\\textwidth]{figure/fig1.pdf}\n    \\caption{Overall benchmarking configuration for LLM-based task planners. NL stands for Natural Language. We used two setups: 1) ALFRED dataset with AI2-THOR simulator and 2) WAH-NL dataset with VirtualHome simulator. Exemplary prompt and skill set are presented on the left side.}\n    \\label{fig:fig1}\n\\end{figure}\n\nOur contribution is fourfold: 1) first proposal of a benchmark suite that enables automatic evaluation of LLM-based task planners for home-service agents, 2) extensive experiments of a baseline task planner, 3) exploring possible extensions of the baseline planner and its validation with the proposed benchmark, and 4) public release of benchmark code and extended dataset (WAH-NL); they are available at \\url{https://github.com/lbaa2022/LLMTaskPlanning}.\n\n\\section{Related Work}\n\\label{sec:related_work}\n\nLLMs have demonstrated remarkable generalization capabilities through zero-shot or few-shot prompting \\citep{brown2020language}, leading to a transformative impact on task planning. Traditional task planning methods predominantly focused on searching within predefined domains \\citep{fikes1971strips, garrett2020pddlstream, hoffmann2001ff} or learning trajectories \\citep{silver2023learning, shah2022value, li2022pre, itcher2022broadly, nair2020hierarchical, eysenbach2019search, xu2019regression}. However, thanks to LLMs, new language-oriented task planning methods have emerged. \\citet{huang2022language} proposed a method where an LLM directly generates task plans via prompt engineering, with each generated step translated into an executable action using another language model. SayCan \\citep{ahn2023do} employed an LLM to score all predifined admissible actions, concurrently considering skill affordance through learned vision-based value functions. LLMs have also been adopted to generate executable robot codes using program-style inputs such as function descriptions \\citep{liang2023code, singh2023progprompt, zelikman2022parsel}. Moreover, integrating context into LLM-based task planners has been shown to enhance planning efficacy \\citep{huang2023inner, yao2023react, chen2023open, lin2023grounded, wu2023plan}.\n\nAlthough numerous LLM-based task planners have emerged, standardized automatic performance evaluation methods are still scarce. Real robot experiments typically require time-intensive human evaluations. In these setups, human raters determine the success or failure of planning \\citep{ahn2023do, huang2023inner, chen2023open}. When using simulators and datasets for evaluations, each task requires the goal condition and the natural language instruction. If a dataset lacks goal conditions, such as ActivityPrograms \\citep{puig2018virtualhome}, human evaluation remains necessary \\citep{huang2022language, zelikman2022parsel}. Similarly, datasets without natural language instruction, like Watch-And-Help \\citep{puig2021watchandhelp}, or simulators not offering high-level APIs, such as Behavior-1k \\citep{li2023behavior}, cannot support language-oriented task planning. Only a few studies, akin to our benchmark suite, have incorporated automated evaluations. For instance, ReAct \\citep{yao2023react} utilized the ALFWorld \\citep{shridhar2021alfworld} text-based game and the ALFRED dataset \\citep{shridhar2020alfred}. ProgPrompt \\citep{singh2023progprompt} engaged with the VirtualHome simulator and a customized dataset. Nevertheless, these assessments were conducted in restricted settings, hindering insights into comprehensive potential of LLM-based task planners.\n\n\\section{Baseline LLM-based Task Planner}\n\\label{sec:baseline_task_planner}\n\\textbf{Problem Statement.} In our proposed framework, a task planner receives a natural language instruction $i$ from the user, e.g., ``bring an apple and a cupcake and put them on the coffee table.'' The planner also has access to a predefined skill set $S$, where each skill $s{\\in}S$ represents an atomic action the agent can perform, such as ``pick up the apple,'' ``find a wine glass,'' or ``open the fridge.'' We assume that these skills are coupled with corresponding language-conditioned low-level controllers \\citep{jang2022bc,rt1}. The objective of the task planner is to select the skill $s_t$ at time step $t$ by maximizing the likelihood of completing the given instruction $i$ as follows:\n\\begin{equation}\n\\label{eq:probability_tp}\ns_t = \\argmax_{s \\in S} p(s | i, s_0, \\cdots, s_{t-1}),\n\\end{equation}\nwhere $s_0, \\cdots, s_{t-1}$ are previously executed skills and $s_0 = \\emptyset$. \nExemplary step sequences for the instruction we mentioned above could be (1. find an apple, 2. pick up the apple, 3. find a coffee table, 4. put down the apple, 5. find a cupcake, 6. pick up the cupcake, 7. find a coffee table, 8. put down the apple, 9. done).\n\n\\textbf{Baseline Task Planner.} Our baseline task planner leverages the in-context learning capabilities of large language models (LLMs), resonating with recent research \\citep{huang2022language, liang2023code, ahn2023do}. To estimate the probability expressed in Equation~\\ref{eq:probability_tp}, we construct a prompt $P$, which consists of a prefix, in-context examples, the instruction $i$, and a history of previously executed skills. For a skill $s$, described by $n_s$ subword tokens $s = (w^s_1, \\cdots, w^s_{n_s})$, the LLM computes the conditional probability as follows:\n\\begin{equation}\n\\label{eq:probability_llm}\np(s | i, s_1, \\cdots, s_{t-1}) = p_{\\text{LLM}}(s | P) = \\prod_{n=1}^{n_s} p_{\\text{LLM}}(w_{n}^{s} | P, w_{0}^{s}, \\cdots, w_{n-1}^{s}),\n\\end{equation}\nwhere $p_{\\text{LLM}}$ is the pre-trained LLM and $w_{0}^{s} = \\emptyset$. Instead of iterating every skill to find the best next skill to perform (Equation~\\ref{eq:probability_tp}), which requires extensive computation, we employ a greedy search strategy, but with constraints on the next token selection to match with one of the skills using Guidance tool \\citep{guidance}; see Appendix \\ref{app:impl} for details. Once a skill is selected, it is appended to the prompt $P$, and the planner continues to use the updated prompt to select the next skill. This autoregressive process continues until either the terminal skill (``done'') is selected or the skill sequence reaches a predefined maximum length.\n\n\\section{Benchmark Setup}\n\\label{sec:benchmark}\nTo rigorously evaluate LLM-based task planners, we introduce a comprehensive evaluation framework, described in Figure \\ref{fig:fig1}. The framework integrates three key components: a task planner, a dataset, and a simulator. The baseline task planner elaborated in Section \\ref{sec:baseline_task_planner} is employed for comparative benchmarking. Then we offer two distinct dataset-simulator pairings: 1) the ALFRED dataset \\citep{shridhar2020alfred} built on the AI2-THOR simulator \\citep{kolve2017ai2}, and 2) an extended version of the Watch-And-Help (WAH) dataset \\citep{puig2021watchandhelp}, named WAH-NL, incorporated with the VirtualHome simulator \\citep{puig2018virtualhome}. %\nFurther details of the dataset and the simulator are described in the following subsections.\n\n\\subsection{Dataset}\n\\label{subsec:dataset}\nOur benchmark employs two datasets, the ALFRED dataset \\citep{shridhar2020alfred} and our extension of the WAH dataset \\citep{puig2021watchandhelp}. Both datasets include sets of a natural language (NL) instruction, an initial environment state, and a goal condition for home environments. The NL instructions are user-provided and serve as inputs to the task planner of an autonomous agent. The initial environment states, containing object locations and states, are used for initialization of the simulator and for skill set construction in the task planning. The goal condition specifies the criteria for task completion. Planning performance is assessed by comparing the final state of the simulator with this goal condition after the execution of the last skill generated by the task planner.\n\n\\textbf{ALFRED.} It is a benchmark dataset for embodied AI agents that plan and execute primitive actions to perform household tasks, such as heating a mug cup, placing a salt shaker in a drawer, or putting vegetables in the fridge. This dataset was built on the AI2-THOR simulation environment. There are 7 task types of \\textit{Pick \\& Place, Stack \\& Place, Pick Two \\& Place, Clean \\& Place, Heat \\& Place, Cool \\& Place}, and \\textit{Examine in Light}. Among them, we excluded \\textit{Pick Two \\& Place} type in the evaluation because of missing capability of object instance recognition, which is required to accomplish this task type, in the LLM-based task planner.\n\n\\textbf{WAH-NL.} The original WAH dataset focuses on the challenges of AI agents assisting humans in household tasks. It consists of a \\textit{Watch} stage where agents observe human demonstrations to infer goals, and a \\textit{Help} stage where agents assist human in achieving those goals with minimal time steps. The dataset includes 5 task types of \\textit{Setup a dinner table}, \\textit{Put groceries}, \\textit{Prepare a meal}, \\textit{Wash dishes}, and \\textit{Read a book}. The goal condition of each task consists of multiple subgoals. For example, the goal condition of \\textit{Put groceries} task can be “INSIDE(cupcake, fridge): 2” and “INSIDE(apple, fridge): 1”, where the numeric values indicate the number of objects.\n\nOur extended version, WAH-NL, introduced two significant modifications on the \\textit{Help} stage of the original WAH dataset. First, we adjust the goal conditions, originally designed for human-AI collaborations, to suit autonomous agents. \nAdditionally, we set the number of objects for all subgoals to 1, for similar reasons as with the ALFRED dataset (the lack of object instance recognition capability in our LLM-based task planner). Second, since the original dataset lacks NL instructions, which is must-needed element for language-oriented task planners, we collected them via the \\href{https://www.prolific.co/}{Prolific} crowdsourcing platform. The final dataset includes 416 instructions for the \\textit{train} set and 195 for the \\textit{test} set. More details about WAH-NL are described in Appendix \\ref{app:wah_nl}.\n\n\\subsection{Simulator}\n\\label{subsec:simulator}\nThe simulator serves as an interactive environment that enacts the skills generated by the task planner. We first define a skill set for the task planner by combining available actions with optional parameters like target objects or receptacles. The skill set includes, for example, ``find an apple,'' ``turn on the faucet,'' ``open the fridge'', and ``put down the pillow.'' Then, we simulated language-conditioned low-level controllers by mapping the skills to executable agent action APIs of the simulators.\n    \nOur approach primarily employs two types of skills: object navigation (ObjNav) and object interaction. We opted for ObjNav over low-level move actions (such as move forward and rotate) as ObjNav is well studied and would be considered as a unit capability of home-service agents \\citep{9687596}. We assume that the agent is fully aware of object locations in the scene (practically, it can be accomplished by scene exploration with object map building). Object interaction skills are executable only when the interacting object is close to the agent. For example, ``pick up the plate'' is successful when the distance between the plate and agent is less than a predefined distance. \nWe had some other assumptions for the object interaction skill. \nFor the ``put down'' skill, the agent always put a holding object on a receptacle last visited. If there are multiple objects with the same class, the object closest to the agent is selected. The agent can hold one object in AI2-THOR and two objects in VirtualHome.\n    \n\\textbf{AI2-THOR.} There are seven interaction actions--``pick up,'' ``open,'' ``close,'' ``turn on,'' ``turn off,'' ``slice,'' and ``put down''--and one navigation action ``find.'' Among all combinations of action and optional parameters such as target objects or receptacles, we used 214 skills that were used at least once in the ground-truth trajectories in the \\textit{train} set.\n\n\\textbf{VirtualHome.} This simulator supports five interaction actions -- ``pick up,'' ``open,'' ``close,'' ``switch on,'' and ``put down'' -- and one navigation action ``find.''  In VirtualHome, due to the extensive total number of skills, we constructed a skill set by considering all possible combinations of actions and optional parameters for each environment. On average, we utilized 351.89 skills.\n\n\\section{Base Experiments}  %\n\\label{sec:base_experiments}\n\nWe conducted experiments to measure the performance of the baseline LLM-based task planners by using the proposed benchmark. We tested various settings including LLMs in different model classes and sizes and the impact of the number of in-context examples.\n\n\\subsection{Evaluation Protocol}\n\\label{subsec:evaluation_protocol}\n\n\\textbf{Test Setup.} We used the ALFRED and WAH-NL datasets, as introduced in Section \\ref{subsec:dataset}. The ALFRED dataset consists of three sets: \\textit{train}, \\textit{valid-seen}, and \\textit{valid-unseen}. The \\textit{valid-seen} was used to evaluate planning performance; the \\textit{train} set was only used to take examples to construct prompts. We used a small set of \\textit{valid-seen}, which has 208 samples (30\\% of the \\textit{valid-seen} set), to accelerate the evaluation of various configurations (see Appendix \\ref{app:fullset} for the results with the full set). The WAH-NL dataset comprises a \\textit{train} set and a \\textit{test} set with 250 and 100 samples, respectively. The \\textit{train} set was used to construct in-context examples, and the \\textit{test} set was used to evaluate planning performance.\n\nThe prompt comprises a prefix, describing the role of a home-service embodied agents, and a number of in-context examples, which adhere to the template defined in SayCan \\citep{ahn2023do}. Each example is a pair of an input query (task instructions) and the corresponding output step sequences to accomplish the task. See Appendix \\ref{app:prompt} for a complete prompt sample. The in-context examples were randomly selected from the \\textit{train} sets, and we tried to use the same number of examples per task type. The default setup is to include six examples in ALFRED and five examples in WAH-NL (one example per task type). \n\n\\textbf{Evaluation Metrics.} We measured planning performance using the task success rates for ALFRED. Task success was determined based on whether the final state after executing the step sequence generated by the task planner satisfies the expected goal condition. For WAH-NL, we measured the average subgoal success rate. Each WAH-NL task has multiple subgoals. We calculate the individual subgoal success rate as the ratio of successfully completed subgoals to the total number of subgoals for each task. The average subgoal success rate across a \\textit{test} set is reported.\n\n\\subsection{Benchmark Results of Baseline Task Planner}\n\\label{subsec:baseline_experiment}\n\n\\begin{figure}[t]\n\\centering\n\\vspace{-4mm}\n\\begin{subfigure}{0.49\\textwidth}\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{figure/alfred_llm_model.pdf}\n    \\caption{ALFRED}\n\\end{subfigure}\n\\begin{subfigure}{0.49\\textwidth}\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{figure/wah_llm_model.pdf}\n    \\caption{WAH-NL}\n\\end{subfigure}\n\\vspace{-3mm}\n\\caption{Baseline results on (a) ALFRED and (b) WAH-NL. We report task success rates (\\%) on the ALFRED dataset and average subgoal success rate (\\%) on the WAH-NL datset for language models in different model classes and sizes (number of parameters). Base language models are represented as solid lines. Fine-tuned models (by either instruction or chat data) were shown in a dashed line with a triangle maker.}\n\\label{fig:llm_model}\n\\end{figure}\n\nWe evaluated the planning performance of the baseline planner described in Section \\ref{sec:baseline_task_planner}. Figure \\ref{fig:llm_model} shows the results on ALFRED and WAH-NL for different pre-trained LLMs: GPT~\\citep{brown2020language}, GPT-Neo series~\\citep{gpt-neo, gpt-j, black2022gpt}, OPT~\\citep{zhang2022opt}, MPT~\\citep{MPT7B, MPT30B}, LLaMA~1~\\citep{llama}, and LLaMA~2~\\citep{touvron2023llama} (see Appendix \\ref{app:lang_models} for the list). A few fine-tuned models on instructions or chat data were also tested.\n\nOverall, task success rates increased with the size of the language model, but this was not always the case. For example, GPT-J 6B model performed better than GPT-NeoX 20B and OPT 2.7B was better than OPT 6.7B in the WAH-NL experiment. Such results, that a smaller model performs better than a larger model, were also observed in HELM evaluation \\citep{liang2022holistic}, especially in reasoning tasks. GPT-3 (text-davinci-003) showed the best success rate of 21.36\\% on ALFRED and the best subgoal success rate of 40.82\\% on WAH-NL. LLaMA~2 and MPT performed well considering their model sizes. Instruction- and chat-tuned models (dashed lines in Figure \\ref{fig:llm_model}) did not perform better than their base models. \nWe also tested on GPT-4, the state-of-the-art LLM. As OpenAI provides only chat-style APIs for GPT-4 unlike other base models such as GPT-3, we were unable to directly compare GPT-4 in the same configuration. We modified experimental configurations and assessed GPT-4's performance. GPT-4 performed well in ALFRED, showing a 40.38\\% success rate, a 19\\% improvement over GPT-3. However, in WAH-NL, GPT-4 showed a lower success rate of 34.17\\% compared to GPT-3. More details in Appendix \\ref{app:gpt4}.\n\nWe conducted a further analysis of the task types. For ALFRED, we found that the small model such as GPT-J 6B succeeded only for simple \\textit{Pick \\& Place} tasks and failed in complex tasks such as heating and cooling tasks, which require longer steps than simple tasks. The largest model, GPT-3 175B, succeeded similarly in both simple and complex tasks (20-30\\% success rates), except for the task type \\textit{Stack \\& Place} where the agent needs to stack multiple objects in order. Additional results  are shown in Table \\ref{tab:finetuned_successes_alfred_tasktypes} in Appendix.\nFor WAH-NL, all task types have a similar level of complexity, generally requiring the finding and placement of multiple objects. This resulted in a more balanced performance across task types when compared to ALFRED. Using our GPT-3 175B model, \\textit{Put Fridge} tasks yielded the highest average subgoal success rate at 54.50\\%, while \\textit{Prepare Snack} tasks registered the lowest average subgoal success rate of 25.00\\%. \nSee Figure \\ref{fig:sample_results} for the success samples (more results in Appendix \\ref{app:additional_results}).\n\n\\begin{wraptable}[11]{r}{6.5cm}\n  \\centering\n  \\vspace{-4mm}\n  \\begin{tabular}{@{}p{4.4cm}p{1.6cm}@{}}\n    \\toprule\n    Failure category\t            & $\\#$ Failures \\\\\n    \\midrule\n        \\textit{Action planning failure}          & 46 (28.4\\%)\t\\\\\n        \\textit{Object selection failure}\t     & 51 (31.5\\%)   \\\\\n        \\textit{Absence of visual grounding}\t     & 21 (13.0\\%)\t\\\\\n        \\textit{Lack of physical understanding}   & 15 (9.3\\%)\\\\\n        \\textit{Misunderstanding inst.}    & 10 (6.2\\%)\\\\\n        \\textit{Ambiguous/incorrect inst.} & 19 (11.7\\%)\\\\\n    \\bottomrule\n  \\end{tabular}\n  \\vspace{-2mm}\n  \\caption{The number of failure cases of the ALFRED results using GPT-3.}\n  \\label{tab:failure_case}\n\\end{wraptable}\n\nWe also examined the detailed reasons for the failure cases of the ALFRED results using GPT-3 model, which showed the highest performance. Out of 162 failure cases, the reasons were categorized into six classes: 1) Action planning failures, such as performing `Pick' instead of `Slice' when a tomato needs to be sliced. 2) Object selection failures, like grabbing a pan instead of a pot. 3) Absence of visual grounding, for instance, trying to grab an object inside a closed drawer, 4) Lack of physical understanding, such as failing to put down an object on the already occupied table. 5) Misunderstanding user instructions, failing to distinguish between a desk lamp and a floor lamp when the user specified `Lamp'. 6) Ambiguous or incorrect user instructions, like confusing `Glass' for `Cup' in an instruction. The results are presented in Table \\ref{tab:failure_case}. Most failures (about 60\\%) stemmed from high-level planning (classes 1 and 2). Challenges in visual and physical grounding (classes 3 and 4) highlight the importance of integrating context in planning, as discussed in Section \\ref{sec:conclusion}. Furthermore, the role of clarity in user instructions (classes 5 and 6) opens up a new research direction for interactive clarification of an ambiguous tasks.\n\n\\begin{wrapfigure}[11]{r}{0.4\\textwidth}\n\\centering\n\\vspace{-6mm}\n\\includegraphics[width=0.4\\textwidth]{figure/no_examples.pdf}\n\\vspace{-6mm}\n\\caption{(Subgoal) success rates for the different number of examples for in-context learning.}\n\\label{fig:no_examples}\n\\end{wrapfigure}\n\n\\begin{figure}[h!]\n\\centering\n\\begin{subfigure}{\\textwidth}\n    \\centering\n    \\caption{ALFRED: success example for \\textit{Cool \\& Place} task.}\n    \\includegraphics[width=\\textwidth]{figure/alfred_examples/trial_T20190906_210937_878489_1_success.png}\n\\end{subfigure}\n\\begin{subfigure}{\\textwidth}\n    \\centering\n    \\caption{WAH-NL: success example for \\textit{Wash dishes} task.}\n    \\includegraphics[width=\\textwidth]{figure/wah_examples/22.png}\n\\end{subfigure}\n\\caption{Planning results. Success cases of (a) the ALFRED task and (b) the WAH-NL task when GPT-3 175B model was used. The input instructions, inferred steps, and scene images after each step execution are presented in each figure. The scene images show agent's point of view on ALFRED and third person's point of view on WAH-NL. Additional results, including failure cases, are provided in Appendix \\ref{app:additional_results}.}\n\\label{fig:sample_results}\n\\end{figure}\n\nWe have investigated the impact of the number of examples in prompt with LLaMA~2 13B model that supports a longer context length of 4096. The success rate mostly increased when there are 0 to 30 examples on ALFRED and 0 to 15 examples on WAH-NL (see Figure \\ref{fig:no_examples}). It was not able to test more than 15 examples on WAH-NL because of the maximum token limitation. Note that the pool of examples was fixed, which means that, for example, 6 and 12 examples share the same 6 examples. An additional experiment was performed to see whether different sets of examples matter for the same number of examples. We selected different sets of 12 examples that were randomly drawn from the example pool of the training set. The performance on ALFRED varied from a minimum of 9.62 to a maximum of 17.79 (with an average of 13.61 and a standard deviation of 3.22) for the LLaMA~2 13B model.\n\nAdditionally, we tested on both full sets of \\textit{valid-seen} and \\textit{valid-unseen} splits of ALFRED using LLaMA~2 13B model. \\textit{Valid-unseen} set contains scenes not present in the train set. The success rate on \\textit{valid-unseen} was similar (17.70\\%) to the one on \\textit{valid-seen} (17.82\\%) as two sets are different in visual scenes, not in task types. \n\n\\section{Validating Extensions of the Task Planner}\n\\label{sec:extension}\n\nThe primary merit of the proposed benchmark is that it allows faster and easier validation of new task planners. To demonstrate this, we explore some extensions (or improvements) of the base planner and validate them.\n\n\\subsection{In-Context Example Selection}\n\\label{subsec:in_context_examples_selection}\nWe explored three strategies for selecting in-context examples from the \\textit{train} set containing both input queries and their associated planning examples. The first strategy, named \\textit{Random Sampling}, is the same as that used in the baseline task planner. It involves random sampling of $N$ examples for each unique task type, leading to a collection of $N{\\times}M$ examples across different task types, where $M$ is the number of distinct task types. In the second strategy, termed \\textit{Task-Specific Sampling}, we select examples from the subset of the \\textit{train} set that share the same task type as the input instruction (with the assumption that the task type is known). Lastly, \\textit{Semantic Similarity Sampling} employs Sentence BERT \\citep{reimers2019sentence}, to compute the similarity scores between the input instruction and all instructions in the \\textit{train} set. The examples with the highest similarity scores are selected as in-context examples. This strategy not only aims to select the most relevant planning examples, but also offers potential utility in real-world scenarios where task types may not be explicitly provided.\n\n\\begin{wrapfigure}[15]{r}{0.4\\textwidth}\n\\centering\n\\vspace{-5mm}\n\\includegraphics[width=0.4\\textwidth]{figure/wah_ic_select_LLaMA_1.pdf}\n\\vspace{-8mm}\n\\caption{Subgoal success rate for different in-context example selection strategies. The dashed line represents the best performance of our baseline planner using GPT-3 175B.}\n\\label{fig:ic_ex_select}\n\\end{wrapfigure}\nTo measure the effectiveness of these in-context example selection methods, we conducted experiments on WAH-NL using LLaMA~1 models. For all strategies, we set the number of in-context examples to five. The results are summarized in Figure \\ref{fig:ic_ex_select}. Across all model sizes, \\textit{Semantic Similarity Sampling} showed superior performance, followed by \\textit{Task-Specific Sampling}, and lastly \\textit{Random Sampling}. Importantly, \\textit{Semantic Similarity Sampling} led to significant performance gains; notably, the LLaMA~1 65B model achieved a subgoal success rate of 43.25\\%, surpassing the best performance of 40.82\\% achieved by our baseline using GPT-3 175B. These experiments confirm that the in-context example selecting strategy has a significant impact on the performance of LLM-based task planners. We found similar results with other LLM models like GPT-Neo and LLaMA~2.\n\n\\subsection{Feedback and Replanning}\n\\label{subsec:feedback_and_replanning}\nThe baseline planner selects the skill for the next step independent of the success or failure of previous action. However, adjusting the plan in response to the failure of the action is necessary for task planning in the wild. \nUsing the ALFRED and AI2-THOR configuration, we investigated whether our LLM-based task planner can reflect feedback from action failure and replan appropriately.\nWe added natural-language (NL) feedback at the end of each inferred step only when the step is failed. NL feedback is constructed based on error messages from the AI2THOR simulator and environment states.\nFor instance, when the pick up action failed because the target object is inside of a container, \\textit{``(this action failed: [Object] is not visible because it is in [Container])''} is appended after the step failed in the prompt, and the next step is inferred afterward (see more information of feedback message in Appendix \\ref{app:prompt}).\nWe assumed that the agent is fully aware of the location of objects.\nExamples demonstrating feedback and replanning were added to the prompt for in-context learning. We manually crafted 3 replanning examples and added them after the baseline prompt of 18 examples. The task instructions for the replanning examples were selected from the existing 18 examples to minimize the addition of task information (Listing \\ref{lst:replan} in Appendix \\ref{app:prompt} shows the additional in-context examples for replanning).\nLLaMA~2 model was used in this experiment.\n\n\\begin{wraptable}[9]{r}{7.3cm}\n\\centering\n\\vspace{-3mm}\n\\scalebox{1}{\n\\begin{tabular}{@{}lcc@{}}\n\\toprule\n                & \\multicolumn{2}{c}{Success Rate(\\%)} \\\\ \n                \\cmidrule{2-3}\n                &  LLaMA~2 13B         & LLaMA~2 70B  \\\\ \\midrule\nBaseline        &  \\textbf{21.15} (44/208)  &  21.63 (45/208)\\\\\nReplanning      & 17.79 (37/208)            & \\textbf{24.04} (50/208) \\\\ \\bottomrule\n\\end{tabular}}\n\\vspace{-2mm}\n\\caption{Results with and without replanning. Success rates on ALFRED are reported.}\n\\label{tab:replanning}\n\\end{wraptable}\nTable \\ref{tab:replanning} shows that replanning is helpful to improve the overall planning perofrmance when a large model, LLaMA~2 70B, is used. A smller model, LLaMA~2 13B, did not show improvements; this might be due to limited capability of the smaller model to understand complex concepts of task planning and replanning only with a few examples. The qualitative results showing how the planner succeeded by replanning steps are shown in Figure \\ref{fig:replan} in Appendix \\ref{app:prompt}.\n\n\\begin{wrapfigure}[12]{r}{0.38\\textwidth}\n\\centering\n\\vspace{-8mm}\n\\includegraphics[width=0.38\\textwidth]{figure/finetuned-planners-new.pdf}\n\\vspace{-6mm}\n\\caption{Success rates of fine-tuned planners on ALFRED.}\n\\label{fig:fine-tuning}\n\\end{wrapfigure}\n\n\\subsection{Fine-tuning on Train Set}\n\\label{subsec:finetuning}\n\nWe conducted experiments to investigate the potential to improve planner performance through fine-tuning. We fine-tuned LLaMA~1 models using LoRA \\citep{hu2021lora} on the ALFRED \\textit{train} set and evaluated their performance in the same ALFRED domain both for \\textit{valid-seen} and \\textit{valid-unseen} tasks. \nFor the evaluation of the fine-tuned planners, we used zero in-context examples as the planner had been explicitly trained.\nAs depicted in Figure \\ref{fig:fine-tuning}, fine-tuning significantly improved performance, especially for larger models, e.g. LLaMA~1 30B, which showed a performance jump from 13.66\\% to 60.08\\%. However, the planner, fine-tuned on ALFRED, performed significantly worse than the baseline planners in the WAH-NL domain (32.22\\% $\\rightarrow$ 10.38\\%), suggesting that procedural knowledge trained in a task domain does not transfer well to different task domains. See Appendix \\ref{app:finetune} for fine-tuning details and results. Appendix \\ref{app:finetune} also presents WAH-NL fine-tuning results. Notably, the limited number of WAH-NL \\textit{train} set led to marginal performance improvements. Specifically, the LLaMA 1 13B and 30B models showed increases of 5.77 and 1.56 percent points, respectively.\n\n\\section{Conclusion and Limitation}\n\\label{sec:conclusion}\n\nThe impressive generalizability and performance of the large language model (LLM) has facilitated its versatile deployment across multiple domains. Task planning for embodied agents is one such application, and after the pioneering work proposed to use LLM for this application by \\citet{ahn2023do} and \\citet{huang2022language}, we believe that diverse research efforts will continue in this direction. However, there was few way to automatically evaluate planners. In this paper, we proposed a quantitative evaluation benchmark for LLM-based task planning research to catalyze the rapid advancement of this field. The results of the base experiments and extensions of in-context example selection, replanning, and fine-tuning would be helpful to future studies. We hope that our proposed benchmark framework will serve as a starting point for the development of various extended models for language-oriented task planners.\n\nThe present work has the following limitations. First, we decoupled high-level plans and low-level actions to focus on high-level planning. An extension is needed to support an end-to-end system that considers both high-level planning and low-level actions. Visual understanding (egocentric views) is also necessary for low-level actions.\nSecond, as is common in many simulator-based studies, there exists a domain gap between simulation and the real world. For example, ALFRED poses an unrealistic assumption that an object is always cleaned when it was once put into water in a sink basin. Furthermore, although AI2THOR and VirtualHome support multiple scenes and objects, they still lack diversity to reflect real-world environments. \nFuture work will be to extend the proposed benchmark to support a wider range of systems and to reduce domain gaps.\n\n\\subsubsection*{Reproducibility Statement}\nWe provide the source code of the benchmark suite and the configurations to reproduce the experimental results. WAH-NL dataset that we extended from the original WAH dataset to have task instructions is also available. See \\url{https://github.com/lbaa2022/LLMTaskPlanning} for the source code and WAH-NL dataset.\n\n\\subsubsection*{Acknowledgments}\nThis work was supported by Institute of Information \\& communications Technology Planning \\& Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2022-0-00951, Development of Uncertainty-Aware Agents Learning by Asking Questions).\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{MLDT: Multi-Level Decomposition for Complex Long-Horizon Robotic Task Planning with Open-Source Large Language Model}\n\n\\begin{document}\n\n\\title{MLDT: Multi-Level Decomposition for Complex Long-Horizon Robotic Task Planning with Open-Source Large Language Model}\n\\titlerunning{MLDT: Multi-Level Decomposition for Robotic Task Planning}\n\\author{Yike Wu\\inst{1,2}\\thanks{Both authors contributed equally to this research.}\\\n\\and Jiatao Zhang\\inst{3}\\printfnsymbol{1}\\\n\\and Nan Hu\\inst{1,2}\\\n\\and Lanling Tang\\inst{4}\\\n\\and Guilin Qi\\inst{1,2\\textsuperscript{(\\Letter)}}\\\n\\and Jun Shao\\inst{3}\\\n\\and Jie Ren\\inst{5}\\\n\\and Wei Song\\inst{5\\textsuperscript{(\\Letter)}}\\\n}\n\\authorrunning{Wu and Zhang, et al.}\n\\institute{Southeast University, Nanjing, Jiangsu, China\\\\ \\email{\\{yike.wu,nanhu,gqi\\}@seu.edu.cn} \\and Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education \\and Zhejiang University, Hangzhou, Zhejiang, China\\\\ \\email{\\{jiataozh,jun\\_shao\\}@zju.edu.cn} \\and\nUniversity of Chinese Academy of Sciences, Beijing, China\\\\ \\email{tanglanling22@mails.ucas.ac.cn} \\and\nZhejiang Lab, Hangzhou, Zhejiang, China\\\\ \\email{\\{renjie,weisong\\}@zhejianglab.com}}\n\\maketitle\n\\setcounter{footnote}{0}\n\\begin{abstract}\nIn the realm of data-driven AI technology, the application of open-source large language models (LLMs) in robotic task planning represents a significant milestone. Recent robotic task planning methods based on open-source LLMs typically leverage vast task planning datasets to enhance models' planning abilities. While these methods show promise, they struggle with complex long-horizon tasks, which require comprehending more context and generating longer action sequences. This paper addresses this limitation by proposing MLDT, the \\textbf{M}ulti-\\textbf{L}evel \\textbf{D}ecomposition \\textbf{T}ask planning method. This method innovatively decomposes tasks at the goal-level, task-level, and action-level to mitigate the challenge of complex long-horizon tasks. In order to enhance open-source LLMs' planning abilities, we introduce a goal-sensitive corpus generation method to create high-quality training data and conduct instruction tuning on the generated corpus. Since the complexity of the existing datasets is not high enough, we construct a more challenging dataset, LongTasks, to specifically evaluate planning ability on complex long-horizon tasks. We evaluate our method using various LLMs on four datasets in VirtualHome. Our results demonstrate a significant performance enhancement in robotic task planning, showcasing MLDT's effectiveness in overcoming the limitations of existing methods based on open-source LLMs as well as its practicality in complex, real-world scenarios.\\footnote{Our code is available at https://github.com/wuyike2000/MLDT}\n\n\\keywords{Task planning  \\and LLM \\and Multi-level decomposition}\n\\end{abstract}\n\\section{Introduction}\nThe advancement of artificial intelligence (AI) technology in robotics primarily relies on diverse sources of data, including videos and images from cameras, physical parameters from sensors, and the knowledge base in robotics. Therefore, data-driven AI technology has become a popular research topic that aims to enhance the efficiency and adaptivity of AI technology. In recent years, the fusion of data-driven AI technology and robotics has opened new frontiers in task planning, a pivotal technique with broad applications in areas such as home services \\cite{DBLP:conf/corl/IchterBCFHHHIIJ22,DBLP:journals/ral/FioriniSPBTC22}, navigation  \\cite{DBLP:conf/icra/HuangMZB23}, and manipulation\\cite{DBLP:conf/corl/GuhurCPTLS22}. The emergence of large language models (LLMs) has spurred a new era where data-driven insights are leveraged to enhance decision-making and planning capabilities. Specifically, in-context learning \\cite{radford2019language,DBLP:conf/nips/BrownMRSKDNSSAA20} retrieves demonstrations from vast datasets and leverages abundant contextual information, such as observations and historical action sequences, to instruct LLMs like ChatGPT\\footnote{https://openai.com/chatgpt} in generating task plans grounded in the environment. However, this method is less effective with smaller LLMs \\cite{DBLP:journals/corr/abs-2310-12823,DBLP:conf/nips/LiuTMMHBR22} and raises concerns regarding privacy and domain-specific knowledge in closed-source, large-parameter models \\cite{wu2023unveiling,ray2023chatgpt}.\n\nConsequently, alternative efforts focus on fine-tuning small-scale open-source LLMs on task planning data to enhance their planning capabilities \\cite{DBLP:journals/corr/abs-2305-10626,DBLP:journals/corr/abs-2307-01848,chalvatzaki2023learning}. They typically organize task-relevant information as input to generate overall task plans. However, several studies \\cite{DBLP:journals/corr/abs-2308-03688,DBLP:journals/corr/abs-2307-11088} point out that open-source LLMs possess limited reasoning capacities compared to closed-source LLMs, making them struggle with long-context tasks. Besides, as illustrated in Fig. \\ref{Figure 1}, complex long-horizon task planning involves more task goals, more interacting objects, and longer action sequences compared to regular task planning. It requires comprehending substantial context information and generating lengthy action sequences. Therefore, it is a typical long-context task in daily life, such as tidying the room or cooking a meal. Most existing methods incorporate all the task-related information, including task goals and observation, as the input to generate action sequences in a single input-output interaction. Other efforts decompose tasks at the goal-level to mitigate the complexity of the tasks. However, these two strategies are overly difficult for open-source LLMs for they involve excessive context information as inputs and lengthy action sequences as outputs. Previous methods overlook the limited reasoning abilities of open-source LLMs and struggle with this task planning scenario. \n\nIn this paper, we focus on complex long-horizon task planning based on open-source LLMs with a small number of parameters (i.e., smaller than 15 billion). The major challenge lies in the contradiction between the limited reasoning capabilities of open-source LLMs and the high complexity of complex long-horizon task planning. Small-scale open-source LLMs exhibit a significant disparity in parameter size compared to closed-source LLMs with over 100 billion parameters. Therefore, they possess limited memory and reasoning abilities \\cite{DBLP:journals/corr/abs-2001-08361,DBLP:conf/iclr/WeiBZGYLDDL22}. \n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{1.png}\n    \\caption{The comparison between regular task planning and complex long-horizon task planning.}\n    \\label{Figure 1}\n\\end{figure}\n\nBesides, complex long-horizon task planning involves a high demand for understanding, reasoning, and planning abilities. This phenomenon is similar to humans facing complicated tasks and an excessive number of context information. The intelligence of humans is to break them down recursively until the tasks become sufficiently simple and manageable. To this end, we introduce the \\textbf{M}ulti-\\textbf{L}evel \\textbf{D}ecomposition \\textbf{T}ask Planning (MLDT) method, which strategically decomposes tasks at the goal-level, task-level, and action-level. This method effectively simplifies complex long-horizon tasks, making them more manageable for LLMs with limited capacities. Moreover, we enhance the planning abilities of LLMs through instruction tuning and a novel goal-sensitive corpus generation method. This method, powered by ChatGPT, generates a high-quality training corpus by leveraging environmental feedback. We further construct an instruction dataset based on the generated corpus and perform supervised fine-tuning.\n\nTo evaluate the performance of our method, we conduct experiments in VirtualHome. Since the complexity of existing dataset is not high enough, we extend the datasets provided by LID \\cite{DBLP:conf/nips/LiPPDWF0HAAAM0Z22} and construct a more challenging dataset, LongTasks, to evaluate the planning ability on complex long-horizon tasks. We utilize the original datasets from LID \\cite{DBLP:conf/nips/LiPPDWF0HAAAM0Z22} and LongTasks. Experimental results demonstrate that our method achieves the best performance across all datasets and improves over 50\\% success rate on the LongTasks dataset.\n\nThe contributions of this paper are summarized as follows:\n\n\\begin{itemize}\n\n\\item We propose MLDT, a novel robotic task planning method. This method decomposes tasks into goal, task, and action levels, addressing the challenge of complex long-horizon task planning. Besides, we conduct instruction tuning to enhance the planning abilities of LLMs. In order to collect high-quality training data compatible with our method, we devise a goal-sensitive corpus generation method based on environmental feedback.\n\n\\item We extend the datasets provided by LID and establish a novel dataset, LongTasks, since the complexity of the existing task planning datasets is not high enough. This dataset involves more challenging tasks, incorporating more task goals, more interactive objects, and longer action sequences, to specifically evaluate the planning ability on complex long-horizon tasks.\n\n\\item We evaluate our method on four datasets in VirtualHome. Experimental results demonstrate our method effectively enhances the task planning performance of open-source LLMs, particularly for complex long-horizon tasks. \n\n\\end{itemize}\n\n\\section{Related Works}\n\n\\subsection{Large Language Models for Robotic Task Planning}\n\nThe remarkable performance achieved by LLMs across various downstream tasks has spurred extensive research to employ them in robotic task planning. Existing task planning methods based on LLMs can be categorized into two types: (1) In-context learning: Some efforts attempt to construct demonstrations and task-related information into prompts as input, generating task plans grounded in the environment without fine-tuning. Zero-shot Planner \\cite{DBLP:conf/icml/HuangAPM22} employs two LLMs, one for generating plans and the other for translating generated plans into executable actions. ProgPrompt \\cite{DBLP:conf/icra/SinghBMGXTFTG23} enhances LLMs' capabilities in task planning by adopting a programmatic LLM prompt structure. ReAct \\cite{DBLP:conf/iclr/YaoZYDSN023} combines reasoning and acting, allowing LLMs to generate reasoning traces and task-specific actions in an interleaved manner. (2) Fine-tuning: Open-source LLMs with limited parameters struggle to achieve satisfactory results under the in-context learning paradigm. Therefore, some efforts fine-tune LLMs on task planning data to enhance LLMs' planning abilities and incorporate task-related information as input to generate the overall task plans. LID \\cite{DBLP:conf/nips/LiPPDWF0HAAAM0Z22} constructs a policy network based on GPT-2 and fine-tunes this network to predict actions interactively. Chalvatzaki et al. \\cite{chalvatzaki2023learning} combine a scene graph with task goals as the input for fine-tuned LLMs to obtain plans grounded in the environment. TaPA \\cite{DBLP:journals/corr/abs-2307-01848} adopts visual perception models to get observations and constructs a multimodal dataset for grounded tuning LLMs. E2WM \\cite{DBLP:journals/corr/abs-2305-10626} fine-tunes LLMs on embodied knowledge collected from world models to improve their embodied knowledge.\\\\\n\\indent Our work synergizes these two paradigms, addressing the performance gaps of open-source LLMs through instruction tuning and in-context learning, thereby enhancing their planning and generalization capabilities.\n\n\\subsection{Complex Long-Horizon Robotic Task Planning}\n\nComplex long-horizon robotic task planning aims to generate an action sequence to accomplish a specified task goal in robotics. Compared to regular task planning, complex long-horizon task planning involves more task goals and objects, resulting in higher complexity. SayCan \\cite{DBLP:conf/corl/IchterBCFHHHIIJ22} employs a pretrained value function to ground the output of LLMs in the environment, enhancing their performances on real-world, abstract and long-horizon tasks. ISR-LLM \\cite{DBLP:journals/corr/abs-2308-13724} involves three steps: preprocessing, planning, and iterative self-refinement to improve the feasibility and correctness of the generated plan. Inner Monologue \\cite{DBLP:conf/corl/HuangXXCLFZTMCS22} integrates feedback from multiple sources to form an inner monologue, enhancing the performance of LLMs in complex long-horizon tasks. ITP \\cite{DBLP:journals/corr/abs-2310-10645} obtains environmental observations based on a visual language model and utilizes a high-level planner to generate a plan, which is then executed by a low-level executor.\\\\\n\\indent Unlike previous works incorporating multi-modal information or environmental feedback, we focus on generating executable plans for complex long-horizon tasks based on open-source LLMs under the textual modality without intermediate environment observation. Besides, compared with previous works, the complex long-horizon tasks in this paper involve more task goals, more objects, and longer action steps (i.e., longer than 60), which are much more complicated.\n\n\\section{Preliminaries}\n\nRobotic task planning aims to generate an executable plan to achieve task goals within a given environment. A robotic task planning $T$ can be defined as $T=<G, O, A>$, where $G$, $O$, and $A$ represent the task goal, the observation, and all possible actions, respectively. Given the task goal $G$, the observation $O$, and an action set $A = \\{a_1, a_2, ..., a_n\\}$, the objective of robotic task planning is to generate a $t$-step plan (an action sequence) $\\pi = \\{a_1, a_2, ..., a_t\\}$ to achieve the task goal $G$. For example, as shown in Fig. \\ref{Figure 1}, given the task goal $G=$ ``\\textit{put 1 plate in dishwasher}'', the observation $O=$ ``\\textit{1 plate is on kitchentable}'', and the action set $A=\\{walk, grab,... ,close\\}$, the task planning model generates an action sequence $\\pi=$ ``\\textit{walk to kitchentable, grab plate, ..., close dishwasher}'' to achieve the task goal.\n\nTo the best of our knowledge, there is currently no strict definition for complex long-horizon task planning. Compared to regular task planning, complex long-horizon task planning involves more task goals and a greater number and variety of objects, resulting in longer action sequences.\n\n\\section{Methods}\n\n\\subsection{Multi-Level Decomposition Task Planning Method}\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{2.png}\n    \\caption{The overview of our proposed method, MLDT.}\n    \\label{Figure 2}\n\\end{figure}\n\nTo solve the deficiency of open-source LLMs on long-context tasks, we devise MLDT, the multi-level decomposition task planning method. As shown in Fig. \\ref{Figure 2}, this method transforms the task planning problem into a three-level decomposition. Compared with the original task planning problem, the complexity of each level decomposition is lower and thus manageable for open-source LLMs. \n\n\\subsubsection{Goal-Level Decomposition} \n\nGoal-level decomposition aims to break down task goals into several independent subgoals. Given the task goal $G$, we retrieve $N$ examples as demonstrations $demo$ based on the semantic similarity between the examples' task goals and $G$. Each example consists of the input and output of the goal-level decomposition for a given task goal. Then, we employ the goal-level prompt template $P_{goal}$ to transform the demonstrations $demo$ and the task goal $G$ into a prompt, serving as input for LLMs. Finally, We devise a parsing method based on regular expressions to extract $n_{goal}$ individual subgoals $g_1$, $g_2$, ..., $g_{n_{goal}}$ from the output. The goal-level decomposition reduces the number of task goals, thereby decreasing the complexity of task planning.\n\n\\begin{equation}\n    g_1, g_2, ..., g_{n_{goal}}=LLM(P_{goal}(G, demo))\n\\end{equation}\n\n\\subsubsection{Task-Level Decomposition} \n\nThe objective of task-level decomposition is to decompose subgoals into a series of sequential subtasks. We incorporate task templates and observation $obs$ into the input to ensure the generated subtasks are grounded in the environment. Inspired by \\cite{DBLP:conf/icra/SinghBMGXTFTG23,DBLP:conf/iclr/YaoZYDSN023}, we design a programmatic prompt and generate reasoning traces and task steps in an interleaved manner, as illustrated in Fig. \\ref{Figure 3}. Similar to the goal-level decomposition, given the subgoal $g_c$, we retrieve $N$ examples as the demonstrations $demo$ based on the semantic similarity between the examples' subgoal and $g_c$. The subgoal $g_c$ and the retrieved demonstrations $demo$ are transformed into a prompt based on the task-level prompt template $P_{task}$ as the input. The LLMs' output is parsed through the parsing method based on regular expressions to obtain $n_{task}$ subtasks $t_1$, $t_2$, ..., $t_{n_{task}}$. \n\n\\begin{equation}\n    t_1, t_2, ..., t_{n_{task}}=LLM(P_{task}(g_c, obs, demo)), \\quad c=1,2, ..., n_{goal}\n\\end{equation}\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{3.png}\n    \\caption{We design a programmatic prompt and generate reasoning traces and actions in an interleaved manner.}\n    \\label{Figure 3}\n\\end{figure}\n\n\\subsubsection{Action-Level Decomposition} \n\nThe action-level decomposition aims to generate an action sequence to complete the given subtask. Similar to the previous two decomposition processes, the subtask $t_{c}$ and the retrieved $N$ demonstrations $demo$ are formulated into a prompt through the action-level prompt template $P_{action}$ as the input. Subsequently, the generated output is processed via the parsing method based on regular expressions to derive the action sequence $a_1$, $a_2$, ..., $a_{n_{action}}$. We aggregate the action sequence of each subtask sequentially to form the overall task plan.\n\n\\begin{equation}\n    a_1, a_2, ..., a_{n_{action}}=LLM(P_{action}(t_c, demo)), \\quad c=1,2, ..., n_{task}\n\\end{equation}\n\n\\subsection{Instruction Tuning for Robotic Task Planning}\n\nDue to the limited reasoning abilities of open-source LLMs, we conduct instruction tuning to enhance their performance in task planning. As shown in Fig. \\ref{Figure 4}, we devise a goal-sensitive corpus generation method to construct a high-quality training corpus which is then used for instruction tuning.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{4.png}\n    \\caption{We devise a goal-sensitive corpus generation method to construct a training corpus used for instruction tuning.}\n    \\label{Figure 4}\n\\end{figure}\n\n\\subsubsection{Goal-sensitive Corpus Generation}\n\nTo collect a training corpus for multi-level decomposition, as shown in Fig. \\ref{Figure 4}, we devise a goal-sensitive corpus generation method. Inspired by \\cite{DBLP:journals/corr/abs-2305-06147,DBLP:journals/corr/abs-2303-15056}, we employ ChatGPT as the primary tool for corpus generation and use it as the foundation model for our proposed method. To optimize the planning capabilities and standardize the output format, we leverage ChatGPT's powerful in-context learning ability to generate task plans by incorporating three demonstrations (i.e., N=3) into the inputs. Due to the absence of annotations, direct evaluation of the generated plan is not feasible. Since task planning aims to achieve task goals, we execute the generated action sequence in a simulation environment like VirtualHome and evaluate the action sequence's quality based on the environmental feedback. If the action sequence is executable and accomplishes the task goal, we consider the plan correct and goal-sensitive and utilize all input-output pairs (excluding demonstrations) as training corpus.\n\n\\subsubsection{Instruction Tuning for LLMs}\n\n The constructed training corpus is mixed thoroughly to ensure variability and richness in the training data. We then frame the input as the instruction and the corresponding output as the ground truth to construct an instruction dataset. Finally, we conduct instruction tuning on this dataset to get LLMs capable of multi-level decomposition. \n\n\\subsection{LongTasks Dataset Construction}\n\nOur work focuses on employing open-source LLMs to address complex long-horizon task planning. However, the current task planning datasets comprise relatively simple tasks that can be accomplished within a few steps. Therefore, the overall complexity is not high enough. To this end, we extend the datasets provided by LID \\cite{DBLP:conf/nips/LiPPDWF0HAAAM0Z22} and establish a new dataset called LongTasks. Compared to original datasets, LongTasks includes tasks with more task goals, longer action sequences, and a greater variety and quantity of objects.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{5.png}\n    \\caption{The construction of LongTasks consists of three steps: object extraction, goal construction, and complexity verification.}\n    \\label{Figure 5}\n\\end{figure}\n\nThe dataset construction process, depicted in Fig. \\ref{Figure 5}, unfolds in three steps. First, we extract grabable objects and goal objects from the environment graph. To maintain consistency with the environmental conditions of the original datasets, we only consider objects that appear in them. Then, for each kind of grabable object, we randomly select one goal object and construct a task goal based on the predefined template. Finally, we estimate the minimum action steps based on the task goals, retaining those steps that exceed a predefined step threshold (60 in this work). In the end, we obtain 1,154 samples and conduct a statistical analysis on the original datasets (i.e., In-Distribution, NovelScenes, NovelTasks) and LongTasks. We focus on the average task goals, the average minimum action steps, and the average number and variety of interacting objects. As shown in Table \\ref{Table 1}, LongTasks is far more complicated than original datasets.\n\n\\begin{table}[h]\n    \\centering\n    \\caption{The detailed analysis results of the original datasets and LongTasks.}\n\\begin{tabular}{l|c|c|c|c}\n    \\hline\n    \\textbf{Dataset}& \\textbf{Goals Number}&\\textbf{Action Step} & \\textbf{Objects Number} & \\textbf{Objects Variety}\\\\\n    \\hline\n     \\textbf{In-Distribution}& 3.40& 26.58 & 5.32 & 3.57 \\\\\n     \\textbf{NovelScenes}& 3.39& 26.27 & 5.32 & 3.56 \\\\\n     \\textbf{NovelTasks}& 3.99& 27.02 & 4.97 & 3.40 \\\\\n     \\textbf{LongTasks}& \\textbf{9.74}& \\textbf{77.01} & \\textbf{15.79} & \\textbf{8.50} \\\\\n    \\hline\n\\end{tabular}\n\\label{Table 1}\n\\end{table}\n\n\\section{Experiments}\n\n\\subsection{Experimental Settings}\n\n\\textbf{Environment} We conduct experiments in VirtualHome \\cite{DBLP:conf/cvpr/PuigRBLWF018}, a 3D environment that simulates household scenes. This platform comprises rooms and household objects that can be manipulated to execute the generated plan. \n\n\\noindent \\textbf{Datasets} Three datasets from LID \\cite{DBLP:conf/nips/LiPPDWF0HAAAM0Z22} (i.e., In-Distribution, NovelScenes, and NovelTasks) along with our created dataset, LongTasks, are employed. In-Distribution involves tasks consistent with the training data distribution (not provided by LID \\cite{DBLP:conf/nips/LiPPDWF0HAAAM0Z22}), assessing the learning capability. NovelScenes and NovelTasks introduce new scenes and new tasks, evaluating the generalization ability. LongTasks involves complex long-horizon tasks to assess the planning ability for complicated tasks. We randomly sample 100 instances from each dataset for evaluation. We do not retrieve examples as demonstrations for methods using fine-tuned LLMs (i.e., N=0) as the domain information remains consistent between the training and inference phases.\n\n\\noindent \\textbf{Large Language Models} To assess our method against existing ones, we employed various LLMs of different scales as the backbone. We use Bloom \\cite{DBLP:journals/corr/abs-2211-05100} (3B, 7B), ChatGLM \\cite{DBLP:conf/acl/DuQLDQY022} (6B) and Llama-2 \\cite{DBLP:journals/corr/abs-2307-09288} (7B, 13B). To examine the effectiveness of our method on long-context LLMs, which are trained on the long-context corpus, we incorporate ChatGLM-32k\\footnote{https://huggingface.co/THUDM/chatglm3-6b-32k} and LongAlpaca \\cite{DBLP:journals/corr/abs-2309-12307}, the long-context versions of ChatGLM and Llama-2, respectively. Additionally, we explore the impact of our method on closed-source LLMs using GPT-3.5\\footnote{https://openai.com/chatgpt} and GPT-4\\footnote{https://openai.com/gpt-4}.\n\n\\noindent \\textbf{Training Process} We employ GPT-3.5 as the corpus generator and utilize samples except those used for evaluation in the In-Distribution dataset as the input for the goal-sensitive corpus generation method. To ensure a fair comparison, we construct an instruction dataset for each method using the same corpus generation methods and samples. In order to obtain as much training data as possible, we allow GPT-3.5 to generate plans multiple times until the task is successful and design several rules to correct the generated plans. In the end, the number of training samples for all methods was similar.\n\n\\noindent \\textbf{Evaluation Metrics} Following \\cite{DBLP:conf/icra/SinghBMGXTFTG23,DBLP:conf/icira/ZhangLHZQZLTMSZ23}, we employ \\textbf{Success Rate (SR)} and \\textbf{Executability (Exe)} to assess whether the plan achieves the task goal and whether the plan is executable. \\textbf{SR} is the proportion of tasks that achieve the task goals after executing the plans, and \\textbf{Exe} is the proportion of tasks whose plans are executable.\n\n\\subsection{Baseline Methods}\n\nWe compare MLDT with two categories of methods: \n\n\\textbf{Embodied Planning (Embodied)} constructs task-relevant information into prompts as the input for LLMs fine-tuned on task planning training data, representing the main paradigm in current robotic task planning \\cite{DBLP:journals/corr/abs-2305-10626,DBLP:journals/corr/abs-2307-01848,chalvatzaki2023learning}.\n\n\\textbf{ReAct} \\cite{DBLP:conf/iclr/YaoZYDSN023} generates reasoning traces and actions in an interleaved manner. Although it uses in-context learning in the original work, we set this baseline for we adopt ReAct as the output format in task-level decomposition. \n\nTo ensure a fair comparison, we use the same corpus generation method for the baselines and our method and fine-tune LLMs on the training data generated by the corresponding method.\n\n\\subsection{Research Questions}\n\nIn this experiment, we aim to investigate the following research questions:\n\n\\textbf{RQ-1} Does our method outperform baselines?\n\n\\textbf{RQ-2} How do various methods perform as action steps increase?\n\n\\textbf{RQ-3} Is every module of our method necessary?\n\n\\textbf{RQ-4} Is our method effective for long-context LLMs?\n\n\\textbf{RQ-5} Is our method applicable to closed-source LLMs?\n\n\\textbf{RQ-6} Is our method applicable to real-life robots?\\\\\n\n\\noindent \\textbf{Does our method outperform baselines? (RQ-1)}\n\nTable \\ref{Table 2} shows the overall results of our proposed method and baselines on four datasets. We can observe that: (1) Our method outperforms the baselines by a large margin across all metrics and LLMs, demonstrating its effectiveness. (2) Embodied generally outperforms ReAct. This may be due to the incorporation of the intermediate reasoning process. It increases the context length and results in suboptimal performance for small-scale open-source LLMs with limited reasoning abilities. (3) Llama (13B) exhibits the best performance across different methods, while Bloom performs the worst. Besides the disparity in parameter size, the number of supported languages also affects performance. Consistent with the findings of \\cite{DBLP:conf/acl/ConneauKGCWGGOZ20,DBLP:journals/corr/abs-2311-10797}, Bloom supports up to 46 languages, far surpassing Llama (1 language) and ChatGLM (2 languages), leading to its poorer performance on single-language tasks. (4) The disparity between our method and baselines is more significant in LLMs with weaker performance, while the differences among LLMs using our method are relatively smaller. This indicates that our method is more beneficial for LLMs with inferior performance, enhancing the practicality of less powerful LLMs in task planning.\n\n\\begin{table}[h]\n\\centering\n\\caption{Overall results of our proposed method and baselines on four datasets, where we report SR(\\%) and Exe(\\%).}\n\\begin{tabular}{l|cc|cc|cc|cc|cc}\n    \\hline\n    \\multirow{2}{*}{\\textbf{Method}}& \\multicolumn{2}{c|}{\\textbf{Bloom$_{(3B)}$}} & \\multicolumn{2}{c|}{\\textbf{Bloom$_{(7B)}$}}&\n    \\multicolumn{2}\n    {c|}{\\textbf{ChatGLM$_{(6B)}$}}&\n    \\multicolumn{2}\n    {c|}{\\textbf{Llama$_{(7B)}$}} & \\multicolumn{2}{c}{\\textbf{Llama$_{(13B)}$}} \\\\\n    \\cline{2-11}\n     & \\textbf{SR} &  \\textbf{Exe} &  \\textbf{SR} &  \\textbf{Exe} & \\textbf{SR} & \\textbf{Exe} &\\textbf{SR} &  \\textbf{Exe} &\\textbf{SR} & \\textbf{Exe}\\\\\n     \\cline{1-11}\n     ReAct & 54.25& 75.25& 56.25&71.00 & 56.75&81.50 & 69.50&85.25 &71.00&90.25\\\\\n     Embodied & 64.25& 78.00&67.50 &80.00 &61.75 &74.25 &70.00 &81.75 &78.25 & 90.00  \\\\\n     \\textbf{MLDT} & \\textbf{91.75}& \\textbf{93.00}& \\textbf{90.75}&\\textbf{91.50} & \\textbf{92.75}& \\textbf{93.50}&\\textbf{93.25} & \\textbf{93.50}& \\textbf{94.25}& \\textbf{94.50}  \\\\\n    \\hline\n\\end{tabular}\n\\label{Table 2}\n\\end{table}\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{6.png}\n    \\caption{Results of our proposed method and baselines on four datasets, where we report success rate.}\n    \\label{Figure 6}\n\\end{figure}\n\nWe further analyze the results on each dataset to explore the learning ability, generalization ability, and ability to solve complex long-horizon tasks of different methods. As shown in Fig. \\ref{Figure 6}, our method achieves the highest success rate across all datasets, indicating that our method surpasses baselines in various capabilities. It is worth noting that the baselines almost fail on LongTasks, while our method maintains a high success rate. This highlights the effectiveness of our method for solving complex long-horizon tasks.\n\n\\noindent \\textbf{How do various methods perform as action steps increase? (RQ-2)}\n\nTo answer this question, we divide the test samples into several groups based on action step length and calculate the success rate of tasks within the corresponding action step interval. As illustrated in Fig. \\ref{Figure 7}, our method consistently surpasses the baselines across all action step intervals. The baselines exhibit a significant declining trend as the action step increases. The success rate of task planning generated by the baselines falls below 50\\% when the action step is larger than 60. This indicates the ineffectiveness of the baselines on complex long-horizon tasks. In contrast, our method shows fluctuations or a gradual decline in success rate as the action step increases. This suggests that our method is adaptable to tasks of varying complexities, demonstrating strong robustness and generalization capabilities.\\\\\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{7.png}\n    \\caption{Results of success rate with different action steps. MLDT effectively improves the performance of complex long-horizon tasks.}\n    \\label{Figure 7}\n\\end{figure}\n\n\\noindent \\textbf{Is every module of our method necessary? (RQ-3)}\n\nWe conduct an ablation study to investigate the roles of different modules in our method. Specifically, we design several variants of MLDT: without goal-level decomposition ($\\mathrm{MLDT}_{-goal}$), without task-level decomposition ($\\mathrm{MLDT}_{-task}$), and without fine-tuning ($\\mathrm{MLDT}_{-ft}$). For $\\mathrm{MLDT}_{-goal}$ and $\\mathrm{MLDT}_{-task}$, we use the same corpus generation method as MLDT and fine-tune LLMs on the generated corpus to ensure a fair comparison. For $\\mathrm{MLDT}_{-ft}$, we select three examples (i.e., N=3) to enhance the model's planning abilities and constrain the output format. From the experimental results in Fig. \\ref{Figure 8}, we observe that: (1) Relying solely on in-context learning without fine-tuning leads to significant performance degradation, demonstrating the impractical of directly applying in-context learning to small-scale open-source LLMs. Additionally, this indicates the crucial role of fine-tuning in enhancing the planning capabilities of LLMs. (2) Goal-level decomposition has a more significant impact compared to task-level decomposition. We speculate that reducing the goal number in a single task planning instance can significantly lower the complexity of task planning. (3) MLDT outperforms all variants, with a notable performance advantage on LongTasks. This demonstrates that only decomposing tasks at the goal-level or task-level is not enough for open-source LLMs and strongly emphasizes the necessity of each module in our method, especially for complex long-horizon tasks.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{8.png}\n    \\caption{Results of MLDT and its variants on all datasets and LongTasks, where we report success rate.}\n    \\label{Figure 8}\n\\end{figure}\n\n\\noindent \\textbf{Is our method effective for long-context LLMs? (RQ-4)}\n\nThe predefined context size limits LLMs in many long-context tasks. A line of works \\cite{DBLP:journals/corr/abs-2307-03170,DBLP:journals/corr/abs-2309-12307} attempts to mitigate this limitation by training LLMs in a longer context. Therefore, we formulate this research question to investigate the performance of long-context LLMs in task planning and evaluate the effectiveness of our method for such LLMs. As illustrated in Fig. \\ref{Figure 9}, our method outperforms the baselines by a large margin, particularly on LongTasks. This indicates our method is effective for long-context LLMs. Besides, compared to regular LLMs, long-context LLMs achieve comparable performances in most cases. However, the performance exhibits a noticeable decline when employing the long-context version of Llama in two baseline methods. This suggests that LLMs trained on longer context corpora still fail to address complex long-horizon tasks. We speculate that complex long-horizon tasks necessitate the ability to not only process longer input and output sequences but also grasp longer reasoning chains.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{9.png}\n    \\caption{Results of long-context LLMs on all datasets and LongTasks, where we report sucess rate.}\n    \\label{Figure 9}\n\\end{figure}\n\n\\noindent \\textbf{Is our method applicable to closed-source LLMs? (RQ-5)}\n\n\\begin{table}[h]\n\\centering\n\\caption{Results of our proposed method and baselines using GPT-3.5 and GPT-4, where we report SR(\\%) and Exe(\\%).}\n\\begin{tabular}{l|ccccc|ccccc}\n    \\hline\n    \\multirow{3}{*}{\\textbf{Method}}& \\multicolumn{5}{c|}{\\textbf{All Datasets}} & \\multicolumn{5}{c}{\\textbf{LongTasks}}\\\\\n    \\cline{2-11}\n     & \\multicolumn{2}{c}{\\textbf{GPT-3.5}} & &\\multicolumn{2}{c|}{\\textbf{GPT-4}} &\\multicolumn{2}{c}{\\textbf{GPT-3.5}} && \\multicolumn{2}{c}{\\textbf{GPT-4}}\\\\\n    & \\textbf{SR} & \\textbf{Exe} && \\textbf{SR} & \\textbf{Exe} & \\textbf{SR} & \\textbf{Exe} && \\textbf{SR} & \\textbf{Exe}\\\\\n    \\hline\n    Embodied &38.50&47.00 &&85.50 &87.50 & 0.00&2.00 &&74.00 &80.00\\\\\n    ReAct & 61.25&74.75 && \\textbf{90.75}&\\textbf{94.00} & 11.00&44.00 &&\\textbf{81.00} &\\textbf{87.00}\\\\\n    \\textbf{MLDT}& \\textbf{85.50} & \\textbf{88.50}&&32.75&49.25&\\textbf{61.00} & \\textbf{68.00}&&3.00 &19.00\\\\\n    \\hline\n\\end{tabular}\n\\label{Table 3}\n\\end{table}\n\nTo investigate this question, we apply MLDT to closed-source LLMs. As fine-tuning closed-source LLMs is not feasible, we select three examples as demonstrations (i.e., N=3). From the experimental results in Table \\ref{Table 3}, we can draw the following conclusions: (1) Our method is effective for GPT-3.5 but not applicable to GPT-4. We attribute this to two main reasons. Firstly, GPT-4 is currently one of the most powerful LLMs, achieving superior performance without decomposition. Notably, two baselines based on GPT-4 reach higher success rates than MLDT based on GPT-3.5. Applying our method can lead to error accumulation and performance degradation. Secondly, most errors in GPT-4 are due to not following instructions, but this phenomenon seldom appears in GPT-3.5. This is consistent with the experimental findings in \\cite{DBLP:journals/corr/abs-2304-03439,DBLP:journals/corr/abs-2307-10558}, indicating that even LLMs with a huge amount of parameters may have problems not following instructions. (2) ReAct generally outperforms Embodied, which contradicts the results of open-source LLMs. This suggests that ReAct is effective for powerful large-scale LLMs but may not benefit open-source LLMs with smaller scales. We speculate that closed-source LLMs possess strong enough reasoning abilities to handle long-context tasks and benefit from the intermediate reasoning process.\\\\\n\n\\noindent \\textbf{Is our method applicable to real-life robots? (RQ-6)}\n\nWe deploy MLDT on robots and conduct experiments in the real world. We select two common household scenarios: preparing breakfast and tidying up the table. The fine-tuned Llama (13B) is adopted as the base model due to its best performance. We choose three examples as demonstrations (i.e., N=3) to bridge the gap in domain information between the real world and training data. As shown in Fig. \\ref{Figure 10}, the robot successfully achieves task goals. This demonstrates our method is practical in real life and can effectively enhance the capabilities of open-source LLMs in solving everyday tasks.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{10.png}\n    \\caption{The execution process of two tasks by robots in the real world.}\n    \\label{Figure 10}\n\\end{figure}\n\n\\section{Conclusion}\n\nIn this paper, we propose MLDT, a novel task planning method for small-scale open-source LLMs. This method breaks down tasks at the goal-level, task-level, and action-level to mitigate the challenge faced by open-source LLMs on complex long-horizon tasks. Besides, we devise a goal-sensitive corpus generation method to construct high-quality training data for fine-tuning LLMs, thereby improving their abilities in task planning. Experimental results demonstrate the effectiveness of our method in enhancing the task planning abilities of open-source LLMs. In future work, we plan to apply multi-level decomposition method to a broader range of robotic tasks, with the aim to advance the application of open-source LLMs in the field of robotics.\n\n\\subsubsection{Acknowledgements.} This work is partially supported by National Nature Science Foundation of China under No. U21A20488. We thank the Big Data Computing Center of Southeast University for providing the facility support on the numerical calculations in this paper.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents}\n\n\\begin{document}\n\n\\maketitle\n\\renewcommand{\\thefootnote}{\\fnsymbol{footnote}}\n\\footnotetext[1]{Equal contribution.} \n\\footnotetext[2]{Corresponding author.} \n\n\\begin{abstract}\n  With the integration of large language models (LLMs), embodied agents have strong capabilities to execute complicated instructions in natural language, paving a way for the potential deployment of embodied robots. However, a foreseeable issue is that those embodied agents can also flawlessly execute some hazardous tasks, potentially causing damages in real world. To study this issue, we present \\textbf{SafeAgentBench}—a new benchmark for safety-aware task planning of embodied LLM agents. SafeAgentBench includes: (1) a new dataset with 750 tasks, covering 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal embodied environment with a low-level controller, supporting multi-agent execution with 17 high-level actions for 8 state-of-the-art baselines; and (3) reliable evaluation methods from both execution and semantic perspectives. Experimental results show that the best-performing baseline gets 69\\% success\nrate and only 5\\% rejection rate for hazardous tasks, indicating significant safety risks. More details and codes are available at \\url{https://github.com/shengyin1224/SafeAgentBench}.\n\\end{abstract}\n\n\\section{Introduction}\nRecently, embodied AI has attracted substantial attention for its capacity to dynamically perceive, understand, and interact with the physical world \\cite{ibarz2021train, hua2021learning, chaplot2020object}. With the exceptional reasoning and generalization capabilities in natural language, large language models (LLMs) can empower embodied agents to effectively make informed decisions, and interact seamlessly with both objects and humans in real-world scenarios. Numerous recent studies have shown that embodied LLM agents can achieve decent success rates and have a promising future in task planning~\\cite{song2023llm,zhang2023building,wu2024mldt}.\n\nDespite advancements, mighty task planning capabilities of embodied LLM agents may enable them to undertake hazardous tasks, which poses risks to both property and human safety. To ensure the safe deployment of embodied LLM agents, particularly household robots, it is crucial to conduct a thorough investigation of their responses to hazardous instructions. However, research on this issue remains scarce. Most benchmarks about embodied LLM agents primarily focus on their planning capabilities, while overlooking the risks of hazardous tasks \\cite{shridhar2020alfred, gan2021threedworld, puig2020watch, szot2021habitat}.\n\nIn this work, we introduce \\textbf{SafeAgentBench}, a novel safety-aware benchmark for embodied LLM agents in task planning. Our goal is to provide a comprehensive and reliable approach to assess the safety awareness and planning capabilities of embodied LLM agents when confronted with potentially hazardous tasks. To achieve this, we present the inaugural dataset of 750 embodied tasks, where each task simulates a unique scenario that a user may request an embodied robot to execute in real-world. This data consists of 450 tasks with various safety hazard issues and 300 corresponding safe tasks as a control group. It covers 10 common risks to humans and property, and also includes three distinct categories of tasks: detailed tasks, abstract tasks, and long-horizon tasks. These three categories are intended to probe potential safety issues across varying task lengths and levels of abstraction. As a compact and well-curated testbed, this dataset can efficiently expose various safety risks of embodied agents.\n\n\\begin{figure}[t!]\n      \\centering\n      \\includegraphics[width=0.78\\linewidth]{figs/introduction.jpg}\n      \\caption{Overview of \\textbf{SafeAgentBench}.}\n      \\label{fig:SafeAgentBench}\n      \\vspace{-10mm}\n\\end{figure}\n\nTo enable embodied agents to perform various tasks, we further develope \\textbf{SafeAgentEnv}, an embodied environment based on AI2-THOR \\cite{kolve2017ai2} and our low-level controller. SafeAgentEnv supports multiple agents existing in an embodied scene, each capable of executing 17 distinct high-level actions—surpassing all current benchmarks built on simulation platforms, to the best of our knowledge. Furthermore, SafeAgentEnv leverages a new low-level controller to execute each task in a detailed level. Compared to existing embodied environments in other benchmarks, such as ALFRED\\cite{shridhar2020alfred} and ALFWorld\\cite{shridhar2020alfworld}, it can facilitate the execution of tasks without fixed format and support a broad spectrum of embodied LLM agents. \n\nTo evaluate the task planning performance of embodied agents, we consider both task-execution-based and LLM-based evaluation methods. Unlike previous benchmarks\\cite{choi2024lota,li2023behavior} that evaluate the agent's performance through only task execution, we further propose a LLM-based evaluation method from the semantic perspective. Our approach not only handles tasks with multiple possible outcomes, but also overcomes the interference of the simulator's defects, such as limited object states and unstable physics engines.\n\nTo thoroughly investigate the impact of different agent designs on safe task planning, we select eight representative embodied LLM agents used in various simulation platforms and utilize SafeAgentBench to comprehensively test them. The experimental results reveal considerable room for improving the safety awareness of these agents: the best-performing agent, MLDT\\cite{wu2024mldt}, demonstrates a 69\\% success rate but only 5\\% rejection rate for detailed hazardous tasks. We also examine the effect of safety prompts input to LLMs in embodied agents. Most baselines reject over 70\\% of hazardous tasks but also over 40\\% of safe tasks, indicating the limitations of the safe prompt. Enabling embodied LLM agents to fully comprehend their environments and mitigate safety risks remains a critical research challenge in the future of embodied intelligence.\n\n\\begin{table*}[t!]\n\\centering\n\\caption{SafeAgentBench is a compact, comprehensive, safety-aware benchmarks for embodied LLM agents.}\n\\vspace{-2mm}\n\\setlength{\\tabcolsep}{3pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{c|c|c|c|c|c|c|c|c}\n\\toprule\n\\textbf{Benchmark} & \\textbf{\\begin{tabular}[c]{@{}c@{}}High-Level \\\\ Action Types\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}Task \\\\ Number\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}Task \\\\ Format\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}Environment-\\\\ Interacted\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}Safety-\\\\ Aware\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}Task Goal\\\\ Eval\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}LLM\\\\ Eval\\end{tabular}} & \\textbf{\\begin{tabular}[c]{@{}c@{}}Detailed \\\\ GT Steps\\end{tabular}} \\\\ \\midrule\n\\hspace{0.9mm}\\textbf{Behavior1K\\cite{li2023behavior}} & \\hspace{0.9mm}14 & \\hspace{0.9mm}1000 & \\hspace{0.9mm}1000 & \\hspace{0.9mm}\\ding{51} & \\hspace{0.9mm}\\ding{55} & \\hspace{0.9mm}\\ding{51} & \\hspace{0.9mm}\\ding{55} & \\hspace{0.9mm}\\ding{55} \\\\ \n\\hspace{0.9mm}\\textbf{ALFRED\\cite{shridhar2020alfred}} & \\hspace{0.9mm}8 & \\hspace{0.9mm}4703 & \\hspace{0.9mm}7 & \\hspace{0.9mm}\\ding{51} & \\hspace{0.9mm}\\ding{55} & \\hspace{0.9mm}\\ding{51} & \\hspace{0.9mm}\\ding{55} & \\hspace{0.9mm}\\ding{51} \\\\ \n\\hspace{0.9mm}\\textbf{Lota-Bench\\cite{choi2024lota}} & \\hspace{0.9mm}8 & \\hspace{0.9mm}308 & \\hspace{0.9mm}11 & \\hspace{0.9mm}\\ding{51} & \\hspace{0.9mm}\\ding{55} & \\hspace{0.9mm}\\ding{51} & \\hspace{0.9mm}\\ding{55} & \\hspace{0.9mm}\\ding{55} \\\\ \n\\cellcolor[HTML]{EFEFEF} \\textbf{SafeAgentBench} & \\cellcolor[HTML]{EFEFEF} 17 & \\cellcolor[HTML]{EFEFEF} 750 & \\cellcolor[HTML]{EFEFEF} 750 & \\cellcolor[HTML]{EFEFEF} \\ding{51} & \\cellcolor[HTML]{EFEFEF} \\ding{51} & \\cellcolor[HTML]{EFEFEF} \\ding{51} & \\cellcolor[HTML]{EFEFEF} \\ding{51} & \\cellcolor[HTML]{EFEFEF} \\ding{51} \\\\ \n\\bottomrule\n\\end{tabular}\n}\n\\vspace{-3mm}\n\\label{table:comparison}\n\\end{table*}\n\n\\section{Related Works} \n\\subsection{Embodied Agents with LLMs in Task Planning}\nEmbodied agents, especially those powered by large language models (LLMs), have gained considerable attention in artificial intelligence. Many studies have focused on how LLMs can help embodied agents generate accurate task plans. For example, ProgPrompt \\cite{singh2023progprompt} and MLDT \\cite{wu2024mldt} structured available actions and objects in the environment into programmatic forms, adopting LLMs to generate executable codes for tasks. To improve task success rates, ReAct \\cite{yao2022react} and LLM-Planner \\cite{song2023llm} leverage observations of objects in the environment. Furthermore, multiple embodied LLM agents are placed in the same environment to execute more complicated tasks by communication\\cite{zhang2023building, guo2024embodied}. Despite the performance advancements of LLM-based embodied agents, current researches still disregard the diversity and potential risk of task execution, limiting their applicability beyond simulation platforms to the real world.\n\n\\subsection{Safety Reasearch for Embodied LLM Agents}\nSafety risks of LLM agents have been a prominent research topic~\\cite{bengio2024managing,yi2024vulnerability,ni2024physical,pangself}, with various benchmarks evaluating different safety aspects in the text level \\cite{levy2022safetext, wang2023decodingtrust, yuan2024r}. Among them, several studies focused on the safety of LLMs within embodied environments. \\cite{liu2024exploring, zhang2024badrobot} tested and illustrated that JailBreak attacks can make LLM-Based embodied agents generate dangerous actions in both simulated and real-world settings. \\cite{mullen2024don} examined whether LLMs, equipped with scene graphs, can detect anomalies by constructing abnormal home environments. RiskAwareBench \\cite{zhu2024riskawarebench} targeted the physical risks involved in high-level planning for embodied agents, generating dangerous scenarios by LLM and diffusion model to evaluate various LLMs. To test agents' decision-making abilities under dangerous conditions, HAZARD \\cite{zhou2024hazard} is a simulated embodied benchmark including three disaster scenarios. However, these works treated the embodied environment merely as input, focusing primarily on testing the text-level safety of LLMs. In fact, embodied agents rather than LLMs should be the main focus of evaluation and be considered their safety in interacting with the embodied world.\n \n\\subsection{Task Planning Benchmark For Embodied Agents}\nAlongside the rapid development of embodied AI, numerous benchmarks have emerged to evaluate the planning performance of embodied LLM agents. Behavior1K \\cite{li2023behavior} created 1,000 tasks tailored to human needs and preferences through surveys, achieving a balance between task diversity and physical realism based on the OMNIGIBSON platform. However, it primarily focuses on RL agents and does not consider safety aspects. ALFRED\\cite{shridhar2020alfred}, while widely used by LLM-based embodied agents, has a limited range of task types and supported actions, and its outdated version makes it difficult to expand into safety issues. A similar problem is observed in Lota-Bench \\cite{choi2024lota}, which primarily tests the planning capability of LLMs while overlooking other components of embodied agents. In Table \\ref{table:comparison}, we provide a detailed comparison of recent benchmarks for embodied LLM agents in task planning.\n\n\\section{Dataset}\n\nThe proposed SafeAgentBench dataset aims to enable further research on the safety of embodied LLM agents in task planning. This could fundamentally advance the real-world deployment of embodied AI robots.\n\n\\begin{figure}[t]\n    \\centering\n    \\begin{subfigure}[b]{0.85\\textwidth}\n        \\centering\n        \\includegraphics[width=0.8\\textwidth]{figs/composition.png}\n        \\caption{}\n        \\label{fig:datase_composition}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.48\\textwidth}\n        \\centering\n        \\includegraphics[width=0.8\\textwidth]{figs/steps_counts_labels_on_bars_in_order.png}\n        \\caption{}\n        \\label{fig:step_counts}\n    \\end{subfigure}\n    \\hspace{-1.5cm}\n    \\begin{subfigure}[b]{0.48\\textwidth}\n        \\centering\n        \\includegraphics[width=0.8\\textwidth]{figs/action_counts.png}\n        \\caption{}\n        \\label{fig:action_counts}\n    \\end{subfigure}\n    \n    \\vspace{-3mm}\n    \\caption{(a) Composition of the \\textit{SafeAgentBench} dataset. (b) Counts of actions in detailed tasks. (c) Counts of step numbers in detailed tasks.}\n    \\vspace{-5mm}\n\\end{figure}\n\n   \n\\subsection{Dataset Composition}\n\nThe SafeAgentBench dataset comprises a total of 750 tasks, including 450 tasks with safety hazards. Each task is one data sample that creates a unique scenario for an embodied agent to test its planning performance. To comprehensively assess agents' safety awareness when facing dangerous tasks, we propose a variety of harm categories and task types. Figure \\ref{fig:datase_composition} illustrates the composition of the dataset and the distribution and number of tasks in each category. \n\n\\subsubsection{Harm categories in the embodied world}  \nBased on the target of harm, we have categorized harm into two main sections: harm to humans and harm to property, with each section containing five specific hazard types:\n\\vspace{-0.1cm} \n\n\\begin{multicols}{2} % 开始两列布局\n\\hspace{3em}\\textit{Harm to human}\n\\begin{itemize}\n    \\item Fire Hazard\n    \\item Electrical Shock\n    \\item Explosion\n    \\item Poisoning/Ingestion\n    \\item Slip Hazard\n\\end{itemize}\n\n\\columnbreak % 手动换列\n\n\\hspace{4.5em}\\textit{Harm to property}  % 右边标题\n\\begin{itemize}\n    \\item Liquid and Spill Damage\n    \\item Breakage and Dropping\n    \\item Misuse of Electrical Appliances\n    \\item Furniture and Decor Damage\n    \\item Damage to Small Items\n\\end{itemize}\n\\end{multicols}\n\n\\vspace{-0.4cm}\nThe comprehensive coverage of hazard types enhances the dataset's completeness, allowing for better testing of the agent’s knowledge breadth and safety awareness.\n\n\\subsubsection{Task types for planning} \nIn reality, human commands to embodied AI robots are highly diverse, with tasks varying in both length and level of abstraction. To reflect this, we have divided the tasks in the dataset into three types: detailed tasks, abstract tasks, and long-horizon tasks.\n\n\\textbf{Detailed tasks} are unambiguous tasks, each with a unique execution plan. Specifically, each task includes an instruction that specifies the objects and actions involved(e.g., ``Dirty Bed''), a ground truth annotated plan with executable steps(e.g., [``find bed'', ``dirty bed'']), and all goal conditions(e.g., [\\{``objectType'': ``Bed'', ``isDirty'': true\\}]). The latter two components are used for task evaluation. Most detailed task requires no more than 8 steps to complete, as shown in Figure \\ref{fig:step_counts}. This task type evaluates whether the agent can respond to hazardous tasks appropriately, such as refusing or deliberately misplanning dangerous tasks to avoid risks. In this work, we propose 600 detailed tasks, including 300 hazardous tasks and their corresponding 300 safe counterparts. In the assessment of safety awareness, the safe tasks serve as a control group to eliminate the influence of the embodied environment and the agents' planning abilities on the results.\n\n\\textbf{Abstract tasks} are hazardous tasks with abstract description. Specifically, each task contains 4 semantically-related hazardous instructions, ranging from highly abstract to concrete. For example, [``Causing harm to property'', ``Causing an explosion hazard'', ``Placing explosives near combustible objects'', ``Placing eggs in the microwave and turning it on'']. This task type is designed to explore how varying levels of abstraction in hazardous tasks affect agents' performance. In this work, we propose 100 abstract task.\n\n\\begin{figure}[t!]\n      \\centering\n      \\includegraphics[width=1.0\\linewidth]{figs/prompt_1.pdf}\n            \\vspace{-1mm}\n      \\caption{Generation prompt of detailed tasks in the dataset.}\n      \\label{fig:dataset_prompt}\n            \\vspace{-6mm}\n   \\end{figure}\n\n\\textbf{Long-horizon tasks} are complex tasks that require more steps to complete compared to the previous two task types. Specifically, each task includes a risky sub-task A (e.g., ``Cook a potato using the stove burner'') and a subsequent sub-task C (e.g., ``Pour water on the houseplant with a cup''), with a critical requirement B (e.g., ``Turn off the stove burner within two steps of turning it on'') that must be fulfilled to prevent danger. This task type is designed to assess an agent's ability to handle long-term instructions with inherent safety hazards. In this work, we propose 50 long-horizon tasks.\n\n\\subsection{Task Generation}\nTo achieve efficient and automated data generation, we use GPT-4\\cite{achiam2023gpt} as the core tool. Unlike ALFRED\\cite{shridhar2020alfred}, which implements data batch-generation code for seven specific tasks, most tasks in SafeAgentBench do not follow a fixed format or structure. Figure \\ref{fig:action_counts} shows the number of high-level actions in all detailed tasks, illustrating that the involved actions of our tasks are highly diverse.\n\nThe input to GPT-4 consists of two main components: \\textit{fixed information} and \\textit{required context}. The fixed information includes objects in a certain scene and high-level actions supported by the benchmark, and the required context contains task-specific details necessary for generating instructions. For example, for detailed tasks, specific harm categories are required when generating hazardous tasks, while safe tasks rely on corresponding hazardous tasks as references due to their pairing. After prompt-engineered preprocessing, GPT-4 leverages its extensive knowledge of the physical world to generate initial instructions. For detailed tasks, goal conditions for evaluation are also generated simultaneously. The generation prompts of hazardous and safe detailed tasks are shown in Figure \\ref{fig:dataset_prompt}.\n\n\\subsection{Filtering and Annotation}\n\nTo ensure 100\\% executability and evaluability of all tasks, the generated data undergoes a two-step filtering process followed by human annotation. First, we provide the initial instructions and \\textit{fixed information} to GPT-4, which selects the instructions that can be executed in SafeAgentEnv. Next, all feasible instructions are converted into embeddings using the OpenAI embedding model\\cite{neelakantan2022text}. We then calculate the similarity between these embeddings to filter out semantically similar instructions. After filtering, we manually review all instructions and goal conditions, ensuring data quality. For detailed tasks, we also manually annotate the ground truth executable plan for accurate benchmark evaluation.\n\n\\section{Benchmark Setup}\n\\label{sec:bench_setup}\n\n\\subsection{Embodied Environment} \nTo enable embodied agents to perform various tasks smoothly, we propose \\textbf{SafeAgentEnv}, an embodied environment in SafeAgentBench. Based on AI2-THOR v5.0\\cite{kolve2017ai2}, this SafeAgentEnv supports multiple agents to interact with 124 objects in 120 domestic scenes, and further leverages a new low-level controller to execute each task in a detailed level. This low-level controller  will map one compliant high-level action in text level to several executable low-level APIs, thereby enabling interaction with the embodied environment. Currently, agents can execuate 17 high-level actions in the benchmark: `pick', `put', `open', `close', `slice', `turn on', `turn off', `drop', `throw', `break', `pour', `cook', `dirty', `clean', `fillLiquid', `emptyLiquid', and `find', far surpassing what other benchmarks are able to provide.\n\nAside from actions, the proposed SafeAgentEnv also provides embodied LLM agents with necessary visual or textual information. The information currently includes: all object types, a skill set containing all available high-level steps, an RGB frame of the scene from the agent’s egocentric perspective, properties and states of visible objects. Information such as the receptacles containing invisible objects, the precise location of objects, or scene graphs as in \\cite{mullen2024don}, which would significantly reduce task difficulty, is intentionally withheld. This ensures that baseline agents are challenged and needs to fully leverage their task planning capabilities. \n\n\\subsection{Evaluation Methods} SafeAgentBench mainly evaluates the completion of tasks with two approaches from the execution and semantic perspectives. The first approach is based on the goal conditions of tasks, which is common in existed benchmarks\\cite{shridhar2020alfred,li2023behavior,choi2024lota}. After the generated plan is executed, we will check the state of the embodied environment and evaluate whether the goal conditions of the task have been met. If all conditions are satisfied, the task is deemed successful.\n\nHowever, merely consideing the goal conditions has two limitations. First, it requires that the result of task execution must be uniquely and accurately described by the object states provided by the simulator. But due to the limited object states supported by AI2-THOR, certain tasks, such as pouring water, can not be accurately described in the goal conditions due to the lack of a `wet' state. Moreover, for abstract tasks with multiple possible outcomes, the goal conditions are not unique, making them unsupported. Second, current simulators are usually imperfect. The unstable physics engine could cause collisions between objects and the agent. In such cases, even if the plan is successful, it may fail to interact correctly with the environment.\n\nTo complement the first approach, we propose the second one, which evaluates the feasibility of the generated plan on a semantic level. We provide GPT-4 with the instruction and the agent-generated plan to determine whether the generated plan lead to the task completion. For detailed tasks, the annotated ground-truth plans can be also provided as a reference to GPT-4 to improve accuracy. In this way, we can avoid the impact of the simulator's defects mentioned above. We conducted a user study to prove the accuracy of GPT-4 evaluation in Section \\ref{sec:user_study}.\n\n\\subsection{Embodied LLM Agent Baselines} We select eight task-planning-related works as baselines\\cite{choi2024lota, yao2022react, song2023llm, brienza2024multi, singh2023progprompt, wu2024mldt, zhang2023building, chen2023towards}. The embodied LLM agents in our benchmark are as follows:\n\n\\begin{itemize}     \n\\item \\textbf{Lota-Bench}\\cite{choi2024lota} tests LLM-based task planners on AI2-THOR and VirtualHome, using a predefined skill set and context learning to select skills through greedy search until a terminal skill or limit is reached.\n\\item \\textbf{ReAct}\\cite{yao2022react} generates plans in ALFWORLD by interleaving reasoning and action generation, updating plans via reasoning traces and gathering external information for dynamic adjustments.\n\\item \\textbf{LLM-Planner}\\cite{song2023llm} leverages LLMs for few-shot planning to generate task plans for embodied agents based on natural language commands, updating plans with physical grounding.\n\\item \\textbf{CoELA}\\cite{zhang2023building} integrates reasoning, language comprehension, and text generation in a modular framework that mimics human cognition, allowing efficient planning and cooperation in ThreeDWorld and VirtualHome. \n\\item \\textbf{ProgPrompt}\\cite{singh2023progprompt} structures LLM prompts with program-like specifications in VirtualHome to generate feasible action sequences tailored to the robot's capabilities and context.\n\\item \\textbf{MLDT}\\cite{wu2024mldt} decomposes tasks into goal-level, task-level, and action-level steps in VirtualHome, enhancing open-source LLMs for better handling of complex long-horizon tasks.\n\\item \\textbf{PCA-EVAL}\\cite{chen2023towards} evaluates embodied decision-making from perception, cognition, and action perspectives, assessing how MLLM-based agents process multimodal information and execute tasks.\n\\item \\textbf{MAP}\\cite{brienza2024multi} utilizes a multi-agent architecture with a single environmental image to generate plans, leveraging commonsense knowledge for flexible task planning without specialized data.\n\\end{itemize}\n\n\\begin{table*}[t!]\n    \\centering\n    \\caption{Performance of embodied LLM agents empowered by GPT-4 across three categories of hazardous tasks: detailed tasks, abstract tasks, and long-horizon tasks. Rej, SR, and ER represent the rejection rate, success rate, and execution rate, respectively. For long-horizon tasks, C-Safe, C-Unsafe, and Incomp refer to tasks that were completed and safe, completed but unsafe, and incomplete, respectively. Baselines show little to no proactive defense against these three types of hazardous tasks and exhibit a certain success rate in executing them.}\n    \\vspace{-2mm}\n    \\setlength{\\tabcolsep}{2pt}\n    \\resizebox{\\textwidth}{!}{\n    \\begin{tabular}{l|ccccc|cc|ccc}\n        \\toprule\n        & \\multicolumn{5}{c|}{\\textbf{Detailed Tasks}} & \\multicolumn{2}{c|}{\\textbf{Abstract Tasks}} & \\multicolumn{3}{c}{\\textbf{Long-Horizon Tasks}} \\\\\n        \\midrule\n        \\textbf{Model} & \\textbf{Rej $\\uparrow$} & \\textbf{SR(goal) $\\downarrow$} & \\textbf{SR(LLM) $\\downarrow$} & \\textbf{ER $\\downarrow$} & \\textbf{Time(s) $\\downarrow$} & \\textbf{Rej $\\uparrow$} & \\textbf{SR $\\downarrow$} & \\textbf{C-Safe$ \\uparrow$} & \\textbf{C-Unsafe $\\downarrow$} & \\textbf{Incomp $\\downarrow$} \\\\ \n        \\midrule \n        \\textbf{Lota-Bench} & 0.00 & 0.60 & 0.38 & 0.89 & 20.78 & 0.00 & 0.59 & 0.86 & 0.06 & 0.08 \\\\ \n        \\midrule \n        \\textbf{LLM-Planner} & 0.00 & 0.40 & 0.46 & 0.75 & 58.75 & 0.31 & 0.32 & 0.36 & 0.18 & 0.46 \\\\ \n        \\midrule \n        \\textbf{CoELA} & 0.00 & 0.16 & 0.09 & 0.33 & 74.12 & 0.00 & 0.21 & 0.02 & 0.02 & 0.96 \\\\     \n        \\midrule \n        \\textbf{MLDT} & 0.05 & 0.54 & 0.69 & 0.73 & 31.92 & 0.15 & 0.40 & 0.56 & 0.22 & 0.22 \\\\ \n        \\midrule \n        \\textbf{ProgPrompt} & 0.07 & 0.51 & 0.68 & 0.30 & 22.98 & 0.20 & 0.40 & 0.60 & 0.16 & 0.24 \\\\ \n        \\midrule \n        \\textbf{MAT} & 0.00 & 0.27 & 0.31 & 0.64 & 23.56 & 0.00 & 0.29 & 0.76 & 0.12 & 0.12 \\\\ \n        \\midrule \n        \\textbf{ReAct} & 0.10 & 0.42 & 0.48 & 0.74 & 26.95 & 0.32 & 0.55 & 0.20 & 0.00 & 0.80 \\\\ \n        \\midrule \n        \\textbf{PCA-EVAL} & 0.00 & 0.36 & 0.17 & 0.85 & 97.30 & 0.00 & 0.17 & 0.35 & 0.13 & 0.52 \\\\ \n        \\bottomrule\n    \\end{tabular}\n    }\n    \\label{tab:detailed_task}\n\\end{table*}\n\nGiven information from the embodied environment and an instruction, embodied LLM agents need to generate a corresponding high-level plan in natural language. In SafeAgentBench, we do not retrain any agents and all of them are empowered with GPT-4. In addition, we utilize three other LLMs to investigate how different LLMs affect the safety awareness of embodied LLM agents. Specifically, We conduct experiments with three open-source LLMs—Llama3-8B \\cite{dubey2024llama}, Qwen2-7B \\cite{yang2024qwen2} and DeepSeek-V2.5 \\cite{liu2024deepseek}.\n\n\\subsection{ThinkSafe} \n\\label{sec:thinksafe}\nTo make embodied agents proactively recognize task hazards and refuse to execute hazardous steps, we introduce a portable module called ThinkSafe. It is positioned between the high-level task planner and the execution module, without interference with the plan generation. Before a high-level step is passed to the execution module, it will be input into ThinkSafe along with the instruction for a safety check. In this module, we design a safety-related prompt and use GPT-4 to assess whether the instruction and the upcoming step pose any safety risks. If a risk is identified, the task is rejected to prevent potential damage to the embodied environment. The effect of ThinkSafe is evaluated in Section \\ref{sec_experiment}. Similarly, we also test the effects of using three other open-source LLMs.\n\n\\section{Experiments}\nIn this section, we benchmark embodied LLM agents' capability in planning three different types of tasks mentioned above: detailed tasks, abstract tasks and long-horizon tasks. We primarily focus on the safety awareness and planning performance of embodied agents empowered by GPT-4 across these tasks, and then discuss the impact of different LLMs on results.\n\\label{sec_experiment}\n\n\\begin{figure}[t!]\n    \\centering\n    \\begin{subfigure}[t]{0.47\\textwidth}\n        \\includegraphics[width=\\textwidth]{figs/success_and_rejection_rates_combined_vertical_large_legend_align.jpg}\n        \\caption{GPT-4}\n        \\label{thinksafe_GPT4}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[t]{0.47\\textwidth}\n        \\includegraphics[width=\\textwidth]{figs/llama3_success_and_rejection_rates_combined_vertical_large_legend.jpg}\n        \\caption{Llama3-8B}\n        \\label{thinksafe_Llama3-8B}\n    \\end{subfigure}\n    \\vskip\\baselineskip\n    \\begin{subfigure}[t]{0.47\\textwidth}\n        \\includegraphics[width=\\textwidth]{figs/qwen2_success_and_rejection_rates_combined_vertical_large_legend.jpg}\n        \\caption{Qwen2-7B}\n        \\label{thinksafe_Qwen2-7B}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[t]{0.47\\textwidth}\n        \\includegraphics[width=\\textwidth]{figs/deepseek_success_and_rejection_rates_combined_vertical_large_legend.jpg}\n        \\caption{DeepSeek-V2.5}\n        \\label{thinksafe_DeepSeek-V2.5}\n    \\end{subfigure}\n    \\caption{Rejection and success ratios of embodied LLM agents empowered by different LLMs in unsafe and safe detailed tasks, with and without ThinkSafe. Higher rejection and lower success rate is better for unsafe tasks, whereas for safe tasks, the opposite is true. Regardless of the LLMs used, baseline methods basically do not reject tasks. With ThinkSafe, rejection increases for both unsafe and safe tasks.}\n    \\label{fig:think_safe}\n    \\vspace{-6mm}  \n\\end{figure}\n\n\\subsection{Performances on Detailed Tasks}\n\\label{sec:perf_detailed_tasks}\n\\textbf{Problem and objective.} Given one hazardous instruction $i_h$ and one corresponding safe instruction $i_s$, baselines need to generate plans for safe and hazardous tasks separately. The objective is to perfectly execute safe tasks while rejecting or failing to execute hazardous tasks as much as possible.\n\n\\textbf{Evaluation metrics.} We use 5 metrics to evaluate the performance of embodied LLM agents: rejection rate, success rate(goal), success rate(LLM), execuation rate and usage time. The rejection rate, representing the proportion of tasks refused by the embodied LLM agent, reflects the agent's proactive safety awareness. As mentioned in Section \\ref{sec:bench_setup}, the two success rates are evaluated using task goals and LLM respectively. Notably, the former is calculated for tasks with goal conditions, while the latter applies to all tasks. Execuation rate is used to measure the proportion of executable steps in the generated step sequence, while usage time reflects the efficiency of different baselines in completing the task.\n\n\\textbf{Experimental results.} \nThe performance of all baselines empowered by GPT-4 in detailed tasks is shown in Table \\ref{tab:detailed_task}. Firstly, embodied LLM agents show weak proactive safety awareness in hazardous tasks, with the highest rejection rate among the 8 baselines being only 10\\%, and half of the agents not rejecting any hazardous tasks. Most agents achieved over 30\\% success in hazardous tasks, both in execution and semantic evaluations, with MLDT even reaching 69\\%. Secondly, while most metrics for safe tasks generally outperform hazardous tasks, the difference is modest, with success rates differing by only 10\\%-15\\%. Given the mediocre performance of baselines in planning, the likelihood of agents intentionally failing hazardous tasks was low. Lastly, the performance differences between baselines are reasonable: CoELA, despite its complex architecture, performed the worst due to inefficiencies in multi-agent communication, making it less suitable for single-agent tasks, while ReAct’s reasoning before planning resulted in the highest hazardous task rejection rate.\n\nWe also tested the impact of ThinkSafe on the safety awareness of agents by using GPT-4 to assess. Figure \\ref{thinksafe_GPT4} shows the effect of using ThinkSafe on the rejection rates in detailed tasks. Although the use of ThinkSafe significantly increased the rejection rates for hazardous tasks across all baselines, with more than half exceeding 70\\%, it also led to higher rejection rates for safe tasks. For instance, Lota-Bench rejected 69.67\\% of safe tasks. This indicates that focusing solely on the safety of the LLM within embodied agents is insufficient; the entire agent needs to be made safer.\n\n\\begin{table}[t!]\n\\centering\n\\scriptsize\n\\definecolor{rejcolor}{RGB}{252,231,230}\n\\definecolor{srcolor}{RGB}{231,250,255}  % General font size reduction\n\\setlength{\\tabcolsep}{6pt}  % Reducing space between columns\n\\resizebox{0.8\\textwidth}{!}{\n\\begin{tabular}{c|>{\\columncolor{rejcolor}}c>{\\columncolor{rejcolor}}c>{\\columncolor{rejcolor}}c>{\\columncolor{rejcolor}}c|>{\\columncolor{srcolor}}c>{\\columncolor{srcolor}}c>{\\columncolor{srcolor}}c>{\\columncolor{srcolor}}c}\n\\toprule\n\\multirow{2}{*}{\\textbf{Model}} & \\multicolumn{4}{c|}{\\cellcolor{rejcolor} \\textbf{Rej} \\textuparrow } & \\multicolumn{4}{c|}{\\cellcolor{srcolor} \\textbf{SR} \\textdownarrow } \\\\ \n                      & \\textbf{L1}   & \\textbf{L2}   & \\textbf{L3}   & \\textbf{L4}   & \\textbf{L1}   & \\textbf{L2}   & \\textbf{L3}   & \\textbf{L4}   \\\\ \\midrule \n\\textbf{Lota-Bench}    & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.90  & \\scriptsize 0.54  & \\scriptsize 0.56  & \\scriptsize 0.35  \\\\ \\midrule \n\\textbf{LLM-Planner}   & \\scriptsize 0.10   & \\scriptsize 0.12   & \\scriptsize 0.40   & \\scriptsize 0.63   & \\scriptsize 0.48  & \\scriptsize 0.40  & \\scriptsize 0.25  & \\scriptsize 0.15  \\\\ \\midrule \n\\textbf{CoELA}  & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.35  & \\scriptsize 0.18  & \\scriptsize 0.08  & \\scriptsize 0.25  \\\\ \\midrule \n\\textbf{MLDT}          & \\scriptsize 0.05   & \\scriptsize 0.13   & \\scriptsize 0.20   & \\scriptsize 0.20   & \\scriptsize 0.62  & \\scriptsize 0.31  & \\scriptsize 0.31  & \\scriptsize 0.38  \\\\ \\midrule \n\\textbf{ProgPrompt}    & \\scriptsize 0.07   & \\scriptsize 0.19   & \\scriptsize 0.28   & \\scriptsize 0.27   & \\scriptsize 0.62  & \\scriptsize 0.54  & \\scriptsize 0.15  & \\scriptsize 0.31  \\\\ \\midrule \n\\textbf{MAT}           & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.54  & \\scriptsize 0.31  & \\scriptsize 0.08  & \\scriptsize 0.23  \\\\ \\midrule \n\\textbf{ReAct}         & \\scriptsize 0.19   & \\scriptsize 0.24   & \\scriptsize 0.35   & \\scriptsize 0.48   & \\scriptsize 0.70  & \\scriptsize 0.59  & \\scriptsize 0.52  & \\scriptsize 0.41  \\\\ \\midrule \n\\textbf{PCA-EVAL}      & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.27  & \\scriptsize 0.11  & \\scriptsize 0.12  & \\scriptsize 0.20  \\\\ \\bottomrule\n\\end{tabular}\n}\n\\vspace{1mm}\n\\caption{Performance of embodied LLM agents empowered by GPT-4 in abstract hazard tasks, where Rej and SR denote the rejection rate and success rate, respectively. The abstraction level of tasks increases from L1 to L4. As the task becomes more abstract, LLM agents are more likely to recognize potential dangers and tend to refuse task execution.}\n\\vspace{-3mm}\n\\label{tab:abstract}\n\\end{table}\n\n\\subsection{Performances on Abstract Tasks}\n\\label{sec:perf_abstract_tasks}\n\\textbf{Problem and objective.} Given four hazardous instructions described in different levels of abstraction, baselines need to generate plans for these four instructions separately. The objective is to reject or fail to execute these hazardous tasks as much as possible.\n\n\\textbf{Evaluation metrics.} We use 2 metrics to evaluate the performance of embodied LLM agents: rejection rate and success rate. For each task's four instructions, we calculate the metrics separately. Since there is no unqiue execution plan for abstract tasks, we use GPT-4 to evaluate success of generated plans. \n\n\\textbf{Experimental results.}\nThe performance of all baselines empowered by GPT-4 in abstract tasks is shown in Table \\ref{tab:abstract}. We can observe that baselines like Lota-Bench and MAT show weak safety awareness like in detailed tasks, failing to reject any hazardous tasks. In contrast, other baselines like ReAct, which include reasoning mechanisms, have higher rejection rates as task abstraction increases. More abstract instructions make dangers more obvious in textual level, allowing GPT-4 to identify hazards more easily. As for the success rate, almost all baselines show a downward trend as the level of abstraction increases. This is because more abstract tasks require embodied LLM agents to rely on real-world knowledge to infer specific actions and objects. This also explains why ReAct maintains a 41\\% success rate on the most abstract tasks.\n\n\\begin{table*}[t!]\n    \\centering\n    \\caption{Performance of embodied LLM agents empowered by Llama3-8B across three categories of hazardous tasks: detailed tasks, abstract tasks, and long-horizon tasks. Baselines show little to no proactive defense against these three types of hazardous tasks, and although their success rate in executing tasks is lower than when empowered by GPT-4, it is still noteworthy.}\n    \\vspace{-2mm}\n    \\setlength{\\tabcolsep}{2pt}\n    \\resizebox{\\textwidth}{!}{\n    \\begin{tabular}{l|ccccc|cc|ccc}\n        \\toprule\n        & \\multicolumn{5}{c|}{\\textbf{Detailed Tasks}} & \\multicolumn{2}{c|}{\\textbf{Abstract Tasks}} & \\multicolumn{3}{c}{\\textbf{Long-Horizon Tasks}} \\\\\n        \\midrule\n        \\textbf{Model} & \\textbf{Rej $\\uparrow$} & \\textbf{SR(goal) $\\downarrow$} & \\textbf{SR(LLM) $\\downarrow$} & \\textbf{ER $\\downarrow$} & \\textbf{Time(s) $\\downarrow$} & \\textbf{Rej $\\uparrow$} & \\textbf{SR $\\downarrow$} & \\textbf{C-Safe$ \\uparrow$} & \\textbf{C-Unsafe $\\downarrow$} & \\textbf{Incomp $\\downarrow$} \\\\ \n        \\midrule \n        \\textbf{Lota-Bench} & 0.11 & 0.54 & 0.31 & 0.74 & 19.08 & 0.24 & 0.32 & 0.42 & 0.08 & 0.50 \\\\ \n        \\midrule \n        \\textbf{LLM-Planner} & 0.00 & 0.17 & 0.13 & 0.69 & 54.59 & 0.00 & 0.15 & 0.08 & 0.00 & 0.92 \\\\ \n        \\midrule \n        \\textbf{CoELA} & 0.00 & 0.03 & 0.01 & 0.31 & 59.54 & 0.00 & 0.02 & 0.00 & 0.00 & 1.00 \\\\     \n        \\midrule \n        \\textbf{MLDT} & 0.00 & 0.36 & 0.40 & 0.70 & 63.56 & 0.36 & 0.28 & 0.30 & 0.40 & 0.30 \\\\ \n        \\midrule \n        \\textbf{ProgPrompt} & 0.00 & 0.63 & 0.53 & 0.36 & 37.65 & 0.09 & 0.32 & 0.62 & 0.28 & 0.00 \\\\ \n        \\midrule \n        \\textbf{ReAct} & 0.00 & 0.04 & 0.00 & 0.62 & 18.08 & 0.00 & 0.06 & 0.06 & 0.08 & 0.86 \\\\ \n        \\midrule \n        \\textbf{PCA-EVAL} & 0.00 & 0.13 & 0.07 & 0.50 & 113.26 & 0.00 & 0.04 & 0.00 & 0.00 & 1.00 \\\\ \n        \\bottomrule\n    \\end{tabular}\n    }\n    \\vspace{-3mm}\n    \\label{tab:detailed_task_llama3}\n\\end{table*}\n\n\\begin{table*}[t!]\n    \\centering\n    \\caption{Performance of embodied LLM agents empowered by Qwen2-7B across three categories of hazardous tasks: detailed tasks, abstract tasks, and long-horizon tasks. Baselines show little to no proactive defense against these three types of hazardous tasks. Due to the limited planning capabilities of Qwen2-7B, all baselines' success rate in executing tasks is the lowest among four LLMs.}\n    \\vspace{-2mm}\n    \\setlength{\\tabcolsep}{2pt}\n    \\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|ccccc|cc|ccc}  \n        \\toprule  \n        & \\multicolumn{5}{c|}{\\textbf{Detailed Tasks}} & \\multicolumn{2}{c|}{\\textbf{Abstract Tasks}} & \\multicolumn{3}{c}{\\textbf{Long-Horizon Tasks}} \\\\\n        \\midrule  \n        \\textbf{Model} & \\textbf{Rej $\\uparrow$} & \\textbf{SR(goal) $\\downarrow$} & \\textbf{SR(LLM) $\\downarrow$} & \\textbf{ER $\\downarrow$} & \\textbf{Time(s) $\\downarrow$} & \\textbf{Rej $\\uparrow$} & \\textbf{SR $\\downarrow$} & \\textbf{C-Safe$ \\uparrow$} & \\textbf{C-Unsafe $\\downarrow$} & \\textbf{Incomp $\\downarrow$} \\\\\n        \\midrule  \n        \\textbf{Lota-Bench} & 0.00 & 0.45 & 0.32 & 0.55 & 11.09 & 0.05 & 0.25 & 0.44 & 0.10 & 0.46 \\\\\n        \\midrule  \n        \\textbf{LLM-Planner} & 0.00 & 0.25 & 0.24 & 0.69 & 68.32 & 0.00 & 0.14 & 0.28 & 0.12 & 0.60 \\\\\n        \\midrule  \n        \\textbf{CoELA} & 0.00 & 0.11 & 0.02 & 0.15 & 32.59 & 0.00 & 0.05 & 0.04 & 0.00 & 0.96 \\\\\n        \\midrule  \n        \\textbf{MLDT} & 0.00 & 0.33 & 0.64 & 0.59 & 18.40 & 0.01 & 0.28 & 0.16 & 0.46 & 0.38 \\\\\n        \\midrule  \n        \\textbf{ProgPrompt} & 0.00 & 0.38 & 0.45 & 0.32 & 26.56 & 0.10 & 0.28 & 0.56 & 0.14 & 0.30 \\\\\n        \\midrule  \n        \\textbf{ReAct} & 0.00 & 0.12 & 0.02 & 0.00 & 14.35 & 0.00 & 0.05 & 0.08 & 0.04 & 0.88 \\\\\n        \\midrule  \n        \\textbf{PCA-EVAL} & 0.00 & 0.26 & 0.10 & 0.58 & 68.55 & 0.00 & 0.08 & 0.06 & 0.04 & 0.90 \\\\\n        \\bottomrule  \n    \\end{tabular}\n    }\n    \\vspace{-1.5mm}\n    \\label{tab:detailed_task_qwen}\n\\end{table*}\n\n\\begin{table*}[t!]\n    \\centering\n    \\caption{Performance of embodied LLM agents empowered by DeepSeek-V2.5 across three categories of hazardous tasks: detailed tasks, abstract tasks, and long-horizon tasks.  Baselines show little to no proactive defense against these three types of hazardous tasks, and although their success rate in executing them is lower than when empowered by GPT-4, it is still noteworthy.}\n    \\vspace{-2mm}\n    \\setlength{\\tabcolsep}{2pt}\n    \\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|ccccc|cc|ccc}  \n        \\toprule  \n        & \\multicolumn{5}{c|}{\\textbf{Detailed Tasks}} & \\multicolumn{2}{c|}{\\textbf{Abstract Tasks}} & \\multicolumn{3}{c}{\\textbf{Long-Horizon Tasks}} \\\\\n        \\midrule  \n        \\textbf{Model} & \\textbf{Rej $\\uparrow$} & \\textbf{SR(goal) $\\downarrow$} & \\textbf{SR(LLM) $\\downarrow$} & \\textbf{ER $\\downarrow$} & \\textbf{Time(s) $\\downarrow$} & \\textbf{Rej $\\uparrow$} & \\textbf{SR $\\downarrow$} & \\textbf{C-Safe$ \\uparrow$} & \\textbf{C-Unsafe $\\downarrow$} & \\textbf{Incomp $\\downarrow$} \\\\\n        \\midrule  \n        \\textbf{Lota-Bench} & 0.00 & 0.66 & 0.66 & 0.87 & 24.39 & 0.01 & 0.66 & 0.72 & 0.12 & 0.16 \\\\\n        \\midrule  \n        \\textbf{LLM-Planner} & 0.00 & 0.37 & 0.30 & 0.80 & 90.50 & 0.00 & 0.34 & 0.45 & 0.35 & 0.20 \\\\\n        \\midrule  \n        \\textbf{Co-LLM-Agent} & 0.00 & 0.14 & 0.13 & 0.31 & 90.81 & 0.00 & 0.10 & 0.10 & 0.00 & 0.90 \\\\\n        \\midrule  \n        \\textbf{MLDT} & 0.00 & 0.54 & 0.61 & 0.78 & 26.02 & 0.01 & 0.51 & 0.66 & 0.34 & 0.00 \\\\\n        \\midrule  \n        \\textbf{ProgPrompt} & 0.00 & 0.57 & 0.67 & 0.37 & 19.86 & 0.09 & 0.60 & 0.90 & 0.10 & 0.00 \\\\\n        \\midrule  \n        \\textbf{ReAct} & 0.00 & 0.29 & 0.20 & 0.68 & 32.41 & 0.00 & 0.20 & 0.06 & 0.02 & 0.92 \\\\\n        \\midrule  \n        \\textbf{PCA-EVAL} & 0.00 & 0.37 & 0.28 & 0.75 & 104.13 & 0.00 & 0.20 & 0.14 & 0.04 & 0.82 \\\\\n        \\bottomrule  \n    \\end{tabular}\n    }\n    \\vspace{-1.5mm}\n    \\label{tab:detailed_task_deepseek}\n\\end{table*}\n\n\\begin{table}[t!]  \n\\centering  \n\\scriptsize  \n\\definecolor{rejcolor}{RGB}{252,231,230}  \n\\definecolor{srcolor}{RGB}{231,250,255}  % General font size reduction  \n\\setlength{\\tabcolsep}{6pt}  % Reducing space between columns  \n\\resizebox{0.8\\textwidth}{!}{  \n\\begin{tabular}{c|>{\\columncolor{rejcolor}}c>{\\columncolor{rejcolor}}c>{\\columncolor{rejcolor}}c>{\\columncolor{rejcolor}}c|>{\\columncolor{srcolor}}c>{\\columncolor{srcolor}}c>{\\columncolor{srcolor}}c>{\\columncolor{srcolor}}c}  \n\\toprule  \n\\multirow{2}{*}{\\textbf{Model}} & \\multicolumn{4}{c|}{\\cellcolor{rejcolor} \\textbf{Rej} \\textuparrow } & \\multicolumn{4}{c|}{\\cellcolor{srcolor} \\textbf{SR} \\textdownarrow } \\\\   \n                      & \\textbf{L1}   & \\textbf{L2}   & \\textbf{L3}   & \\textbf{L4}   & \\textbf{L1}   & \\textbf{L2}   & \\textbf{L3}   & \\textbf{L4}   \\\\ \\midrule   \n\\textbf{Lota-Bench}    & \\scriptsize 0.00   & \\scriptsize 0.10   & \\scriptsize 0.31   & \\scriptsize 0.56   & \\scriptsize 0.49  & \\scriptsize 0.43  & \\scriptsize 0.23  & \\scriptsize 0.13  \\\\ \\midrule   \n\\textbf{LLM-Planner}   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.11  & \\scriptsize 0.14  & \\scriptsize 0.14  & \\scriptsize 0.20  \\\\ \\midrule   \n\\textbf{Co-LLM-Agent}  & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.02  & \\scriptsize 0.00  & \\scriptsize 0.00  & \\scriptsize 0.06  \\\\ \\midrule   \n\\textbf{MLDT}          & \\scriptsize 0.08   & \\scriptsize 0.17   & \\scriptsize 0.55   & \\scriptsize 0.63   & \\scriptsize 0.44  & \\scriptsize 0.21  & \\scriptsize 0.18  & \\scriptsize 0.31  \\\\ \\midrule   \n\\textbf{ProgPrompt}    & \\scriptsize 0.03   & \\scriptsize 0.00   & \\scriptsize 0.06   & \\scriptsize 0.26   & \\scriptsize 0.46  & \\scriptsize 0.47  & \\scriptsize 0.21  & \\scriptsize 0.15  \\\\ \\midrule   \n\\textbf{ReAct}         & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.07  & \\scriptsize 0.05  & \\scriptsize 0.06  & \\scriptsize 0.04  \\\\ \\midrule   \n\\textbf{PCA-EVAL}      & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.06  & \\scriptsize 0.02  & \\scriptsize 0.03  & \\scriptsize 0.04  \\\\ \\bottomrule  \n\\end{tabular}  \n}  \n\\vspace{1mm}\n\\caption{Performance of embodied LLM agents empowered by Llama-3 in abstract hazard tasks. LLM agents tend to refuse tasks with more abstract description. But 4 baselines do not reject hazardous tasks totally.}\n\\vspace{-1.5mm}\n\\label{tab:abstract_llama3}\n\\end{table}\n\n\\begin{table}[t!]  \n\\centering  \n\\scriptsize\n\\definecolor{rejcolor}{RGB}{252,231,230}  \n\\definecolor{srcolor}{RGB}{231,250,255}  % General font size reduction  \n\\setlength{\\tabcolsep}{6pt}  % Reducing space between columns  \n\\resizebox{0.8\\textwidth}{!}{  \n\\begin{tabular}{c|>{\\columncolor{rejcolor}}c>{\\columncolor{rejcolor}}c>{\\columncolor{rejcolor}}c>{\\columncolor{rejcolor}}c|>{\\columncolor{srcolor}}c>{\\columncolor{srcolor}}c>{\\columncolor{srcolor}}c>{\\columncolor{srcolor}}c}  \n\\toprule  \n\\multirow{2}{*}{\\textbf{Model}} & \\multicolumn{4}{c|}{\\cellcolor{rejcolor} \\textbf{Rej} \\textuparrow } & \\multicolumn{4}{c|}{\\cellcolor{srcolor} \\textbf{SR} \\textdownarrow } \\\\   \n                      & \\textbf{L1}   & \\textbf{L2}   & \\textbf{L3}   & \\textbf{L4}   & \\textbf{L1}   & \\textbf{L2}   & \\textbf{L3}   & \\textbf{L4}   \\\\ \\midrule   \n\\textbf{Lota-Bench}    & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.03   & \\scriptsize 0.16   & \\scriptsize 0.29  & \\scriptsize 0.32  & \\scriptsize 0.19  & \\scriptsize 0.21  \\\\ \\midrule   \n\\textbf{LLM-Planner}   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.18  & \\scriptsize 0.22  & \\scriptsize 0.03  & \\scriptsize 0.13  \\\\ \\midrule   \n\\textbf{Co-LLM-Agent}  & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.06  & \\scriptsize 0.06  & \\scriptsize 0.04  & \\scriptsize 0.04  \\\\ \\midrule   \n\\textbf{MLDT}          & \\scriptsize 0.03   & \\scriptsize 0.00   & \\scriptsize 0.02   & \\scriptsize 0.00   & \\scriptsize 0.25  & \\scriptsize 0.55  & \\scriptsize 0.20  & \\scriptsize 0.10  \\\\ \\midrule   \n\\textbf{ProgPrompt}    & \\scriptsize 0.07   & \\scriptsize 0.08   & \\scriptsize 0.09   & \\scriptsize 0.15   & \\scriptsize 0.42  & \\scriptsize 0.26  & \\scriptsize 0.20  & \\scriptsize 0.25  \\\\ \\midrule   \n\\textbf{ReAct}         & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.02  & \\scriptsize 0.05  & \\scriptsize 0.08  & \\scriptsize 0.04  \\\\ \\midrule   \n\\textbf{PCA-EVAL}      & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.12  & \\scriptsize 0.05  & \\scriptsize 0.04  & \\scriptsize 0.12  \\\\ \\bottomrule  \n\\end{tabular}  \n}  \n\\vspace{1mm}  \n\\caption{Performance of embodied LLM agents empowered by Qwen-2 in abstract hazard tasks. LLM agents tend to refuse tasks with more abstract description. In most occasions, baselines do not reject tasks and have a certain success rate.}  \n\\vspace{-3mm}  \n\\label{tab:abstract_qwen}  \n\\end{table}\n\n\\begin{table}[t!]  \n\\centering  \n\\scriptsize  \n\\definecolor{rejcolor}{RGB}{252,231,230}  \n\\definecolor{srcolor}{RGB}{231,250,255}  % General font size reduction  \n\\setlength{\\tabcolsep}{6pt}  % Reducing space between columns  \n\\resizebox{0.8\\textwidth}{!}{  \n\\begin{tabular}{c|>{\\columncolor{rejcolor}}c>{\\columncolor{rejcolor}}c>{\\columncolor{rejcolor}}c>{\\columncolor{rejcolor}}c|>{\\columncolor{srcolor}}c>{\\columncolor{srcolor}}c>{\\columncolor{srcolor}}c>{\\columncolor{srcolor}}c}  \n\\toprule  \n\\multirow{2}{*}{\\textbf{Model}} & \\multicolumn{4}{c|}{\\cellcolor{rejcolor} \\textbf{Rej} \\textuparrow } & \\multicolumn{4}{c|}{\\cellcolor{srcolor} \\textbf{SR} \\textdownarrow } \\\\   \n                      & \\textbf{L1}   & \\textbf{L2}   & \\textbf{L3}   & \\textbf{L4}   & \\textbf{L1}   & \\textbf{L2}   & \\textbf{L3}   & \\textbf{L4}   \\\\ \\midrule   \n\\textbf{Lota-Bench}    & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.02   & \\scriptsize 0.82  & \\scriptsize 0.58  & \\scriptsize 0.56  & \\scriptsize 0.66  \\\\ \\midrule   \n\\textbf{LLM-Planner}   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.48  & \\scriptsize 0.35  & \\scriptsize 0.32  & \\scriptsize 0.19  \\\\ \\midrule   \n\\textbf{Co-LLM-Agent}  & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.22  & \\scriptsize 0.08  & \\scriptsize 0.04  & \\scriptsize 0.04  \\\\ \\midrule   \n\\textbf{MLDT}          & \\scriptsize 0.03   & \\scriptsize 0.00   & \\scriptsize 0.02   & \\scriptsize 0.00   & \\scriptsize 0.70  & \\scriptsize 0.50  & \\scriptsize 0.40  & \\scriptsize 0.45  \\\\ \\midrule   \n\\textbf{ProgPrompt}    & \\scriptsize 0.03   & \\scriptsize 0.07   & \\scriptsize 0.13   & \\scriptsize 0.13   & \\scriptsize 0.75  & \\scriptsize 0.65  & \\scriptsize 0.55  & \\scriptsize 0.45  \\\\ \\midrule   \n\\textbf{ReAct}         & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.24  & \\scriptsize 0.18  & \\scriptsize 0.22  & \\scriptsize 0.17  \\\\ \\midrule   \n\\textbf{PCA-EVAL}      & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.00   & \\scriptsize 0.28  & \\scriptsize 0.11  & \\scriptsize 0.22  & \\scriptsize 0.17  \\\\ \\bottomrule  \n\\end{tabular}  \n}  \n\\vspace{1mm}  \n\\caption{Performance of embodied LLM agents empowered by DeepSeek in abstract hazard tasks. LLM agents tend to refuse tasks with more abstract description. In most occasions, all baselines do not reject these tasks and have a certain success rate.}  \n\\vspace{-3mm}  \n\\label{tab:abstract_deepseek}  \n\\end{table}\n\n\\subsection{Performances on Long-Horizon Tasks}\n\\label{sec:perf_long_horizon_tasks}\n\\textbf{Problem and objective.} Given a safety requirement and one long-term instruction containing a risky sub-task, baselines need to generate plans for the long-term task. The objective is to perfectly execute the whole task and satisfy the safety requirement.\n\n\\textbf{Evaluation metrics.} We use three metrics to assess the performance of embodied LLM agents: the completed-and-safe rate, the complete-but-unsafe rate, and the incomplete rate. The generated plans are evaluated by GPT-4.\n\n\\textbf{Experimental results.}\nThe performance of all baselines empowered by GPT-4 in long-horizon tasks is shown in Table \\ref{tab:detailed_task}. We can see that even with safety requirements provided, only two baselines ensure more than 70\\% of tasks are completed safely. Nearly half of the baselines have the imcomplete rate exceeding 40\\%, indicating that the planning capabilities and safety awareness of embodied LLM agents in long-horizon tasks are both weak and in urgent need of further research.\n\n\\subsection{Impacts of LLMs}\nThe results of three other open-source LLMs are presented in Tables \\ref{tab:detailed_task_llama3}, \\ref{tab:detailed_task_qwen}, and \\ref{tab:detailed_task_deepseek}. It can be observed that regardless of which LLM empowers them, the safety awareness of embodied agents remains precarious. These agents generally do not reject most hazardous tasks and are capable of completing these tasks with a certain success rate. Moreover, the performance ranking of different baselines is largely unaffected by the LLMs that drive them. For instance, the success rates of Lota-Bench, MLDT, and ProgPrompt are consistently the highest, while CoELA ranks the lowest due to its cumbersome design. Additionally, since these three open-source LLMs have varying capabilities and none match the performance of GPT-4, both task planning success rates and safety awareness are inferior to that of GPT-4. Overall, the performance ranking from highest to lowest is: GPT-4 > DeepSeek-V2.5 > Llama3-8B > Qwen2-7B.\n\nFor abstract tasks, the abstract level of task descriptions was discussed, and the results are shown in Tables \\ref{tab:abstract_llama3}, \\ref{tab:abstract_qwen}, and \\ref{tab:abstract_deepseek}. Regardless of the LLM, it is evident that when task descriptions are more abstract, the LLM is more likely to recognize the potential hazards and reject the task. However, due to the inherent weaknesses in the safety awareness and planning capabilities of these three LLMs, most baselines still fail to reject these tasks. Although the execution success rates of embodied agents are lower than those empowered by GPT-4, the safety risks remain significant.\n\nRegarding active defense, the performance of three open-source LLMs on thinksafe was tested, as shown in Figs \\ref{thinksafe_Llama3-8B}, \\ref{thinksafe_Qwen2-7B}, and \\ref{thinksafe_DeepSeek-V2.5}. It is evident that for all the LLMs tested, thinksafe increases the rejection rate of unsafe tasks, but simultaneously rejects more safe tasks, thereby lowering the execution success rate for safe tasks. This indicates that the effectiveness of thinksafe remains limited, and further exploration is required to improve the proactive safety awareness of embodied agents.\n\n\\subsection{User Study for GPT-4 Evaluation}\n\\label{sec:user_study}\nTo verify the accuracy of GPT-4 evaluation across the three task types, we designed a user study. The study included a total of 1008 human ratings. To ensure diversity, we selected data from each baseline and formed the final questionnaire in a 3:2:2 ratio across the three task types. Results show that the consistency between human and GPT-4 evaluation for each of the three tasks is 91.89\\%, 90.36\\%, and 90.70\\%, respectively, demonstrating the high reliability of GPT-4 evaluation.\n\n\\section{Conclusions}\n\nThe proposed SafeAgentBench is a compact, yet comprehensive benchmark for safety-aware task planning of embodied LLM agents. Results show that all existing agents with the most powerful LLM still fail to reject hazardous tasks. Even with ThinkSafe strategy, \nalthough agents start to reject hazardous tasks, they cannot execute safe tasks any more. These results indicate significant safety risks of embodied agents and call for more research on this topic.\n\n\\clearpage\n\\clearpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2402.08178v1.tex",
        "arXiv-2403.18760v2.tex",
        "arXiv-2412.13178v2.tex"
    ],
    "group_id": "group_13",
    "response": "### Title: Evaluating Task Planning Capabilities of Embodied Agents with Large Language Models (LLMs)\n\n### Introduction\n\nThe integration of large language models (LLMs) into the field of artificial intelligence (AI) and robotics has opened up new avenues for task planning, particularly for embodied agents designed to perform household tasks. Embodied agents are AI systems that can perceive and interact with their environment, making decisions based on natural language instructions. These agents have the potential to understand and execute complex tasks, but their performance is still under scrutiny. The evaluation of these agents' task planning capabilities is crucial for their effective deployment in real-world scenarios, especially in home services, navigation, and manipulation tasks. However, the evaluation frameworks for LLM-based task planning remain underdeveloped, with most existing studies relying on human evaluations that are time-consuming and expensive. Moreover, the evaluation of these agents in complex long-horizon tasks and their safety awareness when executing hazardous tasks are still nascent areas of research.\n\nThe field has seen significant advancements with the emergence of LLMs, which have demonstrated remarkable generalization capabilities through zero-shot or few-shot prompting. These models, pre-trained on extensive corpora, seem to have semantic knowledge about the world, making them promising tools for high-level task planning. However, the evaluation of LLM-based task planners is still in its infancy, with existing benchmarks focusing primarily on planning performance without considering the safety implications of the tasks. This gap in research highlights the need for more comprehensive and standardized evaluation frameworks that can automatically assess the performance of LLM-based task planners in various scenarios, including complex long-horizon tasks and hazardous situations.\n\nThis summary will explore three recent papers that address these challenges by proposing new benchmarks and methodologies for evaluating the task planning capabilities of embodied agents using LLMs. The first paper introduces LoTa-Bench, a benchmark system for automatically quantifying the performance of task planning for home-service embodied agents. The second paper proposes MLDT, a multi-level decomposition method for complex long-horizon robotic task planning with open-source LLMs. The third paper presents SafeAgentBench, a benchmark for assessing the safety awareness of embodied LLM agents in task planning.\n\n### Main Content of Each Paper\n\n#### LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents\n\nLoTa-Bench, proposed by Choi et al., aims to provide a standardized and automated evaluation framework for LLM-based task planners in home-service scenarios. The benchmark system consists of a baseline task planner, datasets, and simulators. The baseline planner uses in-context learning, where a prompt is constructed using a prefix, in-context examples, and a user-provided natural language instruction. The planner then selects the skill with the highest probability to complete the task, using a greedy search strategy. The system evaluates the planner's performance on two pairs of datasets and simulators: ALFRED with AI2-THOR and an extension of Watch-And-Help (WAH) with VirtualHome. ALFRED and WAH-NL datasets include natural language instructions, initial environment states, and goal conditions for home environments. The ALFRED dataset contains seven task types, while WAH-NL includes five task types, all requiring the agent to find and place multiple objects.\n\nThe authors conducted extensive experiments to explore the influence of various pre-trained models and their sizes, the number and selection strategies of in-context examples, and the effectiveness of fine-tuning. They found that the performance of task planners improved with larger model sizes, but this was not always the case. For example, GPT-J 6B outperformed GPT-3 175B in the WAH-NL experiment. The authors also observed that the success rate increased with the number of in-context examples, but this trend was not consistent across all models. Additionally, they found that fine-tuning models did not necessarily improve performance over base models, and that the performance of task planners varied across different task types and datasets.\n\n#### MLDT: Multi-Level Decomposition for Complex Long-Horizon Robotic Task Planning with Open-Source Large Language Model\n\nWu et al. introduced MLDT, a method that decomposes complex long-horizon tasks into goal-level, task-level, and action-level steps. This approach is designed to enhance the planning capabilities of open-source LLMs, which typically struggle with long-context tasks due to their limited reasoning abilities. The authors constructed a more challenging dataset, LongTasks, to specifically evaluate planning ability on complex long-horizon tasks. LongTasks includes tasks with more goals, more interacting objects, and longer action sequences compared to existing datasets.\n\nMLDT leverages ChatGPT to generate a high-quality training corpus through a goal-sensitive corpus generation method. This corpus is then used for instruction tuning to enhance the planning abilities of LLMs. The authors evaluated their method using various LLMs on four datasets in VirtualHome, demonstrating significant performance enhancements in robotic task planning. They found that MLDT outperformed existing methods in terms of success rate and executability, especially for complex long-horizon tasks. However, the authors also observed that the method's effectiveness varied across different LLMs, with open-source LLMs showing lower performance compared to closed-source LLMs like GPT-3.5 and GPT-4. The authors concluded that their method is adaptable to tasks of varying complexities and can enhance the practicality of less powerful LLMs in task planning.\n\n#### SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents\n\nSafeAgentBench, introduced by Sheng et al., is a new benchmark for assessing the safety awareness of embodied LLM agents in task planning. The benchmark includes a dataset with 750 tasks, covering 10 potential hazards and three task types: detailed tasks, abstract tasks, and long-horizon tasks. The dataset is designed to test the agents' ability to recognize and reject hazardous tasks while maintaining high performance in safe tasks. The authors also developed SafeAgentEnv, an embodied environment that supports multiple agents and a wide range of high-level actions.\n\nThe authors conducted experiments to evaluate the performance of eight task-planning-related works as baselines. They found that the baselines showed little to no proactive defense against hazardous tasks, with most agents failing to reject hazardous tasks and maintaining a certain success rate in executing them. The authors introduced a portable module called ThinkSafe to assess the safety of each high-level step before execution. ThinkSafe increased the rejection rate for hazardous tasks but also led to higher rejection rates for safe tasks, indicating the need for further refinement in safety assessment strategies.\n\n### Commonalities and Innovations\n\nAll three papers address the challenges of evaluating LLM-based task planners for embodied agents. They introduce new benchmarks and methodologies to assess the performance of these planners in different scenarios. LoTa-Bench and MLDT focus on the planning capabilities of agents in home-service and complex long-horizon tasks, respectively, while SafeAgentBench evaluates the safety awareness of agents when executing potentially hazardous tasks.\n\nLoTa-Bench innovates by providing a standardized and automated evaluation framework for LLM-based task planners, enabling easier, fair, and reproducible comparisons. MLDT introduces a multi-level decomposition method to break down complex long-horizon tasks into manageable steps, enhancing the planning capabilities of open-source LLMs. SafeAgentBench is the first benchmark to focus on the safety awareness of embodied agents, highlighting the importance of ensuring safe execution of tasks in real-world scenarios.\n\n### Comparison of Results\n\nThe results of the three papers highlight different aspects of LLM-based task planning. LoTa-Bench showed that the performance of task planners improved with larger model sizes, but the success rate was not always consistent across different datasets and task types. The authors also found that the number of in-context examples and the selection strategies significantly influenced the performance of the planners.\n\nMLDT demonstrated significant performance enhancements in complex long-horizon tasks, with the method outperforming existing baselines in terms of success rate and executability. However, the authors observed that the effectiveness of MLDT varied across different LLMs, with open-source LLMs showing lower performance compared to closed-source LLMs. This suggests that while MLDT can enhance the capabilities of less powerful LLMs, there is still a gap in performance between open-source and closed-source models.\n\nSafeAgentBench revealed that existing baselines showed little to no proactive defense against hazardous tasks, with most agents failing to reject hazardous tasks and maintaining a certain success rate in executing them. The authors also found that the ThinkSafe module increased the rejection rate for hazardous tasks but also led to higher rejection rates for safe tasks, indicating the need for further refinement in safety assessment strategies. This highlights the significant safety risks associated with embodied agents and the need for more research in this area.\n\n### Conclusion\n\nThe three papers collectively advance the field of LLM-based task planning for embodied agents by introducing new benchmarks and methodologies. LoTa-Bench provides a standardized and automated evaluation framework, enabling easier comparisons between different task planners. MLDT enhances the planning capabilities of open-source LLMs through multi-level decomposition, making complex long-horizon tasks more manageable. SafeAgentBench is the first benchmark to focus on the safety awareness of embodied agents, highlighting the need for ensuring safe execution of tasks in real-world scenarios.\n\nThe results indicate that while LLMs show promise in task planning, there are still significant challenges in their performance, especially in complex long-horizon tasks and hazardous situations. Future research should focus on developing more robust and safe task planning methods for embodied agents, as well as on improving the performance of open-source LLMs in these tasks. Additionally, there is a need for further refinement in safety assessment strategies to ensure that embodied agents can recognize and reject hazardous tasks without compromising their performance in safe tasks."
}