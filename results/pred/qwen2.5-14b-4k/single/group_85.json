{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion}\n\n\\begin{document}\n\n\\title{DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion}\n\n\\author{Johanna Karras$^1$, Aleksander Holynski$^{2,3}$, Ting-Chun Wang$^4$, Ira Kemelmacher-Shlizerman$^1$\\\\ \\\\\n$^1$University of Washington, $^2$UC Berkeley, $^3$Google Research, $^4$NVIDIA\\\\\n}\n\n\\twocolumn[{%\n\\renewcommand\\twocolumn[1][]{#1}%\n\\maketitle\n\\vspace{-3em}\n\\begin{center}%\n    \\captionsetup{type=figure}%\n    \\includegraphics[width=1.0\\linewidth]{Figures/Teaser.png}%\n    \\vspace{1em}\n    \\captionof{figure}{Given an image of a person and a sequence of body poses, DreamPose synthesizes a photorealistic video.% of the person following the motion sequence. %Source: UBC Fashion Dataset \\cite{ubc_fashion}\n     }%\n    \\label{fig:teaser}%\n\\end{center}%\n\\vspace{1em}\n    }]\n\\maketitle\n\n\\begin{figure*}[h!]\n      \\begin{center}\n         \\includegraphics[width=0.9\\linewidth]{Figures/Architecture2.png}\n            \\caption{ Architecture Overview. We modify the original Stable Diffusion architecture in order to enable image and pose conditioning. First, we replace the CLIP text encoder with a dual CLIP-VAE image encoder and adapter module (shown in the blue box). The adapter module jointly models and reshapes the pretrained CLIP and VAE input image embeddings. Then, we concatenate the target pose representation, consisting of 5 consecutive poses surrounding the target pose, to the input noise. During training, we fine-tune the denoising UNet and our Adapter module on the full dataset and further perform subject-specific finetuning of the UNet, Adapter, and VAE decoder on a single input image.  }\n        \\label{fig:architecture}\n        \\end{center}\n    \\end{figure*}\n    \\nopagebreak\n    \n\\vspace{-1em}\n\\begin{abstract}\n\\vspace{-1em}\nWe present DreamPose, a diffusion-based method for generating animated fashion videos from still images. Given an image and a sequence of human body poses, our method synthesizes a video containing both human and fabric motion. To achieve this, we transform a pretrained text-to-image model (Stable Diffusion \\cite{stable_diffusion}) into a pose-and-image guided video synthesis model, using a novel finetuning strategy, a set of architectural changes to support the added conditioning signals, and techniques to encourage temporal consistency. We fine-tune on a collection of fashion videos from the UBC Fashion dataset \\cite{ubc_fashion}. We evaluate our method on a variety of clothing styles and poses, and demonstrate that our method produces state-of-the-art results on fashion video animation. Video results are available on our \\href{https://grail.cs.washington.edu/projects/dreampose/}{project page}: https://grail.cs.washington.edu/projects/dreampose\n\\end{abstract}\n\n\\vspace{-2em}\n\\section{Introduction}\n Fashion photography is incredibly prevalent online, from social media platforms to online retail sites. Unfortunately, these still photographs are limited in the information they convey, and fail to capture many of the crucial nuances of a garment, such as how it drapes and flows when worn. Fashion \\emph{videos}, on the other hand, do showcase all these details, and for this reason are highly informative for consumer decision-making. Despite this benefit, however, these videos are a relatively rare commodity.\n\nIn this paper, we introduce DreamPose, a method that turns fashion photographs into realistic, animated videos, using a driving pose sequence. Our method is a diffusion video synthesis model based upon Stable Diffusion~\\cite{stable_diffusion}. Given one or more images of a human and a pose sequence, DreamPose generates a high-quality video of the input subject following the pose sequence (Figure~\\ref{fig:teaser}).\n \nThis is a challenging task in several ways. While image diffusion models have shown impressive, high-quality results \\cite{stable_diffusion,dalle2,imagen}, video diffusion models have yet to achieve the same quality of results and are often limited to ``textural\" motion or cartoon-like appearance \\cite{flexible-diffusion-modeling,imagen_video,video-diffusion-models,make-a-video,ddpm_video}. Moreover, existing video diffusion models suffer from poor temporal consistency, motion jitter, lack of realism, and the inability to control the motion or detailed object appearance in the target video. This is partly because existing models are primarily conditioned on text, as opposed to other conditioning signals (e.g., motion) which may offer more fine-grained control. In contrast, our image-and-pose conditioning scheme allows for greater appearance fidelity and frame-to-frame consistency. \n\nOur model is fine-tuned from an existing pretrained image diffusion model, which already effectively models the distribution of natural images. When using such a model, the task of image animation can effectively be simplified to \nfinding the subspace of natural images consistent with the conditioning signals. To accomplish this, we redesign the encoder and conditioning mechanisms of the Stable Diffusion~\\cite{stable_diffusion} architecture, in order to enable aligned-image and unaligned-pose conditioning. Further, we propose a two-stage finetuning scheme that consists of finetuning both UNet and VAE from one or more input images.\n\nTo summarize, our contributions include: (1) \\emph{DreamPose}: an image-and-pose conditioned diffusion method for still fashion image animation that achieves photorealistic results on a diverse range of garment patterns and shapes, (2) a simple, yet effective, pose conditioning approach that greatly improves temporal consistency across frames, (3) a split CLIP-VAE encoder that increases the output fidelity to the conditioning image, (4)\na finetuning strategy that effectively balances image fidelity and generalization to new poses. %can be used for as little as a single exemplar image, achiecing  can be fine-tuned on even just a single image and that by including the VAE decoder in the subject-specific finetuning phase, we can significantly improve photorealism and face identity.\n        \n\n    \\begin{figure*}[h!]\n      \\begin{center}\n         \\includegraphics[width=1.0\\linewidth]{Figures/Qualitative_Results.png}\n      \\end{center}\n       \\caption{ Qualitative Results. We showcase the results of our method on a variety of input frames and poses. DreamPose is capable of synthesizing photorealistic video frames consistent with a diverse range of patterns, fabric types, person identities, clothing shapes, and viewpoints. }\n    \\label{fig:qualitative-results}\n    \\end{figure*}\n    \\nopagebreak\n    \n\\vspace{-0.5em}\n\\section{Related Work}\n    \\subsection{Diffusion models}\n        Diffusion models have recently demonstrated impressive results in text-conditioned image synthesis \\cite{stable_diffusion,dalle2,imagen}, video synthesis \\cite{imagen_video,video-diffusion-models,dreamix}, and 3D generation tasks \\cite{dream_fusion,make-a-video-3d}. However, training these models from scratch is computationally expensive and data intensive.\n        Latent Diffusion Models (as in Stable Diffusion~\\cite{stable_diffusion}) perform diffusion and denoising in the latent space, thereby drastically reducing the computational requirements and training time with only marginal reductions to quality.\n        Since its release, Stable Diffusion and its pretrained checkpoints have been used by many for various image generation tasks \\cite{universal_guidance,instruct_pix2pix,dreambooth}. Like these methods, our work leverages a pretrained Stable Diffusion model with subject-specific finetuning. %However, to the best of our knowledge, we are the first to adapt a pretrained text-to-image Stable Diffusion model with pose and image guidance for still image animation.\n            \n    \\subsection{Still Image Animation}\n        Still image animation refers to the task of generating a video from one or more input images. Existing, non-diffusion approaches often consist of multiple separate networks, such as for predicting the background \\cite{articulated_animation,photo_wake-up,thin_plate_spline}, motion representation \\cite{eulerian_motion,monkey-net,fomm,articulated_animation,latent_image_animator,thin_plate_spline}, occlusion maps \\cite{fomm,articulated_animation,photo_wake-up,thin_plate_spline}, or depth maps \\cite{dain}. %Many earlier works follow the paradigm of: (1) encoding the inputs, (2) estimating the motion representation (such as optical flow) from input to target frame, and (3) warping and blending the input frame features to synthesize the target frame \\cite{many_many_splatting,TTVFI,softmax_splatting,bilateral_motion_estimation,fomm,articulated_animation,thin_plate_spline}. \n        However, multiple networks require separate training for each stage and potentially unavailable or imperfect ground-truth intermediate data, such as motion or depth. Especially with large and complex motion, these ground-truth estimates are harder to derive and more error-prone. Several more recent papers explore end-to-end single-network approaches, such as by merging optical flow and warping ~\\cite{film}, replacing motion estimation networks entirely with a cross-attention modules \\cite{implicit_warping}, or generating animatable 3D humans using a NeRF representation ~\\cite{eva3d}. %While there is a robust body of work in human image animation, there are still few methods that tackle this problem using diffusion models. %Now, with the success of diffusion models for image synthesis, diffusion models have become a desirable candidate method for video synthesis.\n\n    \\subsection{Fashion Image Synthesis}\n        Many prior pose-guided fashion image synthesis methods are generative adversarial network (GAN)-based and rely on optical flow to align image features to pose \\cite{pose_with_style,DiOr,try_on_gan,global_flow_local_attn,pise,progressive_pose_transfer}. %While these works have focused on pose transfer and virtual try-on, where the goal is to either change the input subject's pose or garment based on an external pose or image, they can be used sequentially to generate a video. \n        However, GAN-based approaches often struggle with large pose changes, synthesizing occluded regions, and preserving garment style. More recent approaches rely on attention-based mechanisms, where self- and cross-attention are used to warp image features to the target frame \\cite{viton_hd,DynaST,global_flow_local_attn}. \n\n        Relatively few works exist for diffusion-based fashion image and video synthesis. DiffFashion~\\cite{DiffFashion} aims to edit a clothing item by transferring the style of a reference image. Concurrent work PIDM~\\cite{pose-transfer-dm} generates pose-conditioned human images, but is designed for single-image pose transfer, and therefore not optimized for temporal consistency. Our approach uses a unique finetuning and multi-pose input representation to enhance temporal smoothness. Moreover, by leveraging pretrained Stable Diffusion \\cite{stable_diffusion}, our method can be fine-tuned in 2 days with 2 A100 GPU's using a small dataset of 323 videos. In contrast, PIDM \\cite{pose-transfer-dm} is\n        trained from scratch on 101,966 Deep Fashion \\cite{deep-fashion} image pairs with 4 A100s for 26 days. %We compare DreamPose and PIDM in Figure ~\\ref{fig:pose-transfer}.\n\n    \n    \\subsection{Diffusion Models for Video Synthesis}\n        Many text-to-video diffusion models rely on adapting text-to-image diffusion models for video synthesis \\cite{flexible-diffusion-modeling,latent_vdm,imagen_video,video-diffusion-models,make-a-video,ddpm_video}. While the results are promising, these methods still struggle to match the realism that text-to-image models do. Quality is largely hindered due to the new challenges introduced by video synthesis, such as maintaining temporal consistency across frames and generating realistic motion. Some video diffusion methods are instead trained from scratch, requiring expensive computational resources, huge training datasets, and extensive training time~\\cite{imagen_video,video-diffusion-models,dream_fusion,dalle2,dreamix, latent_vdm}. %Some of these issues are alleviated by 3D approaches, though ground truth data is scarce \\cite{dream_fusion,make-a-video-3d,3dim}. %Latent video diffusion models~\\cite{latent_vdm}  a 3D latent diffusion model for video synthesis, but is still trained from scratch. %On the other hand, our novel approach adapts a pretrained text-to-image Stable Diffusion model for video synthesis of humans, while also giving the user control of the resulting motion and appearance. \n        Concurrently, Tune-A-Video fine-tunes a text-to-image pretrained diffusion model for text-and-image conditioned video generation \\cite{tune-a-video}. However, like earlier video diffusion methods, Tune-A-Video's results exhibit textural flickering and structural inconsistencies. Our work aims to address these issues in order to synthesize realistic human and fabric motion.\n\n    \\subsection{Conditioning Mechanisms for Diffusion Models}\n        Text-conditioning is popular among image diffusion models \\cite{stable_diffusion,hierarchical_text_conditional,photorealistic_text2img_diffusion}. While effective at controlling high-level details, text conditioning fails to provide rich, detailed information about the \\textit{exact} identity or pose of a person and garment. \n        \n        Several works tackle the challenge of image conditioning for a pretrained text-to-image Stable Diffusion model~\\cite{universal_guidance,instruct_pix2pix,encoder_personalization,dreamix,dreambooth,3dim}. % These often incorporate text embeddings of some kind. \n        DreamBooth, the first method to perform subject-specific finetuning of Stable Diffusion on a set of images, learns a unique text token to represent the subject in the text encoder~\\cite{dreambooth}. Others incorporate text to edit the appearance of existing images~\\cite{instruct_pix2pix} and videos~\\cite{tune-a-video,dreamix}. PIDM~\\cite{pose-transfer-dm} encodes image textures using a separate textural encoder and concatenates target pose with an input noisy image.\n        DreamPose allows the user to not only control the appearance of subjects in video, but also the structure and motion. Similar to PIDM, our image conditioning approach directly incorporates image embeddings in the cross-attention layers of the UNet, but these image embeddings come from a mixture of two pretrained encoders: CLIP and VAE. Moreover, with our method, we achieve smooth, temporally consistent motion using a multi-pose input representation concatenated to the input noise.\n    \n\\section{Background}\n    \\textit{Diffusion models} are a recent class of generative models that have surpassed GANs at synthesis tasks in terms of quality, diversity, and training stability \\cite{dms_beat_gans}. A standard image diffusion model learns to iteratively recover an image from normally distributed random noise \\cite{diffusion_models}. A \\textit{latent} diffusion model, e.g., Stable Diffusion~\\cite{stable_diffusion}, operates in the encoded latent space of an autoencoder, thereby saving computational complexity, while sacrificing minimal perceptual quality.\n    Stable Diffusion is composed of two models: a variational autoencoder and a denoising UNet. The autoencoder consists of an encoder $\\mathcal{E}$ that distills a frame $x$ into a compact latent representation, $z = \\mathcal{E}(x)$, and a decoder $\\mathcal{D}$ that reconstructs the image from its latent representation, $x' = \\mathcal{D}(z)$. \n    During training, the latent features $z$ are diffused in $T$ timesteps by a deterministic Gaussian process to produce noisy features $\\tilde{z}_T$, indistinguishable from random noise. In order to recover the original image, a time-conditioned UNet is trained to iteratively predict the noise of the latent features corresponding to each timestep $t \\in \\{ 1, ..., T\\}$. The UNet $\\epsilon_{\\theta}$ objective function is:\n\n        \\begin{equation}\n            L_{DM} = \\mathbb{E}_{z, \\epsilon \\in  \\mathcal{N}(0,1)} [ || \\epsilon - \\epsilon_{\\theta}(\\tilde{z}_t, t, c)||^2_2 ]\n        \\end{equation}\n\n    \\noindent where $c$ represents the embeddings of conditional information, such as text, image, segmentation mask, etc. In the case of text-to-image Stable Diffusion, $c$ is obtained using a CLIP text encoder \\cite{clip}.\n    Finally, the predicted denoised latents $z'$ are decoded to recover the predicted image $x' = \\mathcal{D}(x')$.\n\n    \\textit{Classifier-free guidance} is a mechanism in sampling that pushes the distribution of predicted noise towards the conditional distribution via an implicit classifier~\\cite{classifier_free_guidance}. This is practically achieved by dropout, a training scheme that, with a random probability, replaces real conditioning inputs with null inputs ($\\varnothing$). During inference, the conditional prediction is used to guide the unconditional prediction towards the conditional, using a guidance scalar weight $s$:\n\n        \\begin{equation}\n            \\epsilon_{\\theta} = \\epsilon_{\\theta}(\\tilde{z_t}, t, \\varnothing) + s \\cdot (\\epsilon_{\\theta}(\\tilde{z_t}, t, c) - \\epsilon_{\\theta}(\\tilde{z_t}, t, \\varnothing))\n        \\end{equation}\n        \\label{eq:cfg}\n    \n\\vspace{-2em}\n\\section{Method}\n    Our method aims to produce photorealistic animated videos from a single image and a pose sequence. To achieve this, we fine-tune a pretrained Stable Diffusion model on a collection of fashion videos. This involves adapting the architecture of Stable Diffusion (which is a text-to-image model) to accept additional conditioning signals (image and pose), and to output temporally consistent content that can be viewed as a video.  \n\n    In the coming section, we begin by  describing the architectural modifications in Section~\\ref{sec:arch}. Then, we describe the two-stage finetuning strategy in Section~\\ref{sec:ft}. Finally, in Section~\\ref{sec:cfg}, we describe the inference process of generating an animated video from a still image, which involves a novel formulation of classifier-free guidance.\n    \n\n    \\subsection{Overview}\n        Given input image $x_0$ and poses $\\{p_1, ..., p_N\\}$, our method generates a video $\\{x_1', ..., x_N'\\}$, where $x_i'$ is the $i$-th predicted frame corresponding to input pose $p_i$. Our method relies on a pretrained latent diffusion model~\\cite{stable_diffusion}, which is conditioned on an input image and a sequence of poses. At inference time, we generate each frame independently through a standard diffusion sampling procedure: starting with uniformly distributed Gaussian noise, the diffusion model is repeatedly queried with both conditioning signals to gradually denoise the noisy latent to a plausible estimate. Finally, the predicted denoised latent $z'_i$ is decoded to produce the predicted video frame $x'_i = \\mathcal{D}(z'_i)$.\n        \n    \\subsection{Architecture}\n    \\label{sec:arch}\n        The DreamPose model is a pose- and image-conditioned image generation model that modifies and fine-tunes the original text-to-image Stable Diffusion model for the purpose of image animation. The objectives of image animation include: (1) faithfulness to the provided input image, (2) visual quality, and (3) temporal stability across generated frames. As such, DreamPose requires an image conditioning mechanism that captures the global structure, person identity, and fine-grained details of the garment, as well as a method to effectively condition the output image on target pose while also enabling temporal consistency between independently sampled output frames. We describe our approach to achieving these goals in the sections below. A diagram of our architecture can be found in Figure~\\ref{fig:architecture}. Full implementation details are provided in the supplementary material.\n\n        \\begin{figure}[h!]\n            \\begin{center}\n                \\includegraphics[width=\\linewidth]{Figures/Simple_Architecture.png}\n            \\end{center}\n           \\caption{Two-Phase Finetuning Scheme. In the first phase, our method fine-tunes the modified Stable Diffusion model on the full dataset. In the second phase, the model is further fine-tuned on a single subject image.}\n        \\label{fig:finetuning}\n        \\end{figure}\n        \n        \\subsubsection{Split CLIP-VAE Encoder} In many prior works, such as InstructPix2Pix~\\cite{instruct_pix2pix}, image conditioning signals are often concatenated with the input noise to the denoising U-Net. While this is effective for conditioning signals that are spatially aligned with the desired output image, in our case, our network aims specifically to produce images which are \\emph{not} spatially aligned with the input image. As such, we explore alternative approaches for image conditioning. In particular, we implement image conditioning by replacing the CLIP text encoder with a custom conditioning adapter that combines the encoded information from pretrained CLIP image and VAE encoders.% consisting of a pretrained CLIP image encoder, pretrained VAE encoder, and adapter module.\n\n        A crucial objective when finetuning from a pretrained network is to make training gradients as meaningful as possible by making the input signals as similar as possible to those used in the original network training. This helps avoid regressions in network performance during finetuning, or loss of learned priors, which can come from noisy gradients (e.g., if the network does not know how to parse new forms of input signals). For this reason, most diffusion-based finetuning schemes~\\cite{instruct_pix2pix,zhang2023adding} will retain all original conditioning signals, and will initialize network weights that interact with new (previously unseen) conditioning signals to zero.  \n        \n        For our purposes, given that Stable Diffusion is conditioned on CLIP embeddings of text prompts, and CLIP encodes both text and images to a shared embedding space, it may seem natural to simply replace the CLIP conditioning with the embedding derived from the conditioning image. While this would in theory pose a very small change to the original architecture and allow for image conditioning with minimal finetuning, we find that in practice that CLIP image embeddings alone are insufficient for capturing fine-grained details in the conditioning image. So, we instead additionally input the encoded latent embeddings from Stable Diffusion's VAE. Adding these latent embeddings as conditioning has the added benefit of coinciding with the output domain of the diffusion model.\n\n        Since the architecture does not support VAE latents as a conditioning signal by default, we add an adapter module $\\mathcal{A}$ that combines the CLIP and VAE embeddings to produce one embedding that is used in the network's usual cross-attention operations. This adapter blends both the signals together and transforms the output into the typical shape expected by the cross-attention modules of the denoising U-Net. Initially, the weights corresponding to the VAE embeddings are set to zero, such that the network begins training with only the CLIP embeddings (as mentioned before, to mitigate network ``shock'' in training). We define the final image conditioning signal $c_I$ as:\n\n            \\begin{equation}\n                c_I = \\mathcal{A}(c_{\\text{ CLIP}}, c_{\\text{ VAE}})\n            \\end{equation}\n\n        \\def\\Plus{\\texttt{+}}\n        \\subsubsection{Modified UNet} Unlike the image conditioning, the pose conditioning \\emph{is} image-aligned. As such, we concatenate the noisy latents $\\tilde{z}_i$ with a target pose representation $c_p$. To account for noise in the poses (which are estimated from real videos using an off-the-shelf network~\\cite{densepose}) and to maximize temporal consistency in the generated frames, we set $c_p$ to consist of five consecutive pose frames: $c_p = \\{p_{i\\mhyphen2}, p_{i\\mhyphen1}, p_i, p_{i\\Plus1}, p_{i\\Plus2}\\}$. We observe that individual poses are prone to frame-to-frame jitter, but training the network with a set of consecutive poses increases the overall motion smoothness and temporal consistency. Architecturally, we modify the UNet input layer to take in 10 extra input channels, initialized to zero, while the original channels corresponding to the noisy latents are unmodified from the pretrained weights. \n        \n    \\subsection{Finetuning}    \n    \\label{sec:ft}\n        For initialization, the unmodified Stable Diffusion layers are initialized from a pretrained text-to-image Stable Diffusion checkpoint, except for the CLIP image encoder which is loaded from a separate pretrained checkpoint \\cite{stable_diffusion,clip}. As mentioned previously, the novel layers are initialized such that initially the new conditioning signals do not contribute to the network output.\n        \n        Following initialization, DreamPose is fine-tuned in two stages (shown in Figure~\\ref{fig:finetuning}). The first phase fine-tunes the UNet and adapter module on the full training dataset in order to synthesize frames consistent with an input image and pose. The second phase refines the base model by finetuning the UNet and adapter module, then the VAE decoder, on one or more subject-specific input image(s) to create a subject-specific custom model used for inference. \n        \n        Similar to other image-conditional diffusion methods \\cite{dreambooth,dream_fusion,dreamix}, we find that sample-specific finetuning is essential to preserving the identity of the input image's person and garment, as well as maintaining a consistent appearance across frames. However, simply training on a single frame and pose pair quickly leads to artifacts in the output videos, such as texture-sticking. To prevent this, we augment the image-and-pose pair at each step, such as by adding random cropping.\n        \n        We also find that finetuning the VAE decoder is crucial for recovering sharper, more photorealistic details in the synthesized output frames. %Plus, this additional step mitigates high-frequency noise in the output video. \n        Refer to Figure~\\ref{fig:vae-finetuning} and the supplementary videos for an ablated comparison. Furthermore, we show in Figure~\\ref{fig:dual-cfg} that even single image finetuning of the decoder allows increased pose guidance, without sacrificing the person identity or appearance.\n\n        \\begin{figure}[h!]\n            \\begin{center}\n                \\includegraphics[width=1.0\\linewidth]{Figures/With_and_Without_VAE_finetuning.png}\n            \\end{center}\n           \\caption{Ablation of VAE Finetuning. We find that finetuning the VAE decoder, in addition to the UNet, during the subject-specific finetuning phase yields more photorealistic details and reduces high-frequency noise, compared to finetuning the UNet alone.}\n        \\label{fig:vae-finetuning}\n        \\end{figure}\n\n        \\begin{figure*}\n            \\begin{center}\n                \\includegraphics[width=0.8\\linewidth]{Figures/Qualitative_Comparison_Tall.png}\n            \\end{center}\n           \\caption{Qualitative comparisons of our method versus MRAA~\\cite{articulated_animation}, TPSMM~\\cite{thin_plate_spline}, and PIDM~\\cite{pose-transfer-dm}. Our method produces more photorealistic details aligned with the input frame, such as garment folds, fine-grain patterns, and face identity. Our method can also better handle challenging cases, including pattern synthesis in occluded regions.}\n           \\vspace{1em}\n        \\label{fig:qualitative}\n        \\end{figure*}\n        \n    \\subsection{Pose and Image Classifier-Free Guidance}\n    \\label{sec:cfg}\n        At inference time, we generate a video frame-by-frame from a single input image and a sequence of poses using the subject-specific model. We modulate the strength of image conditioning $c_I$ and pose conditioning $c_p$ during inference using dual classifier-free guidance \\cite{instruct_pix2pix}. The dual classifier-free guidance equation is modified from Equation~\\ref{eq:cfg} to be controlled by two guidance weights, $s_I$ and $s_p$, which rule how similar the output image is to the input image $c_I$ and input pose $c_p$, respectively:\n        \\begin{equation}\n        \\begin{aligned}\n            \\epsilon_{\\theta}(z_t, c_I, c_p) & = \\epsilon_{\\theta}(z_t, \\varnothing, \\varnothing) \\\\\n                            & + s_I (\\epsilon_{\\theta}(z_t, c_I, \\varnothing) - \\epsilon_{\\theta}(z_t, \\varnothing, \\varnothing)) \\\\\n                            & + s_p (\\epsilon_{\\theta}(z_t, c_I, c_p) - \\epsilon_{\\theta}(z_t, c_I, \\varnothing))\n        \\end{aligned}\n        \\label{eq:cfg-equation}\n        \\end{equation}\n        \n        In Figure~\\ref{fig:dual-cfg}, we show the effect of varying the classifier free guidance weights ($s_I, s_p$). A large $s_I$ ensures high appearance fidelity to the input image, while a large $s_p$ ensures alignment to the input pose. In addition to strengthening our pose and image guidance, the decoupled classifier-free guidance prevents overfitting to the one input pose after subject-specific finetuning.\n\n    \\begin{figure}[t]\n            \\begin{center}\n                \\includegraphics[width=\\linewidth]{Figures/Dual-CFG.png}\n            \\end{center}\n           \\caption{ a) Pose and Image Classifier-Free Guidance. We demonstrate the effect of the relative weight between image and pose guidance weights, $s_I$ and $s_P$. Results shown are after subject-specific finetuning. b) VAE decoder finetuning improves the appearance and person identity, even with larger relative pose guidance. }\n        \\label{fig:dual-cfg}\n        \\end{figure}\n\n    \\subsection{Dataset}\n\n    We train and test our method on the UBC Fashion dataset \\cite{ubc_fashion}. We follow the provided train/test split of 339 training and 100 test videos. Each video has a frame rate of 30 frames/second and is approximately 12 seconds long. During training, we randomly sample pairs of frames from the training videos. We compute poses with DensePose \\cite{densepose}.\n        \n\n        \\begin{table*}[h!]\n        \\begin{center}\n        \\begin{tabular}{c c c c c c c c c}\n        \\hline\n        & L1 $\\downarrow$ & SSIM $\\uparrow$ & VGG  $\\downarrow$ & LPIPS  $\\downarrow$ & FID $\\downarrow$ & FVD (16f) $\\downarrow$ & AED $\\downarrow$ \\\\\n        \\hline\\hline\n        MRAA \\cite{articulated_animation} &  0.0857 &  0.749 & 0.534 & 0.212 & 23.42 & 253.65 & 0.0139  \\\\\n        TPSMM \\cite{thin_plate_spline}  &  0.0858 & 0.746 & 0.547 & 0.213 & 22.87 & 247.55 & 0.0137 \\\\\n        PIDM \\cite{pose-transfer-dm} & 0.1098 & 0.713 & 0.629 & 0.288 & 30.279 & 1197.39 & 0.0155 \\\\\n        Ours & \\textbf{0.0256} & \\textbf{0.885} & \\textbf{0.235} & \\textbf{0.068} & \\textbf{13.04} & \\textbf{238.75} & \\textbf{0.0110}  \\\\\n        \\hline\n        \\end{tabular}\n        \\end{center}\n        \\caption{Quantitative comparisons of our method with MRAA, TPSMM, PIDM, and our method. Bolded values indicate best scores in each column. %Values are computed based on 50 frames from each of the 100 test videos in the UBC Fashion dataset~\\cite{ubc_fashion}. \n        }\n        \\label{quantitative-comparison}\n        \\end{table*}\n\\section{Results}\n    DreamPose is capable of generating state-of-the-art fashion videos from still images. In Figure~\\ref{fig:qualitative-results}, we showcase frames synthesized by DreamPose from a variety of input images and poses from the UBC Fashion dataset~\\cite{ubc_fashion}. DreamPose handles diverse human and clothing appearances well, even from different viewpoints and loose garments. Moreover, for a given subject image, DreamPose can be conditioned on driving poses derived from a different video, as shown in the supplementary Figure~\\ref{fig:diff-video-pose}. We also show additional results on images from the DeepFashion dataset~\\cite{deep-fashion} in the supplementary materials.\n\n    \\subsection{Comparisons}\n        We compare DreamPose quantitatively and qualitatively to two publicly available state-of-the-art conditional video synthesis methods, Motion Representations for Articulated Animation (MRAA)~\\cite{articulated_animation} and Thin-Plate Spline Motion Model (TPSMM)~\\cite{thin_plate_spline}, and a concurrent diffusion-based pose transfer method, PIDM~\\cite{pose-transfer-dm}. We train all methods from scratch on the UBC Fashion Dataset~\\cite{ubc_fashion}, using the provided training scripts and configurations. For evaluating MRAA~\\cite{articulated_animation} and TPSMM~\\cite{thin_plate_spline}, we use the provided test scripts in the ``AVD\" mode. \n        \n        \\subsubsection{Quantitative Analysis} We present our quantitative analysis in Table~\\ref{quantitative-comparison}. We test all models on the UBC Fashion test set, consisting of 100 unique fashion videos, at 256px resolution \\cite{ubc_fashion}. For each video, we extract 50 frames for testing, where they are at least 50 frames away from the input frame. The full DreamPose model quantitatively outperforms all three methods in all metrics: L1, SSIM~\\cite{ssim}, VGG~\\cite{vgg}, LPIPS~\\cite{lpips}, FID~\\cite{fid}, FVD~\\cite{FVD}, and Average Euclidean Distance (AED). Note that while PIDM produces realistic images, it struggles to preserve clothing and person identity, as indicated by the quantitative results. Moreover, there is poor temporal consistency frame-to-frame, resulting in a large FVD~\\cite{FVD} score.\n\n        \n        \\begin{figure*}[t]\n            \\begin{center}\n                \\includegraphics[width=1.0\\linewidth]{Figures/Ablations.png}\n            \\end{center}\n           \\caption{Qualitative Ablation of Image Conditioning. We compare results of the original text-to-image Stable Diffusion model, our model with CLIP-only image embeddings without finetuning, our model with CLIP-VAE image encoder embeddings without VAE finetuning, and our full model.}\n           \\hspace{2em}\n        \\label{fig:ablations}\n        \\end{figure*}\n\n        \\subsubsection{Qualitative Analysis} We qualitatively compare our method to MRAA, TPSMM, and PIDM in Figure~\\ref{fig:qualitative-results}.  With these other methods, note that the person identity, fabric folds, and fine patterns are lost in new poses, whereas DreamPose accurately retains those details. Plus, during large pose changes, MRAA may produce disjointed or blurry limbs. \n        Compared to PIDM, DreamPose produces higher-fidelity results, in terms of both face identity and clothing patterns. While PIDM synthesizes realistic faces, they do not necessarily align with the identity of the source person. Moreover, we find that both the identity and the dress appearance vary frame-to-frame. As such, PIDM does not work well as-is for video synthesis.  We provide additional comparisons to PIDM on the DeepFashion dataset \\cite{deep-fashion} in the supplementary materials.\n        Furthermore, we conduct a user study and provide the results in the supplementary material.\n        \n        \n\n    \\subsection{Ablation Studies}\n    We perform comparisons of ablated versions of our method to verify our design choices. Namely, we compare five variants: \\textbf{(1) Ours$_{\\text{CLIP}}$:} We use a pretrained CLIP image encoder, instead of our dual CLIP-VAE encoder, \\textbf{(2) Ours$_{\\text{No-VAE-FT}}$:} We do subject-specific finetuning of the UNet only, not the VAE decoder, \\textbf{(3) Ours$_{\\text{1-pose}}$:} We concatenate only one target pose, instead of 5 consecutive poses, to the noise. \\textbf{(4) Ours$_\\text{smooth}$:} Like ablation Ours$_{\\text{1-pose}}$, but apply temporal smoothing to the output frames. \\textbf{(5) Ours$_\\text{full}$:} Our full model, including subject-specific VAE finetuning, CLIP-VAE encoder, and 5-pose input.\n\n    \\paragraph{Quantitative Comparison.} For each ablated version, we compute the L1, SSIM, VGG, and LPIPS for 100 predicted video frames selected from each of the 100 test videos of the UBC Fashion dataset \\cite{ubc_fashion}. Shown in Table~\\ref{tab:quantitative-ablation}, our full model outperforms the ablated versions in all four metrics.\n\n    \\begin{table}[h!]\n        \\begin{center}\n        \\begin{tabular}{c c c c c c }\n        \\hline\n        & L1 $\\downarrow$ & SSIM $\\uparrow$ & VGG  $\\downarrow$ & LPIPS  $\\downarrow$  \\\\\n        \\hline\\hline\n        Ours$_{\\text{CLIP}}$ & 0.025 & 0.882 & 0.247 & 0.070 \\\\\n        Ours$_{\\text{No-VAE-FT}}$ & 0.025 & 0.897 & 0.210 & 0.057 \\\\\n        Ours$_{\\text{1-pose}}$ & 0.019 & 0.899 & 0.208 & 0.056\\\\\n        Ours$_{\\text{smooth}}$ & 0.767 & 0.758 & 0.502 & 0.202\\\\\n        Ours$_{\\text{full}}$ & \\textbf{0.019} & \\textbf{0.900} & \\textbf{0.207} & \\textbf{0.056} \\\\\n        \\hline\n        \\end{tabular}\n        \\end{center}\n        \\caption{Quantitative comparison of ablated versions of our method. Note that the single-pose version of our method, although achieving similar numerical results, fails to achieve the motion smoothness as the full method. Please refer to our project page for a video comparison.\n        }\n        \\label{tab:quantitative-ablation}\n        \\end{table}\n\n    \\paragraph{Qualitative Comparison.} We visually show the effectiveness of our full method in Figure~\\ref{fig:ablations}. We compare results from the original text-conditioned Stable Diffusion model, our method with only a CLIP image encoder, our method with CLIP-VAE encoder, and our full method with CLIP-VAE encoder and subject-specific VAE finetuning. \n    \n    The original Stable Diffusion model with text-only conditioning via CLIP text encoder is unable to preserve rich details of the garment or person identity. Simply replacing the text encoder with a CLIP image encoder helps capture most image details, but there is still information loss about the appearance. Subject-specific finetuning of the UNet, similar to DreamBooth \\cite{dreambooth}, is critical to preserving photorealistic details in the face and garment. Furthermore, we find that also finetuning the VAE decoder on the input image greatly improves the sharpness of these details and does not lead to overfitting to the input pose. \n    \n    Lastly, with only a single input pose, there is noticeable flickering of the subject's shape, especially around the feet and hair. Please refer to video qualitative comparisons of each ablated version on our project page.\n        \\begin{figure}%[h!]\n            \\begin{center}\n                \\includegraphics[width=\\linewidth, height=8cm]{Supplementary-Figures/Failure_Cases.png}\n            \\end{center}\n           \\caption{Examples of failure cases of our method. Our method may merge limbs into underlying fabric textures (left), hallucinate features (middle), or predict front-facing person instead of a back-facing person (right).}\n        \\label{fig:failure-cases}\n        \\end{figure}    \n\\section{Limitations \\& Future Work}  In Figure~\\ref{fig:failure-cases}, we show failure cases of our method. On rare occasions, we observe limbs disappearing into the fabric, hallucinated dress features, and directional misalignment when the target pose is facing backwards. We suspect that some of these failures could be alleviated with improved pose estimation, a larger dataset, or a segmentation mask. Additionally, while our method produces realistic results on most plain and simple-patterned fabrics, some of our results present minor flickering behavior on large and complex patterns. Achieving better temporal consistency on such patterns, ideally without subject-specific finetuning, is left to future work. Lastly, similar to other diffusion models, our finetuning and inference times are slow compared to GAN or VAE methods. finetuning the model on a specific subject takes approximately 10 minutes for the UNet and 20 minutes for the VAE decoder, in addition to an 18 second per-frame rendering time. %Plus, each predicted frame requires ~18 seconds to synthesize with 100 denoising steps on a single A40 GPU. \n\n\\section{Conclusion}\n    In this paper, we presented DreamPose, a novel diffusion-based method for still fashion image animation. Given a single image and pose sequence, we demonstrate how our method generates photorealistic fashion videos from only a single image -- animating a diverse range of fabrics, patterns, and person identities. %Ultimately, DreamPose introduces a novel method to showcase fashion online, from social media platforms to online retail sites.\n\n\\section*{Acknowledgments}\n\\noindent This work was supported by NVIDIA and the UW Reality Lab, Meta, Google, OPPO, and Amazon.\n\n{\\small\n}\n\n\\pagebreak\n\n\\onecolumn\n\\setcounter{section}{0}\n\\begin{center}\n\\textbf{\\Large Supplementary Material}\n\\end{center}\n\\section{Implementation Details}\n    Our experiments are trained on two NVIDIA A100 GPU's with resolution 512x512. In our first phase of training, we fine-tune our base model UNet on the full training dataset for a total of 5 epochs at a learning rate of $5e{\\text{-}6}$. We use an effective batch size of 16 (through 4 gradient accumulation steps). We implement a dropout scheme where null values replace the pose input 5\\% of the time, the input image 5\\% of the time, and both input pose and input image 5\\% of the time during training.\n    We further fine-tune the UNet on a specific sample frame for another 500 steps with a learning rate of $1e{\\text{-}5}$ and no dropout. Lastly, we fine-tune the VAE decoder only for 1500 steps with a learning rate of $5e{\\text{-}5}$. \n    During inference, we use a PNDM sampler for 100 denoising steps \\cite{pndm}.\n    \n\\section{User Studies} \n    We conducted two user studies involving 50 distinct Amazon Mechanical Turk workers to compare our method with state-of-the art image animation approaches \\cite{articulated_animation} \\cite{thin_plate_spline} and evaluate the quality of our videos. In both surveys, workers evaluated results corresponding to 50 unique input images from the test set of the UBC Fashion dataset \\cite{ubc_fashion}.\n    \n    In the first user study, workers were asked their pair-wise preferences between our method and one of the other methods. For each input image, the workers were shown two videos: one containing the input image, our resulting video, and the MRAA resulting video and the other containing the input image, our resulting video, and the TPSMM resulting video. The ordering of our video and other video (MRAA or TPSMM) was randomized for each question. For each videos, workers selected their preference between the videos. The results are shown in Table~\\ref{user-survey}. Overall, the workers had a preference for our method over MRAA and TPSMM.\n    \n    In the second user study, workers were asked to rate our videos and TPSMM videos on a scale of 0 to 5, where 0 corresponds a video that does not match the input image at all and 5 corresponds to a realistic animation of the input image. During training, workers were shown a video of a different dress for as an example of a \"0\" rating and a ground-truth video of the input image as an example of the \"5\" rating. The results are shown in Figure~\\ref{fig:user-ratings}. Our videos achieved higher scores for image similarity and quality than TPSMM and ${85\\%}$ of users rated the results of our method a 3 or higher.\n    \n    \\begin{table}[h!]\n        \\begin{center}\n        \\begin{tabular}{c c c c }\n        \\hline\n        & $\\#$ Responses & Total Responses & ($\\%$) \\\\\n        \\hline\\hline\n        Ours $>$ MRAA \\cite{articulated_animation} & 1637 & 2500 & (65\\%)\\\\\n        Ours $>$ TPSMM \\cite{thin_plate_spline} & 1417 & 2500 & (57\\%) \\\\\n        \\hline\n        \\end{tabular}\n        \\end{center}\n        \\caption{Results of User Study \\#1: Workers choose between pairs of videos corresponding to input images, either our result vs. MRAA result or our result vs. TPSMM result. Overall, participants preferred our method over both MRAA and TPSMM in terms of quality and similarity to the input image.}\n        \\label{user-survey}\n    \\end{table}\n\n    \\begin{figure*}[h!]\n        \\begin{center}\n            \\includegraphics[width=0.5\\linewidth]{Supplementary-Figures/user-ratings.png}\n        \\end{center}\n       \\caption{ Results of User Study \\#2: Amazon Mechanical Turk worker ratings of our videos from 0 (video does not match input image) to 5 (video is a realistic animation of the input image). Overall, $85\\%$ of workers rated our method a 3 or higher.}\n    \\label{fig:user-ratings}\n    \\end{figure*}\n\n\\section{Different Videos for Source Person and Driving Pose Sequence}\n    We show in Figure~\\ref{fig:diff-video-pose} that DreamPose can animate an input image using motion from a video containing a different person and garment identity. As such, our method is applicable in practice when ground-truth motion is unavailable.\n\n        \\begin{figure}[h!]\n            \\begin{center}\n                \\includegraphics[width=0.8\\linewidth]{Figures/Different_Video_for_Image_and_Pose.png}\n            \\end{center}\n        \\caption{Qualitative results for conditioning on subject and pose from different videos.}\n        \\label{fig:diff-video-pose}\n        \\end{figure}\n\n\\section{Multiple Input Frames}\n    While DreamPose demonstrates high-quality results with only a single input image, DreamPose can also be fine-tuned with an arbitrary number of input images of a subject. We showcase the results of training with multiple input images in Figure~\\ref{fig:multi-image}. We find that additional input images of a subject increase the quality and viewpoint consistency.\n\n        \\begin{figure}%[h!]\n            \\begin{center}\n                \\includegraphics[width=0.8\\linewidth]{Figures/Single_vs_Multi_Input_Images_Small.png}\n            \\end{center}\n           \\caption{Results after training with 1, 3, 5, and 7 input images. Increasing the number of input frames improves fidelity of pose, facial identity, and color.  }\n        \\label{fig:multi-image}\n        \\end{figure}\n        \n\\section{Deep Fashion Results} \n    We demonstrate the effectiveness of our method on a popular dataset, DeepFashion, in Figure~\\ref{fig:deep-fashion} \\cite{pose-transfer-dm, deep-fashion}. Although trained exclusively on the UBC Fashion video dataset, DreamPose performs well on unseen retail images, even to new backgrounds, model identities, accessories, and patterns.\n\n        \\begin{figure*}[h!]\n            \\begin{center}\n                \\includegraphics[width=1.0\\linewidth]{Supplementary-Figures/Deep_Fashion_Results.png}\n            \\end{center}\n           \\caption{ DreamPose results on unseen samples from the DeepFashion dataset \\cite{deep-fashion}. Despite being trained exclusively on the UBC Fashion Dataset, our method generalizes to new garments and model identities after subject-specific finetuning of the base model.}\n        \\label{fig:deep-fashion}\n        \\end{figure*}\n\n\\section{Application to Pose Transfer}\n    While adapted for image-to-video synthesis, DreamPose is also an effective pose transfer tool. In Figure~\\ref{fig:pose-transfer}, we compare DreamPose to two state-of-the-art pose transfer models: DynaST \\cite{DynaST} and PIDM \\cite{pose-transfer-dm}. Our method is better able to preserve fine-details, such as shoe appearance, hemline, and face identity, than DynaST or PIDM. \n\n        \\begin{figure*}[h!]\n            \\begin{center}\n                \\includegraphics[width=1.0\\linewidth]{Supplementary-Figures/Deep_Fashion_Comparison.png}\n            \\end{center}\n           \\caption{Comparison of Pose Transfer Results. We compare our method to two state-of-the-art pose transfer methods, DynaST \\cite{DynaST} and PIDM \\cite{pose-transfer-dm}.}\n        \\label{fig:pose-transfer}\n        \\end{figure*}\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{[Article Title]{LEO: Generative Latent Image Animator for Human Video Synthesis}\n\n\n\\author[1]{\\fnm{Yaohui} \\sur{Wang}}\n\\author[1,2]{\\fnm{Xin} \\sur{Ma}}\n\\author[1]{\\fnm{Xinyuan} \\sur{Chen}}\n\\author[2]{\\fnm{Cunjian} \\sur{Chen}}\n\\author[3]{\\fnm{Antitza} \\sur{Dantcheva}}\n\\author[1]{\\fnm{Bo} \\sur{Dai}}\n\\author[1]{\\fnm{Yu} \\sur{Qiao}}\n\\affil[1]{\\orgname{Shanghai Artificial Intelligence Laboratory}, \\city{Shanghai}, \\country{China}}\n\\affil[2]{\\orgname{Monash University}, \\city{Melbourne}, \\country{Australia}}\n\\affil[3]{\\orgname{Inria, Université Côte d'Azur},  \\city{Valbonne}, \\country{France}}\n\n\n\\abstract{Spatio-temporal coherency is a major challenge in synthesizing high quality videos, particularly in synthesizing human videos that contain rich global and local deformations. To resolve this challenge, previous approaches have resorted to different features in the generation process aimed at representing appearance and motion. However, in the absence of strict mechanisms to guarantee such disentanglement, a separation of motion from appearance has remained challenging, resulting in spatial distortions and temporal jittering that break the spatio-temporal coherency. Motivated by this, we here propose LEO, a novel framework for human video synthesis, placing emphasis on spatio-temporal coherency. Our key idea is to represent motion as a sequence of flow maps in the generation process, which inherently isolate motion from appearance. We implement this idea via a flow-based image animator and a Latent Motion Diffusion Model (LMDM). The former bridges a space of motion codes with the space of flow maps, and synthesizes video frames in a warp-and-inpaint manner. LMDM learns to capture motion prior in the training data by synthesizing sequences of motion codes. Extensive quantitative and qualitative analysis suggests that LEO significantly improves coherent synthesis of human videos over previous methods on the datasets TaichiHD, FaceForensics and CelebV-HQ. In addition, the effective disentanglement of appearance and motion in LEO allows for two additional tasks, namely infinite-length human video synthesis, as well as content-preserving video editing. Project page: \\url{https://wyhsirius.github.io/LEO-project/}}\n\n\n\n\\keywords{Video generation, diffusion models, generative modeling}\n\n\n\n\\maketitle\n\n\\section{Introduction}\\label{intro}\nDeep generative models such as generative adversarial networks (GANs)~\\cite{goodfellow2014generative} and Diffusion Models~\\cite{ddpm, ddim} have fostered a breakthrough in\nvideo synthesis~\\cite{vondrick2016generating, tulyakov2017mocogan, TGAN2020, wang2020g3an, wang2021inmodegan, digan, stylegan-v, tats, makeavideo, phenaki, imagenvideo}, elevating tasks such as text-to-video generation~\\cite{makeavideo,phenaki}, video editing~\\cite{bar2022text2live}, as well as 3D-aware video generation~\\cite{bergman2022gnarf}. \nWhile existing work has demonstrated promising results w.r.t. frame-wise visual quality, synthesizing videos of strong spatio-temporal coherency,\ntailored to human videos, containing rich global and local deformations, remains challenging.\n\n\\begin{figure*}[!ht]\n\\centering\n\\includegraphics[width=1.0\\textwidth]{cover-taichi4.pdf}\n\\includegraphics[width=1.0\\textwidth]{cover-face3.pdf}\n\\caption{Our framework caters a set of video synthesis tasks including (i) unconditional video generation (first and second row), (ii) conditional generation based on one single image (fourth row) and (iii) video editing from the starting image (third and fifth row). Results pertain to our model being trained on the datasets TaichiHD, FaceForensics and CelebV-HQ.}\n\\label{fig:cover}\n\\end{figure*}\n\nMotivated by this, we here propose an effective generative framework, placing emphasis on \\textit{spatio-temporal coherency} in \\textit{human video synthesis}. Having this in mind, a fundamental step has to do with the \\textit{disentanglement} of videos w.r.t. \\textit{appearance} and \\textit{motion}.\nPrevious approaches have tackled such disentanglement by two jointly trained distinct networks, respectively providing appearance and motion features \\cite{tulyakov2017mocogan,wang2020g3an,wang2021inmodegan,WANG_2020_WACV,digan},\nas well as by a two-phase generation pipeline that firstly aims at training an image generator, and then at training a temporal network to generate videos in the image generator's latent space \\cite{mocoganhd,tats,videogpt}.\nNevertheless, such approaches encompass limitations related to spatial artifacts (\\textit{e.g.}, distortions of body structures and facial identities in the same sequence), as well as temporal artifacts (\\textit{e.g.}, inter-frame semantic jittering), even in short generated videos of 16 frames. \nWe argue that such limitations stem from incomplete disentanglement of appearance and motion in the generation process.\nSpecifically, without predominant mechanisms or hard constraints to guarantee disentanglement, even a minor perturbation in the high-level semantics will be amplified and will lead to significant changes in the pixel space.\n\n\\begin{figure}[t!]\n\\centering\n\\includegraphics[width=0.47\\textwidth]{inference3.pdf}\n\\caption{\\textbf{Inference stage.} At the inference stage, LMDM firstly accepts a starting motion code $\\alpha_1$ and a sequence of noise-vectors as input, in order to generate a sequence of motion codes $\\mathbf{a}$, further utilized to synthesize a sequence of flow maps $\\phi_i$ by the pre-trained image animator. The output video is obtained in a warp-and-inpaint manner based on $x_1$ and $\\phi_i$.}\n\\label{fig:inference}\n\\end{figure}\n\nDeviating from the above and towards disentangling videos w.r.t. appearance and motion,\nin this paper we propose a novel framework for human video generation, referred to as LEO, streamlined to ensure strong \\textit{spatio-temporal coherency}.\nAt the core of this framework is a \\textit{sequence of flow maps}, representing \\textit{motion semantics},\nwhich inherently isolate motion from appearance.\nSpecifically, LEO incorporates a latent motion diffusion module (LMDM), as well as a flow-based image animator.\nIn order to synthesize a video, an initial frame is either provided externally for \\textit{conditional generation},\nor obtained by a generative module for \\textit{unconditional generation}.\nGiven such initial frame and a sequence of motion codes sampled from the LMDM, the flow-based image animator generates a sequence of flow maps, and proceeds to synthesize the corresponding sequence of frames in a \\textit{warp-and-inpaint} manner.\n\nThe \\textit{training} of LEO is decomposed into \\textit{two phases}. \\textit{Firstly}, we train the flow-based image animator to encode input images into low-dimensional latent motion codes,\nand map such codes to flow maps, which are used for reconstruction via warp-and-inpaint.\nTherefore, once trained, \nthe flow-based image animator naturally provides a space of motion codes that are strictly constrained to only containing motion-related information.\nAt the \\textit{second stage},\nupon the space provided by the image animator,\nwe train the LMDM to synthesize sequences of motion codes and capture \\textit{motion prior} in the training data.\nTo endow LEO with the ability to synthesize videos of arbitrary length beyond the short training videos,\nwe adopt a Linear Motion Condition (LMC) mechanism\nin LMDM. As opposed to directly synthesizing sequences of motion codes,\nLMC enables LMDM to synthesize sequences of residuals w.r.t. a starting motion code,\nin order for longer videos to be easily obtained by concatenating additional sequences of residuals.\n\nTo evaluate LEO,\nwe conduct extensive experiments pertained to three human video datasets,\nincluding TaichiHD~\\cite{fomm}, FaceForensics~\\cite{rossler2018faceforensics}, and CelebV-HQ~\\cite{zhu2022celebvhq}. \nCompared to previous video synthesis methods,\nLEO demonstrates a significantly improved spatio-temporal coherency,\neven on synthesized videos of length of 512 frames.\nIn addition,\nLEO shows great potential in two extended tasks,\nnamely \\textit{infinite-length video synthesis},\nas well as \\textit{video editing} of a style in a synthesized video, while maintaining the content of the original video.\n\n\\section{Related Works}\n\\textbf{Unconditional video generation} aims to generate videos by learning the full distribution of training dataset. Most of the previous works~\\cite{vondrick2016generating,saito2017temporal,tulyakov2017mocogan,wang2020g3an,wang:tel-03551913,wang2021inmodegan,clark2019adversarial, brooks2022generating} are built upon GANs~\\cite{goodfellow2014generative, radford2015unsupervised, brock2018large, karras2019style, stylegan2} towards benefiting from the strong performance of the image generator. Approaches~\\cite{NIPS2017_2d2ca7ee,li2018disentangled,bhagat2020disentangling,xie2020motion} based on VAEs~\\cite{kingma2013auto} were also proposed while only show results on toy datasets. \nRecently, with the progress of deep generative models (\\textit{e.g.}, VQVAE~\\cite{vqvae}, VQGAN~\\cite{vqgan}, GPT~\\cite{gpt} and Denoising Diffusion Models~\\cite{ddpm, ddim, nichol2021improved}) on both image~\\cite{dalle, dalle2} and language synthesis~\\cite{radford2019language}, as well as the usage of large-scale pre-trained models, video generation also started to be explored with various approaches.\n\nMoCoGANHD~\\cite{mocoganhd} builds the model on top of a well-trained StyleGAN2~\\cite{stylegan2} by integrating an LSTM in the latent space towards disentangling content and motion. DIGAN~\\cite{digan} and StyleGAN-V~\\cite{stylegan-v} and MoStGAN-V~\\cite{mostgan}, inspired by NeRF~\\cite{feichtenhofer2019slowfast}, proposed an implicit neural representation approach to model time as a continuous signal aiming for long-term video generation. VideoGPT~\\cite{videogpt} and TATS~\\cite{tats} introduced to first train 3D-VQ models to learn discrete spatio-temporal codebooks, which are then be refined temporally by modified transformers~\\cite{transformer}. Recently, several works{~\\cite{vdm,pvdm,videofusion}} have shown promising capacity to model complex video distribution by incorporating spatio-temporal operations in Diffusion Models. While previous approaches have proposed various attempts either in training strategies~\\cite{mocoganhd,videogpt,tats} or in model architectures~\\cite{wang2020g3an,wang2021inmodegan,digan,stylegan-v} to disentangle appearance and motion, due to the lack of strong constrains, it is still difficult to obtain satisfying results. \n\n\\begin{figure*}[!ht]\n\\centering\n\\includegraphics[width=1.0\\textwidth]{framework3.pdf}\n\\caption{\\textbf{Overview of LEO.} Our framework incorporates two main parts, (i) an image animator, aiming to generate flow maps and synthesize videos in the pixel space, and (ii) Latent Motion Diffusion Model (LMDM), focusing on modeling the motion distribution in a latent motion space. Our framework requires a two-phase training. In the first phase, we train the image animator in a self-supervised manner towards mapping latent codes to corresponding flow maps $\\phi_i$. Once the image animator is well-trained, motion codes $\\mathbf{a}$ are extracted from a frozen encoder and used as inputs of LMDM. In the second phase, LMDMs are trained to learn the motion distribution by providing the starting motion $\\alpha_1$ as condition. Instead of directly learning the distribution of $\\mathbf{a}$, we adopt a Linear Motion Condition (LMC) mechanism in LMDM towards synthesizing sequences of residuals with respect to $x_1$. At the inference stage, given a starting image $x_i$ and corresponding motion code $\\alpha_i$, LMDM firstly generates a motion code sequence, which is then used by the image animator to generate flow maps to synthesize output videos in a warp-and-inpaint manner.}\n\\label{fig:main-img}\n\\end{figure*}\n\nIn contrast to unconditional video generation, \\textbf{conditional video generation} seeks to produces high-quality videos, following image-to-image generation pipeline~\\cite{chu2017cyclegan,isola2017image, huang2018munit}. In this context, additional signals such as semantic maps~\\cite{pan2019video, wang2018vid2vid,wang2019fewshotvid2vid}, human key-points~\\cite{jang2018video, yang2018pose, walker2017pose, chan2019everybody, zakharov2019few, wang2019fewshotvid2vid, wang2021dance}, motion labels~\\cite{WANG_2020_WACV}, 3DMM~\\cite{Zhao_2018_ECCV, yang20223dhumangan} and optical flow~\\cite{li2018flow,ohnishi2018ftgan} have been exploited to guide motion generation. In addition, text description, has been used in large-scale video diffusion models~{\\cite{makeavideo,imagenvideo,videoLDM,lavie,vdm,show1,snapvideo,videocrafter2,latte,seine,svd}} for high-quality video generation. Our framework also supports for conditional video generation based on a single image. However, unlike previous approaches, our method follows the image animation pipeline~\\cite{fomm,mraa,wang2022latent} which leverages the dense flow maps for motion modeling. We introduce our method in details in the following. \n\n\\section{Method}\nFig.~\\ref{fig:main-img} illustrates the training of LEO, comprising of two-phases. We firstly train an image animator towards learning high-quality latent motion codes of the datasets. In the second phase, we train the Latent Motion Diffusion Model (LMDM) to learn a motion prior over the latent motion codes. To synthesize a video, the pre-trained image animator takes the motion codes to generate corresponding flow maps, which are used to warp and inpaint starting frame. The warp-and-inpaint operation is conducted in two modules inside image animator. The warping module firstly produces flow fields based on motion codes to warp starting frame, then the inpainting module learns to fill in the holes in the warped starting frame and refine the entire image. Each video sequence is produced frame by frame.\n\nWe formulate a video sequence $v = \\{x_i\\}^{L}_{i=1}, x_i\\sim \\mathcal{X}\\in\\mathbb{R}^{3\\times H\\times W}$ as $v = \\{\\mathcal{T}(x_1, G(\\alpha_i))\\}^{L}_{i=2}, \\alpha_i\\sim \\mathcal{A}\\in \\mathbb{R}^{1\\times N}$, where $x_i$ denotes the $i^{th}$ frame, $\\alpha_i$ denotes a latent motion code at timestep $i$, $G$ represents the generator in the image animator aiming to generate a flow map $\\phi_{i}$ from $\\alpha_{i}$.\n\n\\subsection{Learning Latent Motion Codes}\nTowards learning a frame-wise latent motion code, we adopt the state-of-the-art image animator LIA~\\cite{wang2022latent} as it enables to encode input images into corresponding motion codes.  LIA consists of two modules, an encoder $E$ and a generator $G$. During training, given a source image $x_s$ and a driving image $x_d$, $E$ encodes $x_s, x_d$ into a motion code $\\alpha = E(x_s, x_d)$, and $G$ generates a flow field $\\phi = G(\\alpha)$ from the code. LIA is trained in a self-supervised manner with the objective to reconstruct the driving image. \n\nTraining LIA in such a self-supervised manner brings two notable benefits for our framework, (i) it enables LIA to achieve high-quality perceptual results, and (ii) as a motion code is strictly equivalent to flow maps, there are guarantees that $\\alpha$ is only motion-related without any appearance interference. \n\n\\begin{figure*}[t!]\n\\centering\n\\begin{subfigure}[t]{1.0\\textwidth}\n\\centering\n\\includegraphics[width=1.0\\textwidth]{taichi-compare.pdf}\n\\caption{\\footnotesize{\\textbf{TaichiHD}}}\n\\end{subfigure}\n\\begin{subfigure}[t]{1.0\\textwidth}\n\\centering\n\\includegraphics[width=1.0\\textwidth]{face-compare.pdf}\n\\caption{\\footnotesize{\\textbf{FaceForensics}}}\n\\end{subfigure}\n\\caption{\\textbf{Qualitative Comparison.} We qualitatively compare LEO with DIGAN, TATS, StyleGAN-V on short video generation. The results indicate that on both (a) TaichiHD (128 and 256 resolutions) and (b) FaceForensics datasets, our proposed method achieves the best visual quality and is able to capture the human structure well. Other approaches either modify the facial structure (e.g., StyleGAN-V) or fail to generate a complete human body (e.g., TATS and DIGAN).}\n\\label{fig:qualitative}\n\\end{figure*} \n\n\\subsection{Leaning a Motion Prior}\nOnce LIA is well-trained on a target dataset, for any given video $v=\\{x_i\\}^{L}_{i=1}$, we are able to obtain a motion sequence $\\mathbf{a}=\\{\\alpha_i\\}^{L}_{i=1}$ with the frozen $E$. In the second phase of our training, we propose to learn a motion prior by temporal Diffusion Models. \n\nUnlike image synthesis, data in our second phase is a set of sequences. We firstly apply a temporal Diffusion Model for modeling the temporal correlation of $\\mathbf{a}$. The general architecture of this model is a 1D U-Net adopted from~\\cite{ddpm}. To train this model, we follow the standard training strategy with a simple mean-squared loss,\n\\begin{align}\n    L_{\\text{LMDM}} = \\mathbb{E}_{\\epsilon \\sim \\mathcal{N} (0,1), t}\\left [ \\left \\| \\epsilon - \\epsilon_{\\theta}(\\mathbf{a}_t, t)\\right \\|^{2}_{2}\\right],  \n\\end{align}\nwhere $\\epsilon$ denotes the unscaled noise, $t$ is the time step, $\\mathbf{a}_t$ is the latent noised motion code to time $t$. During inference, a random Gaussian noise $\\mathbf{a}_T$ is iteratively denoised to $\\mathbf{a}_0=\\{\\alpha_i\\}^{L}_{i=1}$, and the final video sequence is obtained through the generator. %$v^{'} = \\{\\mathcal{T}(x_1, G(\\alpha_{i}))\\}^{L}_{i=1}$.\n\nAt the same time in our experiments, we found that learning motion sequences in a complete unconditional manner brings to the fore limitations, namely (i) the generated codes are not consistent enough for producing smooth videos, as well as (ii) the generated motion codes can only be used to produce fixed length videos. Hence, towards addressing those issues, we propose a \\textbf{conditional Latent Motion Diffusion Model (cLMDM)} which aims for high-quality and long-term human videos.    \n\nOne major characteristic of LIA has to do with the linear motion space. Any motion code $\\alpha_t$ in $\\mathbf{a}$ can be re-formulated as\n\\begin{equation}\\label{eq:2}\n    \\alpha_{i} = \\alpha_{1} + m_{i}, i\\ge 2,\n\\end{equation}\nwhere $\\alpha_{1}$ denotes the motion code at the first timestep and $m_{i}$ denotes the motion difference between timestep $1$ and $i$, so that we can re-formulate $\\mathbf{a}$ as\n\\begin{equation}\\label{eq:3}\n    \\mathbf{a} = \\alpha_{1} + \\mathbf{m},\n\\end{equation}\nwhere $\\mathbf{m}=\\{m_i\\}^{L}_{i=2}$ denotes the motion difference sequence. Therefore, Eq.~\\ref{eq:2} and \\ref{eq:3} indicate that a motion sequence can be represented by $\\alpha_{1}$ and $\\mathbf{m}$. Based on this, we propose a \\textbf{Linear Motion Condition (LMC)} mechanism in cLMDM to condition the generative process with $\\alpha_1$. During training, at each time step, we only add noise onto $\\mathbf{m_t}$ instead of the entire $\\mathbf{a}$ and leave $\\alpha_1$ intact. The objective function of cLMDM is\n\\begin{equation}\n\\begin{split}\n    L_{\\text{cLMDM}} &= \\mathbb{E}_{\\epsilon \\sim \\mathcal{N} (0,1), t}\\left [ \\left \\| \\epsilon - \\epsilon_{\\theta}(\\mathbf{m}_{t}, \\alpha_1, t)\\right \\|^{2}_{2}\\right], \\\\\n\\end{split}\n\\end{equation}\nwhere $\\alpha_{1}$ denotes the condition signal and $\\mathbf{m}_t$ stands for the noised $\\mathbf{m}$ to time $t$. $\\alpha_1$ is first added on $\\mathbf{m}_t$ and then concatenated along temporal dimension. LMC will be applied at each time step until we reach $\\mathbf{m}_0$. The final motion sequence is obtained as $\\mathbf{a}_0 = [\\alpha_1, \\mathbf{m}_0]$. We find that following this, a related generated motion sequence is more stable and contains fewer artifacts, as $\\alpha_1$ serves as a strong signal to constrain the generated $\\mathbf{m}$ to follow the initial motion. \n\nWhile the results from cLMDM outperforms previous models, the groundtruth $\\alpha_1$ is necessitated during both, training and inference stage. Towards \\textit{unconditional generation}, we train an additional simple DM to fit the distribution $p(\\alpha_i)$ in a frame-wise manner. We refer to the cLMDM and such simple DM jointly as \\textbf{Latent Motion Diffusion Model (LMDM)}. By this way, LMDM are able to work in both conditional and unconditional motion generation. \n\nTowards generating videos of arbitrary length, we propose an autoregressive approach based on proposed LMDM. By taking the last motion code from the previous generated sequence as the $\\alpha_1$ in the current sequence, with a randomly sampled noise, LMDM are able to generate an infinite-length motion sequence. By combining such sequence in pre-trained LIA with a starting image, LEO can synthesize photo-realistic and long-term videos.\n\n\\begin{figure*}[t!]\n\\centering\n\\includegraphics[width=1.0\\textwidth]{long-term.pdf}  \n\\caption{\\textbf{Comparison on long-term video generation.} We compare with TATS by generating 512-frame videos. Videos from TATS start crashing around 50 frames while our model is able to continue producing high-quality frames with diverse motion.}\n\\label{fig:long-term}\n\\end{figure*}\n\n\\subsection{Learning Starting Frames}\\label{sec:img-cond}\nIn our framework, a starting image $x_1$ is required to synthesize a video. As image space is modeled independently, here we propose two options to obtain $x_1$.\n\n\\textbf{Option 1: existing images.} The first option is to directly take the images either from a real distribution or from an image generation network. In this context, our model is a conditional video generation model, which learns to predict future motion from $x_1$. Starting motion $\\alpha_1$ is obtained through $\\alpha_1 = E(x_1)$.\n\n\\textbf{Option 2: conditional Diffusion Models.} The second option is to learn a conditional DDPM~\\cite{ddpm} (cDDPM) with $\\alpha_1$ as a condition to synthesize $x_1$. By combining LEO with LMDM as well as cDDPM, we are able to conduct unconditional video synthesis.\n\n\\section{Experiments}\nIn this section, we firstly briefly describe our experimental setup, introducing datasets, evaluation metrics and implementation details. Secondly, we qualitatively demonstrate generated results on both, short and long video synthesis. Then we show quantitative evaluation w.r.t. video quality, comparing LEO with SoTA. Next, we conduct an ablation study to prove the effectiveness of proposed conditional mechanism LMC. Finally, we provide additional analysis of our framework, exhibiting motion and appearance disentanglement, video editing and infinite-length video generation. \n\n\\textbf{Datasets.} As we focus on human video synthesis, evaluation results are reported on three human-related datasets, TaichiHD~\\cite{fomm}, FaceForensics~\\cite{rossler2018faceforensics} and CelebV-HQ~\\cite{zhu2022celebvhq}. We \nuse both $128\\times 128$ and $256\\times 256$ resolution TaichiHD datasets, and only $256\\times 256$ resolution FaceForensics and CelebV-HQ datasets.\n\n\\begin{itemize}\n    \\item \\textbf{TaichiHD}~\\cite{fomm} comprises 3100 video sequences downloaded from YouTube. In train and test splits, it contains 2815 and 285 videos, respectively. We conducted all our experiments on the train split and used both $128\\times 128$ and $256\\times 256$ resolutions in our experiments.\n    \\item \\textbf{FaceForensics}~\\cite{rossler2018faceforensics} includes 1000 video sequences downloaded from YouTube. Following the preprocessing of previous methods~\\citep{TGAN2020, stylegan-v}, face areas are cropped based on the provided per-frame meshes. We resized all videos to $256\\times 256$ resolution.\n    \\item \\textbf{CelebV-HQ}~\\cite{zhu2022celebvhq} comprises 35666 high-quality talking head videos of 3 to 20 seconds each. In total, it represents 15653 celebrities. We resized the original videos to $256\\times 256$ resolution, in order to train our models. \n\\end{itemize}\n\n\\textbf{Evaluation metric.} For quantitative evaluation, we apply the commonly used metrics FVD and KVD, in order to compare with other approaches on video quality and apply Average Content Distance (ACD) towards evaluating the identity consistency of faces and bodies in the generated videos. In addition, we conduct a user study with 20 users towards comparing with objective quantitative evaluation. \n\n\\begin{itemize}\n\\item \\textbf{Frechet video distance (FVD) and Kernel Video Distance (KVD).} We use I3D~\\citep{carreira2017quo} trained on Kinetics-400 as feature extractor to compute FVD and KVD. However, we find FVD is a very sensitive metric, which can be affected by many factors such as frame-rate, single image quality, video length and implementation, which also mentioned in~\\citep{stylegan-v}. Therefore, towards making a fair comparison, on the TaichiHD dataset, we adopt the implementation from DIGAN~\\citep{digan}. As for FaceForensics and CelebV-HQ, we chose to follow the implementation of StyleGAN-V~\\citep{stylegan-v}. \n\\item \\textbf{Average Content Distance (ACD).} ACD measures the content consistency in generated videos. To evaluate results from FaceForensics and TaichiHD, we extract features from each generated frame and proceed to extract a per-frame feature vector in a video. The ACD was then computed using the average pairwise L2 distance of the per-frame feature vectors. We follow the implementation in~\\citep{mocoganhd} to compute ACD for FaceForensics. As for TaichiHD, we employ the pre-trained person-reID model~\\citep{zheng2018discriminatively} to extract person identity features.\n\\item \\textbf{User study.} We asked 20 human raters to evaluate generated video quality, as well as video coherency. In each user study, we show paired videos and ask the raters, to rate 'which clip is more realistic / which clip is more coherent'. Each video-pair contains one generated video from our method, whereas the second video is either \\textit{real} or generated from other methods.\n\n\\end{itemize}\n\n\\textbf{Implementation details.} Our framework requires two-phase training. In the first phase, we follow the standard protocol to train LIA~\\citep{wang2022latent} to encode input images into low-dimensional latent motion codes, and map such codes to flow maps, which are used for reconstruction via warp-and-inpaint. Therefore, once trained, LIA naturally provides a space of motion codes that are strictly constrained to only containing motion-related information. In the second phase, we only train LMDM on the extracted motion codes from Encoder. We note that the LMDM is a 1D U-Net adopted from~\\citep{nichol2021improved}, we set the input size as $64\\times 20$, where 64 is the length of the sequence and 20 is the dimension of the motion code. We use 1000 diffusion steps and a learning rate of $1\\mathrm{e}{-4}$. As the training of LMDM is conducted in the latent space of LIA, the entire training is very efficient and only requires one single GPU. \n\n\\subsection{Qualitative Evaluation}\nWe qualitatively compare LEO with SoTA by visualizing the generated results. We firstly compare our method with DIGAN, TATS and StyleGAN-V on the FaceForensics and TaichiHD datasets for \\textit{short video generation}. As shown in Fig.~\\ref{fig:cover} and~\\ref{fig:qualitative}, the visual quality of our generated results outperforms other approaches w.r.t both, appearance and motion. For both resolutions on TaichiHD datasets, our method is able to generate complete human structures, whereas both, DIGAN and TATS fail, especially for arms and legs. When compared with StyleGAN-V on FaceForensics dataset, we identify that while LEO preserves well facial structures, StyleGAN-V modifies such attributes when synthesizing large motion. \n\nSecondly, we compare with TATS for long-term video generation. Specifically, 512 frames are produced for the resolution $128\\times 128$ pertained to the TaichiHD dataset. As shown in Fig.~\\ref{fig:long-term}, the subject in the videos from TATS starts crashing around 50 frames and the entire video sequence starts to fade. On the other hand, in our results, the subject continues to perform diverse actions whilst well preserving the human structure. We note that our model is only trained using a 64-frame sequence.\n\n\\begin{table*}[!t]\n\\begin{center}\n\\setlength\\arrayrulewidth{1pt}\n\\scalebox{0.8}{\n\\begin{tabular}{ccccccccccccc}\n\\hline\n& \\multicolumn{4}{c}{TaichiHD128} & \\multicolumn{3}{c}{TaichiHD256} & \\multicolumn{3}{c}{FaceForensics} & \\multicolumn{1}{c}{CelebV-HQ} \\\\\nMethod & $\\text{FVD}_{16}$ & $\\text{KVD}_{16}$ & $\\text{ACD}_{16}$ && $\\text{FVD}_{16}$ & $\\text{KVD}_{16}$ && $\\text{FVD}_{16}$ & $\\text{ACD}_{16}$ && $\\text{FVD}_{16}$   \\\\\n\\cmidrule{2-4}\\cmidrule{6-7}\\cmidrule{9-10}\\cmidrule{12-12}\nMoCoGAN-HD & $144.7\\pm 6.0$ & $25.4\\pm 1.9$ & - && - & - && 111.8 & 0.33 && 212.4  \\\\\nDIGAN & $128.1\\pm 4.9$ & $20.6\\pm 1.1$ & 2.17 && $156.7\\pm 6.2$ & - && 62.5 & - && 72.9  \\\\\nTATS & ${136.5\\pm1.2}^{*}$ & ${22.2\\pm1.0}^{*}$ & 2.28 && - & - && - & - && -  \\\\\nStyleGAN-V & - & - & - && - & - && 47.4 & 0.36 && 69.1  \\\\\nMoStGAN-V & - & -& - && - & - && 39.7 & 0.38 && 132.1 \\\\\n\\cmidrule{1-12}\nOurs (uncond) & $100.4\\pm 3.1$ & $11.4\\pm 3.2$ & 1.83 && $122.7\\pm 1.1$ & $20.49\\pm 0.9$ && 52.3 & 0.28 && -  \\\\\nOurs (cond) & $\\mathbf{57.6\\pm 2.0}$ & $\\mathbf{4.0\\pm 1.5}$ & \\textbf{1.22} && $\\mathbf{94.8\\pm 4.2}$ & $\\mathbf{13.47\\pm 2.3}$ && \\textbf{35.9} & \\textbf{0.27} && \\textbf{40.2}  \\\\\n\\hline\n\\end{tabular}}\n\\end{center}\n\\caption{\\textbf{Evaluation for unconditional and conditional short video generation.} LEO systematically outperforms other approaches on conditional video generation, and achieves better or competitive results on unconditional generation w.r.t. FVD, KVD and ACD. (*results are reproduced based on official code and released checkpoints.)}\n\\label{tab:fvd-cond-uncond-short}\n\\end{table*}\n\n\\begin{table}[!t]\n\\begin{center}\n\\setlength\\arrayrulewidth{0.8pt}\n\\scalebox{0.68}{\n\\begin{tabular}{ccccccc}\n\\hline\n& \\multicolumn{4}{c}{TaichiHD128} & \\multicolumn{2}{c}{FaceForensics} \\\\\nMethod & $\\text{FVD}_{128}$ & $\\text{KVD}_{128}$ & $\\text{ACD}_{128}$ && $\\text{FVD}_{128}$ & $\\text{ACD}_{128}$ \\\\\n\\cmidrule{2-4}\\cmidrule{6-7}\nDIGAN & - & - & - && 1824.7 & - \\\\\nTATS & $1194.58\\pm 1.1$ & $462.03\\pm 8.2$ & 2.85 && - & -\\\\\nStyleGAN-V & - & - & - && \\textbf{89.34} & 0.49 \\\\\n\\cmidrule{1-7}\nOurs & $\\mathbf{155.54\\pm 2.6}$ & $\\mathbf{48.82\\pm 5.9}$ & \\textbf{2.06} && 96.28 & \\textbf{0.34}\\\\\n\\hline\n\\end{tabular}}\n\\end{center}\n\\caption{\\textbf{Evaluation for unconditional long-term video generation.} LEO outperforms other methods on long-term (128 frames) video generation w.r.t. FVD, KVD and ACD.}\n\\label{tab:fvd-uncond-long}\n\\end{table}\n\n\\subsection{Quantitative evaluation}\nIn this section, we compare our framework with five state-of-the-art for both, conditional and unconditional short video generation, as well as unconditional long-term video generation.\n\n\\textbf{Unconditional short video generation.} In this context, as described in Sec.~\\ref{sec:img-cond}, Option 2, the $x_1$ is randomly generated by a pre-trained cDDPM. We compare with SoTA by generating 16 frames. To compare with DIGAN on high-resolution generation, we also generate videos of $256\\times 256$ resolution. Related FVDs and KVDs are reported in Tab.~\\ref{tab:fvd-cond-uncond-short}. LEO systematically outperforms other methods w.r.t. video quality, obtaining lower or competitive FVD and KVD on all datasets. On high-resolution generation, our results remain better than DIGAN. \n\nHowever, by comparing the results between StyleGAN-V and ours, we find FVD is not able to represent the quality of generated videos veritably. We observe that StyleGAN-V is not able to preserve facial structures, whereas LEO is able to do so, see Fig.~\\ref{fig:qualitative}. We additionally compute ACD, in order to further analyze the identity consistency in 16-frame videos. Tab.~\\ref{tab:fvd-cond-uncond-short} reflects on the fact that our method achieves significantly better results compared to other approaches. In addition, we conduct user study \\textit{w.r.t.} video quality and coherency of generated videos among different methods. Results in Tab.~{\\ref{tab:user-study}} showcase that as nearly all users rated for our generated results to be superior than other approaches. Hence, we conclude that a metric, replacing FVD is in urgent need in the context of video generation.\n\n\\textbf{Unconditional long video generation} \nWe evaluate our approach for long-term video generation w.r.t. FVD and ACD. In this context, we compare LEO with StyleGAN-V on the FaceForensics dataset, and both DIGAN and TATS on the TaichiHD. We report results based on 128-frame generation in Tab.~\\ref{tab:fvd-uncond-long}, which clearly shows that our method outperforms others in such context. We hypothesize that consistent and stable motion codes produced by our LMDM are key to producing high-quality long-term videos.\n\n\n\\textbf{Conditional short video generation}\nAs described in Sec.~\\ref{sec:img-cond}, Option 1, our framework additionally caters for conditional video generation by taking an existing image to hallucinate the following motion. Specifically, we randomly select 2048 images from both, TaichiHD and FaceForensics datasets as $x_1$ and compute corresponding $\\alpha_1$ as input of LMDM. As depicted in Tab.~\\ref{tab:fvd-cond-uncond-short}, results conditioned on the real images achieve the lowest FVD, KVD and ACD values, suggesting that the quality of a starting image is pertinent for output video quality, which further signifies that in the setting of unconditional generation, training a better cDDPM will be instrumental for improving results.\n\n\\begin{figure*}[t!]\n\\centering\n\\includegraphics[width=1.0\\textwidth]{distanglement.pdf}  \n\\caption{\\textbf{Disentanglement of motion and appearance.} The first and second row share the same appearance, with different motion codes. Results display that our model is able to produce diverse motion from the same content.}%, demonstrating the ability of LEO to disentangle appearance and motion.}\n\\label{fig:disentanglement}\n\\end{figure*}\n\n\\begin{figure*}[t!]\n\\centering\n\\includegraphics[width=1.0\\textwidth]{video-edit-v2.pdf}  \n\\caption{\\textbf{Video editing.} We show video editing results by combining LEO with off-the-shelf image editing model ControlNet. We are able to edit the appearance of the entire video sequence through only editing the starting image.}\n\\label{fig:semantic-edit-taichi}\n\\end{figure*}\n\n\\begin{table}[!t]\n\\begin{minipage}[t]{0.45\\textwidth}\n\\centering\n\\setlength{\\tabcolsep}{3.5pt}\n\\setlength\\arrayrulewidth{1pt}\n\\scalebox{0.9}{\n\\begin{tabular}{cccc}\n\\hline\nMethod & TaichiHD (\\%) & FaceForensics (\\%) \\\\\n\\cmidrule{1-3}\nOurs / TATS & \\textbf{93.00} / 7.00  & - \\\\\nOurs / StyleGAN-V  & - & \\textbf{91.33} / 8.67 \\\\\n\\hline\n\\hline\nMethod & TaichiHD (\\%) & FaceForensics (\\%) \\\\\n\\cmidrule{1-3}\nOurs / TATS & \\textbf{98.60} / 1.40  & - \\\\\nOurs / StyleGAN-V  & - & \\textbf{93.20} / 6.80 \\\\\n\\hline\n\\end{tabular}}\n\\captionof{table}{\\textbf{User study.} We conduct user studies pertaining to the datasets TaichiHD and FaceForensics \\textit{w.r.t.} video quality (up) as well as coherency (down).}\n\\label{tab:user-study}\n\\end{minipage}\n\\begin{minipage}[t]{0.05\\textwidth}\n\\quad\n\\end{minipage}\n\\begin{minipage}[t]{0.45\\textwidth}\n\\centering\n\\setlength\\arrayrulewidth{1pt}\n\\scalebox{1.0}{\n\\begin{tabular}{ccc}\n\\hline\n & TaichiHD & FaceForensics \\\\\n\\cmidrule{1-3}\nw/o LMC & 118.6 & 60.03 \\\\\nwith LMC & \\textbf{100.4} & \\textbf{52.32} \\\\\n\\hline\n\\end{tabular}}\n\\captionof{table}{\\textbf{Ablation study of proposed LMC.} Models with LMC achieved the lowest FVD on both datasets.}\n\\label{tab:ablation-lmc}\n\\end{minipage}\n\\end{table}\n\n\\section{Ablation Study}\nIn this section, we place emphasis on analyzing the effectiveness of proposed Linear Motion Condition (LMC) in  LMDM. We train two models, with and without LMC on both TaichiHD and FaceForensics datasets. As shown in Tab.~\\ref{tab:ablation-lmc}, using LMC significantly improves the generated video quality, which proves that our proposed LMC is an effective mechanism for involving $\\alpha_1$ in LMDM.\n\n\\section{Additional Analysis}\n\n\\textbf{Motion and appearance disentanglement.} We proceed to combine the same $x_1$ with different $\\mathbf{m}$, aiming to reveal whether $\\mathbf{m}$ is only motion-related. Fig.~\\ref{fig:disentanglement} illustrates that different $\\mathbf{m}$ enables the same subject to perform different motion - which proves that our proposed LMDM is indeed learning a motion space, and appearance and motion are clearly disentangled. This experiment additionally indicates that our model does not overfit on the training dataset, as different noise sequences are able to produce diverse motion sequences.\n\n\\textbf{Video Editing.} As appearance is modeled in $x_1$, we here explore the task of video editing by modifying the semantics in thestarting image. Compared to previous approaches, where image-to-image translation is required, our framework simply needs an edit of the semantics in an one-shot manner. Associated results are depicted in Fig.~\\ref{fig:cover} and Fig.~\\ref{fig:semantic-edit-taichi}. We apply the open-source approach ControlNet~\\cite{controlnet} on the starting frame by entering various different prompts. Given that the motion space is fully disentangled from the appearance space, our videos maintain the original temporal consistency, uniquely altering the appearance. \n\n\\textbf{Infinite-length video generation.}  \nIn addition to presented settings, our framework is able to generate infinite-length videos. To generate long-term FaceForensics, as shown in Fig.~\\ref{fig:long-term-face}, we provide the last generated code from the previous sequence as the starting code of the current sequence. The entire long-term video is generated in an \\textit{autoregressive} manner. Surprisingly, we find that such a simple approach is sufficient to produce more than 1000 frames. We note that for TaichiHD dataset, due to limited motion patterns, this setting yields repeated motion. Towards addressing this limitation, as shown in Fig.~\\ref{fig:long-term-taichi}, we design an additional \\textit{Transition Diffusion Model (Transition DM)} aimed at generating transition motion between the last code from original generated sequence and a new motion code generated from the \\textit{simple DM}. Doing so, the Transition DM enforces the network to exit the original motion pattern and transit to new pattern. To evaluate the effectiveness of the proposed method, we generate long videos \\textit{with} and \\textit{without} Transition DM and request human raters to watch respective videos and answer the question `Does the clip contain repeated motion?'. Results are reported in Tab.~{\\ref{tab:tdm}}, which shows the effectiveness of Transition DM to prevent repeated motion.\n\n\\begin{table}[!h]\n\\centering\n\\setlength{\\tabcolsep}{25pt}\n\\setlength\\arrayrulewidth{1pt}\n\\begin{tabular}{cc}\n\\hline\n & Occurrence (\\%) \\\\\n\\cmidrule{1-2}\nw/o Transition DM & 0.45 \\\\\nwith Transition DM & 0.02 \\\\\n\\hline\n\\end{tabular}\n\\caption{\\textbf{User study of repeated motion.} We show the occurrence of repeated motion with and without the usage of Transition DM.}\n\\label{tab:tdm}\n\\end{table}\n\n\\textbf{\\begin{figure*}[th!]\n    \\centering\n    \\includegraphics[width=0.9\\textwidth]{infinite-fig-taichi.pdf}\n    \\caption{Infinite-length video generation for TaichiHD.}\n    \\label{fig:long-term-taichi}\n\\end{figure*}}\n\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=0.47\\textwidth]}\n\n\\begin{document}\n\n\\title[Article Title]{LEO: Generative Latent Image Animator for Human Video Synthesis}\n\n\\author[1]{\\fnm{Yaohui} \\sur{Wang}}\n\\author[1,2]{\\fnm{Xin} \\sur{Ma}}\n\\author[1]{\\fnm{Xinyuan} \\sur{Chen}}\n\\author[2]{\\fnm{Cunjian} \\sur{Chen}}\n\\author[3]{\\fnm{Antitza} \\sur{Dantcheva}}\n\\author[1]{\\fnm{Bo} \\sur{Dai}}\n\\author[1]{\\fnm{Yu} \\sur{Qiao}}\n\\affil[1]{\\orgname{Shanghai Artificial Intelligence Laboratory}, \\city{Shanghai}, \\country{China}}\n\\affil[2]{\\orgname{Monash University}, \\city{Melbourne}, \\country{Australia}}\n\\affil[3]{\\orgname{Inria, Université Côte d'Azur},  \\city{Valbonne}, \\country{France}}\n\n\\abstract{Spatio-temporal coherency is a major challenge in synthesizing high quality videos, particularly in synthesizing human videos that contain rich global and local deformations. To resolve this challenge, previous approaches have resorted to different features in the generation process aimed at representing appearance and motion. However, in the absence of strict mechanisms to guarantee such disentanglement, a separation of motion from appearance has remained challenging, resulting in spatial distortions and temporal jittering that break the spatio-temporal coherency. Motivated by this, we here propose LEO, a novel framework for human video synthesis, placing emphasis on spatio-temporal coherency. Our key idea is to represent motion as a sequence of flow maps in the generation process, which inherently isolate motion from appearance. We implement this idea via a flow-based image animator and a Latent Motion Diffusion Model (LMDM). The former bridges a space of motion codes with the space of flow maps, and synthesizes video frames in a warp-and-inpaint manner. LMDM learns to capture motion prior in the training data by synthesizing sequences of motion codes. Extensive quantitative and qualitative analysis suggests that LEO significantly improves coherent synthesis of human videos over previous methods on the datasets TaichiHD, FaceForensics and CelebV-HQ. In addition, the effective disentanglement of appearance and motion in LEO allows for two additional tasks, namely infinite-length human video synthesis, as well as content-preserving video editing. Project page: \\url{https://wyhsirius.github.io/LEO-project/}}\n\n\\keywords{Video generation, diffusion models, generative modeling}\n\n\\maketitle\n\n\\section{Introduction}\\label{intro}\nDeep generative models such as generative adversarial networks (GANs)~\\cite{goodfellow2014generative} and Diffusion Models~\\cite{ddpm, ddim} have fostered a breakthrough in\nvideo synthesis~\\cite{vondrick2016generating, tulyakov2017mocogan, TGAN2020, wang2020g3an, wang2021inmodegan, digan, stylegan-v, tats, makeavideo, phenaki, imagenvideo}, elevating tasks such as text-to-video generation~\\cite{makeavideo,phenaki}, video editing~\\cite{bar2022text2live}, as well as 3D-aware video generation~\\cite{bergman2022gnarf}. \nWhile existing work has demonstrated promising results w.r.t. frame-wise visual quality, synthesizing videos of strong spatio-temporal coherency,\ntailored to human videos, containing rich global and local deformations, remains challenging.\n\n\\begin{figure*}[!ht]\n\\centering\n\\includegraphics[width=1.0\\textwidth]{cover-taichi4.pdf}\n\\includegraphics[width=1.0\\textwidth]{cover-face3.pdf}\n\\caption{Our framework caters a set of video synthesis tasks including (i) unconditional video generation (first and second row), (ii) conditional generation based on one single image (fourth row) and (iii) video editing from the starting image (third and fifth row). Results pertain to our model being trained on the datasets TaichiHD, FaceForensics and CelebV-HQ.}\n\\label{fig:cover}\n\\end{figure*}\n\nMotivated by this, we here propose an effective generative framework, placing emphasis on \\textit{spatio-temporal coherency} in \\textit{human video synthesis}. Having this in mind, a fundamental step has to do with the \\textit{disentanglement} of videos w.r.t. \\textit{appearance} and \\textit{motion}.\nPrevious approaches have tackled such disentanglement by two jointly trained distinct networks, respectively providing appearance and motion features \\cite{tulyakov2017mocogan,wang2020g3an,wang2021inmodegan,WANG_2020_WACV,digan},\nas well as by a two-phase generation pipeline that firstly aims at training an image generator, and then at training a temporal network to generate videos in the image generator's latent space \\cite{mocoganhd,tats,videogpt}.\nNevertheless, such approaches encompass limitations related to spatial artifacts (\\textit{e.g.}, distortions of body structures and facial identities in the same sequence), as well as temporal artifacts (\\textit{e.g.}, inter-frame semantic jittering), even in short generated videos of 16 frames. \nWe argue that such limitations stem from incomplete disentanglement of appearance and motion in the generation process.\nSpecifically, without predominant mechanisms or hard constraints to guarantee disentanglement, even a minor perturbation in the high-level semantics will be amplified and will lead to significant changes in the pixel space.\n\n\\begin{figure}[t!]\n\\centering\n\\includegraphics[width=0.47\\textwidth]{inference3.pdf}\n\\caption{\\textbf{Inference stage.} At the inference stage, LMDM firstly accepts a starting motion code $\\alpha_1$ and a sequence of noise-vectors as input, in order to generate a sequence of motion codes $\\mathbf{a}$, further utilized to synthesize a sequence of flow maps $\\phi_i$ by the pre-trained image animator. The output video is obtained in a warp-and-inpaint manner based on $x_1$ and $\\phi_i$.}\n\\label{fig:inference}\n\\end{figure}\n\nDeviating from the above and towards disentangling videos w.r.t. appearance and motion,\nin this paper we propose a novel framework for human video generation, referred to as LEO, streamlined to ensure strong \\textit{spatio-temporal coherency}.\nAt the core of this framework is a \\textit{sequence of flow maps}, representing \\textit{motion semantics},\nwhich inherently isolate motion from appearance.\nSpecifically, LEO incorporates a latent motion diffusion module (LMDM), as well as a flow-based image animator.\nIn order to synthesize a video, an initial frame is either provided externally for \\textit{conditional generation},\nor obtained by a generative module for \\textit{unconditional generation}.\nGiven such initial frame and a sequence of motion codes sampled from the LMDM, the flow-based image animator generates a sequence of flow maps, and proceeds to synthesize the corresponding sequence of frames in a \\textit{warp-and-inpaint} manner.\n\nThe \\textit{training} of LEO is decomposed into \\textit{two phases}. \\textit{Firstly}, we train the flow-based image animator to encode input images into low-dimensional latent motion codes,\nand map such codes to flow maps, which are used for reconstruction via warp-and-inpaint.\nTherefore, once trained, \nthe flow-based image animator naturally provides a space of motion codes that are strictly constrained to only containing motion-related information.\nAt the \\textit{second stage},\nupon the space provided by the image animator,\nwe train the LMDM to synthesize sequences of motion codes and capture \\textit{motion prior} in the training data.\nTo endow LEO with the ability to synthesize videos of arbitrary length beyond the short training videos,\nwe adopt a Linear Motion Condition (LMC) mechanism\nin LMDM. As opposed to directly synthesizing sequences of motion codes,\nLMC enables LMDM to synthesize sequences of residuals w.r.t. a starting motion code,\nin order for longer videos to be easily obtained by concatenating additional sequences of residuals.\n\nTo evaluate LEO,\nwe conduct extensive experiments pertained to three human video datasets,\nincluding TaichiHD~\\cite{fomm}, FaceForensics~\\cite{rossler2018faceforensics}, and CelebV-HQ~\\cite{zhu2022celebvhq}. \nCompared to previous video synthesis methods,\nLEO demonstrates a significantly improved spatio-temporal coherency,\neven on synthesized videos of length of 512 frames.\nIn addition,\nLEO shows great potential in two extended tasks,\nnamely \\textit{infinite-length video synthesis},\nas well as \\textit{video editing} of a style in a synthesized video, while maintaining the content of the original video.\n\n\\section{Related Works}\n\\textbf{Unconditional video generation} aims to generate videos by learning the full distribution of training dataset. Most of the previous works~\\cite{vondrick2016generating,saito2017temporal,tulyakov2017mocogan,wang2020g3an,wang:tel-03551913,wang2021inmodegan,clark2019adversarial, brooks2022generating} are built upon GANs~\\cite{goodfellow2014generative, radford2015unsupervised, brock2018large, karras2019style, stylegan2} towards benefiting from the strong performance of the image generator. Approaches~\\cite{NIPS2017_2d2ca7ee,li2018disentangled,bhagat2020disentangling,xie2020motion} based on VAEs~\\cite{kingma2013auto} were also proposed while only show results on toy datasets. \nRecently, with the progress of deep generative models (\\textit{e.g.}, VQVAE~\\cite{vqvae}, VQGAN~\\cite{vqgan}, GPT~\\cite{gpt} and Denoising Diffusion Models~\\cite{ddpm, ddim, nichol2021improved}) on both image~\\cite{dalle, dalle2} and language synthesis~\\cite{radford2019language}, as well as the usage of large-scale pre-trained models, video generation also started to be explored with various approaches.\n\nMoCoGANHD~\\cite{mocoganhd} builds the model on top of a well-trained StyleGAN2~\\cite{stylegan2} by integrating an LSTM in the latent space towards disentangling content and motion. DIGAN~\\cite{digan} and StyleGAN-V~\\cite{stylegan-v} and MoStGAN-V~\\cite{mostgan}, inspired by NeRF~\\cite{feichtenhofer2019slowfast}, proposed an implicit neural representation approach to model time as a continuous signal aiming for long-term video generation. VideoGPT~\\cite{videogpt} and TATS~\\cite{tats} introduced to first train 3D-VQ models to learn discrete spatio-temporal codebooks, which are then be refined temporally by modified transformers~\\cite{transformer}. Recently, several works{~\\cite{vdm,pvdm,videofusion}} have shown promising capacity to model complex video distribution by incorporating spatio-temporal operations in Diffusion Models. While previous approaches have proposed various attempts either in training strategies~\\cite{mocoganhd,videogpt,tats} or in model architectures~\\cite{wang2020g3an,wang2021inmodegan,digan,stylegan-v} to disentangle appearance and motion, due to the lack of strong constrains, it is still difficult to obtain satisfying results. \n\n\\begin{figure*}[!ht]\n\\centering\n\\includegraphics[width=1.0\\textwidth]{framework3.pdf}\n\\caption{\\textbf{Overview of LEO.} Our framework incorporates two main parts, (i) an image animator, aiming to generate flow maps and synthesize videos in the pixel space, and (ii) Latent Motion Diffusion Model (LMDM), focusing on modeling the motion distribution in a latent motion space. Our framework requires a two-phase training. In the first phase, we train the image animator in a self-supervised manner towards mapping latent codes to corresponding flow maps $\\phi_i$. Once the image animator is well-trained, motion codes $\\mathbf{a}$ are extracted from a frozen encoder and used as inputs of LMDM. In the second phase, LMDMs are trained to learn the motion distribution by providing the starting motion $\\alpha_1$ as condition. Instead of directly learning the distribution of $\\mathbf{a}$, we adopt a Linear Motion Condition (LMC) mechanism in LMDM towards synthesizing sequences of residuals with respect to $x_1$. At the inference stage, given a starting image $x_i$ and corresponding motion code $\\alpha_i$, LMDM firstly generates a motion code sequence, which is then used by the image animator to generate flow maps to synthesize output videos in a warp-and-inpaint manner.}\n\\label{fig:main-img}\n\\end{figure*}\n\nIn contrast to unconditional video generation, \\textbf{conditional video generation} seeks to produces high-quality videos, following image-to-image generation pipeline~\\cite{chu2017cyclegan,isola2017image, huang2018munit}. In this context, additional signals such as semantic maps~\\cite{pan2019video, wang2018vid2vid,wang2019fewshotvid2vid}, human key-points~\\cite{jang2018video, yang2018pose, walker2017pose, chan2019everybody, zakharov2019few, wang2019fewshotvid2vid, wang2021dance}, motion labels~\\cite{WANG_2020_WACV}, 3DMM~\\cite{Zhao_2018_ECCV, yang20223dhumangan} and optical flow~\\cite{li2018flow,ohnishi2018ftgan} have been exploited to guide motion generation. In addition, text description, has been used in large-scale video diffusion models~{\\cite{makeavideo,imagenvideo,videoLDM,lavie,vdm,show1,snapvideo,videocrafter2,latte,seine,svd}} for high-quality video generation. Our framework also supports for conditional video generation based on a single image. However, unlike previous approaches, our method follows the image animation pipeline~\\cite{fomm,mraa,wang2022latent} which leverages the dense flow maps for motion modeling. We introduce our method in details in the following. \n\n\\section{Method}\nFig.~\\ref{fig:main-img} illustrates the training of LEO, comprising of two-phases. We firstly train an image animator towards learning high-quality latent motion codes of the datasets. In the second phase, we train the Latent Motion Diffusion Model (LMDM) to learn a motion prior over the latent motion codes. To synthesize a video, the pre-trained image animator takes the motion codes to generate corresponding flow maps, which are used to warp and inpaint starting frame. The warp-and-inpaint operation is conducted in two modules inside image animator. The warping module firstly produces flow fields based on motion codes to warp starting frame, then the inpainting module learns to fill in the holes in the warped starting frame and refine the entire image. Each video sequence is produced frame by frame.\n\nWe formulate a video sequence $v = \\{x_i\\}^{L}_{i=1}, x_i\\sim \\mathcal{X}\\in\\mathbb{R}^{3\\times H\\times W}$ as $v = \\{\\mathcal{T}(x_1, G(\\alpha_i))\\}^{L}_{i=2}, \\alpha_i\\sim \\mathcal{A}\\in \\mathbb{R}^{1\\times N}$, where $x_i$ denotes the $i^{th}$ frame, $\\alpha_i$ denotes a latent motion code at timestep $i$, $G$ represents the generator in the image animator aiming to generate a flow map $\\phi_{i}$ from $\\alpha_{i}$.\n\n\\subsection{Learning Latent Motion Codes}\nTowards learning a frame-wise latent motion code, we adopt the state-of-the-art image animator LIA~\\cite{wang2022latent} as it enables to encode input images into corresponding motion codes.  LIA consists of two modules, an encoder $E$ and a generator $G$. During training, given a source image $x_s$ and a driving image $x_d$, $E$ encodes $x_s, x_d$ into a motion code $\\alpha = E(x_s, x_d)$, and $G$ generates a flow field $\\phi = G(\\alpha)$ from the code. LIA is trained in a self-supervised manner with the objective to reconstruct the driving image. \n\nTraining LIA in such a self-supervised manner brings two notable benefits for our framework, (i) it enables LIA to achieve high-quality perceptual results, and (ii) as a motion code is strictly equivalent to flow maps, there are guarantees that $\\alpha$ is only motion-related without any appearance interference. \n\n\\begin{figure*}[t!]\n\\centering\n\\begin{subfigure}[t]{1.0\\textwidth}\n\\centering\n\\includegraphics[width=1.0\\textwidth]{taichi-compare.pdf}\n\\caption{\\footnotesize{\\textbf{TaichiHD}}}\n\\end{subfigure}\n\\begin{subfigure}[t]{1.0\\textwidth}\n\\centering\n\\includegraphics[width=1.0\\textwidth]{face-compare.pdf}\n\\caption{\\footnotesize{\\textbf{FaceForensics}}}\n\\end{subfigure}\n\\caption{\\textbf{Qualitative Comparison.} We qualitatively compare LEO with DIGAN, TATS, StyleGAN-V on short video generation. The results indicate that on both (a) TaichiHD (128 and 256 resolutions) and (b) FaceForensics datasets, our proposed method achieves the best visual quality and is able to capture the human structure well. Other approaches either modify the facial structure (e.g., StyleGAN-V) or fail to generate a complete human body (e.g., TATS and DIGAN).}\n\\label{fig:qualitative}\n\\end{figure*} \n\n\\subsection{Leaning a Motion Prior}\nOnce LIA is well-trained on a target dataset, for any given video $v=\\{x_i\\}^{L}_{i=1}$, we are able to obtain a motion sequence $\\mathbf{a}=\\{\\alpha_i\\}^{L}_{i=1}$ with the frozen $E$. In the second phase of our training, we propose to learn a motion prior by temporal Diffusion Models. \n\nUnlike image synthesis, data in our second phase is a set of sequences. We firstly apply a temporal Diffusion Model for modeling the temporal correlation of $\\mathbf{a}$. The general architecture of this model is a 1D U-Net adopted from~\\cite{ddpm}. To train this model, we follow the standard training strategy with a simple mean-squared loss,\n\\begin{align}\n    L_{\\text{LMDM}} = \\mathbb{E}_{\\epsilon \\sim \\mathcal{N} (0,1), t}\\left [ \\left \\| \\epsilon - \\epsilon_{\\theta}(\\mathbf{a}_t, t)\\right \\|^{2}_{2}\\right],  \n\\end{align}\nwhere $\\epsilon$ denotes the unscaled noise, $t$ is the time step, $\\mathbf{a}_t$ is the latent noised motion code to time $t$. During inference, a random Gaussian noise $\\mathbf{a}_T$ is iteratively denoised to $\\mathbf{a}_0=\\{\\alpha_i\\}^{L}_{i=1}$, and the final video sequence is obtained through the generator. %$v^{'} = \\{\\mathcal{T}(x_1, G(\\alpha_{i}))\\}^{L}_{i=1}$.\n\nAt the same time in our experiments, we found that learning motion sequences in a complete unconditional manner brings to the fore limitations, namely (i) the generated codes are not consistent enough for producing smooth videos, as well as (ii) the generated motion codes can only be used to produce fixed length videos. Hence, towards addressing those issues, we propose a \\textbf{conditional Latent Motion Diffusion Model (cLMDM)} which aims for high-quality and long-term human videos.    \n\nOne major characteristic of LIA has to do with the linear motion space. Any motion code $\\alpha_t$ in $\\mathbf{a}$ can be re-formulated as\n\\begin{equation}\\label{eq:2}\n    \\alpha_{i} = \\alpha_{1} + m_{i}, i\\ge 2,\n\\end{equation}\nwhere $\\alpha_{1}$ denotes the motion code at the first timestep and $m_{i}$ denotes the motion difference between timestep $1$ and $i$, so that we can re-formulate $\\mathbf{a}$ as\n\\begin{equation}\\label{eq:3}\n    \\mathbf{a} = \\alpha_{1} + \\mathbf{m},\n\\end{equation}\nwhere $\\mathbf{m}=\\{m_i\\}^{L}_{i=2}$ denotes the motion difference sequence. Therefore, Eq.~\\ref{eq:2} and \\ref{eq:3} indicate that a motion sequence can be represented by $\\alpha_{1}$ and $\\mathbf{m}$. Based on this, we propose a \\textbf{Linear Motion Condition (LMC)} mechanism in cLMDM to condition the generative process with $\\alpha_1$. During training, at each time step, we only add noise onto $\\mathbf{m_t}$ instead of the entire $\\mathbf{a}$ and leave $\\alpha_1$ intact. The objective function of cLMDM is\n\\begin{equation}\n\\begin{split}\n    L_{\\text{cLMDM}} &= \\mathbb{E}_{\\epsilon \\sim \\mathcal{N} (0,1), t}\\left [ \\left \\| \\epsilon - \\epsilon_{\\theta}(\\mathbf{m}_{t}, \\alpha_1, t)\\right \\|^{2}_{2}\\right], \\\\\n\\end{split}\n\\end{equation}\nwhere $\\alpha_{1}$ denotes the condition signal and $\\mathbf{m}_t$ stands for the noised $\\mathbf{m}$ to time $t$. $\\alpha_1$ is first added on $\\mathbf{m}_t$ and then concatenated along temporal dimension. LMC will be applied at each time step until we reach $\\mathbf{m}_0$. The final motion sequence is obtained as $\\mathbf{a}_0 = [\\alpha_1, \\mathbf{m}_0]$. We find that following this, a related generated motion sequence is more stable and contains fewer artifacts, as $\\alpha_1$ serves as a strong signal to constrain the generated $\\mathbf{m}$ to follow the initial motion. \n\nWhile the results from cLMDM outperforms previous models, the groundtruth $\\alpha_1$ is necessitated during both, training and inference stage. Towards \\textit{unconditional generation}, we train an additional simple DM to fit the distribution $p(\\alpha_i)$ in a frame-wise manner. We refer to the cLMDM and such simple DM jointly as \\textbf{Latent Motion Diffusion Model (LMDM)}. By this way, LMDM are able to work in both conditional and unconditional motion generation. \n\nTowards generating videos of arbitrary length, we propose an autoregressive approach based on proposed LMDM. By taking the last motion code from the previous generated sequence as the $\\alpha_1$ in the current sequence, with a randomly sampled noise, LMDM are able to generate an infinite-length motion sequence. By combining such sequence in pre-trained LIA with a starting image, LEO can synthesize photo-realistic and long-term videos.\n\n\\begin{figure*}[t!]\n\\centering\n\\includegraphics[width=1.0\\textwidth]{long-term.pdf}  \n\\caption{\\textbf{Comparison on long-term video generation.} We compare with TATS by generating 512-frame videos. Videos from TATS start crashing around 50 frames while our model is able to continue producing high-quality frames with diverse motion.}\n\\label{fig:long-term}\n\\end{figure*}\n\n\\subsection{Learning Starting Frames}\\label{sec:img-cond}\nIn our framework, a starting image $x_1$ is required to synthesize a video. As image space is modeled independently, here we propose two options to obtain $x_1$.\n\n\\textbf{Option 1: existing images.} The first option is to directly take the images either from a real distribution or from an image generation network. In this context, our model is a conditional video generation model, which learns to predict future motion from $x_1$. Starting motion $\\alpha_1$ is obtained through $\\alpha_1 = E(x_1)$.\n\n\\textbf{Option 2: conditional Diffusion Models.} The second option is to learn a conditional DDPM~\\cite{ddpm} (cDDPM) with $\\alpha_1$ as a condition to synthesize $x_1$. By combining LEO with LMDM as well as cDDPM, we are able to conduct unconditional video synthesis.\n\n\\section{Experiments}\nIn this section, we firstly briefly describe our experimental setup, introducing datasets, evaluation metrics and implementation details. Secondly, we qualitatively demonstrate generated results on both, short and long video synthesis. Then we show quantitative evaluation w.r.t. video quality, comparing LEO with SoTA. Next, we conduct an ablation study to prove the effectiveness of proposed conditional mechanism LMC. Finally, we provide additional analysis of our framework, exhibiting motion and appearance disentanglement, video editing and infinite-length video generation. \n\n\\textbf{Datasets.} As we focus on human video synthesis, evaluation results are reported on three human-related datasets, TaichiHD~\\cite{fomm}, FaceForensics~\\cite{rossler2018faceforensics} and CelebV-HQ~\\cite{zhu2022celebvhq}. We \nuse both $128\\times 128$ and $256\\times 256$ resolution TaichiHD datasets, and only $256\\times 256$ resolution FaceForensics and CelebV-HQ datasets.\n\n\\begin{itemize}\n    \\item \\textbf{TaichiHD}~\\cite{fomm} comprises 3100 video sequences downloaded from YouTube. In train and test splits, it contains 2815 and 285 videos, respectively. We conducted all our experiments on the train split and used both $128\\times 128$ and $256\\times 256$ resolutions in our experiments.\n    \\item \\textbf{FaceForensics}~\\cite{rossler2018faceforensics} includes 1000 video sequences downloaded from YouTube. Following the preprocessing of previous methods~\\citep{TGAN2020, stylegan-v}, face areas are cropped based on the provided per-frame meshes. We resized all videos to $256\\times 256$ resolution.\n    \\item \\textbf{CelebV-HQ}~\\cite{zhu2022celebvhq} comprises 35666 high-quality talking head videos of 3 to 20 seconds each. In total, it represents 15653 celebrities. We resized the original videos to $256\\times 256$ resolution, in order to train our models. \n\\end{itemize}\n\n\\textbf{Evaluation metric.} For quantitative evaluation, we apply the commonly used metrics FVD and KVD, in order to compare with other approaches on video quality and apply Average Content Distance (ACD) towards evaluating the identity consistency of faces and bodies in the generated videos. In addition, we conduct a user study with 20 users towards comparing with objective quantitative evaluation. \n\n\\begin{itemize}\n\\item \\textbf{Frechet video distance (FVD) and Kernel Video Distance (KVD).} We use I3D~\\citep{carreira2017quo} trained on Kinetics-400 as feature extractor to compute FVD and KVD. However, we find FVD is a very sensitive metric, which can be affected by many factors such as frame-rate, single image quality, video length and implementation, which also mentioned in~\\citep{stylegan-v}. Therefore, towards making a fair comparison, on the TaichiHD dataset, we adopt the implementation from DIGAN~\\citep{digan}. As for FaceForensics and CelebV-HQ, we chose to follow the implementation of StyleGAN-V~\\citep{stylegan-v}. \n\\item \\textbf{Average Content Distance (ACD).} ACD measures the content consistency in generated videos. To evaluate results from FaceForensics and TaichiHD, we extract features from each generated frame and proceed to extract a per-frame feature vector in a video. The ACD was then computed using the average pairwise L2 distance of the per-frame feature vectors. We follow the implementation in~\\citep{mocoganhd} to compute ACD for FaceForensics. As for TaichiHD, we employ the pre-trained person-reID model~\\citep{zheng2018discriminatively} to extract person identity features.\n\\item \\textbf{User study.} We asked 20 human raters to evaluate generated video quality, as well as video coherency. In each user study, we show paired videos and ask the raters, to rate 'which clip is more realistic / which clip is more coherent'. Each video-pair contains one generated video from our method, whereas the second video is either \\textit{real} or generated from other methods.\n\n\\end{itemize}\n\n\\textbf{Implementation details.} Our framework requires two-phase training. In the first phase, we follow the standard protocol to train LIA~\\citep{wang2022latent} to encode input images into low-dimensional latent motion codes, and map such codes to flow maps, which are used for reconstruction via warp-and-inpaint. Therefore, once trained, LIA naturally provides a space of motion codes that are strictly constrained to only containing motion-related information. In the second phase, we only train LMDM on the extracted motion codes from Encoder. We note that the LMDM is a 1D U-Net adopted from~\\citep{nichol2021improved}, we set the input size as $64\\times 20$, where 64 is the length of the sequence and 20 is the dimension of the motion code. We use 1000 diffusion steps and a learning rate of $1\\mathrm{e}{-4}$. As the training of LMDM is conducted in the latent space of LIA, the entire training is very efficient and only requires one single GPU. \n\n\\subsection{Qualitative Evaluation}\nWe qualitatively compare LEO with SoTA by visualizing the generated results. We firstly compare our method with DIGAN, TATS and StyleGAN-V on the FaceForensics and TaichiHD datasets for \\textit{short video generation}. As shown in Fig.~\\ref{fig:cover} and~\\ref{fig:qualitative}, the visual quality of our generated results outperforms other approaches w.r.t both, appearance and motion. For both resolutions on TaichiHD datasets, our method is able to generate complete human structures, whereas both, DIGAN and TATS fail, especially for arms and legs. When compared with StyleGAN-V on FaceForensics dataset, we identify that while LEO preserves well facial structures, StyleGAN-V modifies such attributes when synthesizing large motion. \n\nSecondly, we compare with TATS for long-term video generation. Specifically, 512 frames are produced for the resolution $128\\times 128$ pertained to the TaichiHD dataset. As shown in Fig.~\\ref{fig:long-term}, the subject in the videos from TATS starts crashing around 50 frames and the entire video sequence starts to fade. On the other hand, in our results, the subject continues to perform diverse actions whilst well preserving the human structure. We note that our model is only trained using a 64-frame sequence.\n\n\\begin{table*}[!t]\n\\begin{center}\n\\setlength\\arrayrulewidth{1pt}\n\\scalebox{0.8}{\n\\begin{tabular}{ccccccccccccc}\n\\hline\n& \\multicolumn{4}{c}{TaichiHD128} & \\multicolumn{3}{c}{TaichiHD256} & \\multicolumn{3}{c}{FaceForensics} & \\multicolumn{1}{c}{CelebV-HQ} \\\\\nMethod & $\\text{FVD}_{16}$ & $\\text{KVD}_{16}$ & $\\text{ACD}_{16}$ && $\\text{FVD}_{16}$ & $\\text{KVD}_{16}$ && $\\text{FVD}_{16}$ & $\\text{ACD}_{16}$ && $\\text{FVD}_{16}$   \\\\\n\\cmidrule{2-4}\\cmidrule{6-7}\\cmidrule{9-10}\\cmidrule{12-12}\nMoCoGAN-HD & $144.7\\pm 6.0$ & $25.4\\pm 1.9$ & - && - & - && 111.8 & 0.33 && 212.4  \\\\\nDIGAN & $128.1\\pm 4.9$ & $20.6\\pm 1.1$ & 2.17 && $156.7\\pm 6.2$ & - && 62.5 & - && 72.9  \\\\\nTATS & ${136.5\\pm1.2}^{*}$ & ${22.2\\pm1.0}^{*}$ & 2.28 && - & - && - & - && -  \\\\\nStyleGAN-V & - & - & - && - & - && 47.4 & 0.36 && 69.1  \\\\\nMoStGAN-V & - & -& - && - & - && 39.7 & 0.38 && 132.1 \\\\\n\\cmidrule{1-12}\nOurs (uncond) & $100.4\\pm 3.1$ & $11.4\\pm 3.2$ & 1.83 && $122.7\\pm 1.1$ & $20.49\\pm 0.9$ && 52.3 & 0.28 && -  \\\\\nOurs (cond) & $\\mathbf{57.6\\pm 2.0}$ & $\\mathbf{4.0\\pm 1.5}$ & \\textbf{1.22} && $\\mathbf{94.8\\pm 4.2}$ & $\\mathbf{13.47\\pm 2.3}$ && \\textbf{35.9} & \\textbf{0.27} && \\textbf{40.2}  \\\\\n\\hline\n\\end{tabular}}\n\\end{center}\n\\caption{\\textbf{Evaluation for unconditional and conditional short video generation.} LEO systematically outperforms other approaches on conditional video generation, and achieves better or competitive results on unconditional generation w.r.t. FVD, KVD and ACD. (*results are reproduced based on official code and released checkpoints.)}\n\\label{tab:fvd-cond-uncond-short}\n\\end{table*}\n\n\\begin{table}[!t]\n\\begin{center}\n\\setlength\\arrayrulewidth{0.8pt}\n\\scalebox{0.68}{\n\\begin{tabular}{ccccccc}\n\\hline\n& \\multicolumn{4}{c}{TaichiHD128} & \\multicolumn{2}{c}{FaceForensics} \\\\\nMethod & $\\text{FVD}_{128}$ & $\\text{KVD}_{128}$ & $\\text{ACD}_{128}$ && $\\text{FVD}_{128}$ & $\\text{ACD}_{128}$ \\\\\n\\cmidrule{2-4}\\cmidrule{6-7}\nDIGAN & - & - & - && 1824.7 & - \\\\\nTATS & $1194.58\\pm 1.1$ & $462.03\\pm 8.2$ & 2.85 && - & -\\\\\nStyleGAN-V & - & - & - && \\textbf{89.34} & 0.49 \\\\\n\\cmidrule{1-7}\nOurs & $\\mathbf{155.54\\pm 2.6}$ & $\\mathbf{48.82\\pm 5.9}$ & \\textbf{2.06} && 96.28 & \\textbf{0.34}\\\\\n\\hline\n\\end{tabular}}\n\\end{center}\n\\caption{\\textbf{Evaluation for unconditional long-term video generation.} LEO outperforms other methods on long-term (128 frames) video generation w.r.t. FVD, KVD and ACD.}\n\\label{tab:fvd-uncond-long}\n\\end{table}\n\n\\subsection{Quantitative evaluation}\nIn this section, we compare our framework with five state-of-the-art for both, conditional and unconditional short video generation, as well as unconditional long-term video generation.\n\n\\textbf{Unconditional short video generation.} In this context, as described in Sec.~\\ref{sec:img-cond}, Option 2, the $x_1$ is randomly generated by a pre-trained cDDPM. We compare with SoTA by generating 16 frames. To compare with DIGAN on high-resolution generation, we also generate videos of $256\\times 256$ resolution. Related FVDs and KVDs are reported in Tab.~\\ref{tab:fvd-cond-uncond-short}. LEO systematically outperforms other methods w.r.t. video quality, obtaining lower or competitive FVD and KVD on all datasets. On high-resolution generation, our results remain better than DIGAN. \n\nHowever, by comparing the results between StyleGAN-V and ours, we find FVD is not able to represent the quality of generated videos veritably. We observe that StyleGAN-V is not able to preserve facial structures, whereas LEO is able to do so, see Fig.~\\ref{fig:qualitative}. We additionally compute ACD, in order to further analyze the identity consistency in 16-frame videos. Tab.~\\ref{tab:fvd-cond-uncond-short} reflects on the fact that our method achieves significantly better results compared to other approaches. In addition, we conduct user study \\textit{w.r.t.} video quality and coherency of generated videos among different methods. Results in Tab.~{\\ref{tab:user-study}} showcase that as nearly all users rated for our generated results to be superior than other approaches. Hence, we conclude that a metric, replacing FVD is in urgent need in the context of video generation.\n\n\\textbf{Unconditional long video generation} \nWe evaluate our approach for long-term video generation w.r.t. FVD and ACD. In this context, we compare LEO with StyleGAN-V on the FaceForensics dataset, and both DIGAN and TATS on the TaichiHD. We report results based on 128-frame generation in Tab.~\\ref{tab:fvd-uncond-long}, which clearly shows that our method outperforms others in such context. We hypothesize that consistent and stable motion codes produced by our LMDM are key to producing high-quality long-term videos.\n\n\\textbf{Conditional short video generation}\nAs described in Sec.~\\ref{sec:img-cond}, Option 1, our framework additionally caters for conditional video generation by taking an existing image to hallucinate the following motion. Specifically, we randomly select 2048 images from both, TaichiHD and FaceForensics datasets as $x_1$ and compute corresponding $\\alpha_1$ as input of LMDM. As depicted in Tab.~\\ref{tab:fvd-cond-uncond-short}, results conditioned on the real images achieve the lowest FVD, KVD and ACD values, suggesting that the quality of a starting image is pertinent for output video quality, which further signifies that in the setting of unconditional generation, training a better cDDPM will be instrumental for improving results.\n\n\\begin{figure*}[t!]\n\\centering\n\\includegraphics[width=1.0\\textwidth]{distanglement.pdf}  \n\\caption{\\textbf{Disentanglement of motion and appearance.} The first and second row share the same appearance, with different motion codes. Results display that our model is able to produce diverse motion from the same content.}%, demonstrating the ability of LEO to disentangle appearance and motion.}\n\\label{fig:disentanglement}\n\\end{figure*}\n\n\\begin{figure*}[t!]\n\\centering\n\\includegraphics[width=1.0\\textwidth]{video-edit-v2.pdf}  \n\\caption{\\textbf{Video editing.} We show video editing results by combining LEO with off-the-shelf image editing model ControlNet. We are able to edit the appearance of the entire video sequence through only editing the starting image.}\n\\label{fig:semantic-edit-taichi}\n\\end{figure*}\n\n\\begin{table}[!t]\n\\begin{minipage}[t]{0.45\\textwidth}\n\\centering\n\\setlength{\\tabcolsep}{3.5pt}\n\\setlength\\arrayrulewidth{1pt}\n\\scalebox{0.9}{\n\\begin{tabular}{cccc}\n\\hline\nMethod & TaichiHD (\\%) & FaceForensics (\\%) \\\\\n\\cmidrule{1-3}\nOurs / TATS & \\textbf{93.00} / 7.00  & - \\\\\nOurs / StyleGAN-V  & - & \\textbf{91.33} / 8.67 \\\\\n\\hline\n\\hline\nMethod & TaichiHD (\\%) & FaceForensics (\\%) \\\\\n\\cmidrule{1-3}\nOurs / TATS & \\textbf{98.60} / 1.40  & - \\\\\nOurs / StyleGAN-V  & - & \\textbf{93.20} / 6.80 \\\\\n\\hline\n\\end{tabular}}\n\\captionof{table}{\\textbf{User study.} We conduct user studies pertaining to the datasets TaichiHD and FaceForensics \\textit{w.r.t.} video quality (up) as well as coherency (down).}\n\\label{tab:user-study}\n\\end{minipage}\n\\begin{minipage}[t]{0.05\\textwidth}\n\\quad\n\\end{minipage}\n\\begin{minipage}[t]{0.45\\textwidth}\n\\centering\n\\setlength\\arrayrulewidth{1pt}\n\\scalebox{1.0}{\n\\begin{tabular}{ccc}\n\\hline\n & TaichiHD & FaceForensics \\\\\n\\cmidrule{1-3}\nw/o LMC & 118.6 & 60.03 \\\\\nwith LMC & \\textbf{100.4} & \\textbf{52.32} \\\\\n\\hline\n\\end{tabular}}\n\\captionof{table}{\\textbf{Ablation study of proposed LMC.} Models with LMC achieved the lowest FVD on both datasets.}\n\\label{tab:ablation-lmc}\n\\end{minipage}\n\\end{table}\n\n\\section{Ablation Study}\nIn this section, we place emphasis on analyzing the effectiveness of proposed Linear Motion Condition (LMC) in  LMDM. We train two models, with and without LMC on both TaichiHD and FaceForensics datasets. As shown in Tab.~\\ref{tab:ablation-lmc}, using LMC significantly improves the generated video quality, which proves that our proposed LMC is an effective mechanism for involving $\\alpha_1$ in LMDM.\n\n\\section{Additional Analysis}\n\n\\textbf{Motion and appearance disentanglement.} We proceed to combine the same $x_1$ with different $\\mathbf{m}$, aiming to reveal whether $\\mathbf{m}$ is only motion-related. Fig.~\\ref{fig:disentanglement} illustrates that different $\\mathbf{m}$ enables the same subject to perform different motion - which proves that our proposed LMDM is indeed learning a motion space, and appearance and motion are clearly disentangled. This experiment additionally indicates that our model does not overfit on the training dataset, as different noise sequences are able to produce diverse motion sequences.\n\n\\textbf{Video Editing.} As appearance is modeled in $x_1$, we here explore the task of video editing by modifying the semantics in thestarting image. Compared to previous approaches, where image-to-image translation is required, our framework simply needs an edit of the semantics in an one-shot manner. Associated results are depicted in Fig.~\\ref{fig:cover} and Fig.~\\ref{fig:semantic-edit-taichi}. We apply the open-source approach ControlNet~\\cite{controlnet} on the starting frame by entering various different prompts. Given that the motion space is fully disentangled from the appearance space, our videos maintain the original temporal consistency, uniquely altering the appearance. \n\n\\textbf{Infinite-length video generation.}  \nIn addition to presented settings, our framework is able to generate infinite-length videos. To generate long-term FaceForensics, as shown in Fig.~\\ref{fig:long-term-face}, we provide the last generated code from the previous sequence as the starting code of the current sequence. The entire long-term video is generated in an \\textit{autoregressive} manner. Surprisingly, we find that such a simple approach is sufficient to produce more than 1000 frames. We note that for TaichiHD dataset, due to limited motion patterns, this setting yields repeated motion. Towards addressing this limitation, as shown in Fig.~\\ref{fig:long-term-taichi}, we design an additional \\textit{Transition Diffusion Model (Transition DM)} aimed at generating transition motion between the last code from original generated sequence and a new motion code generated from the \\textit{simple DM}. Doing so, the Transition DM enforces the network to exit the original motion pattern and transit to new pattern. To evaluate the effectiveness of the proposed method, we generate long videos \\textit{with} and \\textit{without} Transition DM and request human raters to watch respective videos and answer the question `Does the clip contain repeated motion?'. Results are reported in Tab.~{\\ref{tab:tdm}}, which shows the effectiveness of Transition DM to prevent repeated motion.\n\n\\begin{table}[!h]\n\\centering\n\\setlength{\\tabcolsep}{25pt}\n\\setlength\\arrayrulewidth{1pt}\n\\begin{tabular}{cc}\n\\hline\n & Occurrence (\\%) \\\\\n\\cmidrule{1-2}\nw/o Transition DM & 0.45 \\\\\nwith Transition DM & 0.02 \\\\\n\\hline\n\\end{tabular}\n\\caption{\\textbf{User study of repeated motion.} We show the occurrence of repeated motion with and without the usage of Transition DM.}\n\\label{tab:tdm}\n\\end{table}\n\n\\textbf{\\begin{figure*}[th!]\n    \\centering\n    \\includegraphics[width=0.9\\textwidth]{infinite-fig-taichi.pdf}\n    \\caption{Infinite-length video generation for TaichiHD.}\n    \\label{fig:long-term-taichi}\n\\end{figure*}}\n\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=0.47\\textwidth]{infinite-fig-face.pdf}\n    \\caption{Infinite-length video generation for FaceForensics.}\n    \\label{fig:long-term-face}\n\\end{figure}\n\nCompared to current diffusion-based methods, our approach is very efficient in long video generation. We autoregressively run LMDM for the generation of long motion code sequences in latent space. Given that LMDM is a small-scale network, which only focuses on generating 1-D motion code, even for very long sequence generation, it only requires few seconds during inference stage. In addition, the image animator itself is a one-step inference model which enables our proposed method to be significantly efficient.\n\n\\section{Limitations}\nWe list several limitations in current framework and proposed potential solutions for future work.\n\\begin{itemize}\n\\item \\textit{Geometry ambiguity and temporal coherency.} Since we use a 2D generator to predict 2D flow maps, LEO is not able to handle human body occlusion very well especially in Taichi dataset. One solution would be to incorporate the architecture of NeRF or Tri-plane into our generator to support 3D-aware generation. We think in this way, the issues of geometry ambiguity and human body occlusion could be addressed.\n\\item \\textit{Generalizability.} Since the pre-trained image animator focuses on talking head and human bodies, our proposed framework currently performs better on human-centric videos. However, to analyze the generalizability of LEO, we conducted a small-scale experiment on UCF101 and report quantitative evaluation in Tab.~{\\ref{tab:ucf}}. The results show that under current model design, LEO achieves competitive results with previous GAN-based methods but still has large performance gap compared with large-scale video diffusion models. \n\nWe believe our framework is pushing the boundaries of video generation, as it solves a challenge, which constitutes generation of long human-centric videos. While this is a first step, the proposed method has the potential to generalize onto additional settings such as text-to-video generation. However, achieving such goals requires scaling up and re-designing (a) the original LIA, as well as (b) LMDM, and (c) training the entire system on larger-scale well-curated video datasets, which requires extremely expensive computational resources. We will explore such research directions in our future work.\n\n\\begin{table}[!h]\n\\centering\n\\setlength{\\tabcolsep}{25pt}\n\\setlength\\arrayrulewidth{1pt}\n\\begin{tabular}{cc}\n\\hline\nMethods & $\\text{FVD}_{16}$ \\\\\n\\cmidrule{1-2}\nMoCoGAN-HD & 1729.6  \\\\\nDIGAN & 1630.2 \\\\\nStyleGAN-V & 1431.0 \\\\\n\\cmidrule{1-2}\nMake-A-Video & 367.23 \\\\\nVideo LDM & 550.61 \\\\\nLaVie & 540.30 \\\\\n\\cmidrule{1-2}\nOurs & 1356.2 \\\\\n\\hline\n\\end{tabular}\n\\caption{Quantitative evaluation on UCF101 \\textit{w.r.t.} FVD.}\n\\label{tab:ucf}\n\\end{table}\n\n\\item \\textit{Architecture.} Current architect of LEO still relies on convolutional networks in both image animator and latent motion diffusion models. Advanced techniques such as transformers have not been explored yet. Future work would be involving novel architecture design and training LEO on larger-scale dataset to explore the limits of current approach.\n\\end{itemize}\n\n\\section{Conclusions}\nIn this paper, we introduced LEO, a novel framework incorporating a Latent Image Animator (LIA), as well as a Latent Motion Diffusion Model (LMDM), placing emphasis on spatio-temporal coherency in human video synthesis. By jointly exploiting LIA and LMDM in a two-phase training strategy, we endow LEO with the ability to disentangle appearance and motion. We quantitatively and qualitatively evaluated proposed method on both, human body and talking head datasets and demonstrated that our approach is able to successfully produce photo-realistic, long human videos. In addition, we showcased that the effective disentanglement of appearance and motion in LEO allows for two additional tasks, namely infinite-length human video synthesis by autoregressively applying LMDM, as well as content-preserving video editing (employing an off-the-shelf image editor (e.g., ControlNet)). We postulate that LEO opens a new door in design of generative models for video synthesis and plan to extend our method onto more general videos and applications.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation}\n\n\\begin{document}\n\n\\twocolumn[{%\n\\renewcommand\\twocolumn[1][]{#1}%\n\\maketitle\n\\begin{center}\n    \\centering\n    \\captionsetup{type=figure}\n    \\includegraphics[width=1.0\\textwidth]{./figure/f1_final.pdf}\n    \\captionof{figure}{Consistent and controllable character animation results given reference image (the leftmost image in each group) . Our approach is capable of animating arbitrary characters, generating clear and temporally stable video results while maintaining consistency with the appearance details of the reference character. }\n    \\label{fig:f1}\n\\end{center}%\n}]\n\n\\maketitle\n\n\\begin{abstract}\n\nCharacter Animation aims to generating character videos from still images through driving signals. Currently, diffusion models have become the mainstream in visual generation research, owing to their robust generative capabilities. However, challenges persist in the realm of image-to-video, especially in character animation, where temporally maintaining consistency with detailed information from character remains a formidable problem. In this paper, we leverage the power of diffusion models and propose a novel framework tailored for character animation. To preserve consistency of intricate appearance features from reference image, we design ReferenceNet to merge detail features via spatial attention. To ensure controllability and continuity, we introduce an efficient pose guider to direct character's movements and employ an effective temporal modeling approach to ensure smooth inter-frame transitions between video frames. By expanding the training data, our approach can animate arbitrary characters, yielding superior results in character animation compared to other image-to-video methods. Furthermore, we evaluate our method on image animation benchmarks, achieving state-of-the-art results.\n\n\\end{abstract}\n\n\\section{Introduction}\n\nCharacter Animation is a task to animate source character images into realistic videos according to desired posture sequences, which has many potential applications such as online retail, entertainment videos, artistic creation and virtual character. Beginning with the advent of GANs\\cite{gan,wgan,stylegan}, numerous studies have delved into the realms of image animation and pose transfer\\cite{fomm,mraa,ren2020deep,tpsmm,siarohin2019animating,zhang2022exploring,bidirectionally,everybody}. However, the generated images or videos still exhibit issues such as local distortion, blurred details, semantic inconsistency, and temporal instability, which impede the widespread application of these methods.\n\nIn recent years, diffusion models\\cite{denoising} have showcased their superiority in producing high-quality images and videos. Researchers have begun exploring human image-to-video tasks by leveraging the architecture of diffusion models and their pretrained robust generative capabilities. DreamPose\\cite{dreampose} focuses on fashion image-to-video synthesis, extending Stable Diffusion\\cite{ldm} and proposing an adaptar module to integrate CLIP\\cite{clip} and VAE\\cite{vae} features from images. However, DreamPose requires finetuning on input samples to ensure consistent results, leading to suboptimal operational efficiency. DisCo\\cite{disco} explores human dance generation, similarly modifying Stable Diffusion, integrating character features through CLIP, and incorporating background features through ControlNet\\cite{controlnet}. However, it exhibits deficiencies in preserving character details and suffers from inter-frame jitter issues. \n\nFurthermore, current research on character animation predominantly focuses on specific tasks and benchmarks, resulting in a limited generalization capability. \nRecently, benefiting from advancements in text-to-image research\\cite{dalle2,glide,imagen,ldm,composer,ediffi}, video generation (e.g., text-to-video, video editing)\\cite{animatediff,cogvideo,fatezero,imagenvideo,text2videozero,tuneavideo,videocomposer,align,gen1,makeavideo,vdm} has also achieved notable progress in terms of visual quality and diversity.\nSeveral studies extend text-to-video methodologies to image-to-video\\cite{videocomposer,videocrafter1,i2vgen,animatediff}. \nHowever, these methods fall short of capturing intricate details from images, providing more diversity but lacking precision, particularly when applied to character animation, leading to temporal variations in the fine-grained details of the character's appearance. Moreover, when dealing with substantial character movements, these approaches struggle to generate a consistently stable and continuous process. \nCurrently, there is no observed character animation method that simultaneously achieves generalizability and consistency.\n\nIn this paper, we present \\textit{Animate Anyone}, a method capable of transforming character images into animated videos controlled by desired pose sequences. \nWe inherit the network design and pretrained weights from Stable Diffusion (SD) and modify the denoising UNet\\cite{unet} to accommodate multi-frame inputs.\nTo address the challenge of maintaining appearance consistency, we introduce ReferenceNet, specifically designed as a symmetrical UNet structure to capture spatial details of the reference image. At each corresponding layer of the UNet blocks, we integrate features from ReferenceNet into the denoising UNet using spatial-attention\\cite{attention}. This architecture enables the model to comprehensively learn the relationship with the reference image in a consistent feature space, which significantly contributes to the improvement of appearance details preservation. \nTo ensure pose controllability, we devise a lightweight pose guider to efficiently integrate pose control signals into the denoising process. \nFor temporal stability, we introduce temporal layer to model relationships across multiple frames, which preserves high-resolution details in visual quality while simulating a continuous and smooth temporal motion process.\n\nOur model is trained on an internal dataset of 5K character video clips. Fig.~\\ref{fig:f1} shows the animation results for various characters. Compared to previous methods, our approach presents several notable advantages. \nFirstly, it effectively maintains the spatial and temporal consistency of character appearance in videos. Secondly, it produces high-definition videos without issues such as temporal jitter or flickering. Thirdly, it is capable of animating any character image into a video, unconstrained by specific domains. \nWe evaluate our method on three specific human video synthesis benchmarks (UBC fashion video dataset\\cite{dwnet}, TikTok dataset\\cite{tiktok} and Ted-talk dataset\\cite{mraa}), using only the corresponding training datasets for each benchmark in the experiments. Our approach achieves state-of-the-art results.\nWe also compare our method with general image-to-video approaches trained on large-scale data and our approach demonstrates superior capabilities in character animation. \nWe envision that \\textit{Animate Anyone} could serve as a foundational solution for character video creation, inspiring the development of more innovative and creative applications.\n\n\\begin{figure*}[!t]\n\\begin{center}\n\t\\setlength{\\fboxrule}{0pt}\n\t\\fbox{\\includegraphics[width=0.99\\textwidth]{./figure/f2_final.pdf}}\n\\end{center}\n\\vspace{-0.5cm}\n\\caption{The overview of our method. The pose sequence is initially encoded using Pose Guider and fused with multi-frame noise, followed by the Denoising UNet conducting the denoising process for video generation. The computational block of the Denoising UNet consists of Spatial-Attention, Cross-Attention, and Temporal-Attention, as illustrated in the dashed box on the right. The integration of reference image involves two aspects. Firstly, detailed features are extracted through ReferenceNet and utilized for Spatial-Attention. Secondly, semantic features are extracted through the CLIP image encoder for Cross-Attention. Temporal-Attention operates in the temporal dimension. Finally, the VAE decoder decodes the result into a video clip.}\n\\vspace{-0.3cm}\n\\label{fig:overview}\n\\end{figure*}\n\n\\section{Related Works}\n\n\\subsection{Diffusion Model for Image Generation }\nIn text-to-image research, diffusion-based methods\\cite{dalle2,imagen,ldm,glide,ediffi,composer} have achieved significantly superior generation results, becoming the mainstream of research. To reduce computational complexity, Latent Diffusion Model\\cite{ldm} proposes denoising in the latent space, striking a balance between effectiveness and efficiency. ControlNet\\cite{controlnet} and T2I-Adapter\\cite{t2iadapter} delve into the controllability of visual generation by incorporating additional encoding layers, facilitating controlled generation under various conditions such as pose, mask, edge and depth. Some studies further investigate image generation under given image conditions. IP-Adapter\\cite{ip} enables diffusion models to generate image results that incorporate the content specified by a given image prompt. ObjectStitch\\cite{objectstitch} and Paint-by-Example\\cite{paint} leverage the CLIP\\cite{clip} and propose diffusion-based image editing methods given image condition. TryonDiffusion\\cite{tryondiffusion} applies diffusion models to the virtual apparel try-on task and introduces the Parallel-UNet structure. \n\n\\subsection{Diffusion Model for Video Generation }\nWith the success of diffusion models in text-to-image applications, research in text-to-video has extensively drawn inspiration from text-to-image models in terms of model structure. \nMany studies\\cite{text2videozero,fatezero,cogvideo,tuneavideo,rerender,gen1,followyourpose,makeavideo,vdm} explore the augmentation of inter-frame attention modeling on the foundation of text-to-image (T2I) models to achieve video generation.\nSome works turn pretrained T2I models into video generators by inserting temporal layers. Video LDM\\cite{align} proposes to first pretrain the model on images only and then train temporal layers on videos. AnimateDiff\\cite{animatediff} presents a motion module trained on large video data which could be injected into most personalized T2I models without specific tuning. Our approach draws inspiration from such methods for temporal modeling. \n\nSome studies extend text-to-video capabilities to image-to-video.\nVideoComposer\\cite{videocomposer} incorporates images into the diffusion input during training as a conditional control. AnimateDiff\\cite{animatediff} performs weighted mixing of image latent and random noise during denoising. VideoCrafter\\cite{videocrafter1} incorporates textual and visual features from CLIP as the input for cross-attention. \nHowever, these approaches still face challenges in achieving stable human video generation, and the exploration of incorporating image condition input remains an area requiring further investigation.\n\n\\subsection{Diffusion Model for Human Image Animation }\nImage animation\\cite{fomm,mraa,ren2020deep,tpsmm,siarohin2019animating,zhang2022exploring,bidirectionally,everybody,liquid,editable}, aims to generate images or videos based on one or more input images. \nIn recent research, the superior generation quality and stable controllability offered by diffusion models have led to their integration into human image animation. PIDM\\cite{pidm} proposes texture diffusion blocks to inject the desired texture patterns into denoising for human pose transfer. LFDM\\cite{LFDM} synthesizes an optical flow sequence in the latent space, warping the input image based on given conditions. LEO\\cite{leo} represents motion as a sequence of flow maps and employs diffusion model to synthesize sequences of motion codes. DreamPose\\cite{dreampose} utilizes pretrained Stable Diffusion model and proposes an adapter to model the CLIP and VAE image embeddings. DisCo\\cite{disco} draws inspiration from ControlNet, decoupling the control of pose and background. Despite the incorporation of diffusion models to enhance generation quality, these methods still grapple with issues such as texture inconsistency and temporal instability in their results. Moreover, there is no method to investigate and demonstrate a more generalized capability in character animation.\n\n\\section{Methods}\nWe target at pose-guided image-to-video synthesis for character animation. Given a reference image describing the appearance of a character and a pose sequence, our model generates an animated video of the character. The pipeline of our method is illustrated in Fig.~\\ref{fig:overview}. In this section, we first provide a concise introduction to Stable Diffusion in Sec~\\ref{sec:sd}, which lays the foundational framework and network structure for our method. Then we provide a detailed explanation of the design specifics in Sec~\\ref{sec:sd}. Finally, we present the training process in Sec~\\ref{sec:train}.\n\n\\subsection{Preliminariy: Stable Diffusion}\\label{sec:sd}\nOur method is an extension of Stable Diffusion (SD), which is developed from Latent diffusion model (LDM). To reduce the computational complexity of the model, it introduces to model feature distributions in the latent space. SD develops an autoencoder\\cite{vae,vqvae} to establish the implicit representation of images, which consists of an encoder $\\mathcal E$ and a decoder $\\mathcal D$. Given an image $\\mathbf x$, the encoder first maps it to a latent representation: $\\mathbf z$ = $\\mathcal E$($\\mathbf x$) and then the decoder reconstructs it: ${\\mathbf x}_{recon}$ = $\\mathcal D$($\\mathbf z$). \n\nSD learns to denoise a normally-distributed noise $\\epsilon$ to realistic latent $\\mathbf z$. During training, the image latent $\\mathbf z$ is diffused in $\\mathnormal t$ timesteps to produce noise latent ${\\mathbf z}_{t}$. And a denoising UNet is trained to predict the applied noise. The optimization process is defined as follow objective:\n\n\\begin{equation}\n\\label{eq1}\n    {\\mathbf L} = {\\mathbb E}_{{\\mathbf z}_{t},c,{\\epsilon},t}({||{\\epsilon}-{{\\epsilon}_{\\theta}}({\\mathbf z}_{t},c,t)||}^{2}_{2})\n\\end{equation}\n\n\\noindent\nwhere ${\\epsilon}_{\\theta}$ represents the function of the denoising UNet. $\\mathnormal c$ represents the embeddings of conditional information. In original SD, CLIP ViT-L/14\\cite{vit} text encoder is applied to represent the text prompt as token embeddings for text-to-image generation. The denoising UNet consists of four downsample layers , one middle layer and four upsample layers. A typical block within a layer includes three types of computations: 2D convolution, self-attention\\cite{attention}, and cross-attention (terms as Res-Trans block). Cross-attention is conducted between text embedding and corresponding network feature. \n\nAt inference, ${\\mathbf z}_{T}$ is sampled from random Gaussian distribution with the initial timestep $\\mathnormal T$ and is progressively denoised and restored to ${\\mathbf z}_{0}$ via deterministic sampling process (e.g. DDPM\\cite{denoising}, DDIM\\cite{ddim}). \nIn each iteration, the denoising UNet predicts the noise on the latent feature corresponding to each timestep $\\mathnormal t$. \nFinally, ${\\mathbf z}_{0}$ will be reconstructed by decoder $\\mathcal D$ to obtain the generated image. \n\n\\subsection{Network Architecture}\\label{sec:net}\n\n\\noindent\n\\textbf{Overview. }\nFig.~\\ref{fig:overview} provides the overview of our method. The initial input to the network consists of multi-frame noise. The denoising UNet is configured based on the design of SD, employing the same framework and block units, and inherits the training weights from SD. Additionally, our method incorporates three crucial components: 1) ReferenceNet, encoding the appearance features of the character from the reference image; 2) Pose Guider, encoding motion control signals for achieving controllable character movements; 3) Temporal layer, encoding temporal relationship to ensure the continuity of character motion.\n\n\\noindent\n\\textbf{ReferenceNet. }\nIn text-to-video tasks, textual prompts articulate high-level semantics, necessitating only semantic relevance with the generated visual content. However, in image-to-video tasks, images encapsulate more low-level detailed features, demanding precise consistency in the generated results. In preceding studies focused on image-driven generation, most approaches\\cite{ip,objectstitch,paint,dreampose,disco,videocrafter1} employ the CLIP image encoder as a substitute for the text encoder in cross-attention. However, this design falls short of addressing issues related to detail consistency. One reason for this limitation is that the input to the CLIP image encoder comprises low-resolution ($224{\\times}224$) images, resulting in the loss of significant fine-grained detail information. Another factor is that CLIP is trained to match semantic features for text, emphasizing high-level feature matching, thereby leading to a deficit in detailed features within the feature encoding.\n\nHence, we devise a reference image feature extraction network named ReferenceNet. We adopt a framework identical to the denoising UNet for ReferenceNet, excluding the temporal layer. Similar to the denoising UNet, ReferenceNet inherits weights from the original SD, and weight update is conducted independently for each.\nThen we explain the integration method of features from ReferenceNet into the denoising UNet. \nSpecifically, as shown in Fig.~\\ref{fig:overview}, we replace the self-attention layer with spatial-attention layer. Given a feature map ${x}_{1} {\\in} {\\mathbb R}^{{\\mathnormal t}{\\times}{\\mathnormal h}{\\times}{\\mathnormal w}{\\times}{\\mathnormal c}}$ from denoising UNet and ${x}_{2} {\\in} {\\mathbb R}^{{\\mathnormal h}{\\times}{\\mathnormal w}{\\times}{\\mathnormal c}}$ from ReferenceNet, we first copy ${x}_{2}$ by $\\mathnormal t$ times and concatenate it with ${x}_{1}$ along $\\mathnormal w$ dimension. Then we perform self-attention and extract the first half of the feature map as the output. This design offers two advantages: Firstly, ReferenceNet can leverage the pre-trained image feature modeling capabilities from the original SD, resulting in a well-initialized feature. Secondly, due to the essentially identical network structure and shared initialization weights between ReferenceNet and the denoising UNet, the denoising UNet can selectively learn features from ReferenceNet that are correlated in the same feature space.\nAdditionally, cross-attention is employed using the CLIP image encoder. Leveraging the shared feature space with the text encoder, it provides semantic features of the reference image, serving as a beneficial initialization to expedite the entire network training process.\n\nA comparable design is ControlNet\\cite{controlnet}, which introduces additional control features into the denoising UNet using zero convolution. However, control information, such as depth and edge, is spatially aligned with the target image, while the reference image and the target image are spatially related but not aligned. Consequently, ControlNet is not suitable for direct application. We will substantiate this in the subsequent experimental Section~\\ref{ablation}.\n\nWhile ReferenceNet introduces a comparable number of parameters to the denoising UNet, in diffusion-based video generation, all video frames undergo denoising multiple times, whereas ReferenceNet only needs to extract features once throughout the entire process. Consequently, during inference, it does not lead to a substantial increase in computational overhead.\n\n\\noindent\n\\textbf{Pose Guider. }\nControlNet\\cite{controlnet} demonstrates highly robust conditional generation capabilities beyond text. Different from these methods, as the denoising UNet needs to be finetuned, we choose not to incorporate an additional control network to prevent a significant increase in computational complexity. Instead, we employ a lightweight Pose Guider. This Pose Guider utilizes four convolution layers ($4{\\times}4$ kernels, $2{\\times}2$ strides, using 16,32,64,128 channels, similar to the condition encoder in \\cite{controlnet}) to align the pose image with the same resolution as the noise latent. Subsequently, the processed pose image is added to the noise latent before being input into the denoising UNet. The Pose Guider is initialized with Gaussian weights, and in the final projection layer, we employ zero convolution.\n\n\\noindent\n\\textbf{Temporal Layer. }\nNumerous studies have suggested incorporating supplementary temporal layers into text-to-image (T2I) models to capture the temporal dependencies among video frames. This design facilitates the transfer of pretrained image generation capabilities from the base T2I model. Adhering to this principle, our temporal layer is integrated after the spatial-attention and cross-attention components within the Res-Trans block. The design of the temporal layer was inspired by AnimateDiff\\cite{animatediff}. Specifically, for a feature map ${x} {\\in} {\\mathbb R}^{{\\mathnormal b}{\\times}{\\mathnormal t}{\\times}{\\mathnormal h}{\\times}{\\mathnormal w}{\\times}{\\mathnormal c}}$, we first reshape it to ${x} {\\in} {\\mathbb R}^{({\\mathnormal b}{\\times}{\\mathnormal h}{\\times}{\\mathnormal w}){\\times}{\\mathnormal t}{\\times}{\\mathnormal c}}$, and then perform temporal attention, which refers to self-attention along the dimension $\\mathnormal t$. The feature from temporal layer is incorporated into the original feature through a residual connection. This design aligns with the two-stage training approach that we will describe in the following subsection.\nThe temporal layer is exclusively applied within the Res-Trans blocks of the denoising UNet. For ReferenceNet, it computes features for a single reference image and does not engage in temporal modeling. Due to the controllability of continuous character movement achieved by the Pose Guider, experiments demonstrate that the temporal layer ensures temporal smoothness and continuity of appearance details, obviating the need for intricate motion modeling.\n\n\\subsection{Training Strategy}\\label{sec:train}\nThe training process is divided into two stages. In the first stage, training is performed using individual video frames. Within the denoising UNet, we temporarily exclude the temporal layer and the model takes single-frame noise as input. The ReferenceNet and Pose Guider are also trained during this stage. The reference image is randomly selected from the entire video clip. We initialize the model of the denoising UNet and ReferenceNet based on the pretrained weights from SD. The Pose Guider is initialized using Gaussian weights, except for the final projection layer, which utilizes zero convolution. The weights of the VAE's Encoder and Decoder, as well as the CLIP image encoder, are all kept fixed. The optimization objective in this stage is to enable the model to generate high-quality animated images under the condition of a given reference image and target pose.\nIn the second stage, we introduce the temporal layer into the previously trained model and initialize it using pretrained weights from AnimateDiff\\cite{animatediff}. The input for the model consists of a 24-frames video clip. During this stage, we only train the temporal layer while fixing the weights of the rest of the network. \n\n\\begin{figure*}[!t]\n\\begin{center}\n\t\\setlength{\\fboxrule}{0pt}\n\t\\fbox{\\includegraphics[width=1\\linewidth]{./figure/vis_ready.pdf}}\n\\end{center}\n\\vspace{-0.6cm}\n\\caption{Qualitative Results. Given a reference image (the leftmost image of each group), our approach demonstrates the ability to animate diverse characters, encompassing full-body human figures, half-length portraits, cartoon characters, and humanoid figures. The illustration showcases results with clear, consistent details, and continuous motion. }\n\\vspace{-0.2cm}\n\\label{fig:vis}\n\\end{figure*}\n\n\\section{Experiments}\n\n\\subsection{Implementations}\n\nTo demonstrate the applicability of our approach in animating various characters, we collect 5K character video clips from the internet to train our model.\nWe employ DWPose\\cite{dwpose} to extract pose sequence of characters in the video, including body and hands, rendering it as pose skeleton images following OpenPose\\cite{openpose}. Experiments are conducted on 4 NVIDIA A100 GPUs. In the first training stage, individual video frames are sampled, resized, and center-cropped to a resolution of $768{\\times}768$. Training is conducted for 30,000 steps with a batch size of 64. In the second training stage, we train the temporal layer for 10,000 steps with 24-frame video sequences and a batch size of 4. Both learning rates are set to 1e-5. During inference, we rescale the length of the driving pose skeleton to approximate the length of the character's skeleton in the reference image and use a DDIM sampler for 20 denoising steps. We adopt the temporal aggregation method in \\cite{edge}, connecting results from different batches to generate long videos. For fair comparison with other methods, we also train our model on three specific benchmarks (UBC fashion video dataset\\cite{dwnet}, TikTok dataset\\cite{tiktok} and Ted-talk dataset\\cite{mraa}) without using additional data, as will be discussed in Section~\\ref{com}. \n\n\\subsection{Qualitative Results}\nFig.~\\ref{fig:vis} demonstrates that our method can animate arbitrary characters, including full-body human figures, half-length portraits, cartoon characters, and humanoid characters. Our approach is capable of generating high-definition and realistic character details. It maintains temporal consistency with the reference images even under substantial motion and exhibits temporal continuity between frames. \n\n\\subsection{Comparisons}\\label{com}\n\nTo demonstrate the superiority of our approach, we evaluate its performance in three specific benchmarks: fashion video synthesis, human dance generation and talking gesture generation. We also conduct a baseline that combines Stable Diffusion, ControlNet, IP-Adapter\\cite{ip} and AnimateDiff, named SD-I2V.\nFor quantitative assessment of image-level quality, SSIM\\cite{ssim}, PSNR\\cite{psnr} and LPIPS\\cite{lpips} are employed. Video-level evaluation uses FVD\\cite{fvd} metrics. \n\n\\begin{figure}[!t]\n\\begin{center}\n    \\vspace{-0.3cm}\n\t\\setlength{\\fboxrule}{0pt}\n\t\\fbox{\\includegraphics[width=1\\linewidth]{./figure/com_ubc2.pdf}}\n\\end{center}\n\\vspace{-0.8cm}\n\\caption{Qualitative comparison for fashion video synthesis. Other methods exhibit shortcomings in preserving fine-textured details of clothing, whereas our method excels in maintaining exceptional detail features.}\n\\vspace{-0.2cm}\n\\label{fig:dreampose}\n\\end{figure}\n\n\\begin{table}\n    \\setlength{\\tabcolsep}{5pt}\n\t\\centering\n\\begin{center}\n\\begin{tabular}{lccccc} \n\n\\hline\n    & SSIM $\\uparrow$ & PSNR $\\uparrow$ & LPIPS $\\downarrow$  & FVD $\\downarrow$ \\\\\n \\hline\n MRAA\\cite{mraa}     & 0.749 & - & 0.212    & 253.6 \\\\\n TPSMM\\cite{tpsmm}    & 0.746 & - & 0.213  & 247.5 \\\\\n BDMM\\cite{bidirectionally}     & 0.918 & 24.07 & 0.048  & 148.3 \\\\\nDreamPose\\cite{dreampose} & 0.885 & - & 0.068  & 238.7 \\\\\nDreamPose* & 0.879 & 34.75 & 0.111  & 279.6 \\\\\nSD-I2V & 0.894 & 36.01 & 0.095  & 175.4 \\\\\nOurs & \\textbf{0.931} & \\textbf{38.49} & \\textbf{0.044}  & \\textbf{81.6} \\\\\n \n\\hline\n\\end{tabular}\n\\end{center}    \\vspace{-0.5cm}\n\t\\caption{Quantitative comparison for fashion video synthesis. \"Dreampose*\" denotes the result without sample finetuning.}\n    \\vspace{-0.3cm}\n\t\\label{table:ubc}\n\\end{table}\n\n\\noindent\n\\textbf{Fashion Video Synthesis. }\nExperiments are conducted on the UBC fashion video dataset. \nThe quantitative comparison is shown in Tab.~\\ref{table:ubc}. Our result outperforms other methods, particularly exhibiting a significant lead in video metric. Qualitative comparison is shown in Fig.~\\ref{fig:dreampose}. For fair comparison, we obtain results of DreamPose without sample finetuning using its open-source code. In the domain of fashion videos, there is a stringent requirement for fine-grained clothing details. However, other methods fail to maintain the consistency of clothing details and exhibit noticeable errors in terms of color and fine structural elements. In contrast, our method produces results that effectively preserve the consistency of clothing details.\n\n\\begin{figure}[!t]\n\\begin{center}\n\t\\setlength{\\fboxrule}{0pt}\n\t\\fbox{\\includegraphics[width=1\\linewidth]{./figure/com_tiktok.pdf}}\n\\end{center}\n\\vspace{-0.6cm}\n\\caption{Qualitative comparison between DisCo and our method. DisCo displays problems such as pose control errors, color inaccuracy, and inconsistent details. In contrast, our method demonstrates significant improvements in addressing these issues.}\n\\vspace{-0.2cm}\n\\label{fig:disco}\n\\end{figure}\n\n\\begin{table}\n\t\\centering\n\\begin{center}\n\\begin{tabular}{lccccc} \n\n\\hline\n    & SSIM $\\uparrow$ & PSNR $\\uparrow$ & LPIPS $\\downarrow$ & FVD $\\downarrow$ \\\\\n \\hline\nFOMM\\cite{fomm}  & 0.648 & 29.01 & 0.335 & 405.2 \\\\\nMRAA\\cite{mraa} & 0.672 & 29.39 & 0.296 & 284.8 \\\\\nTPSMM\\cite{tpsmm} & 0.673 & 29.18 & 0.299 & 306.1 \\\\\nDisco\\cite{disco} & 0.668 & 29.03 & 0.292 & 292.8 \\\\\nSD-I2V & 0.670 & 29.11 & 0.295 & 225.5 \\\\\nOurs & \\textbf{0.718} & \\textbf{29.56} & \\textbf{0.285} & \\textbf{171.9} \\\\\n \n\\hline\n\\end{tabular}\n\\end{center}    \\vspace{-0.5cm}\n\t\\caption{Quantitative comparison for human dance generation.}\n    \\vspace{-0.3cm}\n\t\\label{table:tiktok}\n\\end{table}\n\n\\noindent\n\\textbf{Human Dance Generation. }\nWe conduct experiments on the TikTok dataset. \nWe conduct a quantitative comparison presented in Tab.~\\ref{table:tiktok}, and our method achieves the best results.\nFor enhanced generalization, DisCo incorporates human attribute pre-training, utilizing a large number of image pairs for model pre-training. In contrast, our training is exclusively conducted on the TikTok dataset, yielding results superior to DisCo. \nWe present qualitative comparison with DisCo in Fig.~\\ref{fig:disco}. \nDuring intricate dance sequences, our model stands out in maintaining visual continuity throughout the motion and exhibits enhanced robustness in handling diverse character appearances.\n\n\\begin{figure}[!t]\n\\begin{center}\n\t\\setlength{\\fboxrule}{0pt}\n\t\\fbox{\\includegraphics[width=1\\linewidth]{./figure/com_ted.pdf}}\n\\end{center}\n\\vspace{-0.6cm}\n\\caption{Qualitative comparison on Ted-talk dataset. Our model is capable of generating more accurate and clear results.}\n\\vspace{-0.2cm}\n\\label{fig:rebuttal_fig}\n\\end{figure}\n\n\\begin{table}\n\t\\centering\n\\begin{center}\n\\begin{tabular}{lccccc} \n\n\\hline\n    & SSIM $\\uparrow$ & PSNR $\\uparrow$ & LPIPS $\\downarrow$ & FVD $\\downarrow$ \\\\\n \\hline\nMRAA\\cite{mraa} & 0.826 & 33.86 & 0.160 & 82.8 \\\\\nTPSMM\\cite{tpsmm} & 0.830 & 33.81 & \\textbf{0.157} & 80.7 \\\\\nDisco\\cite{disco} & 0.754 & 31.25 & 0.193 & 223.5 \\\\\nSD-I2V  & 0.773 & 32.11 & 0.179 & 158.3 \\\\\nOurs & \\textbf{0.832} & \\textbf{33.91} & 0.159 & \\textbf{80.5} \\\\\n \n\\hline\n\\end{tabular}\n\\end{center}    \\vspace{-0.5cm}\n\t\\caption{Quantitative comparison on Ted-talk dataset.}\n    \\vspace{-0.3cm}\n\t\\label{table:tedtalk}\n\\end{table}\n\n\\noindent\n\\textbf{Talking Gesture Generation. }\nWe also evaluate our method on Ted-talk dataset. \nResults are shown in Fig \\ref{fig:rebuttal_fig} and Tab \\ref{table:tedtalk}. Our approach significantly outperforms DisCo and SD-I2V. MRAA and TPSMM employ GT images as driving signals (video reconstruction), while we achieve better results using only pose information. On other two evaluated benchmarks (UBC with more intricate clothing textures and TikTok with more complex human movements), the performance of MRAA and TPSMM falls far behind our method.\n\n\\begin{figure}[!t]\n\\begin{center}\n\t\\setlength{\\fboxrule}{0pt}\n\t\\fbox{\\includegraphics[width=1\\linewidth]{./figure/com_i2v.pdf}}\n\\end{center}\n\\vspace{-0.6cm}\n\\caption{Qualitative comparison with image-to-video methods, which struggle to generate substantial character movements and face challenges in maintaining long-term appearance consistency.}\n\\vspace{-0.2cm}\n\\label{fig:i2v}\n\\end{figure}\n\n\\noindent\n\\textbf{General Image-to-Video Methods. }\nCurrently, numerous studies propose video diffusion models with strong generative capabilities based on large-scale training data. We select two of the most well-known and effective image-to-video methods for comparison: AnimateDiff\\cite{animatediff} and Gen-2\\cite{gen1}. As these two methods do not perform pose control, we only compare their ability to maintain the appearance fidelity to the reference image. As depicted in Fig.~\\ref{fig:i2v}, current image-to-video methods face challenges in generating substantial character movements and struggle to maintain long-term appearance consistency in videos, thus hindering effective support for consistent character animation.\n\n\\subsection{Ablation study}\\label{ablation}\n\n\\noindent\n\\textbf{Image Condition Modeling. }\nTo demonstrate the effectiveness of our image condition modeling, we explore alternative designs, including 1) using only the CLIP image encoder to represent reference image features without integrating ReferenceNet, 2) initially finetuning SD and subsequently training ControlNet with the reference image. 3) integrating the above two designs. \nExperiments are conducted on UBC fashion video dataset. \nAs shown in Fig.~\\ref{fig:ablation}, visualizations illustrate that ReferenceNet outperforms the other three designs. Solely relying on CLIP features as reference image features can preserve image similarity but fails to fully transfer details. ControlNet does not enhance results as its features lack spatial correspondence, rendering it inapplicable. Quantitative results are also presented in Tab.~\\ref{table:ablation}, demonstrating the superiority of our design. \n\n\\begin{figure}[!t]\n\\begin{center}\n    \\vspace{-0.3cm}\n\t\\setlength{\\fboxrule}{0pt}\n\t\\fbox{\\includegraphics[width=1\\linewidth]{./figure/ablation.pdf}}\n\\end{center}\n\\vspace{-0.6cm}\n\\caption{Ablation study of different design. ReferenceNet ensures consistent preservation of details in character's appearance.}\n\\vspace{-0.2cm}\n\\label{fig:ablation}\n\\end{figure}\n\n\\noindent\n\\textbf{Details of ReferenceNet Design. }\nTo demonstrate the effectiveness of our ReferenceNet design, we conduct experiments: 1) Replacing UNet (SD weights) with ResNet (ImageNet weights). 2) Replacing spatial-attention with feature-concatenation. Quantitative result is shown in Tab \\ref{table:ablation_rebuttal}. Our design achieves the optimal performance. Experiment 1) shows the necessity of utilizing SD weights. There exists a certain gap between ImageNet's image features and the implicit features in SD. Utilizing features from SD enhances the integration of conditioning information within the same feature space during the generation process. Experiment 2) demonstrates the necessity of spatial-attention which enables the denoising UNet to effectively integrate detailed image features from ReferenceNet. \n\n\\begin{table}\n    \\setlength{\\tabcolsep}{4pt}\n\t\\centering\n\\begin{center}\n\\begin{tabular}{lcccc} \n\n\\hline\n    & SSIM $\\uparrow$ & PSNR $\\uparrow$ & LPIPS $\\downarrow$  & FVD $\\downarrow$ \\\\\n \\hline\n CLIP     & 0.897 & 36.09 & 0.089    & 208.5 \\\\\n ControlNet    & 0.892 & 35.89 & 0.105  & 213.9 \\\\\nCLIP+ControlNet & 0.898 & 36.03 & 0.086  & 205.4 \\\\\nOurs & \\textbf{0.931} & \\textbf{38.49} & \\textbf{0.044}  & \\textbf{81.6} \\\\\n \n\\hline\n\\end{tabular}\n\\end{center}    \\vspace{-0.5cm}\n\t\\caption{Ablation study for image condition modeling.}\n    \\vspace{-0.3cm}\n\t\\label{table:ablation}\n\\end{table}\n\n\\begin{table}\n    \\setlength{\\tabcolsep}{4pt}\n\t\\centering\n\\begin{center}\n\\begin{tabular}{lcccc} \n\n\\hline\n    & SSIM $\\uparrow$ & PSNR $\\uparrow$ & LPIPS $\\downarrow$  & FVD $\\downarrow$ \\\\\n \\hline\n ImageNet weights     & 0.901 & 36.21 & 0.084    & 165.4 \\\\\n Feature-concat    & 0.909 & 36.53 & 0.071  & 132.8 \\\\\nOurs & \\textbf{0.931} & \\textbf{38.49} & \\textbf{0.044}  & \\textbf{81.6} \\\\\n \n\\hline\n\\end{tabular}\n\\end{center}    \\vspace{-0.5cm}\n\t\\caption{Ablation study for ReferenceNet design.}\n    \\vspace{-0.3cm}\n\t\\label{table:ablation_rebuttal}\n\\end{table}\n\n\\noindent\n\\textbf{Temporal Modeling. }\nWe conduct two experiments to assess the effectiveness of proposed temporal modeling method: \n1) do not apply temporal layer, directly concatenating images temporally to create a video. \n2) do not apply the two-stage training, directly training the entire network. \nQuantitative results are presented in Tab.~\\ref{table:temporal}. The absence of temporal layer results in noticeable texture sticking and inter-frame jitter, leading to a significant decrease in FVD metric. When the two-stage training is not employed, metrics related to image quality experience a decline. We attribute this to the fact that, when optimizing over multiple frames simultaneously, the network tends to focus more on the overall temporal visual coherence, thereby weakening attention to the details of each individual frame. The adoption of the two-stage training method ensures both the quality of generated video frames and temporal smoothness. \n\n\\begin{table}\n    \\setlength{\\tabcolsep}{2pt}\n\t\\centering\n\\begin{center}\n\\begin{tabular}{lcccc} \n\n\\hline\n    & SSIM $\\uparrow$ & PSNR $\\uparrow$ & LPIPS $\\downarrow$  & FVD $\\downarrow$ \\\\\n \\hline\n w/o Temporal Layer     & 0.925 & 38.28 & 0.049    & 176.7 \\\\\n w/o Two Stage Training    & 0.917 & 38.01 & 0.056  & 89.3 \\\\\nOurs & \\textbf{0.931} & \\textbf{38.49} & \\textbf{0.044}  & \\textbf{81.6} \\\\\n \n\\hline\n\\end{tabular}\n\\end{center}    \\vspace{-0.5cm}\n\t\\caption{Ablation study of temporal modeling.}\n    \\vspace{-0.3cm}\n\t\\label{table:temporal}\n\\end{table}\n\n\\section{Discussion and Conclusion}\n\n\\noindent\n\\textbf{Limitations. }\nOur model may struggle to generate stable results for hand movements, sometimes leading to distortions and motion blur. \nBesides, since images provide information from only one perspective, generating unseen parts during character movement is an ill-posed problem which encounters potential instability. \nThird, due to the utilization of DDPM, our model exhibits a lower operational efficiency compared to non-diffusion-model-based methods.\n\n\\noindent\n\\textbf{Potential Impact. }\nThe proposed method may be used to produce fake videos of individuals, which can be detected using some face anti-spoofing techniques\\cite{anti_color,anti_deep,anti_search}. \n\n\\noindent\n\\textbf{Conclusion. }\nIn this paper, we present \\textit{Animate Anyone}, a framework capable of transforming character photographs into animated videos controlled by a desired pose sequence.\nWe propose ReferenceNet, which genuinely preserves intricate character appearances and we also achieve efficient pose controllability and temporal continuity. Our approach not only applies to general character animation but also outperforms existing methods. \n\n{\n    \\small\n        }\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2304.06025v4.tex",
        "arXiv-2305.03989v3.tex",
        "arXiv-2311.17117v3.tex"
    ],
    "group_id": "group_85",
    "response": "### Title: Advances in Diffusion-Based Methods for Human Video Synthesis and Fashion Image Animation\n\n### Introduction\n\nThe field of generative models for video synthesis and image-to-video transformation has seen significant advancements in recent years, driven by the success of deep learning techniques such as Generative Adversarial Networks (GANs) and Diffusion Models (DMs). These models have been pivotal in generating high-quality images and videos, but the task of synthesizing coherent and realistic human videos, especially those involving rich global and local deformations, remains challenging. Traditional approaches often rely on separate networks for appearance and motion, leading to issues such as spatial distortions and temporal jittering. Diffusion models, on the other hand, have shown promise in text-to-image generation but have yet to fully realize their potential in video synthesis, particularly in maintaining temporal consistency and handling complex motions.\n\nIn this summary, we explore three recent papers that tackle the challenge of generating realistic human videos and fashion image animations using diffusion models. These papers introduce novel architectures and training strategies to enhance the quality and consistency of the generated videos. Each paper addresses specific aspects of the problem, such as handling intricate clothing details, ensuring temporal smoothness, and preserving fine-grained character features across substantial movements. The common goal of these studies is to leverage the robust generative capabilities of diffusion models while overcoming their limitations in video synthesis.\n\n### Main Content of Each Paper\n\n#### DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion\n\nDreamPose, introduced by Karras et al., focuses on generating animated fashion videos from still images using a sequence of human body poses. The method builds upon the Stable Diffusion (SD) model, a latent diffusion model that operates in the latent space of an autoencoder. The authors adapt the SD architecture to incorporate image and pose conditioning, addressing the limitations of existing models that are primarily text-conditioned. The key innovation lies in the dual CLIP-VAE image encoder and an adapter module that blends CLIP and VAE embeddings to condition the output on the input image. Additionally, DreamPose concatenates a sequence of five consecutive poses to the input noise, enhancing temporal consistency and reducing motion jitter.\n\nDreamPose's two-stage finetuning strategy involves first training the model on a full dataset and then fine-tuning it on a single subject image. The first stage ensures that the model learns to synthesize frames consistent with the input image and pose sequence, while the second stage refines the model to better preserve the identity and appearance of the subject. The authors also introduce a novel formulation of classifier-free guidance to modulate the strength of image and pose conditioning during inference. This approach allows for greater control over the appearance and motion of the generated video, achieving state-of-the-art results on the UBC Fashion dataset.\n\n#### LEO: Generative Latent Image Animator for Human Video Synthesis\n\nLEO, proposed by Wang et al., aims to generate human videos with strong spatio-temporal coherency by disentangling appearance and motion. The framework consists of two main components: a flow-based image animator and a Latent Motion Diffusion Model (LMDM). The image animator maps latent motion codes to flow maps, which are used to warp and inpaint the starting frame. The LMDM, a 1D U-Net, learns to synthesize sequences of motion codes, capturing the motion prior in the training data. This two-phase training strategy ensures that the motion codes are strictly motion-related, without any appearance interference.\n\nThe authors introduce a Linear Motion Condition (LMC) mechanism in LMDM, which synthesizes sequences of residuals with respect to the starting motion code. This allows for the generation of videos of arbitrary length by concatenating additional sequences of residuals. LEO is trained on three human-related datasets: TaichiHD, FaceForensics, and CelebV-HQ. The framework's ability to disentangle appearance and motion enables it to perform tasks such as infinite-length video synthesis and content-preserving video editing. The authors conduct extensive quantitative and qualitative evaluations, demonstrating that LEO significantly improves the coherence of human videos compared to previous methods.\n\n#### Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation\n\nAnimate Anyone, presented by Wang et al., is designed to generate character videos from still images using desired pose sequences. The method builds upon Stable Diffusion (SD) and introduces three key components: ReferenceNet, Pose Guider, and a Temporal Layer. ReferenceNet captures spatial details of the reference image using a symmetrical UNet structure, which is integrated into the denoising UNet via spatial attention. This ensures that the model can maintain consistency with the appearance details of the reference character. The Pose Guider is a lightweight network that aligns pose images with the noise latent, facilitating efficient pose control. The Temporal Layer models temporal dependencies among video frames, ensuring smooth inter-frame transitions.\n\nThe training process of Animate Anyone is divided into two stages. In the first stage, the model is trained on individual video frames, excluding the temporal layer. In the second stage, the temporal layer is introduced and trained on multi-frame sequences. The authors conduct experiments on an internal dataset of 5K character video clips and three specific benchmarks, demonstrating the effectiveness of their approach. Animate Anyone outperforms existing methods in terms of maintaining fine-grained details and temporal consistency, especially in the context of fashion video synthesis and human dance generation.\n\n### Commonalities and Innovations\n\nAll three papers leverage diffusion models to generate human videos from still images, but they address different aspects of the challenge. DreamPose focuses on fashion image-to-video synthesis, introducing a dual CLIP-VAE image encoder and a multi-pose input representation to enhance temporal consistency and preserve garment details. LEO emphasizes spatio-temporal coherency by representing motion as a sequence of flow maps and using a Linear Motion Condition (LMC) to generate stable motion sequences. Animate Anyone introduces ReferenceNet to capture spatial details of the reference image and ensures controllability and continuity through a lightweight Pose Guider and a Temporal Layer.\n\nThe common theme among these papers is the adaptation of diffusion models for video synthesis, specifically for human videos. They all aim to improve the quality and consistency of generated videos by addressing the limitations of existing models. DreamPose and Animate Anyone introduce novel conditioning mechanisms to preserve fine-grained details, while LEO focuses on disentangling appearance and motion to achieve spatio-temporal coherency. Each paper contributes unique architectural modifications and training strategies to enhance the capabilities of diffusion models in generating realistic human videos.\n\n### Comparison of Results\n\nThe three papers present their results on various datasets and benchmarks, showcasing the effectiveness of their proposed methods. DreamPose achieves state-of-the-art results on the UBC Fashion dataset, outperforming existing methods in terms of L1, SSIM, VGG, LPIPS, FID, FVD, and Average Euclidean Distance (AED) metrics. The authors conduct user studies, where participants prefer DreamPose over other methods in terms of quality and similarity to the input image. The method is also shown to handle different viewpoints and loose garments well, and it can animate an input image using motion from a video containing a different person and garment identity.\n\nLEO demonstrates superior performance on human video synthesis tasks, including unconditional video generation, conditional generation based on a single image, and video editing. The authors evaluate LEO on TaichiHD, FaceForensics, and CelebV-HQ datasets, reporting lower FVD and KVD values compared to existing methods. LEO also achieves better results in terms of Average Content Distance (ACD), indicating improved identity consistency. The authors conduct user studies, where nearly all participants rated LEO's generated videos as more realistic and coherent compared to other methods. The framework's ability to disentangle appearance and motion allows it to produce diverse motion from the same content and maintain temporal consistency.\n\nAnimate Anyone outperforms existing methods in terms of maintaining fine-grained details and temporal consistency across various benchmarks, including fashion video synthesis, human dance generation, and talking gesture generation. The authors compare their method with other image-to-video approaches, demonstrating that Animate Anyone can handle substantial character movements and maintain long-term appearance consistency better. The method achieves the best results on the UBC Fashion dataset, TikTok dataset, and Ted-talk dataset, as shown in Tables \\ref{table:ubc}, \\ref{table:tiktok}, and \\ref{table:tedtalk}. The authors conduct ablation studies to validate the effectiveness of their design choices, such as ReferenceNet and the Temporal Layer.\n\n### Conclusion\n\nIn summary, the three papers present innovative approaches to enhance the capabilities of diffusion models in generating realistic human videos and fashion image animations. DreamPose introduces a dual CLIP-VAE image encoder and a multi-pose input representation to preserve garment details and ensure temporal consistency. LEO uses a sequence of flow maps and a Linear Motion Condition (LMC) to disentangle appearance and motion, achieving superior spatio-temporal coherency. Animate Anyone incorporates ReferenceNet to capture spatial details of the reference image and ensures controllability and continuity through a Pose Guider and a Temporal Layer, outperforming existing methods in maintaining fine-grained details and temporal consistency.\n\nThe results of these studies demonstrate that diffusion models, when adapted with appropriate conditioning mechanisms and training strategies, can generate high-quality and coherent human videos. However, each method has its limitations. DreamPose requires subject-specific finetuning, which can be time-consuming. LEO may struggle with geometry ambiguity and temporal coherency, especially in handling human body occlusion. Animate Anyone faces challenges in generating stable results for hand movements and may exhibit lower operational efficiency due to the use of DDPM.\n\nFuture research directions include addressing these limitations, such as incorporating 3D-aware generation techniques to handle occlusions and improving the efficiency of the models. Additionally, exploring the generalizability of these methods to more diverse and complex video datasets, as well as integrating them with other generative models, could further enhance their capabilities. The development of more robust metrics for evaluating video quality and coherence is also crucial for advancing the field of video synthesis.\n\n### Tables\n\nBelow are the quantitative comparison tables from the papers:\n\n**DreamPose vs. Existing Methods (UBC Fashion Dataset)**\n\\begin{table}[h!]\n\\begin{center}\n\\begin{tabular}{c c c c c c c c c}\n\\hline\n& L1 $\\downarrow$ & SSIM $\\uparrow$ & VGG  $\\downarrow$ & LPIPS  $\\downarrow$ & FID $\\downarrow$ & FVD (16f) $\\downarrow$ & AED $\\downarrow$ \\\\\n\\hline\\hline\nMRAA & 0.0857 & 0.749 & 0.534 & 0.212 & 23.42 & 253.65 & 0.0139 \\\\\nTPSMM & 0.0858 & 0.746 & 0.547 & 0.213 & 22.87 & 247.55 & 0.0137 \\\\\nPIDM & 0.1098 & 0.713 & 0.629 & 0.288 & 30.279 & 1197.39 & 0.0155 \\\\\nOurs & \\textbf{0.0256} & \\textbf{0.885} & \\textbf{0.235} & \\textbf{0.068} & \\textbf{13.04} & \\textbf{238.75} & \\textbf{0.0110} \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\caption{Quantitative comparison of DreamPose with existing methods on the UBC Fashion dataset.}\n\\label{table:dreampose}\n\\end{table}\n\n**LEO vs. Existing Methods (TaichiHD and FaceForensics Datasets)**\n\\begin{table*}[!t]\n\\begin{center}\n\\setlength\\arrayrulewidth{1pt}\n\\scalebox{0.8}{\n\\begin{tabular}{ccccccccccccc}\n\\hline\n& \\multicolumn{4}{c}{TaichiHD128} & \\multicolumn{3}{c}{TaichiHD256} & \\multicolumn{3}{c}{FaceForensics} & \\multicolumn{1}{c}{CelebV-HQ} \\\\\nMethod & $\\text{FVD}_{16}$ & $\\text{KVD}_{16}$ & $\\text{ACD}_{16}$ && $\\text{FVD}_{16}$ & $\\text{KVD}_{16}$ && $\\text{FVD}_{16}$ & $\\text{ACD}_{16}$ && $\\text{FVD}_{16}$   \\\\\n\\cmidrule{2-4}\\cmidrule{6-7}\\cmidrule{9-10}\\cmidrule{12-12}\nMoCoGAN-HD & 144.7 & 25.4 & - && - & - && 111.8 & 0.33 && 212.4  \\\\\nDIGAN & 128.1 & 20.6 & 2.17 && 156.7 & - && 62.5 & - && 72.9  \\\\\nTATS & 136.5 & 22.2 & 2.28 && - & - && - & - && -  \\\\\nStyleGAN-V & - & - & - && - & - && 47.4 & 0.36 && 69.1  \\\\\nMoStGAN-V & - & -& - && - & - && 39.7 & 0.38 && 132.1 \\\\\n\\cmidrule{1-12}\nOurs (uncond) & 100.4 & 11.4 & 1.83 && 122.7 & 20.49 && 52.3 & 0.28 && -  \\\\\nOurs (cond) & \\textbf{57.6} & \\textbf{4.0} & \\textbf{1.22} && \\textbf{94.8} & \\textbf{13.47} && \\textbf{35.9} & \\textbf{0.27} && \\textbf{40.2}  \\\\\n\\hline\n\\end{tabular}}\n\\end{center}\n\\caption{Quantitative comparison of LEO with existing methods on TaichiHD and FaceForensics datasets.}\n\\label{table:leo}\n\\end{table*}\n\n**Animate Anyone vs. Existing Methods (UBC Fashion, TikTok, and Ted-talk Datasets)**\n\\begin{table*}[!t]\n\\begin{center}\n\\begin{tabular}{lccccc} \n\n\\hline\n    & SSIM $\\uparrow$ & PSNR $\\uparrow$ & LPIPS $\\downarrow$  & FVD $\\downarrow$ \\\\\n \\hline\nMRAA & 0.826 & 33.86 & 0.160 & 82.8 \\\\\nTPSMM & 0.830 & 33.81 & \\textbf{0.157} & 80.7 \\\\\nDisco & 0.754 & 31.25 & 0.193 & 223.5 \\\\\nSD-I2V  & 0.773 & 32.11 & 0.179 & 158.3 \\\\\nOurs & \\textbf{0.832} & \\textbf{33.91} & 0.159 & \\textbf{80.5} \\\\\n \n\\hline\n\\end{tabular}\n\\end{center}\n\\caption{Quantitative comparison of Animate Anyone with existing methods on the Ted-talk dataset.}\n\\label{table:animate}\n\\end{table*}\n\n### Discussion\n\nThe three papers present significant advancements in the field of diffusion-based video synthesis, particularly in the context of human videos and fashion image animations. DreamPose, LEO, and Animate Anyone each introduce unique architectural modifications and training strategies to enhance the capabilities of diffusion models. DreamPose's dual CLIP-VAE encoder and multi-pose input representation allow for greater appearance fidelity and temporal consistency. LEO's use of flow maps and the Linear Motion Condition (LMC) mechanism ensures strong spatio-temporal coherency. Animate Anyone's ReferenceNet and Temporal Layer design enable the preservation of fine-grained details and temporal continuity.\n\nWhile these methods achieve impressive results, they also face specific challenges. DreamPose requires subject-specific finetuning, which can be time-consuming and computationally expensive. LEO may struggle with geometry ambiguity and temporal coherency, especially in handling human body occlusion. Animate Anyone faces challenges in generating stable results for hand movements and exhibits lower operational efficiency due to the use of DDPM.\n\nFuture research directions include addressing these limitations and exploring the generalizability of these methods to more diverse and complex video datasets. Incorporating 3D-aware generation techniques could enhance the handling of occlusions and improve the realism of the generated videos. Additionally, developing more robust metrics for evaluating video quality and coherence is essential for advancing the field. The integration of these methods with other generative models, such as GANs, could also lead to further improvements in video synthesis quality and efficiency.\n\n### Conclusion\n\nIn conclusion, the three papers significantly advance the field of diffusion-based video synthesis for human videos and fashion image animations. They introduce novel conditioning mechanisms and training strategies to enhance the capabilities of diffusion models, achieving state-of-the-art results in various benchmarks. However, each method has its limitations, and future research should focus on addressing these challenges and exploring the generalizability of these approaches. The integration of 3D-aware generation techniques and the development of more robust evaluation metrics are crucial for further advancements in this field."
}