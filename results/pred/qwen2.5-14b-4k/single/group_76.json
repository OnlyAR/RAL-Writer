{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Structured Prediction as Translation \\\\ between Augmented Natural Languages}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nWe propose a new framework, \\emph{Translation between Augmented Natural Languages} (\\ourmodel), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between \\emph{augmented natural languages}, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.\n\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:introduction}\n\nStructured prediction refers to inference tasks where the output space consists of structured objects, for instance graphs representing entities and relations between them.\nIn the context of natural language processing (NLP), structured prediction covers a wide range of problems such as entity and relation extraction, semantic role labeling, and coreference resolution.\nFor example, given the input sentence \\textit{``Tolkien's epic novel The Lord of the Rings was published in 1954-1955, years after the book was completed''} we might seek to extract the following graphs (respectively in a joint entity and relation extraction, and a coreference resolution task):\n\n\\begin{figure}[h]\n\\begin{center}\n    \\includegraphics[width=4.5cm,trim={0 0 0 0},clip]{figures/sentence_graph_1.pdf}\n    \\hspace{.5cm}\n    \\includegraphics[width=5cm,trim={0 0 0 0},clip]{figures/sentence_graph_3.pdf}\n\\end{center}\n\\vspace{-0.2cm}\n\\end{figure}\n\nMost approaches handle structured prediction by employing task-specific discriminators for the various types of relations or attributes, on top of \\pretrained transformer encoders such as BERT \\citep{bert}.\nYet, this presents two limitations.\nFirst, a discriminative classifier cannot easily leverage latent knowledge that the \\pretrained model may already have about the meaning (\\emph{semantics}) of task labels such as \\emph{person} and \\emph{author}.\nFor instance, knowing that a \\emph{person} can write a \\emph{book} would greatly simplify learning the \\emph{author} relation in the example above.\nHowever, discriminative models are usually trained without knowledge of the label semantics (their targets are class numbers), thus preventing such positive transfer.\nSecond, since the architecture of a discriminative model is adapted to the specific task, it is difficult to train a single model to solve many tasks, or to fine-tune a model from a task to another (\\emph{transfer learning}) without changing the task-specific components of the discriminator.\nHence, our main question is: can we design a framework to solve different structured prediction tasks with the same architecture, while leveraging any latent knowledge that the \\pretrained model may have about the label semantics?\n\nIn this paper, we propose to solve this problem with a text-to-text model, by framing it as a task of \\emph{Translation between Augmented Natural Languages} (\\ourmodel).\n\\Cref{fig:main_model} shows how the previous example is handled within our framework, in the case of three different structured prediction tasks.\nThe augmented languages are designed in a way that makes it easy to \\emph{encode} structured information (such as relevant entities) in the input, and to \\emph{decode} the output text into structured information.\n\nWe show that out-of-the-box transformer models can easily learn this augmented language translation task.\nIn fact, we successfully apply our framework to a wide range of structured prediction problems, obtaining new state-of-the-art results on many datasets, and highly competitive results on all other datasets.\nWe achieve this by using the same architecture and hyperparameters on all tasks, the only difference among tasks being the augmented natural language formats.\nThis is in contrast with previous approaches that use task-specific discriminative models.\nThe choice of the input and output format is crucial: by using annotations in a format that is as close as possible to natural language, we allow transfer of latent knowledge that the \\pretrained model has about the task, improving performance especially in a low-data regime.\nNested entities and an arbitrary number of relations\nare neatly handled by our models, while being typical sources of complications for previous approaches.\nWe implement an alignment algorithm to robustly match the structural information extracted from the output sentence with the corresponding tokens in the input sentence.\n\nWe also leverage our framework to train a single model to solve all tasks at the same time, and show that it achieves comparable or better results with respect to training separately on each task.\nTo the best of our knowledge, this is the first model to handle such a variety of structured prediction tasks without any additional task-specific modules.\n\n\\begin{figure}[t]\n\\begin{center}\n    \\includegraphics[width=\\linewidth,trim={0 0 0 0},clip]{figures/translation_graph.pdf}\n\\end{center}\n\\caption{\nOur \\ourmodel{} model translates between input and output text in \\emph{augmented natural language}, and the output is then decoded into structured objects. \n}\n\\label{fig:main_model}\n\\end{figure}\n\nTo summarize, our key contributions are the following.\n\n\\begin{enumerate}[leftmargin=*]\n    \\item  We introduce {\\ourmodel}, a framework to solve several structure prediction tasks in a unified way, with a common architecture and without the need for task-specific modules.\n    We cast structured prediction tasks as translation tasks, by designing augmented natural languages that allow us to encode structured information as part of the input or output.\n    Robust alignment ensures that extracted structure is matched with the correct parts of the original sentence (\\Cref{sec:method}).\n\n    \\item We apply our framework to (1) joint entity and relation extraction; (2) named entity recognition; (3) relation classification; (4) semantic role labeling; (5) coreference resolution; (6) event extraction; (7) dialogue state tracking (\\Cref{sec:sp_tasks,sec:experiments}). In all cases we achieve at least comparable results to the current state-of-the-art, and we achieve new state-of-the-art performance on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012).\n\n    \\item We also train a single model simultaneously on all tasks (multi-task learning), obtaining comparable or better results as compared with single-task models (\\Cref{sec:exp_multitask}).\n    \n    \\item We show that, thanks to the improved transfer of knowledge about label semantics, we can significantly improve the performance in the few-shot regime over previous approaches (\\Cref{sec:exp_low_resource}).\n    \n    \\item We show that, while our model is purely generative (it outputs a sentence, not class labels), it can be evaluated discriminatively by using the output token likelihood as a proxy for the class score, resulting in more accurate predictions (\\Cref{sec:method} and \n    \\Cref{supp:relation-classification}).\n\\end{enumerate}\n\nThe code is available at \\url{https://github.com/amazon-research/tanl}.\n\n\\section{Related work} \\label{sec:related} %\n\nMany classical methods for structured prediction (SP) in NLP are generalizations of traditional classification algorithms and include, among others, Conditional Random Fields\n\\citep{crf2001}, Structured Perceptron \\citep{collins-2002}, and Structured Support Vector Machines\n\\citep{ioannis2004}.\nMore recently, multiple efforts to integrate SP into deep learning methods have been proposed. Common approaches include placing an SP layer as the final layer of a neural net \\citep{collobert2011} and incorporating SP directly into DL models \\citep{dyer-2015-dp}.\n\nCurrent state-of-the-art approaches for SP in NLP train a task-specific classifier on top of the features learned by a \\pretrained language model, such as BERT \\citep{bert}.\nIn this line of work, BERT MRC \\citep{bert_mrc} performs NER using two classification modules to predict respectively the first and the last tokens corresponding to an entity for a given input sentence.\nFor joint entity and relation extraction, SpERT \\citep{spert} uses a similar approach to detect token spans corresponding to entities, followed by a relation classification module.\nIn the case of coreference resolution, many approaches employ a higher-order coreference model \\citep{lee-etal-2018-higher} which learns a probability distribution over all possible antecedent entity token spans.\n\nAlso related to this work are papers on sequence-to-sequence (seq2seq) models for multi-task learning and SP.\n\\citet{t5} describe a framework to cast problems such as translation and summarization as text-to-text tasks in natural language, leveraging the transfer learning power of a transformer-based language model.\nOther sequence-to-sequence approaches solve specific structured prediction tasks by generating the desired output directly: see for example WDec \\citep{Wdec} for entity and relation extraction, and SimpleTOD \\citep{hosseiniasl2020simple} and SOLOIST \\citep{SOLOIST} for dialogue state tracking.\nCloser to us, GSL \\citep{gsl}, which introduced the term \\emph{augmented natural language}, showed early applications of the generative approach in sequence labeling tasks such as slot labeling, intent classification, and named entity recognition without nested entities.\nOur approach is also related to previous works that use seq2seq approaches to perform parsing \\citep{oriol_NIPS2015,dyer-etal-2016-recurrent,choe-charniak-2016-parsing,Rongali_2020}, with the main difference that we propose a general framework that uses augmented natural languages as a way to unify multiple tasks and exploit label semantics.\nIn some cases (e.g., relation classification), our output format resembles that of a question answering task \\citep{multitask-learning-QA}.\nThis paradigm has recently proved to be effective for some structured prediction tasks, such as entity and relation extraction and coreference resolution \\citep{multiturnQA, MRC4ERE, CorefQA}.\nAdditional task-specific prior work is discussed in \\Cref{sec:sp-tasks-appendix}.\n\nFinally, TANL enables easy multi-task structured prediction (\\Cref{sec:exp_multitask}). Recent work has highlighted benefits of multi-task learning \\citep{multitask_learning_sequence_tagging} and transfer learning \\citep{transfer-learning-NLP} in NLP, especially in low-resource scenarios.\n\n\\section{Method} \\label{sec:method}\n\nWe frame structured prediction tasks as text-to-text translation problems. \nInput and output follow specific augmented natural languages that are appropriate for a given task, as shown in \\Cref{fig:main_model}.\nIn this section, we describe the format design concept and the decoding procedure we use for inference.\n\n\\paragraph{Augmented natural languages.} %\nWe use the joint entity and relation extraction task as our guiding example for augmented natural language formats.\nGiven a sentence, this task aims to extract a set of \\emph{entities} (one or more consecutive tokens) and a set of \\emph{relations} between pairs of entities.\nEach predicted entity and relation has to be assigned to an entity or a relation type. %\nIn all the datasets considered, the relations are asymmetric; {\\em i.e.}, it is important which entity comes first in the relation (the \\emph{head} entity) and which comes second (the \\emph{tail} entity).\nBelow is the augmented natural language designed for this task (also shown in \\Cref{fig:main_model}):\n\n\\begin{footnotesize}\n\\begin{customquote}\n\\quotespacestart\n    \\textbf{Input:} Tolkien's epic novel The Lord of the Rings was published in 1954-1955, years after the book was completed.\n    \n    \\textbf{Output:}\n    \\nohyphens{\\entitybegin \\entity{Tolkien} \\separator \\attribute{person} \\entityend's epic novel \\entitybegin \\entity{The Lord of the Rings} \\separator \\attribute{book} \\separator \\attribute{author} \\equals Tolkien \\entityend was published in 1954-1955, years after the book was completed.}\n    \\quotespaceend\n\\end{customquote}\n\\end{footnotesize}\nSpecifically, the desired output replicates the input sentence and augments it with patterns that can be decoded into structured objects. %\nFor this task, each group consisting of an entity and possibly some relations is enclosed by the special tokens \\entitybegin \\entityend.\nA sequence of \\separator-separated tags describes the entity type and a list of relations in the format ``X \\equals Y'', where X is the relation type, and Y is another entity (the \\emph{tail} of the relation).\nNote that the objects of interest are all within the enclosed patterns ``\\entitybegin $\\ldots$ | $\\ldots$ \\entityend''. However, we replicate all words in the input sentence, as it\nhelps reduce ambiguity when the sentence contains more than one occurrence of the same entity.\nIt also improves learning, as shown by our ablation studies (\\Cref{sec:exp_ablation} and \\Cref{sec:ablation-studies}).\nIn the target output sentence, entity and relation types are described in natural words (e.g.\\ \\emph{person}, \\emph{location})\n--- not abbreviations such as \\emph{PER}, \\emph{LOC} ---\nto take full advantage of the latent knowledge that a \\pretrained model has about those words.\n\nFor certain tasks, additional information can be provided as part of the input, such as the span of relevant entities in semantic role labeling or coreference resolution (see \\Cref{fig:main_model}).\nWe detail the input/output formats for all structured prediction tasks in \\Cref{sec:sp_tasks}.\n\n\\paragraph{Nested entities and multiple relations.} \nNested patterns allow us to represent hierarchies of entities.\nIn the following example from the ADE dataset, the entity ``lithium toxicity'' is of type \\textit{disease}, and has a sub-entity ``lithium'' of type \\textit{drug}.\nThe entity ``lithium toxicity'' is involved in multiple relations: one of type \\textit{effect} with the entity ``acyclovir'', and another of type \\textit{effect} with the entity ``lithium''.\nIn general, the relations in the output can occur in any order. \n\n\\begin{footnotesize}\n\\begin{customquote}\n\\quotespacestart\n    \\textbf{Input:}\n    Six days after starting acyclovir she exhibited signs of lithium toxicity.\n    \n    \\textbf{Output:}\n    Six days after starting \\entitybegin \\entity{acyclovir} \\separator \\attribute{drug} \\entityend she exhibited signs of \\entitybegin \\entitybegin \\entity{lithium} \\separator \\attribute{drug} \\entityend \\entity{toxicity} \\separator \\attribute{disease} \\separator \\attribute{effect} \\equals acyclovir \\separator \\attribute{effect} \\equals lithium \\entityend .\n\\quotespaceend\n\\end{customquote}\n\\end{footnotesize}\n\n\\paragraph{Decoding structured objects.}\nOnce the model generates an output sentence in an augmented natural language format, we decode the sentence to obtain the predicted structured objects, as follows. \n\\begin{enumerate}[leftmargin=*]\n\\vspace{-.05cm}\n    \\itemsep0em\n    \\item We remove all special tokens and extract entity types and relations, to produce a cleaned output.\n    If part of the generated sentence has an invalid format, that part is discarded.\n    \\item %\n    We match the input sentence and the cleaned output sentence at the token levels using the dynamic programming (DP) based Needleman-Wunsch alignment algorithm \\citep{needleman1970general}. \n    We then use this alignment to identify the tokens corresponding to entities in the original input sentence.\n    This process improves the robustness against potentially imperfect generation by the model, as shown by our ablation studies (\\Cref{sec:exp_ablation} and \\Cref{sec:ablation-studies}).\n    \\item\n    For each relation proposed in the output, we search for the closest entity that exactly matches the predicted tail entity.\n    If such an entity does not exist, the relation is discarded.\n    \\item We discard entities or relations whose predicted type does not belong to the dataset-dependent list of types.\n\\vspace{-.05cm}\n\\end{enumerate}\n\nTo better explain the DP alignment in step 2, consider the example below where the output contains a misspelled entity word, ``Aciclovir'' (instead of ``acyclovir'').\nThe cleaned output containing the word ``Aciclovir'', tokenized as ``A-cicl-o-vir'', is matched to ``a-cycl-o-vir'' in the input, from which we deduce that it refers to ``acyclovir''.\n\n\\begin{footnotesize}\n\\begin{customquote}\n\\quotespacestart\n    \\textbf{Generated output:}\n        Six days after starting \\entitybegin \\entity{Aciclovir} \\separator \\attribute{drug} \\entityend she exhibited signs of \\entitybegin \\entitybegin \\entity{lithium} \\separator \\attribute{drug} \\entityend \\entity{toxicity} \\separator \\attribute{disease} \\separator \\attribute{effect} \\equals Aciclovir \\separator \\attribute{effect} \\equals lithium \\entityend .\n        \n    \n    \\textbf{Cleaned output:}\n            Six days after starting \\entity{Aciclovir} she exhibited signs of \\entity{lithium} \\entity{toxicity} .\n\\quotespaceend\n\\end{customquote}\n\\end{footnotesize}\n\n\\paragraph{Multi-task learning.} Our method naturally allows us to train a single model on multiple datasets that can cover many structured prediction tasks.\nIn this setting, we add the dataset name followed by the task separator \\taskseparator (for example, ``ade \\taskseparator '') as a prefix to each input sentence.\n\n\\paragraph{Categorical prediction tasks.}\nFor tasks such as relation prediction, where there is a limited number of valid outputs, an alternative way to perform classification is to compute class scores of all possible outputs and predict the class with the highest score.\nWe demonstrate that we can use the output sequence likelihood as a proxy for such score. \nThis method offers a more robust way to perform the evaluation in low resource scenarios where generation can be imperfect (see \\Cref{supp:relation-classification}).\nThis approach is similar to the method proposed by \\cite{dos_santos_2020_beyond} for ranking with language models.\n\n\\section{Structured prediction tasks} \\label{sec:sp_tasks}\n\n\\paragraph{Joint entity and relation extraction.} \nFormat and details for this task are provided in \\Cref{sec:method}.\n\n\\paragraph{Named entity recognition (NER).}%\nThis is an entity-only particular case of the previous task. %\n\n\\paragraph{Relation classification.}\nFor this task, we are given an input sentence with \\emph{head} and \\emph{tail} entities \nand seek to classify the type of relation between them, choosing from a predefined set of relations. %\nSince the head entity does not necessarily precede the tail entity in the input sentence, we add a phrase\n``The relationship between \\entitybegin head  \\entityend and \\entitybegin tail  \\entityend is'' after the original input sentence. \nThe output repeats this phrase, followed by the relation type. In the following example, the head and tail entities are ``Carmen Melis'' and ``soprano'' which have a \\emph{voice type} relation.\n\n\\begin{footnotesize}\n\\begin{customquote}\n\\quotespacestart\n    \\textbf{Input:}\n    \\nohyphens{\n    Born in Bologna, Orlandi was a student of the famous Italian \\entitybegin \\entity{soprano} \\entityend and voice teacher \\entitybegin \\entity{Carmen Melis} \\entityend in Milan.\n    The relationship between \\entitybegin \\entity{Carmen Melis} \\entityend and \\entitybegin \\entity{soprano} \\entityend is}\n    \n    \\textbf{Output:}\n    relationship between \\entitybegin \\entity{Carmen Melis} \\entityend and \\entitybegin \\entity{soprano} \\entityend \\equals \\attribute{voice type}\n\\quotespaceend\n\\end{customquote}\n\\end{footnotesize}\n\n\\paragraph{Semantic role labeling (SRL).}\nHere we are given an input sentence along with a \\emph{predicate}, and seek to predict a list of arguments and their types. %\nEvery argument corresponds to a span of tokens that correlates with the predicate in a specific manner (e.g.\\ subject, location, or time).\nThe predicate is marked in the input, whereas arguments are marked in the output and are assigned an argument type.\nIn the following example, ``sold'' is the predicate of interest.\n\n\\begin{footnotesize}\n\\begin{customquote}\n\\quotespacestart\n    \\textbf{Input:}\n    \\nohyphens{The luxury auto maker last year \\entitybegin \\entity{sold} \\entityend 1,214 cars in the U.S.}\n    \n    \\textbf{Output:}\n    \\entitybegin \\entity{The luxury auto maker} \\separator \\attribute{subject} \\entityend \\entitybegin \\entity{last year} \\separator \\attribute{temporal} \\entityend \n    sold\n     \\entitybegin \\entity{1,214 cars} \\separator \\attribute{object} \\entityend \\entitybegin \\entity{in the U.S.} \\separator \\attribute{location} \\entityend\n\\quotespaceend\n\\end{customquote}\n\\end{footnotesize}\n\n\\paragraph{Event extraction.}\nThis task requires extracting\n(1) event triggers, each indicating the occurrence of a real-world event\nand (2) trigger arguments indicating the attributes associated with each trigger.\nIn the following example, there are two event triggers, ``attacked'' of type \\attribute{attack} and ``injured'' of type \\attribute{injury}. We perform trigger detection using the same format as in NER, as shown below. \nTo perform argument extraction, we consider a single trigger as input at a time.\nWe mark the trigger (with its type) in the input, and we use an output format similar to joint entity and relation extraction.\nBelow, we show an argument extraction example for the trigger ``attacked'', where two arguments need to be extracted, namely, ``Two soldiers'' of type \\attribute{target} and ``yesterday'' of type \\attribute{attack time}.\n\n\\begin{footnotesize}\n\\begin{customquote}\n    \\quotespacestart\n    \\textbf{Trigger extraction input:}\n    \\nohyphens{Two soldiers were attacked and injured yesterday.} \n    \n    \\textbf{Trigger extraction output:}\n    \\nohyphens{Two soldiers were \\entitybegin \\entity{attacked} \\separator \\attribute{attack} \\entityend and \\entitybegin \\entity{injured} \\separator \\attribute{injury} \\entityend yesterday.}\n    \n    \\textbf{Argument extraction input:}\n    \\nohyphens{Two soldiers were \\entitybegin \\entity{attacked} \\separator \\attribute{attack} \\entityend and injured yesterday.}\n    \n    \\textbf{Argument extraction output:}\n    \\entitybegin \\entity{Two soldiers} \\separator \\attribute{individual} \\separator \\attribute{target} \\equals attacked \\entityend were attacked and injured \\entitybegin \\entity{yesterday} \\separator \\attribute{time} \\separator \\attribute{attack time} \\equals attacked \\entityend.\n\\quotespaceend\n\\end{customquote}\n\\end{footnotesize}\n\n\\paragraph{Coreference resolution.}\nThis is the task of grouping individual text spans (\\emph{mentions}) referring to the same real-world entity. \nFor each mention that is not the first occurrence of a group, we reference with the first mention. In the following example, ``his'' refers to ``Barack Obama'' and is marked as \\entitybegin \\entity{his} \\separator \\attribute{Barack Obama} \\entityend in the output.\n\n\\begin{footnotesize}\n\\begin{customquote}\n\\quotespacestart\n    \\textbf{Input:}\n    Barack Obama nominated Hillary Rodham Clinton as his secretary of state on Monday. He chose her because she had foreign affairs experience as a former First Lady.\n    \n    \\textbf{Output:}\n   \\nohyphens{\\entitybegin \\entity{Barack Obama} \\entityend nominated \\entitybegin \\entity{Hillary Rodham Clinton} \\entityend as \\entitybegin \\entity{his} \\separator \\attribute{Barack Obama} \\entityend \\entitybegin \\entity{secretary of state} \\separator \\attribute{Hillary Rodham Clinton} \\entityend on Monday.\t\\entitybegin \\entity{He} \\separator \\attribute{Barack Obama} \\entityend chose \\entitybegin\\entity{her} \\separator \\attribute{Hillary Rodham Clinton} \\entityend because \\entitybegin \\entity{she} \\separator \\attribute{Hillary Rodham Clinton} \\entityend had foreign affairs\texperience as a\tformer \\entitybegin \\entity{First Lady} \\separator \\attribute{Hillary Rodham Clinton} \\entityend.}\n\\quotespaceend\n\\end{customquote}\n\\end{footnotesize}\n\n\\paragraph{Dialogue state tracking (DST).}\nHere we are given as input a history of dialogue turns,\ntypically between a user (trying to accomplish a goal) and an agent (trying to help the user).\nThe desired output is the dialogue state, consisting of a value for each key (or \\emph{slot name}) from a predefined list.\nIn the input dialogue history, we add the prefixes ``\\entitybegin user \\entityend\\taskseparator'' and ``\\entitybegin agent \\entityend\\taskseparator'' to delineate user and agent turns, respectively.\nOur output format consists of a list of all slot names with their predicted values.\nWe add ``\\entitybegin belief \\entityend'' delimiters to help the model know when to stop generating the output sequence.\nWe tag slots that are not mentioned in the dialogue history with the value ``not given'' (we do not show them in the example below, for brevity).\n\n\\begin{footnotesize}\n\\begin{customquote}\n\\quotespacestart\n    \\textbf{Input:}\n    \\nohyphens{\\entitybegin user \\entityend\\taskseparator I am looking for a cheap place to stay \\entitybegin agent \\entityend\\taskseparator How long? } \\entitybegin user \\entityend\\taskseparator Two\n    \n    \\textbf{Output:}\n    \\nohyphens{\\entitybegin belief \\entityend \\attribute{hotel price range} \\entity{cheap}, \\attribute{hotel type} \\entity{hotel}, \\attribute{duration} \\entity{two} \\entitybegin belief \\entityend}\n    \\quotespaceend\n\\end{customquote}\n\\end{footnotesize}\n\n\\section{Experiments}\n\\label{sec:experiments}\n\n\\renewcommand{\\ourmodel}{{\\textbf{TANL}}}\n\\newcolumntype{C}{>{\\centering\\arraybackslash}X}\n\\newcommand{\\modelcolwidth}{4.55 cm}\n\\newcommand{\\tanlspace}{0.085cm}  %\n\\newcommand{\\betweentablespace}{\\vspace{0.16cm}}\n\\newcommand{\\best}[1]{\\textbf{#1}}\n\\newcommand{\\topleftdesc}[2]{\\multirow{#2}{*}{ \\rotatebox{90}{ \\textbf{#1} }  } & }\n\\newcommand{\\sidedescwidth}{0.2cm}\n\\newcommand{\\pad}{ & }\n\\begin{table}[]\n\\centering\n\\small\n\\setlength{\\tabcolsep}{5pt}\n\\vspace{-.3cm}\n\\caption{Results on all tasks. All numbers indicate F1 scores except noted otherwise. Datasets marked with an asterisk (*) have nested entities.\n}\n\\label{tab:results}\n\n\\begin{tabularx}{\\textwidth}{p{\\sidedescwidth} p{\\modelcolwidth} *{8}{C}}\n\n \\topleftdesc{Entity Relation Extr.}{13}\n    & \\multicolumn{2}{c}{\\bf CoNLL04} & \\multicolumn{2}{c}{{\\bf ADE}*} & \\multicolumn{2}{c}{\\bf NYT} &\n    \\multicolumn{2}{c}{\\bf ACE2005} \\\\\n\n   \\cmidrule(lr){3-4} \\cmidrule(lr){5-6} \\cmidrule(lr){7-8} \\cmidrule(lr){9-10}\n &  & Entity & Rel. & Entity & Rel. & Entity & Rel. & Entity & Rel. \\\\\n    \\cmidrule(lr){2-10}\n    \n &  SpERT \\citep{spert} & 88.9 & 71.5 & 89.3 & 78.8 && \\\\\n &  DyGIE \\citep{dygie} & && && && 88.4 & 63.2 \\\\\n    \n &  MRC4ERE \\citep{MRC4ERE} & 88.9 & 71.9 & && && 85.5 & 62.1  \\\\\n &  RSAN \\citep{RSAN} & & & & & & 84.6 & &  \\\\\n [\\tanlspace]\n\n    \n &  \\ourmodel                   & 89.4 & 71.4 &  90.2 &  80.6 & \\best{94.9} & \\best{90.8} &  \\best{88.9} &  \\best{63.7} \\\\\n &  \\ourmodel{} (multi-dataset) & 89.8 & \\bf 72.6 &\t90.0 &\t80.0 &\t94.7 &\t90.5 &\t88.2 &\t62.5\n \\\\\n &  \\ourmodel{} (multi-task)    & \\best{90.3} & 70.0 & \\best{91.2} & \\best{83.8} & 94.7 & 90.7 \\\\ %\n\\end{tabularx}\n\n\\betweentablespace\n\n\\begin{tabularx}{\\textwidth}{p{\\sidedescwidth} p{\\modelcolwidth} *{4}{C}}\n    \\topleftdesc{NER}{10}\n     & {\\bf CoNLL03} & {\\bf OntoNotes} & {{\\bf GENIA}*} & {{\\bf ACE2005}*} \\\\\n\n    \\cmidrule(lr){2-6}\n    \n &  BERT-MRC \\citep{bert_mrc} & 93.0 & 91.1 &  \\best{83.8} & \\best{86.9} \\\\\n &  BERT-MRC+DSC \\citep{dice_loss} & 93.3 & \\best{92.1} & \\\\\n &  Cloze-CNN \\citep{cloze_pretrain} & \\best{93.5} && \\\\\n &  GSL \\citep{gsl} & 90.7 & 90.2 & \\\\ \n    [\\tanlspace]\n &  \\ourmodel{} & 91.7 & 89.8 & 76.4 & 84.9 \\\\\n &  \\ourmodel{} (multi-dataset) & 92.0 & 89.8 & 75.9 & 84.4 \\\\ \n &  \\ourmodel{} (multi-task) & 91.7 & 89.4 & 76.4 & \\\\ %\n\\end{tabularx}\n\n\\betweentablespace\n\n\\begin{tabularx}{\\textwidth}{p{\\sidedescwidth} p{\\modelcolwidth} *{6}{C}}\n    \\topleftdesc{Relation Class.}{14}\n    & %\n    & \\multicolumn{4}{c}{\\textbf{FewRel 1.0} (validation)} \\\\\n    \\cmidrule(lr){4-7}\n    \n\\pad & \\multirow{2}{*}{\\bf TACRED } & 5-way 1-shot & 5-way 5-shot & 10-way 1-shot & 10-way 5-shot \\\\\n\n    \\cmidrule(lr){2-7}\n    \n\\pad BERT-EM \\citep{matching_the_blank} & 70.1 & 88.9 && 82.8 \\\\\n\\pad BERT$_{\\text{EM}}$+MTB \\citep{matching_the_blank} & 71.5 & 90.1 && \\best{83.4} \\\\\n\\pad DG-SpanBERT \\citep{dg-spanbert} & 71.5 & \\\\\n\\pad BERT-PAIR \\citep{bertpair_fewrel} & & 85.7 & 89.5 & 76.8 & 81.8 \\\\\n    [\\tanlspace]\n\\pad \\ourmodel{} & \\best{71.9} & \\best{94.0 \\tpm 4.1}  & \\best{96.4 \\tpm 4.2}  & 82.6 \\tpm 4.5  & \\best{88.2 \\tpm 5.9} \\\\\n\\pad \\ourmodel{} (multi-task) &  69.1 & \\\\ %\n\\end{tabularx}\n\n\\betweentablespace\n\n\\begin{tabularx}{\\textwidth}{p{\\sidedescwidth} p{\\modelcolwidth} *{3}{C}}\n    \\topleftdesc{SRL}{9}\n    & \\textbf{CoNLL05 WSJ}          & \\textbf{CoNLL05 Brown}         & \\textbf{CoNLL2012}               \\\\\n    \\cmidrule(lr){2-5}\n\\pad Dep and Span \\citep{span_dependency_srl} & 86.3 & 76.4 & 83.1 \\\\\n\\pad BERT SRL \\citep{bert_re_srl}               & 88.8 & 82.0   & \\,86.5 \\\\\n[\\tanlspace]\n\\pad \\ourmodel{}                    &    89.3                      &  82.0              &            \\textbf{87.7}      \\\\\n\\pad \\ourmodel{} (multi-dataset) & \\best{89.4}  & \\best{84.3}  & 87.6  \\\\\n\\pad \\ourmodel{} (multi-task) & 89.1  & 84.1  & 87.7  \\\\ %\n\\end{tabularx}\n\n\\betweentablespace\n\n\\begin{tabularx}{\\textwidth}{p{\\sidedescwidth} p{\\modelcolwidth} *{4}{C}}\n    \\topleftdesc{Event Extr.}{10}\n    & \\multicolumn{4}{c}{\\textbf{ACE2005}} \\\\\n    \\cmidrule(lr){3-6} %\n    \n\\pad & \n    {\\makecell{Trigger Id.}} &  {\\makecell{Trigger Cl.}} & {\\makecell{Argument Id.}} &  {\\makecell{Argument Cl.}} \\\\\n    \\cmidrule(lr){2-6} %\n    \n    \n\\pad J3EE \\citep{nguyen2019one} & 72.5 & \\best{69.8} & \\best{59.9} & 52.1 \\\\\n\\pad DyGIE++ \\citep{wadden2019entity}  &  & 69.7 & 55.4 & \\,\\best{52.5} \\\\\n    [\\tanlspace]\n\\pad \\ourmodel{} & \\best{72.9} & 68.4 & 50.1 & 47.6  \\\\\n\\pad \\ourmodel{} (multi-task) & 71.8 & 68.5 & 48.5 & 48.5 \\\\\n\\end{tabularx}\n\n\\betweentablespace\n\n\\begin{tabularx}{\\textwidth}{p{\\sidedescwidth} p{5.2cm} *{8}{C}}\n    \\topleftdesc{Coreference Res.}{12}\n    & \\multicolumn{8}{c}{\\textbf{CoNLL-2012}* (BERT-base $\\brokenvert$ BERT-large) } \\\\\n    \\cmidrule(lr){3-10} %\n    \\pad \\bf & \\multicolumn{2}{c}{MUC} & \\multicolumn{2}{c}{B$^{3}$} & \\multicolumn{2}{c}{CEAF$_{\\phi_{4}}$} & \\multicolumn{2}{c}{Avg.\\ F1} \\\\\n    \\cmidrule(lr){2-10} %\n\\pad Higher-order c2f-coref \\citep{lee-etal-2018-higher} & \\multicolumn{1}{c:}{80.4} & & \\multicolumn{1}{c:}{70.8} &  & \\multicolumn{1}{c:}{67.6} &  & \\multicolumn{1}{c:}{73.0} & \\\\ \n\\pad SpanBERT\n\\citep{spanbert} & \\multicolumn{1}{c:}{} & 85.3 & \\multicolumn{1}{c:}{} & 78.1 & \\multicolumn{1}{c:}{} & 75.3 & \\multicolumn{1}{c:}{} & 79.6 \\\\\n\\pad BERT+c2f-coref \\citep{joshi2019bert} & \\multicolumn{1}{c:}{81.4} & 83.5 & \\multicolumn{1}{c:}{71.7} & 75.3 & \\multicolumn{1}{c:}{68.8} & 71.9 & \\multicolumn{1}{c:}{73.9} & 76.9 \\\\\n\\pad CorefQA+SpanBERT \\citep{CorefQA} & \\multicolumn{1}{c:}{\\bf 86.3} & \\bf 88.0 & \\multicolumn{1}{c:}{\\bf 77.6} & \\bf 82.2 & \\multicolumn{1}{c:}{\\bf 75.8} & \\bf 79.1 & \\multicolumn{1}{c:}{\\bf 79.9} & \\bf \\,83.1 \\\\\n    [\\tanlspace]\n\\pad \\ourmodel{}             & \\multicolumn{1}{c}{81.0} & & \\multicolumn{1}{c}{69.0} & & \\multicolumn{1}{c}{68.4} & & \\multicolumn{1}{c}{72.8} & \\\\\n\\pad \\ourmodel{} (multi-task) & \\multicolumn{1}{c}{78.7} &  & \\multicolumn{1}{c}{65.7} & & \\multicolumn{1}{c}{63.8} & & \\multicolumn{1}{c}{69.4} & \\\\\n\\end{tabularx}\n\n\\betweentablespace\n\n\\begin{tabularx}{\\textwidth}{p{\\sidedescwidth}  p{5.2cm} *{2}{C}}\n    \\topleftdesc{DST}{7} \n    & \\multicolumn{2}{c}{\\textbf{MultiWOZ 2.1} (Joint Accuracy)} \\\\\n    \\cmidrule(lr){2-4}\n\\pad TRADE \\citep{WuTradeDST2019} & 45.6 \\\\\n\\pad SimpleTOD \\citep{hosseiniasl2020simple} & \\bf \\,55.7  \\\\\n    [\\tanlspace]\n\\pad \\ourmodel{} & 50.5 \\\\\n\\pad \\ourmodel{} (multi-task) & 51.4 \\\\\n\\end{tabularx}\n\\end{table}\n\\renewcommand{\\ourmodel}{TANL}\n\nIn this section, we show that our TANL framework, with the augmented natural languages outlined in \\Cref{sec:sp_tasks}, can effectively solve the structured prediction tasks considered and exceeds the previous state of the art on multiple datasets. \n\nAll our experiments start from a \\pretrained T5-base model \\citep{t5}. %\nTo keep our framework as simple as possible, hyperparameters are the same across all experiments, except for some dataset-specific ones, such as the maximum sequence length.\nDetails about the experimental setup, datasets, and baselines are described in \\Cref{sec:experimental-setup}.\n\n\\begin{figure*}[]\n\\vspace{-.3cm}\n\\centering\n    \\newcommand{\\plotwidth}{0.48\\textwidth}\n  \\begin{subfigure}[t]{\\plotwidth}\n    \\includegraphics[trim=20 0 30 21, clip, width=1.\\textwidth]{figures/low_resourse_relation.pdf}\n    \\caption{Low-resource scenarios} \\label{fig:low_resource_conll04_relation}\n  \\end{subfigure}\n  \\begin{subfigure}[t]{\\plotwidth}\n    \\includegraphics[trim=20 0 30 21, clip, width=1.\\textwidth]{figures/ablation_relation_diff.pdf}\n    \\caption{Ablation studies }\\label{fig:ablation_relation}\n  \\end{subfigure}\n\\vspace{-.2cm}\n\\caption{\nExperiments on the CoNLL04 dataset.\n(a) Our model outperforms the previous state-of-the-art \nmodel SpERT, in low-resource scenarios.\n(b) Ablation studies where we remove label semantics (numeric labels), augmented natural language format (abridged output) or dynamic programming alignment (no DP alignment), and plot the score difference with the non-ablated \\ourmodel{}.\n}\n\\label{fig:low-resource-conll}\n\\vspace{-.4cm}\n\\end{figure*}\n\n\\subsection{Single-task and Multi-task experiments}  \\label{sec:exp_singletask}  \\label{sec:exp_multitask}\nWe use three data settings in our experiments: (1) single dataset, (2) multiple datasets for the same task (multi-dataset), and (3) all datasets across all tasks (multi-task).\n\\Cref{tab:results} shows the results.\\footnote{We are grateful to Wenxuan Zhou and Tianyu Gao for pointing out an inconsistency in computing results on the TACRED dataset, which have been corrected in \\Cref{tab:results}.}\n\nWith the single-task setup, we achieve state-of-the-art performance on the following datasets: ADE, NYT, and ACE2005 (joint entity and relation extraction), FewRel and TACRED (relation classification), CoNLL-2005 and CoNLL-2012 (semantic role labeling). \nFor example, we obtain a +6.2 absolute improvement in F1 score on the NYT dataset over the previous state of the art.\nInterestingly, this result is higher than the performance of models that use ground-truth entities to perform relation extraction, such as REDN \\citep{REDN}, which achieves a relation F1 score of 89.8. %\nIn coreference resolution, \\ourmodel{} performs similarly to previous approaches that employ a BERT-base model, except for CorefQA \\citep{CorefQA}.\nTo the best of our knowledge, ours is the first end-to-end approach to coreference resolution not requiring a separate mention proposal module and not enforcing a maximum mention length.\n\nFor other datasets, we obtain a competitive performance within a few points of the best baselines. \nWe highlight that our approach uses a single model architecture that can be trained to perform \\emph{any} of the tasks without model modification. This is in stark contrast with typical discriminative models, which tend to be task-specific, as can be seen from \\Cref{tab:results}.\n\nIn fact, under this unified framework, a single model can be trained to perform multiple or all tasks at once, with the performance being on par or even better than the single-task setting. %\nIn particular, when the dataset sizes are small such as in ADE or CoNLL04, we obtain sizable improvements and become the new state of the art (from 80.6 to 83.7 for ADE relation F1, and from 89.4 to 90.6 for CoNLL04 entity F1).\nThe only case where our multi-task model has notably lower scores is coreference resolution, where the input documents are much longer than in the other tasks.\nSince the maximum sequence length in the multi-task experiment (512 tokens) is smaller than in the single-dataset coreference experiment (1,536 tokens for input and 2,048 for output), the input documents need to be split into smaller chunks, and this hurts the model's ability to connect multiple mentions of the same entity across different chunks.\nFrom the multi-task experiment, we leave out all datasets based on ACE2005 except for event extraction due to overlap between train and test splits for different tasks.\nWe discuss our experiments in more detail in \\Cref{sec:sp-tasks-appendix}.\n\nAll results presented in this paper are obtained from a \\pretrained T5-base model.\nIn principle, any \\pretrained generative language model can be used, such as BART \\citep{bart} or GPT-2 \\citep{gpt2}.\nIt would be interesting to check whether these models are as capable as T5 (or even better) at learning to translate between our augmented languages.\nWe leave this as a direction for future investigation.\n\n\\subsection{Low-resource settings} \\label{sec:exp_low_resource}\nMultiple experiments suggest that \\ourmodel{} is data-efficient compared to other baselines.\nOn the FewRel dataset, a benchmark for few-shot relation classification, our model outperforms the best baselines BERT$_{\\text{EM}}$ and BERT$_{\\text{EM}}$+MTB \\citep{bert,matching_the_blank}, where the MTB version uses a large entity-linked text corpus for \\pretraining.\nOn the TACRED relation classification dataset, our model also improves upon the best baselines (from 71.5 to 71.9).\nWhile TACRED is not specifically a few-shot dataset, we observe that there are many label types that rarely appear in the training set, some of them having less than 40 appearances out of approximately 70,000 training label instances.\nWe show the occurrence statistics for all label types in the appendix (\\Cref{tab:sup_tacred}), demonstrating that the dataset is highly imbalanced.\nNonetheless, we find that our model performs well, even on instances involving scarce label types. \nThis ability distinguishes our models from other few-shot approaches such as prototypical networks \\citep{protonet} or matching networks \\citep{matching_net}, which are designed only for few-shot scenarios but do not scale well on real-world data which often contains a mix of high and low-resource label types. \n\nOur low-resource study on the joint entity and relation extraction task also confirms that our approach is more data-efficient compared to other methods.\nWe experiment on the CoNLL04 dataset, using only 0.8\\% (9 sentences) to 6\\% (72 sentences) of the training data. Our approach outperforms SpERT (a state-of-the-art discriminative model for joint entity and relation extraction) in this low-resource regime, whereas the performance is similar when using the full training set.\n\nThanks to the unified framework, we can easily train on a task, potentially with larger resources, and adapt to other low-resource end tasks (transfer learning). To show this, we train a model with a large dataset from joint entity and relation extraction (NYT) and fine-tune it on a limited portion of the CoNLL04 dataset (\\Cref{fig:low-resource-conll}), obtaining a significant increase in performance (up to +9 relation F1).\n\nFinally, in \\Cref{sec:appendix-errors} we analyze how the size of the training dataset affects the number of generation errors of our model.\n\n\\subsection{Ablation studies} \\label{sec:exp_ablation}\nWe conduct ablation studies to demonstrate that label semantics, augmented natural language format, and optimal alignment all contribute to the effectiveness of \\ourmodel{} (\\Cref{fig:ablation_relation}).\nFurther details on these ablation studies can be found in \\Cref{sec:ablation-studies}.\n\n\\textbf{Numeric labels:} To prevent the model from understanding the task through label semantics, we use numeric labels. This substantially hurts the performance, especially in a low-resource setting where transfer learning is more important.\n\\textbf{Abridged output:} Second, to determine the impact of the augmented natural language format outlined in \\Cref{sec:sp_tasks}, we experiment with a format which does not repeat the entire input sentence.\nWe find that this abridged format consistently hurts model performance, especially in low-resource scenarios. \nIn other tasks, we generally find that a more natural-looking format usually performs better (see \\Cref{supp:relation-classification}).\n\\textbf{No DP alignment:} We use exact word matching instead of the dynamic programming alignment described in \\Cref{sec:method}.\n\n\\section{Discussion and Conclusion} \\label{sec:discussion}\n\nWe have demonstrated that our unified text-to-text approach to structured prediction can handle all the considered tasks within a simple framework and offers additional benefits in low-resource settings.\nUnlike discriminative models common in the literature, \\ourmodel{} is generative as it translates from an input to an output in augmented natural languages.\nThese augmented languages are flexible and can be designed to handle a variety of tasks, some of which are complex and previously required sophisticated prediction modules. %\nBy streamlining all tasks to be compatible with a single model, multi-task learning becomes seamless and yields state-of-the-art performance for many tasks.\n\nGenerative models, and in particular sequence-to-sequence models, have been used successfully in many NLP problems such as machine translation, text summarization, etc. %\nThese tasks involve mappings from one \\emph{natural} language input to another \\emph{natural} language output. %\nHowever, the use of sequence modeling for structured prediction has received little consideration.\nThis is perhaps due to the perception that the generative approach is too unconstrained and that it would not be a robust way to generate a precise output format that corresponds to structured objects, or that it may add an unnecessary layer of complexity with respect to discriminative models.\nWe demonstrate that this is quite the opposite.\nThe generative approach can easily handle disparate tasks, even at the same time, by outputting specific structures appropriate for each task with little, if any, format error.\n\nWe note that one drawback of the current generative approach is that the time complexity for each token generation is $\\O(L^2)$ where $L$ is the sentence length. %\nHowever, there have been recent advances in the attention mechanism that reduce the complexity to $\\mathcal{O}(L \\log L)$ as in Reformer \\citep{reformer}, or to $\\mathcal{O}(L)$ as in Linformer \\citep{linformer}.\nIncorporating these techniques in the future can significantly reduce computation time and allow us to tackle more complex tasks, as well as improve on datasets with long input sequences such as in coreference resolution.\n\nBased on our findings, we believe that generative modeling is highly promising but has been an understudied topic in structured prediction.\nOur findings corroborate a recent trend where tasks typically treated with discriminative methods have been successfully solved using generative approaches \\citep{gpt3,fusion_in_decoder,small_lm_fewshot}.\nWe hope our results will foster further research in the generative direction.\n\n \n\n    \n    \n\n    \n\n    \n\n    \n    \n\n    \n\n    \n\n    \n    \n    \n\n\\clearpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Entity-to-Text based Data Augmentation for various \\\\Named Entity Recognition Tasks}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\n\nData augmentation techniques have been used to alleviate the problem of scarce labeled data in various NER tasks (flat, nested, and discontinuous NER tasks). Existing augmentation techniques either manipulate the words in the original text that break the semantic coherence of the text, or exploit generative models that ignore preserving entities in the original text, which impedes the use of augmentation techniques on nested and discontinuous NER tasks. In this work, we propose a novel \\textit{Entity-to-Text} based data augmentation technique named {\\modelname} to add, delete, replace or swap entities in the entity list of the original texts, and adopt these augmented entity lists to generate semantically coherent and entity preserving texts for various NER tasks. Furthermore, we introduce a diversity beam search to increase the diversity during the text generation process. Experiments on thirteen NER datasets across three tasks (flat, nested, and discontinuous NER tasks) and two settings (full data and low resource settings) show that {\\modelname} could bring more performance improvements compared to the baseline augmentation techniques.\n\n\\end{abstract}\\section{Introduction}\n\\label{sec:intro}\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=0.99\\linewidth]{figures/example.pdf}\n    \\caption{Comparison of augmented cases generated by Rule-based model and \\textit{Text-to-Text} based generative model vs. Our \\textit{Entity-to-Text} based generative model.}\n    \\label{fig:example}\n\\vspace{-4mm}\n\\end{figure}\nRecent neural networks show decent performance when a large amount of training data is available.\nHowever, these manually labeled data are labor-intensive to obtain. Data augmentation techniques \\cite{shorten2019survey} expand the training set by generating synthetic data to improve the generalization and scalability of deep neural networks, and are widely used in NLP \\cite{feng2021survey,li2022data}.\nOne successful attempt for data augmentation in NLP is manipulating a few words in the original text, such as word swapping \\cite{csahin2018data,min2020syntactic} and random deletion \\cite{kobayashi2018contextual,wei2019eda}. These methods generate synthetic texts effortlessly without considering the semantic coherence of sentences.\nMore importantly, these augmentation approaches work on sentence-level tasks like classification but cannot be easily applied to fine-grained and fragile token-level tasks like Named Entity Recognition (NER).\n\nNamed Entity Recognition aims at inferring a label for each token to indicate whether it belongs to an entity and classifies entities into predefined types. Due to transformations of tokens that may change their labels, \\citet{dai-adel-2020-analysis} augment the token-level text by randomly replacing a token with another token of the same type. However, it still inevitably introduces incoherent replacement and results in syntax-incorrect texts. DAGA \\cite{ding-etal-2020-daga} and MELM \\cite{zhou2022melm} investigate the Text-to-Text data augmentation technique using generative methods that preserve semantic coherence and recognize entities through entity tagging during text generation.\nHowever, since it is difficult to use flat $\\langle B-Type\\rangle$ and $\\langle I-Type\\rangle$ labels to mark nested and discontinuous entities during text generation, these methods can only be used for flat NER tasks. In addition, only the entities are masked during the generation process, so that the diversity of generated texts is also limited. For example, as shown in Figure \\ref{fig:example}, rule-based models replace tokens or shuffle segments, such as ``with'' and ``cancer may'' are shuffled, which makes the augmented text no longer semantically coherent, and even modifies the semantic consistency of the text to affect the prediction of entity labels. The Text-to-Text based generative models cannot leverage flat $\\langle B-Type\\rangle$ and $\\langle I-Type\\rangle$ labels to mark the ``stomach'' token in the discontinuous entities: ``stomach discomfort'' and ``stomach pain'', thus limiting the application of this method to nested and discontinuous NER tasks.\n\nTo maintain text semantic coherence during augmentation and preserve entities for various NER tasks, in this work, we propose a novel \\textbf{Entity-to-Text} instead of \\textbf{Text-to-Text} based data augmentation approach named {\\modelname}. As illustrated in Figure \\ref{fig:overview}, we first obtain the entity list [EU, German, British] in the original text, and then add, delete, swap, and replace the entity in the entity list to obtain the augmented entity list, e.g. [EU, German, British, Spanish]. We investigate that leveraging the rule-based methods to modify the entities in the entity list could generate more combinatorial entity lists without introducing grammatical errors. Then we adopt a conditional language model to generate the semantically coherent augmented text based on the augmented entity list. Thanks to the augmented entity list (including flat, nested, and discontinuous entities) we have already obtained, we can mark these preserved entities in the augmented text as shown in Figure \\ref{fig:illustration}. Since the augmented entity list provide the \nsimilar entity information in the text augmented by the language model, which may leads to insufficient diversity of text generation. Therefore, we propose a diversity beam search method for generative models to enhance text diversity.\nOverall, the main contributions of this work are as follows:\n\\squishlist\n\\item To the best of our knowledge, we propose the first Entity-to-Text based data augmentation technique {\\modelname}. {\\modelname} leverages the pretrained large language model with semantic coherence and entity preserving to generate the augmented text, which could be used to benefit for all NER tasks (flat, nested, and discontinuous NER tasks).\n\\item We propose the diversity beam search strategy for {\\modelname} to increase the diversity of the augmented text during generation process. \n\\item We show that {\\modelname} outperforms strong data augmentation baselines across three NER tasks and two settings (full data and low resource settings).\n\\squishend\n\n\\begin{table}\n\\centering\n\\scalebox{0.56}{\n\\begin{tabular}{clccccc}\n\\thickhline\n\\multicolumn{2}{c}{\\multirow{2}{*}{Techniques}} & \\multicolumn{1}{c}{\\multirow{2}{*}{Coher.}}& \n\\multicolumn{1}{c}{\\multirow{2}{*}{Diver.}}& \\multicolumn{3}{c}{NER Tasks} \n\\\\ \\cmidrule(lr){5-7} \n& & & & Flat & Nested & Discon.  \\\\\n\\midrule\n\n\\multicolumn{1}{c}{\\multirow{4}{*}{\\makecell[c]{Rule \\\\ Based\\\\ Techniques}}} &Label-wise token rep. & -- & -- & \\Checkmark & \\Checkmark& \\Checkmark \\\\ \n&Synonym replacement & -- & -- & \\Checkmark & \\Checkmark& \\Checkmark \\\\ \n&Mention replacement & -- & -- & \\Checkmark & \\Checkmark& \\xmark\\\\ \n&Shuffle within segments & -- & -- & \\Checkmark & \\Checkmark& \\Checkmark \\\\ \n\n\\midrule\n\\multicolumn{1}{c}{\\multirow{3}{*}{\\makecell[c]{Generative \\\\ Techniques}}} &DAGA \\cite{ding-etal-2020-daga} & \\Checkmark & -- & \\Checkmark & \\xmark& \\xmark \\\\ \n&MELM \\cite{zhou2022melm} & \\Checkmark & -- & \\Checkmark &\\xmark & \\xmark \\\\ \n\\cmidrule(lr){2-7} \n&\\textbf{\\modelname} & \\Checkmark & \\Checkmark & \\Checkmark & \\Checkmark& \\Checkmark \\\\ \n\n\\thickhline\n\\end{tabular}}\n\\caption{Comparison of different categories of techniques. ``Coher.'' means ``Semantic Coherence'' and ``Diver.'' means ``Diveristy''.}\n\\label{tab:comparsion}\n\\vspace{-4mm}\n\\end{table}\n\\begin{figure*}[t!]\n    \\centering\n    \\includegraphics[width=0.94\\linewidth]{figures/model.pdf}\n    \\caption{Overview of the proposed Entity-to-Text based data augmentation approach {\\modelname}. We first augment entity list via adding, deleting, replacing and swapping entities. Then the augmented entities will generate texts by adopting pretrained language model with diversity beam search. We finally mark the preserved entities in the augmented texts. Note that the texts that do not match the preserved entity list will be discarded.} \n    \\label{fig:overview}\n\\vspace{-0.1in}\n\\end{figure*}\n\\section{Related Work}\n\\label{sec:related}\n\n\\subsection{Various NER Tasks}\nNamed Entity Recognition (NER) is a pivotal task in IE which aims at locating and classifying named entities from texts into the predefined types such as \\texttt{PERSON}, \\texttt{LOCATION}, etc. \\cite{chiu2016named,xu2017local,yu2020named}. In addition to flat NER task \\cite{sang2003introduction}, \\citet{kim2003genia} proposed nested NER task in the molecular biology domain. For example, in the text: \\textit{Alpha B2 proteins bound the PEBP2 site}, the entity \\textit{PEBP2} belongs to the type \\texttt{PROTEIN} and \\textit{PEBP2 site} belongs to \\texttt{DNA}. \n\nFurthermore, some entities recognized in the text could be discontinuous \\cite{Mowery2013Task1S, Mowery2014Task2S, karimi2015cadec}. For example, in the text: \\textit{I experienced severe pain in my left shoulder and neck}, the entities \\textit{pain in shoulder} and \\textit{pain in neck} contain non-adjacent mentions. Some previous works proposed the unified frameworks which are capable of handling both three NER tasks \\cite{li-etal-2020-unified,yan-etal-2021-unified-generative,li-etal-2021-span}. However, there is no unified data augmentation method designed for all three NER tasks due to the complexity of entity overlap. In this work, we try to bridge this gap and propose the first generative augmentation approach {\\modelname} that can be used to generate augmented data for all NER tasks (flat, nested, and discontinuous NER tasks). \n\n\\subsection{Data Augmentation for NLP and NER}\nAs shown in Table \\ref{tab:comparsion}, we compare {\\modelname} with rule-based and traditional generative techniques, and present the comparison results below.\n\\paragraph{Rule-based Augmentation}\nVarious rule-based augmentations for NLP tasks such as word replacement \\cite{zhang2015character,cai2020data}, random deletion \\cite{kobayashi2018contextual,wei2019eda}, and word swapping \\cite{csahin2018data,min2020syntactic} manipulate the words in the original texts to generate synthetic texts.\nHowever, these manipulated tokens could not maintain the original labels since the change of syntax and semantics.\n\n\\citet{dai-adel-2020-analysis} proposes a replacement augmentation method to decide whether the selected token should be replaced by a binomial distribution, and if so, then the token will be replaced by another token with the same label. Furthermore, the similar approaches could be extended from token-level to mention-level.\nHowever, these methods still inevitably introduce incoherent replacement. In this work, we try to introduce the Entity-to-Text based augmentation approach to improve the coherence of the augmented texts.\n\n\\paragraph{Generative Augmentation}\nClassic generative augmentations for NLP tasks such as back translation, which could be used to train a question answering model \\cite{yu2018qanet} or transfer texts from a high-resource language to a low-resource language \\cite{hou2018sequence,xia2019generalized}. \n\\citet{anaby2020not,kumar2020data} adopt language model which is conditioned on sentence-level tags to modify original data for classification tasks exclusively. \nTo utilize generative augmentation on more fine-grained and fragile token-level NER tasks, \\citet{ding-etal-2020-daga} treats the NER labeling task as a text tagging task and requires generative models to annotate entities during generation. \\citet{zhou2022melm} builds the pre-trained masked language models on corrupted training sentences and focuses on entity replacement. However, these methods rely on the Text-to-Text based generative models which cannot tag a token with nested labels during generation. In this work, we adopt the Entity-to-Text based generative model to tackle all NER tasks and bootstrap the diversity of the model with diversity beam search.\n\n\\section{General NER Task Formulation}\\label{NER Tasks}\nConsidering that {\\modelname} has sufficient augmentation ability on flat, nested and discontinuous NER, we first formulate the general NER task framework as follows. Given an input text ${X = [x_{1}, x_{2}, ..., x_{n}]}$ of length ${n}$ and the entity type set ${T}$, the output is an entity list ${E = [{e}_{1}, {e}_{2}, ..., {e}_{m}, ..., {e}_{l}]}$ of ${l}$ entities, where ${{e}_{m}=\\left[s_{m1}, d_{m1}, ..., s_{mj}, d_{mj}, t_{m}\\right]}$. The ${s,d}$ are the start and end indexes of a space in the text ${X}$. The ${j}$ indicates that the entity consists of ${j}$ spans. The $t_{m}$ is an entity type in the entity type set ${T}$. \nFor example, the discontinuous entity \\texttt{stomach pain} in the text: ``\\textit{The cancer patient has constant \\underline{stomach} discomfort and \\underline{pain}}'' will be represented as ${{e}_{m}=\\left[5,5,8,8,DISORDER\\right]}$.\n\n\\section{Proposed Method}\nThe proposed Entity-to-Text based data augmentation approach {\\modelname} consists of three modules: Entity List Augmentation, Entity-to-Text Generation, and Augmented Text Exploitation. Now we give the details of the three modules.\n\\label{sec:baseline}\n\n\\subsection{Entity List Augmentation}\nEntity List Augmentation aims to adopt four rule-based methods: Add, Delete, Replace, and Swap to modify the entities in the entity list obtained from the original sentences. Now, we give the details of four operations on the original entity list ${E = [{e}_{1}, {e}_{2}, ..., {e}_{m}, ..., {e}_{l}]}$ as follows:\n\\begin{enumerate}[label=\\protect\\circled{\\arabic*}]\n    \\item \\textbf{Add}. We first randomly select an entity ${{e}_{m}}$ from the entity list ${E}$. Then we search for other entities in the training set and add ${{e}_{m}^{'}}$ with the same entity type as ${{e}_{m}}$ to the original entity list: ${E = [{e}_{1}, {e}_{2}, ..., {e}_{m}, {e}_{m}^{'}, ..., {e}_{l}]}$.\n    \\item \\textbf{Delete}. We randomly select an entity ${{e}_{m}}$ from the original entity list ${E}$ and delete it as ${E = [{e}_{1}, {e}_{2}, ..., {e}_{m-1}, {e}_{m+1},..., {e}_{l}]}$.\n    \\item \\textbf{Replace}. We first randomly select an entity ${{e}_{m}}$ from the original entity list ${E}$. Similar to  \\circledd{1}, we search ${{e}_{m}^{'}}$ with the same entity type to replace ${{e}_{m}}$ as ${E = [{e}_{1}, {e}_{2}, ..., {e}_{m}^{'}, ..., {e}_{l}]}$.\n    \\item \\textbf{Swap}. We randomly select two entities ${{e}_{m}}$, ${{e}_{m}^{'}}$ in the original entity list ${E}$ and swap their positions as ${E = [{e}_{1}, {e}_{2}, ..., {e}_{m}^{'},}$\n    ${ ...,{e}_{m},..., {e}_{l}]}$.\n\\end{enumerate}\n\n\\subsection{Entity-to-Text Generation}\nAfter we obtain the augmented entity lists, the Entity-to-Text Generation module aims to generate the text for each entity list. Since the augmented entity list provide the similar entity information for augmented text, so we propose a diversity beam search method to increase text diversity.\n\nCompared to traditional generation models that rely on greedy decoding \\cite{chickering2002optimal} and choosing the highest-probability logit at every generation step, we adopt a diversity beam search decoding strategy. More specifically, we first inject the entity types into the augmented entity list $E = [[{t}_{1}], {e}_{1}, [/{t}_{1}],...,[{t}_{m}],{e}_{m},[/{t}_{m}], ...,[{t}_{l}], {e}_{l},[/{t}_{l}]]$\nas the input sequence, which should provide sufficient type guidance for the generation model, then we adopt T5 \\cite{raffel2020exploring} as the generation model. We first fine-tune T5 on the original Entity-to-Text data and then adopt T5 ($\\theta$) to estimate the conditional probability distribution over all tokens in the dictionary $\\mathcal{V}$ at time step $t$ as:\n\\begin{align}\\label{eq:pro}\n\\theta\\left(y_{t}\\right)=\\log \\operatorname{Pr}\\left(y_{t} \\mid y_{t-1}, \\ldots, y_{1}, E\\right).\n\\end{align}\nwhere ${y_{t}}$ is the ${{t}^{th}}$ output token ${y}$ in texts. We simplify the sum of $\\log$-probabilities (Eq. \\ref{eq:pro}) of all previous tokens decoded $\\Theta(\\mathbf{y}_{[t]})$ as:\n\\begin{align}\\label{eq:decoding}\n\\Theta\\left(\\mathbf{y}_{[t]}\\right)=\\sum_{\\tau \\in[t]} \\theta\\left(y_{\\tau}\\right),\n\\end{align}\nwhere $\\mathbf{y}_{[t]}$ is the token list consisting of $[y_{1},y_{2},...,y_{t}]$. Therefore, our decoding problem is transformed into the task of finding the text that could maximize $\\Theta(\\mathbf{y})$. The classical approximate decoding method is the beam search \\cite{wiseman2016sequence}, which stores top beam width $B$ candidate tokens at time step $t$. Specifically, beam search selects the $B$ most likely tokens from the set:\n\\begin{align}\n\\mathcal{Y}_{t}=Y_{[t-1]} \\times \\mathcal{V},\n\\end{align}\nwhere $Y_{[t-1]}=\\left\\{\\mathbf{y}_{1,[t-1]}, \\ldots, \\mathbf{y}_{B,[t-1]}\\right\\}$ and $\\mathcal{V}$ is the dictionary. However, traditional beam search keeps a small proportion of candidates in the search space and generates the texts with minor perturbations \\cite{huang2008forest}, which impedes the diversity of generated texts. Inspired by \\citet{vijayakumar2016diverse}, we introduce an objective to increase the dissimilarities between candidate texts and finalize the Eq. \\ref{eq:decoding} as diversity beam search decoding:\n\\begin{align}\n\\hat{\\Theta}\\left(\\mathbf{y}_{[t]}\\right)=\\sum_{\\tau \\in[t]} (\\theta\\left(y_{\\tau}\\right) - \\gamma k_{\\tau}),\n\\end{align}\nwhere $\\gamma$ is a hyperparameter and represents the punishment degree. $k_{\\tau}$ denotes the ranking of the current tokens among candidates. In practice, it's a penalty text of beam width: $[1,2,...,B]$ which punishes bottom ranked tokens among candidates and thus generates tokens from diverse previous tokens. For a better understanding, we give an example about the text with beam search decoding and diversity beam search decoding in Figure \\ref{fig:DBS}. \n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.98\\linewidth]{figures/DBS_example.pdf}\n    \\caption{The example about the generated text with beam search decoding (left) and diversity beam search decoding (right). The hyperparameter $\\gamma$ is set to 1 and beam width $B$ is set to 2 here.}\n    \\label{fig:DBS}\n\\vspace{-4mm}\n\\end{figure} \n\nThe traditional greedy decoding chooses the highest-probability logit at every generation step and results in \\textit{British farmer}. Compared to the diversity beam search decoding method, the beam search decoding method maintains a small proportion of candidates in the search space without the introduction of a penalty text of beam width $[1,2,...,B]$. This additional objective increases the dissimilarities between candidate texts and thus generates tokens from diverse previous tokens. For example, \\textit{British farmer} and \\textit{German farmer} are generated instead of \\textit{British farmer} and \\textit{British market}, which brings the diversity token \\textit{German}. Likewise, the diversity token \\textit{market} will also be considered in the subsequent generation.\nOverall, at each time step $t$:\n\\begin{align}\nY_{[t]}=\\underset{\\mathbf{y}_{1,[t]}, \\ldots, \\mathbf{y}_{B,[t]} \\in \\mathcal{Y}_{t}}{\\operatorname{argmax}} \\sum_{b \\in[B]} \\hat{\\Theta}\\left(\\mathbf{y}_{b,[t]}\\right).\n\\end{align}\nThis process will generate the most likely texts that are selected by ranked the $B$ beams based on the diversity beam search decoding.\n\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=0.98\\linewidth]{figures/illustration.pdf}\n    \\caption{Details of marking the texts with the augmented entity lists for three NER tasks.}\n    \\label{fig:illustration}\n\\vspace{-4mm}\n\\end{figure}\n\n\\subsection{Augmented Text Exploitation}\\label{exploitation}\nTo utilize these augmented Entity-to-Text data, we need to mark the texts with the augmented entity lists. \nAs illustrated in Figure \\ref{fig:overview}, we first automatically judge whether the entities match the tokens in the texts and remove these noisy texts of mismatched entities.\nFor example, \\textit{EU} is generated as \\textit{United Nations} and this generated text is automatically deleted.\nThen as illustrated in Figure \\ref{fig:illustration}, we provide the details of the text marking process:\n\n(1) If the entity is \\textbf{flat}, we obtain the start and end position indexes through the exact match between entity and text.\n\n(2) If the entity is \\textbf{nested}, we first store all the overlapping entity mentions belonging to the same nested entity and match these mentions with text to obtain start and end position indexes.\n\n(3) If the entity is \\textbf{discontinuous}, we match the entity mentions which belong to the same discontinuous entity with text to obtain start and end position indexes. \n\nNote that the process of text marking is done automatically based on the above three situations. After we obtain these augmented data with marked flat, nested, and discontinuous entities, we naturally formulate the texts as input to NER tasks.\n\n\\label{sec:experiments}\n\nWe conduct extensive experiments on thirteen NER datasets across three tasks (flat, nested, and discontinuous NER) and two settings (full data and low resource NER) to show the effectiveness of {\\modelname} on NER, and give a detailed analysis.\n\n\\subsection{Backbone Models}\nWe adopt two SOTA backbone models which could solve all three NER tasks: \n\n\\noindent1) \\textbf{The unified Seq2Seq framework} \\cite{yan-etal-2021-unified-generative} formulates three NER tasks as an entity span text generation task without the special design of the tagging schema to enumerate spans. \n\n\\noindent2) \\textbf{The unified Word-Word framework} \\cite{li2022unified} models the neighboring relations between entity words as a 2D grid and then adopts multi-granularity 2D convolutions for refining the grid representations.\n\nThese two backbone models are leveraged to solve the general NER tasks illustrated in Section \\ref{NER Tasks} and demonstrate the effectiveness of {\\modelname}.\n\n\\begin{table*}[th!]\n\\centering\n\\vspace{-1mm}\n\\scalebox{0.68}{\n\\begin{tabular}{lcccccccccc}\n\\thickhline\n\\multicolumn{1}{c}{\\multirow{2}{*}{Method / Datasets}} & \\multicolumn{2}{c}{Flat NER datasets} & \\multicolumn{3}{c}{Nested NER datasets} & \\multicolumn{3}{c}{Discontinuous NER datasets} & \\multicolumn{1}{c}{\\multirow{2}{*}{AVG.}}  & \\multicolumn{1}{c}{\\multirow{2}{*}{$\\Delta$}} \n\\\\ \\cmidrule(lr){2-3} \\cmidrule(lr){4-6} \\cmidrule(lr){7-9} \n&CoNLL2003 & OntoNotes & ACE2004 & ACE2005 & Genia & CADEC & ShARe13 & ShARe14  \\\\\n\\midrule \nUnified Word-Word Framework  &93.14 &90.66 &87.54 &86.72 &81.34 &73.22 &82.57 &81.79 &84.62 &-- \\\\\n\\multicolumn{1}{l}{$+$Label-wise token rep.} &93.32 &90.78 &87.83 &\\underline{86.98} &\\underline{81.65} &73.47 &82.84 &82.07 &84.87&\\color{red}0.25$\\uparrow$ \\\\\n\\multicolumn{1}{l}{$+$Synonym replacement} &93.35 &90.75 &87.87 &86.93 &81.63 &\\underline{73.50} &\\underline{82.87} &\\underline{82.10} &\\underline{84.88} &\\color{red}0.26$\\uparrow$ \\\\\n\\multicolumn{1}{l}{$+$Mention replacement} &93.29 &90.80 &\\underline{87.89} &86.97 &81.64 &-- &-- &-- &-- &--\\\\\n\\multicolumn{1}{l}{$+$Shuffle within segments} &93.30 &90.68 &87.68 &86.84 &81.47 &73.36 &82.71 &81.92 &84.75 &\\color{red}0.13$\\uparrow$ \\\\\n\\multicolumn{1}{l}{$+$DAGA } & 93.47 &90.89 &-- &-- &-- &-- &-- &-- &-- &--  \\\\\n\\multicolumn{1}{l}{$+$MELM } &\\underline{93.60} &\\underline{91.06} &-- &-- &-- &-- &-- &-- &-- &--  \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{$+\\modelname$ (Delete)}} &93.82\t&91.23&\t\\textbf{88.29}\t&87.54\t&82.12&\t73.86\t&83.31\t&82.45  &85.33 &\\color{red}0.71$\\uparrow$ \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{$+\\modelname$ (Add)}} &\\textbf{93.93}\t&91.26\t&88.27&\t\\textbf{87.60}\t&82.19\t&\\textbf{73.89}\t&83.34\t&\\textbf{82.55} & \\textbf{85.42} &\\color{red}\\textbf{0.76}$\\uparrow$ \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{$+\\modelname$ (Replace)}} &93.87\t&91.21\t&88.18&\t87.46\t&\\textbf{82.40}\t&73.82&\t83.19\t&82.52  &85.33 &\\color{red}0.71$\\uparrow$ \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{$+\\modelname$ (Swap)}} &93.91\t&91.25&\t88.18\t&87.54\t&82.32\t&73.81\t&83.30\t&82.52 &85.35 &\\color{red}0.73$\\uparrow$ \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{$+\\modelname$ (All)}} &93.88\t&\\textbf{91.34}  &\t88.21\t&87.56\t&82.25\t&73.86&\t\\textbf{83.35}&\t82.47  &85.37 &\\color{red}0.75$\\uparrow$ \\\\\n\\cmidrule{2-11} \n\\multicolumn{1}{l}{\\textbf{$+\\modelname$ (None)}} &93.44 &90.89 &87.84 &87.01 &81.73 &73.57 &82.90 &82.09  &84.93 &\\color{red}0.31$\\uparrow$ \\\\\n\\multicolumn{1}{l}{\\textbf{$+\\modelname$ (All) w/o Diver.}} &93.55 &91.01 &87.93 &87.23 &81.91 &73.75 &83.02 &82.20  &85.08 &\\color{red}0.46$\\uparrow$ \\\\\n\\midrule \n\\midrule \nUnified Seq2Seq Framework &92.78 &89.51 &86.19 &84.74 &79.10 &70.76 &79.69 &79.40 &82.78 &-- \\\\\n\\multicolumn{1}{l}{$+$Label-wise token rep.} &92.91 &89.68 &\\underline{86.33} &85.04 &79.41 &\\underline{71.22} &\\underline{79.93} &\\underline{79.64} &83.03 &\\color{red}0.25$\\uparrow$ \\\\\n\\multicolumn{1}{l}{$+$Synonym replacement} &92.85 &89.59 &86.28 &\\underline{85.32} &79.36 &71.18 &79.86 &79.55 &83.00 &\\color{red}0.22$\\uparrow$ \\\\\n\\multicolumn{1}{l}{$+$Mention replacement} &92.80 &89.80 &86.14 &85.01 &\\underline{79.44} &-- &-- &-- &-- &--  \\\\\n\\multicolumn{1}{l}{$+$Shuffle within segments } &92.85 &89.40 &86.22 &84.99 &79.28 &71.13 &79.72 &79.50 &82.89 &\\color{red}0.11$\\uparrow$ \\\\\n\\multicolumn{1}{l}{$+$DAGA } &92.92 &\\underline{89.97} &-- &-- &-- &-- &-- &-- &-- &--  \\\\\n\\multicolumn{1}{l}{$+$MELM } &\\underline{92.95} &89.95 &-- &-- &-- &-- &-- &-- &-- &--  \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{$+\\modelname$ (Delete)}} &93.38 &90.23 &86.51 &86.26 &80.80 &71.51 &80.58 &80.04  &83.67 &\\color{red}0.89$\\uparrow$ \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{$+\\modelname$ (Add)}} &93.27 &90.27 &86.73 &86.39 &80.88 &71.50 &\\textbf{80.92} &80.16  &83.77 &\\color{red}0.99$\\uparrow$ \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{$+\\modelname$ (Replace)}} &93.32 &90.16 &86.55 &\\textbf{86.41} &80.74 &71.64 &80.64 &80.23 &83.71 &\\color{red}0.93$\\uparrow$ \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{$+\\modelname$ (Swap)}} &93.45 &90.04 &86.40 &86.30 &80.67 &71.37 &80.37 &80.12  &83.59 &\\color{red}0.81$\\uparrow$ \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{$+\\modelname$ (All)}} &\\textbf{93.51} &\\textbf{90.31} &\\textbf{86.92} &86.39 &\\textbf{80.94} &\\textbf{71.70} &80.83 &\\textbf{80.36}  &\\textbf{83.87} &\\color{red}\\textbf{1.09}$\\uparrow$ \\\\\n\\cmidrule{2-11} \n\\multicolumn{1}{l}{\\textbf{$+\\modelname$ (None)}} &92.90 &90.02 &86.28 &85.57 &79.66 &71.30 &80.13 &79.71  &83.20 &\\color{red}0.42$\\uparrow$ \\\\\n\\multicolumn{1}{l}{\\textbf{$+\\modelname$ (All) w/o Diver.}} &93.13 &90.21 &86.47 &85.78 &79.88 &71.54 &80.31 &79.97  &83.41 &\\color{red}0.63$\\uparrow$ \\\\\n\n\\thickhline\n\n\\end{tabular}}\n\\caption{F1 results of various NER tasks. For all three backbone models and six baseline augmentation approaches, we rerun their open source code and adopt the given parameters.}\n\\label{tab:verification}\n\n\\vspace{-4mm}\n\\end{table*}\\subsection{Datasets}\nTo demonstrate that {\\modelname} could be used in various NER tasks and backbone models, we follow \\citet{yan-etal-2021-unified-generative,li2022unified} and adopt the same datasets (split) as follows:\n\n1) \\textbf{Flat NER Datasets}: We adopt the CoNLL-2003 \\cite{sang2003introduction} and OntoNotes \\cite{pradhan2013towards} datasets. For OntoNotes, we evaluate in the English corpus with the same setting as \\citet{yan-etal-2021-unified-generative}.\n\n2) \\textbf{Nested NER Datasets}: We adopt the ACE 2004 \\cite{doddington2004automatic}, ACE 2005 \\cite{Walker2005Ace} and GENIA \\cite{kim2003genia} datasets. Following \\citet{yan-etal-2021-unified-generative}, we split the ACE 2004/ACE 2005 into train/dev/test sets by 80\\%/10\\%/10\\% and GENIA into 81\\%/9\\%/10\\% respectively.\n\n3) \\textbf{Discontinuous NER Datasets}\nWe adopt the CADEC \\cite{karimi2015cadec}, ShARe13 \\cite{Mowery2013Task1S} and ShARe14 \\cite{Mowery2014Task2S} datasets from biomedical domain. Following \\citet{yan-etal-2021-unified-generative}, we split the CADEC into train/dev/test sets by 70\\%/15\\%/15\\% and use 10\\% training set as the development set for ShARe13/ShARe14.\n\nWe show the detailed statistics and entity types of the datasets in Appendix \\ref{Dataset}.\n\n\\subsection{Baseline Augmentation Methods}\nUnlike sentence-level classification tasks, NER is a fine-grained token-level task, so we adopt six entity-level data augmentation baselines, which are designed for various NER tasks.\n\nThe four rule-based baseline augmentation techniques: (1) \\textbf{Label-wise token replacement} \\cite{dai-adel-2020-analysis} utilizes a binomial distribution to decide whether each token should be replaced, and then replaces the chosen token with another token that has the same entity type. (2) \\textbf{Synonym replacement} \\cite{dai-adel-2020-analysis} replaces the chosen token with the synonym retrieved from WordNet. (3) \\textbf{Mention replacement} \\cite{dai-adel-2020-analysis} replaces the chosen entity with another entity, which has the same entity type. (4) \\textbf{Shuffle within segments} \\cite{dai-adel-2020-analysis} splits the sentences into segments based on whether they come from the same entity type, and uses a binomial distribution to decide whether to shuffle tokens within the same segment. The two generative baseline augmentation techniques are: (5) \\textbf{DAGA} \\cite{ding-etal-2020-daga} treats the NER labeling task as a text tagging task and annotates entities with generative models during generation. (6) \\textbf{MELM} \\cite{zhou2022melm} generates augmented data with diverse entities, which is built upon pre-trained masked language models. MELM is further finetuned on corrupted training sentences with only entity tokens being randomly masked to focus on entity replacement.\n\nWe present another model: \\textbf{$\\modelname$  (All)}, which adopts four entity list operations simultaneously to generate augmented texts. Note that we focus on entity-level NER augmentation tasks, so to the best of our knowledge, we have employed all entity-level augmentation techniques.\n\n\\subsection{Experiment Settings}\nFor $\\modelname$, we fine-tune the T5-Base \\cite{raffel2020exploring} with the initial parameters on the Entity-to-Text data of the training set and utilize the default tokenizer with max-length as 512 to preprocess the data. We use AdamW \\cite{loshchilov2018fixing} with $5e{-5}$ learning rate to optimize the cross entropy loss. The batch size is set to 5 and the number of training epoch is set to 3. During diversity beam search decoding, we set $\\gamma$ as 10 and beam width ${B}$ as 3, which means that each entity set will generate three texts. \n\n{\\modelname} and all baselines augment the training set by 3x for a fair comparison. For example, the number of texts in the training set is 100, we generate 300 texts and add them to the training set. We replace the language model in MELM \\cite{zhou2022melm} with XLM-RoBERTa-large (355M) \\cite{conneau2020unsupervised}, and we use T5-Base (220M) with fewer parameters for comparison.\n\n\\subsection{Results and Analyses}\nTable \\ref{tab:verification} shows the average F1 results on three runs. All backbone NER models gain F1 performance improvements from the augmented data when compared with the models that only use original training data, demonstrating the effectiveness of data augmentation approaches in the various NER tasks.\nSurprisingly, ${\\modelname}$ (None) outperforms the baseline methods by 0.11\\% F1 performance among the backbone models, which shows that the generative models using a diversity beam search have sufficient capacity to generate high-quality augmented data. \n\nMore specifically, for flat NER datasets, MELM is considered as the previous SOTA data augmentation approach. The proposed ${\\modelname}$ (All) on average achieves 0.23\\% higher in F1 among flat NER datasets and two backbone models.\nFor nested and discontinuous NER datasets, \nthe label-wise token replacement method achieves the best performance among baselines. ${\\modelname}$ (All) achieve an average 0.78\\% F1 boost among nested and discontinuous NER datasets, which demonstrates that leveraging generative model to augment semantically coherent texts is effective.\n\\begin{table}[bt!]\n\\centering\n\\resizebox{0.98\\linewidth}{!}{\n\\begin{tabular}{lccc}\n\\thickhline\nMethod / Datasets & CoNLL2003  & ACE2005 & CADEC   \\\\\n\\midrule \nUnified Word-Word Framework &86.83  &79.56 &65.03\\\\\n\\multicolumn{1}{l}{+Label-wise token rep.} &87.23 &79.97 &\\underline{65.50} \\\\\n\\multicolumn{1}{l}{+Synonym replacement} &87.16 &80.01 &65.46 \\\\\n\\multicolumn{1}{l}{+Mention replacement} &87.30 &\\underline{80.10} &--\\\\\n\\multicolumn{1}{l}{+Shuffle within segments} &87.04 &79.85 &65.28\\\\\n\\multicolumn{1}{l}{+DAGA} &87.82 &-- &-- \\\\\n\\multicolumn{1}{l}{+MELM} &\\underline{88.24} &-- &-- \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{+$\\modelname$ (Delete)}} & 89.91 &81.94 &69.12 \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{+$\\modelname$ (Add)}} &90.13 &\\textbf{82.15} &69.03 \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{+$\\modelname$ (Replace)}} &90.07 &82.01 &69.29\\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{+$\\modelname$ (Swap)}} &89.97 &81.98 &69.25 \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{+$\\modelname$ (All)}} &\\textbf{90.22} &82.08 &\\textbf{69.31} \\\\\n\n\\midrule \n\\midrule \nUnified Seq2Seq Framework &85.90  &77.32 &62.24 \\\\\n\\multicolumn{1}{l}{+Label-wise token rep.} &86.44 &77.81 &62.56 \\\\\n\\multicolumn{1}{l}{+Synonym replacement}&86.73 &77.79 &\\underline{62.61} \\\\\n\\multicolumn{1}{l}{+Mention replacement} &86.94 &\\underline{77.83} &-- \\\\\n\\multicolumn{1}{l}{+Shuffle within segments} &86.26 &77.65 &62.49 \\\\\n\\multicolumn{1}{l}{+DAGA} &87.05 &-- &-- \\\\\n\\multicolumn{1}{l}{+MELM} &\\underline{87.43} &-- &-- \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{+$\\modelname$ (Delete)}} & 89.20 &79.10 &66.04 \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{+$\\modelname$ (Add)}} &89.62 &79.23 &\\textbf{66.42} \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{+$\\modelname$ (Replace)}} &89.41 &79.02 &66.21 \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{+$\\modelname$ (Swap)}} &88.96 &78.96 &65.93 \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{+$\\modelname$ (All)}} &\\textbf{89.82} &\\textbf{79.51} &66.40 \\\\\n\n\\thickhline\n\\end{tabular}}\n\\caption{F1 results of various NER tasks under low resource scenarios.}\n\\label{tab:low-resource}\n\\vspace{-4mm}\n\\end{table}\nAmong all NER datasets, ${\\modelname}$ is undoubtedly capable of achieving state-of-the-art results (with student's T test $p<0.05$). Except ${\\modelname}$ (All), {\\modelname} (Add) achieves the largest F1 performance gains of 0.99\\% and 0.76\\% on the unified Seq2Seq and Word-Word frameworks, respectively. We attribute this delightful improvement of the ``Add'' operation to the additionally introduced knowledge: we add the entity from the training set with the same entity type.\n\n\\noindent\\textbf{Ablation Study}\n\nIn Table \\ref{tab:verification}, we remove the entity list augmentation module (\\modelname (None)), or change the diversity beam search to the traditional beam search (\\modelname (All) w/o Diver.). We can conclude that entity list augmentation and diversity beam search modules bring an average F1 improvement of 0.56\\% and 0.38\\% on the eight datasets. Using the entity list augmentation module can give a richer entity combination, which brings more improvement. Adopting the diversity beam search brings more diverse texts and gains greater improvements.\n\n\\noindent\\textbf{Handling Low Resource NER Scenarios}\n\nWe further introduce an extreme yet practical scenario: only limited labeled data is available. This low resource NER scenario demonstrates that our {\\modelname} approach bootstraps the generalization ability of the NER model and is a quite appealing approach for data-oriented applications in the real-world. In practice, we randomly choose 10\\% training data from CoNLL2003/ACE2005/CADEC to represent the three NER tasks. Note that the fine-tuning of T5-large and our four operations on the entity list are also done on 10\\% training data.\n\nFrom Table \\ref{tab:low-resource}, compared to training directly on the 10\\% training set, leveraging the augmented data achieves the performance improvement in F1. We also observe that {\\modelname} approach obtains the most competitive F1 performance improvement when compared with baseline data augmentation approaches. \nMore specifically, ${\\modelname}$ (All) achieve an average 2.97\\% F1 boost among three backbone models, which means {\\modelname} obtains more performance gains under the low resource scenario than in the full data scenario. Especially for the most challenging discontinuous dataset CADEC, ${\\modelname}$ (All) obtains the largest F1 performance gain of 4.22\\%.\nSurprisingly, on 10\\% CoNLL2003, ${\\modelname}$ (All) has only a 2.94\\% decrease in F1 performance compared to using the full training data, but ${\\modelname}$ (All) saves 10x the annotated data, which shows that adopting ${\\modelname}$ is quite appealing for real-world applications.\n\\begin{table}[bt!]\n\\centering\n\\resizebox{0.99\\linewidth}{!}{\n\\begin{tabular}{lccccc}\n\\thickhline\nMethod / Datasets & Politics & Natural Science & Music & Literature & AI   \\\\\n\\midrule \nSeq2Seq Framework &70.11  &70.72 &72.90 &63.69 &56.77 \\\\\n\\multicolumn{1}{l}{+Label-wise token rep.} &70.45  &70.91 &73.48 &63.97 &57.04\\\\\n\\multicolumn{1}{l}{+Synonym replacement}&70.43  &71.04 &73.66 &63.92 &57.34\\\\\n\\multicolumn{1}{l}{+Mention replacement} &70.47  &71.07 &\\underline{73.54} &64.02 &57.42 \\\\\n\\multicolumn{1}{l}{+Shuffle within segments} &70.39  &70.94 &73.30 &63.88 &57.26 \\\\\n\\multicolumn{1}{l}{+DAGA} &\\underline{71.06}  &\\underline{71.51} &73.46 &\\underline{64.21} &\\underline{57.83} \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{+$\\modelname$ (Delete)}} & 72.60 &72.05 &75.87 & 67.18 &61.58 \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{+$\\modelname$ (Add)}} &72.81 &\\textbf{72.55} &76.20 &67.82 &61.97 \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{+$\\modelname$ (Replace)}} &72.94 &72.46 &76.12 &67.57 &61.89\\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{+$\\modelname$ (Swap)}}& 72.47 &71.89 &75.58 & 67.06 &61.37 \\\\\n\\rowcolor{gray!10}\n\\multicolumn{1}{l}{\\textbf{+$\\modelname$ (All)}}& \\textbf{72.98} &72.47 &\\textbf{76.55} & \\textbf{68.04} &\\textbf{62.31} \\\\\n\\thickhline\n\\end{tabular}}\n\\vspace{-2mm}\n\\caption{F1 results of real low resource NER tasks.}\n\\label{tab:real}\n\\vspace{-2mm}\n\\end{table}\n\\noindent\\textbf{Tackling Real Low Resource NER Tasks}\n\nWe adopt real low resource NER datasets \\cite{liu2021crossner} from Wikipedia which contains politics, natural science, music, literature and artificial intelligence domains with only 100 or 200 labeled texts in the training set. {\\modelname} and baseline data augmentation approaches still augment the training set by 3x. \nFrom Table \\ref{tab:real}, we are delighted to observe {\\modelname} could quickly learn from the extremely limited Entity-to-Text data and bring 3.45\\% F1 performance gains over various domains. Compared with baseline augmentation methods, {\\modelname} generates more diverse texts and undoubtedly gains greater advantages.\n\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=0.99\\linewidth]{figures/low_resource.pdf}\n    \\caption{F1 results of the unified Seq2Seq framework with augmented data at various multiples on different low resource datasets.}\n    \\label{fig:real}\n\\vspace{-4mm}\n\\end{figure}\n\n\\begin{table}[t!]\n\\centering\n\\resizebox{0.9\\linewidth}{!}{\n\\begin{tabular}{lccc}\n\\thickhline\nMethod / Datasets & CoNLL2003 & CADEC  & AI   \\\\\n\\midrule \n\\multicolumn{1}{l}{Label-wise token rep.} &8.12  &8.87 &7.52 \\\\\n\\multicolumn{1}{l}{Synonym replacement}& 7.44  &7.88 &7.01 \\\\\n\\multicolumn{1}{l}{Mention replacement} & 7.07  &7.42 &6.54 \\\\\n\\multicolumn{1}{l}{Shuffle within segments} & 10.24  &12.32 &9.65 \\\\\n\\multicolumn{1}{l}{DAGA} &5.46 &6.23 &5.07 \\\\\n\\multicolumn{1}{l}{MELM} &5.27 &6.29 &4.82 \\\\\n\\multicolumn{1}{l}{\\textbf{$\\modelname$ (All)}}& \\textbf{4.74} &\\textbf{5.19} &\\textbf{4.28} \\\\\n\n\\thickhline\n\\end{tabular}}\n\\caption{Perplexity of the augmented data with various augmentation approaches. Lower perplexity is better.}\n\\label{tab:perplexity}\n\\vspace{-1mm}\n\\end{table}\n\n\\noindent\\textbf{Various Augmentation Multiples Performance}\n\nWe further vary the multiples of augmented data from 2x to 10x the training set to study the influence of data augmentation approaches for the NER backbone models under low resource scenarios. \nWe choose different low resource datasets and three representative augmentation approaches (Mention replacement, MELM, and {\\modelname} (All)), then represent the results in Figure \\ref{fig:real}.\n\nWe could observe that the unified Seq2Seq framework has more performance gains with ever-increasing augmented data. {\\modelname} (All) consistently achieves better F1 performance, with a clear margin, compared to baseline augmentation approaches under various augmentation multiples. Especially for Music, {\\modelname} (All) brings an incredible 4.01\\% improvement in F1 performance with only 300 augmented data.\n\n\\noindent\\textbf{Semantic Coherence Analysis}\n\nCompared with baseline augmentation approaches, {\\modelname} conditionally generates texts with the diversity beam search decoding, which provides more coherent texts. We analyze the coherence through perplexity based on a large Transformer language model: GPT-2 \\cite{radford2019language}. From Table \\ref{tab:perplexity}, {\\modelname} obtains the lowest perplexity. Although DAGA and MELM are also based on generative models, the texts are not natural enough since only partial text is replaced.\n\\begin{table}[t]\n\\centering\n\\scalebox{0.62}{\n\\begin{tabular}{lcccccc}\n\\thickhline\n\\multicolumn{1}{c}{\\multirow{2}{*}{Methods / Datasets}} & \\multicolumn{2}{c}{CoNLL2003} & \\multicolumn{2}{c}{CADEC} &  \\multicolumn{2}{c}{AI} \n\\\\ \\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-7}\n&TTR & Diver.  &TTR  & Diver. &TTR & Diver.  \\\\\n\\midrule \n\\multicolumn{1}{l}{Label-wise token rep.} & 81.2 & 3.1 & 80.5  &  3.4 & 81.9 & 3.3  \\\\\n\\multicolumn{1}{l}{Synonym replacement} & 81.9 & 3.3  & 80.1 & 3.5 & 82.6  & 3.4 \\\\\n\\multicolumn{1}{l}{Mention replacement} & \\underline{83.8} & \\underline{3.9} & \\underline{82.9} & \\underline{3.6} & \\textbf{84.2} & \\underline{3.8}\\\\\n\\multicolumn{1}{l}{Shuffle within segments} & 72.9 & 2.4 &71.6 &  2.0 & 73.7 & 2.1\\\\\n\\multicolumn{1}{l}{DAGA} & 73.8 & 2.8 & 74.1 & 2.6 & 74.3 & 3.1\\\\\n\\multicolumn{1}{l}{MELM} & 77.2 & 3.2 & 78.1 & 2.9 &76.6 & 3.0 \\\\\n\\multicolumn{1}{l}{\\textbf{$\\modelname$ (All)}} & \\textbf{86.4} & \\textbf{4.3} & \\textbf{85.1} & \\textbf{4.5} & \\underline{83.7} & \\textbf{4.4}\\\\\n\n\\thickhline\n\\end{tabular}}\n\\caption{Diversity Evaluation on three datasets.}\n\\label{tab:human}\n\\vspace{-1mm}\n\\end{table}\n\n\\noindent\\textbf{Diversity Evaluation}\n\nWe measure the diversity of augmented sentences through automatic and manual metrics. For automatic metric, we introduce the Type-Token Ratio (TTR) \\cite{tweedie1998variable} to evaluate the ratio of the number of different words to the total number for each original text. Higher TTR (\\%) indicates more diversity in sentences. Besides that, we ask 5 annotators to give a score for the degree of diversity of the 200 generated texts, with score range of 1\\textasciitilde5. According to the annotation guideline in Appendix \\ref{guideline}, a higher score indicates the method can generate more diverse texts. \n\nWe present the average scores on the datasets in Table \\ref{tab:human}. {\\modelname} could obtain 7.8\\% TTR and 1.4 diversity performance boost in average compared to MELM.\n\\begin{table}[t!]\n\\centering\n\\resizebox{1.00\\linewidth}{!}{\n\\begin{tabular}{l}\n\\toprule\n\\begin{tabular}[c]{@{}l@{}}Approach: {\\color{blue}Original Sentence}\n\\\\ \nEntity: \\textbf{\\texttt{unsupervised learning, principal component analysis}}\\\\\n${\\quad\\quad}$\\textbf{\\texttt{, cluster analysis}}\\\\\nEntity Type: \\textbf{\\texttt{field, algorithm, algorithm}}\\\\\nText: The main methods used in {\\color{red}\\textit{unsupervised learning}} are\\\\\n${\\quad\\quad}$ {\\color{red}\\textit{principal component analysis}} and {\\color{red}\\textit{cluster analysis}}.\n\\end{tabular}     \n\\\\\\hline\n\\begin{tabular}[c]{@{}l@{}}Approach: {\\color{blue}{\\modelname} (Add)}\n\\\\ \nEntity: \\textbf{\\texttt{unsupervised learning, principal component analysis}}\\\\\n${\\quad\\quad}$\\textbf{\\texttt{, cluster analysis, dimension reduction}}\\\\\nEntity Type: \\textbf{\\texttt{field, algorithm, algorithm, algorithm}}\\\\\nText: In {\\color{red}\\textit{unsupervised learning}}, {\\color{red}\\textit{principal component analysis}}, {\\color{red}\\textit{cluster analysis}} and\\\\\n${\\quad\\quad}$ {\\color{red}\\textit{dimension reduction}} are used to reduce the number of variables in a task.\n\\end{tabular}     \n\\\\\n\\hline\n\\begin{tabular}[c]{@{}l@{}}Approach: {\\color{blue}{\\modelname} (Delete)}\n\\\\ \nEntity: \\textbf{\\texttt{unsupervised learning, principal component analysis}}\\\\\nEntity Type: \\textbf{\\texttt{field, algorithm}}\\\\\nText: In the field of {\\color{red}\\textit{unsupervised learning}}, {\\color{red}\\textit{principal component analysis}}\\\\\n${\\quad\\quad}$ is used to model the learning process.\n\\end{tabular}\n\\\\\n\\hline\n\\begin{tabular}[c]{@{}l@{}}Operation: {\\color{blue}{\\modelname} (Replace)}\n\\\\ \nEntity: \\textbf{\\texttt{unsupervised learning, principal component analysis}}\\\\\n${\\quad\\quad}$ \\textbf{\\texttt{, dimension reduction}}\\\\\nEntity Type: \\textbf{\\texttt{field, algorithm, algorithm}}\\\\\nText: In the field of {\\color{red}\\textit{unsupervised learning}}, {\\color{red}\\textit{principal component analysis}} and \\\\\n${\\quad\\quad}${\\color{red}\\textit{dimension reduction}} are used to reduce the size of the data.\n\\end{tabular} \\\\\n\\hline\n\\begin{tabular}[c]{@{}l@{}}Operation: {\\color{blue}{\\modelname} (Swap)}\n\\\\ \nEntity: \\textbf{\\texttt{unsupervised learning, cluster analysis}}\\\\\n${\\quad\\quad}$\\textbf{\\texttt{, principal component analysis}}\\\\\nEntity Type: \\textbf{\\texttt{field, algorithm, algorithm}}\\\\\nText: {\\color{red}\\textit{Unsupervised learning}} uses {\\color{red}\\textit{cluster analysis}} and {\\color{red}\\textit{principal component analysis}}\\\\\n${\\quad\\quad}$ to learn a task.\n\\end{tabular} \\\\\n\\hline\n\\begin{tabular}[c]{@{}l@{}}Operation: {\\color{blue}{\\modelname} (All)}\n\\\\ \nEntity: \\textbf{\\texttt{unsupervised learning, dimension reduction}}\\\\\n${\\quad\\quad}$\\textbf{\\texttt{, principal component analysis}}\\\\\nEntity Type: \\textbf{\\texttt{field, algorithm, algorithm}}\\\\\nText: {\\color{red}\\textit{Unsupervised learning}} uses {\\color{red}\\textit{cluster analysis}} to achieve the purpose of \\\\\n${\\quad\\quad}${\\color{red}\\textit{dimension reduction}} for better learning a task.\n\\end{tabular} \\\\\n\\hline\n\\begin{tabular}[c]{@{}l@{}}Approach: {\\color{blue}Mention Replacement}\n\\\\ \nEntity: \\textbf{\\texttt{heterodyning, principal component analysis}}\\\\\n${\\quad\\quad}$\\textbf{\\texttt{, cluster analysis}}\\\\\nEntity Type: \\textbf{\\texttt{field, algorithm, algorithm}}\\\\\nText: The main methods used in {\\color{red}\\textit{heterodyning}} are {\\color{red}\\textit{principal component analysis}}\\\\\n${\\quad\\quad}$ and {\\color{red}\\textit{cluster analysis}}.\n\\end{tabular}   \\\\\n\\hline\n\\begin{tabular}[c]{@{}l@{}}Operation: {\\color{blue}{DAGA}}\n\\\\ \nText: {\\color{red}\\textit{Unsupervised learning}} uses {\\color{red}\\textit{principal component analysis}} and {\\color{red}\\textit{cluster analysis}}.\\\\\nEntity (Unchanged): \\textbf{\\texttt{unsupervised learning, principal component}}\\\\\n${\\quad\\quad}$\\textbf{\\texttt{analysis, cluster analysis}}\\\\\nEntity Type (Unchanged): \\textbf{\\texttt{field, algorithm, algorithm}}\\\\\n\\end{tabular} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{The augmented texts for AI domain. We show six {\\color{blue}approaches} to generate texts marked with the corresponding {\\color{red}\\textit{entity}} list.\n}\\label{tab:example_ai}\n\\vspace{-0.1in}\n\\end{table}\n\\section{Case Study}\\label{case}\n\nWe show eight approaches to obtain augmented data for the AI domain in Table \\ref{tab:example_ai}. Compared with baseline augmentation methods, {\\modelname} introduces a knowledge expansion and conditionally generates texts based on the diversity beam search, which provides more coherent and diverse texts. For example, The Mention Replacement approach replaces the entities \\texttt{unsupervised learning} with \\texttt{heterodyning}, which ignores the semantics of the context and makes an ungrammatical replacement, resulting in incoherent and unreasonable texts. For the DAGA approach, it simply stacks three entities: \\texttt{unsupervised learning, principal component analysis, cluster analysis} in the text, which could not provide knowledge expansions to the NER models.\n\n\\section{Conclusions and Future Work}\n\\label{sec:conclusion}\nIn this paper, we propose an Entity-to-Text based data augmentation approach {\\modelname} for NER tasks. \nCompared with traditional rule-based augmentation methods that break semantic coherence, or use Text-to-Text based augmentation methods that cannot be used on nested and discontinuous NER tasks, our method can generate semantically coherent texts for all NER tasks, and use the diversity beam search to improve the diversity of augmented texts. Experiments on thirteen public real-world datasets, and coherence and diversity analysis show the effectiveness of {\\modelname}. Moreover, we can also apply the method of data augmentation to low-resource relation extraction \\cite{hu2020selfore,hu2021gradient,hu2021semi,liu2022hierarchical,hu2023think}, natural language inference \\cite{li2023multi,li2022pair}, semantic parsing \\cite{liu2022semantic,liu2023comprehensive}, and other NLP application tasks, thus realizing knowledge enhancement based on data augmentation approach.\n\n\\section{Limitations}\nWe discuss the limitations of our method from three perspectives. \n\nFirst, our method is based on pre-trained language models, so compared to rule-based data augmentation methods (synonym replacement, shuffle within segments, etc.), our method requires higher time complexity. \n\nSecond, the entity matching process (Section \\ref{exploitation}) will discard sentences which cannot match entities in the entity list, which will affect the utilization of data. \n\nThird, our data augmentation method based on the pre-trained language models, whose generalization ability is limited since the augmented knowledge comes from the pre-trained language models. However, the knowledge in pre-trained language models is limited and not domain-specific. How to improve the generalization ability of the data augmentation methods is a future research work.\n\n\\section{Acknowledgement}\nWe thank the reviewers for their valuable comments. Yong Jiang and Lijie Wen are the corresponding authors. Xuming Hu, Aiwei Liu and Lijie Wen were partially supported by the National Key Research and Development Program of China (No. 2019YFB1704003), the National Nature Science Foundation of China (No. 62021002), Tsinghua BNRist and Beijing Key Laboratory of Industrial Bigdata System and Application. Philip S. Yu was partially supported by the NSF under grants III-1763325, III-1909323, III-2106758, SaTC-1930941.\n\n\\clearpage\n\\newpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\n\nIn this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct Universal Information Extraction (UIE) via code generation. KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately. To achieve these, KnowCoder introduces a code-style schema representation method to uniformly transform different schemas into Python classes, with which complex schema information, such as constraints among tasks in UIE, can be captured in an LLM-friendly manner. We further construct a code-style schema library covering over $\\textbf{30,000}$ types of knowledge, which is the largest one for UIE, to the best of our knowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase learning framework that enhances its schema understanding ability via code pretraining and its schema following ability via instruction tuning. After code pretraining on around $1.5$B automatically constructed data, KnowCoder already attains remarkable generalization ability and achieves relative improvements by $\\textbf{49.8\\%}$ F1, compared to LLaMA2, under the few-shot setting. After instruction tuning, KnowCoder further exhibits strong generalization ability on unseen schemas and achieves up to $\\textbf{12.5\\%}$ and $\\textbf{21.9\\%}$, compared to sota baselines, under the zero-shot setting and the low resource setting, respectively. Additionally, based on our unified schema representations, various human-annotated datasets can simultaneously be utilized to refine KnowCoder, which achieves significant improvements up to $\\textbf{7.5\\%}$ under the supervised setting.\n\n\\end{abstract}\n\n\\section{Introduction}\n\n\\begin{figure}[tbp]  \n    \\vspace{8mm}\n    \\centering\n    \\includegraphics[width=1\\linewidth]{./pic/intro-schema.pdf}\n    \\caption{An illustration of KnowCoder schemas.} \n    \\vspace{-4mm}\n    \\label{fig:illustration}\n\\end{figure}\n  \nInformation Extraction (IE) aims to extract explicit and structured knowledge\nfollowing the manually designed schemas. The IE schemas define high-level types\nof knowledge (i.e., concepts) and structures among them~\\cite{kg-book}, which\ninclude various types of entities, relations, and events. To simultaneously\nextract various knowledge under different schemas via a single model, the\nUniversal Information Extraction (UIE) task is proposed~\\cite{lin2020joint}.\nRecently, Large Language Models (LLMs) have demonstrated general understanding\nabilities through large-scale pretraining, which drives their increasing\nutilization in UIE. However, their performance on UIE is still limited because\nof two main challenges: (1) the lack of a unified schema representation method\nthat LLMs can easily understand; (2) the lack of an effective learning framework\nthat encourages LLMs to accurately follow specific schemas for extracting\nstructured knowledge.\n\nFor the first challenge, the existing UIE models first represent different\nschemas in a universal way, such as classification labels~\\cite{lin2020joint},\nkeywords~\\cite{gui2023instructie}, or a specifically-designed formal\nlanguage~\\cite{lu-etal-2022-unified}. These schema representation methods have\nthree main restrictions: (1) ignoring information like taxonomies (e.g.,\n``fairytale'' is a subclass of ``written work'') and constraints among concepts\n(e.g., ``spouse'' relation exists between two ``human'' entities); (2)\nclassification labels or a specifically designed formal language is hard for\nLLMs to understand and follow; (3) designed for specific IE datasets and lacking\na general schema library.\n\nTo solve these restrictions, in this paper, we propose a kind of code-style\nschema representation method, with which various types of knowledge are\ngenerally defined as Python classes. As shown in Figure~\\ref{fig:illustration},\nthe class inheritance mechanism is adopted to describe the concept taxonomies. A\nmechanism of type hint is employed to model constraints among different\nconcepts. The class comments are used to provide clear definitions of concepts.\nAnd, the class methods are used to post-process the results according to\nspecific IE guidelines. Upon this method, we construct a comprehensive\ncode-style schema library covering over $29,000$ entity types, $900$ relation types,\nand $500$ event types based on Wikidata, the largest one for UIE, to the best of\nour knowledge, currently reported in the open literature.\n\nFor the second challenge, the existing learning framework for UIE directly\nconducts instruction tuning on LLMs to extract knowledge following specific and\nlimited schemas~\\cite{sainz2023gollie,wang2023instructuie}. The enormous\nconcepts in the constructed schema library challenge the existing training\nframework. To help LLMs better understand and follow these schemas, we propose\nan effective two-phase framework containing a schema understanding phase and a\nschema following phase. The former improves the ability of LLMs to understand\ndifferent concepts in schemas via large-scale code pretraining on the schema\ndefinition code and corresponding instance code. The latter advances their\nabilities to follow specific schemas in an IE task via instruction tuning. After code pretraining on around 1.5B automatically constructed data, KnowCoder already attains remarkable generalization ability and achieves NER improvements compared to the base model, LLaMA2, by $\\textbf{49.8\\%}$ relative F1 point under the few-shot setting on NER. After instruction tuning on 1.5B automatically annotated data,  KnowCoder experimentally demonstrates strong generalization ability on unseen schemas. Under the zero-shot setting,  KnowCoder achieves average relative improvements up to $\\textbf{12.5\\%}$ on  the NER task. Under the low-resource setting, KnowCoder gets average relative improvements up to $\\textbf{21.9\\%}$ on all the IE tasks. Additionally, based on our unified schema representation, various IE datasets can be simultaneously utilized to refine KnowCoder. After refinement, KnowCoder achieves consistent improvements across all IE tasks under the supervised setting, getting up to $\\textbf{7.5\\%}$ improvement on the relation extraction task, respectively. \n\nIn general, the main contributions of this paper include: \n\\begin{itemize}\n  \\item We propose a code-style schema representation method to uniformly\n  represent different schemas for UIE. Using this method, we construct a large\n  code-style schema library covering more than $30,000$ types of knowledge.\n  \\item We propose an effective learning framework for LLMs in a two-phase\n  manner, which first enhances the schema understanding through code pretraining\n  and then boosts schema following via instruction tuning. \n  \\item After training on billions of automatically annotated data and refining\n  with human-annotated IE datasets, KnowCoder demonstrates superior performance\n  on different IE tasks under the zero-shot, low-resource, and supervised\n  settings.\n  \\item The constructed schema library, training data, code, and models are released for future research.\n\\end{itemize}\n\n\\section{KnowCoder Schema} \nThe proposed schema representation method uses code, a language that LLMs easy to understand, to define schemas. Specifically, KnowCoder schema adopts a series of programming language features to comprehensively model schema information, including the concept taxonomies, the constraints among different concepts, the definition of concepts, and other extraction requirements. Besides, considering that previous schema representation methods are only designed for specific datasets and contain limited types of knowledge, we further construct a\nlarge-scale schema corpus containing a wide range of knowledge.\n\n\\subsection{Code-style Schema Representation Method}\n\n The code-style schema representation method comprises three basic classes,\n namely, \\texttt{``\\textcolor{darkgreen}{Entity}''}, \\texttt{``\\textcolor{darkgreen}{Relation}''}, and \\texttt{``\\textcolor{darkgreen}{Event}''}. Based on the three basic\n classes, we represent all the concepts in the schemas by the corresponding\n classes. Then, the instances of each concept can be represented by the objects\n of the corresponding class. In the following, we will introduce four features\n of the proposed representation method.\n\n  \\paragraph{Class Inheritance.} We adopt the class inheritance mechanism to\n  account for the taxonomies in the schemas. Specifically, we let class A\n  inherit all the class members from class B if the corresponding concept A is\n  the hyponym of concept B in the taxonomies. For a concept with multiple\n  hypernyms, the hypernym concept with the most instances is selected. The class\n  of an unseen concept can inherit from an existing class or directly from the\n  basic class.\n  \n  \\paragraph{Class comment.} Similar to ~\\citet{sainz2023gollie}, we adopt\n  class comments to provide clear definitions of concepts. As shown in\n  Figure~\\ref{fig:illustration}, a class comment includes a natural language\n  description that explains the corresponding concept and the examples of instances\n  corresponding to that type. When there is an unseen concept, we use the\n  description in its annotation guidelines~\\footnote{If the annotation\n  guidelines are missing, we use the description generated by GPT-4.} and\n  manually give out a few examples. \n\n  \\paragraph{Type Hint.} Type hint is a formal solution to indicate the type of\n  a value in the code. We adopt type hints in the initialization function of a\n  class to define its constraints with other classes strictly. Thus, the\n  constraints among the concepts in the schemas are modeled. As shown in\n  Figure~\\ref{fig:illustration}, taking the relation ``PlaceOfBirth'' for\n  example, \\texttt{``def \\_\\_init\\_\\_(self, head\\_entity: \\textcolor{darkgreen}{Human}, tail\\_entity:\n  \\textcolor{darkgreen}{SpatialEntity})''} denotes that the head entity must be a \\texttt{``\\textcolor{darkgreen}{Human}''} and the tail\n  entity must be a \\texttt{``\\textcolor{darkgreen}{SpatialEntity}''}.\n\n  \\paragraph{Class Method.} A class method is bound to the class and not the\n object of the class. They are utilized to post-process the extracted instance\n results of a class. For example, some IE tasks may not consider the pronouns\n ``he'' and ``she'' as instances of the \\texttt{``\\textcolor{darkgreen}{Human}''} concept. To address this, a\n class method can be added to the \\texttt{``\\textcolor{darkgreen}{Human}''} class to filter out such pronouns\n from the extraction results, ensuring that the output aligns with the task's\n unique criteria. Note that, class methods are manually designed for specific IE\n tasks based on their task constraints. We take a few IE datasets to demonstrate\n the effectiveness of class methods in our experiments, as shown in the\n Appendix~\\ref{sec:class_method}.\n\n\\subsection{Schema Library Construction}\nWe construct the code-style schema library based on Wikidata~\\footnote{We use the Wikidata dump up to 20220704.}. We\nselect the concepts included in the existing IE datasets created from Wikidata, i.e., KELM~\\cite{agarwal-etal-2021-knowledge}, UniversalNER~\\cite{zhou2023universalner}, InstructIE~\\cite{knowlm}, and LSEE~\\cite{chen-etal-2017-automatically}. We derive the constraints among concepts according to their co-occurrences. To construct the taxonomies, we\nextract the \\texttt{``\\textcolor{darkgreen}{SubclassOf}''} relations among these concepts from Wikidata. To obtain the description of a concept, we use its definition from Wikidata directly or generate its descriptions using GPT-4 if its definition in Wikidata\nis missing. Finally, the constructed schema library encompasses over $29,177$ entity types, $876$ relation types, and $519$ event types. The detailed statistics of the schema are in Appendix \\ref{appendix:stat}.\n\n\\begin{figure*}[tbp]  \n  \\centering\n  \\includegraphics[width=1.0\\textwidth]{./pic/knowcoder-framework.pdf}\n  \\caption{An diagram of training and inference processes of KnowCoder.}\n \\vspace{-4mm}\n  \\label{fig:training_framework}\n  \\end{figure*}\n\n\\section{Learning Framework of KnowCoder}\n\nTo discriminate enormous concepts defined in schemas, we first let KnowCoder understand each concept through its definition and instances. Subsequently, we enhance KnowCoder to discriminate among a few concepts and extract corresponding knowledge. Thus, as shown in Figure~\\ref{fig:training_framework}, the proposed learning framework contains two phases, i.e., the schema understanding phase and the schema following phase. In the schema understanding phase, KnowCoder undergoes code pretraining to understand each concept in two manners: 1) Go through the class definition code of each concept. 2) Go through the instance codes of each concept. In the schema following phase, KnowCoder is finetuned using instruction tuning code, where multiple task-demanded concepts are given in the schemas, enhancing KnowCoder's ability to follow schemas and generate instantiating code accordingly.\n\n\\subsection{Schema Understanding Phase}\n\n\\subsubsection{Training Data Generation}\nTo enhance KnowCoder's schema understanding abilities, we construct a large-scale training dataset based on the schema library. As shown in the left part of Figure~\\ref{fig:training_framework}, the training data consists of two kinds of codes, i.e., schema definition codes and instance codes. The schema definition codes are generated based on the schema library, where we randomly sample a certain number of concepts (decided by the maximum sequence length) from the schema library to consist of a training sample. As the aim of the schema understanding phase is to understand each concept but not to discriminate various concepts, the instance code corresponding to a single concept contains three parts, i.e., a sentence containing instances of the given concept, an import clause to introduce the corresponding class of the given concept, and an instantiating clause to give out all the instances of the given concept in the sentence. The schema-instance codes are constructed based on KELM corpus~\\cite{agarwal-etal-2021-knowledge}, which contains $15, 628, 486$ synthetic sentences to describe the structured knowledge from Wikidata. We do data cleaning for the corpus. The cleaning details are in Appendix \\ref{sec: data_clean}.\n\n\\subsubsection{Code Pretraining}\n\nAfter obtaining the data, we apply regular code pretraining to make LLM understand the diverse concepts in the schemas. Given a training sample with length of $L$, $X = {x_0, x_1, ..., x_{i}, ..., X_{L-1}}$, the model attempts to predict every token $x_{l}$ based on the ${x_0, ..., x_{l-1}}$, where $l={0,...,L-1}$. Some training details are as follows:\n\n\\paragraph{Schema Importing.}\nThe straightforward way to construct a pretraining sample is to directly give the whole schema definition for the corresponding instance code. However, this manner may cause the model to overfit the schema definition code because they are frequently repeated in every instance code. To address this problem, we separate the schema definition code from the instance code and use the ``import'' clause to introduce the corresponding schema definition to the instance code.\n\nThe position of the ``import'' clause is also critical for the LLMs to learn. We study two positions for the ``import'' clause, i.e., ``Import-First'' and ``Sentence-First''. We adopt ``Sentence-First'' in the learning framework because it performs better than the others. The comparison results are in Appendix~\\ref{sec:import_position}.\n\n\\subsection{Schema Following Phase}\n\n\\subsubsection{Training Data Generation}\n\n  To enhance the schema following abilities of KnowCoder, we construct instruction tuning training data for UIE tasks. As shown in the middle part of Figure~\\ref{fig:training_framework}, a typical instruction tuning sample contains three parts of codes, i.e., instruction code $T$, input code $I$, and output code $O$. \n  \n  The instruction code $T$ comprises two snippets, i.e., schema definition and task description. The schema definition snippet includes definitions of some concepts selected from the former phase, which defines specific concepts to be extracted. The task description snippet includes a comment that contains a natural language description of an IE task. For example, the task description of Relation Extraction (RE) is ``This is an object-oriented programming task: some Classes are defined above. Please instantiate all the corresponding Objects in the following sentence.''. The input $I$ contains the sentence to be extracted, which is denoted as a variable ``sentence'', i.e., ``sentence = ...''. The output $O$ contains all the golden knowledge in the sentence, denoted as a list variable ``results'', i.e., ``results = [...]''. We have conducted a performance comparison of different versions of the instructions, and the corresponding results are in Appendix~\\ref{sec:prompt}.\n  \n  We construct the training corpus from three data sources. For Named Entity Extraction (NER), ChatGPT-annotated Pile corpus~\\cite{zhou2023universalner} is selected. For Relation Extraction (RE) and Event Extraction (EE), we adopt the data sources constructed in \\citet{gui2023instructie}\\footnote{We use the English version of the constructed data source.} and LSEE~\\cite{chen-etal-2017-automatically}, respectively. \n\n\\subsubsection{Instruction Tuning}\n\nThe objective of instruction tuning is to learn an LLM $\\mathbf{f}:(I \\times T) \\rightarrow O$. The LLM takes input code $I$, and instruction code $T$ as input. Subsequently, the LLM is tuned to generate every token in the output $O$. Some training details are as follows:\n\n\\paragraph{Negative Class Sampling.} In the constructed schema library, there are more than $30000$ concepts. It is challenging for the model to accommodate all the corresponding class definitions in a single prompt. Consequently, KnowCoder employs a negative class sampling strategy. For each training sample, in addition to the classes annotated in the sentence, we randomly sample several classes ($20\\%$ number of the golden classes) from the remaining classes.\n\n\\paragraph{Fully negative Sample Construction.} In real-world scenarios, many sentences do not contain any knowledge of a specific IE task, called fully negative samples in this paper. However, the selected data sources neglect such samples. To address this problem, we randomly sample $5\\%$ sentences from the data sources. For each sentence, we replace the golden classes with five random negative classes.\n\n\\subsection{Refinement}\n\nAfter schema understanding and following, we obtain KnowCoder, an LLM that demonstrates strong generalization ability on unseen schemas. Additionally, based on our unified schema representation, KnowCoder can be further refined by various human-annotated datasets simultaneously. In this phase, we conduct instruction tuning based on the datasets used in previous work~\\cite{wang2023instructuie, sainz2023gollie}.\n\nIn different IE datasets, concepts with the same name may follow different annotation guidelines. Take \\texttt{``\\textcolor{darkgreen}{PERSON}''} for example, in MultiNERD~\\cite{multiNERD_DATASET}, entities do not include the pronouns, e.g., ``he'' and ``she'', while ACE05~\\cite{ACE2005_DATASET} consider personal pronouns as \\texttt{``\\textcolor{darkgreen}{PERSON}''}. To alleviate the problem, we add specific dataset information in the instructions to distinguish annotation guidelines for different datasets. For example, the instruction for the ACE05 dataset is ``... Please instantiate all the corresponding Event Objects in the following sentence \\texttt{\\textcolor{darkgreen}{from DATASET ACE05}}.''\n\n\\section{Experiment Setup}\n\n\\paragraph{Datasets.}\nWe conducted experiments using $33$ specific domain Information Extraction (IE) datasets, including $23$ datasets for Named Entity Extraction (NER), $8$ datasets for Relation Extraction (RE), $2$ datasets for Event Detection (ED) and Event Argument Extraction (EAE). The detailed statistics of these datasets are in Appendix \\ref{appendix:stat}. Among these NER datasets, following ~\\citet{wang2023instructuie, zhou2023universalner}, we take $7$ datasets as the zero-shot benchmark, including $5$ datasets of different domains from CrossNER~\\cite{CrossNERDATASET}, MIT-Movie~\\cite{MITReviewDataset} and MIT-Restaurant~\\cite{MITReviewDataset}. For RE, we adopt GIDS~\\cite{Jat2018ImprovingDS} as the zero-shot dataset. Following~\\cite{sainz2023gollie}, we adopt CASIE~\\cite{Lu2021Text2EventCS} as the zero-shot ED dataset.\n\nTo balance the evaluation coverage and costs, we introduce the KnowCoder benchmark, a composite derived from existing NER, RE, and EE datasets. Under the supervised setting, a sampling strategy was developed for NER and RE tasks to maintain the distributions of original datasets and ensure the broad coverage of knowledge types. Details on the proposed strategy and comprehensive benchmark information are available in Appendix~\\ref{appendix:benchmark}. \n\n\\begin{table*}\n  \\small\n  \\centering\n  \\setlength\\tabcolsep{2pt}  % control column spacing\n  \\resizebox{0.8\\linewidth}{!}\n  {\\begin{tabular}{@{}l|ccccccc|c@{}}\n  \\toprule\n  \\textbf{Model} &\n  \\textbf{Movie.} &\n  \\textbf{Rest.} &\n  \\textbf{AI} &\n  \\textbf{Litera.} &\n  \\textbf{Music} &\n  \\textbf{Politics} &\n  \\textbf{Science} &\n  \\textbf{Average} \\\\ \\midrule\n\n  LLaMA2-7B & 31.0 & 19.6 & 30.8 & 24.1 & 28.0 & 38.7 & 44.1 & 30.9 \\\\\n  LLaMA2-13B  & 32.6 & 25.2 & 37.5 & 36.5 & 37.0 & 60.3 & \\textbf{51.7} & 40.1 \\\\\n\n  \\midrule\n  LLaMA2-7B & 31.0 & 19.6 & 30.8 & 24.1 & 28.0 & 38.7 & 44.1 & 30.9 \\\\\n  KnowCoder-7B~(SU. only)   & \\textbf{37.2}& \\textbf{36.4} & \\textbf{41.8} & \\textbf{42.6} & \\textbf{53.8} & \\textbf{60.6} & 51.6 & \\textbf{46.3}$^{\\uparrow\\textbf{49.8\\%}}$ \\\\\n\n  \\bottomrule\n  \\end{tabular}}\n  \\caption{Results on NER under the few-shot setting.}\n  \\label{tab:fewshot-ner-results-table}\n\\end{table*}\n\n\\begin{table*}\n  \\centering\n  \\resizebox{1\\linewidth}{!}\n  {\\begin{tabular}{@{}lcccccccc}\n  \\toprule\n  \\textbf{Model} &\n  \\textbf{Movie.} &\n  \\textbf{Rest.} &\n  \\textbf{AI} &\n  \\textbf{Litera.} &\n  \\textbf{Music} &\n  \\textbf{Politics} &\n  \\textbf{Science} &\n  \\textbf{Average} \\\\ \n  \\midrule\n  \\textit{\\textbf{w. refinement}} \\\\\n  \\rowcolor{softblue} InstructUIE-11B~\\cite{wang2023instructuie} & - & - & 48.4 & 48.8 & 54.4 & 49.9 & 49.4 & -  \\\\\n  \\rowcolor{softblue} GoLLIE-7B~\\cite{sainz2023gollie} & 63.0  & 43.4 & 59.1 & 62.7 & 67.8 & 57.2  & 55.5 & 58.4 \\\\\n  \\rowcolor{softblue} GoLLIE-13B~\\cite{sainz2023gollie} & 62.5  & 49.8 & 56.7 & 59.7 & 65.5 & 54.4  & 56.2 & 57.8 \\\\\n  \\rowcolor{softblue} UniNER-7B~(refined)~\\cite{zhou2023universalner} &  59.4 & 31.2 & 62.6 & 64.0 & 66.6 & 66.3 & 69.8 & 60.0 \\\\\n  \\midrule\n  \\textit{\\textbf{w.o. refinement}} \\\\\n  Vicuna-7B~\\cite{chiang2023vicuna}  & 6.0 & 5.3 & 12.8 & 16.1 & 17.0 & 20.5 & 13.0 & 13.0  \\\\\n  Vicuna-13B~\\cite{chiang2023vicuna}  & 0.9 & 0.4 & 22.7 & 22.7 & 26.6 & 27.2 & 22.0 & 17.5  \\\\\n  ChatGPT~\\cite{ouyang2022training}  & 5.3 & 32.8 & 52.4 & 39.8 & 66.6 & 68.5 & \\textbf{67.0} & 47.5  \\\\\n  UniNER-7B~\\cite{zhou2023universalner}  & 42.4 & 31.7 & 53.5 & 59.4 & 65.0 & 60.8 & 61.1 &53.4\\\\\n  \\rowcolor{softyellow} KnowCoder-7B   & \\textbf{50.0} & \\textbf{48.2} & \\textbf{60.3} & \\textbf{61.1} & \\textbf{70.0} & \\textbf{72.2} & 59.1 & \\textbf{60.1}$^{\\uparrow\\textbf{12.5\\%}}$\\\\\n  \\bottomrule\n  \\end{tabular}}\n  \\caption{Results on NER under the zero-shot setting. \\hl{\\textit{\\textbf{w. refinement}}} denotes methods that are refined on human-annotated data, which is unfair for KnowCoder to compare with.}\n  \\label{tab:zeroshot-ner-results-table}\n  \\vspace{-4mm}\n  \\end{table*}\n\n\\paragraph{Metrics.}\nWe report the span-based offset Micro-F1 following previous methods~\\cite{lu-etal-2022-unified, lin-etal-2020-joint}. For NER, an entity is considered correct if the entity boundary and type are correctly predicted. For RE, a relation is considered correct if its triplet matches a golden annotation, including relation type, subject entity, and object entity. For ED, an event trigger is correct if its event type and trigger match a golden annotation. For the EAE task, given an event type, an argument is correct if the argument and its role type match a golden annotation. \n\n\\subsection{Implementation Details}\n\nKnowCoder is finetuned based on LLaMA2-base-7B~\\cite{touvron2023llama2}. We\nutilize the Megatron-LM framework~\\cite{shoeybi2019megatron} for schema\nunderstanding. We set the context length to $2048$, the learning rate to $5\n\\times 10^{-6}$, the global batch size to $1$M tokens, and the maximum training\nstep to $4500$. For the schema following and refinement phases, we use\nLoRA~\\cite{hu2021lora} for parameter-efficient fine-tuning. We set the lora rank\nand lora alpha parameters to $32$ and $64$, respectively. The warmup ratio is\nset to $0.03$ and the dropout ratio is set to $0.1$. The learning rates for\nthese two phases are set to $3 \\times 10^{-4}$. We limit the sequence length to\n$4096$ and set the batch size to $256$. Detailed information about the training\nprocess is available in Appendix \\ref{implementation_details}. During the\ninference phase, we use greedy search and set the temperature to $0$. The\nmaximum output length is set to $640$.\n\n\\section{Results and Analyses}\n\n\\subsection{Few-shot Evaluation After Schema Understanding}\n\nConsidering that a pre-trained LLM cannot give proper results without given examples, we study the generalization ability of KnowCoder after the schema understanding phase, denoted as KnowCoder (SU. only), under the few-shot setting. Specifically, We utilize the first five samples from the training data as examples and report the NER F1 score in Table~\\ref{tab:fewshot-ner-results-table} across seven zero-shot NER datasets. The results demonstrate that KnowCoder (SU. only) outperforms LLaMA2-7B with an average relative improvement of $\\textbf{49.8\\%}$. Remarkably, KnowCoder (SU. only) gets an average F1 score of $46.3\\%$ with only a few examples, which are comparable to InstructUIE refined using human-annotated datasets. These results strongly support the effectiveness of the schema understanding phase in enhancing model generalization and performance in NER tasks.\n\n\\begin{table}\n\\centering\n\\small\n\n\\begin{tabular}{@{}r|r|l@{}}\n\\toprule\n\n\\textbf{Dataset}   & \\multicolumn{1}{r|}{\\textbf{SoTA}} & \\multicolumn{1}{c}{\\KnowCoder~\\textbf{7B}} \\\\ \\midrule\nGIDS$_{RE}$      &     \\cite{ouyang2022training} 9.9  &       \\textbf{25.5}                        \\\\\nCASIE$_{ED}$   &         \\cite{sainz2023gollie} \\textbf{59.3$^{\\dag}$}          &       58.2                        \\\\\n\\midrule\nAverage    &             34.6                    &         \\textbf{41.9$^{\\uparrow\\textbf{21.1\\%}}$}                      \\\\ \\bottomrule\n\\end{tabular}\n\n\\caption{Results on RE and ED tasks under the zero-shot setting. $^{\\dag}$ indicates that it is unfair for KnowCoder to compare with the score.}\n\\label{tab:re-ed-zero-shot-results}\n\\vspace{-5mm}\n\\end{table}\n\n\\subsection{Zero-Shot Evaluation After Schema Following}\n\nTo verify the generalization ability of KnowCoder, we conduct zero-shot experiments on 9 datasets across NER, RE, and ED tasks. In this setting, we employ KnowCoder after schema understanding and following to conduct extraction. KnowCoder is compared with two kinds of baselines. One is the LLM-based IE method that refined on human-annotated data, including InstructUIE~\\cite{wang2023instructuie}, GoLLIE~\\cite{sainz2023gollie}, and UniNER~\\cite{zhou2023universalner}. The other is models without refinement, including Vicuna~\\cite{chiang2023vicuna}, ChatGPT, UniNER~\\cite{zhou2023universalner}. The results of these three baselines are from \\citet{zhou2023universalner}.  Note that KnowCoder is unfair when compared with methods after refinement.\n\n\\paragraph{Main Results.}\nThe results of zero-shot NER are in Table~\\ref{tab:zeroshot-ner-results-table}.  It can be seen that KnowCoder surpasses baselines without refinement across four NER datasets, registering a relative performance enhancement of $\\textbf{12.5\\%}$. This improvement is attributed to KnowCoder's training on a large-scale, automatically generated dataset within a two-phase learning framework, which enhances its generalization capabilities for NER, even surpassing methods refined with human-annotated data. The results of zero-shot RE and ED are in Table~\\ref{tab:re-ed-zero-shot-results}. For ED, KnowCoder's performance is inferior to GoLLIE, a baseline model trained on high-quality, human-annotated data. This emphasizes that human-annotated datasets can enhance performance for more difficult IE tasks, such as ED. To further substantiate the point, we further refine KnowCoder with the ACE05 dataset, the same EE training data employed by GoLLIE. This refinement significantly improves zero-shot F1 performance to $72.0\\%$ on the CASIE dataset. This represents a significant advancement over GoLLIE's performance of $59.3\\%$, marking a relative improvement of $\\textbf{21.4\\%}$.\n\n\\begin{table}\n  \\centering\n  \\setlength\\tabcolsep{2.3pt}  % control column spacing\n  \\resizebox{1\\linewidth}{!}\n  {\n  \\begin{tabular}{@{}c|c|cccc|c@{}}\n  \\toprule\n  \\multirow{2}{*}{\\textbf{Ratio}} & \\multirow{2}{*}{\\textbf{Model}} & \\multicolumn{4}{c|}{\\textbf{Task}} & \\multirow{2}{*}{\\textbf{Average}} \\\\\n  \\cmidrule{3-6}\n  \n  &       & ${NER}$   & ${RE}$   & ${ED}$  & ${EAE}$  &            \\\\ \n      \\midrule\n  \\multirow{3}{*}{1\\%}   & UIE-base                    &   \\textbf{82.8}    &   30.8   &  41.5   &   12.8   &    42.0                  \\\\\n  \n  & LLaMA2-7B              &    72.3   &    32.1     &   35.3  &   33.3  &    43.3                  \\\\\n         \n  & KnowCoder-7B           &    79.2   &    \\textbf{43.3}     &   \\textbf{50.3}  &  \\textbf{38.5}  &     \\textbf{52.8$^{\\uparrow\\textbf{21.9\\%}}$}                 \\\\ \n  \n  \\midrule\n                         \n  \\multirow{3}{*}{5\\%}   & UIE-base                   &   88.3    &    \\textbf{51.7}   &  55.7   &   30.4  &            56.5            \\\\\n  \n  & LLaMA2-7B              &   89.3    &    35.7     &   52.6  &   46.3  &    56.0                \\\\\n  \n  & KnowCoder-7B           &   \\textbf{90.6}    &    51.1     &   \\textbf{59.0}  &   \\textbf{48.3} &      \\textbf{62.3$^{\\uparrow\\textbf{10.3\\%}}$}               \\\\ \n     \n  \\midrule\n  \n  \\multirow{3}{*}{10\\%}  & UIE-base                 &   89.6    &   \\textbf{59.2}   &   60.3  &    36.3  &          61.4            \\\\\n  \n  & LLaMA2-7B              &   91.2    &    48.6     &   60.7  &   52.3 &      63.2                \\\\\n  \n  & KnowCoder-7B           &   \\textbf{92.2}    &    53.6     &   \\textbf{62.2}  &   \\textbf{55.1} &      \\textbf{65.8$^{\\uparrow\\textbf{4.1\\%}}$}                \\\\ \n  \n  \\bottomrule\n  \\end{tabular}\n  }\n  \\caption{\n      Low-resource results on IE tasks, where \\textbf{Average} is the average F1 across four IE tasks.\n  }\n  \\label{tab:lowresource}\n  \\vspace{-3mm}\n\\end{table}\n\n\\subsection{Low Resource Evaluation After Schema Following} \n\nTo further investigate the generalization ability of KnowCoder for IE tasks, we conduct low-resource experiments by fine-tuning KnowCoder with three different partitions of the original training sets (1/5/10\\% ratio) across four tasks. Following \\citet{lu-etal-2022-unified},\nwe adopt CoNLL03, CoNLL04, ACE05$_{ED}$ and ACE05$_{EAE}$ as the benchmarks for NER, RE, ED, and EAE tasks. LLaMA2 denotes directly fine-tuning LLaMA2 with these partial training data. The results are in Table~\\ref{tab:lowresource}. It\ncan be shown that KnowCoder gets the highest average F1 scores across all IE tasks in low-resource settings at varying ratios. In ratio $1\\%$, KnowCoder gets the relative average improvement of $\\textbf{21.9\\%}$ compared to UIE, which shows that KnowCoder has strong adaptability to downstream IE tasks after pretraining on large-scale data under the two-phase learning framework. \n\n\\subsection{Supervised Evaluation After Refinement}\n  \n  \nUnder the supervised evaluation, KnowCoder is further refined with the IE datasets. We conduct supervised experiments on four IE tasks, including NER, RE, ED, and EAE. KnowCoder is compared with three kinds of methods. The first is the traditional UIE method~\\cite{lou2023universal, lu-etal-2022-unified}, which is based on relatively small language models (i.e., million-level parameters). The latter two are based on LLMs (i.e., ChatGPT, LLaMA2). They adopt the in-context learning~\\cite{guo2023retrieval, codeie, ashok2023promptner} and supervised fine-tuning paradigms~\\cite{zhou2023universalner,wang2023instructuie,sainz2023gollie}, respectively. As some baselines only report results for specific IE tasks, we report the SOTA results of the above methods in each dataset, denoted as ``SoTA'' in the tables. As highlighted by ~\\citet{zhou2023universalner}, the evaluation script of InstructUIE~\\cite{wang2023instructuie} contains issues. Furthermore, the benchmark in ~\\citet{zhou2023universalner} remains pending release. In the end, we have implemented these two baselines on KnowCoder benchmark using their released models.\n\n\\begin{table}[]\n  \\centering\n  \\setlength\\tabcolsep{1.6pt}  % control column spacing\n  \\resizebox{0.95\\linewidth}{!}{\n  \\begin{tabular}{@{}c|r|c@{}}\n  \\toprule\n  \n  \\textbf{Dataset}   & \\multicolumn{1}{r|}{\\textbf{SoTA}} & \\multicolumn{1}{c}{\\KnowCoder~7B} \\\\ \\midrule\n  ACE04      &         \\cite{lu-etal-2022-unified} \\textbf{87.6}               &      86.2                        \\\\\n  ACE05      &         \\cite{sainz2023gollie} \\textbf{89.6}               &       86.1                        \\\\\n  AnatEM   &         \\cite{zhou2023universalner} \\textbf{88.9}         &       86.4                       \\\\\n  Broad Twitter  &         \\cite{zhou2023universalner} \\textbf{79.8}                  &     78.3                        \\\\\n  CoNLL03   &         \\cite{zhou2023universalner} 94.8          &       \\textbf{95.1}                        \\\\\n  DIANN      &          \\cite{sainz2023gollie} 84.1         &       \\textbf{94.7}                       \\\\\n  FabNER &          \\cite{zhou2023universalner} 82.3        &       \\textbf{82.9}                      \\\\\n  FindVehicle        &         \\cite{zhou2023universalner} 98.4       &       \\textbf{99.4}                     \\\\\n  GENIA        &         \\cite{zhou2023universalner} \\textbf{80.3}        &       76.7                    \\\\\n  Movie       &         \\cite{zhou2023universalner} 90.2        &       \\textbf{90.6}                        \\\\\n  Rest. &          \\cite{wang2023instructuie} \\textbf{82.6}        &       81.3                    \\\\\n  MultiNERD       &         \\cite{zhou2023universalner} 93.9        &       \\textbf{96.1}                        \\\\\n  OntoNotes 5  &         \\cite{sainz2023gollie} 84.6                  &      \\textbf{88.2}                        \\\\\n  WikiANN     &         \\cite{zhou2023universalner} 85.4                  &      \\textbf{87.0}                       \\\\ \n  WNUT17     &         \\cite{sainz2023gollie} 54.3                  &      \\textbf{66.4}                       \\\\ \n  bc2gm      &          \\cite{wang2023instructuie} 80.5         &       \\textbf{82.0}                      \\\\\n  bc5cdr     &         \\cite{zhou2023universalner} \\textbf{91.5}                  &       89.3                        \\\\ \n  ncbi     &         \\cite{wang2023instructuie} \\textbf{85.0}                  &  83.8                     \\\\\n  \\midrule\n  Average    &             85.2                    &         \\textbf{86.1$^{\\uparrow\\textbf{1.1\\%}}$}                      \\\\ \\bottomrule\n  \\end{tabular}}\n  \\caption{Results on NER under the supervised setting.}\n  \\vspace{-4mm}\n  \\label{tab:ner-supervised-results}\n\\end{table}\n\n\\paragraph{Main Results.}\nThe results for NER, RE, EE (including ED and EAE) tasks are shown in Tables~\\ref{tab:ner-supervised-results}, ~\\ref{tab:re-supervised-results} and ~\\ref{tab:ed-eae-supervised-results}, respectively. We can observe that: (1) KnowCoder outperforms the SOTA baselines on most datasets for NER, RE, ED, and EAE, respectively. Based on the code-style schemas, KnowCoder universally models IE tasks and effectively transfers IE abilities after conducting schema understanding, following, and refinement on large-scale training data. (2) In more challenging UIE tasks, such as RE, KnowCoder demonstrates impressive advancements up to the relative improvement of 8.6\\% compared to the SOTA baselines. KnowCoder achieves the performances of 73.9\\% for ED and 66\\% for EAE. This is \\textbf{the first time} LLM-based UIE methods surpass smaller models like UIE in ED and EAE tasks. The code-style schemas and the learning framework enable a more precise definition and understanding of this complex structured knowledge, leading to a significant improvement. (4) UniNER~\\cite{zhou2023universalner} achieves comparable results to KnowCoder on NER. Nonetheless, KnowCoder surpasses UniNER in several respects. Primarily, UniNER is limited to extracting one type of entity per iteration, leading to a cost-time complexity. In contrast, KnowCoder can extract multiple entity types in a single iteration, enhancing efficiency. Additionally, UniNER relies on a text-style schema, making it hard to represent and extract relations and events effectively. Conversely, KnowCoder, as a UIE model, offers broader versatility and efficacy comparing to UniNER.  (3) KnowCoder gets better results than baselines with code-style prompt ~\\cite{codeie, guo2023retrieval, sainz2023gollie}. This is because KnowCoder provides a more comprehensive schema representation method and conducts two-phase training to understand and follow these schemas.\n\n\\begin{table}[]\n  \\centering\n  \\setlength\\tabcolsep{2pt}  % control column spacing\n  \\resizebox{0.85\\linewidth}{!}{\n  \\begin{tabular}{@{}c|r|c@{}}\n  \\toprule\n  \n  \\textbf{Dataset} & \\textbf{SoTA} & \\KnowCoder~7B \\\\\n  \\midrule\n  ACE05      &        \\cite{sainz2023gollie}~\\textbf{70.1}               &       64.5                        \\\\\n  semevalRE   &       \\cite{wang2023instructuie}~65.8          &       \\textbf{66.3}                        \\\\\n  CoNLL04  &          \\cite{lou2023universal}~\\textbf{78.8}                  &       73.3                        \\\\\n  NYT        &        \\cite{wang2023instructuie}~91.0        &       \\textbf{93.7}                       \\\\\n  ADE corpus &        \\cite{wang2023instructuie}~82.8         &       \\textbf{84.3}                        \\\\\n  kbp37      &        \\cite{wang2023instructuie}~30.6         &       \\textbf{73.2}                        \\\\\n  GIDS      &         \\cite{wang2023instructuie}~76.9         &       \\textbf{78.0}                        \\\\\n  SciERC     &        \\cite{lou2023universal}~37.4                  &       \\textbf{40.0}                        \\\\ \\midrule\n  Average    &        66.7                        &         \\textbf{71.7$^{\\uparrow\\textbf{7.5\\%}}$}                      \\\\ \\bottomrule\n  \\end{tabular}}\n  \\caption{Results on RE under the supervised setting.}\n  \\label{tab:re-supervised-results}\n\\end{table}\n\n\\begin{table}\n  \\centering\n  \\resizebox{0.8\\linewidth}{!}\n  {\n  \\begin{tabular}{@{}c|cc}\n  \\toprule\n  \\textbf{Model}  & \\textbf{ACE05$_{ED}$}         & \\textbf{ACE05$_{EAE}$}    \\\\ \n  \n  \\midrule\n  UIE                     &   73.4    &   69.3   \\\\\n  USM                     &   69.3    &   63.3   \\\\ \n  \\midrule\n  Code4UIE                &   37.4    &   57.0   \\\\ \n  \n  \\midrule\n  \n  InstructUIE-11B             &   43.2    &   56.8   \\\\\n  GoLLIE-7B                  &   72.2    &   66.0   \\\\ \n  \n  \\midrule\n  KnowCoder-7B              &   \\textbf{74.2}   &   \\textbf{70.3}     \\\\\n  \\bottomrule\n  \\end{tabular}}\n  \\caption{Results on ED and EAE under the supervised setting.}\n  \\label{tab:ed-eae-supervised-results}\n  \\vspace{-4mm}\n\\end{table}\n\n\\subsection{Ablation Study}\nTo show how the schema following and understanding phases contribute to KnowCoder under the zero-shot setting, we further conduct ablation studies removing the schema understanding and following phase, denoted as KnowCoder (w.o. SU) and KnowCoder (w.o. SF), respectively. The results on seven zero-shot NER datasets are shown in Table~\\ref{tab:ablation_zero_shot}. It can be seen that: (1) KnowCoder gets better results than KnowCoder (w.o. SF) on most NER datasets. It is because the schema understanding phase helps KnowCoder to understand concepts in the schema by training on definition and instance codes and increases its generalization ability. (2) Results of KnowCoder (w.o. SF) decrease extremely, which proves the importance of schema following. Due to the lack of in-context learning ability, a 7B model without instruction tuning is hard to understand instructions under the zero-shot setting, thus making it hard to finish the IE tasks. \n\n  \n  \n  \n\n\\section{Related Work}\n\\subsection{Universal Information Extraction}\nUniversal information extraction aims to conduct different IE tasks via a single\nmodel. The existing UIE models first represent different schemas for IE tasks in\na universal way. OneIE~\\cite{lin2020joint} represents schemas as classification\nlabels, InstructUIE~\\cite{wang2023instructuie} uses\nkeywords~\\cite{gui2023instructie, lou2023universal} of concepts to represent\nschemas, and UIE~\\cite{lu-etal-2022-unified} uses a specifically-designed formal\nlanguage to represent schemas. Based on such schema representations, these\nmodels adopt language models to understand the schemas and extract the\ncorresponding structured knowledge. \n\n\\begin{table}[]\n\\centering\n\\resizebox{0.85\\linewidth}{!}\n{\n\\begin{tabular}{@{}c|c|cc@{}}\n\\toprule\n\\textbf{Dataset}  & \\KnowCoder~\\textbf{7B} & \\textbf{w.o. SU} & \\textbf{w.o. SF} \\\\ \n\n\\midrule\n\nMovie.   & 50.0      & +1.6    & \\textbf{-50.0}      \\\\\nRest.    & 48.2      & \\textbf{-0.8}    & \\textbf{-46.1}      \\\\\nAI       & 60.3      & \\textbf{-4.5}    & \\textbf{-57.7}      \\\\\nLitera.  & 61.1      & +0.6    & \\textbf{-59.0}      \\\\\nMusic    & 70.0      & \\textbf{-3.1}    & \\textbf{-69.0}      \\\\\nPolitics & 72.2      & \\textbf{-1.8}    & \\textbf{-70.8}      \\\\\nScience  & 59.1      & \\textbf{-2.7}    & \\textbf{-55.6}      \\\\\n\n\\bottomrule\n\\end{tabular}}\n\\caption{Ablation study under the zero-shot setting.}\n\\label{tab:ablation_zero_shot}\n\\vspace{-4mm}\n\\end{table}\n\n\\subsection{Large Language Models for IE}\nDue to the strong generation abilities of LLMs, they have been used in\nIE recently~\\cite{xu2023large}. LLM-based IE methods can be divided into\ntwo categories: In-Context Learning (ICL) based methods and Supervised Finetuning (SFT) based methods. The\nICL-based IE methods~\\cite{codeie, guo2023retrieval, ashok2023promptner,\nwang2023gpt} make predictions only based on contexts augmented with a few\nexamples. The SFT-based methods~\\cite{wangdeepstruct, gui2023instructie,\nwang2023instructuie, zhou2023universalner, xu2023large, sainz2023gollie} use the\nannotated data to finetune LLMs.\n\nSome existing work uses code-style prompts to conduct IE. Most of them are\nICL-based methods. ~\\citet{wang2022code4struct} uses the code-style prompt to\nconduct event argument extraction. ~\\citet{codeie} uses the code-style prompt to\nconduct the named entity extraction and relation extraction.\n~\\cite{guo2023retrieval} proposes a reterive-argumented method to conduct the\nuniversal IE. These methods show relatively poor performance compared to\nSFT-based methods because of the lack of training to follow the schemas in the\nprompt. The most similar work with KnowCoder is GoLLIE, an SFT-based UIE method\nthat gives out definitions of schemas as code comments. The difference between\nKnowCoder and GoLLIE is that KnowCoder designs a more comprehensive code-style\nschema representation method, including taxonomies, constraints, and class\nmethods, and further constructs a large-scale schema library. Besides, GoLLIE\nconducts instruction tuning on human-annotated data, while KnowCoder contains a\ntwo-phase learning framework that enhances schema understanding and following\nability via automatically annotated data. \n\n\\section*{Conclusion}\n\n  In this paper, we introduced KnowCoder for UIE leveraging Large Language\n  Models. KnowCoder is based on a code-style schema representation method and\n  an effective two-phase learning framework. The code-style schema representation\n  method uniformly transforms different schemas into Python classes, with which\n  the UIE task can be converted to a code generation process. Based on the schema representation\n  method, we constructed a comprehensive code-style schema library covering over $30,000$\n  types of knowledge. To let LLMs understand and follow these schemas, we\n  further proposed a two-phase learning framework that first enhances the\n  schema comprehension ability and then boosts its schema following ability. After training on billions of automatically annotated data and refining with human-annotated IE datasets, KnowCoder demonstrates remarkable performance improvements on different IE tasks under the various evalution settings.\n\n\\section*{Limitations}\nThe schemas utilized in our approach are predominantly constructed from Wikidata, which occasionally results in some schemas lacking definitions or other relevant information. This necessitates the generation of additional data to supplement these missing elements. During the pretraining phase, we adopted a combination of automatic generation and distant supervision methods to amass a large corpus. However, this approach inevitably introduces a certain degree of noise. Furthermore, there remains room for improvement in terms of the richness and complexity of the current corpus. Further exploration of pretraining settings could also be beneficial in enhancing the zero-shot capabilities for relation and event-related tasks.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2101.05779v3.tex",
        "arXiv-2210.10343v2.tex",
        "arXiv-2403.07969v2.tex"
    ],
    "group_id": "group_76",
    "response": "### Title: Advances in Structured Prediction and Data Augmentation Techniques in Natural Language Processing\n\n### Introduction\n\nStructured prediction in Natural Language Processing (NLP) is a broad category of tasks that involve inferring structured outputs, such as graphs, from input sentences. This includes tasks like joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Traditionally, these tasks have been approached using task-specific discriminative classifiers, which often require significant customization for each task and can struggle with leveraging semantic knowledge inherent in pre-trained language models. Additionally, the scarcity of labeled data in NLP tasks poses a challenge, often necessitating data augmentation techniques to improve model performance, especially in low-resource scenarios.\n\nData augmentation in NLP has seen various methods, ranging from rule-based techniques that manipulate words in the original text to generative models that create semantically coherent sentences. However, these methods often fail to maintain semantic coherence or preserve entities, particularly in nested and discontinuous NER tasks. The field has also seen the emergence of unified frameworks that attempt to handle multiple NER tasks with a single model, but these frameworks have limitations in terms of schema representation and learning.\n\nThis summary will cover three recent papers that address these challenges by proposing novel approaches to structured prediction and data augmentation. The first paper introduces Translation between Augmented Natural Languages (TANL), a unified framework for solving various structured prediction tasks. The second paper presents Entity-to-Text (ET2T) based data augmentation for NER tasks, which maintains semantic coherence and entity preservation. The third paper introduces KnowCoder, a Large Language Model (LLM) that uses code generation to conduct Universal Information Extraction (UIE) tasks.\n\n### Main Content of Each Paper\n\n#### Paper 1: Translation between Augmented Natural Languages (TANL)\n\nTANL proposes a novel framework for structured prediction tasks by framing them as translation tasks between augmented natural languages. The approach leverages pre-trained transformer models like T5 to translate between input and output sentences that are annotated in a specific format. This format includes special tokens to mark entities and relations, allowing the model to easily extract structured information from the output text. The framework is designed to handle nested entities and multiple relations, which are common challenges in structured prediction tasks. \n\nThe paper demonstrates that TANL can achieve state-of-the-art (SOTA) performance on several datasets, including CoNLL04, ADE, NYT, ACE2005, FewRel, TACRED, CoNLL-2005, and CoNLL-2012. It also shows that TANL can perform multi-task learning, training a single model to solve all tasks simultaneously. This is achieved without any additional task-specific modules, making the framework highly adaptable and efficient. The authors also introduce a decoding procedure that uses dynamic programming (DP) alignment to match the generated structured information with the original input sentence, improving robustness against imperfect generation.\n\n#### Paper 2: Entity-to-Text Based Data Augmentation for Various Named Entity Recognition Tasks\n\nThis paper introduces Entity-to-Text (ET2T), a data augmentation technique specifically designed for NER tasks. ET2T aims to maintain semantic coherence and preserve entities during the augmentation process, addressing the limitations of previous methods. The technique involves modifying entity lists via operations like adding, deleting, replacing, and swapping entities, and then generating semantically coherent texts based on these modified lists. To enhance diversity, the authors propose a diversity beam search (DBS) method, which injects a penalty to encourage the generation of diverse texts.\n\nET2T is evaluated on thirteen NER datasets across three tasks (flat, nested, and discontinuous NER) and two settings (full data and low resource). The paper demonstrates that ET2T outperforms existing augmentation methods, especially in low-resource scenarios. The authors use two backbone models: the unified Seq2Seq framework and the unified Word-Word framework, both of which are capable of handling all three NER tasks. ET2T achieves an average F1 improvement of 0.75% across all datasets under the full data setting and up to 2.97% under the low resource setting.\n\n#### Paper 3: KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction\n\nKnowCoder is a Large Language Model (LLM) designed to perform Universal Information Extraction (UIE) tasks, such as NER, Relation Extraction (RE), Event Detection (ED), and Event Argument Extraction (EAE). The model uses a code-style schema representation method to uniformly define different schemas as Python classes, which include features like class inheritance, type hints, and class methods. This representation method allows KnowCoder to capture complex schema information, such as taxonomies and constraints, in a manner that is easier for LLMs to understand.\n\nKnowCoder's learning framework consists of two phases: schema understanding and schema following. The schema understanding phase involves code pretraining on a large-scale dataset, while the schema following phase uses instruction tuning to enhance the model's ability to follow specific schemas. The model is further refined using human-annotated datasets, leading to significant performance improvements across various IE tasks. KnowCoder demonstrates strong generalization ability under zero-shot and low-resource settings, outperforming existing baselines by up to 12.5% and 21.9%, respectively. The model also shows competitive performance in supervised settings, surpassing smaller models like UIE in more challenging tasks such as ED and EAE.\n\n### Commonalities and Innovations\n\nAll three papers address the challenge of handling structured prediction tasks in NLP. They aim to improve model performance by leveraging pre-trained language models and innovative schema representation methods. TANL and KnowCoder both use pre-trained transformers to generate structured outputs, while ET2T uses pre-trained language models to generate semantically coherent texts for data augmentation.\n\nThe key innovation in TANL is the translation framework, which allows the model to handle various structured prediction tasks with a single architecture. This framework uses augmented natural languages to encode structured information, enabling the model to leverage semantic knowledge and perform multi-task learning effectively. ET2T introduces a novel data augmentation method that maintains semantic coherence and entity preservation, enhancing model performance in low-resource scenarios. KnowCoder proposes a code-style schema representation method and a two-phase learning framework to improve the generalization and performance of LLMs in UIE tasks.\n\n### Comparison of Results\n\nThe papers present different evaluation metrics and datasets, making direct comparison challenging. However, they all demonstrate significant improvements over existing baselines.\n\n- **TANL**: Achieves SOTA results on CoNLL04, ADE, NYT, and ACE2005 datasets for joint entity and relation extraction, FewRel and TACRED for relation classification, and CoNLL-2005 and CoNLL-2012 for semantic role labeling. The model also shows comparable or better performance in multi-task learning scenarios.\n  \n- **ET2T**: Outperforms existing augmentation methods on thirteen NER datasets, achieving an average F1 improvement of 0.75% under the full data setting and up to 2.97% under the low resource setting. The method is particularly effective in low-resource scenarios, where it can significantly improve performance even with limited training data.\n  \n- **KnowCoder**: Demonstrates strong generalization ability under zero-shot and low-resource settings, outperforming existing baselines by up to 12.5% and 21.9%, respectively. In supervised settings, KnowCoder surpasses smaller models like UIE in more challenging tasks such as ED and EAE, achieving up to 7.5% relative improvements.\n\n### Conclusion\n\nThe three papers present innovative approaches to structured prediction and data augmentation in NLP. TANL introduces a unified translation framework that leverages semantic knowledge and performs multi-task learning effectively. ET2T proposes a data augmentation method that maintains semantic coherence and entity preservation, improving model performance in low-resource scenarios. KnowCoder uses a code-style schema representation and a two-phase learning framework to enhance the generalization and performance of LLMs in UIE tasks.\n\nThese approaches highlight the potential of pre-trained language models in handling structured prediction tasks and demonstrate the importance of schema representation and learning frameworks in improving model performance. Future research could explore the integration of these methods to further enhance the capabilities of LLMs in structured prediction tasks, particularly in low-resource settings. Additionally, the development of more comprehensive and domain-specific schema libraries could improve the performance of KnowCoder and similar models in more specialized IE tasks.\n\n### Future Research Directions\n\n1. **Integration of Methods**: Future work could integrate the approaches from TANL and ET2T into KnowCoder to enhance its performance in both structured prediction and data augmentation.\n2. **Domain-Specific Schema Libraries**: Developing more comprehensive and domain-specific schema libraries could improve the performance of KnowCoder and similar models in specialized IE tasks.\n3. **Efficiency Improvements**: Investigating methods to reduce the time complexity of token generation in TANL, such as using advanced attention mechanisms like Reformer or Linformer, could make the framework more scalable.\n4. **Transfer Learning**: Exploring the transfer learning capabilities of TANL and ET2T to adapt models trained on one task to perform well on related tasks could further streamline the training process.\n5. **Evaluation Metrics**: Developing more robust evaluation metrics for generative models in structured prediction tasks could provide a clearer understanding of their performance.\n6. **Real-World Applications**: Applying these methods to real-world applications, such as medical text analysis or legal document processing, could demonstrate their practical utility and potential for further improvement.\n\nIn summary, these papers contribute significantly to the field of structured prediction in NLP by proposing innovative frameworks and techniques that improve model performance and adaptability."
}