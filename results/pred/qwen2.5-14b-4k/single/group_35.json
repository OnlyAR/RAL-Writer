{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Query Rewriting for Retrieval-Augmented Large Language Models}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nLarge Language Models (LLMs) play powerful, black-box readers in the \\textit{retrieve-then-read} pipeline, making remarkable progress in knowledge-intensive tasks.\nThis work introduces a new framework, \\textit{Rewrite-Retrieve-Read} instead of the previous \\textit{retrieve-then-read} for the retrieval-augmented LLMs from the perspective of the query rewriting.\nUnlike prior studies focusing on adapting either the retriever or the reader, our approach pays attention to the adaptation of the search query itself, for there is inevitably a gap between the input text and the needed knowledge in retrieval.\nWe first prompt an LLM to generate the query, then use a web search engine to retrieve contexts.\nFurthermore, to better align the query to the frozen modules, we propose a trainable scheme for our pipeline.\nA small language model is adopted as a trainable rewriter to cater to the black-box LLM reader.\nThe rewriter is trained using the feedback of the LLM reader by reinforcement learning.\nEvaluation is conducted on downstream tasks, open-domain QA and multiple-choice QA. \nExperiments results show consistent performance improvement, indicating that our framework is proven effective and scalable, and brings a new framework for retrieval-augmented LLM \\footnote{https://github.com/xbmxb/RAG-query-rewriting}.\n\\end{abstract}\n\n\\section{Introduction}\nLarge Language Models (LLMs) have shown remarkable abilities for human language processing and extraordinary scalability and adaptability in few- or zero-shot settings.\\cite{ouyang2022training, brown2020language, chowdhery2022palm}.\nHowever, the training process depends on large-scale high-quality corpora but without the perception of the real world. Thus, LLMs still have to face the issue of hallucination \\cite{yao2023react, bang2023multitask} and temporal misalignment \\cite{rottger-pierrehumbert-2021-temporal-adaptation, luu-etal-2022-time, jang2022temporalwiki}.\nThis affects the reliability of LLMs and hinders wider practical application, because the consistency between the LLM responses with the real world needs further validation.\nExisting work has proved that incorporating external knowledge (i.e., non-parametric knowledge) with internal knowledge (i.e., parametric knowledge) can effectively alleviate hallucination, especially for knowledge-intensive tasks. \nIn fact, retrieval-augmented LLMs have been shown so effective that they have been regarded as a standard solution to alleviate the factuality drawbacks in naive LLM generations. \nRetrieval augmentation is applied to select relative passages as external contexts for the language model, which is \\textit{retrieve-then-read} framework  \\cite{lewis2020rag, karpukhin-etal-2020-dense, atlas_few-shot_2022}.\nTake the open-domain Question-Answering task (open-domain QA) as an example, a retriever first searches for related documents for a question. Then the LLM receives the question and the documents, then predicts an answer.\n\nAs most LLMs are only accessible through inference APIs, they play the part of black-box frozen readers in the pipeline.\nThis makes previous retrieval augmentation methods that require complete access \\cite{lewis2020rag, guu2020realm, atlas_few-shot_2022} no longer feasible.\nRecent studies on retrieval-augmented language models lean more on the LLM-oriented adaptation.\nAn idea is to train a dense retrieval model to cater to the frozen language model \\cite{shi2023replug}.\nBy using feedback from the LLM as a training objective, the retrieval model is tuned for better LLM input contexts. \nAnother research line focuses on the design of interactions between the retriever and the reader \\cite{yao2023react, khattab2022dsp}, where both the retriever and the reader are usually frozen.\nThe idea is to trigger the emergent ability through carefully crafted prompts or a sophisticated prompt pipeline. \nMultiple interactions with external knowledge allow the LLM to approach the correct answer step by step.\n\nHowever, there are still problems remaining to be solved.\nExisting approaches overlook the adaptation of the query, i.e., the input of the \\textit{retrieve-then-read} pipeline. \nThe retrieval query is either original from datasets or directly determined by the black-box generation, thus is always fixed. \nHowever, there is inevitably a gap between the input text and the knowledge that is really needed to query.\nThis limits performance and places a burden on retrieval capability enhancement and prompt engineering.\n\nIn consideration of this issue, this paper proposes \\textit{Rewrite-Retrieve-Read}, a new framework for retrieval augmentation, which can be further tuned for adapting to LLMs.\nIn front of the retriever, a step of \\textit{rewriting the input} is added, filling the gap between the given input and retrieval need, as is shown in Figure \\ref{overview}.\nWe adopt the off-the-shelf tool, an internet search engine, as the retriever, which avoids the maintenance of the search index and can access up-to-date knowledge \\cite{lazaridou2022internet}.\nDifferent from previous studies \\cite{khattab2022dsp, yao2023react} that require the memory of multiple interaction rounds between the retriever and the LLM for each sample,   \nthe motivation of our rewriting step is to clarify the retrieval need from the input text. \n\nWe also propose a trainable scheme for our \\textit{rewrite-retrieve-read} framework (Figure \\ref{overview} (c)).\nThe black-box retriever and the reader form a frozen system.\nTo further smooth the steps of our pipeline,\nwe apply a small, trainable language model to perform the rewriting step, denoted as the \\textit{rewriter}.\nThe rewriter is trained by reinforcement learning using the LLM performance as a reward, learning to adapt the retrieval query to improve the reader on downstream tasks. \n\nOur proposed methods are evaluated on knowledge-intensive downstream tasks including open-domain QA (HotpoQA \\cite{yang-etal-2018-hotpotqa}, AmbigNQ \\cite{min2020ambigqa}, PopQA \\cite{mallen2023llm_memorization}) and multiple choice QA (MMLU \\cite{hendryckstest2021mmlu}). \nThe experiments are implemented on T5-large \\cite{2020t5} as the rewriter, ChatGPT \\cite{ouyang2022training} and Vicuna-13B \\cite{vicuna2023} as the LLM reader. \nThe results show that query rewriting consistently improves the retrieve-augmented LLM performance. \nThe results also indicate that the smaller language model can be competent for query rewriting.\n\nTo sum up, our proposed novel retrieval-augmentation method, \\textit{rewrite-retrieve-read} is the first framework where the input text is adapted for the frozen retriever and LLM reader. \nWe introduce a tuneable scheme with a small, trainable model, achieving performance gains with less resource consumption. \n\n\\section{Related Work}\n\\subsection{Retrieval Augmentation}\nLanguage models require external knowledge to alleviate the factuality drawbacks.\nRetrieval augmentation has been regarded as the standard effective solution. \nWith a retrieval module, related passages are provided to the language model as the context of the original input. \nThus factual information like common sense or real-time news helps with output prediction through contextualized reading comprehension. \n\nEarlier studies use sparse retriever \\cite{chen2017reading} or dense retriever \\cite{karpukhin-etal-2020-dense} in front of a pre-trained language model (PrLM).\nThe neural retriever and reader are both PrLMs of trainable size like BERT \\cite{bert2019Devlin} or BART \\cite{bart2020Lewis}.\nHence, the whole \\textit{retrieve-then-reader} framework is a tuneable end-to-end system, where the retrieved contexts can be regarded as the intermediate results \\cite{karpukhin-etal-2020-dense, lewis2020rag}.\nApproaches to smooth the two-step framework are proposed to optimize the retrieval and the reading comprehension \\cite{emdr22021SachanRHDY21, lee-etal-2022-need, jiang-etal-2022-reatt}.\nMore recently, retrieval remains a powerful enhancement as the size of models and data scales rapidly \\cite{mallen2023llm_memorization, shi2023replug, brown2020language}.\nOn the other hand, retrieval enhancement can compensate for the shortfall in parameter size, compared to large-scale language models.\nFor example,\nby jointly training the retriever and the reader, Atlas \\cite{atlas_few-shot_2022} shows few-shot performance on par with 540B PalM \\cite{chowdhery2022palm} but be of 50$\\times$ smaller size.\n\n\\begin{figure*}[t]\n\t\t\\centering\n\t\t\\includegraphics[width=0.9\\textwidth]{figs/f1-crop.pdf}\n\t\t\\caption{\\label{overview} Overview of our proposed pipeline. From left to right, we show (a) standard \\textit{retrieve-then-read} method, (b) LLM as a query rewriter for our \\textit{rewrite-retrieve-read} pipeline, and (c) our pipeline with a trainable rewriter. }\n\\end{figure*}\n\n\\noindent \\textbf{The Internet as a knowledge base} \\quad\nMore related to our work, the search engine can assume the role of the retriever and use the Internet as the source of external knowledge.\n\\citet{komeili-etal-2022-internet} use an internet search for relevant information based on the dialogue history to perform dialogue response generation.\nSeeKeR \\cite{DBLP:conf/emnlp/0001KARSW22} use a single Transformer to iteratively perform search query generation, then knowledge extraction for dialogue generation and sentence completion.\nFor large-scale models, web search still shows effective for knowledge augmentation \\cite{lazaridou2022internet}, fact-checking \\cite{menick2022teaching}, and LLM agent enhancement \\cite{yao2023react}.\n\n\\subsection{Cooperation with Black-box LLMs}\nLarge Language Models, such as ChatGPT \\cite{ouyang2022training}, Codex \\cite{codex2021chen}, PaLM \\cite{chowdhery2022palm}, emerge impressive natural language processing ability as well as remarkable scalability.\nThis leads to a tendency to embrace LLMs on a wide range of NLP tasks.\nHowever, LLMs are only accessible as a black box in most cases, which is because\n(i) Some like ChatGPT are not open-source and kept private;\n(ii) The large parameter scale requires computational resources that are not always affordable to users.\nThis constraint means nothing is available except input and output texts.\n\nExisting studies have proved that LLMs' abilities can be better leveraged by carefully designed interaction methods.\nGenRead \\cite{yu2023generate} prompts an LLM to generate context instead of deploying a retriever, showing that LLMs can retrieve internal knowledge by prompting.\nReAct \\cite{yao2023react} and Self-Ask \\cite{press2022measuring} combines the Chain-of-Thought (CoT) \\cite{cot/nips/Wei0SBIXCLZ22, selfconsistency2022wang} and inter-actions with web APIs. Only relying on prompt construction, ReAct provides novel baselines for interactive tasks.\nDemonstrate–Search–Predict (DSP) \\cite{khattab2022dsp} defines a sophisticated pipeline between an LLM and a retriever. Unlike ReAct, DSP integrates prompts for demonstration bootstrap besides multi-hop breakdown and retrieval.\n\nDespite the promising performance in the zero or few-shot setting, the behavior of LLMs sometimes needs adjustments.\nA feasible approach is to append trainable small models in front of or after the LLM. \nThe small models, as a part of the parameters of the system, can be fine-tuned for optimization. \nRePlug \\cite{shi2023replug} is proposed to fine-tune a dense retriever for the frozen LLM in the \\textit{retrieve-then-read} pipeline. The retriever is trained under the LLM's supervision to retrieve documents that are suitable for the LLM. \nWith the same purpose, Directional Stimulus Prompting \\cite{li2023guiding} deploys a small model to provide the LLM with stimulus (e.g., keywords for summarization, or dialogue actions for response generation), which is updated according to the LLM reward.\n\nDifferent from the inspiring work mentioned above, our proposed pipeline contains a query rewriting step in front of the \\textit{retrieve-then-read} module. We further propose a trainable scheme with a small rewriting model, which is a novel enhancement for retrieval-augmented LLM by reconstructing the search query.\n\n\\section{Methodology}\nWe present \\textit{Rewrite-Retrieve-Read}, a pipeline that improves the retrieval-augmented LLM from the perspective of query rewriting. Figure \\ref{overview} shows an overview.\nThis section first introduces the pipeline framework in section \\ref{fw}, then the trainable scheme in section \\ref{ts}.\n\n\\subsection{\\textit{Rewrite-Retrieve-Read}} \\label{fw}\nA task with retrieval augmentation can be denoted as follows. \nGiven a dataset of a knowledge-intensive task (e.g., open-domain QA), $D = \\{(x, y)_i\\}, i = 0,1,2, \\dots, N$, $x$ (e.g., a question) is the input to the pipeline, $y$ is the expected output (e.g., the correct answer). \nOur pipeline consists of three steps. \n(\\romannumeral1) Query rewrite: generate a query $\\tilde{x}$ for required knowledge based on the original input $x$. \n(\\romannumeral2) Retrieve: search for related context, $doc$.\n(\\romannumeral3) Read: comprehend the input along with contexts $[doc, x]$ and predict the output $\\hat{y}$.\n\nA straightforward but effective method is to ask an LLM to rewrite queries to search for information that is potentially needed.\nWe use a few-shot prompt to encourage the LLM to think, and the output can be none, one or more queries to search.\n\n\\subsection{Trainable Scheme} \\label{ts}\nBesides, total reliance on a frozen LLM has shown some drawbacks. \nReasoning errors or invalid search hinders the performance \\cite{yao2023react, behnamghader2022can}.\nOn the other hand, retrieved knowledge may sometimes mislead and compromise the language model \\cite{mallen2023llm_memorization}.\nTo better align to the frozen modules, it is feasible to add a trainable model and adapt it by taking the LLM reader feedback as a reward.\n\nBased on our framework, we further propose to utilize a trainable small language model to take over the rewriting step, as is shown in the right part of Figure \\ref{overview}. \nThe trainable model is initialized with the pre-trained T5-large (770M) \\cite{2020t5}, denoted as \\textit{trainable rewriter}, $G_{\\theta}$.\nThe rewriter is first trained on pseudo data to warm up (\\S \\ref{warmup}), then continually trained by reinforcement learning (\\S \\ref{rein}).\n\n\\subsubsection{Rewriter Warm-up}\\label{warmup}\nThe task, query rewriting, is quite different from the pre-training objective of sequence-to-sequence generative models like T5.\nFirst, we construct a pseudo dataset for the query rewriting task.\nInspired by recent distillation methods \\cite{Hsieh2023DistillingSO,ho2022large}, we prompt the LLM to rewrite the original questions $x$ in the training set and collect the generated queries $\\tilde{x}$ as pseudo labels. \nThe collected samples are then filtered: Those that get correct predictions from the LLM reader are selected into the warm-up dataset, denoted as $D_{Train} = \\{(x, \\tilde{x}) | \\hat{y} = y\\}$. \nThe rewriter $G_{\\theta}$ is fine-tuned on $D_{Train}$ with the standard log-likelihood as the training objective, denoted as\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{5pt}\n\\setlength{\\belowdisplayskip}{5pt}\n\\begin{split}\n& \\mathcal{L}_{warm} = - \\sum_{t} logp_{\\theta} \\textup{(} \\textit{ $\\hat{\\tilde{x}}$}_{t} \\mid \\textit{ $\\tilde{x}$}_{<t} \\textup{, } \\textit{x} \\textup{ )}.\n\\end{split}\n\\label{loss}\n\\end{equation}\n\nThe rewriter model after warm-up shows modest performance, which depends on the pseudo data quality and rewriter capability. \nHighly relying on the human-written prompt line, $\\tilde{x}$ can be sub-optimal. \nThe relatively small scale of the rewriter size is also a limitation of the performance after the warm-up.\nThen we turn to reinforcement learning to align the rewriter to the following retriever and LLM reader.\n\n\\subsubsection{Reinforcement Learning}\\label{rein}\nTo further fine-tune the rewriter to cater to the LLM reader, we adopt a policy gradient reinforcement learning framework. \n\n\\noindent \\textbf{Task Formulation} \\quad\nIn the context of reinforcement learning, the rewriter optimization is formulated as a Markov Decision Process 5-tuple $\\langle\\mathcal{S}, \\mathcal{A}, P, R, \\gamma \\rangle$. \n(\\romannumeral1) The state space $\\mathcal{S}$ is a finite set limited by the vocabulary and the sequence length. \n(\\romannumeral2) The action space $\\mathcal{A}$ is equals to the vocabulary.\n(\\romannumeral3) The transition probability $P$ is determined by the policy network, which is the rewriter model $G_{\\theta}$.\n(\\romannumeral4) The reward function $R$ gives a reward value that depends on the current state. The policy gradient is derived from rewards, used as the training objective.\n(\\romannumeral5) $\\gamma$ denotes the discount factor.\nMore specifically, the rewriter $G_{\\theta}$ after the warm-up is the initial policy model $\\pi_{0}$. \nAt each step $t$, the action $a_t$ is to generate the next token $\\hat{\\tilde{x}}_{t}$ based on the observation of the present state, $s_t = [x, \\hat{\\tilde{x}}_{<t}]$. \nWhen the generation is stopped by the End-Of-Sentence token, one episode is ended. \nAfter finishing the retrieval and reading, a reward is computed by evaluating the final output, i.e., a score for the LLM reader prediction.\n\n\\noindent \\textbf{Policy Optimization} \\quad\nWe adopt Proximal Policy Optimization (PPO) \\cite{schulman2017ppo}, following \\cite{Ramamurthy2022IsRL}. \nMaximization of the expectation of the reward $R$ is formulated as\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{5pt}\n\\setlength{\\belowdisplayskip}{5pt}\n\\begin{split}\n&\\max _{\\theta} \\mathbb{E}_{\\hat{\\tilde{x}} \\sim p_{\\theta}(\\cdot \\mid x)}[R(x,\\hat{\\tilde{x}})],\\\\\n&\\max _{\\theta} \\mathbb{E}_{\\left(s_t, a_t\\right) \\sim \\pi_{\\theta^{\\prime}}}[min\\{k_{t, \\theta} A^{\\theta^{\\prime}}\\left(s_t, a_t\\right);\\\\\n&\\quad \\quad \\operatorname{clip}\\left(k_{t, \\theta}, 1-\\varepsilon, 1+\\varepsilon\\right) A^{\\theta^{\\prime}}\\left(s_t, a_t\\right)\\}], \\\\\n& k_{t, \\theta} = \\frac{p_\\theta\\left(a_t \\mid s_t\\right)}{p_{\\theta^{\\prime}}\\left(a_t \\mid s_t\\right)},\\\\\n\\end{split}\n\\end{equation}\nwhere $\\theta^{\\prime}$ is the temporarily fixed policy for sampling and $\\theta$ is updated.\n$A$ denotes the advantage function, which is formulated based on the estimation of value network $V_{\\phi}$.\nThe value network $V_{\\phi}$ is initialized from the policy network $\\pi_{0}$. \nThe formulation follows Generalized Advantage Estimation (GAE) \\cite{schulman2015gae}.\n\\begin{equation}\n\\begin{split}\n\\setlength{\\abovedisplayskip}{5pt}\n\\setlength{\\belowdisplayskip}{5pt}\n&\\delta_t=R\\left(s_t, a_t\\right)+V_\\phi\\left(s_{t+1}\\right)-V_\\phi\\left(s_t\\right), \\\\\n&\\hat{A}^{\\theta}_{t}\\left(s_t, a_t\\right)=\\sum_{t^{\\prime}=0}^{\\infty} \\lambda^{t^{\\prime}} \\delta_{t+t^{\\prime}},\n\\end{split}\n\\end{equation}\nwhere $\\lambda$ is the bias-variance trade-off parameter.\n\nThe reward function $R$ reflects the quality of the generated queries, which needs to be consistent with the final evaluation of the task. %, denoted as $R_{lm}$. \n$\\hat{\\tilde{x}}$ is fed to the retriever and the reader for a final prediction $\\hat{y}$. \nA part of the reward function is the measures of $\\hat{y}$ compared to the golden label $y$ (e.g., exact match and F$_{1}$ of the predicted answers), denoted as $R_{lm}$. \nBesides, a KL-divergence regularization is added to prevent the model from deviating too far from the initialization \\cite{Ramamurthy2022IsRL, ziegler2019fine}.\n\\begin{equation}\n\\begin{split}\n\\setlength{\\abovedisplayskip}{5pt}\n\\setlength{\\belowdisplayskip}{5pt}\n&R\\left(s_t, a_t\\right)=R_{lm}(\\hat{\\tilde{x}}, y)-\\beta \\mathrm{KL}\\left(\\pi_\\theta \\| \\pi_0\\right).\n\\end{split}\n\\label{beta}\n\\end{equation}\nThe final loss function is composed of policy loss and value loss.\n\\begin{equation}\n\\begin{split}\n\\setlength{\\abovedisplayskip}{5pt}\n\\setlength{\\belowdisplayskip}{5pt}\n&\\mathcal{L}_{\\theta}= - \\frac{1}{\\left|\\mathcal{S}\\right| T} \\sum_{\\tau \\in \\mathcal{S}} \\sum_{t=0}^T \\min (k_{t,\\theta} A^{{\\theta}^{\\prime}}, \\operatorname{clip}A^{{\\theta}^{\\prime}}),\\\\\n&\\mathcal{L}_{\\phi}= \\frac{1}{\\left|\\mathcal{S}\\right| T} \\sum_{\\tau \\in \\mathcal{S}} \\sum_{t=0}^T\\left(V_\\phi\\left(s_t\\right)-R_t\\right)^2, \\\\\n&\\mathcal{L}_{ppo} = \\mathcal{L}_{\\theta} + \\lambda_{v} \\mathcal{L}_{\\phi}.\n\\end{split}\n\\end{equation}\nHere, $\\mathcal{S}$ denotes the sampled set, and $T$ is for step numbers.\n\n\\section{Implementation}\n\\noindent \\textbf{Rewriter} \\quad\nFor the frozen pipeline in \\S \\ref{fw}, we prompt an LLM to rewrite the query with few-shot in-context learning \\cite{brown2020language,min-etal-2022-rethinking}.\nOur prompt follows the formulation of \\textit{[instruction, demonstrations, input]}, where the input is $x$.\nThe instruction is straightforward and demonstrations are 1-3 random examples from training sets and are kept constant across all runs, mainly for the task-specific output format illustration, i.e., a short phrase as an answer for HotpotQA, and an option as an answer for MMLU.\nFor the training scheme in \\S \\ref{ts}, we fine-tuning a T5 as the rewriter.\n\n\\noindent \\textbf{Retriever} \\quad \nWe use the Bing search engine as the retriever. \nIt requires no candidate index construction like a dense retriever, nor candidates like a textbook. \nBut it allows for a wide knowledge scope and up-to-time factuality.\nWith Bing API, the retrieval is performed in two approaches.\n(\\romannumeral1) For all retrieved web pages, we concatenate the snippets that are related sentences selected by Bing. \nThis method is similar to using a search engine in a browser, input a query and press Enter, then collect the texts shown on the search result page.\n(\\romannumeral2) For retrieved web pages, we request the URLs and parser to get all the texts. This is similar to clicking on items on the search result page. \nThen we use BM25 to keep those with higher relevance scores with the query, reducing the document length.\n\n\\noindent \\textbf{Reader} \\quad \nThe reader is a frozen LLM, where we adopt ChatGPT (gpt-3.5-turbo) and Vicuna-13B.\nIt performs reading comprehension and prediction with few-shot in-context learning.\nIn our prompt, following the brief instruction and the demonstrations, the input is $x$ or $[doc, \\hat{\\tilde{x}}]$ with retrieval augmentation.\n\nIt has been proved that both the phrasing of prompt lines \\cite{zhang2023tempera} and the selection of demonstrations show effects on the in-context learning performance \\cite{su2022selective, zhang2023autocot}.\nAs it is not the focus of this work, we pay no more attention to prompt editing.\n\n\\section{Experiments}\n\\subsection{Task Settings}\n\\begin{table*}[tbh]\n\t\\centering\\small\n        \\resizebox{\\linewidth}{!}{\n\t\\begin{tabularx}{0.9\\linewidth}{X}\n\t\t\\toprule\n\t\t\\textbf{Direct prompt}   \\\\\n             \\midrule\n\t\t Answer the question in the following format, end the answer with '**'. \\{demonstration\\} Question: \\{$x$\\} Answer:\n\t\t\\\\\n\t\t\\midrule\n\t\t\\textbf{Reader prompt in retrieval-augment pipelines} \\\\\n            \\midrule\n            Answer the question in the following format, end the answer with '**'. \\{demonstration\\} Question: \\{$doc$\\} \\{$x$\\} Answer: \\\\\n            \\midrule\n            \\textbf{Prompts for LLM as a frozen rewriter}\\\\\n            \\midrule\n            \\textit{Open-domain QA: }Think step by step to answer this question, and provide search engine queries for knowledge that you need. Split the queries with ';' and end the queries with '**'. \\{demonstration\\} Question: \\{$x$\\} Answer: \\\\\n            \\textit{Multiple choice QA: }Provide a better search query for web search engine to answer the given question, end the queries with '**'. \\{demonstration\\} Question: \\{$x$\\} Answer: \\\\\n\t\t\\bottomrule\n\t\\end{tabularx}}\n\t\\caption{Prompt lines used for the LLMs.}\n        \\label{prompt}\n        \\vspace{-1.2em}\n\\end{table*}%\n\\subsubsection{Open-domain QA}\nThree open-domain QA datasets are used for evaluation.\n(\\romannumeral1) HotPotQA \\cite{yang-etal-2018-hotpotqa} consists of complex questions that require multi-hop reasoning. We evaluate the full test set.\n(\\romannumeral2) AmbigNQ \\cite{min2020ambigqa} provides a disambiguated version of Natural Questions (NQ) \\cite{kwiatkowski2019natural}. For ambiguous questions in NQ, minimal constraints are added to break it into several similar but specific questions. The first 1000 samples are evaluated in the test set.\n(\\romannumeral3) PopQA \\cite{mallen2023llm_memorization} includes long-tail distributions as it contains more low-popularity knowledge than other popular QA tasks. \nWe split the dataset into 13k for training and 714 for testing.\n\nOpen-domain QA benchmarks are sets of question-answer pairs denoted as $\\{(q, a)_i\\}$.\nWe use ChatGPT for both the reader and the frozen rewriter.\nThe evaluation metrics are Exact Match ($EM$) and $F_{1}$ scores.\nFor the reward function in RL, we use an indicator to reward if the retrieved content hits the answer and penalize if misses the answer, denoted as $Hit$.\nThe total reward is a weighted sum of EM, F$_{1}$, and $Hit$.\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{6pt}\n\\setlength{\\belowdisplayskip}{6pt}\n\\begin{split}\n&Hit= \\begin{cases}1 & a \\enspace \\text{in} \\enspace doc, \\\\ -1 & else\\end{cases} \\\\\n&R_{lm} = EM + \\lambda_{f} {F}_{1} + \\lambda_{h} Hit.\n\\end{split}\n\\label{hit}\n\\end{equation}\n\n\\subsubsection{Multiple-choice QA}\n\nFor multiple-choice QA, our evaluation is conducted on\nMassive Multi-task Language Understanding (MMLU) \\cite{hendryckstest2021mmlu}, an exam question dataset including 4 categories: Humanities, STEM, Social Sciences, and Other. \nEach category is split into 80\\% for the training set and 20\\% for the test set.\n\nMultiple-choice QA can be formulated as $\\{(q^{\\prime}, a)_i\\}$, where $q^{\\prime} = [q, c_0, c_1, c_2, c_3]$. $c$ denotes the options, generally there are four for each question. \nThe retrieved documents that are included in the officially provided contaminated lists are ignored.\nThe questions with options are rewritten into search queries. The answer is one option. $EM$ is reported as metrics and used for the reward.\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{5pt}\n\\setlength{\\belowdisplayskip}{5pt}\n\\begin{split}\n&R_{lm} = EM.\n\\end{split}\n\\end{equation}\nWe use ChatGPT as a frozen rewriter and the reader. We also use Vicuna-13B as the reader for evaluation due to the rate limit issue of ChatGPT. \nMore information on datasets and training setup are presented in the appendix.\n\n\\subsection{Baselines}\nThe following settings are implemented to evaluate and support our methods.\n(\\romannumeral1) \\textbf{Direct}:\nThe standard in-context learning without any augmentations. \n(\\romannumeral2) \\textbf{Retrieve-then-read}:\nThe standard retrieval-augmented method. Retrieved documents are concatenated with the question.\n(\\romannumeral3) \\textbf{LLM as a frozen rewriter}:\nAs is introduced in \\S \\ref{fw}, we prompt a frozen LLM to reason and generate queries by few-shot in-context learning. \n(\\romannumeral4) \\textbf{Trainable rewriter}:\nApplying the fine-tuned rewriter, the output queries are used by the retriever and the reader. \nTable \\ref{prompt} presents prompt line forms. Please note that the prompts for prediction are kept the same for each task.\n\n\\subsection{Results}\nExperimental results on open-domain QA are reported in Table \\ref{mainqa}.\nFor the three datasets, query rewriting consistently brings performance gain with both a frozen rewriter and a trainable rewriter.\nOn AmbigNQ and PopQA, the standard retrieval augments the reader, indicating useful external knowledge is retrieved.\nOn HotpotQA, the standard retrieval hurts the reader. This shows that using complex questions as queries cannot compensate for the parametric knowledge, but bring noises instead \\cite{mallen2023llm_memorization}.\nThis suggests that multi-hop questions are not suitable queries for the web search engine.\nThe scores increase by adding the rewriting step.\nOn PopQA, our trainable rewriter surpasses standard retrieval while being inferior to the LLM rewriter.\nThis indicates that the distillation of query rewriting is sub-optimal.\n\nThe scores on multiple-choice QA are presented in Table \\ref{mainmc}.\nWith ChatGPT as a reader, it can be observed that query rewriting improves the scores in most of the settings, except for the social sciences category.\nWith Vicuna as a reader, our method achieves more gains on the four categories compared to ChatGPT.\nThis agrees with the intuition that a more powerful reader has more parametric memories, thus more difficult to compensate with external knowledge.\n\n\\begin{table}[tbh]\n\t\\centering\\small\n\t{\\begin{tabular}{p{4cm}p{0.9cm}p{0.9cm}}\n\t\t\\toprule\n\t\t\\textbf{Model} & \\textbf{EM} &\\textbf{F$_{1}$} \\\\\n            \\midrule\n            \\multicolumn{3}{c}{\\emph{HotpotQA}} \\\\\n            Direct &32.36& 43.05 \\\\\n            Retrieve-then-read &30.47& 41.34 \\\\\n            LLM rewriter &32.80& 43.85 \\\\\n            Trainable rewriter &34.38& 45.97 \\\\\n            \\midrule\n            \\multicolumn{3}{c}{\\emph{AmbigNQ}} \\\\\n            Direct &42.10& 53.05 \\\\\n            Retrieve-then-read &45.80 & 58.50  \\\\\n            LLM rewriter &46.40 &58.74  \\\\\n            Trainable rewriter &47.80 & 60.71 \\\\\n            \\midrule\n            \\multicolumn{3}{c}{\\emph{PopQA}} \\\\\n            Direct &41.94 & 44.61  \\\\\n            Retrieve-then-read & 43.20 &47.53 \\\\\n            LLM rewriter &46.00 & 49.74  \\\\\n            Trainable rewriter &45.72& 49.51 \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t}\n        \\caption{Metrics of open-domain QA.}\n\t\\label{mainqa}\n \\vspace{-1.2em}\n\\end{table}\n\n            \n\n\\begin{table}[tbh]\n\t\\centering\\small\n\t{\\begin{tabular}{p{2.8cm}p{0.7cm}p{0.6cm}p{0.6cm}p{0.6cm}}\n\t\t\\toprule\n            \\textbf{MMLU} & \\multicolumn{4}{c}{\\textbf{EM}}\\\\\n             \\midrule\n             &Human. &STEM & Other & Social \\\\\n             \\midrule\n             \\multicolumn{5}{c}{\\emph{ChatGPT}} \\\\\n            Direct &75.6 &58.8 & 69.0 &71.6 \\\\\n            Retrieve-then-read & 76.7 &63.3 &70.0 &78.2 \\\\\n            LLM rewriter &77.0 &63.5 &72.6 & 76.4\\\\\n            \\midrule\n            \\multicolumn{5}{c}{\\emph{Vicuna-13B}} \\\\\n            Direct &39.8 &34.9 & 50.2 &46.6 \\\\\n            Retrieve-then-read & 40.2 &39.8 &55.2 &50.6 \\\\\n            LLM rewriter &42.0 &41.5 &57.1  & 52.2\\\\\n            Trainable rewriter &43.2 & 40.9& 59.3 & 51.2\\\\\n            \n\t\t\\bottomrule\n\t\\end{tabular}\n\t}\n        \\caption{Metrics of multiple choice QA.}\n\t\\label{mainmc}\n \\vspace{-1.2em}\n\\end{table}\n\n\\section{Analysis}\n\n\\begin{figure}[tbh]     \n \\vspace{-1em}\n    \\centering      \n        \\begin{minipage}{0.49\\textwidth}    \n            \\centering      \n            \\includegraphics[width=\\textwidth]{figs/curve-hot.pdf}\n        \\end{minipage}            \n        \\begin{minipage}{0.49\\textwidth}\n            \\centering      \n            \\includegraphics[width=\\textwidth]{figs/curve-amb.pdf}      \n        \\end{minipage}     \n        \\begin{minipage}{0.49\\textwidth}\n            \\centering      \n            \\includegraphics[width=\\textwidth]{figs/curve-pop.pdf}      \n        \\end{minipage}    \n    \\caption{Reinforcement learning validation scores of (a)HotpotQA, (b)AmbigNQ, and (c)PopQA.  The solid lines show EM (red) and F1 (blue) numbers through training iterations. The dashed lines are EM scores of the standard retrieve-then-read method (orange) and retrieval with an LLM as the rewriter (green).}\n    \\label{curves}\n \\vspace{-1.5em}\n\\end{figure} \n\\subsection{Training Process}\nThe training process includes two stages, warm-up and reinforcement learning.\nThis section shows the validation scores of the three open-domain QA datasets for further analysis.\nFigure \\ref{curves} presents the metric scores through training iterations in the process of reinforcement learning. \nAs the rewriting models have been warmed up on the pseudo data before RL, scores at ``0 iteration'' denote the ability acquired from the warm-up training.\n\nIt can be observed that the curves show upward trends with some fluctuations on all the datasets.\n(\\romannumeral1) For multi-hop questions in HotpotQA, the standard retrieval is relatively weaker. \nComplex questions can be not specific search queries and show a larger gap from rewritten queries, i.e., the green and red lines.\n(\\romannumeral2) On AmbigNQ and PopQA, our method surpasses the baselines after several iterations (3 or 4). % which indicates that our pseudo data is insufficient for this distillation.\nThis indicates that the RL training stage can compensate for the insufficiency of the distillation on the pseudo data during warm-up training.\n(\\romannumeral3) In particular, on PopQA, the trainable rewriter remains inferior to the LLM rewriter. This can be explained as the dataset is constructed for adaptive retrieval \\cite{mallen2023llm_memorization}, which only uses retrieval\nwhere it helps to avoid harmful redundant retrieval. Thus, \\textit{``None''} is a possible query that means no retrieval. This causes more complexity and uncertainty. LLM rewriter knows better when the retrieval is needed for itself as a reader, although the rewriting step is not concatenated as the input context of the reader.\n\nWe calculate the performance of query \\textit{``None''}.\nThe questions that can be correctly answered without retrieval (i.e., the ``Direct'' method) are those samples that need no more context. \nComparing this retrieval-free set with those that are rewritten to be\\textit{``None''} query,\nthe F$_{1}$ score of the LLM rewriter is 71.9\\% and the T5 rewriter score is 67.1\\%.\nIf we consider the questions that can be correctly answered without retrieval but go wrong with retrieval as the retrieval-free set, the F$_{1}$ scores are 78.7\\% for LLM rewriter and 77.4\\% for T5.\n\n\\begin{table}[htb]\n\t\\centering\\small\n        \\setlength{\\belowcaptionskip}{-0.5cm}\n\t{\\begin{tabular}{p{2.5cm}p{1.0cm}p{1.0cm}p{1.2cm}}\n\t\t\\toprule\n\t\t\\textbf{Model} & \\textbf{EM} &\\textbf{F$_{1}$} & \\textbf{Hit ratio} \\\\\n            \\midrule\n            No retrieval &42.10& 53.05& --\\\\\n            Upper bound &58.40 &69.45 & 100 \\\\\n            \\multicolumn{3}{c}{\\emph{Retrieve-then-read}} \\\\\n            w/ snippet &38.70 & 50.50 &61.1 \\\\\n            w/ BM25 &45.80 & 58.50 & 76.4\\\\\n            \\multicolumn{3}{c}{\\emph{LLM rewriter}} \\\\\n             w/ snippet &39.80 &52.64 & 63.5\\\\\n            w/ BM25 &46.40 &58.74 &77.5\\\\\n            \\multicolumn{3}{c}{\\emph{Trainable rewriter}} \\\\\n            w/ BM25\\tablefootnote{Our trainable rewriter is adapted to the retriever using BM25 during RL training. Using the output queries of the test set after training, the snippet hit rate is 73.4\\%.} &47.80 & 60.71 & 82.2 \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t}\n        \\caption{Retrieval analysis on AmbigNQ.}\n\t\\label{ana}\n\\end{table}\n\n\\subsection{Retrieval Result}\nOur proposed method is a pipeline framework, instead of an end-to-end system.\nThe query rewriting first affects the retrieved context, then the context makes a difference to the output of the reader.\nHence, QA metrics are indirect measurements.\nWe take a closer look at the retrieved context and the reader capability through the retrieval metric, hit ratio.\nAfter text normalization, the hit rate is computed to measure whether the retrieved context contains the correct answers.\n\nTable \\ref{ana} shows the scores on AmbigNQ.\nThe scores in the second line are computed on a selection of the samples whose retrieved contexts hit correct answers (under the standard retrieve-then-read setting).\nThe scores show the approximate upper bound ability of the reader with retrieval augmentation, abbreviated as the ``upper bound'' score.\nThe effectiveness of retrieval is proved compared to the no retrieval setting (the first line). \nFor each retrieval method, two settings are presented:\n(\\romannumeral1) collecting Bing snippets, (\\romannumeral2) selecting from URLs by BM25.\nThe metrics show that content selection with BM25 recalls better documents than snippets, while query rewriting makes progress on both settings.\nWe also observed that the improvement in the hit rate of the retriever is more significant than the improvement in the reader. This is consistent with the findings in related search \\cite{mallen2023llm_memorization, liu2023lost}.\n\n\\begin{figure}[t]\n    \\centering\n    \\setlength{\\belowcaptionskip}{-0.3cm}\n    \\includegraphics[width=0.5\\textwidth]{figs/case-crop.pdf}\n    \\caption{\\label{case}Examples for intuitive illustration. Q0 denotes original input, Q1 is from the LLM rewriter, and Q2 is from the trained T5 rewriter. \\textbf{Hit} means retriever recall the answer, while \\textbf{Correct} is for the reader output.}\n\\end{figure}\n\n\\subsection{Case Study}\nTo intuitively show how the query rewriting makes a difference in the retrieved contexts and prediction performance, we present examples in Figure \\ref{case} to compare the original questions and the queries.\nIn example 1, the original question asks for a film that \\textit{the youngest daughter of Lady Mary-Gaye Curzon} co-stars with two certain actors.\nBoth query 1 and query 2 put the keyword \\textit{film} forward, closely following \\textit{the youngest daughter of Lady Mary-Gaye Curzon}. With both, the actress \\textit{Charlotte Calthorpe} and her movie information can be retrieved and the answer is included.\nThe second is an example where the query from the LLM rewriter failed but the query from T5 gets the correct answer. The number \\textit{2000} is misunderstood in query 1, while query 2 keeps \\textit{200 movie} together, avoiding meaningless retrieval.\nExample 3 is for multiple choice. The query simplifies the background and enhances the keyword \\textit{community planner}. The retrieve contexts are mainly about \\textit{Introduction to Community Planning} where the answer \\textit{environment} appears several times.\n\n\\section{Conclusion}\nThis paper introduces the \\textit{Rewrite-Retrieve-Read} pipeline, where a query rewriting step is added for the retrieval-augmented LLM. This approach is applicable for adopting a frozen large language model as the reader and a real-time web search engine as the retriever. Further, we propose to apply a tuneable small language model the rewriter, which can be trained to cater to the frozen retriever and reader. \nThe training implementation consists of two stages, warm-up and reinforcement learning.\nEvaluation and analyses on open-domain QA and multiple-choice QA show the effectiveness of query rewriting.\nOur work proposes a novel retrieval-augmented black-box LLM framework, proves that the retrieval augmentation can be enhanced from the aspect of query rewriting, and\nprovides a new method for integrating trainable modules into black-box LLMs.\n\n\\section*{Limitations}\nWe acknowledge the limitations of this work. (i) There is still a trade-off between generalization and specialization among downstream tasks. \nAdding a training process, the scalability to direct transfer is compromised, compared to few-shot in-context learning.\n(ii) The research line of \\textit{LLM agent} has shown impressive performance but relies on multiple calls to the LLM for each sample \\cite{khattab2022dsp, yao2023react}, where the LLM plays as an agent to flexibly call the retriever multiple times, reads the context in earlier hops, and generates follow-up questions. Different from these studies, our motivation is to enhance the one-turn retriever-then-read framework with a trainable query rewriter.\n(iii) Using a web search engine as the retriever also leads to some limitations. Neural dense retrievers that are based on professional, filtered knowledge bases may potentially achieve better and controllable retrieval. More discussion is included in the appendix.\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading}\n\n\\begin{document}\n\n\\maketitle\n\\renewcommand{\\thefootnote}{\\fnsymbol{footnote}}\n\\footnotetext[1]{Work done during internship at Meta AI.}\n\n\\begin{abstract}\nLarge language models (LLMs) have advanced in large strides due to the effectiveness of the self-attention mechanism that processes and compares all tokens at once.\nHowever, this mechanism comes with a fundamental issue --- the predetermined context window is bound to be limited. Despite attempts to extend the context window through methods like extrapolating the positional embedding, using recurrence, or selectively retrieving essential parts of the long sequence, long-text understanding continues to be a challenge.\nWe propose an alternative approach which instead treats the LLM as an interactive agent,  allowing it to decide \\textit{how} to read the text via iterative  prompting.\nWe introduce \\sysname{}, a method that first processes the long context into a tree of summary nodes. Upon receiving a query, the model navigates this tree in search of relevant information, and responds once it gathers sufficient information. On long-text question answering tasks\nour method outperforms  baseline approaches that use long context windows, recurrence, and retrieval. We show that, beyond effective reading, \\sysname{} enhances explainability by highlighting the reasoning steps as it interactively reads the text; pinpointing the relevant text segments related to the query.\\end{abstract}\n\n\\section{Introduction}\\label{sec:intro}\n\nLarge language models (LLMs) have witnessed significant advancements due to the increased model size, expanded pretraining data, and the adoption of the Transformer architecture with self-attention \\citep{vaswani2017attention}. As LLMs evolve in capability, users increasingly seek to use longer input sequences during inference. This results in a growing demand in querying for information in long documents, analyzing legal or scientific papers, and managing extended conversational dialogues. \nThese tasks involve consuming large amount of information, highlighting the importance of longer context processing.\n\nDespite the rapid development, the limitation of the self-attention mechanism becomes apparent as its memory usage increases with longer sequences, consequently limiting the size of the context window. To address this, different approaches have been employed, such as designing lighter and more efficient attention schemes \\citep{zaheer2020bigbird}, finetuning with extrapolated or interpolated positional embeddings \\citep{press2022train, chen2023extending}, incorporating recurrence to bring forward information from preceding text segments into the next \\citep{rae2019compressive,fan2020feedback,xu2022beyond}, or retrieving relevant parts of the text \\citep{lewis2020retrieval,izacard2021leveraging}. However, these approaches are still limited by design. The context window, no matter how long it is extended, assumes a fixed size, and not all positions within it hold equivalent significance \\citep{liu2023lost}. While recurrence can manage infinite-length sequences, it often misses out on retaining information from earlier segments.  Additionally, retrieving segments from the coherent long-text might be ineffective, given that many retrieval systems are tailored to distinguish similar but distinct documents \\citep{chen2017reading}.\n\nTo address these issues, we develop a fundamentally different approach which\ntreats the model with a finite context window as an interactive agent, rather than simply processing the entire sequence in one go. To this end, we introduce \\sysname{}, a method that enables the model to read the long-text interactively via iterative LLM prompting. \\sysname{} operates through a two-stage approach: 1) \\textit{memory tree construction} and 2) \\textit{navigation}. During the first stage, the long-text is segmented into small chunks that fit within the LLM's context window. The LLM then subsequently summarizes each segment into a textual summary node. These summary nodes are progressively further summarized into higher-level summary nodes, thus building a tree structure (Figure~\\ref{fig:teaser}). To answer a user query, the LLM begins navigation from the tree's root node. It traverses the tree, inspecting various parts of the text to identify the path and segment relevant to answer the query. As a result, \\sysname{} can go beyond the context limit, efficiently processing texts and localizing the important segments of the long-text,  without additional finetuning.\n\nWe evaluate \\sysname{} on three long context question answering tasks and show superior performance against recurrence, retrieval, and %\nvanilla LLM baselines. \\sysname{} also outperforms other open long context systems that can take $8,000$ to $16,000$ tokens. We provide an analysis of the effectiveness of  %\n\\sysname{},  and show it can\nreason about navigation decisions, incorporate working memory during traversal, and\nrecover from errors made in early navigational steps.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.85\\textwidth]{figs/teaser_.pdf}\n    \\caption{\n        The two-stage procedure of \\sysname{}. Top (stage 1): the memory tree is constructed. The long text is split into segments of a predetermined size and  each segment is first summarized into a summary node. The summary nodes are recursively summarized into higher level nodes until it reaches the root.\n        Bottom (stage 2): Given a query, the LLM navigates the tree structure via iterative prompting and finds the node that contains relevant segment to form the answer. At each node, the LLM decides the action by first reasoning about the child summary nodes by sampling from the distribution $\\texttt{LLM}(reasoning, action \\mid summ, query)$.\n        The LLM can choose the {\\em revert} action to return to the parent node if it chose the wrong path or the segment at hand is irrelevant (dashed red arrow). See Table~\\ref{table:examples} for a detailed example showing the LLM prompts that enable navigation.\n    }\n    \\label{fig:teaser}\n\\end{figure}\\section{Related Work}\\label{sec:related}\n\n\\paragraph{Context window scaling.}\nA straightforward approach to enable a longer context sequence is to tune the pre-trained language models and extrapolate their positional embeddings on longer text sequences \\citep{press2022train, chen2023extending}.\nAnother direction is modified self-attention \\citep{beltagy2020longformer, zaheer2020bigbird, guo2022longt5, ainslie2023colt5}.\nThis approach has advanced in large strides thanks to training techniques such as Flash Attention \\citep{dao2022flash} that greatly reduce the memory footprint.\nDespite the recent advances, this approach comes with two natural limitations: 1) to enable models on longer sequences, the model needs to be fine-tuned, incurring a non-negligible cost and 2) the attention mechanism may become less effective due to positional biases as the sequence length becomes very long \\citep{liu2023lost}.\n\n\\paragraph{Recurrence.}\nRecurrent architectures have been extensively studied to tackle long sequence problems, from  recurrent neural network based models \\cite{hochreiter1997lstm, miller2016memorynetwork} to the modern Transformer based models \\citep{dai2019transformerxl, rae2019compressive, fan2020feedback, xu2022beyond, bulatov2023rmt, chevalier2023adapting}. However, each recurrence step incurs information loss and the training objective does not guide ``how to compress'' with regard to downstream tasks. Typically this compression means that recall of older sequence information is weaker compared to recent information.\n\n\\paragraph{Retrieval.}\nRetrieval systems are commonly used to select relevant documents from a large pool of documents, and have been incorporated into neural models in various ways \\citep{chen2017reading,dinan2018wizard,lewis2020retrieval}. For long sequence reading, retrieval based methods typically first  embed the text segments into vector representations and retrieve them based on the query instead of feeding the entire sequence into the model such as in Fusion-in-Decoder \\cite{izacard2021leveraging} or kNN variants that attend to external memory such as Memorizing Transformers \\citep{wu2022memorizing}.\n\n\\paragraph{Reasoning agents.}\nInstead of taking the long text as a single monolithic input, a model can act as an agent that reads part of the text and takes flexible actions. Work such as WebGPT \\citep{nakano2021webgpt} and WebShop \\citep{yao2022webshop} allow the model to  scroll through the internet and search for the requested answer or item. While their atomic actions allow for interactive search for relevant content, the models were not designed for understanding long and coherent texts. On the other hand, PEARL \\citep{sun2023pearl} prompts the model to generate pseudo APIs for the model to call in order to focus on the right parts of the long text. However, the method operates %\nwithin the LLM's context window, rather than being a memory-access approach that goes beyond the context limit.\nOther works leveraged iterative prompting to reason and plan for long text generation tasks such as Re3 \\citep{yang2022re3} and RecurrentGPT \\citep{zhou2023recurrentgpt}. Self-Notes \\citep{lanchantin2023selfnote} interleaved self-generating notes and the input data to perform better reasoning.\nPrior to current LLMs, LSTMs were also applied to searching through document structures (titles, subsections) \\cite{geva2018learning}. \nRecursive tree structure has also been explored  in the context of summarization of long text such as books in  \\citep{wu2021recursively}, but was not used for memory navigation in that work. \n\n\\section{\\sysname{}: An interactive reader}\\label{sec:method}\\label{interactivereading}\n\n\\begin{table*}[t]\n    \\centering\n    \\caption{\n        Example trajectory from the QuALITY dataset. The LLM first sees the content of the children nodes at the root node (\\texttt{\\sethlcolor{summ9}\\hl{summ 9}} in Figure~\\ref{fig:teaser}) and generates the response (takes action 0 to enter \\texttt{\\sethlcolor{summ7}\\hl{summ 7}}). When arriving at the leaf node (\\texttt{\\sethlcolor{summ2}\\hl{summ 2}}), the LLM determines that there is not enough information, therefore takes the action to revert (action -1) to the parent node.\n        After hopping back-and-forth between nodes, the LLM commits to a Leaf node (\\texttt{\\sethlcolor{summ2}\\hl{summ 3}}) and answers the question. Yellow indicates \\sethlcolor{triageprompt}\\hl{triage prompt} and purple indicates \\sethlcolor{leafprompt}\\hl{leaf prompt} described in \\S\\ref{sec:method}. Text after // denotes comments that are not processed by the LLM.\n    }\n    \\label{table:examples}\n    \\resizebox{1.0\\columnwidth}{!}{\n        \\begin{tabular}{lp{15cm}}\n        \\toprule\n        \\multicolumn{2}{c}{\\textbf{Navigation Trajectory}} \\\\\n        \\midrule\n        \\texttt{\\sethlcolor{summ9}\\hl{summ 9}} & \\sethlcolor{triageprompt}\\hl{The following passage(s) are the summaries of the different parts of a story.} \\newline\n        \\sethlcolor{triageprompt}\\hl{To answer the question:} \\textit{Why did Ro change his mind about the people on Mars being backwards?} \\newline\n        \\sethlcolor{triageprompt}\\hl{Which of the following summary is MOST LIKELY to contain information about the answer?} \\newline\n        \\sethlcolor{triageprompt}\\hl{First provide reasoning to compare the summaries before you make the decision.} \\newline \\newline\n        Summary 0: The story is set on Mars and follows the adventures of Ro, [...] // \\texttt{\\sethlcolor{summ7}\\hl{summ 7}} \\newline\n        Summary 1: Ro, a young Martian, is climbing down a cliff to rescue [...] // \\texttt{\\sethlcolor{summ7}\\hl{summ 8}} \\newline\n        \\newline\n        \\sethlcolor{triageprompt}\\hl{Relpy with the passage number as your action.} \\newline\n        \\sethlcolor{triageprompt}\\hl{You MUST choose one summary number and you should reply with the following format:} \\newline\n        \\sethlcolor{triageprompt}\\hl{\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#} \\newline\n        \\sethlcolor{triageprompt}\\hl{Reasoning: ...} \\newline\n        \\sethlcolor{triageprompt}\\hl{Action: 0 / 1 / 2, ...} \\newline\n        \\sethlcolor{triageprompt}\\hl{\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#} \\\\\n        Response & Reasoning: Summary 0 is most likely to contain information about why Ro changed his mind about the people on Mars being backwards, as it mentions Ro's interactions with the people from Earth and their advanced method of communication. \\newline Action: 0 \\\\\n        \\midrule\n        \\multicolumn{2}{l}{LLM enters \\texttt{\\sethlcolor{summ7}\\hl{summ 7}} then enters \\texttt{\\sethlcolor{summ2}\\hl{summ 2}}} \\\\\n        \\midrule\n        \\texttt{\\sethlcolor{summ2}\\hl{summ 2}} & \\sethlcolor{leafprompt}\\hl{Read the text in triple quotes and answer a question:} \\newline\n        \\sethlcolor{leafprompt}\\hl{Story background information:} Ro, a young Martian who has returned from his travels to find his home overrun by the Oan, a tribe of rat-like creatures. [...] // $\\texttt{[WORKING\\_MEMORY]}$ carried from \\texttt{\\sethlcolor{summ7}\\hl{summ 7}} \\newline\n        \\sethlcolor{leafprompt}\\hl{Main text:} In his left hand and under his armpit Ro carried stones. They were of a good weight and would make short work of any Oan who was foolish enough to cross his path. [...] // \\texttt{seg 2} \\newline \\newline\n        Why did Ro change his mind about the people on Mars being backwards? \\newline\n        (A) He realized that despite human's technological advancements, they have over-complicated marriage. \\newline\n        (B) He realized that while the humans are physically vulnerable without their weapons, the red people have formidable strength in their arms. \\newline\n        (C) He realized that human males suppress public affection when they are intimidated by other males, whereas male Martians don't hide their affection. \\newline\n        (D) He realized that male humans were petty and even brute when it came to rivalry over women, whereas male Martians were much more civilized. \\newline\n        \\sethlcolor{leafprompt}\\hl{If the answer CANNOT be inferred from the text above, reply with action -1.} \\newline\n        \\sethlcolor{leafprompt}\\hl{If the answer CAN be inferred from the text above, reply with action -2, and also provide your reasoning, and the final answer.} \\newline\n        \\sethlcolor{leafprompt}\\hl{You are ONLY allowed to reply with action -2 or -1.} \\newline\n        \\sethlcolor{leafprompt}\\hl{Your should reply with the following format:} \\newline\n        \\sethlcolor{leafprompt}\\hl{\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#} \\newline\n        \\sethlcolor{leafprompt}\\hl{Reasoning: ...} \\newline\n        \\sethlcolor{leafprompt}\\hl{Action: -2 or -1} \\newline\n        \\sethlcolor{leafprompt}\\hl{Answer: (A) ...} \\newline\n        \\sethlcolor{leafprompt}\\hl{\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#} \\\\\n        Response & Reasoning: The text does not explicitly mention Ro changing his mind about the people on Mars being backwards. Therefore, the answer cannot be inferred from the text. \\newline Action: -1 \\\\\n        \\midrule\n        \\multicolumn{2}{l}{LLM reverts to \\texttt{\\sethlcolor{summ7}\\hl{summ 7}} then enters \\texttt{\\sethlcolor{summ2}\\hl{summ 3}}} \\\\\n        \\midrule\n        \\texttt{\\sethlcolor{summ2}\\hl{summ 3}} & \\sethlcolor{leafprompt}\\hl{[...] Main text:} ``Last night I thought that we on Mars are backward. Now I'm not so sure.'' {[...]}  // \\texttt{seg 3} \\sethlcolor{leafprompt}\\\\\n        Response & Reasoning: Ro initially sees Earth's customs as backward compared to Mars, However, after discussing [...] \\newline\n        Action: -2 \\newline\n        Answer: (A) \\\\\n        \\bottomrule\n        \\end{tabular}\n    }\n\\end{table*}\n\nWe study tasks related to long-context question answering -- given a long-text $x$ and a query $q$, the  model aims to generate the response $r$. %\n\n\\sysname{} follows two steps: 1) \\textit{memory tree construction}, where the long-context is broken down into a tree data structure. This construction does not depend on the query, and can hence be computed in advance if the sequence data is available beforehand.  2) \\textit{navigation}, in which the model navigates this structure upon receiving a query, gathering information to craft a suitable response. \\sysname{}~assumes access to an underlying LLM, and both construction and navigation are achieved through iterative LLM prompting.\n\n\\paragraph{Memory tree construction.}\n\\sysname{} first creates a tree data structure, $\\mathcal{T}(x)$, from the long-text $x$. Each node is represented by text that encapsulates the summaries of all its child nodes below it. Specifically, the long-text $x$ is divided into segments $(c_1, \\dots, c_n)$. The LLM then summarizes each segment into a summary at the first level, represented as $s^{l=1}_i = \\texttt{LLM}(c_{\\leq i})$, $i = 1 .. n$. The initial summary nodes are subsequently summarized further into higher level nodes, $s_j^{l+1} = \\texttt{LLM}(s^l_i, \\dots, s^l_{i + M_t})$ where $M_t$ denotes the number of nodes in the $t$-th grouping at level $l$. This process continues until the topmost root node, $s^L$ is generated. The complete tree generation process is illustrated in Figure~\\ref{fig:teaser}. Summarization is performed using LLM prompting.  We include the prompts for memory tree construction in Appendix~\\ref{app:prompts}. %\n\n\\paragraph{Navigation.}\nThe language model, upon receiving a query $q$, starts from the root node $s^L$ to navigate the tree to generate the response $r$. \nAt node $s^{l}$ that the LLM traverses, it observes the summaries of the nodes one level below $\\{ s^{l-1}_i, \\dots, s^{l-1}_{i+M_t} \\}$. The LLM decides among $\\lvert M_t \\rvert + 1$ actions --- choosing one of the child nodes to further inspect, or to revert to the parent node. At leaf node $s^{l=1}_i$, the LLM can decide one of two actions: \\textit{commit} to the leaf node and respond to the query or \\textit{revert} to the parent node ($s^{l+1}_j$) if the information in the leaf node (i.e., $c_i$) is insufficient.\nTo make a navigation decision, we can also ask the LLM (via prompting) to \n first generate a reason in natural language to justify the action, followed by the action choice itself. Specifically, at each node, the model generates a response $r \\sim \\texttt{LLM}(r \\mid s, q)$ where the response is either of the two tuples: \n1) $r = (reasoning, action, answer)$ when the LLM is at a leaf node or 2) $r = (reasoning, action)$ when the LLM is at non-leaf nodes.\n\n\\paragraph{Navigational prompt design.}\n\nWe enable LLM navigation through zero-shot \nprompting. \nOur method requires two types of prompt:\n1) \\textit{\\sethlcolor{triageprompt}\\hl{triage prompt}} and 2) \\textit{\\sethlcolor{leafprompt}\\hl{leaf prompt}} (highlighted in Table~\\ref{table:examples}).\nTriage prompt contains the the query, the summaries of the children nodes, and instructions for the LLM to follow. Triage prompt is used at non-leaf nodes.\nLeaf prompt contains the content of the segment, the query (and options), and instructions that ask the LLM to either generate the answer or revert to the parent node. Both the triage prompt and leaf prompt specify an output format that the LLM needs to follow. Failure to conform to the format results in invalid actions and the LLM is required to regenerate. If the LLM fails to generate parsable output three consecutive times, the navigation terminates and returns ``no answer''. \n\n\\paragraph{Working memory.} As the LLM traverses the tree, it can keep information throughout the navigation trajectory and add it to the context. Formally, the LLM generates the response $r \\sim \\texttt{LLM}(r \\mid s, q, m)$ where the extra working memory $m \\in \\{\\O\\} \\cup \\{ (s_i, s_{i+1}, \\dots) \\}$ is either empty or consists of contents from previously visited nodes. We truncate the working memory such that they can fit in the LLM's context window.\\footnote{Further summarizing the working memory as it accumulates would be an alternative approach, which we have not explored in this study.} Table~\\ref{table:examples} illustrates the way working memory is added via  \\texttt{[WORKING\\_MEMORY]} in the prompt.\n\n\\section{Experimental Setup}\\label{sec:exp}\n\n\\subsection{Datasets \\& Evaluation}\nWe use three datasets: QuALITY, SummScreenFD, and GovReport from the SCROLLS benchmark \\citep{shaham2022scrolls}.\nWe report accuracy for all datasets.\n\n\\paragraph{QuALITY.} QuALITY is a multiple choice question answering dataset collected by \\cite{pang2022quality}. The dataset contains long-form stories sourced from Project Gutenberg and questions annotated by human annotators. We use a subset of $187$ examples for our experiments.\n\n\\paragraph{SummScreenFD.} SummScreenFD \\citep{chen2022summscreen} is a dataset of TV and movie scripts in the form of dialogues among actors originally designed for summarization. We repurpose the dataset into a question answering task where the original provided ground truth summary text is used to generate a ``who'' question using Stable Beluga 2, with answers then checked by a human expert. The question paired with the original long text becomes %\nthe repurposed QA task of $306$ examples.\n\n\\paragraph{GovReport.} The GovReport dataset aggregates documents from Congressional Research Service and the U.S. Government Accountability Office together with summaries provided by experts \\citep{huang2021efficient}. We repurpose the dataset into a question answering dataset of $101$ examples the same way as for SummScreenFD.\n\nAll three datasets feature long contexts per example of varying length -- some shorter examples, and some longer sequences. We therefore both report results on the original dataset, and also report on a subset of each task containing only longer sequences, to better evaluate memory access in the harder, longer context case.  The thresholds are above 8, 000 tokens for QuALITY, 6, 000 tokens for SummScreenFD, and 12, 000 tokens for GovReport.\n\n\\subsection{Model}\nWe use Stable Beluga 2 \\citep{StableBelugaModels} as the base LLM for the majority of our experiments, as it provides  state-of-the-art performance compared to several other LLM variants, as we will show. \nStable Beluga 2 is an instruction-tuned model\nbuilt on top of  70B LLaMA-2\\citep{touvron2023llama2}, where the finetuning does not overlap with our evaluation tasks.\nIt has a maximum $4,096$ token context length.\nWe use the model in a zero-shot prompting fashion without further fine-tuning or in-context few shot examples for our tasks. We use top-$p$ sampling for both memory tree construction as well as generating action and reasoning for navigation. We set the maximum number of nodes $\\max_t M_t = 8, 5, 8$ and segment size $\\lvert c \\rvert = 1000, 1000, 1200$ for QuALITY, SummScreenFD, and GovReport respectively.\n\n\\subsection{Baselines}\nWe compare with three baselines memory techniques all based on the same underlying LLM, Stable Beluga 2: 1) full context window, 2) recurrence, and 3) retrieval.\nThe full context window baselines utilize the full $4,096$ tokens to process both the long input text and generation. Since the instances in the dataset often exceed the context limit, we perform truncation of the length to the right (most recent) or left (least recent) of the text as the input, as evaluate both approaches. For retrieval, we use Contriever \\citep{izacard2022unsupervised} to select segments from the long context based on the query. The highest scored segments are concatenated as the input context to the LLM until they fill the context. \nFinally, we implement a baseline that recurrently carries information from previous segment tokens to the current one through summarization \\citep{xu2022beyond}, where each segment is $2,500$ tokens and the maximum summary size is $500$ tokens.\n\\section{Results \\& Analysis}\\label{sec:results}\n\n\\begin{table*}[t]\n    \\centering \n    \\caption{\n        Results on the three question answering tasks, reporting test accuracy. Orig. denotes using the entire dataset and Long denotes the subset of longer sequences. \n        Top: comparison to open long context models. Bottom: baselines and \\sysname{} performance, with all methods using the underlying Stable Beluga 2 LLM with a maximum $4,096$-token context length. \\sysname{}~outperforms all other systems on longer sequences.\n    }\n    \\resizebox{0.8\\columnwidth}{!}{\n        \\begin{tabular}{lccc}\n        \\toprule\n         & QuALITY & SummScreenFD & GovReport \\\\\n         & Orig. / Long & Orig. / Long & Orig. / Long \\\\\n        \\midrule\n        MPT 13B (8k) & 44.4 / 47.3 & 65.0 / 63.5 & 44.6 / 43.8\\\\\n        LongChat 13B (16k) & 43.3 / 48.4 & 62.4 / 61.1 & 54.5 / 52.1 \\\\\n        \\midrule\n        Recurrence & 51.3 / 56.0 & 47.7 / 45.4 & 35.6 / 33.8 \\\\\n        Retrieval & 63.1 / 64.8 & 63.7 / 62.2 & 54.0 / 52.1 \\\\\n        Full Context (keep left) & 56.7 / 64.8 & 62.7 / 62.7 & \\textbf{59.4} / 56.3 \\\\\n        Full Context (keep right) & \\textbf{70.1} / 72.5 & 64.7 / 63.1 & 50.5 / 50.0 \\\\\n        \\sysname{} & 67.4 / \\textbf{73.6} & \\textbf{67.3} / \\textbf{64.5} & \\textbf{59.4} / \\textbf{60.4} \\\\\n        \\bottomrule\n        \\end{tabular}\n    }\n    \\label{table:main_results}\n\\end{table*}\n\\paragraph{Main results.}\nTable~\\ref{table:main_results} shows comparisons between \\sysname{} and other baselines. \\sysname{} outperforms both the recurrence  baseline across all tasks by a large margin. This shows the limitation of recurrence, where  relevant information to the query is lost after several steps. \n \\sysname{}  also outperforms retrieval where the segments are from a coherent long story instead of separate documents.\nOn these tasks, the full context baselines can perform well in the ``Original'' task setting, which can contain relatively shorter sequences, although choosing either left or right truncate for best performance seems to be dataset dependent.  Still, \\sysname{} achieves higher performance in the Original setting against the Full Context baselines except for the keep right variant on QuALITY and the keep left variant on GovReport, likely due to the positional bias in the dataset where relevant segment often appears at the beginning or the end of the text. However, on the Long version of all three tasks \\sysname{} outperforms all baselines, that is it shows strong performance when memory access becomes more critical.\n\\sysname{} also outperforms other publicly available models, including LongChat \\citep{longchat2023} and MPT \\citep{mosaic2023introducing}.\n\n\\paragraph{\\sysname{} improves performance on long sequences.}\nWe provide a breakdown of performance by input sequence length for each task in Figure~\\ref{fig:perf_breakdown_by_length}.\n\\sysname{} is not advantageous over Full Context (with truncation left or right) baselines when the text length is short, but outperforms both types of truncation for all tasks for longer sequences. The benefit of interactive reading emerges after the text length is suitably large, i.e. showing better performance once the sequence length is sufficiently larger than the LLM context length of $4,096$.\n\n\\begin{table*}[t]\n    \\centering\n    \\vspace{-5pt}\n    \\resizebox{1.0\\columnwidth}{!}{\n        \\begin{tabular}{lccc}\n        \\toprule\n         & QuALITY & SummScreenFD & GovReport \\\\\n         & Acc. / Valid Action (\\%) & Acc. / Valid Action (\\%) & Acc. / Valid Action (\\%) \\\\\n        \\midrule\n        LLaMA 2 Chat (13B)    & 39.6 / ~~73.2 & 20.9 / ~~75.5 & 15.8 / ~~69.0 \\\\\n        ~~~~ w/o reasoning    & 48.1 / ~~97.4 & 25.8 / ~~95.8 & 21.8 / ~~93.1 \\\\\n        \\midrule\n        LLaMA 2 Chat (70B)    & 52.0 / ~~86.1 & 55.6 / ~~99.5 & 41.6 / ~~97.8  \\\\\n        ~~~~ w/o reasoning    & 59.9 / 100.0 & 58.5 / 100.0 & 42.6 / 100.0 \\\\\n        \\midrule\n        Stable Beluga 2 (70B) & 67.4 / ~~92.5 & 67.3 / ~~95.1 & 59.4 / ~~97.0\\\\\n        ~~~~ w/o reasoning    & 66.8 / 100.0 & 64.1 / ~~90.5 & 52.5 / ~~98.2 \\\\\n        \\bottomrule\n        \\end{tabular}\n    }\n    \\caption{\n        \\sysname{} performance using different underlying LLMs with different reasoning capabilities, and an ablation on their reason justification component when making a navigation decision (``w/o reasoning'' simply predicts the action, with no reason generated, see e.g. \\autoref{table:examples}). Valid Action shows the percent of generated actions that are a valid navigation action. We find that the strongest performing LLM (Stable Beluga 2) benefits from reasoning with improved accuracy, while weaker performing LLMs do not (get worse in terms of accuracy and valid actions).  \n    }\n    \\label{table:model_reason_capability}\n\\end{table*}\n\\begin{figure}[t!]\n    \\vspace{10pt}\n    \\centering\n    \\includegraphics[width=1.0\\textwidth]{figs/perf_breakdown_by_length_.pdf}\n    \\caption{\n        Performance breakdown by context length (in tokens). Each dataset is thresholded into two bucket of equal sizes. \\sysname{} outperforms full context baselines (truncated either left or right, when the sequence does not fit) on longer context sequences, for all three tasks.\n    }\n    \\label{fig:perf_breakdown_by_length} \n    \\vspace{-9pt}\n\\end{figure}\n\n\\paragraph{Reasoning capability is essential for memory tree navigation.}\nThe effectiveness of \\sysname{} is highly dependent on the underlying LLM's reasoning capability. \nFor each navigation decision, we employ an LLM prompt that requires the LLM to first generate a reason in natural language that justifies the following predicted action, see \\autoref{table:examples}.\nWe show in Table~\\ref{table:model_reason_capability} how reasoning impacts performance by comparing Llama 2 Chat (13B and 70B parameter variants) and Stable Beluga 2 (70B)  with and without the reasoning justification by removing the \nline ``First provide reasoning \\dots before you make your decision'' from the prompt. With the smaller, less capable models (13B), the performance lags behind 70B models by a large margin due to its inability to follow instructions. %\nIn fact, asking for reasoning justifications for weaker models \n{\\em decreases performance}, presumably due to their inability to generate and make use of such reasons.\nStable Beluga 2 outperforms Llama 2 Chat for the same LLM size, and also displays heightened reasoning ability.\nFor Stable Beluga 2, asking for reasoning justification {\\em improves performance} across all tasks.\nThis highlights the main characteristic of \\sysname{}: if an LLM passes a critical reasoning ability threshold, it can reason about a long input in multiple rounds without errors cascading quickly across rounds. For weaker LLMs that cannot make good navigation decisions, \nerrors could compound and overall performance suffers.\nAs LLMs will only improve in reasoning ability over the coming years, we expect methods like \\sysname~will become more and more effective.\n\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=.8\\textwidth]{figs/working_memory_.pdf}\n    \\caption{\n        \\sysname{} performance comparisons between using working memory and without (i.e., the LM only looks at the content of the children memory tree nodes, rather than memory from all the nodes it has traversed). Inclusion of working memory yields large gains.\n    }\n    \\label{fig:working_memory} \n    \\vspace{-8pt}\n\\end{figure}\\begin{table}[t]\n\\RawFloats\n\\begin{minipage}{.47\\linewidth}\n    \\centering\n    \\caption{\\sysname{}~navigation analysis. Stray ratio: percentage of paths that contain the \\textit{revert} action. Recovery Rate: percentage of stray paths that recover and answer the query correctly.}\n    \\label{table:recovery}\n    \\begin{tabular}{lcc}\n        \\toprule\n         & Stray & Recovery \\\\\n         & Ratio & Rate \\\\\n        \\midrule\n        QuALITY & 15.0 & 70.0 \\\\\n        SummScreenFD & 18.6 & 59.6 \\\\\n        GovReport & 18.8 & 79.0 \\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{minipage}\n\\begin{minipage}{.52\\linewidth}\n    \\centering\n    \\includegraphics[width=0.9\\textwidth]{figs/efficient_reading.pdf}\n    \\captionof{figure}{Percentage comparison of total tokens processed against the tokens of the original example between all paths vs. successful paths.}\n    \\label{fig:efficient_reading}\n\\end{minipage}\n\\end{table}\n\\vspace{-7pt}\n\n\\paragraph{Navigating the memory tree requires working memory.}\nAs \\sysname{} makes decisions to traverse the memory tree and read  relevant segments, it might lose sight of the overall context. The model thus carries information from the nodes along the navigation path as working memory, where the content of the working memory updates as the model selects the next path. \nWe evaluate the performance of \\sysname{}~with and without working memory, with results given in Figure~\\ref{fig:working_memory}.\nWe find a significant performance\ndegradation without working memory across all tasks, with a 5--13\\% drop in accuracy,\nshowing the importance of this component.\n\n\\paragraph{\\sysname{} can recover from stray paths.}\nAs \\sysname{}~navigates the memory tree, it needs to not only find the path towards the most pertinent segments, but also potentially to recover from traversal errors should they occur. We report recovery statistics in  Table~\\ref{table:recovery}.  \n\\sysname{} executes a revert navigation action (and hence changes path) for around $15\\%-20\\%$ of examples, but of those examples can recover and get those examples correct $70\\%$ of the time for QuALITY, $\\sim60\\%$ for SummScreenFD, and $\\sim80\\%$ for GovReport.\n\n\\paragraph{\\sysname{} enables efficient reading.}\nSince \\sysname{} determines which parts of the long text it needs to read, the effective content that needs to be read may be  smaller than the entire sequence. We report the percentage of the long context read  averaged over all examples, for each of the three tasks, in Figure~\\ref{fig:efficient_reading}. We find that between only 63\\%-69\\%  of the text on average needs to be read to answer the question including the content of the tree nodes. Among successful paths, the reading required further reduces to 59\\% - 64\\%.\n\n\\paragraph{Memory tree construction trade-off.}\nA fundamental trade-off arises as we construct the memory tree --- summarizing larger segments compresses more information into a node to reduce the depth of the tree, but risks losing fidelity of the content. Similarly, connecting many lower level nodes to the upper one can help flatten the tree, yet render the navigation task harder for the LLM at each node.\nFigure~\\ref{fig:tradeoff} shows  the performance of different configurations of the memory tree on QuALITY. Summarizing larger segments is generally more beneficial than smaller segments as well as connecting more children nodes to the parent. However, the performance plateaus as the maximum number of nodes increases, showing the trade-off with respect to how much information can be packed into the nodes during memory tree construction. \n\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{figs/tradeoff_.pdf}\n    \\caption{\n        Performance trade-off of different memory construction configurations on QuALITY. x-axis: maximum number of nodes that can be connected to a parent node. Red: summarizing $1,000$-token segments. Blue: summarizing $500$-token segments.\n    }\n    \\label{fig:tradeoff}\n    \\vspace{-12pt}\n\\end{figure}\\section{Conclusion}\\label{sec:conclusion}\n\\vspace{-5pt}\nWe propose \\sysname{}, an interactive reading agent \nwhich uses iterative LLM prompting to decide which part of the content should be read closely based on its own reasoning.\nOur approach first builds a structured memory given long context sequence data, and then makes navigation decisions of the pertinent parts to read given a query.  \nOur method shows superior performance against \na number of baselines including various long context length models, retrieval and recurrence baselines,\nin particular for longer sequence tasks.\nDetailed analysis highlights a number of important factors, including our method's ability to reason about navigation decisions, ability to revert navigation to a different path when necessary, and incorporation of a working memory.\nFuture work should explore many new directions that \\sysname{}~opens up, in particular its application to different data structures other than trees, and finetuning its performance specific to the interactive reading goal.\n\n\\section{Limitations}\\label{sec:limitation}\n\\vspace{-5pt}\n\\sysname{} exhibits three major limitations. First, the memory tree generation might not scale too well if the sequence's length becomes extremely long. The increase in sequence length entails more nodes in the tree and hence renders the tree construction process onerous. Workaround such as trading off the granularity of the summary in exchange for speed might be viable. Nonetheless, the issue of scaling remains a limit. In this setting it may make sense to generalize \\sysname{} to a combination of tree and hash \\cite{bawa2005lsh} or other alternative data structure, whilst retaining its travesersal ability via LLM prompting. \nSecond, \\sysname{} only works when the LLM exhibits a strong enough reasoning capability, which according to our experiments is required to be large (over 70B) and instruction-tuned. If the reasoning capability falls short, the error compounds and the method would fail. Enabling a smaller model that can perform a similar instruction following procedure could be useful for scaling the method. This could be made possible by removing the following third limitation.\nThird, \\sysname{} only uses zero-shot prompting and does not leverage fine-tuning to further improve the interactive reading capability. This could be done, for example, by performing interactive reading and collect the successful paths for further fine-tuning.\n\n\\newpage\n\\clearpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{\\large PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design}\n\n\\begin{document}\n\n\\twocolumn[\n\\icmltitle{\\large PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design}\n\n\\begin{icmlauthorlist}\n\\icmlauthor{Wenqi Jiang$^\\dagger$}{eth}\n\\icmlauthor{Shuai Zhang}{aws}\n\\icmlauthor{Boran Han}{aws}\n\\icmlauthor{Jie Wang$^\\dagger$}{meta}\n\\icmlauthor{Bernie Wang}{aws}\n\\icmlauthor{Tim Kraska}{aws,mit}\n\\end{icmlauthorlist}\n\n\\icmlaffiliation{eth}{ETH Zurich}\n\\icmlaffiliation{aws}{Amazon Web Services}\n\\icmlaffiliation{meta}{Meta}\n\\icmlaffiliation{mit}{MIT}\n\n\\icmlcorrespondingauthor{Wenqi Jiang}{wenqi.jiang@inf.ethz.ch}\n\\icmlcorrespondingauthor{Shuai Zhang}{shuaizs@amazon.com}\n\n\\icmlkeywords{Machine Learning, ICML}\n\n\\vskip 0.3in\n]\n\n\\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution\n\\def\\thefootnote{$^\\dagger$}\\footnotetext{Work conducted while at Amazon.}\\def\\thefootnote{\\arabic{footnote}}\n\\begin{abstract}\nRetrieval-augmented generation (RAG) can enhance the generation quality of large language models (LLMs) by incorporating external token databases. However, retrievals from large databases can constitute a substantial portion of the overall generation time, particularly when retrievals are periodically performed to align the retrieved content with the latest states of generation.\nIn this paper, we introduce PipeRAG, a novel algorithm-system co-design approach to reduce generation latency and enhance generation quality.\nPipeRAG integrates (1) pipeline parallelism to enable concurrent retrieval and generation processes, (2) flexible retrieval intervals to maximize the efficiency of pipeline parallelism, and (3) a performance model to automatically balance retrieval quality and latency based on the generation states and underlying hardware.\nOur evaluation shows that, by combining the three aforementioned methods, PipeRAG achieves up to 2.6$\\times$ speedup in end-to-end generation latency while improving generation quality. These promising results showcase the effectiveness of co-designing algorithms with underlying systems, paving the way for the adoption of PipeRAG in future RAG systems.\n\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\n\nRetrieval-augmented generation (RAG) enhances auto-regressive large language models (LLMs) by conditioning on contextually relevant content retrieved from external databases. \nWhile one retrieval prior to the generation process can be enough when generating short sequences~\\cite{lewis2020retrieval, izacard2020leveraging}, a more general approach involves periodic retrievals throughout the generation~\\cite{borgeaud2022improving, norlund2023generalization, ram2023context, jiang2023active, trivedi2022interleaving}. This necessity arises due to the potential shift in the generation context, such as changes in topics.  Therefore, periodic retrievals ensure the retrieved content remains relevant to the latest context of the generation (Appendix~\\ref{sec:append_more_background} showcases a concrete example). \nA popular example of this category is \\textsc{Retro}~\\cite{borgeaud2022improving}, which tailors the transformer neural network architecture to support the integration of retrieved content at regular intervals.\n\nHowever, periodic retrievals on large databases, potentially comprising trillions of tokens~\\cite{borgeaud2022improving}, can lead to a significant slowdown of the sequence generation. \\textit{We ask: can we optimize the system performance of RAG while preserving or even improving generation quality?} \n\n \nWe propose PipeRAG, a pioneering approach to improve RAG efficiency via a collaborative algorithm-system co-design --- including a system-aware RAG algorithm and an algorithm-aware retrieval system as overviewed in Figure~\\ref{fig:overview}. \n\nThe foundation of PipeRAG is established on three observations centered on performance. Firstly, the dependencies between retrievals and LLM inferences lead to hardware underutilization, with either the inference or retrieval system being idle at any given time during the generation process~(\\textbf{O1}). Secondly, the inference latency per token increases with sequence lengths, due to the growing workloads of the attention mechanism in transformer neural networks~(\\textbf{O2}). Lastly, the retrieval process, particularly the approximate nearest neighbor search, exhibits a trade-off between search latency and search quality~(\\textbf{O3}).\n\n\\begin{figure*}[t]\n\t\\centering\n  \\includegraphics[width=0.87\\linewidth]{fig/overview.png}\n  \\vspace{-1.2em}\n  \\caption{Based on three performance-centric observations (O1$\\sim$O3), PipeRAG combines a system-aware algorithm integrating pipeline parallelism (S1) with flexible retrieval intervals (S2) and an algorithm-aware retrieval system guided by a performance model (S3).}\n  \\vspace{-1.2em}\n  \\label{fig:overview}\n\\end{figure*}\n\n\\textit{The key idea of PipeRAG is to prefetch content from databases to facilitate pipeline parallelism between the inference and retrieval systems.} This solution reduces end-to-end generation latency by allowing simultaneous inference and retrievals, effectively addressing the hardware inefficiencies identified in O1~(\\textbf{S1}). \nWe then enhance this key idea with two additional solutions.\nOn the model side, PipeRAG modifies \\textsc{Retro}'s attention mechanism to support flexible retrieval intervals, because the intervals must be carefully tuned to capitalize the efficiency of pipeline parallelism~(\\textbf{S2}). \nOn the system side, the retrieval system adopts a performance model informed by O2 and O3 to dynamically adjust the retrieval search space according to the latency expectation of the upcoming token inferences in the pipeline, thereby optimizing search quality without increasing end-to-end generation latency~(\\textbf{S3}).\n\nOur evaluation of PipeRAG, involving various evaluation datasets and using large databases based with up to 200 billion tokens, clearly illustrates its efficiency in both generation performance (latency) and generation quality (perplexity). Specifically, the quality-performance Pareto frontier of PipeRAG significantly outperforms that of \\textsc{Retro}: PipeRAG can achieve up to 2.6$\\times$ speedup in latency without compromising perplexity; alternatively, maintaining the same latency allows PipeRAG to reduce perplexity by as much as 0.93 compared to \\textsc{Retro}. These encouraging results highlight the importance of algorithm-system co-design in retrieval-augmented generation, paving the way for deploying PipeRAG in future RAG systems.\n\n\\textbf{Contributions:} We propose PipeRAG, the first algorithm-system co-design approach aimed at improving retrieval-augmented generation efficiency. Specifically:\n\\begin{itemize}\n\\vspace{-0.9em}\n\\item We design a system-aware RAG algorithm that leverages pipeline parallelism, whose efficiency is further improved by supporting flexible retrieval intervals.\n\\vspace{-0.5em}\n\\item We propose an algorithm-aware retrieval system that uses performance models to dynamically balance search quality and performance.\n\\vspace{-0.5em}\n\\item We showcase the impressive performance of PipeRAG in various datasets, demonstrating the importance of algorithm-system co-design in optimizing RAG.\n\\end{itemize}\n\n\\section{Background and Motivation}\n\\label{sec:background}\n\nSequence generation quality of LLMs can be improved through periodically retrieving from large token databases~\\cite{borgeaud2022improving, norlund2023generalization, ram2023context}. Here, periodic retrievals, instead of retrieving only once, are essential in handling potential contextual shifts during generation, such as topic changes, ensuring alignments between the retrieved content and the evolving generation context (a concrete example can be found in Appendix~\\ref{sec:append_more_background}). \n\\textsc{Retro} is a representative model in this category~\\cite{borgeaud2022improving}. \nAs illustrated in Figure~\\ref{fig:retro}, \\textsc{Retro} integrates a retrieval system with an inference system for token generation. It employs an encoder for incorporating retrieved tokens and a decoder for token generation.\n\n\\textbf{Database construction.} A \\textsc{Retro} database comprises a large collection of documents segmented into \\( n \\) chunks of tokens \\( S = (S_1, \\ldots, S_n) \\), where each chunk \\( S_i \\) spans \\( m \\) tokens. These token chunks are each converted into vector representations \\( R(S) \\). The database is then structured as a key-value store, with keys being the vector representations \\( R(S) \\) and values corresponding to the original token chunks \\( S \\), along with \\( F \\), in which \\( F_i \\) representing the immediately following token chunks of each chunk \\( S_i \\). Given a query vector \\( q \\), the database performs an approximate nearest neighbor (ANN) search to retrieve \\( k \\) closest token chunks and their immediately following chunks. \n\n\\textbf{Retrieval process.}  \\textsc{Retro} performs retrievals at regular intervals during the generation phase. Specifically, when generating a sequence of $t$ tokens \\( X = (x_1, \\ldots, x_t) \\), \\textsc{Retro} partitions \\( X \\) into \\( l \\) chunks \\( (C_1, \\ldots, C_l) \\), each consisting of \\( m \\) tokens. Consequently, token \\( x_{i} \\) belongs to chunk \\( C_{\\lceil \\frac{i}{m} \\rceil} \\). For the generation of chunk \\( C_i \\), \\textsc{Retro} employs the preceding chunk \\( C_{i-1} \\) as the query to retrieve \\( k \\) nearest neighbors \\( \\textsc{Ret}(C_{i-1}) \\) from the database.\n\n\\textbf{Attention mechanisms.} \n\\textsc{Retro} involves both decoder-to-encoder and encoder-to-decoder attention mechanisms. \nThe decoder within \\textsc{Retro} utilizes chunked cross-attention to integrate the retrieved information encoded by the encoder. To preserve causality, the generation of a chunk $C_i$ incorporates the retrieved tokens \\( \\textsc{Ret}(C_{i-1})\\) by integrating the encoder states \\( \\textsc{Enc}(\\textsc{Ret}(C_{i-1}))\\).\nOn the other hand, the \\textsc{Retro} encoder states \\( \\textsc{Enc}(\\textsc{Ret}(C_{i-1}))\\) integrates the decoder's states of the \\( \\textsc{Dec}(C_{i-1}) \\) via a standard cross-attention (CA) mechanism, such that the encoder can blend the retrieved information with the generation context.\nBecause both decoder-to-encoder and encoder-to-decoder attention mechanisms operate on a chunk-wise basis, \\textsc{Retro} avoids the excessive computational demands of attending to all previous retrieval and generation states.\n\n\\begin{figure}%[t]\n\t\\centering\n  \\includegraphics[width=0.9\\linewidth]{fig/RETRO.png}\n  \\vspace{-1em}\n  \\caption{Retrieval-augmented generation with \\textsc{Retro}.}\n  \\vspace{-2em}\n  \\label{fig:retro}\n\\end{figure}\n\n\\textbf{Motivation: improving RAG efficiency.}\nAlthough periodically retrieving tokens from a large database can effectively improve the generation quality of LLMs, frequent retrievals can account for a considerable portion of the total generation time, thereby significantly slowing down the end-to-end generation process.\n\nIn this paper, we ask the following question: \\textbf{is it possible to further enhance the efficiency of retrieval augmented generation?} Here, we conceptualize \\textit{RAG efficiency} as a Pareto frontier considering two objectives: \\textit{generation quality} and \\textit{system performance}. \nSpecifically, given a quality requirement (achieving certain perplexity), can we optimize RAG's system performance (reducing generation latency)? On the other hand, given a system performance requirement, can we improve the quality of generation? \n\n\\section{Our Approach: PipeRAG}\n\\label{sec:approach}\n\nWe propose PipeRAG, a novel retrieval augmented generation approach to improve the performance-quality Pareto frontier through an in-depth algorithm-system co-design.\nThe development of PipeRAG stems from performance-centric observations revealing (1) the \\textit{fundamental} system inefficiencies in existing RAG algorithms and (2) the distinct performance characteristics of LLM inference and retrieval systems. \nBased on these observations, PipeRAG includes (1) a system-aware RAG algorithm to address the system inefficiencies and (2) an algorithm-aware retrieval system to dynamically balance retrieval quality and latency. \n\n\\subsection{Performance-Centric Observations in RAG}\n \n\\textbf{O1: Hardware inefficiency due to RAG dependencies.} A conventional RAG process introduces dependencies between retrievals and inferences: the current generation context is used as a query to retrieve relevant token chunks stored in the database; the inference process must wait for the retrieval to finish before it can continue generating a few more tokens, until the next retrieval is triggered.\n\nA RAG system typically comprises two sub-systems: the retrieval system and the inference system, each hosted on separate hardware platforms.\nAI accelerators such as GPUs and TPUs are the ideal hardware platforms for LLM inference due to the high demands for computation and memory bandwidth during inference. \nOn the other hand, the retrieval systems consisting of large databases are usually not based on GPUs. This is because (1) the limited memory capacity of individual GPUs (GPUs adopt high-bandwidth memory that is fast but limited in capacity) makes the hosting of large databases cost-prohibitive, necessitating the setup comprising many GPUs, and (2) the communication bandwidth between the CPU and GPU is significantly lower compared to GPU's device memory bandwidth, thus the CPU-GPU solution, in which database vectors are stored in CPU-side memory and then transferred to GPUs at query time, could be exceedingly slow.\nGiven the capacity requirements, the retrieval system is typically CPU-based~\\cite{borgeaud2022improving, lewis2020retrieval}, with the database either held in substantial main memory (DRAM), or, in more budget-friendly setups, stored on disks. \n\nGiven that the two systems are based on separate hardware, the dependencies between retrievals and inferences in RAG result in significant underutilization of hardware resources. Figure~\\ref{fig:overview} illustrates this inefficiency using RETRO as a representative example: due to the dependencies, either the inference or retrieval system is idle at any given time during the generation process, leading to hardware inefficiencies. \n\n\\textbf{O2: Increasing inference time with sequence length.} \nIn a standard transformer neural network~\\cite{vaswani2017attention}, the cost of generating each new token correlates with the sequence length, rather than remaining a constant. This is due to the attention mechanism in transformers: although the workload of the fully-connected layers remains constant throughout the generation process, the cost of attention layers increases with the sequence length~\\cite{beltagy2020longformer}. Specifically, for each new token generated, the query states (Q) of the most recent token are compared against the key states (K) of all preceding tokens to calculate relevance scores. These scores are then utilized for a weighted sum over the value states (V) (note that the queries, keys, and values mentioned here under the context of transformers are distinct from those terms in RAG systems).\nConsequently, the inference cost per token can be approximated as a linear function to sequence length.\n\n\\textbf{O3: Trade-offs between retrieval quality and latency.} Large-scale vector search in RAG employs approximate nearest neighbor (ANN) search instead of exact nearest neighbor search due to the latter's prohibitive cost on large databases. In ANN search, database vectors are indexed, with popular choices including clustering-based inverted-file (IVF) indexes~\\cite{IVF} and graph-based indexes~\\cite{malkov2014approximate, malkov2018efficient}. Optionally, database vectors may also be compressed via product quantization (PQ)~\\cite{PQ} to shrink database sizes and reduce memory bandwidth usage at query time at the expense of search accuracy. During a search, a query vector is only compared against a subset of database vectors selected by the index. \n\nRegardless of the index types, there exists a \\textit{fundamental} trade-off between search quality and latency in ANN search. Typically, the index first directs the search towards those database vectors that are most likely to be the nearest neighbors of the query vector, and then gradually expands the search space. The number of database vectors scanned per query can be directly or indirectly controlled by ANN search hyper-parameters. Expanding the search space would enhance the probability of finding the query vector's true nearest neighbors in the database (improved search quality), but also would also lead to higher latency (lower search performance) due to the greater number of comparisons between query vectors and database vectors.\n\nFigure~\\ref{fig:overview} visualizes the relationship between search quality and latency~\\cite{PQ}. As the search space expands (number of scanned database vectors), the search quality (recall of the retrieval) gradually improves until reaching a plateau where the nearest neighbors are likely found. Simultaneously, the search cost (latency) grows linearly with the search space, with an initial cost of scanning the index (which could be zero in some graph-based indexes). \n\n\\subsection{Algorithm-System Co-deisgn in PipeRAG}\n\nGiven the aforementioned performance-centric observations, we propose PipeRAG, an algorithm-system co-design approach aimed at enhancing RAG's performance-quality Pareto frontier. \nPipeRAG addresses the \\textit{fundamental} issue of hardware inefficiency (O1) by employing pipeline parallelism (S1) and allowing flexible retrieval intervals (S2). Leveraging the distinct performance characteristics of the inference and retrieval sub-systems (O2, O3), PipeRAG further offers an option to enable automatic search space selection within the retrieval system, facilitating high-quality generation without introducing additional generation latency.\n\n\\textbf{S1: Pipeline parallelism across RAG sub-systems.} \nBecause the hardware under-utilization issue in RAG is caused by dependencies between retrievals and inferences, our first solution is about revisiting RAG algorithms to enable pipeline parallelism: the retrievals and inferences should be executed concurrently, thus overlapping their execution latency and improving hardware utilization. \n\nTo facilitate pipeline parallelism, we relax the RAG dependencies as illustrated in Figure~\\ref{fig:overview}: instead of depending on the content retrieved using the query representing the most recent generation context (the latest generated tokens),\nthe inference process can utilize a slightly older, or \\textit{stale}, query window to \\textit{prefetch} content from the database.\nThe intuition here is that if the stale query window closely aligns with the latest generation context, it is likely to retrieve content similar to that obtained using the most recent query tokens.\nOnce the dependency constraint is relaxed, retrievals can be proactively initiated to \\textit{prefetch} content from the database, thus enabling pipeline parallelism as shown in Figure~\\ref{fig:overview}.\n\nFormally, when generating token chunk \\( C_{j+1} \\), PipeRAG does not use the immediately preceding chunk as the query \\( Q = C_j = (x_{jm}, \\ldots, x_{jm + m - 1}) \\) to retrieve \\( \\textsc{Ret}(Q) \\). Instead, it opts for a stale token window \\( \\hat{Q} = (x_{jm - s}, \\ldots, x_{jm + m - 1 - s}) \\) as an approximate query, offset by \\( s \\) tokens from the latest query window. Subsequently, \\( \\hat{\\textsc{Ret}(Q)} = \\textsc{Shift}(\\textsc{Ret}(\\hat{Q}), s) \\) serves as the approximation of \\( \\textsc{Ret}(Q) \\). Given that the stale query is \\( s \\) tokens behind the most recent generation context, the retrieved results \\( \\textsc{Ret}(\\hat{Q}) \\) are correspondingly left-shifted by \\( s \\) tokens. This shift ensures that the first \\( s \\) retrieved tokens, which are likely less relevant for the upcoming generation due to staleness, are excluded while maintaining the overall length of retrieval tokens. Note that the concept of stale query windows does not apply for the initial retrieval, which is conducted using the first chunk \\( C_{1} \\), as illustrated in Figure~\\ref{fig:overview}.\n\n\\begin{figure}[t]\n\t\\centering\n  \\includegraphics[width=1.0\\linewidth]{fig/Attention.png}\n  \\vspace{-2em}\n  \\caption{Attention mechanisms and query windows in PipeRAG.}\n  \\vspace{-1em}\n  \\label{fig:attention}\n\\end{figure}\n\n\\textbf{S2: Flexible retrieval intervals.} \n\\textsc{Retro} utilizes a fixed retrieval interval of \\( m=64 \\), aligning with the generation chunk size, database token chunk size, and query window size. \nHowever, the effectiveness of pipeline parallelism (S1) is maximized when the retrieval and inference subsystems have similar latencies --- generating \\( m=64 \\) tokens does not always consume similar time as one retrieval. \n\nIn order to improve the effectiveness of pipeline parallelism, PipeRAG supports alternative retrieval intervals \\( m' \\) and modifies \\textsc{Retro}'s attention mechanism accordingly.\nHere, \\( m' \\) remains constant during a single generation process but can vary from the default value of 64. When using shorter intervals, such as \\( m'=32 \\), the staleness of queries is also reduced (\\( s=32 \\), thereby improving the quality of the retrieved content to more closely resemble that obtained from a non-stale query.\nFigure~\\ref{fig:attention} illustrates the differences in retrievals and attention mechanisms between \\textsc{Retro} and PipeRAG, taking \\( m'=32 \\) as an example. As shown in the figure, while a query \\( Q_i \\) still has a window size of \\( m=64 \\) tokens, the retrieval interval is halved. This necessitates adjustments in the attention regions to align with these modified intervals. For encoder-to-decoder attention, the attention is directed from the retrieved chunk to the query window whose position is different from that of \\textsc{Retro}. For decoder-to-encoder attention, the generation of chunk \\( C_{j+1} \\) of length \\( m' \\) applies chunked cross-attention on \\( \\textsc{Ret}(Q_{j-1}) \\).\n\n\\textbf{S3: Performance-model-driven retrievals.} \nPipeRAG has the potential to match the generation latency of LLMs that do not introduce retrievals, especially when the retrievals and inferences are completely overlapped in the pipeline.\nHowever, achieving this ideal overlap is challenging because of the distinct performance characteristics of the retrieval and inference systems as introduced in O2 and O3. \n\nTo address this, we propose a performance-model-driven retrieval system to automatically enable perfectly overlapped pipeline windows.\nIn this context, a performance model refers to any model (not limited to neural networks) designed to predict the performance characteristics of a system.\nSpecifically, the retrieval system takes the generation states as inputs and automatically adjusts the search space using performance models, ensuring that the retrieval latency can be hidden by the generation latency of the next token chunk. By maximizing the search space under the latency constraint, the retrieval quality is also maximized without incurring extra generation latency.\n\nThe inference performance can be modeled as follows. \nThe time required to generate a token chunk is \\( T_{C} = T_{\\textsc{Enc}} + T_{\\textsc{Dec}} \\). The latency of encoder inference is related to the number of retrieved neighbors and the number of tokens per neighbor, while the decoder inference latency depends on the current sequence length and the chunk size (O2). \n\nOn the other hand, the retrieval latency can be represented modeled as \\( T_{\\textsc{Ret}} = T_{Network} + T_{EncQuery} + T_{ScanIndex} + T_{ScanVec}\\), encompassing the time spent on network communications, encoding the query tokens as vectors, scanning the vector index, and scanning a subset of database vectors. \nIn this paper, we apply the widely-adopted IVF-PQ vector search algorithm~\\cite{PQ} that combines a clustering-based inverted-file (IVF) index with product quantization (PQ). The IVF index clusters the database to \\( nlist \\) IVF lists. At query time, \\( nprobe \\) out of the \\( nlist \\) IVF lists are selected to scan (database vectors within the selected lists are compared to the query vectors). \n\nGiven that the performance of both retrievals and inferences are related to hardware, we measure and model their performance on the deployment hardware. We record the time consumption of both encoder and decoder inferences with various input sequence lengths. For retrieval, we model the relationship between \\( nprobe \\) and search latency using linear regression, given that \\( nprobe \\) is approximately proportional to the number of scanned database vectors.\n\nThe retrieval system then leverages these performance models to predict the maximal search space, indicated by $nlist$, given the latency constraint for generating the next token chunk, ensuring that \\( T_{\\textsc{Ret}} \\leq T(C) \\). \nSince the \\( T(C) \\) can be easily obtained from the recorded performance numbers, we can then derive the maximal \\( nprobe \\) during the search based on the retrieval performance model.\n\nWhile an alternative approach to achieve a perfectly overlapped pipeline is adjusting the retrieval intervals in the inference system, we rule out this option due to generalizability concerns. In future deployment scenarios, a retrieval system may serve multiple inference systems. Thus, the retrieval performance is impacted by the number of concurrent queries being processed. In this case, it could be challenging for the inference system to accurately predict the retrieval latency, as it lacks the information about the retrieval system's workload at the moment. Therefore, it is the retrieval system, instead of the inference system, that should be responsible for constructing a perfectly overlapped pipeline via performance modeling.\n\n\\begin{figure*}[t]\n\\begin{subfigure}\n    \\centering\n    \\includegraphics[width=0.32\\linewidth]{fig/paper_ppl_db_size_wikipedia_chunk9_1K.png}\n\\end{subfigure}\n\\hfill\n\\begin{subfigure}\n    \\centering\n    \\includegraphics[width=0.32\\linewidth]{fig/paper_ppl_db_size_realnews_chunk31_1K.png}\n\\end{subfigure}\n\\hfill\n\\begin{subfigure}\n    \\centering\n    \\includegraphics[width=0.32\\linewidth]{fig/paper_ppl_db_size_c4_chunk1023_1K.png}\n\\end{subfigure}\n\\hfill\n\n  \\vspace{-1em}\n  \\caption{The effect of database sizes and retrieval strategies on language modeling perplexity (lower perplexity means higher quality).}\n  \\label{fig:eval_dbsize}\n\\end{figure*}\n\n\\begin{figure*}%[t]\n\\begin{subfigure}\n    \\centering\n    \\includegraphics[width=0.32\\linewidth]{fig/ppl_eval_wikipedia_chunk9_1K_db_c4_chunk_0_to_999.png}\n\\end{subfigure}\n\\hfill\n\\begin{subfigure}\n    \\centering\n    \\includegraphics[width=0.32\\linewidth]{fig/ppl_eval_realnews_chunk31_1K_db_c4_chunk_0_to_999.png}\n\\end{subfigure}\n\\hfill\n\\begin{subfigure}\n    \\centering\n    \\includegraphics[width=0.32\\linewidth]{fig/ppl_eval_c4_chunk1023_1K_db_c4_chunk_0_to_999.png}\n\\end{subfigure}\n\\hfill\n\n  \\vspace{-1em}\n  \\caption{Perplexity of retrieval-augmented generation when applying various retrieval intervals and search space configurations (\\( nprobe \\)).}\n  \\vspace{-1em}\n  \\label{fig:eval_ppl}\n\\end{figure*}\n\n\\section{Evaluation}\n\\label{sec:evaluation}\n\n\\subsection{Experimental Setup}\n\nWe briefly introduce our experimental setup below and leave more details in Appendix~\\ref{sec:append_eval_setup}.\n\n\\textbf{Database.}\nOur token database was constructed from the C4 corpus containing deduplicated English documents. Adhering to~\\citet{borgeaud2022improving}, we segmented the documents into chunks of \\( m=64 \\) tokens, yielding a total of three billion chunks, and set the number of nearest neighbors per retrieval as \\(k=2 \\). Following~\\citet{norlund2023generalization}, we transformed each token chunk into a 384-dimensional vector using a sentence transformer\\cite{reimers2019sentence}. \n\n\\textbf{Model.}\nWe developed PipeRAG based on the \\textsc{Retro} checkpoint with 582M parameters provided by~\\citet{norlund2023generalization}, the only available pre-trained \\textsc{Retro} model when we conducted the experiments. \n\n \n\\textbf{Evaluation Set.}\nTo evaluate language modeling quality, we used the Wikipedia dataset~\\cite{wikipedia}, the RealNews subset of the C4 dataset, and C4's English document subset~\\cite{dodge2021documenting, raffel2020exploring}.\n\n\\textbf{Software.}\nFor model inference, we adopted the ONNX runtime, which, according to our experiments, achieves 2 to 3 times speedup in latency over PyTorch. For retrieval, we used the Faiss library~\\cite{johnson2019billion} and the IVF-PQ vector search algorithm. Communication between the inference and retrieval systems was managed via gRPC.\n\n\\textbf{Hardware.}\nFor model inference, we used an NVIDIA A100 GPU (40 GB), while the retrievals were conducted on a server equipped with Intel(R) Xeon(R) Platinum 8259CL CPUs @2.50GHz (48 cores) and 384 GB memory. \n\n\\subsection{Perplexity Evaluation}\n\nFigure~\\ref{fig:eval_dbsize} shows the impact of various retrieval strategies across different database sizes. This comparison includes PipeRAG, \\textsc{Retro}, retrieval-augmented generation with only one retrieval at the beginning of generation, and generation without retrieval. For the last two strategies, \\textsc{Retro} still serves as the base model. \nAs indicated in the figure, retrieval, especially on large databases, plays a crucial role in improving generation quality (lower perplexity is better). Across all evaluated datasets, generation without retrieval performs the worst, followed by only retrieving once, showing the effectiveness of periodic retrieval in \\textsc{Retro}. Additionally, perplexity decreases as the dataset size increases, highlighting the importance of comprehensive content coverage in the databases. Notably, when pairing with the largest database, PipeRAG outperforms \\textsc{Retro} in generation quality, as we will analyze in greater detail later on. \n\n\\textit{From now on, we report results in generation quality and performance based on the full (largest) database}, as using subsets significantly compromises generation quality.\n\nFigure~\\ref{fig:eval_ppl} compares the perplexity between PipeRAG and \\textsc{Retro} across various retrieval configurations. We assess PipeRAG with different retrieval intervals, setting the search space through \\( nprobe \\), which represents the number of scanned vector lists per query in the IVF index. As shown in Figure~\\ref{fig:eval_ppl}, both PipeRAG and \\textsc{Retro} show reduced perplexity with an expanded search space, which leads to better search quality (O3).\n\n\\begin{tcolorbox}[\n    enhanced,\n    arc=2mm, % Adjust the roundness of the corners\n    outer arc=2mm, % Adjust the roundness of the corners for the shadow as well\n    boxrule=0.8pt, % Border thickness\n    colframe=black, % Border color\n    colback=white, % Background color\n    boxsep=0pt, % Separation between content and box border\n    drop shadow southeast, % Drop shadow to bottom right\n]\n\n\\textbf{Takeaway 1:} The quality of retrieval-augmented generation benefits from higher retrieval quality achieved by expanding the search space during vector search.\n\\end{tcolorbox}\n\n\\begin{figure*}[t]\n\n\\begin{subfigure}\n    \\centering\n    \\includegraphics[width=0.32\\linewidth]{fig/ppl_pareto_eval_wikipedia_chunk9_1K.png}\n\\end{subfigure}\n\\hfill\n\\begin{subfigure}\n    \\centering\n    \\includegraphics[width=0.32\\linewidth]{fig/ppl_pareto_eval_realnews_chunk31_1K.png}\n\\end{subfigure}\n\\hfill\n\\begin{subfigure}\n    \\centering\n    \\includegraphics[width=0.32\\linewidth]{fig/ppl_pareto_eval_c4_chunk1023_1K.png}\n\\end{subfigure}\n\n  \\vspace{-1em}\n  \\caption{PipeRAG significantly outperforms \\textsc{Retro} on the latency-perplexity Pareto frontier (lower latency and perplexity are better).}\n  \\vspace{-0.5em}\n  \\label{fig:eval_e2e}\n\\end{figure*}\n\n\\begin{table*}\n\\centering \n\n\\caption{Performance-driven retrieval (S3) facilitates latency comparable to non-retrieval models while significantly reducing perplexity. Values in parentheses indicate the difference compared to the baseline model without retrieval (lower latency and perplexity are better).} \n\n\\vspace{0.5em}\n\\scalebox{0.78}{\n\\begin{tabular}{\nL{4em} \nM{5em} M{6em} M{14em} \nM{0em}\nM{5em} M{6em} M{14em} \n}\\toprule\n\\multirow{2}{*}{Eval Set} & \\multicolumn{3}{c}{Latency (s)} & \\phantom{}& \\multicolumn{3}{c}{Perplexity} \\\\\n\\cmidrule{2-4} \\cmidrule{6-8}\n & No retrieval & RETRO & Performance-driven retrieval (S3) &\\phantom{}& No retrieval & RETRO & Performance-driven retrieval (S3) \\\\\n\\midrule\nWikipedia & 9.35 & 14.59 (+5.23) & 10.34 (\\textbf{+0.99})  && 16.74 & 13.49 (-3.25) & 13.47 (\\textbf{-3.28})    \\\\ \nRealNews & 9.35 & 12.36 (+3.00) & 10.58 (\\textbf{+1.22})  && 17.37 & 14.94 (-2.43) & 14.87 (\\textbf{-2.50})   \\\\ \nC4 & 9.35 & 11.13 (+1.78) & 10.58 (\\textbf{+1.22})  && 24.18 & 19.48 (-4.70) & 19.36 (\\textbf{-4.82})   \\\\ \n\n\\bottomrule \n\\end{tabular} }\n\n\\label{tab:dynamic_nprobe} \n\n\\end{table*}\n\nFurthermore, PipeRAG demonstrates superior generation quality over \\textsc{Retro}, particularly when using shorter retrieval intervals of no more than 32 (Figure~\\ref{fig:eval_ppl}). This advantage is attributed to PipeRAG's revised attention mechanism. Shorter intervals not only reduce query staleness (equivalent to the interval) but improve the content integration frequency, in contrast to \\textsc{Retro} with a fixed interval of 64. The increased retrieval frequency in PipeRAG does not necessarily add to generation latency thanks to the pipeline parallelism, a point we will further elaborate on.\n\n\\begin{tcolorbox}[\n    enhanced,\n    arc=2mm, % Adjust the roundness of the corners\n    outer arc=2mm, % Adjust the roundness of the corners for the shadow as well\n    boxrule=0.8pt, % Border thickness\n    colframe=black, % Border color\n    colback=white, % Background color\n    boxsep=0pt, % Separation between content and box border\n    drop shadow southeast, % Drop shadow to bottom right\n]\n\n\\textbf{Takeaway 2:} PipeRAG can surpass \\textsc{Retro} in generation quality when using shorter retrieval intervals backed by PipeRAG's attention mechanism.\n\\end{tcolorbox}\n\n\\subsection{RAG Efficiency: Performance-Quality Trade-offs}\n\nIn this section, we assess the efficiency of PipeRAG. Our primary performance metric is the end-to-end latency to generate a 1024-token sequence, which we reported by taking the median latency of five individual runs. \n\nFigure~\\ref{fig:eval_e2e} compares the Pareto frontiers of the performance-quality (latency-perplexity) trade-offs between PipeRAG and \\textsc{Retro}.\nFor \\textsc{Retro}, we manipulate the search space by tuning \\( nprobe \\).\nFor PipeRAG, we explore a range of retrieval intervals in conjunction with either a fixed search space or the performance-model-driven search space selection (S3).  \nAcross all datasets, the Pareto frontier of PipeRAG demonstrates significant advantages over \\textsc{Retro}, as shown in Figure~\\ref{fig:eval_e2e}. For example, PipeRAG can attain up to a 2.6$\\times$ reduction in latency while maintaining or reducing perplexity relative to \\textsc{Retro}; alternatively, under the same latency constraint, PipeRAG can lower perplexity by as much as 0.93 points compared to \\textsc{Retro}.\n\n\\begin{tcolorbox}[\n    enhanced,\n    arc=2mm, % Adjust the roundness of the corners\n    outer arc=2mm, % Adjust the roundness of the corners for the shadow as well\n    boxrule=0.8pt, % Border thickness\n    colframe=black, % Border color\n    colback=white, % Background color\n    boxsep=0pt, % Separation between content and box border\n    drop shadow southeast, % Drop shadow to bottom right\n]\n\\textbf{Takeaway 3:} PipeRAG shows impressive efficiency, achieving up to 2.6$\\times$ speedup in latency over \\textsc{Retro} without compromising generation quality.\n\\end{tcolorbox}\n\nTable~\\ref{tab:dynamic_nprobe} demonstrates the effectiveness of the proposed performance-model-driven retrieval system. The objective of the performance model is to dynamically maximize search quality while minimizing additional performance costs.\nTo evaluate this, we compare the generation latency and quality of PipeRAG applying performance-model-driven retrievals to that of \\textsc{Retro} as well as the same base \\textsc{Retro} model without invoking retrievals.\nAs shown in Table~\\ref{tab:dynamic_nprobe}, PipeRAG achieves a notable reduction in perplexity (2.50$\\sim$4.82) with a minor increase in performance overhead (merely 10.6\\%$\\sim$13.2\\% in latency overhead), outperformance \\textsc{Retro} in both latency and perplexity. \nThis slight increase in latency is attributed to the extra computational workload of the cross-attention mechanism when integrating the retrieved content from the encoder. \n\n\\begin{tcolorbox}[\n    enhanced,\n    arc=2mm, % Adjust the roundness of the corners\n    outer arc=2mm, % Adjust the roundness of the corners for the shadow as well\n    boxrule=0.8pt, % Border thickness\n    colframe=black, % Border color\n    colback=white, % Background color\n    boxsep=0pt, % Separation between content and box border\n    drop shadow southeast, % Drop shadow to bottom right\n]\n\n\\textbf{Takeaway 4:} \nLeveraging the performance-model-driven retrieval system, PipeRAG can achieve comparable latency to models without retrievals while significantly improving generation quality. \n\\end{tcolorbox}\n\n\\begin{figure}%[t]\n\t\\centering\n  \\includegraphics[width=1.0\\linewidth]{fig/ppl_alternative_system_performance_eval_c4_chunk1023_1K_db_c4_chunk_0_to_999.png}\n  \\vspace{-1.5em}\n  \\caption{Trends in PipeRAG efficiency when deployed on future hardware that enables faster retrieval or inference.}\n  \\vspace{-1em}\n  \\label{fig:different_performance}\n\\end{figure}\n\nFigure~\\ref{fig:different_performance} illustrates the projected performance trends of PipeRAG across a range of system and hardware configurations. Considering the rapid advancements in hardware accelerators, we expect shifts in performance of both retrieval and inference systems over years. To analyze PipeRAG's effectiveness on future hardware, we model the latency of PipeRAG and \\textsc{Retro} when using faster retrieval or inference systems, with the methodology described in Appendix~\\ref{sec:performance_trend}. \nThe first row of Figure~\\ref{fig:different_performance} demonstrates the generation latency when the inference system becomes 4$\\times$ and 16$\\times$ faster, while the second row examines the effects of accelerated retrieval. Across all scenarios, PipeRAG achieves superior efficiency compared to \\textsc{Retro}. When either system experiences an order of magnitude speedup (e.g., 16$\\times$), however, the benefits of applying PipeRAG become less significant. This trend aligns with our expectations, as the effectiveness of pipeline parallelism peaks when both system components have comparable latencies and diminishes when one component significantly outpaces the other.\n\n\\begin{tcolorbox}[\n    enhanced,\n    arc=2mm, % Adjust the roundness of the corners\n    outer arc=2mm, % Adjust the roundness of the corners for the shadow as well\n    boxrule=0.8pt, % Border thickness\n    colframe=black, % Border color\n    colback=white, % Background color\n    boxsep=0pt, % Separation between content and box border\n    drop shadow southeast, % Drop shadow to bottom right\n]\n\n\\textbf{Takeaway 5:} PipeRAG outperforms \\textsc{Retro} in efficiency across different hardware, though the extent of improvements depends on sub-system performance.\n\\end{tcolorbox}\n\n\\subsection{Ablation Study}\n\nSince PipeRAG not only introduces pipeline parallelism but also modifies \\textsc{Retro}'s attention mechanism to maximize the effectiveness of pipelining, it is natural to ask how a baseline model would perform if it integrates the same attention mechanism. To illustrate the effectiveness of pipeline parallelism itself, we compare PipeRAG with an enhanced variant of \\textsc{Retro}, named \\textsc{Retro+}, which also supports flexible retrieval intervals by integrating PipeRAG's attention mechanism.  \n\nFigure~\\ref{fig:eval_e2e_retro_flexible_interval} compares the performance-quality Pareto-frontier between PipeRAG and \\textsc{Retro+}. \nBoth models use retrieval intervals ranging from 8 to 64.\nWhile \\textsc{Retro+}, benefiting from flexible intervals, matches PipeRAG in perplexity, PipeRAG consistently achieves lower latency given the same perplexity. This is attributed to the proposed pipeline parallelism: PipeRAG effectively hides the retrieval latencies by overlapping them with generation latencies, whereas for \\textsc{Retro+}, more frequent retrievals lead to increased total generation latency. More detailed comparisons between PipeRAG and \\textsc{Retro+} under identical retrieval intervals (corresponding to the same number of database requests) can be found in Appendix~\\ref{sec:append_more_results}.\n\n\\begin{tcolorbox}[\n    enhanced,\n    arc=2mm, % Adjust the roundness of the corners\n    outer arc=2mm, % Adjust the roundness of the corners for the shadow as well\n    boxrule=0.8pt, % Border thickness\n    colframe=black, % Border color\n    colback=white, % Background color\n    boxsep=0pt, % Separation between content and box border\n    drop shadow southeast, % Drop shadow to bottom right\n]\n\n\\textbf{Takeaway 6:} Pipeline parallelism is essential to achieve superior RAG efficiency, as PipeRAG outperforms \\textsc{Retro+} that supports flexible retrieval intervals using PipeRAG's attention mechanism.\n\\end{tcolorbox}\n\n\\begin{figure}[t]\n\n\\begin{subfigure}\n    \\centering\n    \\includegraphics[height=6.2em]{fig/ppl_RETRO_flexible_interval_pareto_eval_wikipedia_chunk9_1K.png}\n\\end{subfigure}\n\\hfill\n\\begin{subfigure}\n    \\centering\n    \\includegraphics[height=6.2em]{fig/ppl_RETRO_flexible_interval_pareto_eval_realnews_chunk31_1K.png}\n\\end{subfigure}\n\\hfill\n\\begin{subfigure}\n    \\centering\n    \\includegraphics[height=6.2em]{fig/ppl_RETRO_flexible_interval_pareto_eval_c4_chunk1023_1K.png}\n\\end{subfigure}\n\n  \\vspace{-1em}\n  \\caption{Even if the baseline model supports flexible retrieval intervals (\\textsc{Retro+}), PipeRAG still significantly outperforms it in efficiency thanks to the proposed pipeline parallelism.}\n  \\vspace{-1em}\n  \\label{fig:eval_e2e_retro_flexible_interval}\n\\end{figure}\n\n\\section{Related Work}\n\nTo the best of our knowledge, PipeRAG represents the first endeavor to enhance RAG efficiency through an in-depth algorithm-system co-design, diverging from existing RAG research that mainly focuses on improving generation quality. We now briefly introduce these related works.\n\nSince knowledge is primarily retrieved rather than encoded in the LLM's parameters, RALMs, even with LLMs of one to two orders of magnitude fewer parameters, can achieve superior or comparable performance to conventional LLMs on various natural language processing (NLP) tasks~\\cite{lewis2020pre, izacard2022few, komeili2021internet, guu2020retrieval}. While the generation may only involve a single passage retrieval at the beginning~\\cite{lewis2020retrieval, izacard2020leveraging, sachan2021end}, the generated sequence may gradually diverge from the initially retrieved contents as the sequence grows longer. Thus, a more generaral RAG approach involves multiple retrievals during text generation to improve token generation quality~\\cite{ram2023context, borgeaud2022improving}.\n\nAnother line of RAG research emphasizes token-level retrievals, exemplified by kNN-LM~\\cite{khandelwal2019generalization} and subsequent works~\\cite{khandelwal2020nearest, meng2021fast, xu2023nearest}. In these models, during each token generation step, the hidden state of the last layer is used as a query to retrieve contextually similar passages as well as their subsequent tokens (with a retrieval interval of one). The next token of the current context is then predicted by interpolating the model's next-token probability distribution with that of the retrieved contents. There are also arguments suggesting that token-level content integration may not be as effective as integrating longer passages~\\cite{wang2023knn}.\n\\section{Conclusion}\n\\label{sec:conclusion}\n\nRetrieval-augmented generation presents both opportunities and efficiency challenges, due to the significant overheads when retrieving from large databases. \nWe propose PipeRAG, a novel RAG approach that improves generation efficiency by adopting pipeline parallelism, allowing flexible retrieval intervals, and dynamically adjusting retrieval quality via performance modeling. PipeRAG achieves up to 2.6$\\times$ speedup over \\textsc{Retro} without compromising generation quality. This not only establishes a solid foundation for integrating pipeline parallelism in future RAG systems but also showcasing future research opportunities in optimizing RAG through algorithm-system co-design.\n\n\\newpage\n\\section*{Impact Statements}\n\\label{sec:impact}\n\nThis paper focuses on enhancing the system efficiency of retrieval-augmented generation, aiming to reduce both energy consumption and carbon emissions during large-scale LLM inference. As our work does not involve training new models, we anticipate minimal ethical concerns or adverse societal impacts.\n\n\\newpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2305.14283v3.tex",
        "arXiv-2310.05029v1.tex",
        "arXiv-2403.05676v1.tex"
    ],
    "group_id": "group_35",
    "response": "### Summary of Recent Advances in Retrieval-Augmented Large Language Models\n\n#### Title: Recent Advances in Retrieval-Augmented Large Language Models: Query Rewriting, Interactive Reading, and Algorithm-System Co-design\n\n#### Introduction\nLarge Language Models (LLMs) have revolutionized natural language processing (NLP) tasks, showcasing remarkable abilities in understanding and generating human-like text. However, their reliance on vast internal parameter sets often leads to issues such as hallucination and temporal misalignment, where the model's predictions may not align with real-world knowledge or recent events. To address these challenges, researchers have increasingly turned to retrieval-augmented LLMs (RAGs), where external knowledge is integrated to enhance the model's factual accuracy and contextual relevance. This approach typically involves a two-step process: retrieval of relevant documents followed by reading and generating responses based on the retrieved information. However, this standard \\textit{retrieve-then-read} framework faces several limitations, including the inefficiency of query formulation and the challenge of processing long texts within the context window constraints of LLMs. This summary explores three recent papers that tackle these issues through innovative methods: query rewriting, interactive reading, and algorithm-system co-design.\n\nHistorically, RAGs have been designed to integrate external knowledge into LLMs, improving their performance on knowledge-intensive tasks. Early studies focused on sparse and dense retrieval methods, where the retriever and reader were both trainable models of significant size. However, with the advent of large-scale LLMs that are often used as black-box readers, the traditional RAG framework has become less viable due to the computational and resource constraints. Recent advancements have shifted towards leveraging black-box LLMs as readers and integrating them with retrieval systems in a more efficient manner. Despite these improvements, the field still grapples with challenges such as query formulation, handling long texts, and optimizing the retrieval process to ensure efficient and high-quality generation.\n\n#### Paper 1: Query Rewriting for Retrieval-Augmented Large Language Models\n\nThis paper introduces a novel framework called \\textit{Rewrite-Retrieve-Read} (R3) to enhance retrieval-augmented LLMs. The core innovation lies in the addition of a query rewriting step before the standard \\textit{retrieve-then-read} process. The rewriting step aims to bridge the gap between the input text and the required knowledge, thereby improving the retrieval process. The authors propose two methods for query rewriting: using a black-box LLM to rewrite queries and training a small language model (T5-large) as a rewriter through reinforcement learning (RL). The rewriter is initially trained on a pseudo dataset generated by the LLM, and then fine-tuned using RL to optimize its performance based on the feedback from the LLM reader. The framework is evaluated on open-domain Question-Answering (QA) tasks and multiple-choice QA tasks, demonstrating consistent performance gains over traditional RAG methods.\n\nThe paper's main contributions include:\n- **Query Rewriting**: The introduction of a query rewriting step to improve retrieval effectiveness.\n- **Trainable Rewriter**: The proposal of a small, trainable language model to perform query rewriting, which can be fine-tuned using RL.\n- **Evaluation Framework**: The evaluation of the proposed framework on various QA tasks, showcasing its effectiveness.\n\n#### Paper 2: Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading\n\nThis work presents \\sysname{}, a method designed to enhance the processing of long texts by large language models (LLMs). Unlike traditional approaches that either extend the context window or use recurrence to manage long sequences, \\sysname{} treats the LLM as an interactive agent, allowing it to navigate a tree-like structure of summaries to retrieve relevant information. The tree is constructed by segmenting the long text into manageable chunks and summarizing each segment into nodes, which are then further summarized into higher-level nodes. Upon receiving a query, the LLM navigates this tree, deciding which segments to read based on its reasoning capabilities. This method improves the LLM's ability to handle long texts by reducing the need for the LLM to process the entire sequence at once, thereby enhancing both performance and explainability.\n\nThe paper's main contributions include:\n- **Memory Tree Construction**: The development of a structured memory tree to facilitate efficient retrieval of long texts.\n- **Interactive Reading**: The use of iterative prompting to enable the LLM to navigate the memory tree and retrieve relevant information.\n- **Working Memory**: The incorporation of working memory to maintain context throughout the navigation process, ensuring that the LLM does not lose sight of the overall context.\n\n#### Paper 3: PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design\n\nPipeRAG introduces a new approach to optimize retrieval-augmented generation (RAG) by co-designing the algorithm and the system. The method leverages pipeline parallelism to enable concurrent retrieval and generation processes, thereby reducing the end-to-end generation latency. Additionally, PipeRAG supports flexible retrieval intervals and employs a performance model to dynamically adjust the retrieval search space based on the current generation state and hardware performance. This ensures that the retrieval process is optimized for both quality and latency, allowing PipeRAG to achieve superior performance compared to traditional RAG methods.\n\nThe paper's main contributions include:\n- **Pipeline Parallelism**: The use of pipeline parallelism to enable concurrent retrieval and generation processes.\n- **Flexible Retrieval Intervals**: The support for varying retrieval intervals to optimize the retrieval process.\n- **Performance Model**: The development of a performance model to dynamically adjust the retrieval search space, balancing quality and latency.\n\n#### Commonalities and Innovations\n\nAll three papers aim to enhance the performance of retrieval-augmented LLMs by addressing specific challenges:\n- **Query Rewriting**: Paper 1 introduces a novel query rewriting step to improve the retrieval process, while Paper 2 and Paper 3 do not explicitly focus on query rewriting.\n- **Handling Long Texts**: Paper 2 and Paper 3 both address the issue of long texts by developing methods that allow the LLM to process information in smaller, more manageable chunks.\n- **Efficiency Improvements**: Paper 1 and Paper 3 focus on improving the efficiency of the retrieval-augmented framework, with Paper 1 doing so through a trainable rewriter and Paper 3 through algorithm-system co-design.\n\nThe innovations in each paper are:\n- **Paper 1**: The introduction of a query rewriting step and the use of RL to fine-tune a small language model as a rewriter.\n- **Paper 2**: The development of an interactive reading method that constructs a memory tree and enables the LLM to navigate it based on its reasoning capabilities.\n- **Paper 3**: The co-design of an algorithm and system to optimize the retrieval process, ensuring that retrieval and generation processes are balanced for efficiency.\n\n#### Comparison of Results and Discussion\n\nThe evaluation of the proposed methods in each paper reveals significant performance gains:\n- **Paper 1**: On open-domain QA tasks, the \\textit{Rewrite-Retrieve-Read} (R3) framework shows consistent improvements over traditional RAG methods, with the trainable rewriter outperforming the LLM rewriter on the PopQA dataset. For multiple-choice QA tasks, the R3 framework also demonstrates superior performance, especially when using Vicuna-13B as the reader.\n- **Paper 2**: \\sysname{} outperforms traditional RAG methods on long-context QA tasks, achieving higher accuracy and lower latency. The method's effectiveness is highly dependent on the LLM's reasoning capabilities, with larger models performing better.\n- **Paper 3**: PipeRAG achieves up to 2.6$\\times$ speedup in end-to-end generation latency while maintaining or improving perplexity scores. The method's performance is particularly notable when using shorter retrieval intervals and larger databases.\n\nThe results highlight the effectiveness of each proposed method in addressing specific challenges:\n- **Paper 1**: The query rewriting step is crucial for improving retrieval effectiveness, especially when using a black-box LLM as the reader.\n- **Paper 2**: The interactive reading method is effective for long texts, as it allows the LLM to focus on relevant segments and recover from errors in navigation.\n- **Paper 3**: The algorithm-system co-design approach is effective for optimizing the retrieval process, ensuring that retrieval and generation are balanced for efficiency.\n\n#### Conclusion\nThe three papers contribute significantly to the field of retrieval-augmented LLMs, each addressing a unique challenge:\n- **Paper 1** introduces a query rewriting step to improve retrieval effectiveness, demonstrating the potential of using small, trainable models to adapt to black-box LLMs.\n- **Paper 2** proposes an interactive reading method that enhances the LLM's ability to handle long texts, showcasing the importance of reasoning and working memory in retrieval-augmented frameworks.\n- **Paper 3** presents an algorithm-system co-design approach to optimize retrieval-augmented generation, highlighting the benefits of concurrent retrieval and generation processes.\n\nThese contributions not only improve the performance of retrieval-augmented LLMs but also open up new avenues for future research. Future work could explore the integration of multiple techniques, such as query rewriting and interactive reading, to further enhance the efficiency and effectiveness of RAG frameworks. Additionally, the development of more sophisticated performance models and the exploration of alternative data structures for memory navigation could lead to even greater improvements in RAG systems.\n\n#### Future Research Directions\n- **Combining Techniques**: Investigate the potential of combining query rewriting and interactive reading to create a more robust RAG framework.\n- **Advanced Performance Models**: Develop more sophisticated performance models that can dynamically adjust retrieval and generation processes based on real-time hardware performance.\n- **Alternative Data Structures**: Explore the use of alternative data structures, such as hash-based or graph-based structures, for memory navigation in interactive reading methods.\n- **Scalability**: Address the scalability issues of the memory tree construction and retrieval processes in Paper 2 and Paper 3, ensuring that these methods can handle extremely long texts and large databases efficiently.\n\nBy addressing these challenges, the field of retrieval-augmented LLMs can continue to advance, offering more reliable and efficient solutions for knowledge-intensive NLP tasks."
}