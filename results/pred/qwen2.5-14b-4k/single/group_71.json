{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Nopol: Automatic Repair of Conditional Statement Bugs in Java Programs}\n\n\\begin{document}\n\n\\title{Nopol: Automatic Repair of Conditional Statement Bugs in Java Programs}\n\n\\author{Jifeng~Xuan,~\\IEEEmembership{Member,~IEEE,} \n        Matias~Martinez,\n        Favio~DeMarco$^{\\dag}$,\n        Maxime~Cl\\'{e}ment$^{\\dag}$,\n        Sebastian~Lamelas~Marcote$^{\\dag}$, \n        Thomas~Durieux$^{\\dag}$, \n        Daniel~Le~Berre,\n        and~Martin~Monperrus,~\\IEEEmembership{Member,~IEEE,} \n\\IEEEcompsocitemizethanks{\n  \\IEEEcompsocthanksitem J. Xuan is with the State Key Lab of Software Engineering, School of Computer, Wuhan University, Wuhan, China. \\protect E-mail: \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \n  jxuan@whu.edu.cn.\n  \\IEEEcompsocthanksitem M. Martinez is with the University of Lugano, Lugano, Switzerland. He was with the University of Lille \\& INRIA, Lille, France, when this work is done. \\protect E-mail: matias.sebastian.martinez@usi.ch. \n  \\IEEEcompsocthanksitem F. DeMarco and S. Lamelas Marcote are with the University of Buenos Aires, Buenos Aires, Argentina. \\protect E-mail: faviod@gmail.com, srlm@gmx.com. \n  \\IEEEcompsocthanksitem M. Cl\\'{e}ment and T. Durieux are with the University of Lille, Lille, France. \\protect E-mail: maxime.clement@etudiant.univ-lille1.fr, thomas.durieux@inria.fr.  \n  \\IEEEcompsocthanksitem D. Le Berre is with the University of Artois \\& CNRS, Lens, France. \\protect E-mail: leberre@cril.fr.\n  \\IEEEcompsocthanksitem M. Monperrus is with the University of Lille \\& INRIA, Lille, France. \\protect E-mail: martin.monperrus@univ-lille1.fr.\n}\n\n\\thanks{$^{\\dag}$\\ F. DeMarco, M. Cl\\'{e}ment, S. Lamelas Marcote, and T. Durieux have contributed to this work during their internship at INRIA Lille -- Nord Europe.}\n\\thanks{Manuscript received xxx; revised yyy}\n}\n\n\\markboth{IEEE Transactions on Software Engineering}{Nopol}\n\n\\IEEEcompsoctitleabstractindextext{\n\\begin{abstract}\n\nWe propose \\nopol, an approach to automatic repair of buggy conditional statements (i.e., \\mycode{if-then-else} statements). This approach takes a buggy program as well as a test suite as input and generates a patch with a conditional expression as output. The test suite is required to contain passing test cases to model the expected behavior of the program and at least one failing test case that reveals the bug to be repaired. The process of \\nopol consists of three major phases. First, \\nopol employs angelic fix localization to identify expected values of a condition during the test execution. Second, runtime trace collection is used to collect variables and their actual values, including primitive data types and objected-oriented features (e.g., nullness checks), to serve as building blocks for patch generation. Third, \\nopol encodes these collected data into an instance of a Satisfiability Modulo Theory (SMT) problem; then a feasible solution to the SMT instance is translated back into a code patch. \nWe evaluate \\nopol on \\numbug real-world bugs (16 bugs with \\buggyconditions and 6 bugs with \\preconditions) on two large open-source projects, namely Apache Commons Math and Apache Commons Lang. Empirical analysis on these bugs shows that our approach can effectively fix bugs with \\buggyconditions and \\preconditions. We illustrate the capabilities and limitations  of \\nopol using case studies of real bug fixes.  \n\n\\end{abstract}\n\\begin{keywords}\nAutomatic repair, patch generation, SMT, fault localization\n\\end{keywords}\n}\n\n\\maketitle\n\n\\IEEEdisplaynotcompsoctitleabstractindextext\n\n\\IEEEpeerreviewmaketitle\n\n\\section{Introduction}\n\\IEEEPARstart{A}{utomatic} software repair aims to automatically fix bugs in programs.\nDifferent kinds of techniques are proposed for automatic repair, including patch generation \\cite{le2012genprog, pei2014automated} and dynamic program state recovery \\cite{Perkins2009, carzaniga2013automatic}.\n\nA family of techniques has been developed around the idea of ``test-suite based repair'' \\cite{le2012genprog}.\nThe goal of test-suite based repair is to generate a patch that makes failing test cases pass and keeps the other test cases satisfied. Recent test-suite based repair approaches include the work by Le~Goues et al. \\cite{le2012genprog}, Nguyen et al. \\cite{nguyen2013semfix}, Kim et al. \\cite{Kim2013}. \n\nIn recent work \\cite{Martinez2013}, we have shown that \\ourif conditions are among the most error-prone program elements in Java programs. \nIn our dataset, we observed that 12.5\\% of one-change commits simply update an \\ourif condition. \\emph{This motivates us to study the automatic repair of conditional statements in real-world bugs}. \n\nIn this paper, we present a novel automatic repair system called \\nopol.\\footnote{\\nopol is an abbreviation for ``no polillas'' in Spanish, which literally means ``no moth anymore''.} This system fixes conditional bugs in object-oriented programs and is evaluated on real bugs from large-scale open-source programs. For instance, \\nopol can synthesize a patch that updates a buggy \\ourif condition as shown in Fig. \\ref{fig:example-buggy} or adds a guard precondition as in Fig. \\ref{fig:example-pre}. Both figures are excerpts of real-world bugs taken from the bug tracking system of Apache Commons Math.\n \n\\nopol takes a buggy program as well as a test suite as input and generates a conditional patch as output. This test suite must contain at least one failing test case that embodies the bug to be repaired. Then, \\nopol analyzes program statements that are executed by failing test cases to identify the source code locations where a patch may be needed.\n\nFor each statement, the process of generating the patch consists of three major phases. \nFirst, we detect whether there exists a fix location for a potential patch in this statement with a new and scalable technique called ``angelic fix localization'' (Section \\ref{sect:angelic-fix-localization}). For one fix location, this technique reveals angelic values, which make all failing test cases pass. \n\n\\begin{figure}[!t]\n\\centering\n\\noindent\\begin{minipage}{0.45\\textwidth}\n\\begin{lstlisting}\n-  if (u * v == 0) { \n+  if (u == 0 || v == 0) {\n    return (Math.abs(u) + Math.abs(v));\n  }  \n\\end{lstlisting}\n\\end{minipage}\n\\caption{Patch example of Bug CM5: a bug related to a \\buggycondition. The original condition with a comparison with \\mycode{==} is replaced by a disjunction between two comparisons.}\n\\label{fig:example-buggy}\n\\end{figure}\n\n\\begin{figure}[!t]\n\\centering\n\\noindent\\begin{minipage}{0.45\\textwidth}\n\\begin{lstlisting}\n+ if (specific != null) {\n    sb.append(\": \"); //sb is a string builder in Java\n+ }\n\\end{lstlisting}\n\\end{minipage}\n\\caption{Patch example of Bug PM2: a precondition is added to avoid a null dereference.}\n\\label{fig:example-pre}\n\\end{figure}\n\nSecond, \\nopol collects runtime traces from test suite execution through code instrumentation (Section \\ref{sect:data-collection}). These traces contain snapshots of the program state at all candidate fix locations. The collected trace consists of both primitive data types (e.g., integers and booleans) and object-oriented data (e.g., nullness or object states obtained from method calls). \n\nThird, given the runtime traces, the problem of synthesizing a new conditional expression that matches the angelic values is translated into a Satisfiability Modulo Theory (SMT) problem (Section \\ref{subsect:encoding}).\nOur encoding extends the technique by Jha et al. \\cite{jha2010oracle} by handling rich object-oriented data.\nWe use our own implementation of the encoding together with an off-the-shelf SMT solver (Z3 \\cite{z3}) to check whether there exists a solution.\n\nIf such a solution exists, \\nopol translates it back to source code, i.e., generates a patch. We re-run the whole test suite to validate whether this patch is able to make all test cases pass and indeed repairs the bug under consideration.\n\nTo evaluate and analyze our repair approach \\nopol, we collect a dataset of \\numbug bugs (16 bugs with \\buggyconditions and 6 bugs with \\preconditions) from real-world projects. Our result shows that 17 out of \\numbug bugs can be fixed by \\nopol, including four bugs with manually added test cases. Four case studies are conducted to present the benefits of generating patches via \\nopol and five bugs are employed to explain the limitations. \n\nThe main contributions of this paper are as follows. \n\\begin{itemize} \n\\item The design of a repair approach for fixing conditional statement bugs of the form of \\buggyandpres.\n\\item Two algorithms of angelic fix localization for identifying potential fix locations and expected values.\n\\item An extension of the SMT encoding in \\cite{jha2010oracle} for handling nullness and certain method calls of object-oriented programs.\n\\item An evaluation on a dataset of \\numbug bugs in real-world programs with an average of 25K executable lines of code for each bug.\n\\item A publicly-available system for supporting further replication and research. \n\\item An analysis of the repair results with respect to fault localization. \n\\end{itemize}\n\nThis paper is an extension of our previous work \\cite{demarco2014automatic}. This extension adds an evaluation on a real-world bug dataset, four detailed case studies, a discussion of the limitations of our approach, and a detailed analysis on patches. \n\nThe remainder of this paper is organized as follows. Section \\ref{background} provides the background of test-suite based repair. Section \\ref{sect:approach} presents our approach for repairing bugs with \\buggyconditions and \\preconditions. Section \\ref{sect:evaluation} details the evaluation on \\numbug real-world bugs. Section \\ref{sect:discussions} further analyzes the repair results. Section \\ref{sect:threats} presents potential issues and Section \\ref{sect:relatedwork} lists the related work. Section \\ref{sect:conclusion} concludes.\n\n\\section{Background}\n\\label{background}\n\nWe present the background on test-suite based repair and the two kinds of bugs targeted in this paper. \n\n\\subsection{Test-Suite Based Repair}\n\\label{sect:test-suite-based-repair}\n\nTest-suite based repair consists in repairing programs according to a test suite, which contains both passing test cases as a specification of the expected behavior of the program and at least one failing test case as a specification of the bug to be repaired.\nFailing test cases can either identify a regression bug or reveal a new bug that has just been discovered.\nThen, a repair algorithm searches for patches that make all the test cases pass.\n\nThe core assumption of test-suite based repair is that the test suite is good enough to thoroughly model the program to repair \\cite{monperrus2014critical}. This is a case when the development process ensures a very strong programming discipline. For example, most commits of Apache projects (e.g., Apache Commons Lang) contain a test case specifying the change. If a commit is a bug fix, the commit contains a test case that highlights the bug and fails before the fix.\n\nTest-suite based repair, which has been popularized by the work of GenProg by Le~Goues et al. \\cite{le2012genprog}, has become an actively explored research area \\cite{nguyen2013semfix,Kim2013,qi2014strength,qi2015efficient,DBLP:conf/sigsoft/LongR15}.\nThe approach presented in this paper, \\nopol, is also an approach to test-suite based repair. Other kinds of repair methods include repair based on formal models \\cite{Jobstmann2005} and dynamic repair of the program state \\cite{Perkins2009}. \n\n\\subsection{Buggy \\ourif Condition Bugs}\n\nConditional statements (e.g., \\mycode{if (condition)\\{\\ldots\\} else \\{\\ldots\\}}), are widely-used in programming languages. Pan et al. \\cite{pan2009toward} show that among seven studied Java projects, up to 18.6\\% of bug fixes have changed a buggy condition in \\ourif statements. A buggy \\ourif condition is defined as a bug in the condition of an \\mycode{if-then-else} statement. \n\nThe bug in Fig. \\ref{fig:example-buggy} is a real example of a buggy \\ourif condition in Apache Commons Math. This bug is a code snippet of a method that calculates the greatest common divisor between two integers. The condition in that method is to check whether either of two parameters \\mycode{u} and \\mycode{v} is equal to 0. In the buggy version, the developer compares the product of the two integers to zero. However, this may lead to an arithmetic overflow. A safe way to proceed is to compare each parameter to zero. This bug was fixed by \\nopol (see Bug CM5 in Table \\ref{tab:patch}).\n\n\\subsection{Missing Precondition Bugs}\nAnother class of common bugs related conditions is the class of missing preconditions. \nA precondition aims to check the state of certain variables before the execution of a statement.\nExamples of common preconditions include detecting a null pointer or an invalid index in an array.  \nIn software repositories, we can find commits that add preconditions (i.e., which were previously missing).\n\nThe bug in Fig. \\ref{fig:example-pre} is a missing precondition with the absence of null pointer detection. The buggy version without the precondition throws an exception signaling a null pointer at runtime. \\nopol fixed this bug by adding the precondition (see Bug PM2 in Table~\\ref{tab:patch}).\n\n\\section{Our Approach}\n\\label{sect:approach}\nThis section presents our approach to automatically repairing buggy \\ourif conditions and missing preconditions.\nOur approach is implemented in a tool called \\nopol that repairs Java code.\n\n\\subsection{Overview}\n\\label{subsect:overview}\n\n\\nopol is a repair approach, which is dedicated to buggy \\ourif conditions and missing preconditions. As input, \\nopol requires a test suite which represents the expected program functionality with at least one failing test case that exposes the bug to be fixed. Given a buggy program and its test suite, \\nopol returns the final patch as output. Fig. \\ref{fig:example-buggy} and Fig. \\ref{fig:example-pre} are two examples of output patches for \\buggyandpres by \\nopol, respectively. \n\n\\textbf{How to use {\\sc \\textbf{Nopol}}}. From a user perspective, given a buggy program with a test suite, including failing test cases, the user would run \\nopol and obtain a patch, if any. Before applying \\nopol to the buggy program, the user does not need to know whether the bug relates to conditions. Instead, the user runs \\nopol for any buggy program. \nIf \\nopol finds a patch, then the user would manually inspect and validate it before the integration in the code base. As further discussion in Section \\ref{subsect:general-repair}, the user can also add a pre-defined timeout, e.g., 90 seconds as suggested in experiments or a longer timeout like five hours instead of exhaustively exploring the search space. \n\nFig. \\ref{fig:framework} shows the overview of \\nopol.  \n \\nopol employs a fault localization technique to rank statements according to their suspiciousness of containing bugs. For each statement in the ranking, \\nopol considers it as a \\buggycondition candidate if the statement is an \\ourif statement; or \\nopol considers it as a \\precondition candidate if the statement is any other non-branch or non-loop statement (e.g., an assignment or a method call). \\nopol processes candidate statements one by one with three major phases. \n\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=0.5\\textwidth]{framework}\n\\caption{Overview of the proposed automatic repair approach, \\nopol.}\n\\label{fig:framework}\n\\end{figure}\n\nFirst, in the phase of \\emph{angelic fix localization}, \\nopol arbitrarily tunes a conditional value (\\mycode{true} or \\mycode{false}) of an \\ourif statement to pass a failing test case. If such a conditional value exists, the statement is identified as a fix location and the arbitrary value is viewed as the expected behavior of the patch. In \\nopol, there are two kinds of fix locations, \\ourif statements for repairing buggy conditions and arbitrary statements for repairing missing preconditions.\n\nSecond, in the phase of \\emph{runtime trace collection}, \\nopol runs the whole test suite in order to collect the execution context of each fix location. The context includes both variables of primitive types (e.g., booleans or integers) and a subset of object-oriented features (nullness and certain method calls); then such runtime collection will be used in synthesizing the patch in the next phase. \n\nThird, in the phase of \\emph{patch synthesis}, the collected trace is converted into a Satisfiability Modulo Theory (SMT) formula. The satisfiability of SMT implies that there exists a program expression that preserves the program behavior and fixes the considered bug. That is, the expression makes all test cases pass. If the SMT formula is satisfiable, the solution to the SMT is translated as a source code patch; if unsatisfiable, \\nopol goes to the next statement in the statement ranking, until all statements are processed. \n\nAfter the above three phases, the whole test suite is re-executed to validate that the patch is correct. This validation could be skipped if the SMT encoding is proven to be correct.\nIndeed, theoretically, if the SMT solver says ``satisfiable'', it means that a patch exists. \nHowever, there could be an implementation bug in the trace collection, in the SMT problem generation, in the off-the-shelf SMT solver, or in the patch synthesis.  \nConsequently, we do make the final validation by re-executing the test suite.\n\n\\subsection{Angelic Fix Localization}\n\\label{sect:angelic-fix-localization}\n\nIn \\nopol, we propose to use value replacement \\cite{jeffrey2008fault} to detect potential fix locations.\nValue replacement \\cite{jeffrey2008fault} comes from fault localization research. It consists in replacing at runtime one value by another one. \nMore generally, the idea is to artificially change the program state for localizing faults. \nThere are a couple of papers that explore this idea. \nFor instance, Zhang et al. \\cite{zhang2006locating} use the term ``predicate switching'' and Chandra et al. \\cite{chandra2011angelic} use the term ``angelic debugging''. \n\n\\nopol replaces conditional values in \\ourif statements. We refer to conditional values that make test cases pass as \\emph{angelic values}. \n\n{\\bf Definition (Angelic Value)}\nAn angelic value is an arbitrarily-set value at a given location during test execution, which enables a failing test case to pass. \n\nTo facilitate the description of our approach, we follow existing work \\cite{gulwani2011synthesis} to introduce the concept of locations. A \\textit{location} is an integer value, which identifies the absolute position of a statement in source code.\n\n{\\bf Definition (Angelic Tuple)}\nAn angelic tuple is a triplet $(loc, val, test)$, where the statement at a location $loc$ is evaluated to a value $val$ to make a failing test case $test$ pass.\n\nIn this paper, we refer to the technique of modifying the program state to find the values for angelic tuples $(loc, val, test)$ as \\emph{angelic fix localization}. If an angelic tuple $(loc, val, test)$ is found, there may exist a patch in the location $loc$ in source code. In the phase of angelic fix localization, only failing test cases are needed, not the whole test suite. \n\nA single test case $test$ may evaluate the statement at the location $loc$ several times. Consequently, according to our definition, the value $val$ is fixed across all evaluations of a given statement for one test case. This is the key point for having a tractable search space (will be discussed in Section \\ref{subsubsect:search-space}). On the other hand, one angelic value is specific to a test case: for a given location $loc$, different failing test cases may have different angelic values.\n\n\\subsubsection{For Buggy \\ourif Conditions}\n\n\\begin{algorithm}[!t]\n\n\\SetKwInOut{Input}{Input}\n\\SetKwInOut{Output}{Output}\n\n\\Input{ \\\\\n\\noindent $stmt$, a candidate \\ourif statement; \\\\\n\\noindent $T_f$, a set of failing test cases. \n}\n\\Output{ \\\\\n\\noindent $R$, a set of angelic tuples. \n}\n\n\\BlankLine \n$R \\leftarrow \\emptyset$\\;\nInitialize two sets $T_{true} \\leftarrow \\emptyset$ and $T_{false} \\leftarrow \\emptyset$\\; \nLet $cond$ be the condition in $stmt$ and let $loc$ be the location of $cond$\\; \n\n\\BlankLine \nForce $cond$ to \\mytrue and execute test cases in $T_f$\\; \n\\ForEach{failing test case $t_i \\in T_f$}{\n  \\If{$t_i$ passes}{\n    $T_{true} \\leftarrow T_{true} \\cup \\{t_i\\}$\\;\n  }\n}\nForce $cond$ to \\myfalse and execute test cases in $T_f$\\; \n\\ForEach{failing test case $t_i \\in T_f$}{\n  \\If{$t_i$ passes}{\n    $T_{false} \\leftarrow T_{false} \\cup \\{t_i\\}$\\;\n  }\n}\n\n\\BlankLine \n\\tcp{All test cases in $T_f$ are passed}\n\\If{$(T_f \\setminus T_{true}) \\cap (T_f \\setminus T_{false}) = \\emptyset$}{\n  \\ForEach{$t_i \\in T_{true}$}{\n    $R \\leftarrow R \\cup \\{(loc, \\mbox{\\mytrue}, t_i)\\}$\\;\n  }\n  \\ForEach{$t_i \\in T_{false}$}{\n    $R \\leftarrow R \\cup \\{(loc, \\mbox{\\myfalse}, t_i)\\}$\\;\n  }\n}\n\\BlankLine \n\\caption{Angelic Fix Localization Algorithm for Buggy \\ourif Conditions}\n\\label{alg:if-angelic-fix-localization}\n\\end{algorithm}\n\nFor buggy \\ourif conditions, angelic fix localization works as follows.\nFor each \\ourif condition that is evaluated during test suite execution, an angel forces the \\ourif condition to be \\mycode{true} or \\mycode{false} in a failing test case. \nAn angelic tuple $(loc, val, test)$, i.e., (\\ourif condition location, boolean value, failing test case), indicates that a fix modifying this \\ourif condition may exist (if the subsequent phase of patch synthesis succeeds, see Section \\ref{subsect:encoding}).\n\nAlgorithm \\ref{alg:if-angelic-fix-localization} is the pseudo-code of angelic fix localization for \\buggyconditions. For a given \\ourif statement \\textit{stmt} and its condition \\textit{cond}, both \\mytrue and \\myfalse are set to pass originally failing test cases at runtime. Lines 4 to 9 and Lines 10 to 15 describe how to set \\textit{cond} to be \\mytrue and \\myfalse, respectively. If all failing test cases are passed, angelic tuples are collected, i.e., Lines 17 to 22, for further patch synthesis; otherwise, \nthere exists no angelic value for the test case and the location under consideration.  \nThe same idea of forcing the execution can be used to identify angelic values for loop conditions \\cite{Lamelas2015}.\n\n\\subsubsection{For Missing Preconditions}\n\\label{subsubsect:precondition}\n\nAngelic fix localization for \\preconditions is slightly different from that for \\ourif conditions.\nFor each non-branch and non-loop statement that is evaluated during test suite execution, an angel forces to skip it. \nIf a failing test case now passes, it means that a potential fix location has been found. \nThe oracle for repair is then  ``false''; that is, the added precondition must be \\myfalse, i.e., the statement should be skipped. Then, an angelic tuple $(loc, val, test)$ is (precondition location, \\mycode{false}, failing test case). \n\nAlgorithm \\ref{alg:missing-pred-angelic-fix-localization} is the pseudo-code of this algorithm. Given a non-\\ourif statement \\textit{stmt}, we skip this statement to check whether all failing test cases are passed, i.e., Lines 4 to 9. If yes, the location \\textit{loc} of the precondition as well as its angelic value \\mycode{false} is collected, i.e., Lines 11 to 13. \nIf skipping the statement does not pass all the failing test cases, no angelic values will be returned. \nThis technique also works for \\preconditions for entire blocks since blocks are just specific statements in Java. In our implementation, we only consider adding missing preconditions for single statements rather than blocks. Manual examination on the dataset in Section \\ref{subsect:general-repair} will show that our dataset does not contain missing preconditions for blocks. \n\n\\begin{algorithm}[!t]\n\\SetKwInOut{Input}{Input}\n\\SetKwInOut{Output}{Output}\n\n\\Input{ \\\\\n\\noindent $stmt$, a candidate non-\\ourif statement; \\\\\n\\noindent $T_f$, a set of failing test cases. \n}\n\\Output{ \\\\\n\\noindent $R$, a set of angelic tuples. \n}\n\n\\BlankLine  \n$R \\leftarrow \\emptyset$\\;\nInitialize a test case set $T_{pre} \\leftarrow \\emptyset$\\; \nLet $loc$ be the location of a potential precondition of $stmt$;\n\n\\BlankLine\nForce $stmt$ to be skipped and execute $T_f$\\; \n\\ForEach{failing test case $t_i \\in T_f$}{\n  \\If{$t_i$ passes}{\n    $T_{pre} \\leftarrow T_{pre} \\cup \\{t_i\\}$\\;\n  }\n}\n\n\\BlankLine\n\\tcp{All test cases in $T_f$ are passed}\n\\If{$T_{pre} = T_f$}{\n  \\ForEach{$t_i \\in T_{pre}$}{\n    $R \\leftarrow R \\cup \\{(loc, \\mbox{\\myfalse}, t_i)\\}$\\;\n  }\n}\n\\BlankLine\n\\caption{Angelic Fix Localization Algorithm for Missing Preconditions}\n\\label{alg:missing-pred-angelic-fix-localization}\n\\end{algorithm}\n\n\\subsubsection{Characterization of the Search Space}\n\\label{subsubsect:search-space}\nWe now characterize the search space of angelic values. \nIf an \\ourif condition is executed more than once in a failing test case, there may exist a sequence of multiple different angelic values resulting in a passing test case. For example, a \\buggycondition that is executed three times by one failing test case may require a sequence of three different angelic values to pass the test case. \n\n\\textbf{Search space for \\buggyconditions}. \nIn general, if one failing test case executes a \\buggycondition for $t_c$ times, the search space of all sequences of angelic values is $2^{t_c}$.\nTo avoid the problem of combinatorial explosion, \\nopol assumes that, for a given failing test case, the angelic value is the same during the multiple executions on one statement. \nThe search space size becomes $2$ for one failing test case instead of $2^{t_c}$. Under this assumption, the search space is shown as follows. \n\nFor \\buggycondition, the search space is $2\\times n_c$ where $n_c$ is the number of executed \\ourif statements by a given failing test case.\n\n\\textbf{Search space for \\preconditions}.\nSimilarly to angelic fix localization for buggy \\ourif conditions, if a statement is executed several times by the same failing test case, angelic fix localization directly adds a precondition (with a \\mycode{false} value) and completely skips the statement for a given test case. \n\nFor \\precondition bugs, the search space size is $n_p$, where $n_p$ is the number of executed statements by test cases. It is not $2\\times n_p$ because we only add a precondition and check whether the \\mycode{false} value passes the failing test case.\n\n\\nopol does not decide a priority between updating existing conditions or adding new preconditions. A user can try either strategy, or both. There is no analytical reason to prefer one or the other; our evaluation does not give a definitive answer to this question. In our experiment, we perform both strategies for statements one by one (see Section \\ref{subsect:overview}). \n\nIf no angelic tuple is found for a given location, there are two potential reasons. \nFirst, it is impossible to fix the bug by changing the particular condition (resp. adding a precondition before the statement).\nSecond, only a sequence of different angelic values, rather than a single angelic value, would enable the failing test case to pass.\nHence, \\nopol is incomplete: there might be a way to fix an \\ourif condition by alternating the way of finding angelic values, but we have not considered it in this paper.\n\n\\subsection{Runtime Trace Collection for Repair}\n\\label{sect:data-collection} \n\nOnce an angelic tuple is found, \\nopol collects the values that are accessible at this location during program execution. Those values are used to synthesize a correct patch (in Section \\ref{subsect:encoding}). In our work, different kinds of data are collected to generate a patch.\n\n\\subsubsection{Expected Outcome Data Collection}\n\\label{subsubsect:outcome}\n\nAs mentioned in Section \\ref{sect:angelic-fix-localization}, an angelic value indicates that this value enables a failing test case to pass. To generate a patch, \\nopol collects the expected outcomes of conditional values to pass the whole test suite: angelic values for failing test cases as well as actual execution values for the passing ones. \n\nLet $O$ be a set of expected outcomes in order to pass all test cases. An expected outcome $O_{loc,m,n} \\in O$ refers to the value at location $loc$ during the $m$-th execution in order to pass the $n$-th test case. \\nopol collects $O_{loc,m,n}$ for all executions of location $loc$.\n\n\\textbf{For buggy \\ourif conditions}. $O_{loc,m,n}$ is the expected outcome of the condition expression at $loc$.\nFor a failing test case, the expected outcome is the angelic value; \nfor a passing test case, the expected outcome is the runtime value $eval(loc)$, i.e., the result of the evaluation during the actual execution of the \\ourif condition expression.\n$$ O_{loc,m,n} =\n\\begin{cases}\neval(loc), & \\mbox{for passing test cases} \\\\\n\\mbox{angelic value} & \\mbox{for failing test cases} \n\\end{cases}\n$$\n\n\\textbf{For missing preconditions}. $O_{loc,m,n}$ is the expected value of the precondition at $loc$, i.e., \\mycode{true} for passing test cases and \\mycode{false} for failing test cases. The latter comes from angelic fix localization: if the precondition returns \\mycode{false} for a failing test case, the buggy statement is skipped and the test case passes.\n$$ O_{loc,m,n} =\n\\begin{cases}\ntrue & \\mbox{for passing test cases} \\\\\nfalse & \\mbox{for failing test cases} \n\\end{cases}\n$$\n  \nNote that not all the bugs with missing preconditions can be fixed with the above definition. Section \\ref{subsubsect:limit-pl3} will present the limitation of this definition with a real example.    \n\n\\subsubsection{Primitive Type Data Collection}\n\\label{subsect:primitive}\n\nAt the location of an angelic tuple, \\nopol collects the values of all local variables, method parameters, and class fields that are typed with a basic primitive type (booleans, integers, floats, and doubles). \n\nLet $C_{loc,m,n}$ be the set of collected values at location $loc$ during the $m$-th execution of the $n$-th test case.\nIn order to synthesize conditions that use literals (e.g., \\mycode{if (x > 0)}), $C_{loc,m,n}$ is enriched with constants for further patch synthesis.\nFirst, \\nopol collects static values that are present in the program.\\footnote{Besides collecting static fields in a class, we have also tried to collect other class fields of the class under repair, but the resulting patches are worse in readability than those without collecting class fields. Hence, no class fields other than static fields are involved in data collection. } Second, we add three standard values  \\{0, -1, 1\\}, which are present in many bug fixes in  the wild (for instance for well-known off-by-one errors).\nBased on these standard values, other values can be formed via wiring building blocks (in Section \\ref{subsubsect:wiring}). For example, a value $2$ in a patch can be formed as $1+1$, if $2$ is not collected during runtime trace collection. \n\n\\subsubsection{Object-Oriented Data Collection}\n\\label{subsubsect:oo}\n\n\\nopol aims to support automatic repair for object-oriented programs. \nIn particular, we would like to support nullness checks and some particular method calls.\nFor instance, \\nopol is able to synthesize the following patch containing a method call. \n\n\\begin{lstlisting}\n  +  if (obj.size() > 0) {\n       compute(obj);\n  +  }\n\\end{lstlisting}\n\nTo this end, in addition to collecting all values of primitive types, \\nopol collects two kinds of object-oriented features.\nFirst, \\nopol collects the nullness of all variables of the current scope.\nSecond, \\nopol collects the output of ``state query methods'', defined as the methods that inspect the state of objects and are side-effect free. A state query method is an argumentless method with a primitive return type. \nFor instance, methods \\mycode{size()} and \\mycode{isEmpty()} of \\mycode{Collection} are state query methods.\nThe concept of state query methods is derived from ``argument-less boolean queries'' of  ``object states'' by Pei et al. \\cite{pei2014automated}.  \n\n\\nopol is manually fed with a list of such methods. The list is set with domain-specific knowledge. For instance, in Java, it is easy for developers to identify such side-effect free state query methods on core library classes such as \\mycode{String}, \\mycode{File} and \\mycode{Collection}. For each object-oriented class $T$, those predefined state query methods are denoted as $sqm(T)$.\n\n\\nopol collects the nullness of all visible variables and the evaluation of state query methods for all objects in the scope (local variables, method parameters, and fields) of a location where an angelic tuple exists. Note that this incorporates inheritance; the data are collected based on the polymorphism in Java.\nFor instance, when the value of \\texttt{obj.size()} is collected, it may be for one implementation of \\texttt{size()} based on array lists and for another implementation of \\texttt{size()} based on linked lists. This means that a patch synthesized by \\nopol can contain polymorphic calls. \n\n\\subsubsection{On the Size of Collected Data}\n\nLet us assume there are $u$ primitives values and a set $O$ of $w$ objects in the scope of an angelic tuple. \nIn total, \\nopol collects the following values:\n\n\\begin{itemize}\n\\item $u$ primitive variables in the scope;\n\\item $w$ boolean values corresponding to the nullness of each object;\n\\item $\\sum_{o \\in O}|sqm(class(o))|$ values corresponding to the evaluation of the state query methods of all objects available in the scope, where $class(o)$ denotes the class of the object $o$;\n\\item constants, i.e., 0, -1, and 1 in our work.\n\\end{itemize}\n\n\\subsection{Patch Synthesis: Encoding Repair in SMT}\n\\label{subsect:encoding}\n\nThe patch synthesis of buggy \\ourif conditions and missing preconditions consists of synthesizing an expression $exp$ such that\n\\begin{equation}\n\\forall_{loc,m,n} \\mbox{~~}exp(C_{loc,m,n}) =  O_{loc,m,n} \n\\label{eq:repair-if}\n\\end{equation}\n\nAs a synthesis technique, \\nopol, as SemFix \\cite{nguyen2013semfix}, uses a variation of oracle-guided component-based program synthesis \\cite{jha2010oracle}, which is based on SMT.\n\nThe solution of the SMT problem is then translated back to a boolean source code expression $exp$ representing the corrected \\ourif condition or the added precondition.\n\n\\subsubsection{Building Blocks}\n\\label{sect:bb}\n\nWe define a {\\em building block} (called a {\\em component} in \\cite{jha2010oracle}) as a type of expression that can appear in the boolean expression to be synthesized. \nFor instance, the logical comparison operator ``<'' is a building block. As building block types, we consider comparison operators ($<$, $>$, $\\leq$, $\\geq$, $=$, and $\\neq$), arithmetic operators ($+$, $-$, and $\\times$),\\footnote{Adding the division is possible but would require specific care to avoid division by zero.} and boolean operators ($\\wedge$, $\\vee$, and $\\lnot$). The same type of building blocks can appear multiple times in one expression. \n\nLet $b_i$ be the $i$th building block ($i=1,2,\\ldots,k$). Then $b_i$ is identified as a tuple of a set of input variables $I_i$, an output variable $r_i$, and an expression $\\phi_{i}(I_i,r_i)$ encoding the operation of the building block.\nThat is $b_i=(\\phi_{i}(I_i,r_i),I_i,r_i)$ ($b_i=(\\phi_{i},I_i,r_i)$ for short). For example, given a boolean output $r_i$ and an input $I_i=\\{I_{i,1}, I_{i,2}\\}$ consisting of two boolean values, a building block could be $b_i=(\\phi_{i},\\{I_{i,1}, I_{i,2}\\},r_i)$, where $\\phi_{i}$ is implemented with the operator $\\wedge$, i.e.,  $I_{i,1} \\wedge I_{i,2}$. \n\nLet $r$ be the final value of the synthesized patch. Hence there exists one building block $b_i$ whose output is bound to the return value $r_i = r$.\n\nSuppose we are given a set $B$ of building blocks and a list $CO$ of pairs $(C_{loc,m,n},O_{loc,m,n})$, i.e., pairs of collected values and expected values at the location $loc$ during the $m$-th execution of the $n$-th test case. $C_{loc,m,n}$ includes values of different types: BOOL, INT, or REAL.\\footnote{In the context of SMT, we use BOOL, INT, and REAL to denote the types of booleans, integers, and doubles as well as floats in Java, respectively.} A patch is a sequence of building blocks $<b_1,b_2,...,b_k>$ with $b_i \\in B$, whose input values are taken from either $C_{loc,m,n}$ or other building blocks. \n\n\\subsubsection{Wiring Building Blocks}\n\\label{subsubsect:wiring}\n\nThe problem of patch synthesis is thus to wire the input of building blocks $<b_1,b_2,...,b_k>$ to the input values $I_0$ or the output values of other building blocks. To synthesize a condition, we need to make sure that the types of the variables are valid operands (e.g., an arithmetic operator only manipulates integers). \n\n\\textbf{Example}. Let us assume that $C_{loc,m,n}$ has three values, an integer variable $i_0$, a boolean constant $c_1 \\leftarrow False$, and an integer constant $c_2 \\leftarrow 3$. Assume we have two building blocks,  $\\mbox{BOOL} \\leftarrow f_1(\\mbox{BOOL})$ and \n$\\mbox{BOOL} \\leftarrow f_2(\\mbox{INT}, \\mbox{INT})$. \nThen the goal of patch synthesis is to find a well formed expression, such as $False$, $f_1(False)$, $f_2(i_0, 3)$, $f_2(3,3)$, and $f_1(f_2(3,3))$; meanwhile, one of these expressions is expected to match the final output $r$. \n\n\\subsubsection{Mapping Inputs and Outputs with Location Variables}\n\nLet $I = \\cup I_i$ and $O = \\cup \\{r_i\\}$ be the sets of input and output values of all building blocks $b_i \\in B$. \nLet $I_0$ be the input set $\\{ C_{loc,m,n} \\}$ and let $r$ be the output in the final patch.\nWe define $IO$ as $IO = I \\cup O \\cup I_0 \\cup \\{ r \\}$. \nWe partition the elements of $IO$ according to their types in BOOL, INT, and REAL.\n\nThe SMT encoding relies on the creation of location variables. A {\\em location variable} $l_x \\in L$ represents an index of an element $x\\in IO$. Note that the concept of location variables in SMT encoding is different from the concept of locations in Section \\ref{sect:angelic-fix-localization}. A location variable $l_x$ indicates a relative position, i.e., an index, of $x$ in a patch while a location $loc$ indicates an absolute position of a statement in a source file. \nA {\\em value variable} $v_x \\in V$ represents a value taken by an elements $x \\in IO$. Values of location variables are actually integers ($L \\subseteq \\mbox{INT}$);\nvalue variables are of any supported type, i.e., BOOL, INT, or REAL. \n\nInformally, a location variable $l_x$ serves as an index of $x \\in IO$ in a code fraction while a value variable $v_x$ indicates its value during test execution. Section \\ref{subsubsect:domain} will further illustrate how to index the code via location variables. \nLocation variables are invariants for the execution of all test cases: they represent the patch structure.\nValue variables are used internally by the SMT solver to ensure that the semantics of the program is preserved.\n\n\\subsubsection{Domain Constraints}\n\\label{subsubsect:domain}\n\nLet us first define the domain constraints over the location variables. Given the input set $I_0$ and the building block set $B$, let $p$ be the number of possible inputs and $p=|I_0|+|B|$. \nThe location variables of the elements of $I_0$ and $r$ are fixed:\n$$\\phi_{FIXED}(I_0,r) = ( \\land_{i=1}^{|I_0|} \\ l_{I_0,i} = i ) \\bigwedge l_r = p$$\n\nGiven building blocks $b_i \\in B$, the location variable $l_{r_i}$ for the output $r_i$ ($r_i \\in O$) of $b_i$ belongs to a range of $[|I_0|+1, p]$:\n$$\\phi_{OUTPUT}(O) = \\bigwedge_{i=1}^{|O|} ( |I_0|+1 \\leq l_{r_i} \\leq p)$$\n\n\\textbf{Handling types}. Only the location variables corresponding to the values of the same type are allowed.\nSuppose that $type(x)$ returns the set of elements with the same type of $x$ among BOOL, INT, and REAL. Then we can restrict the values taken by the location variables of the input values of building blocks using the following formula:\n\n$$\\phi_{INPUT}(I) = \\bigwedge_{x \\in I} \\ \\ \\bigvee_{y \\in type(x), x \\neq y} (l_x = l_y)$$\n\nRecall the example in Section \\ref{subsubsect:wiring}, we have the input $I_0=\\{i_0, c_1, c_2\\}$, the output $r$, and two building blocks $f_1$ and $f_2$. We assume that each building block is involved in the patch for at most once for simplifying the example; in our implementation, a building block can be used once or more in a synthesized patch. Then we have the location variables as follows. The location variables of $i_0$, $c_1$, and $c_2$ are 1, 2, and 3; the locations variables of building blocks are 4 and 5, respectively. Based on the types, candidate values of location variables of $I_{f_1, 1}$, $I_{f_2, 1}$, and $I_{f_2, 2}$ are calculated. \n\n\\begin{table}[H]\n\\centering\n\\resizebox{0.5\\textwidth}{!}{\n\\begin{tabular}{l l}\n$l_{i_0}=1$  &   input variable, integer  \\\\     \n$l_{c_1} = 2 $ &  boolean constant, $False$ \\\\\n$l_{c_2}  = 3$ &  integer constant, 3  \\\\\n$l_{r_{f_1}} \\in [4,5]$  &  output of $f_1$,  boolean  \\\\\n$l_{r_{f_2}} \\in [4,5]$  &  output of $f_2$, boolean \\\\ \n$l_r     = 5$    &  expected output value, boolean  \\\\\n$l_{I_{f_1,1}} \\in \\{l_{c_1}, l_{r_{f_1}}, l_{r_{f_2}}\\}$    &  the parameter of $f_1$, boolean \\\\\n$l_{I_{f_2,1}}\\in \\{l_{i_0}, l_{c_2}\\}$    &  the first parameter of $f_2$, integer \\\\\n$l_{I_{f_2,2}} \\in \\{l_{i_0}, l_{c_2}\\}$   &  the second parameter of $f_2$, integer \\\\\n\\end{tabular}\n}\n\\end{table}\n\nThe following additional constraints are used to control the values of location variables.\nFirst, we ensure that each output of a building block is mapped to one distinct input (wires are one-to-one).\n$$\\phi_{CONS}(L,O) = \\bigwedge_{x,y \\in O, x \\neq y} l_x \\neq l_y$$\n\nSecond, we need to order the building blocks in such a way that its arguments have already been defined.\n$$\\phi_{ACYC}(B,L,I,O) = \\bigwedge_{(\\phi_{i},I_i,r_i) \\in B} \\ \\  \\bigwedge_{x \\in I_i} \\ \\  l_x < l_{r_i}$$\n \nThen, we combine all constraints together. \n\\begin{align*}\n\\phi_{WFF}(B,L,I,O, I_0,r) = \\phi_{FIXED}(I_0,r)  \\land \\phi_{OUTPUT}(O) \\\\ \\land \\phi_{INPUT}(I) \\land \\phi_{CONS}(L,O) \\land \\phi_{ACYC}(B,L,I,O)\n\\end{align*}\n\nAn assignment of $L$ variables respecting the predicate $\\phi_{WFF}(B,L,I,O,I_0,r)$ corresponds to a syntactically correct patch. \n\nValue variables corresponding to the input and the output of a building block are related according to the functional definition of a predicate $pb_i$. Given a building block $b_i = \\{ \\phi_i, I_i, r_i\\}$, let $value(I_i)$ be a function that returns the value for the input $I_i$. For a value variable $v_{r_i}$, let $pb_i(value(I_i),v_{r_i})=true$ iff $\\phi_i(I_i)=r_i$. Given $V_{IO} = \\{v_x | x \\in I \\cup O\\}$, we define the following constraint.  \n$$\\phi_{LIB}(B,V_{IO}) = \\bigwedge_{(\\phi_i, I_i, r_i) \\in B, v_{r_i}\\in V_{IO}}\\ pb_i \\Big(value(I_i),v_{r_i}\\Big)$$\n\nThe location variables and the value variables are connected together using the following rule which states that elements at the same position should have the same value. Note that we need to limit the application of that rule to values of the same type because in our case, input or output values can be of different types. Such a limitation to the elements of the same type is valid since the domain of the location variables are managed using constraints $ \\phi_{INPUT}(I)$. \n\\begin{align*}\n\\phi_{CONN} & (L,V_{IO}) =     \\\\\n  & \\bigwedge_{S \\in \\{\\mbox{\\footnotesize BOOL},\\mbox{\\footnotesize INT},\\mbox{\\footnotesize REAL}\\}}\\ \\bigwedge_{x,y \\in S}\\ l_x = l_y \\Rightarrow v_x = v_y\n\\end{align*}\n\nLet the notation $\\alpha[v \\leftarrow x]$ mean that the variable $v$ in the constraint $\\alpha$ has been set to the value $x$. For a given location $loc$, the patch for a given input $I_0$ and a given output $r$ is preserved using the following existentially quantified constraint.\n\\begin{align*}\n\\phi&_{FUNC} (B,L,C_{loc,m,n},O_{loc,m,n}) = \\\\\n  & \\exists V_{IO} \\Big( \\phi_{LIB}(B,V_{IO}) \\bigwedge  \\\\\n  & \\phi_{CONN}(L,V_{IO}) [value(I_0) \\leftarrow  C_{loc,m,n}, v_r \\leftarrow O_{loc,m,n}] \\Big)\n\\end{align*}\n\nFinally, finding a patch which satisfies all expected input-output pairs $(C_{loc,m,n},O_{loc,m,n})$ requires to satisfy the following constraint.\n\\begin{align*}\n\\phi&_{PATCH}(B,I,O,CO,I_0,r) =  \\\\\n  \\exists & L \\Big( \\bigwedge_{(C_{loc,m,n},O_{loc,m,n}) \\in CO} \\  \\phi_{FUNC}(B,L,C_{loc,m,n},O_{loc,m,n}) \\\\\n  & \\bigwedge \\ \\phi_{WFF}(B,L,I,O,I_0,r)   \\Big)\n\\end{align*}\n\n\\subsubsection{Complexity Levels of Synthesized Expressions}\n\\label{sec:smt-level}\n\nIdeally, we could feed SMT with many instances of all kinds of building blocks (see Section \\ref{sect:bb}). \nOnly the required building blocks would be wired to the final result.\nThis is an inefficient strategy in practice: some building blocks require expensive computations, e.g., a building block for multiplication (which is a hard problem in SMT).\n\nTo overcome this issue, we use the same technique of complexity levels as SemFix \\cite{nguyen2013semfix}.\nWe first try to synthesize an expression with only one instance of easy building blocks ($<$, $\\leq$, $\\neq$, and $=$)\\footnote{$>$ and $\\geq$ are obtained by symmetry, e.g., $a~\\geq~b$ as $b~\\leq~a$.}.\nThen, we add new building blocks (e.g., building blocks of logical operators and arithmetic operators, successively) and eventually we increase the number of instances of building blocks. We refer to those successive SMT satisfaction trials as the ``SMT level''.\n\n\\subsubsection{Patch Pretty-Printing}\n\\label{subsubsect:patch-print}\n\n\\nopol translates a solution to a patch in source code if there is a feasible solution to the SMT problem. Since \\nopol repairs bugs with \\buggyandpres, the patch after translation is a conditional expression, which returns a boolean value. \n\nThe translation is obtained with a backward traversal starting at the final output location $l_r$, as explained in Algorithm \\ref{alg:patch-pretty-print}. A function $traverse$ returns the traversal result according to the location variables while a function $code$ converts a variable into source code. For example, for a variable $a$, $code(a)$ is translated to $a$; if $\\phi$ denotes the conjunction of boolean values, $code(\\phi(traverse(a),traverse(b)))$ is translated to $a \\wedge b$. As shown in Algorithm \\ref{alg:patch-pretty-print}, patch translation from a SMT solution to a patch is a deterministic algorithm, which generates an identical patch for a given SMT solution. Once a patch is translated from the SMT solution, \\nopol returns this patch to developers as the final patch.   \n\n\\begin{algorithm}[!t]\n\\SetKwFunction{traverse}{traverse}\n\n\\SetKwInOut{Input}{Input}\n\\SetKwInOut{Output}{Output}\n\n\\Input{ \\\\\n\\noindent $L$, an assignment of location variables, i.e., an SMT solution; \\\\\n\\noindent $r$, a final and expected output variable of patch; \\\\\n}\n\\Output{ \\\\\n\\noindent $patch$, a source code patch. \n}\n\n\\BlankLine  \nFind a location variable $l_x = l_r$\\;\n$patch=$ \\traverse{$l_x$}\\; \n\\BlankLine\n\\SetKwProg{myproc}{Function}{}{end}\n\\myproc{\\traverse{$l_x$}}{\n  \\uIf(\\ \\ // Output of a building block){$x \\in O$} \n  {\n    Find the expression $\\phi_x(I_x, x)$ and $I_x=(I_{x,1},I_{x,2},...)$\\; \n    \\KwRet \\footnotesize $code(\\phi_x(traverse(I_{x,1}),traverse(I_{x,2}),...))$\\;  \n  }\n  \\uElseIf(// Input of a building block){$x \\in I$}\n  {\n    Find $y$ for $l_y = l_x$; \\ \\ // $l_y \\in O \\cup I_0$\\\\\n    \\KwRet $traverse(l_y)$\\;\n  }\n  \\Else(\\ \\ // $x \\in I_0$, from collected runtime trace)\n  {\n    \\KwRet $code(x)$\\;\n  }\n}\n\\BlankLine\n\\caption{Translation Algorithm from an SMT Solution to a Source Code Patch.}\n\\label{alg:patch-pretty-print}\n\\end{algorithm}\n\nHere is a possible solution to the SMT instance for our running example (in Section \\ref{subsubsect:wiring}): \n\\noindent \n$l_{i_0}   = 1$,              \n$l_{c_1}   = 2    $,    \n$l_{c_2}   = 3        $,\n$l_{r}    = 5       $,\n$l_{r_{f_1}}  = 4       $, \n$l_{r_{f_2}}  = 5       $, \n$l_{I_{f_1,1}}= 2  $,\n$l_{I_{f_2,1}} = 1    $,\n$l_{I_{f_2,2}} = 1  $.\n\nIn our example, the output is bound to $l_r=5$ that is the output of $f_2$. Then\n$f_2$ takes the integer input value $i_0$ in $l_{i_0}$ as a parameter.\nThe final patch is thus the expression $f_2(i_0, i_0)$ which returns a boolean. This patch could be the repair of a bug, i.e., a fixed \\ourif condition or an added precondition. In this example, $f_1$ is never used.\n\n \n\n\\subsection{Fault Localization}\n\\label{subsect:fault-localization}\n\n\\nopol uses an existing fault localization technique to speed up finding an angelic value, if one exists. In fault localization, statements are ranked according to their suspiciousness. The \\textit{suspiciousness} of a statement measures its likelihood of containing a fault.\n\nIn \\nopol, a spectrum-based ranking metric, Ochiai \\cite{abreu2007accuracy}, is used as the fault localization technique. Existing empirical studies \\cite{steimann2013threats,xuan2014learning} show that Ochiai is more effective on localizing the root cause of faults in object-oriented programs than other fault localization techniques. In Section \\ref{subsect:discussion-fault}, we will compare the effectiveness among different fault localization techniques.  \n\nGiven a program and a test suite, the suspiciousness $susp(s)$ of a statement $s$ is defined as follows. \n\\begin{equation*}\nsusp(s) =  \\frac{failed(s)}{\\sqrt{total\\_failed*(failed(s)+passed(s))}}\n\\end{equation*}\nwhere $total\\_failed$ denotes the number of all the failing test cases and $failed(s)$ and $passed(s)$ respectively denote the number of failing test cases and the number of passing test cases, which cover the statement $s$. Note that $0 \\leq susp(s) \\leq 1$ where $susp(s)=1$ indicates the highest probability of localizing the bug and $susp(s)=0$ indicates there is no likelihood between this statement and the bug.  \n\nWe rank all the statements based on their suspiciousness in descending order. For all the statements with the suspiciousness over zero, we detect whether this statement is an \\ourif statement or not. As previously mentioned in Section \\ref{subsect:overview}, for an \\ourif condition, \\nopol tries to synthesize a new condition while for a non-\\ourif statement, \\nopol tries to add a precondition. \n\n\\section{Automatic Repair of Real-World If Bugs}\n\\label{sect:evaluation}\n\nWe now evaluate our repair approach, \\nopol, on a dataset of \\numbug real-world bugs. First, we describe our evaluation methodology in Section \\ref{subsect:methodology}; second, we introduce the setup of our dataset in Section \\ref{subsect:data-set} and the implementation details in Section \\ref{subsect:impl}; third, we present the general description of the synthesized patches in Section \\ref{subsect:general-repair}; fourth, four bugs are employed as case studies in Section \\ref{subsect:case-study-repair} and five bugs are used to illustrate the limitations in Section \\ref{subsect:limitation}. \n\n\\subsection{Evaluation Methodology}\n\\label{subsect:methodology}\n\nOur evaluation methodology is based on the following principles. \n\n\\textbf{P1}. We evaluate our tool, \\nopol, on real-world buggy programs (Section \\ref{subsect:general-repair}). \n\n\\textbf{P2}. For bugs that \\nopol can fix, we examine the automatically generated patches, and compare them with human-produced patches (Section \\ref{subsect:case-study-repair}). \n\n\\textbf{P3}. For bugs that \\nopol cannot correctly fix, we check the details of these bugs and highlight the reasons behind the unrepairability (Section \\ref{subsect:limitation}). When the root cause is an incorrect test case (i.e., an incomplete specification), we modify the test case and re-run \\nopol.\n\n\\textbf{P4}. We deliberately do not compute a percentage of repaired bugs because this is a potentially unsound measure. According to our previous investigation \\cite{monperrus2014critical}, this measure is sound if and only if 1) the dataset is only composed of bugs of the same kind and 2) the distribution of complexity within the dataset reflects the distribution of all in-the-field bugs within this defect class. In our opinion, the second point is impossible to achieve.\n\nWe have not quantitatively compared our approach against existing repair approaches on the same dataset because\n1) either existing approaches are inapplicable on this dataset (e.g., GenProg \\cite{le2012genprog} and SemFix \\cite{nguyen2013semfix} are designed for C programs);\n2) or these approaches are not publicly available (e.g., PAR \\cite{Kim2013} and mutation-based repair \\cite{debroy2010using}).\n\n\\subsection{Dataset of Real-World Bugs}\n\\label{subsect:data-set}\n\n\\nopol focuses on repairing conditional bugs, i.e., bugs in \\ourif conditions and preconditions. Hence, we build a dataset of 22 real-world bugs of buggy \\ourif conditions and missing preconditions. Since our prototype implementation of \\nopol repairs Java code, these \\numbug bugs are selected from two open-source Java projects, Apache Commons Math\\footnote{Apache Commons Math, \\url{http://commons.apache.org/math/}.} and Apache Commons Lang\\footnote{Apache Commons Lang, \\url{http://commons.apache.org/lang/}.} (\\textit{Math} and \\textit{Lang} for short, respectively).  \n\nBoth Math and Lang manage source code using Apache Subversion\\footnote{Apache Subversion, \\url{http://subversion.apache.org/}.} (SVN for short) and manage bug reports using Jira.\\footnote{Jira for Apache, \\url{http://issues.apache.org/jira/}.} Jira stores the links between bugs and related source code commits. In addition, these projects use the FishEye browser to inspect source code and commits.\\footnote{FishEye for Apache, \\url{http://fisheye6.atlassian.com/}.} \n\nIn our work, we employ the following four steps to collect bugs for the evaluation. \nFirst, we automatically extract small commits that modify or add an \\ourif condition using Abstract Syntax Tree (AST) analysis \\cite{Falleri2014}.\nWe define a \\textit{small commit} as a commit that modifies at most 5 files, each of which introduces at most 10 AST changes (as computed by the analytical method, GumTree \\cite{Falleri2014}).\nIn Math, this step results in 161 commits that update \\ourif conditions and 104 commits that add preconditions; in Lang, the commits are 165 and 91, respectively. The lists of commits are available at the \\nopol project \\cite{nopol2014}.\nSecond, for each extracted commit, we collect its related code revision, i.e., the source program corresponding to this commit. \nWe manually check changes between the code revision and its previous one; we only accept changes that contain an \\ourif condition or a missing precondition and do not affect other statements. Those commits could also contain other changes that relate to neither a bug nor a patch, such as a variable renaming or the addition of a logging statement. \nIn this case, changes of the patch are separated from irrelevant changes. \nThird, we extract the test suite at the time of the patch commit, including failing test cases.\\footnote{In considered commits, bug fixes are always committed together with originally failing test cases (which are passed after fixing the bugs). This is a rule in Apache development process \\cite{apache2014}.} \nFourth, we manually configure programs and test suites to examine whether bugs can be reproduced. \nNote that the reproducibility rate is very low due to the complexity of the projects Math and Lang. \n\n\\begin{table*}[!t]\n\\caption{The Evaluation Dataset of \\nopol. It contains \\numbug bugs related to buggy \\ourif conditions and missing preconditions. }\n\\label{tab:bug}\n\\centering\n\\resizebox{1.01\\textwidth}{!}{\n\\setlength\\tabcolsep{0.2 ex}\n\\begin{tabular}{|c|c|ccc|c|c|c|c|lc|}\n\n\\hline \n\\multirow{2}{*}{ \\tabincell{c}{ Bug \\\\ type }} & \\multirow{2}{*}{Project} & \\multicolumn{3}{c|}{ Bug description }  & \\multirow{2}{*}{\\tabincell{c}{\\scriptsize Executable \\\\ LoC }} &  \\multirow{2}{*}{\\#Classes}  &  \\multirow{2}{*}{\\#Methods}  &  \\multirow{2}{*}{\\tabincell{c}{ \\#Unit \\\\ tests }}    &  \\multicolumn{2}{c|}{ Buggy method }           \\\\ \\cline{3-5}\\cline{10-11}\n &   &   Index &   Commit ID\\dag &   Bug ID\\ddag &    & & & &      Description & \\tiny Complexity      \\\\ \\hline\\hline\n\\multirow{16}{*}{ \\rotatebox{90}{Buggy \\ourif condition} }\n &  Math & CM1  & 141003& -        &    4611 &  153 &  947 &  363 & Returns a specified percentile from an array of real numbers &   7 \\\\\n &  &  CM2  &   141217  & -        &    5539 &  212 & 1390 &  543 & Returns an exact representation of the Binomial Coefficient  &   8 \\\\\n &  &  CM3  &   141473  & -        &    6633 &  191 & 1504 &  654 & Returns the natural logarithm of the factorial for a given value  &   3 \\\\\n &  &  CM4  &   159727  & -        &    7442 &  206 & 1640 &  704 & Computes the a polynomial spline function        &  5 \\\\\n &  &  CM5  &   735178  & Math-238 &   25034 &  468 & 3684 & 1502 &  Gets the greatest common divisor of two numbers & 12 \\\\\n &  &  CM6  &   791766  & Math-280 &   37918 &  632 & 5123 & 1982 & Finds two real values for a given univariate function   & 11 \\\\\n &  &  CM7  &   831510  & Math-309 &   38841 &  650 & 5275 & 2105 & Returns a random value from an Exponential distribution & 3  \\\\\n &  &  CM8  &   1368253 & Math-836 &   64709 &  975 & 7374 & 3982 & Converts a double value into a fraction       & 11 \\\\\n &  &  CM9  &   1413916 & Math-904 &   70317 & 1037 & 7978 & 4263 & Computes a power function of two real numbers & 38 \\\\\n &  &  CM10 &   1453271 & Math-939 &   79701 & 1158 & 9074 & 4827 & Checks whether a matrix has sufficient data to calculate covariance &   3 \\\\  \n & Lang & CL1 & 137231  & -        &   10367 &  156 & 2132 &  781 & Replaces a string with another one inside      &  4 \\\\\n &  &  CL2  &   137371  & -        &   11035 &  169 & 2240 &  793 & Removes a line break at the end of a string   &  4 \\\\\n &  &  CL3  &   137552  & -        &   12852 &  173 & 2579 &  994 & Gets a sub-string from the middle of a string from a given index & 5\\\\\n &  &  CL4  &   230921  & -        &   15818 &  215 & 3516 & 1437 & Finds the first matched string from the given index    & 10 \\\\\n &  &  CL5  &   825420  & Lang-535 &   17376 &   86 & 3744 & 1678 & Extracts the package name from a string       &  6 \\\\\n &  &  CL6  &   1075673 & Lang-428 &   18884 &  211 & 3918 & 1934 & Checks whether the char sequence only contains unicode letters &  4 \\\\\n \\hline\\hline\n\\multirow{6}{*}{ \\rotatebox{90}{ \\tabincell{c}{ Missing \\\\ precondition } } }\n &  Math &PM1 & 620221  & Math-191 &   16575 &  396 & 2803 & 1168 & Checks the status for calculating the sum of logarithms & 1 \\\\\n &  &  PM2  &  1035009  & -        &   44347 &  745 & 5536 & 2236 & Builds a message string from patterns and arguments &   3 \\\\     \n & Lang & PL1 & 504351  & Lang-315 &   17286 &  233 & 4056 & 1710 & Stops a process of timing   &   3 \\\\\n &  &  PL2  &   643953  & Lang-421 &   17780 &  240 & 4285 & 1829 & Erases a string with the Java style from the character stream &   19  \\\\\n &  &  PL3  &   655246  & Lang-419 &   18533 &  257 & 4443 & 1899 & Abbreviates a given string  &   9 \\\\\n &  &  PL4  &   1142389 & Lang-710 &   18974 &  218 & 3887 & 1877 & Counts and translates the code point from an XML numeric entity  &   19  \\\\\n\\hline\\hline\n\\multicolumn{5}{|c|}{ Average }   & 25480.6 & 399.1 & 3960.4 & 1784.6 &   &  8.6  \\\\\n\\multicolumn{5}{|c|}{ Median }    & 17585.0 & 225.5 & 3818.5  & 1694.0    &   &  5.5  \\\\\n\\hline\n       \n\\end{tabular}\n}\n\\\\\n\\ \n\n\\tabfootnote{\\dag \\ A commit ID is an identifier that indicates the commit of the patch, in both SVN and the FishEye system. According to this commit, we can manually check relevant patched code and test cases. For instance, the commit of Bug CM1 can be found at {\\tiny \\url{https://fisheye6.atlassian.com/changelog/commons?cs=141003}.}} \\\\\n\n\\tabfootnote{\\ddag \\ For some bugs, bug IDs are not obviously identified in the bug tracking system. These bugs can be found in the version control system. For example, Apache projects previously used Bugzilla as a bug tracking system before moving to Jira. The Bugzilla system is not available anymore.}\n\\end{table*}\n\nTable \\ref{tab:bug} summarizes the \\numbug bugs in two categories, i.e., bug types of buggy \\ourif conditions and missing preconditions. We index these bugs according to their types and projects. A \\textit{bug index} (Column 3) is named based on the following rule. Letters \\textit{C} and \\textit{P} indicate bugs with \\buggyconditions and \\preconditions, respectively; \\textit{M} and \\textit{L} are bugs from Math and Lang, respectively. For instance, CM1 refers to a bug with a \\buggycondition in the project Math. We also record the number of executable Lines of Code (LoC, i.e., the number of lines that exclude empty lines and comment lines) for each source program (Column 6). \nMoreover, we show the number of classes, the number of methods in the buggy program, and the number of unit test cases (Columns 7-9). \nFor each method that contains the buggy code, we describe the functionality of this method and record its Cyclomatic Complexity (Columns 10 and 11). \nThe \\textit{Cyclomatic Complexity} \\cite{mccabe1976complexity} is the number of linearly independent paths through the source code of a method. This complexity indicates the testability of a method and the difficulty of understanding code by developers. \n\nAs shown in Table \\ref{tab:bug}, the dataset contains 16 bugs with buggy \\ourif conditions and 6 bugs with missing preconditions. Among these bugs, 12 bugs are from Math and 10 bugs are from Lang. In average, a buggy program consists of 25.48K executable lines of code. The average complexity is 8.6; that is, a buggy method consists of 8.6 independent paths in average. Note that the method complexity of Bug PM1 is 1 since its buggy method contains only one \\mycode{throw} statement (which misses a precondition); the method complexity of Bug CM9 is 38 and its buggy method contains 30 \\ourif statements.   \n\n\\subsection{Implementation Details}\n\\label{subsect:impl}\n\nOur approach, \\nopol, is implemented with Java 1.7 on top of Spoon 3.1.0.\\footnote{Spoon 3.1.0, \\url{http://spoon.gforge.inria.fr/}.} Spoon \\cite{pawlak2015spoon} is a library for transforming and analyzing Java source code. It is used for angelic fix localization, instrumentation, and final patch synthesis in our work. Fault localization is implemented with GZoltar 0.0.10.\\footnote{GZoltar 0.0.10, \\url{http://gzoltar.com/}.} GZoltar \\cite{CamposRPA12} is a fault localization library for ranking faulty statements. The SMT solver inside \\nopol is Z3 4.3.2.\\footnote{Z3, \\url{http://github.com/Z3Prover/z3/}.} We generate SMT-LIB\\footnote{SMT-LIB, \\url{http://smt-lib.org/}.} files using jSMTLIB.\\footnote{jSMTLIB, \\url{http://sourceforge.net/projects/jsmtlib/}.} jSMTLIB \\cite{cok2011jsmtlib} is a library for checking, manipulating, and translating SMT-LIB formatted problems. The test driver is JUnit 4.11. For future replication of the evaluation, the code of \\nopol is available on GitHub \\cite{nopolcode2014}. \n\nAll experiments are run on a PC with an Intel Core i7 3.60 GHz CPU and a Debian 7.5 operating system. The maximum heap size of Java virtual machine was set to 2.50 GB. \n\n\\subsection{Main Research Questions}\n\\label{subsect:general-repair}\n\nWe present the general evaluation of \\nopol on the dataset via answering six Research Questions (RQs). \n\n\\medskip\n\\begin{mdframed}\nRQ\\refstepcounter{rqs}\\arabic{rqs}\\label{rq-fix}: Can \\nopol fix real bugs in large-scale Java software?\n\\end{mdframed}\n\nIn test-suite based repair, a bug is \\textit{fixed} if the patched program passes the whole test suite \\cite{le2012genprog}. \nTable \\ref{tab:patch} presents the evaluation of patches on \\numbug bugs. \nColumn 3 shows the buggy code (the condition for each bug with a \\buggycondition and the statement for each bug with a \\precondition).\nColumn 4 shows the patches that were manually-written by developers as found in the version control system: the updated condition for each bug with a \\buggycondition and the added precondition for each bug with a \\precondition. Column 5 presents the generated patches by \\nopol. \nColumn 6 is the result of our manual analysis of the correctness of the patches (will be explained in RQ\\ref{rq-correct}).\nFinally, Column 7 shows whether we had to modify existing test cases: ``A'' stands for additional test cases, ``T'' for transformed test cases, and ``D'' for deleted test cases.\nThe purpose of test case modification is to yield a correct repair (will be explained in RQ\\ref{rq-testcase}).\n\n\\begin{table*}[!t]\n\\caption{Buggy code, manually-written patches, and generated patches for the bugs of the dataset.}\n\\label{tab:patch}\n\\centering\n\\resizebox{1.01\\textwidth}{!}{\n\\setlength\\tabcolsep{0.1 ex}\n\n\\renewcommand\\arraystretch{1.5}\n\\begin{tabular}{|c|c|c|c|c|c|c|}\n\n\\hline \n\\tabincell{c}{Bug \\\\ type} & \\tabincell{c}{Bug \\\\ index} &  Buggy code &Patch written by developers & Patch generated by \\nopol  & \\scriptsize Correctness & \\scriptsize \\tabincell{c}{ Test case\\\\ modification}       \\\\   \\hline\\hline\n\\multirow{16}{*}{ \\rotatebox{90}{Buggy \\ourif condition} } & CM1 & \\mycode{pos > n } & \\mycode{pos >= n }  & \\mycode{length <= fpos} & Correct  & A \\ddag \\\\  \\cline{2-7}\n\n  & CM2 & \\mycode{n <= 0 }  &  \\mycode{n < 0 } & \\mycode{if (n < MathUtils.ZS) \\{ \\ldots \\ } \\dag & Correct   & A T \\\\  \\cline{2-7}\n\n  & CM3 & \\mycode{n <= 0 }  & \\mycode{n < 0 }& \\mycode{if (MathUtils.ZS != n) \\{ \\ldots }& Correct   & A T\\\\  \\cline{2-7}\n  & CM4 & \\mycode{v < knots[0] || v >= knots[n]} & \\mycode{v < knots[0] || v > knots[n]} & \\scriptsize \\texttt{if(v <= -1 || knots.length != v \\&\\& n < v +1)\\{\\ldots}&  Incorrect   & A\\\\ \\cline{2-7}  \n  & CM5 & \\mycode{u * v == 0 }  & \\mycode{u == 0 || v == 0 }& \\mycode{if(u> MathUtils.NB \\&\\& u<= MathUtils.ZB} & Correct   & A T \\\\  \n  &     &                       &                           & \\mycode{|| v == MathUtils.ZB) \\{ \\ldots \\ }       &           & \\\\ \\cline{2-7} \n\n  & CM6 & \\mycode{fa * fb >= 0.0 }  & \\mycode{fa * fb > 0.0} & \\mycode{if (-1 == b) \\{ \\ldots  }   & Incorrect   &  T D \\\\  \\cline{2-7}\n  & CM7 & \\mycode{mean < 0 } & \\mycode{mean <= 0 }& \\mycode{mean <= 0.0}   & Correct   & A T D\\\\  \\cline{2-7}\n  & CM8 & \\mycode{p2 > overflow || q2 > overflow}  & \\mycode{FastMath.abs(p2) > overflow || }  & \\mycode{-} (Timeout in SMT) & -  & - \\\\  \n  &     &                                          & \\mycode{FastMath.abs(q2) > overflow}      &  &   &  \\\\   \\cline{2-7}\n  & CM9 & \\scriptsize \\texttt{y >= TWO\\_POWER\\_52 || y <= -TWO\\_POWER\\_52} & \\scriptsize \\texttt{y >= TWO\\_POWER\\_53 || y <= -TWO\\_POWER\\_53}  & \\mycode{-} (Timeout in test execution) &  -   & -\\\\  \\cline{2-7}\n  \n\n & CM10 & \\mycode{nRows < 2 || nCols < 2 }  & \\mycode{nRows < 2 || nCols < 1 } & \\mycode{nRows < 2} & Correct & -\\\\ \\cline{2-7} \n\n  & CL1 & \\mycode{text == null }  & \\mycode{text == null || repl == null ||}  & \\mycode{if (with.length() == 0) \\{ \\ldots}  & Incorrect  & T D \\\\  \n  &     &                         & \\mycode{with == null || repl.length() == 0}  &                             &   &  \\\\  \\cline{2-7}\n  & CL2 & \\mycode{lastIdx == 0 }  & \\mycode{lastIdx <= 0 }  & \\mycode{str.length() <= blanks.length()}    & Correct &  T\\\\  \\cline{2-7}\n  & CL3 & \\mycode{pos > str.length()}  & \\mycode{len < 0 || pos > str.length()}  & \\mycode{len <= -1 || str.length() < pos}  & Correct   & T\\\\  \\cline{2-7}\n  & CL4 & \\mycode{startIndex >= size}  & \\mycode{substr == null || startIndex >= size} & \\mycode{!(substr != null)||startIndex >= size} & Correct  & T D \\\\ \\cline{2-7} \n  & CL5 & \\mycode{className == null } & \\scriptsize \\texttt{className == null || className.length() == 0 }& \\mycode{className.length() == 0 }    & Incorrect    & T \\\\  \\cline{2-7}   \n  & CL6 & \\mycode{cs == null }  & \\mycode{cs == null || cs.length() == 0 }  & \\mycode{-} (Timeout due to a \\mycode{null} pointer)  & - & T \\\\    \n  \\hline\\hline\n\\multirow{6}{*}{\\rotatebox{90}{\\tabincell{c}{Missing \\\\ precondition }} } & PM1 & \\scriptsize \\texttt{throw new IllegalStateException(\"\")} & \\mycode{if (getN() > 0) \\{ \\ldots }  & \\mycode{-} (No angelic value found) & -  & - \\\\    \\cline{2-7}\n  & PM2 & \\mycode{sb.append(\": \") } & \\mycode{if (specific != null) \\{ \\ldots }  & \\mycode{if (specific != null) \\{ \\ldots}    & Correct  & - \\\\    \\cline{2-7}\n  & PL1 & \\scriptsize \\texttt{stopTime $\\leftarrow$ System.currentTimeMillis()} & \\scriptsize \\texttt{if (this.runningState == STATE\\_RUNNING) \\{ \\ldots}& \\mycode{if (stopTime < STATE\\_RUNNING) \\{ \\ldots}   & Correct  & -\\\\    \\cline{2-7}\n  & PL2 & \\mycode{out.write('\\char`\\\\\\char`\\\\') } & \\mycode{if (escapeForwardSlash) \\{ \\ldots }& \\mycode{if (escapeSingleQuote) \\{ \\ldots }   & Correct  & -\\\\    \\cline{2-7}\n  \n  & PL3 & \\mycode{lower $\\leftarrow$ str.length() }  & \\mycode{if (lower > str.length()) \\{ \\ldots } & \\mycode{-} (Timeout in SMT)   & - & T \\\\    \\cline{2-7} \n  & PL4 & \\mycode{return 0 }  & \\mycode{if (start == seqEnd) \\{ \\ldots }& \\mycode{if (start == seqEnd) \\{ \\ldots  }    & Correct   & - \\\\   \\hline \n\\end{tabular}\n}\n\\ \n\n\\tabfootnote{\\dag \\ An added precondition is in a form of ``\\texttt{if( ) \\{ \\ldots }'' to distinguish with an updated condition. }\\\\\n\n\\tabfootnote{\\ddag \\ For test case modification, ``A'' stands for additional test cases, ``T'' for transformed test cases, and ``D'' for deleted test cases.}\\\\ \n\\end{table*}\n\nAs shown in Table \\ref{tab:patch}, among \\numbug bugs, \\nopol can fix 17 bugs: 13 out of 16 bugs with \\buggyconditions and 4 out of 6 bugs with \\preconditions. Meanwhile, four out of five unfixed bugs relate to timeout. In our work, the execution time of \\nopol is limited to up to five hours. We will empirically analyze the fixed bugs in Section \\ref{subsect:case-study-repair} and explore the limitations of our approach as given by the five unfixed bugs in Section \\ref{subsect:limitation}. \n\nTable \\ref{tab:patch} also shows that patches generated by \\nopol consist of both primitive values and object-oriented features. For the object-oriented features, two major types can be found in the generated patches: nullness checking (patches of Bugs CL4 and PM2) and the \\mycode{length()} method of strings (patches of Bugs CL1, CL2, CL3, and CL5).\n\nNote that six bugs (Bugs CM2, CM3, CM4, CM5, CM6, and CM10) with \\buggyconditions are fixed by adding preconditions rather than updating conditions. One major reason is that a non-\\ourif statement is ranked above the buggy \\ourif statement during the fault localization; then \\nopol adds a patch, i.e., a precondition to this non-\\ourif statement. Hence, the condition inside the buggy \\ourif statement cannot be updated. This shows that those two kinds of patches intrinsically relate to each other. To further understand this phenomenon, we have performed repair only in the mode of ``condition'' in \\nopol: the six bugs could also be fixed via only updating \\ourif conditions. \n\n\\medskip\n\\begin{mdframed}\nRQ\\refstepcounter{rqs}\\arabic{rqs}\\label{rq-correct}: Are the synthesized patches as correct as the manual patches written by the developer?\n\\end{mdframed}\n\nIn practice, a patch should be more than making the test suite pass since test cases may not be enough for specifying program behaviors \\cite{monperrus2014critical}, \\cite{qi2015efficient}. \nIn this paper, a generated patch is \\textit{correct} if and only if the patch is functionally equivalent to the manually-written patch by developers.\n\nFor each synthesized patch, we have followed Qi et al. \\cite{qi2015efficient} to perform a manual analysis of its correctness. \nThe manual analysis consists of understanding the domain (e.g., the mathematical function under test for a bug in the project Math), understanding the role of the patch in the computation, and understanding the meaning of the test case as well as its assertions. \n\nAs shown in Table \\ref{tab:patch}, 13 out of 17 synthesized patches are as correct as the manual patches. Among these 13 correct patches, five patches (for Bugs CM1, CM2, CM3, CM5, and CM7) are generated based on not only the original test suite but also additional test cases. The reason is that the original test suite is too weak to drive the synthesis of a correct patch; then we had to manually write additional test cases (all additional test cases are publicly-available on the companion website \\cite{nopol2014}\\footnote{Additional test cases, \\url{http://sachaproject.gforge.inria.fr/nopol/dataset/data/projects/math/}.}). This will be discussed in next research question. \n\nFor four bugs (Bugs CM4, CM6, CL1, and CL5), we are not able to synthesize a correct patch. This will be further discussed in Section \\ref{subsect:discussion-repair}. \n\n\\medskip\n\\begin{mdframed}\nRQ\\refstepcounter{rqs}\\arabic{rqs}\\label{rq-testcase}: What is the root cause of test case modification?\n\\end{mdframed}\n\nAs shown in Table \\ref{tab:patch}, some bugs are correctly repaired only after the test case modification (including test case addition, transformation, and deletion).  \nThe most important modification is test case addition. \nSix bugs (Bugs CM1, CM2, CM3, CM4, CM5, and CM7) with additional test cases correspond to too weak specifications. \nWe manually added test cases for these bugs to improve the coverage of buggy statements. \nWithout the additional test case, the synthesized patch is degenerated.\nA case study of Bug CM1 (Section \\ref{subsubsect:bug-cm1}) will further illustrate how additional test cases help to synthesize patches. \nNote that all additional test cases appear in the bugs, which are reported in the early stage of the project Math. One possible reason is that the early version of Math is not in test-driven development and the test suites are not well-designed. \n\nIn test case transformation (Bugs CM2, CM3, CM5, CM6, CM7, CL1, CL2, CL3, CL4, CL5, CL6, and PL3), we simply break existing test cases into smaller ones, in order to have one assertion per test case. This is important for facilitating angelic fix localization since our implementation of \\nopol has a limitation, which collects only one runtime trace for a test case (Section \\ref{sect:angelic-fix-localization}). We note that such test case transformation can even be automated \\cite{brefactoring}. \n\nThe reason behind the four bugs with deleted test cases (Bugs CM6, CM7, CL1, and CL4) is accidental and not directly related to automatic repair: these deleted test cases are no longer compatible with the Java version and external libraries, which are used in \\nopol. \n\n\\medskip\n\\begin{mdframed}\nRQ\\refstepcounter{rqs}\\arabic{rqs}\\label{rq-specify}: How are the bugs in the dataset specified by test cases?\n\\end{mdframed}\n\nTo further understand the repair process of bugs by \\nopol, Tables \\ref{tab:patch-feature} and \\ref{tab:nopatch-feature} show the detailed analysis for patched statements in 17 fixed bugs and buggy statements in five non-fixed bugs, respectively. Table~\\ref{tab:patch-feature} gives the following information:\nwhether a synthesized patch is at the same location as the one written by the developer, \nthe number of failing ($e_f$) and passing ($e_p$) test cases executing the patched statements,\nthe fault localization metrics (the rank of the patched statement and the total number of suspicious statements), the overall execution time of \\nopol, and the SMT level (see Section \\ref{sec:smt-level}).\nIn Table \\ref{tab:nopatch-feature}, $e_f$ and $e_p$ denote the number of failing and passing test cases executing the buggy statements while the rank of the buggy statement is listed. \n\nTables \\ref{tab:patch-feature} and \\ref{tab:nopatch-feature} show the numbers of failing ($e_f$) and passing ($e_p$) test cases that execute the patched or buggy statements. Such numbers reflect to which extent the statement under repair is specified by test cases.\nAs shown in Table \\ref{tab:patch-feature}, the average of $e_f$ and $e_p$ are 1.8 and 7.1 for 17 bugs with synthesized patches. In Table \\ref{tab:nopatch-feature}, the average of $e_f$ and $e_p$ are 4.4 and 27.4 for five bugs without patches. \n\nFor all 22 bugs under evaluation, only one bug has a large number of failing test cases  ($e_f\\geq 10$): Bug PL3 with $e_f=18$. For this bug, although the buggy statement is ranked at the first place, \\nopol fails in synthesizing the patch. \nThis failure is caused by an incorrectly identified output of a precondition. \nSection \\ref{subsubsect:limit-pl3} will explain the reason behind this failure.\n\n\\begin{table}[!t]\n\\caption{Analysis of the 17 fixed bugs (patched statement).}\n\\label{tab:patch-feature}\n\\centering\n\\resizebox{0.5\\textwidth}{!}{\n\\setlength\\tabcolsep{0.2 ex}\n\\begin{tabular}{|c|c|c|cc|cc|c|c|}\n\n\\hline \n\\multirow{2}{*}{ \\tabincell{c}{ Bug \\\\ type }}  & \\multirow{2}{*}{ \\tabincell{c}{ Bug \\\\ index}} & \\multirow{2}{*}{\\scriptsize \\tabincell{c}{ Patch \\\\location }} &  \\multicolumn{2}{c|}{\\scriptsize \\#Test cases }  & \\multirow{2}{*}{\\scriptsize \\tabincell{c}{\\textbf{Patched} \\\\ statement rank }}  & \\multirow{2}{*}{ \\scriptsize \\tabincell{c}{  \\#Suspicious \\\\statements \\ddag}} &  \\multirow{2}{*}{\\tiny \\tabincell{c}{Execution time\\\\ (seconds)}}  &  \\multirow{2}{*}{ \\tabincell{c}{SMT \\\\ level}} \\\\ \\cline{4-5}\n  & & & $e_f$\\dag & $e_p$\\dag  & & & & \\\\\n\\hline\\hline\n\n\\multirow{13}{*}{ \\rotatebox{90}{Buggy \\ourif condition} } & CM1 & \\scriptsize  Same as dev. & 1 & 4 & 57 & 203  &  12  & 2  \\\\  [0.5ex]\n  & CM2 & Different & 1 & 2 & 179 & 559 &  11  & 2 \\\\\n  & CM3 & Different & 1 & 2 & 26 & 35  &  10 & 2 \\\\\n  & CM4 & Different & 6 & 2 & 24  & 114 &  13  & 3  \\\\  [0.5ex]\n  & CM5 & Different & 4 & 4 & 3 & 60 &  43  & 3  \\\\  [0.5ex]\n  & CM6 & Different & 2 & 0 & 17  & 254 &  41  & 2  \\\\\n  & CM7 & \\scriptsize  Same as dev. & 3 & 13 & 143  & 155  &  51  & 2  \\\\  [0.5ex]\n  & CM10  & \\scriptsize  Same as dev. & 1 & 3 & 89 & 102 &  21  & 1  \\\\\n  & CL1 & Different & 2 & 4 & 2 & 8 &  32  & 2 \\\\  [0.5ex]\n  & CL2 & \\scriptsize  Same as dev. & 1 & 9 & 2 & 3 &  6 & 2  \\\\  [0.5ex]\n  & CL3 & \\scriptsize  Same as dev. & 2 & 10 & 4 & 5 &  7  & 3  \\\\  [0.5ex]\n  & CL4 & \\scriptsize  Same as dev. & 2 & 23 & 1 & 20  &  10  & 3  \\\\  [0.5ex]\n  & CL5 & \\scriptsize  Same as dev. & 1 & 36 & 1 & 2 &  37  & 1  \\\\  \n  \\hline \\hline \n\\multirow{4}{*}{ \\rotatebox{90}{\\scriptsize \\tabincell{c}{Missing\\\\ precondition}}}   & PM2 & \\scriptsize  Same as dev. & 1 & 2 & 1 & 12  &  84  & 1  \\\\  [0.5ex]\n  & PL1 & \\scriptsize  Same as dev. & 1 & 4 & 6 & 22  &  32  & 2  \\\\  [0.5ex]\n  & PL2 & \\scriptsize  Same as dev. & 1 & 1 & 1 & 21  &  6 & 1  \\\\  [0.5ex]\n  & PL4 & \\scriptsize  Same as dev. & 1 & 1 & 1 & 25  &  6 & 2  \\\\ \n  \\hline\\hline\n\\multicolumn{2}{|c|}{  Median } &  \\multirow{2}{*}{}   & 1  & 4  & 4 & 25 &  13 & 2\\\\  [0.5ex]\n\\multicolumn{2}{|c|}{  Average } &                     & 1.8  & 7.1 & 32.8 & 94.1   &  24.8 & 2 \\\\\n\n\\hline\n\\end{tabular}\n}\n\\ \n\n\\tabfootnote{\\dag \\ $e_f$ and $e_p$ denote the number of failing and passing test cases that execute the \\textbf{patched} statement.}\\\\\n\n\\tabfootnote{\\ddag \\ \\#Suspicious statements denotes the number of statements whose suspiciousness scores by fault localization are over zero.}\\\\\n\n\\end{table}\n\n\\begin{table}[!t]\n\\caption{Analysis of the 5 non-fixed bugs (buggy statement).}\n\\label{tab:nopatch-feature}\n\\centering\n\\resizebox{0.45\\textwidth}{!}{\n\\setlength\\tabcolsep{0.3 ex}\n\\begin{tabular}{|c|c|cc|cc|c|}\n\n\\hline \n\\multirow{2}{*}{ \\tabincell{c}{ Bug \\\\ type }}  & \\multirow{2}{*}{ \\tabincell{c}{ Bug \\\\ index}} & \\multicolumn{2}{c|}{\\scriptsize \\#Test cases }  & \\multirow{2}{*}{\\scriptsize \\tabincell{c}{\\textbf{Buggy} \\\\statement rank }}  & \\multirow{2}{*}{ \\scriptsize \\tabincell{c}{  \\#Suspicious \\\\statements \\ddag}} &  \\multirow{2}{*}{\\scriptsize \\tabincell{c}{Execution time\\\\ (seconds) }}   \\\\ \\cline{3-4}\n  & & $e_f$\\dag & $e_p$\\dag  & & & \\\\\n\n\\hline\\hline\n\\multirow{3}{*}{\\scriptsize \\tabincell{c}{Buggy \\ourif\\\\ condition}}  \n  & CM8 & 1 & 51 & 21  & 77  &  -  \\\\  [0.5ex]\n  & CM9 & 1 & 73 & 1203 & 1606 &  -  \\\\  [0.5ex]\n  & CL6 & 1 & 10 & 4 & 4 &  -   \\\\    \n  \\hline\\hline\n\n\\multirow{2}{*}{\\scriptsize \\tabincell{c}{Missing \\\\precondition}} \n  & PM1 & 1 & 0 & 52 & 132 &  37  \\\\  [0.5ex]\n  & PL3 & 18  & 3 & 1 & 16  & - \\\\  \n  \\hline\\hline\n\n\\multicolumn{2}{|c|}{  Median }  & 1  & 10 & 21 & 77 &  -  \\\\  [0.5ex]\n\\multicolumn{2}{|c|}{  Average } & 4.4  & 27.4 & 256.2 & 367    &  -  \\\\\n\\hline\n\n\\end{tabular}\n}\n\\ \n\n\\tabfootnote{\\dag \\ $e_f$ and $e_p$ denote the number of failing and passing test cases that execute the \\textbf{buggy} statement.}\\\\\n\n\\tabfootnote{\\ddag \\ \\#Suspicious statements denotes the number of statements whose suspiciousness scores by fault localization are over zero.}\\\\\n\n\\end{table}\n\n\\medskip\n\\begin{mdframed}\nRQ\\refstepcounter{rqs}\\arabic{rqs}: Where are the synthesized patches localized? How long is the repair process? \n\\end{mdframed}\n\nA patch could be localized in a different location from the patch which is manually-written by developers. We present the details for patch locations for all the 17 patched bugs in Table \\ref{tab:patch-feature}. \nFor 11 out of 17 fixed bugs, the locations of patched statements (i.e., locations of fixes) are exactly the same as those of the buggy ones. For the other six bugs, i.e., Bugs CM2, CM3, CM4, CM5, CM6, and CL1, \\nopol generates patches by adding new preconditions rather than updating existing conditions, as mentioned in Table \\ref{tab:patch}. \n\nFor 17 fixed bugs in Table \\ref{tab:patch-feature}, the average execution time of repairing one bug is 24.8 seconds while for five non-fixed bugs in Table \\ref{tab:nopatch-feature}, four bugs are run out of time and the other one spends 37 seconds. The execution time of all the 22 bugs ranges from 6 to 84 seconds. We consider that such execution time, i.e., fixing one bug within 90 seconds, is acceptable. \n\nIn practice, if applying \\nopol to a buggy program, we can directly set a timeout, e.g., 90 seconds (over 84 seconds as shown in Table \\ref{tab:patch-feature}) or a longer timeout like five hours in our experiment. Then for any kind of buggy program (without knowing whether the bug is with a buggy condition or a missing precondition), \\nopol will synthesize a patch, if it finds any. Then a human developer can check whether this patch is correct from the user perspective. \n\n\\medskip\n\\begin{mdframed}\nRQ\\refstepcounter{rqs}\\arabic{rqs}:  How effective is fault localization for the bugs in the dataset?\n\\end{mdframed}\n\nFault localization is an important step in our repair approach. As shown in Table \\ref{tab:patch-feature}, for patched statements in 17 fixed bugs, the average fault localization rank is 32.8. In four out of 17 bugs (Bugs CM1, CM2, CM7, and CM10), patched statements are ranked over 50. This fact indicates that there is room for improving the fault localization techniques. Among the five unfixed bugs, the buggy statements of Bugs CM9 and PM1 are ranked over 50. Section \\ref{subsect:discussion-fault} will further compare the effectiveness of six fault localization techniques on 22 bugs. \n\nNote that in Tables \\ref{tab:patch-feature} and \\ref{tab:nopatch-feature}, Bugs CM6 and PM1 have no passing test cases. Bug PM1 cannot be fixed by our approach while Bug CM6 can be still fixed because the two failing test cases give a non-trivial input-output specification.\nThe reason behind the unfixed Bug PM1 is not $e_p=0$, but the multiple executions of the buggy code by one test case. This reason will be discussed in Section \\ref{subsubsect:limit-pm1}.\n\n\\subsection{Case Studies for Fixed Bugs}\n\\label{subsect:case-study-repair}\n\nWe conduct four case studies to show how \\nopol fixes bugs with \\buggyconditions and \\preconditions. These bugs are selected because they highlight different facets of the repair process. The patch of Bug PL4 (Section \\ref{subsubsect:bug-pl4}) is syntactically the same as the manually-written one; \nthe patch of Bug CL4 (Section \\ref{subsubsect:bug-cl4}) is as correct as the manually-written patch (beyond passing the test suite); \nthe patch of Bug CM2 (Section \\ref{subsubsect:bug-cm2}) is correct by adding a precondition rather than updating the buggy condition, as written by developers; \nand the patch of Bug CM1 (Section~\\ref{subsubsect:bug-cm1}) is correct, but its patch requires an additional test case. \n\n\\subsubsection{Case Study 1, Bug PL4}\n\\label{subsubsect:bug-pl4}\n\n\\nopol can generate the same patches for three out of \\numbug bugs as the manually-written ones. \nWe take Bug PL4 as an example to show how the same patch is generated. This bug is fixed by adding a precondition. Fig. \\ref{fig:bug-pl4} shows a method \\mycode{translate()} at Line 1 and the buggy method \\mycode{translateInner()} at Line 9 of Bug PL4. The method \\mycode{translate()} is expected to translate a term in the regular expression of \\mycode{\\&\\#[xX]?\\char`\\\\d+;?} into codepoints, e.g., translating the term \\mycode{\"\\&\\#x30\"} into \\mycode{\"\\char`\\\\u0030\"}. \n\nTo convert from \\mycode{input} to codepoints, the characters in \\mycode{input} are traversed one by one. Note that for a string ending with \\mycode{\"\\&\\#x\"}, no codepoint is returned. Lines 15 to 20 in Fig. \\ref{fig:bug-pl4} implement this functionality. However, the implementation at Lines 17 to 19 ignores a term in a feasible form of \\mycode{\"\\&\\#[xX]\\char`\\\\d+\"}, e.g., a string like \\mycode{\"\\&\\#x30\"}. A precondition should be added to detect this feasible form, i.e., the comment at Line 18 of \\mycode{start == seqEnd}.\n\nThe buggy code at Line 19 is executed by one passing test case and one failing test case. Table \\ref{tab:test-pl4} shows these two test cases. For the passing test case, the behavior of the method is expected not to change the variable \\mycode{input} while for the failing test case, the \\mycode{input} is expected to be converted. In the passing test case, the value of the precondition of the statement at Line 19 is expected to be \\mycode{true}, i.e., both \\mycode{start} and \\mycode{seqEnd} equal to \\mycode{8}, while in the failing test case, the condition is expected to be \\mycode{false}, i.e., \\mycode{start} and \\mycode{seqEnd} are \\mycode{8} and \\mycode{19}, respectively. The \\mycode{false} value is the angelic value for a missing precondition.   \n\n\\begin{figure}[!t]\n\\centering\n\\noindent\\begin{minipage}{0.4\\textwidth}\n\\begin{lstlisting}[numbers=left]\nString translate(CharSequence input, int index) {\n  int consumed = translateInner(input, index);\n  if(consumed == 0)\n    ... //  Return the original input value\n  else\n    ... //  Translate code points\n}\n\nint translateInner(CharSequence input, int index) {\n  int seqEnd = input.length();\n  ...\n  int start = index + 2;\n  boolean isHex = false;\n  char firstChar = input.charAt(start);\n  if(firstChar == 'x' || firstChar == 'X') {\n    start++;\n    isHex = true;\n//  FIX: if(start == seqEnd)    \n    return 0;\n  }\n  int end = start;\n  //Traverse the input and parse into codepoints\n  while(end < seqEnd && ... )\n  ...  \n}\n\\end{lstlisting}\n\n\\end{minipage}\n\\caption{Code snippet of Bug PL4. The manually-written patch is shown in the \\mycode{FIX} comment at Line 18. Note that the original method \\mycode{translate} consists of three overloaded methods; for the sake of simplification, we use two methods \\mycode{translate} and \\mycode{translateInner} instead.}\n\\label{fig:bug-pl4}\n\\end{figure}\n\n\\begin{table}[!t]\n\\centering\n\\caption{Sample of test cases for Bug PL4}\n\\label{tab:test-pl4}\n\n\\resizebox{0.51\\textwidth}{!}{\n\\setlength\\tabcolsep{0.2 ex}\n\\begin{tabular}{|cc|cc|c|}\n\n\\hline\n\\multicolumn{2}{|c|}{Input} & \\multicolumn{2}{c|}{Output, \\mycode{translate(input)}} & \\multirow{2}{*}{ \\tabincell{c}{ Test \\\\ result} } \\\\ \n\\cline{1-4}\n \\mycode{input} & \\mycode{index} &  Expected &  Observed & \\\\\n\\hline \\hline\n\\mycode{\"Test \\&\\#x\"} & 5 &  \\mycode{\"Test \\&\\#x\"} & \\mycode{\"Test \\&\\#x\"}  & Pass\\\\\n\\mycode{\"Test \\&\\#x30 not test\"} & 5 & \\mycode{\"Test \\char`\\\\u0030 not test\"} & \\mycode{\"Test \\&\\#x30 not test\"} & Fail\\\\\n\n\\hline\n\\end{tabular}\n}\n\\end{table}\n\nAccording to those expected precondition values for test cases, \\nopol generates a patch via adding a precondition, i.e., \\mycode{start == seqEnd}, which is exactly the same as the manually-written patch by developers. Besides the patch of Bug PL4, patches of Bugs CM7 and PM2 are also syntactically the same as the patch written by developers, among \\numbug bugs in our dataset.\n\n\\subsubsection{Case Study 2, Bug CL4}\n\\label{subsubsect:bug-cl4}\n\nFor several bugs, \\nopol generates patches that are literally different from the manually-written patches, but these generated patches are as correct as manually-written patches. In this section, we present a case study where \\nopol synthesizes a correct patch for a bug with a \\buggycondition. \nBug CL4 in Lang fails to find the index of a matched string in a string builder. Fig. \\ref{fig:bug-cl4} presents the buggy method of Bug CL4: to return the first index of \\mycode{substr} in a parent string builder from a given basic index \\mycode{startIndex}. The condition at Line 4 contains a mistake of \\mycode{startIndex >= size}, which omits checking whether \\mycode{substr == null}. A variable \\mycode{size} is defined as the length of the parent string builder. The manually-written fix is shown at Line 3. \n\nThe buggy code at Line 4 in Fig. \\ref{fig:bug-cl4} is executed by 23 passing test cases and two failing test cases. One of the passing test cases and two failing test cases are shown in Table \\ref{tab:test-cl4}. For the passing test case, a value \\mycode{-1} is expected because no matched string is found. For the two failing test cases, each input \\mycode{substr} is a \\mycode{null} value, which is also expected to return a non-found index \\mycode{-1}. This requires the checking of \\mycode{null} to avoid \\mycode{NullPointerException}, i.e., the condition at Line 3.     \n\nFor the passing test case in Table \\ref{tab:test-cl4}, the condition at Line 4 is \\mycode{false}. For the two failing test cases, \\nopol extracts the angelic value \\mycode{true} to make both failing test cases pass. According to these condition values, a patch of \\mycode{! (substr != null) || startIndex >= size} can be synthesized. \nThis synthesized patch is equivalent to \\mycode{substr == null || startIndex >= size}, which is correct.\nThe resulted expression based on the solution to the SMT does not weaken the repairability of synthesized patches.\nA recent method for finding simplified patches, proposed by Mechtaev et al. \\cite{mechtaev2015directfix}, could be used to avoid such a redundant expression. \n\n\\iffalse\nFor the passing test case, the condition at Line 4 is \\mycode{false}. For the two failing test cases, \\nopol extracts the angelic value \\mycode{true} to make both failing test cases pass. According to these condition values, a patch of \\mycode{substr == null || startIndex == size} can be synthesized. \n\nHowever, this patch is different from the manually-written one at Line 3. The difference is that the generated patch omits the comparison of \\mycode{startIndex > size}. But the following code at Lines 9 to 20 in Fig. \\ref{fig:bug-cl4} shows that the checking of \\mycode{startIndex > size} is not necessary. The \\mycode{for} loop at Line 13 starts the traversal of the string builder. Once \\mycode{startIndex > size} holds, the loop at Line 13 does not perform and the execution of the code jumps to Line 24. The code at Line 24 returns the same value as the expected values in the failing test cases. Hence the implementation of checking \\mycode{startIndex > size} already exists in the code. Such existing implementation inside the source code makes the equivalent patches possible. \n\\fi\n\n\\subsubsection{Case Study 3, Bug CM2} \n\\label{subsubsect:bug-cm2}\n\nIn this section, we present Bug CM2, a correctly patched bug via adding a precondition, rather than updating an existing condition, as written by developers. \nThe buggy method in Bug CM2 is to calculate the value of Binomial Coefficient by choosing \\mycode{k}-element subsets from an \\mycode{n}-element set. Fig. \\ref{fig:bug-cm2} presents the buggy method. The input number of elements \\mycode{n} should be no less than zero. But the condition at Line 4 reads \\mycode{n <= 0} instead of \\mycode{n < 0}. The manually-written patch by developers is in the \\mycode{FIX} comment at Line 4. \n\n\\begin{figure}[!t]\n\\centering\n\\noindent\\begin{minipage}{0.4\\textwidth}\n\\begin{lstlisting}[numbers=left]\nint indexOf(String substr, int startIndex) {\n  startIndex = (startIndex < 0 ? 0 : startIndex);\n// FIX: if (substr == null || startIndex >= size) {\n  if (startIndex >= size) {\n    return -1;\n  }\n  int strLen = substr.length();\n  if (strLen > 0 && strLen <= size) {\n    if (strLen == 1) \n      return indexOf(substr.charAt(0), startIndex);\n    char[] thisBuf = buffer;\n    outer:\n    for (int i = startIndex; i < thisBuf.length\n            - strLen; i++) {\n      for (int j = 0; j < strLen; j++) {\n        if (substr.charAt(j) != thisBuf[i + j]) \n          continue outer;\n      }\n      return i;\n    }\n  } else if (strLen == 0) {\n    return 0;\n  }\n  return -1;\n}\n\\end{lstlisting}\n\n\\end{minipage}\n\\caption{Code snippet of Bug CL4. The manually-written patch is shown in the \\mycode{FIX} comment at Line 3, which updates the buggy \\ourif condition at Line 4.}\n\\label{fig:bug-cl4}\n\\end{figure}\n\n\\begin{table}[!t]\n\\centering\n\\caption{Sample of test cases for Bug CL4}\n\\label{tab:test-cl4}\n\n\\resizebox{0.51\\textwidth}{!}{\n\\setlength\\tabcolsep{0.2 ex}\n\\begin{tabular}{|ccc|cc|c|}\n\n\\hline\n\\multicolumn{3}{|c|}{Input} &  \\multicolumn{2}{c|}{\\tiny Output, \\texttt{indexOf(substr, startIndex)}} & \\multirow{2}{*}{ \\tabincell{c}{ Test \\\\ result} }  \\\\ \n\\cline{1-5}\n\\mycode{parent} & \\mycode{substr} & \\mycode{startIndex} &  Expected &  Observed & \\\\\n\\hline\\hline\nabab & z & 2 & -1 & -1 & Pass \\\\\nabab & (String) null & 0  & -1 & \\tiny \\texttt{NullPointerException} & Fail  \\\\\nxyzabc & (String) null & 2 & -1 &  \\tiny \\texttt{NullPointerException} & Fail  \\\\\n\\hline\n\\end{tabular}\n}\n\\end{table}\n\nThe buggy code at Line 4 in Fig. \\ref{fig:bug-cm2} is executed by two passing test cases and one failing test case. Table \\ref{tab:test-cm2} shows one passing test case and one failing test case. For the passing test case, an expected exception is observed; for the failing test case, an \\mycode{IllegalArgumentException} is thrown rather than an expected value. \n\nTo fix this bug, \\nopol generates a patch via adding a missing precondition \\mycode{n < MathUtils.ZS} to the statement at Line 5, where \\mycode{MathUtils.ZS} is a constant equal to \\mycode{0}. \nThen this statement owns two embedded preconditions, i.e., \\mycode{n <= 0} and \\mycode{n < 0}. \nHence, the generated patch is equivalent to the manually-written patch, i.e., updating the condition at Line 4 from \\mycode{n <= 0} to \\mycode{n < 0}. \nThe reason of adding a precondition instead of updating the original condition is that the statement at Line 5 is ranked prior to the statement at Line 4. This has been explained in Section \\ref{subsect:general-repair}.  \nConsequently, the generated patch of Bug CM2 is correct and syntactically equivalent to the manually-written patch.\n\n\\begin{figure}[!t]\n\\centering\n\\noindent\\begin{minipage}{0.4\\textwidth}\n\\begin{lstlisting}[numbers=left]\nlong binomialCoefficient(int n, int k) {\n  if (n < k) \n    throw new IllegalArgumentException(...);\n  if (n <= 0)    //  FIX: if (n < 0)  \n    throw new IllegalArgumentException(...);\n  if ((n == k) || (k == 0)) \n    return 1;\n  if ((k == 1) || (k == n - 1)) \n    return n;\n  long result = Math.round(\n      binomialCoefficientDouble(n, k));\n  if (result == Long.MAX_VALUE) \n    throw new ArithmeticException(...);\n  return result;\n}\n\\end{lstlisting}\n\n\\end{minipage}\n\\caption{Code snippet of Bug CM2. The manually-written patch is shown in the \\mycode{FIX} comment at Line 4.}\n\\label{fig:bug-cm2}\n\\end{figure}\n\n\\begin{table}[!t]\n\\centering\n\\caption{Sample of test cases for Bug CM2}\n\\label{tab:test-cm2}\n\n\\resizebox{0.47\\textwidth}{!}{\n\\setlength\\tabcolsep{0.3 ex}\n\\begin{tabular}{|cc|cc|c|}\n\n\\hline\n\\multicolumn{2}{|c|}{Input} & \\multicolumn{2}{c|}{Output, \\mycode{\\scriptsize{binomialCoefficient(n,k)}} } & \\multirow{2}{*}{ \\tabincell{c}{ Test \\\\ result} } \\\\ \n\\cline{1-4}\n \\mycode{n} & \\mycode{k} &  Expected&  Observed& \\\\\n\\hline \\hline\n -1 & -1 & Exception & Exception & Pass\\\\\n 0 & 0 &  1 & Exception  & Fail\\\\\n\\hline\n\\end{tabular}\n}\n\\end{table}\n\n\\subsubsection{Case Study 4, Bug CM1}\n\\label{subsubsect:bug-cm1}\n\nInsufficient test cases lead to trivial patch generation. \nWe present Bug CM1 in Math, with a \\buggycondition. This bug cannot be correctly patched with the original test suite due to the lack of test cases. In our work, we add two test cases to support the patch generation. \nFig. \\ref{fig:bug-cm1} presents the buggy source code in the method \\mycode{evaluate()} of Bug CM1. This method returns an estimate of the percentile \\mycode{p} of the values stored in the array \\mycode{values}. \n\nAccording to the API document, the algorithm of \\mycode{evaluate()} is implemented as follows. \nLet \\mycode{n} be the length of the (sorted) array. The algorithm computes the estimated percentile position \\mycode{pos = p * (n + 1) / 100} and the difference \\mycode{dif} between \\mycode{pos} and \\mycode{floor(pos)}. If \\mycode{pos >= n}, then the algorithm returns the largest element in the array; otherwise the algorithm returns the final calculation of percentile. Thus, the condition at Line 15 in Fig. \\ref{fig:bug-cm1} contains a bug, which should be corrected as \\mycode{pos >= n}. \n\nAs shown in Table \\ref{tab:patch-feature}, this bug is executed by four passing test cases and one failing test case. Table \\ref{tab:test-cm1} shows one of the four passing test cases and the failing test case. In the failing test case, an \\mycode{ArrayIndexOutOfBounds} exception is thrown at Line 16. For the passing test case, the value of the condition at Line 15 is equal to the value of the existing condition \\mycode{pos > n}, i.e., \\mycode{true}; for the failing test case, setting the condition to be \\mycode{true} makes the failing test case pass; that is, the angelic value for the failing test case is also \\mycode{true}. Thus, according to these two test cases, the generated patch should make the condition be \\mycode{true} to pass both test cases. \n\n\\begin{figure}[!t]\n\\centering\n\\noindent\\begin{minipage}{0.4\\textwidth}\n\\begin{lstlisting}[numbers=left]\ndouble evaluate(double[] values, double p) {\n  ...\n  int length = values.length;\n  double n = length;\n  ...\n  double pos = p * (n + 1) / 100;\n  double fpos = Math.floor(pos);\n  int intPos = (int) fpos;\n  double dif = pos - fpos;\n  double[] sorted = new double[n];\n  System.arraycopy(values, 0, sorted, 0, n);\n  Arrays.sort(sorted);\n  if (pos < 1) \n      return sorted[0];   \n  if (pos > n)     //  FIX: if (pos >= n) \n      return sorted[n - 1];    \n  double lower = sorted[intPos - 1];\n  double upper = sorted[intPos];\n  return lower + dif * (upper - lower);\n}\n\\end{lstlisting}\n\n\\end{minipage}\n\\caption{Code snippet of Bug CM1. The manually-written patch is shown in the \\mycode{FIX} comment at Line 15.}\n\\label{fig:bug-cm1}\n\\end{figure}\n\n\\begin{table}[!t]\n\\centering\n\\caption{Two original test cases and one additional test case for Bug CM1}\n\\label{tab:test-cm1}\n\n\\resizebox{0.48\\textwidth}{!}{\n\\setlength\\tabcolsep{0.3 ex}\n\\begin{tabular}{|cc|cc|c|}\n\n\\hline\n\\multicolumn{2}{|c|}{Input} & \\multicolumn{2}{c|}{Output, \\mycode{\\scriptsize evaluate(values,p)}} & \\multirow{2}{*}{ \\tabincell{c}{ Test \\\\ result} } \\\\ \n\\cline{1-4}\n \\mycode{values} & \\mycode{p} &  Expected&  Observed& \\\\\n\\hline \\hline\n \\multicolumn{5}{|c|}{Two original test cases} \\\\ \\hline\n \\{0,1\\} & 25 & 0.0 & 0.0 & Pass\\\\\n \\{1,2,3\\} & 75 &  3.0 & Exception  & Fail\\\\\n\\hline \\hline\n \\multicolumn{5}{|c|}{Two additional test cases} \\\\ \\hline \n \\{1,2,3\\} & 100 &  3.0 & 3.0 & Pass \\\\\n\\hline\n\\end{tabular}\n}\n\\end{table}\n\n\\begin{table*}[!t]\n\\caption{Summary of five limitations.}\n\\label{tab:limitation}\n\\centering\n\\resizebox{1\\textwidth}{!}{\n\\setlength\\tabcolsep{0.7 ex}\n\\begin{tabular}{|c|lp{45 ex}p{62 ex}|}\n\n\\hline\nBug index & Root cause & Result of repair & Reason for the unfixed bug \\\\ \\hline\\hline\nPM1 & Angelic fix localization & No angelic value found. Termination before runtime trace collection and patch synthesis & One failing test case executes the missing precondition for more than once. \\\\  [0.6 ex]\nCM9 & Angelic fix localization & Timeout during test suite execution &  An infinite loop is introduced during the trial of angelic values.  \\\\ [0.6 ex]\nPL3 & Runtime trace collection & Timeout during SMT solving & The expected value of a precondition is incorrectly identified.  \\\\ [0.6 ex]\nCM8 & Patch synthesis & Timeout during SMT solving& A method call with parameters is not handled by SMT.  \\\\ [0.6 ex]\nCL6 & Patch synthesis & Timeout during SMT solving & A method of a \\mycode{null} object yields an undefined value for SMT. \\\\\n\\hline\n\n\\hline\n\\end{tabular}\n}\n\\end{table*}\n\nWith the original test suite, \\nopol generates a patch as \\mycode{sorted.length <= intPos}, which passes all test cases. This patch is incorrect. \nTo obtain a correct patch (the one shown in Table \\ref{tab:patch}), we add one test case of \\mycode{values $\\leftarrow$ \\{1,2,3\\}}, \\mycode{p $\\leftarrow$ 100}, as shown in Table~\\ref{tab:test-cm1}. Then the expected value of \\mycode{evaluate()} is \\mycode{3.0}. After running \\nopol, a patch of \\mycode{length <= fpos}, which is different from the manually-written one, i.e., \\mycode{pos >= n}. However, from the source code at Line 7, \\mycode{fpos} is the \\mycode{floor()} value of \\mycode{pos}, i.e., \\mycode{fpos} is the largest integer that no more than \\mycode{pos}. That is, \\mycode{fpos <= pos}. Meanwhile, \\mycode{n == length} holds according to Line 4. As a result, the generated patch \\mycode{length <= fpos} implies the manually-written one, i.e., \\mycode{pos >= n}. We can conclude that \\nopol can generate a correct patch for this bug by adding one test case. \n\n\\subsection{Limitations}\n\\label{subsect:limitation}  \n\nAs shown in Section \\ref{subsect:general-repair}, we have collected five bugs that reveal five different limitations of \\nopol. Table \\ref{tab:limitation} lists these five bugs in details. We analyze the related limitations in this section.  \n\n\\subsubsection{No Angelic Value Found}\n\\label{subsubsect:limit-pm1}\n\nIn our work, for a buggy \\ourif condition, we use angelic fix localization to flip the boolean value of conditions for failing test cases. \nFor Bug PM1, no angelic value is found as shown in Table \\ref{tab:patch}. \nThe reason is that both \\mycode{then} and \\mycode{else} branches of the \\ourif condition are executed by one failing test case. Hence, no single angelic value (\\texttt{true} or \\texttt{false}) can enable the test case to pass. As discussed in Section \\ref{subsubsect:search-space}, the search space of a sequence of angelic values is exponential and hence discarded in our implementation of \\nopol.\n\nTo mitigate this limitation, a straightforward solution is to discard the failing test case, which leads to no angelic values (keeping the remaining failing ones). However, this may decrease the quality of the generated patch due to the missing test data and oracles. Another potential solution is to refactor test cases into small snippets, each of which covers only \\mycode{then} or \\mycode{else} branches \\cite{brefactoring}. A recent proposed technique SPR \\cite{DBLP:conf/sigsoft/LongR15} could help \\nopol to enhance its processing of sequential angelic values.\n\n\\subsubsection{Performance Bugs Caused by Angelic Values}\n\n\\nopol identifies angelic values as the input of patch synthesis. In the process of angelic fix localization, all failing test cases are executed to detect conditional values that make failing test cases pass (see Section \\ref{sect:angelic-fix-localization}). However, sometimes the trial of angelic fix localization (forcing to \\mycode{true} or \\mycode{false}) may result in a performance bug. In our work, Bug CM9 cannot be fixed due to this reason, i.e., an infinite loop caused by angelic fix localization.\n\nA potential solution to this issue is to set a maximum execution time to avoid the influence of performance bugs. But a maximum execution time of test cases may be hard to be determined according to different test cases. For instance, the average execution time of test cases in Math 3.0 is much longer than that in Math 2.0. We leave the setting of maximum execution time as one piece of future work. \n\n\\subsubsection{Incorrectly Identified Output of a Precondition}\n\\label{subsubsect:limit-pl3}\n\nAs mentioned in Section \\ref{subsubsect:outcome}, the expected output of a missing precondition is set to be \\mycode{true} for a passing test case and is set to be \\mycode{false} for a failing one. The underlying assumption for a passing test case is that the \\mycode{true} value keeps the existing program behavior. However, it is possible that given a statement, both \\mycode{true} and \\mycode{false} values can make a test case pass. \nIn these cases, synthesis may not work for bugs with missing preconditions.\n\nThis is what happens to Bug PL3. Fig. \\ref{fig:bug-pl3} shows a code snippet of Bug PL3. The manually-written patch is a precondition at Line 3; Table \\ref{tab:test-pl3} shows one passing test case and one failing test case. \nBased on the angelic fix localization in Algorithm \\ref{alg:missing-pred-angelic-fix-localization}, the expected precondition values of all passing test cases are set to be \\mycode{true}. However, in the manually-written patch, the precondition value by the passing test case in Table~\\ref{tab:test-pl3} is \\mycode{false}, i.e., \\mycode{lower > str.length()} where \\mycode{lower} is \\mycode{0} and \\mycode{str.length()} is \\mycode{10}. \nThus, it is impossible to generate a patch like the manually-written one, due to a conflict in the input-output specification. Consequently, in the phase of patch synthesis, the SMT solver executes with timeout. \n\nThe example in Bug PL3 implies that for some bugs, the assumption (i.e., a missing precondition is expected to be \\mycode{true} for passing test cases) can be violated. For Bug PL3, we have temporarily removed this assumption and only used the failing test cases to synthesize a patch. The resulting patch is \\mycode{if(lower >= str.length()) lower = str.length()}, which has the same program behavior as the manually-written patch, i.e., \\mycode{lower > str.length()}. In \\nopol, we are conservative and assume that the expected value of a precondition by passing test cases is \\mycode{true} (in Section~\\ref{subsubsect:precondition}). \n\n\\subsubsection{Complex Patches using Method Calls with Parameters}\n\nIn our work, we support the synthesis of conditions that call unary methods (without parameters). However, our approach cannot generate a patch if a method with parameters has to appear in a condition. For example, for Bug CM8, the patch that is written by developers contains a method \\mycode{abs(x)} for computing the absolute value. Our approach cannot provide such kinds of patches because methods with parameters cannot be directly encoded in SMT. Then the lack of information of method calls leads to the timeout of an SMT solver.\n\n\\begin{figure}[!t]\n\\centering\n\\noindent\\begin{minipage}{0.4\\textwidth}\n\\begin{lstlisting}[numbers=left]\nString abbreviate(String str, int lower, int upper){\n  ...\n  //  FIX: if (lower > str.length()) \n  lower = str.length();    \n\n  if (upper == -1 || upper > str.length()) \n    upper = str.length();\n  if (upper < lower) \n    upper = lower;\n  StringBuffer result = new StringBuffer();\n  int index = StringUtils.indexOf(str, \" \", lower);\n  if (index == -1) \n    result.append(str.substring(0, upper));    \n  else ...\n  return result.toString();\n}\n\\end{lstlisting}\n\n\\end{minipage}\n\\caption{Code snippet of Bug PL3. The manually-written patch is shown in the \\mycode{FIX} comment at Line 3.}\n\\label{fig:bug-pl3}\n\\end{figure}\n\n\\begin{table}[!t]\n\\centering\n\\caption{Sample of test cases for Bug PL3}\n\\label{tab:test-pl3}\n\n\\resizebox{0.51\\textwidth}{!}{\n\\setlength\\tabcolsep{0.3 ex}\n\\begin{tabular}{|ccc|cc|c|}\n\n\\hline\n\\multicolumn{3}{|c|}{Input} & \\multicolumn{2}{c|}{\\scriptsize Output, \\mycode{\\scriptsize{abbreviate(str,lower,upper)}} } & \\multirow{2}{*}{ \\tabincell{c}{ Test \\\\ result} } \\\\ \n\\cline{1-5}\n \\mycode{str} & \\mycode{\\scriptsize lower} & \\mycode{\\scriptsize upper} &  Expected&  Observed& \\\\\n\\hline \\hline\n \\mycode{\"0123456789\"} & 0 & -1 & \\mycode{\"0123456789\"} & \\mycode{\"0123456789\"} & pass\\\\\n \\mycode{\"012 3456789\"} & 0 &  5 & \\mycode{\"012\"} & \\mycode{\"012 3456789\"} & fail\\\\\n\\hline\n\\end{tabular}\n}\n\\end{table}\n\n  \n\nA workaround would generate a runtime variable to collect existing side-effect free method calls with all possible parameters. For example, one could introduce a new variable \\mycode{double tempVar = abs(x)} and generate a patch with the introduced variable \\mycode{tempVar}. However, this workaround suffers from the problem of combinatorial explosion.\n\n\\subsubsection{Unavailable Method Values for a Null Object}\n\\label{subsubsect:limit-cl6}\n\nOur repair approach can generate a patch with objected-oriented features. For example, a patch can contain \nstate query methods on Java core library classes, such as \\mycode{String.length()}, \\mycode{File.exists()} and \\mycode{Collection.size()}. We map these methods to their return values during the SMT encoding. However, such methods require that the object is not \\mycode{null}; otherwise, a \\mycode{null} pointer exception in Java is thrown. \n\nLet us consider Bug CL6, whose manually-written patch is \\mycode{cs == null || cs.length() == 0}. For this bug, one passing test case detects whether the object \\mycode{cs} is \\mycode{null}. For this test case, the value of \\mycode{cs.length()} is undefined and not given to SMT. Thus, it is impossible to generate a patch, which contains \\mycode{cs.length()} if \\mycode{cs} is \\mycode{null} by at least one test case. Consequently, the SMT solver times-out because it tries to find a complex patch that satisfies the constraints. \n\nA possible solution is to encode the undefined values in the SMT. Constraints should be added to ensure that the unavailable values are not involved in the patch. This needs important changes in the design of the encoding, which is left to future work.  \n\n\\section{Discussions}\n\\label{sect:discussions}\n\nWe now discuss \\nopol features with respect to four important aspects.\n\n\\subsection{Differences with SemFix}\n\\label{subsect:discussion-semfix}\n\nAs mentioned in Section \\ref{subsect:encoding}, \\nopol uses the same technique in the phase of patch synthesis as SemFix \\cite{nguyen2013semfix}, i.e., component-based program synthesis \\cite{jha2010oracle}. However, there exist a number of important differences between \\nopol and SemFix.\n\nFirst, SemFix does not address missing pre-conditions. As shown in Table \\ref{tab:patch}, adding preconditions enables us to repair more bugs than only updating conditions. We think that it is possible to extend the SemFix implementation to support repairing missing preconditions via adding the encoding strategy as in \\nopol.\n\nSecond, \\nopol does not use symbolic execution to find an angelic value. \nIt is known that symbolic execution may have difficulties with the size and complexity of analyzed programs \\cite{DBLP:journals/tosem/LiCZL14}. According to our work, we have the following observations. The angelic value in angelic fix localization is possible only when the domain is finite. For booleans, the domain of variables is not only finite but also very small. This results in a search space that can be explored dynamically and exhaustively as in \\nopol. Symbolic execution as done in SemFix is capable of also reasoning on integer variables, because the underlying constraint solver is capable of exploring the integer search space. To sum up, our analytical answer is that for boolean domains, angelic fix localization is possible and probably much faster (this is claimed, but not empirically verified). For integer domains, only symbolic execution is appropriate. Meanwhile, \\nopol can also handle the right-hand side of assignments as in SemFix. If we encode the synthesis as in SemFix, the right-hand side of assignments can be directly processed by \\nopol.  \n\nThird, \\nopol supports object-oriented code. We have adapted the code synthesis technique so that the generated patch can contain null checks and method calls. \n\nFinally, the evaluation of \\nopol is significantly larger than that of SemFix. We have run \\nopol on larger programs and real bugs. In SemFix, 4/5 of subject programs have less than 600 LoC and the bugs are artificially seeded. In the evaluation in our paper, the average number of lines of code per subject program is 25K LoC and the bugs are extracted from real programs that happened in practice. \n\n\\subsection{Effectiveness of Fault Localization Techniques}\n\\label{subsect:discussion-fault}\n\nFault localization plays an important role during the repair process. In our approach, a fault localization technique ranks all the suspicious statements and \\nopol attempts to generate patches by starting with analyzing the most suspicious statement first. \nWe use Ochiai \\cite{abreu2007accuracy} as the fault localization technique. \nFor our dataset, we wonder whether there is a difference between Ochiai and other techniques. In this section, we study the accuracy of different fault localization techniques on the bugs of our dataset. \n\nWe employ the absolute wasted effort to measure fault localization techniques. The wasted effort is defined as the ranking of the actual buggy statement. \nGiven a set $S$ of statements, the wasted effort is expressed as follows, \n\n\\vspace{-2ex}\n$$effort=|\\{susp(x)>susp(x^{*})\\}|+1 $$\n\\vspace{-2ex}\n\n\\noindent where $x \\in S$ is any statement, $x^{*}$ is the actual buggy statement, and $|\\cdot|$ calculates the size of a set. A low value indicates that the fault localization technique is effective.    \n\nIn our experiment, we compare six well-studied fault localization techniques: Ochiai \\cite{abreu2007accuracy}, Tarantula \\cite{jones2002visualization}, Jaccard \\cite{abreu2007accuracy}, Naish2 \\cite{naish2011model}, Ochiai2 \\cite{naish2011model}, and Kulczynski2 \\cite{xu2013general}. Table \\ref{tab:localization} presents the comparison on the two types of bugs considered in this paper. \n\nAs shown in Table \\ref{tab:localization}, for bugs with \\buggyconditions, Jaccard obtains the best average wasted effort while Jaccard and Naish2 get the same median value. For bugs with \\preconditions, Tarantula obtains the best average wasted effort while Ochiai, Jaccard, and Naish2 get the same median value. \nThose results assess that, according to our dataset of real bugs, the sensitivity of \\nopol with respect to fault localization is not a crucial point and Ochiai is an acceptable choice.\n\n\\subsection{Potential Parallelization of \\nopol}\n\\label{subsect:parallelization}\n\nOur method \\nopol is originally implemented to not perform parallelization. Based on the design of this method, it is possible to enhance the implementation by parallelizing \\nopol to reduce the execution time. \nIndeed, the core algorithms of \\nopol are highly parallelizable. \n\n\\begin{table}[!t]\n\\centering\n\\caption{Wasted effort comparison among six fault localization techniques.}\n\\label{tab:localization}\n\\setlength\\tabcolsep{0.4 ex}\n\\resizebox{0.5\\textwidth}{!}{\n\\begin{tabular}{|c|cc|cc|}\n\n\\hline\n\n\\multirow{2}{*}{ \\tabincell{c}{Fault localization \\\\ technique}  }  & \\multicolumn{2}{c|}{  Buggy \\ourif condition}   & \\multicolumn{2}{c|}{  Missing precondition} \\\\ \\cline{2-5}\n  & Average & Median  & Average & Median \\\\ \n\\hline\\hline\nOchiai  & 131.88  & 32.00 & 10.33  & \\textbf{1.00} \\\\ \nTarantula & 127.63  & 45.50 & \\textbf{7.00}  & 1.50 \\\\ \nJaccard & \\textbf{121.44}  & \\textbf{25.50} & 10.33  & \\textbf{1.00} \\\\ \nNaish2  & 135.06  & \\textbf{25.50} & 10.33  & \\textbf{1.00} \\\\ \nOchiai2 & 133.06  & 44.50 & 8.83  & 1.50 \\\\ \nKulczynshi2 & 127.44  & 45.50 & 11.50  & 7.50 \\\\\n\\hline\n\\end{tabular}\n}\n\\end{table}\n\nFirst, during angelic fix localization, two feasible ways of parallelization can be performed: over test cases and over potential locations. \nLet us assume that there are 3 failing test cases with respectively 50, 100 and 200 \\buggyconditions executed. Since the search space of \\buggyconditions is $2\\times n_c$ ($n_c$ is the number of executed \\ourif statements by one test case, see Section \\ref{subsubsect:search-space}), we could automatically run in parallel $(50+100+200)\\times 2 = 700$ sessions of angelic fix localization on many different machines.\n\nSecond, our synthesis technique is based on different SMT levels (see Section \\ref{sec:smt-level}). Synthesis at each SMT level corresponds to one independent SMT instance. Hence, the synthesis can also be run in parallel. However, we should mention that parallelizing the synthesis may lead to multiple resulted patches. Synthesis at a low SMT level can generate a simple patch; for the same bug, synthesis at a higher SMT level may not generate a better patch and may waste the running cost. \n\n\\subsection{Insight on Test-Suite Based Repair}\n\\label{subsect:discussion-repair}\n\n\\nopol is a test-suite based repair approach, as other existing work (\\cite{le2012genprog,nguyen2013semfix,Kim2013,qi2014strength}, etc.) in the field of automatic software repair.\nHowever, the foundations of test-suite based repair are little understood. Our experience with \\nopol enables us to contribute to better understanding the strengths and the weaknesses of test-suite based repair.\n\nThere are two grand research questions behind test-suite based repair. The first one is about the quality of test suites \\cite{monperrus2014critical}: do developers write good-enough test suites for automatic repair?\nQi et al. \\cite{qi2015efficient} have shown that the test suites considered in the GenProg benchmark are not good enough, in the sense that they accept trivial repairs such as directly removing the faulty code. The experiments we have presented in this paper shed a different light. For nine bugs considered in this experiment, the test suite leads to a correct patch. For four additional bugs, a slight addition in the test suite allows for generating a correct patch. We consider this as encouraging for the future of the field. However, there is a need for future work on recommendation systems that tell when to add additional test cases for the sake of repair, and what those test cases should specify.\n\nThe second grand research question goes beyond standard test suites such as JUnit ones and asks whether repair operators do not overfit the inputs of input-output specifications \\cite{Smith15fse}. \nFor instance, for Bug CM6, one of the test inputs always equals to $-1$ when the buggy code is executed. As a result, the patch simply uses this value (corresponding to the number of rows) to drive the control flow, which is wrong. On the other hand, there are other cases when the repair operator yields a generic and correct solution upfront. This fact indicates that the same repair operator may overfit or not according to different bugs. It is necessary to conduct future research on the qualification of repair operators according to overfitting. \n\n\\section{Threats to Validity}\n\\label{sect:threats}\n\nWe discuss the threats to the validity of our results along four dimensions. \n\n\\subsection{External Validity} \n\\label{subsect:threat-number-bug}\nIn this work, we evaluate our approach on \\numbug real-world bugs with \\buggyconditions and \\preconditions. One threat to our work is that the number of bugs is not large enough to represent the actual effectiveness of our technique. While the number of bugs in our work is fewer than that in previous work \\cite{le2012genprog,nguyen2013semfix,Kim2013}, the main strength of our evaluation is twofold. On one hand, our work focuses on two specific types of bugs, i.e., \\buggyandpres (as opposed to general types of bugs in \\cite{Kim2013}); on the other hand, our work is evaluated on real-world bugs in large Java programs (as opposed to bugs in small-scale programs in \\cite{nguyen2013semfix} and bugs without object-oriented features in \\cite{le2012genprog}). We note that it is possible to collect more real-world bugs, with the price of more human labor. As mentioned in Section \\ref{subsect:data-set}, reproducing a specific bug is complex and time-consuming. \n\n\\subsection{Single Point of Repair} \nAs all previous works in test-suite based repair, the program under repair must be repaired at one single point. In the current implementation of \\nopol, we do not target programs with multiple faults, or bugs which require patches at multiple locations.\n\n\\subsection{Test Case Modification} \nIn our work, we aim to repair bugs with \\buggyconditions and \\preconditions. Test cases are employed to validate the generated  patch.  In our experiment, several test cases are modified to facilitate repair. As mentioned in Section \\ref{subsect:general-repair}, such test case modification consists of test case addition, transformation, and deletion. The answer to RQ\\ref{rq-testcase} analyzes the root causes of test case modification. All test case modifications are listed in our project website \\cite{nopol2014}.\n\n\\subsection{Dataset Construction} \nWe describe how to construct our dataset in Section \\ref{subsect:data-set}. The manually-written patches of conditional statements are extracted from commits in the version control system. However, it is common that a commit contains more code than the patch in \\buggyandpres. In our work, we manually separate these patch fragments. In particular, the fixing commit of Bug PM2 contains two nested preconditions within a complex code snippet. We manually separate the patch of this bug according to the code context and keep only one precondition. \nHence, there exists a potential bias in the dataset construction.\n\n\\section{Related Work}\n\\label{sect:relatedwork}\n\nWe list related work in four categories: approaches to test-suite based repair, repair besides test-suite based repair, empirical foundations of test-suite based repair, and related techniques in \\nopol. \n\n\\subsection{Test-Suite Based Repair}\n\\label{subsect:related-approach}\n\n\\textbf{GenProg}. Test-suite based repair generates and validates a patch with a given test suite. Le~Goues et al. \\cite{le2012genprog} propose GenProg, an approach to test-suite based repair using genetic programming for C programs. In GenProg, a program is viewed as an Abstract Syntax Tree (AST) while a patch is a newly-generated AST by weighting statements in the program. Based on genetic programming, candidate patches are generated via multiple trials. \nThe role of genetic programming is to obtain new ASTs by copying and replacing nodes in the original AST. \nA systematic study by Le~Goues et al. \\cite{le2012systematic} shows that GenProg can fix 55 out of 105 bugs in C programs. \nThe difference between \\nopol and GenProg are as follows. \n\\nopol targets a specific defect class while GenProg is generic;\n\\nopol uses component-based program synthesis while GenProg only copies existing code from the same code base;\n\\nopol uses a four-phase repair approach (fault localization, angelic fix localization, runtime trace collection, and patch synthesis) while GenProg uses a different two-phase approach (fault localization and trial);\n\\nopol is designed for object-oriented Java programs while GenProg is for C. \n\n\\textbf{AE}. Weimer et al. \\cite{DBLP:conf/kbse/WeimerFF13} report an adaptive repair method based on program equivalence, called AE. This method can fix 54 out of the same 105 bugs as in the work \\cite{le2012systematic} while evaluating fewer test cases than GenProg. \n\n\\textbf{PAR}. Kim et al. \\cite{Kim2013} propose PAR, a repair approach using fix patterns representing common ways of fixing bugs in Java. These fix patterns can avoid nonsensical patches, which are caused by the randomness of some operators in genetic programming. Based on the fix patterns, 119 bugs are examined for patch generation. In this work, the evaluation of patches is contributed by 253 human subjects, including 89 students and 164 developers. \n\n\\textbf{RSRepair}. Qi et al. \\cite{qi2014strength} design RSRepair, a random search based technique for navigating the search space. This work indicates that random search performs more efficiently than genetic programming in GenProg \\cite{le2012genprog}. RSRepair can fix 24 bugs, which are derived from a subset of 55 fixed bugs by GenProg \\cite{le2012systematic}. \nAnother work by Qi et al. \\cite{DBLP:conf/icsm/QiML13} reduces the time cost of patch generation via test case prioritization. \n\n\\textbf{SemFix}. Nguyen et al. \\cite{nguyen2013semfix} propose SemFix, a constraint based repair approach. This approach generates patches for assignments and conditions by semantic analysis via SMT encoding. Program components are synthesized into one patch via translating the solution of the SMT instance. \nOur proposed approach, \\nopol, is motivated by the design of SemFix. The major differences between \\nopol and SemFix were discussed in Section \\ref{subsect:discussion-semfix}. \n\n\\textbf{Mutation-based repair}. Debroy \\& Wong \\cite{debroy2010using} develop a mutation-based repair method, which is inspired by the concept of mutation testing. Their method integrates program mutants with fault localization to explore the search space of patches. \n\n\\textbf{DirectFix}. Mechtaev et al. \\cite{mechtaev2015directfix} propose DirectFix, a repair method for simplifying patch generation. Potential program components in patches are encoded into a Maximum Satisfiability (MaxSAT) problem, i.e. an optimization problem; the solution to the MaxSAT instance is converted into the final concise patch.\n\n\\textbf{SearchRepair}. Ke et al. \\cite{Ke15ase} develop SearchRepair, a repair method with semantic code search, which encodes human-written code fragments as SMT constraints on input-output behavior. This method reveals 20\\% newly repaired defects, comparing with GenProg, AE, or RSRepair. \n\n\\textbf{SPR}. \nAfter the original publication presenting \\nopol \\cite{demarco2014automatic}, Long \\& Rinard \\cite{DBLP:conf/sigsoft/LongR15} have proposed a repair technique called SPR using condition synthesis. SPR addresses repairing conditional bugs, as well as other types of bugs, like missing non-\\ourif statements. The differences are as follows.\nFirst, a major difference between \\nopol and SPR is that \\nopol synthesizes a condition via component-based program synthesis while SPR is based on multiple trials of pre-defined program transformation schemas. For instance, in SPR, a transformation schema for conditional bugs is called \\textit{condition refinement}, which updates an existing condition in an \\ourif via tightening or loosening the condition. To repair a bug, SPR tries a potential patch with the transformation schemas one by one and validates the patch with the test suite; the technique of \\nopol is entirely different, based on runtime data collection during test execution. \nSecond, another difference is that SPR is for repairing C programs. Patches by SPR only contain primitive values while patches by \\nopol contain both primitive values and object-oriented expressions (e.g., fields and unary method calls). \nThird, in SPR, the technique of collecting angelic values is based on \\nopol's, yet extends it.\nIt finds sequences of values rather than one simplified trace during collecting angelic values in \\nopol (Section~\\ref{sect:angelic-fix-localization}). As mentioned in Section \\ref{subsubsect:search-space}, the simplified trace in \\nopol reduces the search space of patch synthesis, but may result in failed repair attempts for specific bugs, where a condition is executed more than once by a test case. Examples of these bugs can be found in the SPR evaluation \\cite{DBLP:conf/sigsoft/LongR15}. The simplification in \\nopol can be viewed as a trade-off between repairability and time cost. \n\n\\textbf{Prophet}. Also by Long \\& Rinard, Prophet is an extension of SPR that uses a probability model for prioritizing candidate patches. Based on historical patches, Prophet learns model parameters via maximum likelihood estimation. Experiments show that this method can generate correct patches for 15 out of 69 real-world defects of the GenProg benchmark. We have also noticed that in \\nopol, it is possible to synthesize more than one patch with our SMT-based synthesis implementation. Hence, the probability model in Prophet can be leveraged to direct the synthesis of more correct patches by \\nopol.  \n\n\\subsection{Other Kinds of Repair}\n\\label{subsect:other-kinf-repair}\n\nBesides test-suite based repair, other approaches are designed for fixing software bugs and improving software quality. \nDallmeier et al. \\cite{dallmeier2009generating} propose Pachika, a fix generation approach via object behavior anomaly detection. This approach identifies the difference between program behaviors by the execution of passing and failing test cases; then fixes are generated by inserting or deleting method calls. \nCarzaniga et al. \\cite{carzaniga2010automatic} develop an automatic technique to avoid failures by a faulty web application. This technique is referred as an automatic workaround, which aims to find and execute a correct program variant. \nAutoFix by Pei et al. \\cite{pei2014automated}, employs a contract-based strategy to generate fixes. This approach requires simple specifications in contracts, e.g., pre-conditions and post-conditions of a function, to enhance the debugging and fixing process. Experiments on Eiffel programs show that this approach can fix 42\\% of over 200 faults.  \n\n\\subsection{Empirical Foundations of Repair}\n\\label{subsect:related-foundation}\n\nApplying automatic repair to real-world programs is limited by complex program structures and semantics. We list existing work on the investigation of empirical foundations of test-suite based repair. \n\nMartinez \\& Monperrus \\cite{Martinez2013} mine historical repair actions to reason about future actions with a probabilistic model. Based on a fine granularity of ASTs, this work analyzes over 62 thousands versioning transactions in 14 repositories of open-source Java projects to collect probabilistic distributions of repair actions. Such distributions can be used as prior knowledge to guide program repairing. \n\nFry et al. \\cite{DBLP:conf/issta/FryLW12} design a human study of patch maintainability with 150 participants and 32 real-world defects. This work indicates that machine-generated patches are slightly less maintainable than human-written ones; hence, patches by automatic repair could be used as the patches written by humans. Another case study is conducted by Tao et al. \\cite{DBLP:conf/sigsoft/TaoKKX14}. They investigate the possibility of leveraging patches by automatic repair to assist the process of debugging by humans.  \n\nBarr et al. \\cite{DBLP:conf/sigsoft/BarrBDHS14} address the ``plastic surgery hypothesis'' of genetic-programming based repair, such as GenProg. Their work presents evidences of patches based on reusable code, which make patch reconstitution from existing code possible. Martinez et al. \\cite{DBLP:conf/icse/MartinezWM14} conduct empirical investigation to the redundancy assumption of automatic repair; this work indicates that code extracted from buggy programs could form a patch that passes the test suite.  \n\nMonperrus \\cite{monperrus2014critical} details the problem statement and the evaluation of automatic software repair. This work systematically describes the pitfalls in software repair research and the importance of explicit defect classes; meanwhile, this paper identifies the evaluation criteria in the field: understandability, correctness, and completeness. \nZhong \\& Su \\cite{zhong2015an} examine over 9,000 real-world patches and summarize 15 findings in two key ingredients of automatic repair: fault localization and faulty code fix. This work provides empirical foundations for localization and patch generation of buggy statements. \n\nQi et al. \\cite{qi2015efficient} propose Kali, an efficient repair approach based on simple actions, such as statement removal. Their work presents the repair results via simple methods; meanwhile, their work checks previous empirical results by GenProg \\cite{le2012genprog}, AE \\cite{DBLP:conf/kbse/WeimerFF13}, and RSRepair \\cite{DBLP:conf/icsm/QiML13}. Empirical studies show that only two bugs by GenProg, three bugs by AE, and two bugs by RSRepair are correctly patched. All the reported patches for the other bugs are incorrect due to improper experimental configurations or semantic issues; an incorrect patch either fails to produce expected outputs for the inputs in the test suite, or fails to implement functionality that is expected by developers. \nAs the latest result in test-suite based repair, the work by Qi et al. \\cite{qi2015efficient} shows that repairing real-world bugs is complex and difficult. Hence, it is worth investigating the empirical results on fixing real bugs. \n\nRecent work by Smith et al. \\cite{Smith15fse} investigates the overfitting patches on test cases in automatic repair. They report a controlled experiment on a set of programs written by novice developers with bugs and patches; two typical repair methods, GenProg \\cite{le2012genprog} and RSRepair \\cite{qi2014strength}, are evaluated to explore the factors that affect the output quality of automatic repair. \n\nRecent work by Le Goues et al. \\cite{LeGoues15tse} presents two datasets of bugs in C programs to support comparative evaluation of automatic repair algorithms. The detailed description of these datasets is introduced and a quantified empirical study is conducted on the datasets. \nDefects4J by Just et al. \\cite{JustJE2014} is a bug database that consists of 357 real-world bugs from five widely-used open-source Java projects. It has recently been shown \\cite{durieux2015} that \\nopol is capable of fixing 35 bugs of this benchmark.\n\n\\subsection{Related Techniques: Program Synthesis and Fault Localization}\n\\label{subsect:related-localization}\n\nOur approach, \\nopol, relies on two important techniques, program synthesis and fault localization. \n\nProgram synthesis aims to form a new program by synthesizing existing program components. Jha et al. \\cite{jha2010oracle} mine program oracles based on examples and employ SMT solvers to synthesize constraints. In this work, manual or formal specifications are replaced by input-output oracles. They evaluate this work on 25 benchmark examples in program deobfuscation. Their follow-up work \\cite{gulwani2011synthesis} addresses the same problem by encoding the synthesis constraint with a first-order logic formula. In general, any advance in program synthesis can benefit program repair by enabling either more complex or bigger expressions to be synthesized. \n\nIn our work, fault localization is used as a step of ranking suspicious statements to find out locations of bugs. A general framework of fault localization is to collect program spectra (a matrix of testing results based on a given test suite) and to sort statements in the spectra with specific metrics (e.g., Tarantula \\cite{jones2002visualization} and Ochiai \\cite{abreu2007accuracy}). Among existing metrics in fault localization, Ochiai \\cite{abreu2007accuracy} has been evaluated as one of the most effective ones. In Ochiai, statements are ranked according to their suspiciousness scores, which are values of the Ochiai index between the number of failed test cases and the number of covered test cases. Fault localization techniques are further improved recently, for example, the diagnosis model by Naish et al. \\cite{naish2011model}, the localization prioritization by Yoo et al.~\\cite{yoo2013fault}, and the test purification by Xuan \\& Monperrus~\\cite{xuan2014test}.\n\n\\section{Conclusion}\n\\label{sect:conclusion}\n\nIn this paper, we have proposed \\nopol, a test-suite based repair approach using SMT. \\nopol targets two kinds of bugs: buggy \\ourif conditions and missing preconditions. Given a buggy program and its test suite, \\nopol employs angelic fix localization to identify potential locations of patches and expected values of \\ourif conditions. For each identified location, \\nopol collects test execution traces of the program. Those traces are then encoded as an SMT problem and the solution to this SMT is converted into a patch for the buggy program. We conduct an empirical evaluation on \\numbug real-world programs with \\buggyconditions and \\preconditions. We have presented four case studies to show the benefits of generating patches with \\nopol as well as the limitations. \n\n\\nopol is publicly-available to support further replication and research on automatic software repair: \\url{http://github.com/SpoonLabs/nopol/}.\n\nIn future work, we plan to evaluate our approach on more real-world bugs. Our future work also includes addressing the current limitations, e.g., designing better strategy for angelic fix localization, collecting more method calls, and improving the SMT encoding. \n\n\\section*{Acknowledgment}\n\nThe authors would like to thank David Cok for giving us full access to jSMTLIB. \nThis work is partly supported by the INRIA Internship program, the INRIA postdoctoral research fellowship, the CNRS delegation program, the National Natural Science Foundation of China (under grant 61502345), and the Young Talent Development Program of the China Computer Federation. \n\n\\balance\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Neural Program Repair with Execution-based Backpropagation}\n\n\\begin{document}\n\n\\title{Neural Program Repair with Execution-based Backpropagation}\n\n\\author{He Ye}\n\\email{heye@kth.se}\n\\affiliation{\n  \\institution{KTH Royal Institute of Technology}\n  \\country{Sweden}\n}\n\n\\author{Matias Martinez}\n\\email{matias.martinez@uphf.fr}\n\\affiliation{%\n  \\institution{Université Polytechnique Hauts-de-France, France \\& KTH Royal Institute of Technology, Sweden }\n \\country{}\n}\n\n\\author{Martin Monperrus}\n\\email{martin.monperrus@4open.science}\n\\affiliation{%\n  \\institution{KTH Royal Institute of Technology}\n  \\country{Sweden}\n}\n\n\\begin{abstract}\n\nNeural machine translation (NMT) architectures  have achieved promising results  for  automatic program repair.\nYet, they  have the limitation of generating\nlow-quality  patches (e.g., not compilable patches). \nThis is because the existing works only optimize a purely syntactic loss function based on characters and tokens without incorporating program-specific information during neural network weight optimization. \nIn this paper, we propose a novel program repair model called RewardRepair. \nThe core novelty of RewardRepair is to improve NMT-based program repair with a loss function based on program compilation and test execution information, rewarding the network to produce patches that compile and that do not overfit.\nWe conduct several experiments to evaluate RewardRepair showing that it is feasible and effective to use compilation and test execution results to optimize the underlying neural repair model. RewardRepair correctly repairs 207 bugs over four benchmarks. \nwe report on repair success for 121 bugs that are fixed for the first time in the literature.\nAlso, RewardRepair produces up to 45.3\\% of compilable patches, an improvement over the 39\\%  by the state-of-the-art.\n\\end{abstract}\n\n\\maketitle\n\n\\section{Introduction}\n\nAutomatic program repair (APR) aims to reduce manual work related to bug localization and bug fixing~\\cite{Monperrus2015,TSE-repair-survey}. With recent advances in deep learning, research has been proposed to use neural networks for program repair, a subarea of a trend on using machine learning on code \\cite{machineonlearningoncodesurvey}. This line of work, put here under the umbrella term ``neural program repair'', mostly uses neural machine translation (NMT) approaches \\cite{CURE-icse21,SEQUENCER,CoCoNuT,Tufano-ICSE19,codit-tse20,Tufano-tse19,deepfix}. \n\nProgram repair systems based on neural machine translation treat the repair task as a translation from buggy code to correct code, both represented as a sequence of tokens~\\cite{SEQUENCER}. \nGiven sufficient training data, NMT-based repair has achieved promising\nperformance \\cite{CURE-icse21,SEQUENCER,CoCoNuT,Tufano-ICSE19,codit-tse20,Tufano-tse19,deepfix}. \nAll the prior works on program repair based on neural machine translation use the static loss function: cross-entropy loss based on token similarity. \n\nDespite early successes, NMT-based program repair approaches suffer from two major drawbacks.  First,\nthey often generate patches that do not compile \\cite{SEQUENCER}. The reason is that a lower cross-entropy value does not necessarily lead to a compilable patch.\nSecond, the loss function under optimization forces the neural network to learn to produce absolutely identical patches, thus missing the opportunity to explore semantically equivalent.\nHowever, Rabin et al. \\cite{rabin2021generalizability} made the case that generalizability in machine learning on code relates to generalization over equivalent programs.\nBoth problems (uncompilability and being stuck with syntactic identity) share the same root: there is a discrepancy between the core training objective,  learning to generate compilable and correct patches, and the loss function that is being optimized.\nThe cross-entropy loss used in related work requires a strict pairwise matching between the generated patch and the human-written ground truth patch, and not more~\\cite{NMT-bridging}. Nothing in cross-entropy loss encourages the neural network to produce compilable or syntactically different but semantically equivalent patches. \nThis is the problem we address in this paper.\n\nWe introduce a novel neural repair model, called \\approach based on a mixed learning objective. The key insight in the design of \\approach is the combination of a syntactic training objective at the token-level and a semantic training objective based on program execution.  \n\\approach defines a discriminator to discriminate good patches from low-quality ones during the training of the neural network. The discriminator is based on executing the compiler and running the test cases on the generated patches, providing high qualified execution feedback on their quality. \nThis feedback is transformed as a quantified reward signal that modulates the cross-entropy loss.\nThen, the neural network's weights are updated based on this novel discriminator. In other words, in \\approach, backpropagation embeds essential compilation and execution information. \n\nWe conduct large experiments to evaluate \\approach based on four well-accepted datasets from the literature, including Defects4J version 1.2~\\cite{defects4j}, Defects4J  version 2.0~\\cite{defects4j}, Bugs.jar~\\cite{Bugsjar-MSR18} and QuixBugs~\\cite{lin2017quixbugs}.\nFirst, we show that RewardRepair produces more correct patches than the recent related work. In total, RewardRepair repairs 207 on the four benchmarks.\nRewardRepair achieves an improvement in two benchmarks Defects4J(v2.0) and Bugs.jar, and achieves the top-2 performance in the other two benchmarks Defects4J(v1.2) and QuixBugs. \nSecond, we demonstrate that RewardRepair outperforms the state-of-the-art on addressing the compilability problem in neural program repair, CURE \\cite{CURE-icse21}, by producing a higher ratio of compilable patches in all considered beam size configuration (45.3\\% versus 39\\% in top-30, and 37.5\\% versus 28\\% in top-100 candidate patches).\n\nTo sum up, our contributions are:\n\\begin{itemize}\n    \\item We devise, \\approach, a neural program repair approach with execution-based backpropagation. \\approach defines a novel training objective that employs compilation and test execution information to optimize the neural network.\n    \\item We perform an original series of experiments to show that  \\approach's training objective outperforms the cross-entropy loss used in related work. Our experimental results demonstrate that embedding execution information in backpropagation  improves the quality of generated patches (more compilable patches and correct patches).\n    \\item We provide evidence that \\approach can correctly fix 45 bugs for Defects4J(v1.2), 45 bugs for Defects4J(v2.0), 97 bugs for Bugs.jar and 20 bugs for QuixBugs. We report that ReparRepair can correctly fix 121 bugs that were never repaired in previous literature, including 5 unique Defects4J(v1.2) bugs.\n    \\item We make all our data and code publicly available for future research \\cite{experiment}.\n\\end{itemize}\n\n\\section{Background}\n\\label{sec:background_neural}\n\n\\subsection{Neural Program Repair}\n\nNeural Machine Translation (NMT) systems have recently achieved  state-of-the-art performance on program repair tasks, forming a field called ``neural program repair'' \\cite{SEQUENCER,CoCoNuT,codit-tse20,Tufano-ICSE19,deepfix,Tufano-tse19, Recoder,CURE-icse21}.\nDespite the difference in their inputs and neural models, those works are  similar in the sense that they are all based on\na typical NMT model formulated as an encoder-decoder-attention architecture optimized with cross-entropy loss function \\cite{nmt,attention-all-you-need,Vaswani2021Transformer,NMT-bridging,overcorrection}. \nAll the prior works on program repair are based on the NMT architecture with a cross-entropy loss function to update the neural network weights \\cite{SEQUENCER,CoCoNuT,codit-tse20,Tufano-ICSE19,deepfix,Tufano-tse19, Recoder,CURE-icse21}. \nThe cross-entropy loss (\\textit{a.k.a.} log loss) is a measure from information theory, building upon entropy and calculating the difference between two probability distributions \\cite{nmt,aligncrossentropy}. During the training of a neural repair model, the cross-entropy loss calculates the difference between the generated tokens and the human-written patch  tokens in a strict pairwise matching manner \\cite{nmt,NMT-bridging,overcorrection},\nand is used to update the neural network weights by backpropagation. \nIn program repair patches, a low cross-entropy value  means that the generated patch is syntactically close to the ground truth patch at the token-level. \n\n\\subsection{Limitations of Current Neural Repair}\n\\label{sec:background:problems}\n\nThe token-based cross-entropy loss optimization is effective at guiding the model to generate patches syntactically identical or close to the human-written patches given as input during training.\nHowever, it suffers from two major limitations.\nFirstly, a major goal in patch generation is to generate well-formed patches that compile. \nUnfortunately, the cross-entropy loss does not favor patches that compile over non-compilable patches.\nEven worse, if a generated patch has a significantly lower loss per the token-based cross-entropy, although being non-compilable, it would be favored by the model at training time.\nSecondly, the cross-entropy loss function fails to learn from semantically equivalent patches: a syntactically different but semantically equivalent patch could potentially have a high cross-entropy loss value. \nThis means that the cross-entropy loss \ndiscourages the network to explore equivalent solutions. \nIn the field of neural machine translation (NMT), this problem is known as the overcorrection problem of cross-entropy loss~\\cite{NMT-bridging,overcorrection}: cross-entropy based models tend to learn strictly identical translations and to overcorrect synonymous tokens which would be acceptable.\nOverall, the problem we address in this paper is that the cross-entropy loss used in previous research cannot learn programming knowledge beyond tokens. \n\n\\subsection{Motivating Example}\n\\label{sec:motivating-example}\n\n\\begin{listing}[t!]\n\\noindent\\begin{minipage}[b]{0.5\\textwidth}\n    \\begin{lstlisting} [firstnumber=419] \n<@\\colorbox{red!30}{- return FastMath.pow(2 * FastMath.PI, -dim / 2 ) *\\quad}@>           \n<@\\colorbox{green!30!}{+ return FastMath.pow(2 * FastMath.PI, -0.5 * dim ) *}@>\n    \\end{lstlisting}\n \n     \\subcaption{The human-written patch}\n   \\label{motivate-human-patch}  \n    \\end{minipage}%\n    \\hfill\n    \\begin{minipage}[b]{0.49\\textwidth}\n    \\begin{lstlisting}[firstnumber=419] \n<@\\colorbox{green!30!}{+ return FastMath.pow(2 * FastMath.PI, -d * dim ) * }@> //loss value 0.1157 \n    \\end{lstlisting}\n \n    \\subcaption{A generated non-compilable patch receives a smaller loss score}\n \\label{motivate-noncompile-patch}   \n\\end{minipage}%\n\\hfill\n    \\begin{minipage}[b]{0.49\\textwidth}\n    \\begin{lstlisting}[firstnumber=419] \n<@\\colorbox{green!30!}{+ return FastMath.pow(2 * FastMath.PI, -dim / 2d ) *}@> //loss value 0.4224 \n    \\end{lstlisting} \n    \\subcaption{A generated semantically-equivalent patch receives a bigger loss score}\n    \\label{motivate-correct-patch} \n\\end{minipage}%\n\\caption{Motivating example: NMT based repair models based on cross-entropy loss may favor non-compilable patches.}\n\\label{lst:motivatingExample}\n\\end{listing}\n\n\\autoref{lst:motivatingExample} is a motivating example to show the drawbacks of a neural repair model based on cross-entropy loss optimization. \n\\autoref{lst:motivatingExample}a presents the buggy code and the human-written patch for bug \\textit{Math-11} from Defects4J(v1.2).\n\\autoref{lst:motivatingExample}b  gives one  generated non-compilable patch because of the undefined variable $d$. The network generates this patch\nwith maximum likelihood estimation because its cross-entropy loss value is low (\\textit{0.1157}).\n\\autoref{lst:motivatingExample}c is a semantically equivalent patch compared to the human-written patch and its computed loss is \\textit{0.4224}, which is higher than the non-compilable patch.\n\nWith cross-entropy loss, the NMT-based repair model penalizes the semantically equivalent patch and favors the non-compilable patch.  \nThis is because there is only one token difference for the generated non-compilable patches, where the wrong token \\texttt{d} is not the expected token \\texttt{0.5} from the ground truth patch.\nHowever, the token $d$ is an undefined variable in the buggy program. \nOn the contrary, there exist three token differences in the semantically equivalent patch: ($dim \\rightarrow \\texttt{0.5}$), ($/\\rightarrow *$), ($2d \\rightarrow  dim$). \nConsequently, the NMT-based repair model considers the generated \nnon-compilable patch is closer to the human-written patch, and thus should be favored during backpropagation.\nHowever, in the context of program repair, the semantically equivalent patch should have a loss close to zero, because it is a valid solution to the bug.\nThe generated non-compilable patch  has a  lower loss, but still cannot satisfy the compiler, which is inconsistent with our goal.\nThis example shows the fundamental limitation of optimizing neural networks with the traditional token-based cross-entropy loss for the program repair task.\n\n\\begin{figure*}\n\\includegraphics[width=0.98\\textwidth,height=0.36\\textheight]{figures/pre-adv-training-gan.pdf} \n \\caption{An Overview of \\approach. }\n\\label{fig:RewardRepair-overview}\n\\end{figure*}\n\n\\section{RewardRepair}\n\\label{sec:rewardrepair}\n\n\\subsection{Overview}\n\\label{sec:overview}\n\nIn this paper, we propose a novel neural repair model called\nRewardRepair.\nThe core idea of RewardRepair is to improve the learning process of neural program repair and in particular to address the limitations of using the cross-entropy loss function at the token-level. \n\\autoref{fig:RewardRepair-overview} gives an overview of \\approach.\nThe top, middle and bottom parts represent three stages of RewardRepair: syntactic training, semantic training and inference. \n\\approach is trained with two datasets respectively, a syntactic training dataset with pairs of buggy and patch code, and a semantic training dataset.\nThey are fundamentally different. The syntactic training dataset only consists of textual patches as in the related work \\cite{CURE-icse21,CoCoNuT,SEQUENCER}.\nHowever, the requirements for the semantic training dataset are full execution: each sample in the dataset comes with a compiler configuration and test cases. Achieving full execution enables us to derive execution-based information to be used for optimizing the neural network weights with programming knowledge during backpropagation.\n\n\\textit{Syntactic training} is our first training phase.\nWe train \\approach to optimize the cross-entropy loss based on a large bug fix corpus per the related work \\cite{CURE-icse21,CoCoNuT,SEQUENCER,deepfix,DLFix,Tufano-ICSE19,codit-tse20}. \nSyntactic training is meant to provide a good initial model  for semantic training. \n\\textit{Semantic training} is our second phase after syntactic training.\nSemantic training is based on a discriminative model.\nThe discriminative model of \\approach analyzes the candidate patches with four discriminators (\\textit{difference discriminator, compilability discriminator, plausibility discriminator and regression discriminator}), and  modulates the cross-entropy loss before the start of backpropagation on training.\nThe \\textit{inference} is the final phase. Once \\approach has been trained,  it can generate patches for new and unseen bugs based on the trained patch generator.\n\n\\subsection{Code Representation}\n\\label{sec:coderepresentation}\nAs code representation, we follow Lutellier et al. \\cite{CoCoNuT} to represent the buggy code and context code as two separate token sequences.\nIn \\approach, the context code is considered as 10 lines of code surrounding the buggy code.\nIn addition, the context code is enriched with a summary of the buggy class as proposed by Chen et al. \\cite{SEQUENCER} as follows: we keep all the instance variables and their initializers, along with the signature of the constructors and methods. \nWe follow the existing work \\cite{CURE-icse21} to use subword tokenization with SentencePiece \\cite{Kudo2018SentencePieceAS}, as demonstrated useful by Karampatsis et al. \\cite{karampatsis2020big}.\n\n\\subsection{Patch Generator}\n\\label{sec:patch_generator}\n\nIn \\approach, the patch generator is trained in a supervised manner based on an encoder-decoder architecture \\cite{T5,attention-all-you-need}.\nSyntactic training takes as input buggy code tokens $B = (b_0, b_1\\ldots b_n )$, context code tokens $C = (c_0, c_1\\ldots c_m )$, and tokens from the ground truth fix patch $F = (f_0, f_1\\ldots f_t )$.\n\\approach transforms the $B$ and $C$ into a predicted patch  $F'= (f'_0, f'_1\\ldots f'_k )$. \nNote that size of the buggy code, context code, ground truth patch and predicted patch, i.e., $n$, $m$, $t$ and $k$, can be different. \nIn \\approach, patch generation is shared by both syntactic training and semantic training.\n\n\\subsection{Syntactic Training of \\approach}\n\\label{sec3:syntactic_training}\n\n\\approach initially trains the patch generator with cross-entropy loss function per the state-of-the-art of NMT for program repair \\cite{CoCoNuT,SEQUENCER,deepfix,DLFix,Tufano-ICSE19,codit-tse20,CURE-icse21}. \nFor each training point, the optimization target is to minimize the loss between ground truth fix patch $F$ and the predicted patch $F'$. \nAs shown in the previous study \\cite{CURE-icse21,CoCoNuT},  syntactic training is trained with a large corpus of buggy code and fixed code pairs.\nSyntactic training could be trained for multiple epochs to achieve convergence and obtain the best combination of weights.\nBy the end of syntactic training, the patch generator's weights between the connections of the networks are optimized.\n\n\\subsection{Semantic Training of \\approach}\n\\label{sec-discriminator}\n\\label{sec:semantic_training}\n\nThe goal of semantic training is to let the patch generator be aware of program-specific knowledge (compilation and execution) beyond the syntactic loss computation at the token-level. For that, we propose a mixed learning objective \\cite{overcorrection}, where ``mixed'' means that the core learning objective is combined with two or more sub-learning objectives. \nOur  mixed learning objective  combines the core cross-entropy objective with compilation and execution information.\nOnce the \\approach has been sufficiently trained with syntactic training, we start the semantic training process alternately with syntactic training.\n\nFor semantic  training,  \\approach employs a discriminative model to assess the quality of generated patches based on compilation and test oracles. As a result, the discriminative model outputs a reward value that quantifies the patch quality, which is used to adjust the weights of the patch generator during backpropagation.  \nA higher reward means  better quality for the generated patch, e.g., the patch is compilable and passes all test cases.   On the contrary, a lower reward means that the quality of the generated patch may be unsatisfying, e.g., the patch is non-compilable.\n\nPrecisely, the patch reward value modulates the token-level loss value before the start of the backward pass \\cite{micikevicius2018mixed,lossscale,AnchorLoss}, so that the updated loss can then be properly represented with program-specific knowledge.  \nIn such a way, the discriminator guides the neural network to generate high-quality patches.\nTo our knowledge, we are the first to introduce  semantic training based on a patch discriminative model for neural program repair.\n\n\\subsubsection{Discriminative model} \n\nIn machine learning, a discriminator is a model for identifying ``good'' and ``bad'' predictions~\\cite{GanGoodFellow}.\nThe discriminator of \\approach is the key component of the semantic training phase.\nIts goal is to measure the quality of the generated patches, which is then quantified as a reward signal. \nIn \\approach, the discriminative model is composed of four discriminators, each of them specialized in one aspect of patch quality.\nThese four discriminators are serially executed. \nEach discriminator does a binary classification: whether a patch fulfills the discriminator's criterion or not. When a discriminator is affirmative, it means the patch passes the quality criterion, and it is passed to the next discriminator. \nIn the rest of this section, we present the four discriminators of \\approach.\n\n\\paragraph{Difference discriminator} \n\\label{sec:difference_discriminator}\nThe difference discriminator validates whether the generated patches are different from the given buggy code.  \nIt is required because we found that neural repair models regularly generate a patch that is identical to the given buggy code, i.e., the output of the model is identical to the input of the model (called a ``no-change'' patch in this paper). \nThis happens when the neural nets discover that the buggy code achieves the maximum likelihood estimation. \nThis is explained by previous research \\cite{TianASE20}, which has shown that the many buggy codes are similar to correct code, with only minor transformations and few changed tokens.\nConsequently, the generator tends to copy the buggy code because it is the maximum likelihood prediction, per the data seen at training time.\nWe design the difference discriminator to syntactically compare the generated patch code with the input buggy code, with a token sequence comparison.\nIf the buggy code and generated patched code are the same, the  difference discriminator assigns the generated patch a penalty, called in this paper the \\textit{no-change penalty} ($R_{no-change}$), i.e., a negative reward.\nThis penalty signal modulates the \\approach loss to avoid the generation of no-change patches.\n\n\\paragraph{Compilability discriminator} \n\\label{sec:compiler_discriminator}\n\nThe compilability discriminator validates whether the generated patches are compilable. \nAs shown in previous research \\cite{CURE-icse21}, neural repair models suffer from outputting  non-compilable patches. \nFor example, the golden sequenceR model produces 79.7\\% of patches that are not compilable in top-50 candidate patches. \nTo force the generator towards producing compilable patches, we introduce a compilability discriminator in the semantic training phase.\nThe compilability discriminator employs the compiler to compile the generated patched program.\nIf the patched program is compilable, \\approach assigns a \\textit{compilable reward} ($R_{compilable}$) and passes the patch to the next discriminator for further assessment. Otherwise, a \\textit{non-compilable penalty} ($R_{non-compilable}$), i.e., a negative reward, is returned to the \\approach model.\n\n\\paragraph{Plausibility  discriminator} \n\\label{sec:plausible_discriminator}\nThe plausibility discriminator aims at encouraging the generator to produce patches that pass the human-written test cases provided by developers.\nRecall that, per the seminal work on program repair \\cite{LeGoues2012GenProg}, test cases can be used as an executable program specification.\nIn other words, if a patch passes the human-written tests, it is considered as a plausible patch. \nDuring semantic training, \\approach is trained on buggy projects with executable human-written tests.\nEach candidate patch is executed against the human-written tests.\nIf a patch makes all human-written tests pass, a \\textit{plausible reward} ($R_{plausible}$) is assigned. \nBy doing so, the plausibility discriminator leverages the human-written tests to drive the network to produce plausible  patches that pass all human-written tests.\n\n\\paragraph{Regression discriminator}\n\\label{sec:behavior_discriminator}\nThe last discriminator used in  \\approach's discriminative model is the regression discriminator.\nThe goal of this discriminator is to minimize the behavioral changes introduced by the patch.\nThis discriminator complements the plausible patch discriminator by specifying behavior outside the human-written tests, in order to avoid patch overfitting \\cite{zhongxing-EMSE18,Le:overfitting,le:reliability-patch-assess,CURE-worse-15}.\nThe \\approach regression discriminator employs automatically generated tests, per the RGT technique of Ye et al. \\cite{drr}.  The effectiveness of this technique to identify correct patches from plausible patches has been shown in recent work \\cite{ASE20Wang,ODS,quixbugs-jss}.\nThe idea of the RGT technique is to automatically generate test cases based on the ground truth patched program to expose program execution behavior differences between a generated patch and a ground truth patch \\cite{le:reliability-patch-assess,ASE20Wang,drr}. \nIf a candidate patch makes all RGT tests pass, i.e., it does not contradict the ground truth program behavior, it is considered as likely-correct.\n\nDuring semantic training, all patches are executed against the RGT tests.\nIf a candidate patch makes all automatically generated tests pass, meaning the same program execution behavior with ground truth program,  then  a \\textit{likely-correct reward} signal ($R_{l-correct}$) is assigned to this patch. \nThis discriminator's reward is used to encourage the \\approach to avoid regressions.\n\\emph{This means we encourage \\approach to generate non-overfitting patches beyond the existing test cases.}\n\n\\subsubsection{Defining reward values from discriminators}\n\\label{sec:reward-value}\n\nAs shown previously, \\approach defines five reward signals for patch quality assessment.\nThe discriminators are executed serially, that is, if a patch \\emph{does not satisfy} one discriminator, then the reward $R$ obtained up to that moment is returned immediately and other discriminators are not executed. \nConsequently, the $R$ is the maximum  of the five reward values as follows:\n\n\\begin{equation}\n\\footnotesize\nR = \nmax\n\\begin{cases}\nR_{no-change} = s_0\\\\\nR_{non-compilable} = s_1 \\\\\nR_{compilable} = s_2   \\\\\nR_{plausible} = s_3   \\\\\nR_{l-correct} = s_4  \\\\\n\\end{cases}\n\\label{equ-reward-values}\n\\end{equation}\n\nwhere  $s_{i}$  ($i\\in \\{0, 1, 2, 3, 4\\}$) are five scaling parameters that control the range of the reward values.\nThe scaling parameters of $s_{i}$ define a configuration space that is controlled by end-users of \\approach. \nAdditionally, those reward values must fulfill the following constraint: \n\\begin{equation}\n\\footnotesize\nR_{no-change} <  R_{non-compilable} <  R_{compilable} < R_{plausible} <  R_{l-correct}\n\\end{equation}\nwhere the higher reward value represents the better quality of the generated patch.\n\nGiven the cross-entropy $\\mathcal{L}$,\nthe loss function of \\approach  $\\mathcal{L}_{RewardRepair}$ dynamically reweighs $\\mathcal{L}$ with respect to the discriminator's reward signal $R$, which encodes the quality of the generated patch.\nWe follow \\cite{seqGan-aaai17,AnchorLoss} to formulate \\approach loss function as a scaling of the cross-entropy, as follows:\n\n\\begin{equation}\n\\footnotesize\n\\mathcal{L}_{RewardRepair}  = (1-R)*\\mathcal{L} \\quad  (where \\quad R<1)\n\\label{loss-equ}\n\\end{equation}\n\nThe reward modulator $(1-R)$  constrains the domain of $R \\in (-\\infty, 1)$, as it is meaningless for the  objective function to minimize a negative loss. \nIn this formulation, the\nsyntactic cross-entropy at the token-level is combined with the semantic reward at patch-level, embedding compilation and execution knowledge deep into the neural model.\nIt mitigates the limitations of only considering cross-entropy loss in program repair tasks and solves the problem discussed in  Section \\ref{sec:motivating-example}. \n\nFor low-quality  patches, \\approach assigns a penalty, i.e., negative reward value, to increase the original cross-entropy loss  $\\mathcal{L}$. Thus the domain for $R_{no-change}, R_{non-compilable} \\in (-\\infty, 0)$.\nOn the contrary, for the high-quality patches, \\approach scales down the original $\\mathcal{L}$ by assigning positive reward values for $R_{compilable}$, $ R_{plausible}$, and $R_{l-correct}$ to encourage the model. The domain for these three reward values is \n$[0, 1)$.\nBased on \\autoref{loss-equ}, the higher the reward, the smaller computed loss is back-propagated into the neural model. \nThe extreme case is the highest reward value approximate to \\textit{1}, where the \\approach loss goes close to \\textit{0}. This indicates that the generated patch is likely correct and the network should not be changed. \n\nCombining the domain constraints discussed above, \nthe range of scaling parameters $s_{i}$ must meet the following criteria:\n\n\\begin{equation}\n\\footnotesize\n\\begin{cases}\ns_0 \\in (-\\infty,0) \\\\\ns_1 \\in (s_0, 0) \\\\\ns_i \\in [0,1),\\xspace s_{i-1} < s_{i}, i \\in \\{2,3,4\\} \\\\\n\\end{cases}\n\\label{scale-values}\n\\end{equation}\n\n  \n    \n  \n   \n\n\\begin{algorithm}[t]\n\\footnotesize\n  \\caption{One step of semantic training in RewardRepair}\n  \\begin{algorithmic}[1]\n  \\STATE \\textbf{Input:}  buggy code b, context code c, ground-truth patch code p, human-written test cases $t_h$,\n   RGT test cases $t_m$,\n  learning rate $\\alpha$, G is the \\approach patch generator, D is the discriminator function\n  \\STATE $\\widetilde{b} \\gets \\varphi (b)$ \\{Encode buggy code\\}\\label{algo1:buggycode}\n  \\STATE $\\widetilde{c} \\gets \\varphi (c)$ \\{Encode context code\\} \\label{algo1:contextcode}\n  \\STATE $\\widetilde{p} \\gets \\varphi (p)$ \\{Encode ground-truth patch code\\} \\label{algo1:gtcode}\n  \\STATE $ \\widetilde{q} \\gets G (\\widetilde{b}, \\widetilde{c})$ \\{Generate candidate patch \\}\\label{algo1:generate} \n  \\STATE $\\mathcal{L} =  \\sum_{x \\in X } \\widetilde{p}(x)  log \\,  \\widetilde{q}(x) $ \\label{algo1:celoss}\n  \\STATE $ R \\gets D (\\widetilde{b},\\widetilde{c},\\widetilde{q}, t_h, t_m)$ \\label{algo1:discrim}\n  \\STATE $\\mathcal{L}_{\\approach}  = (1-R)* \\mathcal{L} $ \\label{algo1:semanticloss}\n  \\STATE $G \\gets G -  \\alpha \\partial L_{\\approach}/\\partial G  $ \\{update patch generator with backpropagation\\} \\label{algo1:updategenerator}\n  \\end{algorithmic}\n  \\label{alg:SemanticRewardRepair}\n\\end{algorithm}\n\n\\subsubsection{Algorithm}\n\nAlgorithm~\\autoref{alg:SemanticRewardRepair}  presents one step of semantic training. \nGiven the encoded buggy $\\widetilde{b}$ and context code $\\widetilde{c}$ (line \\ref{algo1:buggycode} and \\ref{algo1:contextcode}), the patch generator $G$ generates a candidate patch (line \\ref{algo1:generate}).\nThen, the cross-entropy loss $\\mathcal{L}$ is computed by comparing the token distribution between the ground truth patch $\\widetilde{p}$ and the generated patch $ \\widetilde{q}$ (line \\ref{algo1:celoss}), where the $x$ indicates the index of tokens.\nThe discriminator provides a reward $R$ based on generated patch  $ \\widetilde{q}$ and the corresponding program compilability and test execution information (line \\ref{algo1:discrim}). \nLastly, \\approach combines the cross-entropy loss at token-level  and reward value at patch-level  to form \\approach loss (line \\ref{algo1:semanticloss}), which encodes the program-specific knowledge. \n\n\\subsection{Inference}\n\\label{sec:inferencephase}\n\nAt inference phase, for a given suspicious statement found by fault localization tools (e.g., Ochiai \\cite{fl-tool}), \\approach{} represents it with two sequences of tokens: one for the suspicious statement, the other one for its context (see Section \\ref{sec:patch_generator}).\nThose tokens are given to the patch generator of \\approach{}, previously trained as explained in Sections~ \\ref{sec3:syntactic_training}~and~\\ref{sec:semantic_training}.\nAs  \\approach is configured by the inference beam size $n$ (see \\cite{CURE-icse21,SEQUENCER}), it outputs  the $n$ best patches for that suspicious statement.\nRewardRepair can be used with any fault localization technique in  real-world bug repair tasks, as shown by \\cite{Rhero}. \n\n\\subsection{Implementation}\n\\label{sec-implementation}\n\nWe implement \\approach's patch generator  with the state-of-the-art Transformer based architecture \\cite{T5} from Hugging Face. \nRewardRepair is trained with 15 syntactic training epochs and 4 semantic training epochs. \nFor hyper-parameters configuration, we use a vocabulary size of \\numprint{32128}.\nWe configure \\approach to take a maximum of $512$ input tokens from buggy and context code, and generate a patch with a maximum of $100$ tokens. \nThe learning rate sets to $1\\mathrm{e}{-4}$ for both syntactic and semantic training. We configure reward scaling values $s_{i}, i \\in \\{0,1,2,3,4\\}$ respectively to \\{-0.4, -0.2, 0.2, 0.4, 0.6\\} for the best experiment result. \nThe encoder and decoder consist of 6 layers.\n \\approach is configured by a beam size of $200$ and outputs the $200$ best patches per bug. We consider the beam size of $200$ rather than $1000$ used in CURE \\cite{CURE-icse21} and CoCoNuT \\cite{CoCoNuT} due to the limitations of our available GPUs.\n\n\\section{Experimental Methodology}\n\nIn this section, we describe our methodology for evaluating \\approach by defining three research questions and how we propose to answer them.\n\n\\subsection{Research Questions}\n\n\\begin{itemize}\n\n\\item RQ1 (comparison with other tools): To what extent is RewardRepair effective at repairing bugs compared with the state-of-the-art repair approaches?\n\n\\item RQ2 (compilable rate): To what extent does RewardRepair improve the compilability of generated patches? \n\n\\item RQ3 (impact of semantic training): To what extent does semantic training improve the effectiveness of RewardRepair? \n\n\\end{itemize}\n\n\\subsection{Dataset}\n\\label{sec:datasets}\n\\begin{table}[t]\n\\footnotesize\n\\renewcommand{\\arraystretch}{1.28}\n\\begin{tabular}{p{0.1\\linewidth}p{0.35\\linewidth}p{0.28\\linewidth}p{0.1\\linewidth}}\n\n\\hline\n \\bf{Phases} & \\bf{Requirements} & \\bf{Name \\& Source}  & \\bf{\\#Patches} \\\\\n\\hline\n \\multirow{3}{*}{\\makecell{Syntactic\\\\training}} & \\multirow{3}{*}{Tokenization}   &CoCoNuT \\cite{CoCoNuT} & \\numprint{3241966}\\\\\n \n \n \n &&MegaDiff \\cite{monperrus2021megadiff}  & \\numprint{240306} \\\\\n  &&CodRep \\cite{Chen2018Coderep}  & \\numprint{24969} \\\\\n\n  \n\n\\hline\n\\multirow{2}{*}{\\makecell{Semantic\\\\training}}  &\\footnotesize Tokenization, Compilation &\\multirow{2}{*}{Bears \\cite{Bears}} & \\multirow{2}{*}{123} \\\\\n& \\footnotesize Developer and RGT Tests&&\\\\\n\\hline\n\n\\multirow{4}{*}{Testing} & \\multirow{4}{*}{ \\makecell{\\footnotesize Tokenization, \\footnotesize Compilation,\\\\ Developer Tests}} & Defects4J(v1.2) \\cite{defects4j} & 120 \\\\\n&& Defects4J(v2.0) \\cite{defects4j} & 257 \\\\\n&& Bugs.jar \\cite{Bugsjar-MSR18} & 490 \\\\\n&&QuixBugs \\cite{lin2017quixbugs} & 34\\\\\n\n\\hline\n\n\\end{tabular}\n\\caption{Datasets used for the different steps of our experiment.}\n\\label{tab:dataset}\n\\end{table}\n\nRecall that we need three datasets for syntactic training,  semantic training and testing. \n\nWe have common criteria for both training and testing datasets: \n\\begin{inparaenum}[1)]\n\\item all datasets are composed of bug-fix patches;\n\\item we focus on single-file patches, per previous work \\cite{SEQUENCER,DLFix,tbar};\n\\item we focus on single-hunk patches, where the patch is confined to a single contiguous chunk of code, at a single location, per previous work \\cite{SEQUENCER,DLFix,tbar}; \n\\item  we discard patches that do not make program behavior differences, e.g., those with only changes in comments or logging.\n\\end{inparaenum}\n\nNext, we have specific requirements per dataset.\n\\autoref{tab:dataset} shows the dataset of patches that we use for training and evaluating \\approach.  \nThe first column indicates the phase where each dataset is used. \nThe second column gives the requirements for each dataset. \nThe third column gives the source of the dataset and the fourth column indicates the number of patches in this dataset.\nFor example, as shown in the first row, \\approach is syntactically trained with data from three different sources, CoCoNuT \\cite{CoCoNuT}, Megadiff \\cite{monperrus2021megadiff} and CodRep \\cite{Chen2018Coderep}. \n\nAs aforementioned in Section \\ref{sec:rewardrepair}, \nthe selection criteria for semantic training dataset are:\n\\begin{inparaenum}[1)]\n\\item to be able to compile the patched program;\n\\item to run the test cases on the patched program;\n\\item to be able to automatically generate tests to specify the expected program behavior. \n\\end{inparaenum}\nAll criteria are met for 123 single-hunk bugs of the Bears dataset~\\cite{Bears}, for which some available RGT tests were generated by previous research~\\cite{ODS}. \nWe use the evaluation mode of \\approach to create more semantic training points based on beam search, as done in \\cite{kommrusch2021selfsupervised}.\nIt is to be noted that those criteria are very strong, and neither CoCoNuT \\cite{CoCoNuT}, MegaDiff \\cite{monperrus2021megadiff} nor CodRep \\cite{Chen2018Coderep} meets them, in particular, the patched program cannot be compiled.\n\nTo test \\approach, we  use well-accepted datasets from program repair research \\cite{Liu2020Efficiency, Martinez2017experiment, Durieux:2019:RepairThemAll, quixbugs-jss}:\nDefects4J~\\cite{defects4j}, \nBugs.jar \\cite{Bugsjar-MSR18}, and QuixBugs \\cite{lin2017quixbugs}.\nFor all those bug datasets, the requirements of compilation and test execution are met.\nIn line with the most recent work \\cite{Recoder}, we also consider the bugs of Defects4J version 2.0.\nAfter filtering single-hunk bugs, we use 120 bugs from Defects4J(v1.2) and 257 additional new bugs from Defects4J version 2.0  denoted as Defects4J(v2.0) in our paper.\nWe use the same single-hunk bugs criteria for Bugs.jar and QuixBugs.\n\n\\subsection{Methodology for RQ1}\n\\label{sec:method_rq1}\nIn RQ1, we compare \nRewardRepair against the state-of-the-art neural repair approaches: CURE \\cite{CURE-icse21}, Recoder \\cite{Recoder}, CoCoNuT~\\cite{CoCoNuT}, and other approaches \\cite{hercules,tbar,Simfix:2018,capgen-ICSE18,Yuan2017ARJAAR,sharpFix,elixir,nopol,astor}.\nPer  previous studies \\cite{CoCoNuT,CURE-icse21}, we take the quantitative results from the literature.\n\\ASERevision{\nWe run RewardRepair under two fault localization modes.\nFirst, we use spectrum-based fault localization with Gzoltar \\cite{GZoltar} per previous work \\cite{LeGoues2012GenProg,nopol}.\nSecond, we assume that the fault has been localized, an evaluation technique known as perfect fault localization and extensively used in recent work \\cite{Liu2020Efficiency,CURE-icse21,CoCoNuT}.}\n\nWe compute the two traditional APR performance metrics for each testing dataset:\n\\begin{inparaenum}[\\it 1)]\n\\item  the number of bugs that are correctly repaired. In our paper, a patch is deemed correctly repaired if it meets either of the two following criteria:  it is identical to the developer patch, or it is considered as correct by manual analysis done by at least two authors;\n\\item the number of bugs that can be uniquely repaired by individual repair approaches.\n\\end{inparaenum}\n\n\\subsection{Methodology for RQ2}\nIn RQ2, we calculate the compilable rate of RewardRepair.\nAs compilable rates were reported in SequenceR~\\cite{SEQUENCER}, CoCoNuT~\\cite{CoCoNuT} and CURE~\\cite{CURE-icse21}, we use the same benchmarks Defects4J(v1.2) and QuixBugs as they do, and compare against the numbers reported in the original papers. \nWe also follow the existing work \\cite{CURE-icse21} and compute the compilable rate depending on the beam size.\nWe report on beam sizes in 30, 100 and 200.\nWe do not consider larger beams due to the limitations of our available GPUs. \n\n\\subsection{Methodology for RQ3}\nIn RQ3, we conduct an ablation study with the goal of measuring the effect of semantic training.\nTo understand the contribution of semantic training, we compare the effectiveness of  \\approach considering:\n\\begin{inparaenum}[\\it 1)]\n\\item only syntactic training; \n\\item both syntactic and semantic training.\n\\end{inparaenum}\nFor doing this study, we apply the same protocol as the one used for responding to the RQ1.\nWe conduct manual analysis on the unique bugs that are only repaired by including semantic training and present the most interesting categories on repair action changes.\n\n\\section{Experimental Results}\n\n\\begin{table}[t!]\n\\footnotesize\n\\renewcommand{\\arraystretch}{1.28}\n\\begin{tabular}{lccrr}\n\\hline\n \\textbf{Approaches} &\\textbf{D4J(v1.2)} & \\textbf{D4J(v2.0)} &\n\\textbf{Bugs.jar}& \\textbf{QuixBugs} \\\\\n\n& 120 bugs& 257 bugs & 490 bugs & 34 bugs   \\\\\n\\hline\n\n \\multicolumn{5}{c}{Using Spectrum-based Fault Localization}  \\\\\n\\hline\njGenProg \\cite{astor} & 5 & -& - & 1 \\\\\nNopol \\cite{nopol} & 5& - & - & 2  \\\\\nElixir \\cite{elixir} &26 & -&22 & -  \\\\\nsharpFix \\cite{sharpFix} &27& -&15&- \\\\\nSimFix \\cite{Simfix:2018}& 27 & 2 &- & -\\\\\nHercules \\cite{hercules} &33& - & - &-\\\\\nRecoder \\cite{Recoder}&39 & 19 &-&17 \\\\\nRewardRepair (this paper) & 29 & 24 & 42 & 19\\\\\n\\hline\n\n \\multicolumn{5}{c}{Assuming Perfectly Localized Fault} \\\\\n \\hline\nSequenceR \\cite{SEQUENCER} &14 & -& - & -  \\\\\nDLFix \\cite{DLFix} & 33 &- & -&- \\\\\nTBar \\cite{Liu2020Efficiency}&33 & 8 & - & -   \\\\\n\nCoCoNuT \\cite{CoCoNuT} &33& -&-&12 \\\\\nCURE \\cite{CURE-icse21}&45&-&-& \\textbf{24} \\\\\nRecoder \\cite{Recoder}&\\textbf{52}& - &-&- \\\\\n\\hline\nRewardRepair (this paper) &45  & \\textbf{45} &\\textbf{97}&20\\\\\nRewardRepair  Unique & 5 & 34 &78&4\\\\\n\\hline\n\\end{tabular}\n\\caption{Comparison of RewardRepair against the related work, the numbers from the related work are filtered by single-hunk bugs. Across all benchmarks, RewardRepair correctly fixes 207 bugs  and uniquely fixes 121 ones. We use 4 testing benchmarks to maximize generalizability.}\n\\label{tab:comparison-sota}\n\\end{table}\n\n\\subsection{RQ1: Comparative Study with Other Repair Approaches}\n\\label{sec:result_comparison}\n\n\\begin{listing}[t!]\n\\noindent    \\begin{lstlisting} [firstnumber=419] \nif (_dataFormatReaders != null) {\n  return _detectBindAndReadValues(_dataFormatReaders.findFormat(\n     src,offset,length), false);}\n<@\\colorbox{red!30}{-  return \\_bindAndReadValues(\\_considerFilter(\\_parserFactory.createParser(src),  }@>        \n<@\\colorbox{green!30!}{+  return \\_bindAndReadValues(\\_considerFilter(\\_parserFactory.createParser \\quad\\quad\\quad }@>\n<@\\colorbox{green!30!}{+\\quad\\quad (src,offset,length), \\quad\\quad\\quad\\quad \\quad\\quad \\quad\\quad\\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad \\quad\\quad \\quad\\quad \\quad\\quad\\quad\\quad \\quad\\quad }@>\n   true));\n    \\end{lstlisting}\n \n\n\\caption{RewardRepair correct patch for Defects4J(v2.0) JacksonDatabind\\_57}\n\\label{lst:JacksonDataBind-unique-patches}\n\\end{listing}\n\n\\autoref{tab:comparison-sota} shows the patch generation results of RewardRepair and  12 other APR approaches on four benchmarks: the two versions of Defects4J, Bugs.jar and QuixBugs.\nThe numbers are the correctly repaired bugs by each APR approach.\nThe results are those reported in the literature, by the authors of the tool or by subsequent comparative experiments \\cite{Liu2020Efficiency,quixbugs-jss}. A ‘-’ indicates that the APR approach has not been evaluated on the considered benchmark, to the best of our knowledge.\n\\ASERevision{\nNote that the first seven APR approaches were executed with spectrum-based fault localization (FL), where the later approaches assumed perfect FL. We measure RewardRepair's effectiveness with both spectrum-based and perfect FL. Next, we focus on the comparison under perfect FL as the most state-of-the-art techniques only report effectiveness with perfect FL.}\n\n\\textbf{Repaired bugs.} Overall,  RewardRepair is able to correctly repair 45 of 120 bugs on Defects4J(v1.2), 45 of 257 bugs on Defects4J(v2.0), 97 of 490 bugs on Bugs.jar and 20 of 34 bugs on QuixBugs benchmark. From these results, we make the following observations.\n\nRewardRepair outperforms all APR approaches in two benchmarks: Defects4J(v2.0) and Bugs.jar.  \nRewardRepair sets new baselines of repaired bugs for these two benchmarks. \nWhile the majority of APR papers showcase bugs from version 1.0 or 1.2 of Defects4J, \\autoref{lst:JacksonDataBind-unique-patches} gives the correct RewardRepair patch for a Defects4J(v2.0) bug: \\textit{JacksonDatabind\\_57}. \nAs shown, RewardRepair succeeds in reusing the surrounding variables \\texttt{offset} and \\texttt{length} to construct the parameter list for overridden method $createParser$. \nRecoder \\cite{Recoder}, which is so far the best tool evaluated on Defects4J(v2.0) fails at repairing this bug.\n\n\\begin{figure}\n         \\includegraphics[width=0.338\\textwidth]{figures/newvenn.png}\n         \\caption{Uniquely repaired bugs on Defects4J(v1.2).}\n         \\label{fig:venn-compare}\n\\end{figure}\n\n\\begin{listing}[t!]\n\\noindent\\begin{minipage}[b]{0.49\\textwidth}\n    \\begin{lstlisting} [firstnumber=419] \nprivate void removeUnreferencedFunctionArgs(...){          \n<@\\colorbox{green!30!}{+    if (!removeGlobals) \\{ \\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad \\quad  }@>\n<@\\colorbox{green!30!}{+  \\quad   return; \\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad \\quad\\quad\\quad\\quad}@>\n<@\\colorbox{green!30!}{+    \\} \\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad \\quad\\quad }@>\n    \\end{lstlisting}\n\n\\end{minipage}%\n\\caption{\\approach patch for Closure-1 from Defects4J(v1.2), identical to the developer patch.}\n\\label{lst:Closure1}\n\\end{listing}\n\nIn addition, RewardRepair achieves the top-2 performance on Defects4J(v1.2) and QuixBugs benchmarks.\nRewardRepair performs better than all APR approaches on Defects4J(v1.2) but Recoder. \nRegarding Recoder's performance on Defects4J(v1.2), we analyze the bugs that cannot be repaired by RewardRepair. \nWe find that three bugs (\\textit{Closure-14}, \\textit{Closure-104}, and \\textit{Closure-118}) require fixing tokens outside the considered buggy class  and three bugs (\\textit{Lang-26}, \\textit{Lang-43} and \\textit{Closure-33}) require fixing tokens outside the context code scope as implemented by RewardRepair. \nThis analysis suggests that RewardRepair could achieve  better performance by enlarging the context code scope.\nRegarding CURE's performance on QuixBugs, the reason is likely the beam size: recall that CURE generates 10,000 candidate patches for each bug, while RewardRepair generates 200 patches per bug. \nAs shown in previous research \\cite{Tufano-ICSE19}, a larger beam size leads to more correct patches. \n\n\\textbf{Uniquely repaired bugs.}\nLet us now focus on the last row of \\autoref{tab:comparison-sota}, which gives the number of uniquely repaired bugs by RewardRepair. \nRewardRepair respectively repairs 5, 34, 78 and 4 unique bugs for Defects4J(v1.2), Defects4J(v2.0), Bugs.jar and QuixBugs, all of which were never repaired by any other APR approaches in the literature. \nThis shows RewardRepair complements all the existing works on the four considered benchmarks.\n\n\\autoref{fig:venn-compare} gives the detailed uniqueness analysis on Defects4J(v1.2) with the state-of-the-art APR approaches. \nWe give the exact results for the most recent neural repair approaches (CoCoNuT, CURE and Recoder).\nWe combine the rest  of the top-ranked related work in a unique bin for sake of readability.\nAs shown,  RewardRepair fixes 5 unique bugs compared with the other APR approaches on Defects4J(v1.2). Notably, they come from three different Defects4J projects (Closure, Lang and Math) showing that the additional learned knowledge is not specific to one single domain.\n\n\\autoref{lst:Closure1} gives the RewardRepair patch for \\textit{Closure-1}, which can only be fixed by RewardRepair. This is a patch with an addition of an \\texttt{if} block, including the code of the \\texttt{then} statement. RewardRepair learns the \\texttt{if} condition from the given context code.  \nWhile pattern-based repair is able to synthesize conditions (e.g., TBar \\cite{tbar}), no pattern-based repair systems have this complete \\texttt{if/then/return} pattern. \nThe recent neural repair models, CoCoNuT and CURE, do not generate this patch, we suspect that with a strict token-based cross-entropy optimization, Recoder does not learn such a complex patch structure. \n\\approach  is the first to produce this addition-only patch based on a non-trivial \\texttt{if/then/return} structure. \n\n\\begin{table}[t!]\n\\small\n\\renewcommand{\\arraystretch}{1.78}\n \\begin{tabular}{p{0.35\\linewidth} p{0.15\\linewidth} p{0.15\\linewidth} p{0.18\\linewidth}}\n\\hline\nModel &  \\textbf{Top-30} &  \\textbf{Top-100} &   \\textbf{Top-200} \\\\\n\n\\hline\nSequenceR~\\cite{SEQUENCER} & 33\\% &- &- \\\\\nCoCoNuT~\\cite{CoCoNuT} & 24\\% & 15\\% & 6\\%-15\\% \\\\\nCURE~\\cite{CURE-icse21} & 39\\%  & 28\\% & 14\\%-28\\%  \\\\\n\\hline\nRewardRepair& \\textbf{45.3\\%}  &\\textbf{37.5\\%} &\\textbf{33.1\\%}  \\\\\n\\hline\n\n\\end{tabular}\n\\caption{Average compilable rates of the Top-K candidate patches in Defects4J(v1.2) and QuixBugs. ‘-’ indicates data unavailability.}\t\n\\label{tab:compilability}\n\\end{table}\n\n\\textbf{Generalizability}.\nDurieux et al. \\cite{Durieux:2019:RepairThemAll} revealed the phenomenon of ``benchmark overfitting\" in program repair, meaning that performance results reported in the APR literature do not generalize to other benchmarks. The main reason is that APR approaches were typically evaluated in a single dataset, in particular, Defects4J(v1.2) in Java evaluation. \nRewardRepair is evaluated on four benchmarks in order to maximize the generalizability of our claims. \nTo our knowledge, this is one of the experiments with the largest number of testing benchmarks used for assessing the proposed repair approach. \n\n\\vspace{0.2cm}\n\n\\begin{mdframed}\nAnswer  to  RQ1: \n\\approach  correctly fixes 45, 45, 97 and 20 bugs on the considered Java benchmarks Defects4J(v1.2), Defects4J(v2.0), Bugs.jar and QuixBugs, respectively.\nThere are 121 unique bugs that are repaired by RewardRepair for the\nfirst time ever w.r.t. the APR literature.\nThe external validity of our results is founded on 4 testing benchmarks.\n\\end{mdframed}\n\n\\begin{figure}[t!]\n     \\includegraphics[width=0.5\\textwidth]{figures/compilable_figure}\n     \\caption{RewardRepair compilable rate on Defects4J(v1.2) by project.}\n     \\label{fig:compilable-rate}\n\\end{figure}\n\n\\vspace{1cm}\n\n\\subsection{RQ2: Improvement of Compilable Rate}\n\n\\begin{listing}[t!]\n\\noindent\\begin{minipage}[b]{0.49\\textwidth}\n\\begin{lstlisting} [firstnumber=419] \n for (FormattingOption formattingOption : flags.formatting) {        \n      formattingOption.applyToOptions(options); }\n<@\\colorbox{red!30!}{- if (flags.process\\_closure\\_primitives) \\{ \\quad\\quad \\quad \\quad\\quad\\quad\\quad \\quad \\quad\\quad \\quad\\quad \\quad\\quad\\quad }@>\n<@\\colorbox{red!30!}{-  \\quad options.closurePass = true; \\quad\\quad \\quad\\quad \\quad\\quad \\quad \\quad\\quad\\quad\\quad \\quad \\quad\\quad \\quad\\quad \\quad \\quad\\quad \\quad }@>\n<@\\colorbox{red!30!}{-\\quad \\}\\quad\\quad \\quad\\quad \\quad\\quad \\quad \\quad\\quad \\quad\\quad\\quad\\quad \\quad\\quad \\quad\\quad \\quad \\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad \\quad\\quad \\quad \\quad\\quad\\quad\\quad}@>\n<@\\colorbox{green!30!}{+  options.closurePass = flags.process\\_closure\\_primitives; \\quad\\quad \\quad \\quad\\quad\\quad \\quad }@>\n  initOptionsFromFlags(options);\n  return options;\n }\n\\end{lstlisting}\n\\end{minipage}%\n\\caption{\\approach correct patch generated for Closure-101}\n\\label{lst:closure-101}\n\\end{listing}\n\n\\autoref{tab:compilability} shows the average compilable rates of the top-k candidate patches where k relates to the beam size. For sake of a fair comparison, we use  the same methodology as Jiang et al. \\cite{CURE-icse21} and we combine Defects4J(v1.2) and QuixBugs together (our appendix website gives the results per benchmark \\cite{experiment}). \nWe provide ranges for CoCoNuT and CURE regarding the top-200 results due to data unavailability:\nthe range is the bracket [top-100,  top-1000] of compilable rates as reported in the original paper of CURE \\cite{CURE-icse21}.\n\nNotably,  the compilable rate of RewardRepair outperforms the three considered approaches CURE, CoCoNuT and SequenceR for all beam size configurations.\nIn the best case, RewardRepair achieves a compilable rate of up to 45.3\\%, which is the highest among the three considered beam sizes. \nNext, we see that the compilable rate of \\approach decreases when we increase the beam size,  this result is consistent with the ones of CURE and CoCoNuT. Since the beam enumerates by decreasing probability, it suggests that the learned neural model does capture compilability and favors it. \n\nRecall that the key contribution of CURE \\cite{CURE-icse21} is to introduce  two strategies to increase the patch compilable rate of NMT based neural repair models: \\textit{valid-identifier checker strategy} and \\textit{length-control strategy}.\nBoth strategies work in the inference phase, by filtering out the invalid tokens in the Java code or patches that are not\nclose in length to the buggy code.\nThis means that CURE does not embed program-specific knowledge in the neural network. \nOn the contrary, RewardRepair learns this knowledge during semantic training.  That is if an invalid identifier is used during semantic training  and results in a non-compilable patch, RewardRepair punishes the patch by increasing the loss. \nBeyond identifiers, RewardRepair is also able to learn other programming knowledge during semantic training, such as structure and typing constraints related to the domain classes and methods.\nTo some extent, CURE is limited to the static analysis checks devised and implemented by its authors while RewardRepair works in a fully agnostic, data-driven way to identify important compilation constraints.\n\n\\ASERevision{We analyze the 1883 uncompilable patches from the top-30 patches generated for Defects4J(v1.2).  We show the most frequent 10 compilation errors in\n\\autoref{tab:uncompilable-analysis}.  If there are multiple errors for one patch, we only count the first error per patch.\nThe first column gives the compilation error type and the second column shows the number of patches that fail on the corresponding error type. \nWe see most common error types are related to semantics that very few errors are related to syntax (the first one being \";\" expected). To overcome the typing problems, this suggests doing some specific training on the project under repair to capture this missing knowledge \\cite{selfapr}.}\n\n\\autoref{lst:closure-101} shows the RewardRepair patch for bug \\textit{Closure-101} from Defects4J(v1.2), which is identical to the developer patch. RewardRepair incorporates two repair actions in this patch: First,  RewardRepair removes the \\textit{if/then} condition. Second, RewardRepair generates a new statement by combining the logical expression from the \\textit{if-condition} and from the statement in the \\textit{then} block.  This is arguably a complex patch, and no repair system has reported generating a patch for this bug. This case shows that RewardRepair generates a compilable and correct patch with the complex structure. CURE fails to generate this patch because of its \\textit{length-control strategy} that encourages patches similar to the buggy code in length. \n\n\\begin{table}[t!]\n\\footnotesize\n\\renewcommand{\\arraystretch}{1.28}\n \\begin{tabular}{p{0.75\\linewidth} r }\n\\hline\nCompile Errors &No. Failures\\\\\n\\hline\ncannot find symbol&606\\\\\nillegal start of expression&329\\\\\nno suitable method constructor found for...&132\\\\\nincompatible types&123\\\\\nnot a statement&102\\\\\n\";\" expected&76\\\\\nunreachable statement&63\\\\\ncase, default,or \\} expected&59\\\\\nincomparable types&56\\\\\nmethod X in class Y cannot be applied to given types&53\\\\\n\\hline\n\n\\end{tabular}\n\\caption{Analysis of Top-10 reasons for uncompilable generated patches after semantic training.}\t\n\\label{tab:uncompilable-analysis}\n\\end{table}\n\n\\begin{table}[t!]\n\\footnotesize\n\\renewcommand{\\arraystretch}{2.2}\n\\begin{tabular}{c p{0.1\\linewidth}p{0.1\\linewidth}p{0.1\\linewidth}p{0.1\\linewidth}p{0.1\\linewidth}}\n\n\\hline\n\\textbf{Model} & \\textbf{D4J(v1.2)} &\\textbf{D4J(v2.0)}&\\textbf{Bugs.jar}& \\textbf{QuixBugs} & \\textbf{Total} \\\\\n\\hline\n\\makecell{RewardRepair\\\\(Syntatic)} & 42 &  40 & 93  &18 & 193 \\\\\n\n\\makecell{RewardRepair\\\\(Syntatic+Semantic)} & 45 &45 & 97 & 20 & 207  \\\\\n\\hline\n\n\\end{tabular}\n\\caption{Ablation study w.r.t correct patches. }\t\n\\label{tab:ablation}\n\\end{table}\n\n\\autoref{fig:compilable-rate} shows the compilable rate of RewardRepair per project of Defects4J(v1.2) with top 30, 100 and 200 candidate patches according to beam. \nWe make the two observations as follows:\nFirst, for all  projects, increasing the beam size of RewardRepair decreases the compilable rate for each project. This confirms the conclusion made in \\autoref{tab:compilability} at the level of aggregate results over bugs and benchmarks.\nSecond, the range of compilable rates over bugs decreases with beam size, both the range of extreme values (whiskers) and the range of interquartile values (boxes). \nWe explain this by statistical sampling (sampling 30 items yields less stable results than sampling 200). However, it may also be that the compilable rate does change significantly for some bugs. \nThis latter explanation is supported by the fact that there is a clear difference in compilable rate depending on the project (Lang patches compile much more than Time patches). \nThis latter phenomenon -- the compilable rate significantly varying over projects, in the worst case being 0\\% -- is a yet unknown limitation of neural program repair and \nsuggests more future research on this to increase the compilable rate in a more uniform way.\n\n\\begin{mdframed}\nAnswer to RQ2: \nFor all considered beam sizes, RewardRepair improves the compilable rate over the state-of-the-art.\nOver all benchmarks, RewardRepair reaches up to 45.3\\% of compilable patches (approximately one out of two patches compile), showing that the RewardRepair neural model has captured important information w.r.t. compilation.\n\\end{mdframed}\n\n\\subsection{RQ3: Impact of Semantic Training}\n\n\\autoref{tab:ablation} shows the results of the ablation study w.r.t semantic training. \nPer the same protocol as RQ1, the considered metric is the number of correct patches.\nThe first row shows RewardRepair's effectiveness with only syntactic training, the second row shows  RewardRepair with both syntactic and semantic training. \nFor example, RewardRepair with only\nsyntactic training generates 42 correct patches on Defects4J(v1.2),\nand RewardRepair with semantic training generates 45 correct\npatches.\nOverall, the addition of semantic training after syntactic training does yield more correct patches on  all considered benchmarks. This  shows that semantic training addresses the limits of syntactic training, and the improvement is not tied to specific benchmarks.\n\n\\begin{table}[t]\n\\footnotesize\n\\renewcommand{\\arraystretch}{1.1}\n\\begin{tabular}{p{0.45\\linewidth}cc}\n\\hline\n Actions & Syntactic Training & Semantic Training  \\\\\n \\hline\n \n\\multirow{1}{*}{Add if conditions} &\\multirow{1}{*}{1852} &\\multirow{1}{*}{2316} \\\\ \n\nMethod invocation & 1040 & 1352  \\\\\n\nAdd return statement & 1123 & 1554  \\\\\n\nTernary operator for null checking & 0&212\\\\\n\\hline\n\\end{tabular}\n\\caption{Examples of repair action differences between syntactic training and semantic training of RewardRepair.}\n\\label{tab:ablation-cases}\n\\end{table}\n\\begin{listing}[t!]\n\\noindent\\begin{minipage}[b]{0.5\\textwidth}\n    \\begin{lstlisting} [firstnumber=419] \n<@\\colorbox{red!30}{- this.dataset = dataset; \\quad\\quad \\quad\\quad \\quad\\quad \\quad \\quad\\quad \\quad\\quad\\quad\\quad\\quad \\quad\\quad}@>           \n    \\end{lstlisting}\n \n     \\subcaption{Buggy line}\n   \\label{motivate-human-patch}  \n    \\end{minipage}%\n    \\hfill\n    \\begin{minipage}[b]{0.49\\textwidth}\n    \\begin{lstlisting}[firstnumber=419] \n<@\\colorbox{green!30!}{+  .setDataSet(dataset) \\quad \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad \\quad\\quad \\quad\\quad}@> // patch 1\n<@\\colorbox{green!30!}{+   setDataset( ); \\quad\\quad \\quad\\quad \\quad\\quad \\quad \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad \\quad\\quad}@> // patch 2\n    \\end{lstlisting}\n \n    \\subcaption{Two non-compilable patches by syntactic training}\n \\label{motivate-noncompile-patch}   \n\\end{minipage}%\n\\hfill\n    \\begin{minipage}[b]{0.49\\textwidth}\n    \\begin{lstlisting}[firstnumber=419] \n<@\\colorbox{green!30!}{+ setDataset(dataset); \\quad\\quad \\quad\\quad \\quad\\quad \\quad \\quad\\quad \\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad}@>\n    \\end{lstlisting} \n    \\subcaption{Correct compilable patch by semantic training}\n    \\label{motivate-correct-patch} \n\\end{minipage}%\n\n\\caption{Bug Chart-12 only fixed by semantic training}\n\\label{lst:chart-12-rq3}\n\\end{listing}\n\nTo better understand the effectiveness of semantic training, we manually analyze those  unique bugs that are only generated by RewardRepair by including semantic training (and not generated with pure syntactic training). This leads to an analysis of \\numprint{14800} patches (\\numprint{7400} candidate patches by syntactic training and  \\numprint{7400} candidate patches by semantic training). \nWe group those patches by category and summarize the most interesting categories in \\autoref{tab:ablation-cases}.  \nThe first column gives the type of repair action employed in the patch, and the numbers in the second column and the third column indicate the numbers of patches based on those repair actions from syntactic and semantic training respectively.\nFor example, the first row shows there are \\numprint{1852} patches generated by syntactic training which add  \\textit{if/then} statements,  while semantic training yields \\numprint{2316} patches using this construct. This means that the usage of \\textit{if/then} statements is increased by 25.1\\% with semantic training. \nNotably, the unique bugs that benefit from semantic training come from different projects,  showing that adding semantic training is beneficial in general. \n\nRecall that syntactic training is done on more than 3 million training samples, while semantic training is done on 123 training samples, which is much less.\nThis suggests that the improvement obtained with semantic training does not come from the number of additional training points, but more from the training process   described in Section \\ref{sec-discriminator}. RewardRepair's loss function is able to better optimize the neural network, improving the quality of generated patches, with only a few training data points. \n\nFinally, \\autoref{lst:chart-12-rq3} discusses the case of \\textit{Chart-12}.\nThe first line is the buggy line.\nNext, in part (b), two patches after syntactic training are shown, they are both close to the correct patch but none of them compile (extraneous dot in the first patch and missing parameter in the second patch). \nFinally, part (c) shows the correct patch by RewardRepair with semantic training, which is identical to the developer patch.\nIn this case, it suggests that the neural model with semantic training has understood that leading dots before method calls is not correct per the Java grammar, and that \\texttt{setDataset} is a method likely to take a parameter called  \\texttt{dataset}.\nA subtle character may lead to a huge difference in program execution, but this knowledge is hard to be obtained by syntactic training with cross-entropy loss.\n\n\\begin{mdframed}\nAnswer to RQ3: \nOur ablation study shows semantic training of RewardRepair contributes to improving the overall effectiveness in terms of correctly fixed bugs. \n\\end{mdframed}\n\n\\section{ Discussion}\n\n\\subsection{Impact of Inference Beam Size}\nWe investigate the impact of beam search size in the inference time, and our experiment shows a bigger beam size indeed leads to more correct patches generated, which confirms the study of Tufano et al. \\cite{Tufano-tse19}.\nWe provide the results of configuring beam size as 500 in our online appendix repository \\cite{experiment}. \n\n\\subsection{Threats to Validity}\n\\label{sec:threats}\nA threat to external validity relates to whether the performance of \\approach generalizes to arbitrary programming languages. Per the standards of the field, our approach has been tested in one language (Java) and the evaluation is carried out on established benchmarks. In principle, our approach can be applied to other programming languages and datasets. \nA threat to internal validity relates to the hyper-parameter configuration we\nadopted.  To ensure replicability and extension, \nwe make all the source code and results publicly available for future research ~\\cite{experiment}.\n\n\\section{Related Work}\n\n\\subsection{Automatic Program Repair}\n\nA decade of research has generated a rich body of work on automatic program repair \\cite{Monperrus2015,TSE-repair-survey}. \nWe have already discussed neural repair approaches  \\cite{SEQUENCER,CoCoNuT,DLFix,Tufano-ICSE19,codit-tse20,Tufano-tse19,deepfix, Recoder,CURE-icse21} in Section \\ref{sec:background_neural}. \nThese approaches only use the syntactic cross-entropy loss objective, which poses a discrepancy between the training objective of generating compilable correct patches and the loss criterion.\nThe key novelty of \\approach is the discriminative model to capture the compilation and execution knowledge during model training and backpropagation.\n\nWe mentioned generate and validate (G\\&V) program repair approaches in RQ1 (Section \\ref{sec:result_comparison}). Other notable G\\&V systems include \\cite{jaid,Yuan2017ARJAAR,astor,ali-issta19-bytecode}.\nMoreover, the third line of research is about synthesis-based repair \\cite{acs,nopol,Angelixicse16,s3,directfix,CrashProgramRepair-ISSTA19,concolic-repair-PLDI21}\nwhich converts the search problem to a satisfiability problem.\nAll these approaches often work by extracting a repair constraint typically via symbolic execution incorporated with human knowledge for patch generation.\nOn the contrary, \\approach automatically learns such fix operators,  language grammar and semantics from the training corpus.\n\nIn the field of APR, the recent work of Jiang et al. \\cite{CURE-icse21} is the most closely related to ours, also focusing on the  non-compilable patch problem. They address this problem by employing a valid-identifier checker in the inference stage to filter invalid tokens.\nHowever, many reasons could lead to a non-compilable patch, and the presence of invalid identifiers is only one of them. \nOur approach is fundamentally different: 1) RewardRepair works at training time and not at inference time; 2) RewardRepair is based on the actual compilation and test execution of training patches; 3) RewardRepair encourages syntactic diversity while CURE encourages patches similar to the buggy code (the length-control strategy in CURE).\n\n\\subsection{Discriminators for Machine Learning on Code}\n\nSeveral works propose deep learning on code based on a discriminator \\cite{Harer-GAN-NIPS18, adversarial-repair-gan} where the discriminator provides a loss that solves the discrepancy between the generated and real distributions of the object under study, as pioneered by generative adversarial networks (GAN)\\cite{GanGoodFellow}.  \nHarer et al. \\cite{Harer-GAN-NIPS18} propose an adversarial learning approach to solve software vulnerabilities. \nAlhefdhi et al. \\cite{adversarial-repair-gan} leverage a similar GAN architecture to suggest repairs that are as close as possible to human-written repairs. \n\\approach shares the concepts of employing a traditional NMT model as a generator, and of replacing the cross-entropy loss with the feedback from a discriminator.  \nThe key difference between this related work and ours is that our discriminator uses execution information, through compilation and test execution. \n\n\\subsection{Improving Backpropagation}\nPast research has improved the cross-entropy loss based on domain-specific knowledge. \nIn neural machine translation, \nZhang et al. \\cite{NMT-bridging} show the limitation of considering cross-entropy loss and its tendency to overcorrect synonymous words and phrases. To relieve the problem,  further research \\cite{NMT-bridging,overcorrection} proposed to combine  cross-entropy loss and add translation evaluation at the sentence level.  \nIn object detection, Ryou et al. \\cite{AnchorLoss} proposed AnchorLoss to dynamically rescale the cross-entropy based on prediction difficulty.\nLoss scaling is a technique used in floating-point optimization, consisting of scaling up the loss value up before the start of backpropagation ~\\cite{micikevicius2018mixed,lossscale}. \n\n\\subsection{Training based on Execution}\n\nRecently, semantic information has been used in program synthesis tasks. \nChen et al. \\cite{chen2018executionguided} and Gupta et al. \\cite{sed-nips20} propose execution-guided synthesis leveraging the semantics of the language.  These approaches execute a partial program to obtain intermediate states to guide program synthesis.\nWang et al. \\cite{wang-sar}  use dynamic information from execution to measure semantic redundancy between student programs. \nMesbah et al. \\cite{deepdelta} extract compiler diagnostic information as an input source for repairing compilation errors. \nAs in our work, these approaches use execution information as an additional input for the considered model. The key difference is that none of them employ the execution information as a reward signal to update the neural network weights through backpropagation. \n\nWang and colleagues \\cite{wang2018dynamic,embed-ke-PLDI20} leverage the full execution traces to learn neural semantic program embeddings. These related works improve the code representation based on semantic information.\nOur novelty is not on the representation, but on the training objective improvement, which is not addressed in \\cite{wang2018dynamic,embed-ke-PLDI20}.\n\n\\section{Conclusion}\n\nWe have presented a novel neural program repair model \\approach  based on compilation and test execution. The key idea is to employ a discriminative model to provide a reward signal on the generated patches according to the actual execution outcome. This signal modulates the purely syntactic cross-entropy loss function in what we call semantic training.  \nWe have conducted an extensive empirical evaluation, including a comprehensive experiment on  the widely used benchmark Defects4J, Bugs.jar and QuixBugs.  \nOur results  show that it is possible to embed execution information in the backpropagation process to improve neural program repair.\n\n\\section{Acknowledgments}\n\\ASERevision{We thank the anonymous reviewers for the insightful feedback.\nThis work was supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.\nSome experiments were performed on resources provided by the Swedish National Infrastructure for Computing.}\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{A Syntax-Guided Edit Decoder for Neural Program Repair}\n\n\\begin{document}\n\n\\title{A Syntax-Guided Edit Decoder for Neural Program Repair}\n\n\\begin{abstract}\n  Automated Program Repair (APR) helps improve the efficiency of software development and maintenance. Recent APR techniques use deep learning, particularly the encoder-decoder architecture, to generate patches.\n  Though existing DL-based APR approaches have proposed different encoder architectures, the decoder remains to be the standard one, which generates a sequence of tokens one by one to replace the faulty statement.\n  This decoder has multiple limitations: 1) allowing to generate syntactically incorrect programs, 2) inefficiently representing small edits, and 3) not being able to generate project-specific identifiers.\n  \n  In this paper, we propose \\techname, a syntax-guided edit decoder with placeholder generation. \\techname is novel in multiple aspects: 1) \\techname generates edits rather than modified code, allowing efficient representation of small edits; 2) \\techname is syntax-guided, with the novel provider/decider architecture to ensure the syntactic correctness of the patched program and accurate generation;  3) \\techname generates placeholders that could be instantiated as project-specific identifiers later.\n\n  We conduct experiments to evaluate \\techname on 395 bugs from \\textit{Defects4J v1.2}, 420 additional bugs from \\textit{Defects4J v2.0}, 297 bugs from \\textit{IntroClassJava} and 40 bugs from \\textit{QuixBugs}. Our results show that \\techname repairs 51 bugs on Defects4J v1.2, which achieves 21.4\\% (9 bugs) improvement over the previous state-of-the-art approach for single-hunk bugs (TBar). Importantly, to our knowledge, \\textit{\\techname is the first DL-based APR approach that has outperformed the traditional APR approaches on this benchmark}. Furthermore, \\techname repairs 19 bugs on the additional bugs from Defects4J v2.0, which is 137.5\\% (11 bugs) more than TBar  and 850\\% (17 bugs) more than SimFix. \\techname also achieves 775\\% (31 bugs) and 30.8\\% (4 bugs) improvement on IntroClassJava and QuixBugs over the baselines respectively. These results suggest that \\techname has better generalizability than existing APR approaches.\n\\end{abstract}\n\n\\author{Qihao Zhu}\n\\affiliation{%\n  \\institution{Key Laboratory of HCST, MoE \\\\DCST, Peking University}\n   \\country{Beijing, China}\n  }\n\\email{Zhuqh@pku.edu.cn}\n\n\\author{Zeyu Sun}\n\\affiliation{%\n  \\institution{Key Laboratory of HCST, MoE \\\\DCST, Peking University}\n  \\country{Beijing, China}\n  }\n\\email{szy_@pku.edu.cn}\n\n\\author{Yuan-an Xiao}\n\\affiliation{%\n  \\institution{Key Laboratory of HCST, MoE \\\\DCST, Peking University}\n   \\country{Beijing, China}\n  }\n\\email{xiaoyuanan@pku.edu.cn}\n\n\\author{Wenjie Zhang}\n\\affiliation{%\n  \\institution{Key Laboratory of HCST, MoE \\\\DCST, Peking University}\n   \\country{Beijing, China}\n  }\n\\email{zhang_wen_jie@pku.edu.cn}\n\n\\author{Kang Yuan}\n\\affiliation{%\n  \\institution{Stony Brook University}\n  \\country{New York, US}\n  }\n\\email{kang.yuan@stonybrook.edu}\n\n\\author{Yingfei Xiong}\n\\authornote{Corresponding author. \\\\HCST: High Confidence Software Technologies.}\n\\affiliation{%\n  \\institution{Key Laboratory of HCST, MoE \\\\DCST, Peking University}\n   \\country{Beijing, China}\n  }\n\\email{xiongyf@pku.edu.cn}\n\n\\author{Lu Zhang}\n\\affiliation{%\n  \\institution{Key Laboratory of HCST, MoE \\\\DCST, Peking University}\n  \\country{Beijing, China}\n  }\n\\email{zhanglucs@pku.edu.cn}\n\\begin{CCSXML}\n<ccs2012>\n<concept>\n<concept_id>10011007.10011074.10011099.10011102.10011103</concept_id>\n<concept_desc>Software and its engineering~Software testing and debugging</concept_desc>\n<concept_significance>500</concept_significance>\n</concept>\n</ccs2012>\n\\end{CCSXML}\n\\ccsdesc[500]{Software and its engineering}\n\\ccsdesc[300]{Computing methodologies~Software testing and debugging}\n\\ccsdesc[300]{Computing methodologies~Neural networks}\n\\keywords{Automated program repair, Neural networks}\n\\newcommand{\\techname}{Recoder\\xspace}\n\n\\maketitle\n\n\\section{Introduction}\nAutomated program repair (APR) aims to reduce bug-fixing effort by generating patches to aid the developers. Due to the well-known problem of weak test suites~\\cite{PatchPlausibility}, even if a patch passes all the tests, the patch still has a high probability of being incorrect. To overcome this problem, existing approaches have used different means to guide the patch generation. A typical way is to learn from existing software repositories, such as learning patterns from existing patches~\\cite{6606626,2018Shaping,jiang2019inferring,liu2019tbar,long2017automatic,bader2019getafix,DBLP:conf/icse/RolimSDPGGSH17}, and using program code to guide the patch generation~\\cite{xiong2017precise,long2016automatic,2018Shaping,ELIXIR,xiong2018learning}. \n\nDeep learning is known as a powerful machine learning approach. Recently, a series of research efforts have attempted to use deep learning (DL) techniques to learn from existing patches for program repair~\\citep{8827954,DBLP:journals/corr/abs-1812-07170,9000077,dlfix}. A typical DL-based approach generates a new statement to replace the faulty statement located by a fault localization approach. Existing DL-based approaches are based on the encoder-decoder architecture~\\cite{Bahdanau2015NeuralMT}: the encoder encodes the faulty statement as well as any necessary code context into a fixed-length internal representation, and the decoder generates a new statement from it. For example, \\citet{DBLP:journals/corr/abs-1812-07170} and \\citet{9000077} adopt an existing neural machine translation architecture, NMT, to generate the bug fix; SequenceR~\\cite{8827954} uses a sequence-to-sequence neural model with a copy mechanism; DLFix~\\citep{dlfix} further treats the faulty statement as an AST rather than a sequence of tokens, and encodes the context of the statement. \n\nHowever, despite multiple existing efforts, DL-based APR approaches have not yet outperformed traditional APR approaches. Since deep learning has outperformed traditional approaches in many domains, in this paper we aim to further improve the performance of DL-based APR to understand whether we could outperform traditional APR using a DL-based approach. We observe that, though existing DL-based APR approaches have proposed different encoder architectures for APR, the decoder architecture remains to be the standard one, generating a sequence of tokens one by one to replace the original faulty program fragment. The use of this standard decoder significantly limits the performance of DL-based APR. Here we highlight three main limitations.\n\n{\\bf Limitation 1: Including syntactically incorrect programs in the patch space}. The goal of the decoder is to locate a patch from a patch space. The smaller the patch space is, the easier the task is. However, viewing a patch as a sequence of tokens unnecessarily enlarges the patch space, making the decoding task difficult. In particular, this space representation does not consider the syntax of the target programming language and includes many syntactically incorrect statements, which can never form a correct patch. \n\n{\\bf Limitation 2: Inefficient representation of small edits}.\nMany patches only modify a small portion of a statement, and re-generating the whole statement leads to an unnecessarily large patch space. For example, let us consider the patch of defect Closure-14 in the Defects4J benchmark~\\cite{defects4j}, as shown in Figure~\\ref{fig:pattern-de1}. This patch only changes one token in the statement, but under existing representation, it is encoded as a sequence of length 13. The program space containing this patch would roughly contain $n^{13}$ elements, where $n$ is the total number of tokens. On the other hand, let us consider a patch space including only one-token change edits. To generate that patch, only selecting a token in the faulty statement and a new token for replacement is needed. This patch space contains only $mn$ elements, where $m$ is the number of tokens in the faulty statement. Therefore, the size of the patch space is significantly reduced.\n\n{\\bf Limitation 3: Not being able to generate project-specific identifiers}. Source code of programs often contains project-specific identifiers like variable names. Since it is impractical to include all possible identifiers in the patch space, existing DL-based APR approaches only generate identifiers that have frequently appeared in the training set. However, different projects have different sets of project-specific identifiers, and therefore only considering identifiers in the training set may exclude possible patches from the patch space. For example, Figure~\\ref{fig:pattern-de2} shows the patch for defect Lang-57 in Defects4J. To generate this patch, we need to generate the identifier ``{\\tt availableLocaleSet}'', which is a method name of the faulty class, and is unlikely to be included in the training set. As a result, existing DL-based approaches cannot generate patches like this. \n\n\\smallskip\nIn this paper, we propose a novel DL-based APR approach, \\techname, standing for \\underline{\\textbf{re}}pair de\\underline{\\textbf{coder}}. Similar to existing approaches, \\techname is based on the encoder-decoder architecture. To address the limitations above, the decoder of \\techname has following two novel techniques.\n\n{\\bf Novelty 1: Syntax-Guided Edit Decoding with Provider/Decider Architecture} (concerning limitation 1 \\& 2).\nTo address limitation 2, the decoder component of \\techname produces a sequence of edits rather than a new statement. Our edit decoder is based on the idea of the syntax-guided decoder in existing neural program generation approaches~\\cite{treegen,DBLP:conf/acl/YinN17,DBLP:conf/acl/RabinovichSK17,DBLP:conf/aaai/SunZMXLZ19}. For an unexpanded non-terminal node in a partial AST, the decoder estimates the probability of each grammar rule to be used to expand the node. Based on this, the decoder selects the most probable sequence of rules to expand the start symbol into a full program using a search algorithm such as beam search. %selects a grammar rule to expand the node until a full program is generated.\nWe observe that edits could also be described by a grammar. For example, the previous patch for defect Closure-14 could be described by the following grammar: \n\\[\\begin{array}{rcl}\n    {\\it Edit} &\\rightarrow & {\\it Insert} \\mid {\\it Modify} \\mid \\ldots \\\\\n    {\\it Modify} &\\rightarrow & {\\tt modify}({\\it NodeID}, {\\it NTS}) \\\\\n\\end{array}\\]  \nHere {\\tt modify} represents replacing an AST subtree denoted by its root node ID ({\\it NodeID}) in the faulty statement with a newly generated subtree ({\\it NTS\\footnote{``NTS'' stands for ``non-terminal symbol in an AST''.}}).\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{picture/Closure-14.pdf}\n    \\vspace{-8mm}\n    \\caption{The Patch for Closure-14 in Defects4J }\n    \\vspace{-2mm}\n    \\label{fig:pattern-de1}\n\\end{figure}\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{picture/Lang-57.pdf}\n    \\vspace{-8mm}\n    \\caption{The Patch for Lang-57 in Defects4J}\n    \\vspace{-2mm}\n    \\label{fig:pattern-de2}\n\\end{figure}\n\nHowever, directly applying the existing syntax-guided decoder to the grammar above would not form an effective program repair approach, because the choice of expanding different non-terminal nodes may need to be deduced along with different types of dependencies.\nFirst, the expansion of some non-terminals depends on the local context, e.g., the choice of {\\it NodeID} depends on the faulty statement, and the neural network needs to be aware of the local context to make a suitable choice. \nSecond, to guarantee syntax correctness (limitation 1), dependency exists among the choices for expanding different non-terminal nodes, e.g., when {\\it NodeID} expands to an ID pointing to a node with non-terminal {\\it JavaExpr}, {\\it NTS} should also expand to {\\it JavaExpr} to ensure syntactic correctness. These choices cannot be effectively pre-defined, and thus the existing syntax-guided decoders, which only select among a set of pre-defined grammar rules, do not work here. \n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{picture/pd.pdf}\n    \\vspace{-7mm}\n    \\caption{Provider/Decider Architecture}\n    \\vspace{-4mm}\n    \\label{fig:pd}\n\\end{figure}\n\nTo overcome these problems, \\techname introduces a provider/decider architecture, as shown in Figure~\\ref{fig:pd}. A provider is a neural component that provides a set of choices for expanding a non-terminal and estimates the probability $p_i$ of each choice. A basic provider is the rule predictor, which, similar to existing syntax-guided decoders, estimates the probability of each grammar rule to expand the node. Fixing the Closure-14 example needs another provider, namely the subtree locator, which estimates the probability of each subtree in the faulty statement to be replaced. On the other hand, the decider is a component that estimates the probability of $q_j$ using each provider. In this example, when expanding {\\it Edit}, the probability of using the rule predictor is 1, and the probability of using the subtree locator is 0; when expanding {\\it Modify}, the probability of using the rule predictor is 0 and the probability of using the subtree locator is 1 (the located subtree decides both the content of {\\it NodeID} and the root symbol of {\\it NTS}). Finally, the choices provided by all providers form the final list of choices, while the probability of each choice is the product of the probability predicted by its provider and the probability of the provider itself, i.e., $p_i*q_j$. \n\nIn this example, for each non-terminal, we use the choices from only one provider, and thus the probabilities of providers are either 0 or 1. Later we will see that expanding some non-terminals requires comparing the choices of multiple providers, and the probabilities of providers could be a real number between 0 and 1.\n\n{\\bf Novelty 2: Placeholder Generation} (concerning limitation 3). \nTo generate project-specific identifiers, a direct idea is to add another provider that selects an identifier from the local context. However, to implement such a provider, the neural component needs to access all of the name declarations within the current project. This is a difficult job, as the neural component could hardly encode all source code from the whole project. \n\nInstead of relying on the neural network to generate project-specific identifiers, in \\techname the neural network generates placeholders for such identifiers, and these placeholders are instantiated with all feasible identifiers when applying the edits. A feasible identifier is an identifier compatible with constraints in the programming language, such as the type system. As for defect Lang-57 shown in Figure~\\ref{fig:pattern-de2}, \\techname first generates a placeholder for ``{\\tt availableLocaleSet}'', and it will be replaced with all methods accessible in the local context that takes no arguments and returns an object with a member method ``{\\tt contains}''. Each replacement forms a new patch. The key insight is that, when considering constraints in the programming language, the number of choices for replacing a placeholder with an identifier is small, and thus instantiating the placeholders with all possible choices is feasible.\n\nTo train the neural network to generate placeholders, we replace infrequent user-defined identifiers in the training set with placeholders. In this way, the neural network learns to generate placeholders for these identifiers.\n\n\\modify{}{Our experiment is conducted on four benchmarks: (1) 395 bugs from Defects4J v1.2 for comparison with existing approaches. (2) 420 additional bugs from  Defects4J v2.0, (3) 297 bugs from IntroClassJava, and (4) 40 bugs from QuixBugs to evaluate the generalizability of \\techname. The results show that \\techname correctly repairs 53 bugs on the first benchmark, which are 26.2\\% (11 bugs) more than TBar~\\cite{liu2019tbar} and 55.9\\% (19 bugs) more than SimFix~\\cite{2018Shaping}, two best-performing single-hunk APR approaches on Defects4J v1.2; \\techname also correctly repairs 19 bugs on the second benchmark, which are 137.5\\% (11 bugs) more than TBar and 850.0\\% (17 bugs) more than SimFix . On IntroClassJava and QuixBugs, \\techname repairs 35 bugs and 17 bugs respectively, which also achieves better performance than the existing APR tools that were evaluated on the two benchmarks. The results suggest that \\techname has better performance and better generalizability than existing approaches. To our knowledge, \\emph{this is the first DL-based APR approach that has outperformed traditional APR approaches}. \n}\nTo summarize, this paper makes the following contributions:\n\\begin{itemize}\n    \\item We propose a syntax-guided edit decoder for APR with a provider/decider architecture to accurately predict the edits and ensure that the edited program is syntactically correct and uses placeholders to generate patches with project-specific identifiers.\n    \\item We design \\techname, a neural APR approach based on the decoder architecture described above.\n    \\item We evaluate \\techname on 395 bugs from Defects4J v1.2 and 420 additional bugs from Defects4J v2.0. The results show that \\techname significantly outperforms state-of-the-art approaches for single-hunk bugs in terms of both repair performance and generalizability.\n\\end{itemize}\n \n\n\\tcbset{colframe = white,left=0mm,colback=white}\n\\section{Edits}\nWe introduce the syntax and semantics of edits and their relations to providers in this section. The neural architecture to generate edits and implement providers will be discussed in the next section.\n\n\\subsection {Syntax and Semantics of Edits}\n\n\\begin{figure}\n\\begin{tcolorbox}\n$\\begin{array}{rlrl}\n1.\\hspace{-2mm}&{\\it Edits} &\\rightarrow & {\\it Edit}; {\\it Edits} \\mid {\\tt end}\\\\\n2.\\hspace{-2mm}&{\\it Edit} &\\rightarrow & {\\it Insert} \\mid {\\it Modify}\\\\\n3.\\hspace{-2mm}&{\\it Insert} &\\rightarrow & {\\tt insert}({\\it \\langle HLStatement\\rangle}) \\\\\n4.\\hspace{-2mm}&{\\it Modify} &\\rightarrow & {\\tt modify}( \\\\\n& & &\\  \\langle\\textit{ID of an AST Node with a NTS}\\rangle, \\\\\n& & &\\  \\langle\\textit{the same NTS as the above NTS}\\rangle) \\\\\n5.\\hspace{-2mm}&{\\langle \\textit{Any NTS in HL}\\rangle }\\hspace{-2mm}&\\rightarrow \\\\\n&\\multicolumn{3}{r} {{\\tt copy}(\\langle \\textit{ID of an\\ AST\\ Node with the same NTS}\\rangle) } \\\\\n&&\\mid&{\\it \\langle The\\ original\\ production\\ rules\\ in\\ HL\\rangle} \\\\\n6.\\hspace{-2mm}&{\\it \\langle HLIdentifier\\rangle} \\hspace{-2mm}&\\rightarrow &{\\tt placeholder}\\\\\n&&\\mid&{\\it \\langle Identifiers\\ in\\ the\\ training\\ set\\rangle} \\\\\n\\end{array}$\n\\end{tcolorbox}    \n\\parbox{\\columnwidth}{\\footnotesize ``HL'' stands for ``host language''. ``NTS'' stands for ``non-terminal symbol''. ``${\\it \\langle HLStatement\\rangle}$'' is the non-terminal in the grammar of the host language representing a statement. ``${\\it \\langle HLIdentifier\\rangle}$'' is the non-terminal in the grammar of the host language representing an identifier. }\n\\vspace{-3mm}\n\\caption{The Syntax of Edits}\n    \\vspace{-2mm}\n    \\label{fig:syntax}\n\\end{figure}\n\nFigure \\ref{fig:syntax} shows the syntax of edits. %Rule {1} defines that a patch can contain several different edits. Thus, \\textit{Edits} is expanded iteratively when generating the patch. This process is repeated until a special symbol, \\texttt{End}, is generated. The semantic of this rule is that the edit will be applied to the program iteratively. Furthermore, rule {2} shows that \\textit{Edit} can be expanded into two different edit types, \\textit{Insert} and \\textit{Modify}. \nNote that our approach is not specific to a particular programming language and can be applied to any programming language (called the \\emph{host language}) that has a concept similar to the statement. In particular, it is required that when a statement is present in a program, a sequence of statements can also be present at the same location. In other words, inserting a statement before any existing statement would still result in a syntactically correct program. \nTo ensure syntactic correctness of the edited program, the syntax of edits depends on the syntax of the host language. In Figure~\\ref{fig:syntax}, ``HL'' refers to the host programming language our approach applies to. \\modify{Link rules to figure 4}{In the following we explain each rule in Figure~\\ref{fig:syntax} in order.} \n\nAs defined by Rule 1 and Rule 2, an {\\it Edits} is a sequence of {\\it Edit} ended by a special symbol {\\tt end}. An {\\it Edit} can be one of two edit operations, {\\tt insert} and {\\tt modify}.\n\nRule 3 defines the syntax of {\\tt insert} operation. The \\texttt{insert} operation inserts a newly generated statement before the faulty statement. As shown in Rule 3, the {\\tt insert} operation has one parameter, which is the statement to insert. Here $\\langle HLStatement\\rangle$ refers to the non-terminal in the grammar of the host language that represents a statement. This non-terminal could be expanded into a full statement, or a copy operation that copies a statement from the original program, or a mixture of both. This behavior will be explained later in Rule 5.\n\nRule 4 defines the syntax of {\\tt modify} operation. The \\texttt{modify} operation replaces an AST subtree in the faulty statement with a new AST subtree. The {\\tt modify} operation has two parameters. The first parameter is the ID of the root node from the AST subtree to be replaced. The ID of a node is defined as the order of a node in the pre-order traversal sequence, e.g., the 6th visited node has the ID of 6. \nThe second parameter is an AST subtree whose root node has the same symbol, i.e., the root node cannot be changed. In this way, the replacement ensures syntactic correctness. To ensure that there is an actual change, the subtree to be replaced should have more than one node, i.e., the root node should have a non-terminal symbol. \n\nFor both {\\tt insert} and {\\tt modify}, we need to generate a new AST subtree. It is noticeable that in many patches, the AST subtree being inserted or modified is not completely original; some of its subtrees may be copied from other parts of the program. Taking advantage of this property, {\\tt copy} operation is introduced to further reduce the patch space. Rule 5 defines the syntax of this operation. It is a meta-rule applied to any non-terminal symbol of the host language. For any non-terminal symbol in the host language, we add a production rule that expands it into a {\\tt copy} operation. The original production rules for this non-terminal are also kept, so that when generating the edits, the neural network could choose to directly generate a new subtree or to copy one.\n\nThe {\\tt copy} operation has one parameter, which identifies the root node of the AST subtree to be copied. The AST subtree can be selected from the faulty statement or its context. \nIn our current implementation, we allow copying from the method surrounding the faulty statement. Also, to ensure syntactic correctness, the root node of the subtree to be copied should have the same non-terminal symbol as the symbol being extended. \n\nFinally, Rule 6 introduces {\\tt placeholder} into the grammar. Normally, the grammar of a programming language uses a terminal symbol to represent an identifier. To enable the neural network to generate concrete identifiers as well as the {\\tt placeholder}, we change identifier nodes into non-terminals, which expand to either {\\tt placeholder} or one of the frequent identifiers in the training set. In our current implementation, an identifier is considered frequent if it appears more than 100 times in the training set. \n\nWhen applying the edits, the {\\tt placeholder} tokens are replaced with feasible identifiers within the context. We first collect all identifiers in the current projects by performing a lexical analysis and collect the tokens whose lexical type is $\\tt \\langle HLIdentifier\\rangle$, the symbol representing an identifier in the host language. Then we filter identifiers based on the following criteria: (1) the identifier is accessible from the local context, and (2) replacing the placeholder with the identifier would not lead to type errors. The remaining identifiers are feasible identifiers.\n\nFigure~\\ref{fig:insert} and Figure~\\ref{fig:modify} show two example patches represented by edits. The patch in Figure~\\ref{fig:insert} inserts an {\\tt if} statement, and the conditional expression contains a method invocation that is copied from the faulty statement. The patch in Figure~\\ref{fig:modify} replaces the qualifier of a method invocation with another invocation, where the name of the method is a placeholder to be instantiated later.\n\n\\begin{theorem}\n    The edited programs are syntactically correct. \n\\end{theorem}\n\\begin{proof}\nIt is easy to see that the theorem holds by structural induction on the grammar of the edits. First, the requirement on the host programming language ensures that inserting a statement before another statement is syntactical correct. Second, when replacing a subtree with {\\tt modify}, the root symbol of the subtree remains unchanged. Third, the new subtree in {\\tt insert} and {\\tt modify} is generated by either using the grammar rules of the host language, or copying a subtree with the same root symbol. Finally, instantiating a {\\tt placeholder} ensures syntactic correctness because we only replace a {\\tt placeholder} with a token whose lexical type is $\\tt \\langle HLIdentifier\\rangle$.\n\\end{proof}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{picture/Insert.pdf}\n    \\vspace{-6mm}\n    \\caption{Example of Insert Operation (Closure-2)}\n    \\vspace{-2mm}\n    \\label{fig:insert}\n\\end{figure}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{picture/Modify.pdf}                            \\vspace{-8mm}\n\n    \\caption{Example of Modify Operation (Lang-57)}\n    \\label{fig:modify}\n\\end{figure}\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=\\linewidth]{picture/overviewmodel.pdf}\n                            \\vspace{-9mm}\n    \\caption{\\modify{Overview of Model}{Overview of \\techname}}\\vspace{-2mm}\n    \\label{fig:overview-model}\n\\end{figure*}\n\\subsection{Generation of Edits}\n\n\\begin{table}[!t]\n    \\caption{Providers for non-terminals}\n    \\vspace{-2mm}\n    \\centering\n    \\small\n    \\begin{tabular}{l|l}\n        \\toprule\n        {\\textbf{ Component}} & \\textbf{Associated Non-terminals}   \\\\\n        \\midrule\n        Rule Predictor & \\it Edits, Edit, Insert, $\\langle \\it HLIdentifier \\rangle$, $\\langle \\textit{Any NTS in HL} \\rangle$ \\\\\n        \\midrule\n        Subtree Locator & \\it Modify  \\\\\n        \\midrule\n        Tree Copier & $\\langle \\textit{Any NTS in HL} \\rangle$  \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\label{tab:type}\n\\end{table}\n\nSince the choice of expanding a non-terminal may depend on the local context or a previous choice, we use providers to provide choices and estimate their probabilities. Our current implementation has three types of providers. Table~\\ref{tab:type} shows these providers and their associated non-terminals.\n\nFor non-terminals {\\it Edits, Edit, Insert} and $\\langle \\it HLIdentifier \\rangle$, the rule predictor is responsible for providing choices and estimates the probability of each production rule. The rule predictor consists of a neural component and a logic component. After the neural component assigns the probability for each production rule, the logic component resets the probability of rules whose left-hand side is not the corresponding non-terminal to zero and normalizes the remaining probabilities.\n\nFor {\\it Modify}, the subtree locator is responsible for providing the choices. The subtree locator estimates the probability of each AST subtree with a size larger than 1 in the faulty statement. The choice of a subtree $t$ means that we should expand {\\it Modify} into {\\tt modify({\\it ID}, {\\it NTS})} where {\\it ID} is the root ID of $t$ and {\\it NTS} is the root symbol of $t$.\n\nFor any non-terminal in the grammar of the host language (note that $\\langle \\it HLIdentifier \\rangle$ is a terminal symbol in the host language), both the rule predictor and the tree copier are responsible to provide the choices. The tree copier estimates the probabilities of each AST subtree with a size larger than 1 in the method surrounding the faulty statement. The choice of a subtree $t$ means that we should expand the non-terminal into {\\tt copy({\\it ID})}, where {\\it ID} is the root ID of $t$. Similar to the rule predictor, the tree copier employs a logic component after the neural component to reset the probabilities of subtrees whose root symbols are different from the non-terminal symbol being expanded.\n\nFinally, the decider assigns a probability to each provider. The decider also includes a similar logic component, which resets the probability of a provider to zero if that provider is not responsible for the current non-terminal symbol. For example, if the symbol being expanded is \\textit{Modify}, the decider resets the probability of rule predictor and tree copier to zero.\n\n\\vspace{-3mm}\n\\section{Model Architecture}\nThe design of our model is based on the state-of-the-art syntax-guided code generation model, TreeGen~\\cite{treegen}. It is a tree-based Transformer~\\cite{transformer} that takes a natural language description as input and produces a program as output. Since our approach takes a faulty statement and its context as input and produces edits as output, we replace the components in TreeGen for encoding natural language description and decoding the program.\n\nFigure~\\ref{fig:overview-model} shows an overview of our model. The model performs one step in the edit generation process, which is to predict probabilities of choices for expanding a non-terminal node. Beam search is used to find the best combination of choices for generating the complete edits. The model consists of four main components: \n\\begin{itemize}\n    \\item The {\\bf code reader} that encodes the faulty statement and its context.\n    \\item The {\\bf AST reader} that encodes the partial AST of the edits that have been generated.\n    \\item The {\\bf tree path reader} that encodes a path from the root node to a non-terminal node which should be expanded.\n    \\item The {\\bf edit decoder} that takes the encoded information from the previous three components and produces a probability of each choice for expanding the non-terminal node.\n\\end{itemize}\nAmong them, the AST reader and the tree path reader are derived from TreeGen, where the code reader and the edit decoder are newly introduced in this paper. In this section, we focus on describing the latter two components in detail. %Description of the AST reader and the tree path reader can be found in the paper of TreeGen~\\cite{treegen}.\n\n\\subsection{Code Reader}\nThe code reader component encodes the faulty statement and the method surrounding the faulty statement as its context, \\modify{}{where the faulty statement is localized by a fault localization technique}. It uses the following three inputs. \\textbf{(1) AST traversal sequence.} This is a sequence of tokens following the pre-order traversal of the AST, $\\bm{c}_1, \\bm{c}_2, \\cdots{}, \\bm{c}_L$, where $\\bm{c}_i$ is the token encoding vector of the $i$th node embedded via word embedding~\\cite{Mikolov2013EfficientEO}. \\textbf{(2) Tag embedding.} This is a sequence of tags following the same pre-order traversal of the AST, where each tag denotes which of the following cases the corresponding node belongs to: 1. in the faulty statement, 2. in the statement before the faulty statement, 3. in the statement after the faulty statement, or 4. in other statements. Each tag is embedded via an embedding-lookup table. We denote the tag embedding as $\\bm{t}_1,\\bm{t}_2,\\cdots{},\\bm{t}_L$. \\textbf{(3) AST-based Graph.} Considering that the former two inputs do not capture the neighbor relations between AST nodes, in order to capture such information, we treat an AST as a directional graph where the nodes are AST nodes and the edges link a node to each of its children and its left sibling, as shown in Figure~\\ref{fig:ast}(b). This graph is embedded as an adjacent matrix.\n\nThe code reader uses three sub-layers to encode the three inputs above, as discussed in the following sections.\n\n\\subsubsection{Self-Attention}\nThe self-attention sub-layer encodes the AST traversal sequence, following the Transformer~\\cite{transformer} architecture to capture the long dependency information in the AST. \n\nGiven the embedding of the input AST traversal sequence, we use position embedding to represent positional information of the AST token. The input vectors are denoted as $\\bm{c}_1, \\bm{c}_2, \\cdots{}, \\bm{c}_L$, and the position embedding of $i$th token is computed as\n        \\begin{align}\n        \\vspace{-2mm}\n                 p_{(i,2j)} &= \n                    \\sin (pos/(10000^{2j/d} )) \\\\ \n                     p_{(i,2j + 1)} &= \\cos (pos/(10000^{2j/d} )) \n        \\vspace{-3mm}\n        \\end{align}\nwhere $pos = i + step$, $j$ denotes the element of the input vector and $step$ denotes the embedding size. After we get the vector of each position, it is directly added to the corresponding input vector, where $\\bm{e}_i = \\bm{c}_i + \\bm{p}_i$.\n\nThen, we adopt multi-head attention layer to capture non-linear features. Following the definition of \\citet{transformer}, we divide the attention mechanism into $H$ heads. Each head represents an individual attention layer to extract unique information. The single attention layer maps the query $Q$, the key $K$, and the value $V$ into a weighted-sum output. The computation of the $j$th head layer can be represented as\n\\begin{equation}\n    \\vspace{-2mm}\n    head_j = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\\label{eq:att2}\n    \\vspace{-1mm}\n\\end{equation}\nwhere $d_k = d/H$ denotes the length of each extracted feature vector, and $Q$, $K$ and $V$ are computed by a fully-connected layer from $Q$, $K$, $V$. In the encoder, vectors $Q$, $K$ and $V$ are all the outputs of the position embedding layer $\\bm{e}_1, \\bm{e}_2, \\cdots, \\bm{e}_L$. The outputs of these heads are further joint together with a fully-connected layer, which is computed by\n\\begin{equation}\n    \\vspace{-1mm}\n    Out = [head_1;\\cdots;head_H]\\cdot W_h \\label{eq:att3}\n    \\vspace{-1mm}\n\\end{equation}\nwhere $W_h$ denotes the weight of the fully-connected layer and $Out$ denotes the outputs $\\bm{a_1}, \\bm{a_2}, \\cdots{}, \\bm{a_L}$ of the self-attention sub-layer. % However, these vectors still lack the positional information of the buggy statement in the buggy method. Thus, we will then describe how we address this issue via a gating layer. \n\n\\subsubsection{{Gating Layer}}\nThis sub-layer takes the outputs of the previous layer and the tag embedding as input. %To integrate the positional information of the buggy statement, we define some positional tags for the AST node. The positional tag contains four types in total, Buggy-Statement, Pre-Statement, Post-Statement, and X-Statement. After the method is parsed into an AST, we label the subtree of the buggy statement with the tag, \"Buggy-Statement\". The sub-tree of the statement, which is the last statement before the buggy statement, is labeled as \"Pre-Statement\". Similarly, the subtree of the statement, which is the next statement after the buggy statement, is labeled as \"Post-Statement\". Then, all the left AST nodes are labeled as \"X-Statement\". The tag sequence of the AST nodes can be represented as $t_1, t_2,\\cdots{},t_L$. We also embed this sequence into vectors $\\bm{t}_1, \\bm{t}_2,\\cdots{},\\bm{t}_L$ via a embedding-lookup table. \nGating mechanism, as defined in TreeGen~\\cite{treegen}, is used in this layer. It takes three vectors named $\\bm{q}, \\bm{c}_1, \\bm{c}_2$ as input and aims to corporate $c_1$ with $c_2$ based on $\\bm{q}$. The computation of gating mechanism can be represented as \n\\begin{align}\n    \\alpha_i^{c_1} = \\exp(\\bm{q}_i^T\\bm{k}_i^{c_1})&/\\sqrt{d_k}\\\\\n\\alpha_i^{c_2} = \\exp(\\bm{q}_i^T\\bm{k}_i^{c_2})&/\\sqrt{d_k}\\\\\n    \\bm{h}_i = (\\alpha_i^{c_1}\\bm{v}_i^{c_1} + \\alpha_i^{c_2}\\bm{v}_i^{c_2})&/(\\alpha_i^{c_1} + \\alpha_i^{c_2})\n    \\label{eq:gate}\n\\end{align}\nwhere $d_k = d / H$ is a normalization factor, $H$ denotes the number of heads, and $d$ denotes the hidden size; $\\bm{q}_i$ is computed by a fully-connected layer over the control vector $\\bm{q}_i$; $\\bm{k}_i^{c_1}, \\bm{v}_i^{c_1}$ is computed by another fully-connected layer over vector $\\bm{c}_1$; $\\bm{k}_i^{c_2}$ and $\\bm{v}_i^{c_2}$ are also computed by the same layer with different parameters over the vector $\\bm{c}_2$. \n\nIn our model, we treat the outputs of the self-attention sub-layer $\\bm{a}_1, \\bm{a}_2, \\cdots{}, \\bm{a}_L$ as $\\bm{q}$ and $\\bm{c}_1$, and the tag embedding $\\bm{t}_1, \\bm{t}_2, \\cdots{}, \\bm{t}_L$ as $\\bm{c}_2$. Thus, embedding of the $i$th AST node of the gating-layer can be represented as $\\bm{u}_i = \\text{Gating}(\\bm{a_i}, \\bm{a_i}, \\bm{t}_i)$. % The AST of the buggy method contains rich structral information between different AST nodes. However, the first three sub-layers fail to extract the structural information. Thus, we design a tree based convolutional layer to integrate the structure information.\n \n \\begin{figure}\n    \\centering\n    \\includegraphics[width=.95\\linewidth]{picture/ast.pdf}\n                            \\vspace{-5mm}\n    \\caption{Example of AST-based Graph}\n    \\vspace{-2mm}\n    \\label{fig:ast}\n\\end{figure}\n\n\\subsubsection{{Tree Conv Layer}}\nThis sub-layer takes the output $\\bm{u}_i$ of the previous layer and the AST-based graph $G$ (represented as an adjacency matrix) as input.\nWe adopt a GNN~\\cite{gnn08, Zhang_2020} layer to process the inputs, and the encoding of the neighbors $r_i$ is computed as \n\\begin{equation}\n    \\bm{g}_i = W_g \\sum_{r^j \\in G}A^{n}_{r^i r^j}\\bm{u}^j\n    \\vspace{-1mm}\n\\end{equation}\nwhere $W_g$ is the weight of a fully-connected layer and $\\hat{A}$ is a normalized adjacency matrix of $G$. The computation of the normal operation proposed by \\citeauthor{kipf2016semi}~\\cite{kipf2016semi} is represented as $\\hat{A} = S_1^{-1/2} A S_2^{-1/2}$,\nwhere $A$ is the adjacency matrix of $G$, and $S_1, S_2$ are the diagonal matrices with a summation of $A$ in columns and rows. Then, the encoding of the neighbors is directly added to the input vector.\n\nIn summary, the code reader has $N_1$ blocks of these three sub-layers, and yields the features of the input AST, $\\bm{t_1}, \\bm{t_2},\\cdots,\\bm{t_L}$, which would be used for the AST reader and the tree path reader.\n\\subsection{AST Reader}\n\\modify{Description of TreeGen}{\nThe AST reader encodes the partial generated AST of the edit, which has the same structure as the one in TreeGen~\\cite{treegen}. This component takes three inputs derived from the partial generated AST as code reader. The rule sequence is represented as real-value vectors and then is fed into a self-attention layer. We then integrate the output of the self-attention layer with the rule encoding via a gating layer as Equation~\\ref{eq:gate}. We also adopt a multi-head attention layer over the outputs of the code reader and the gating layer like the decoder-encoder attention in Transformer. Finally, we use a tree convolutional layer like the code reader to extract the structural information. More details of this component can be found in the publication of TreeGen~\\cite{treegen}.}\n\\subsection{Tree Path Reader}\n\\modify{Description of TreeGen}{\nThe tree path reader encodes the information of the non-terminal node to be expanded and is the same as the one in TreeGen~\\cite{treegen}. This component represents the non-terminal node as a path from the root to the node to be expanded and transforms the nodes in this path into real-value vectors. As shown in Figure~\\ref{fig:overview-model}, these vectors are fed into two attention layers like Equation~\\ref{eq:att3}. Finally, a set of two fully-connected layers, where the first layer has a $GELU$~\\cite{2016Bridging} activation function, are followed to extract features for edit decoder. More details of this component can be found in the publication of TreeGen~\\cite{treegen}. We denote the output of the tree path reader as $\\bm{d}_1, \\bm{d}_2,\\cdots{}, \\bm{d}_T$.}\n\\subsection{Edit Decoder}\nThe edit decoder takes the output of the tree path reader, $\\bm{d}_1, \\bm{d}_2,\\cdots{}, \\bm{d}_T$ with length $T$, as input. These vectors are produced by the tree path reader and contain the encoded information from all the inputs: the faulty statement with its surrounding method, the partial AST generated so far, and the tree path denoting the node to be expanded.\n\n\\subsubsection{Provider}\nAs mentioned before, there are currently three types of providers: rule predictor, tree copier, and subtree locator. These providers take the vector $\\bm{d}_1, \\bm{d}_2,\\cdots{}, \\bm{d}_T$ as input and output the probability of choices for different non-terminals.\n\\paragraph{\\textbf{Rule Predictor}}\n The rule predictor estimates the probability of each production rule in the grammar of edits. The neural component of this decider consists of a fully-connected layer. The output of the fully-connected layer is denoted as $\\bm{s}_1, \\bm{s}_2, \\cdots{}, \\bm{s}_T$. Then, these vectors are normalized via softmax, which computes the normalized vectors $\\bm p_1^r, \\bm p_2^r,\\cdots{}, \\bm p_T^r$ by\n\\begin{equation}\n    \\bm p^r_k(m) = \\frac{\\exp\\{\\bm{s}^m_k\\}}{\\sum_{j=1}^{N_r}\\exp\\{\\bm{s}_k^j\\}}\n    \\label{con:soft}\n\\end{equation}\nwhere $N_r$ denotes the number of production rules in the grammar of edits, and $m$ denotes the $m$th dimension of the vector $\\bm p^r_k$ (i.e., the production rule with ID $m$). \nIn particular, invalid rules whose left-hand side is not the corresponding non-terminal are not allowed in our approach. For these rules, the logic component resets the output of the fully-connected layer to $-\\infty$. Thus, the probability of invalid rules will be zero after softmax normalization. \n\n\\paragraph{\\textbf{Tree Copier}}\n\nThis provider is designed for any non-terminal symbol in the grammar of edits to choose a subtree in the local context. The neural component is based on a pointer network~\\cite{pointer}. The computation can be represented as \n\\begin{equation}\n    \\bm{\\theta}_i = \\bm{v}^T\\text{tanh}({W}_1\\bm{d}_i + {W}_2\\bm{t})\n    \\label{con:1}\n\\end{equation}\nwhere $\\bm{t}$ denotes the output of the code reader, and $\\bm{v}, {W}_1, {W}_2$ denote the trainable parameters. The logic component also resets $\\bm{\\theta}$ to $-\\infty$ if the root symbol of the corresponding subtree is different from the symbol being expanded. These vectors are then normalized via softmax as Equation~\\ref{con:1}. We denote the normalized vector as $\\bm p_1^t, \\bm p_2^t,\\cdots{}, \\bm p_T^t$.\n\\paragraph{\\textbf{Subtree Locator}}\nThis component outputs an ID of the subtree in the faulty statement for not-terminal symbol, \\textit{Modify}, in the grammar of edits. The computation of this component is the same as the tree copier. We denote the output vector of this provider as $\\bm p_1^s, \\bm p_2^s,\\cdots{}, \\bm p_T^s$ \n\n\\subsubsection{Decider}\nFor these three providers, the decider estimates the probability of using each provider. The neural component also takes the output of the tree path reader, $\\bm{d}_1, \\bm{d}_2,\\cdots{}, \\bm{d}_T$, as input, and produces the probability of using each provider as output. The computation can be represented as $\\bm \\lambda_i = {W}\\bm{d}_i + \\bm{b}$, where ${W}$ and $\\bm{b}$ denote the parameters of a fully-connected layer. The logic component resets $\\bm \\lambda$ to $-\\infty$ if the corresponding provider is not responsible for the symbol being expanded following Table~\\ref{tab:type}. Then, the vectors are normalized via softmax as Equation \\ref{con:soft}. We denote the normalized vectors as $\\bm \\lambda_1, \\bm \\lambda_2,\\cdots{},\\bm \\lambda_T$. The final probability of each choice can be computed as \n\\begin{equation}\n    \\bm o_i = [\\bm \\lambda_i^r \\bm p_i^r;\\bm \\lambda_i^t \\bm p_i^t;\\bm \\lambda_i^s \\bm p_i^s]\n\\end{equation}\nwhere $\\bm o_i$ will be the probability vector of the next production rule at $i$th step during patch generation.\n\n\\subsection{Training and Inference}\nDuring training, the model is optimized by maximizing the negative log-likelihood of the oracle edit sequence and \\modify{Explanation}{we do not use the logic component in the providers and decider. Here we would like Recoder to learn the distribution of the rules handled by the logic component. If the logic component is present at training, Recoder would not be trained for a large portion of rules. During inference, these unseen rules would distort the distribution of output, making Recoder fail to distinguish the part of rules that it is supposed to distinguish.}\n\nWhen generating edits, inference starts with the rule $start: \\textit{start} \\longrightarrow \\textit{Edits}$, expanding a special symbol \\texttt{start} to \\texttt{Edits}. The recursive prediction terminates if every leaf node in the predicted AST is a terminal. We use beam search with a size of 100 to generate multiple edits. %Invalid rules are excluded during beam search. \n\nGenerated edits may contain placeholders. Though the number of choices for a single placeholder is small, the combination of multiple placeholders may be large. Therefore, we discard patches containing more than one \\texttt{placeholder} symbol during beam search.\n\n\\subsection{Patch Generation and Validation}\nPatches are generated according to the result of the fault localization technique.\nIn our approach, the model described above is invoked for each suspicious faulty statement according to the result of fault localization. For each statement, we generate 100 valid patch candidates via beam search: when beam search generates a valid patch, we remove it from the search set and continue to search for the next patch until 100 candidates are generated in total for that statement. %Each candidate has a probability approximated by beam search. \nAfter patches are generated, the final step is to validate them via the test suite written by developers. The validation step filters out patches that do not compile or fail a test case. %Thus, we validate each patch from top to bottom until a patch can pass all the test cases. If all candidate patches for fixing a location cannot pass all test cases, we then select the next suspicious location to repeat patch validation. \nAll generated patches are validated until a plausible patch (a patch that passes all test cases) is found.\n\n\\section{Experiment Setup}\n\\modify{}{We have implemented \\techname for the Java programming language. In this and the next sections we report our experiments on repairing Java bugs.}\n\n\\subsection{Research Questions}\nOur evaluation aims to answer the following research questions:\n\n\\noindent\n    \\textbf{RQ1:} \\textbf{What is the performance of \\techname?}\n\nTo answer this question, we evaluated our approach on the widely used APR benchmark, \\textit{Defects4J v1.2}, and compared it with traditional and DL-based APR tools.\n\n\\noindent\n    \\textbf{RQ2:} \\textbf{What is the contribution of each component in \\techname?}\n \n To answer this question, we started from the full model of \\techname, and removed each component in turn to understand its contribution to performance. \n \n \n\\noindent\n\\textbf{RQ3:} \\textbf{What is the generalizability of \\techname?}\n \n To answer this question, we fisrt conducted an experiment on 420 additional bugs from \\textit{Defects4J v2.0}. To our best knowledge, this is the first APR approach that has been applied to this benchmark. We compared \\techname with the previous two best-performing APR approaches for single-hunk bugs on Defects4J v1.2, namely TBar~\\cite{liu2019tbar} and SimFix~\\cite{2018Shaping}. In addition, we also applied \\techname to other two benchmarks, \\textit{QuixBugs} and \\textit{IntroClassJava}, via \\textit{RepairThemAll}~\\cite{RepairThemAll2019} framework, which allows the execution of automatic program repair tools on benchmarks of bugs.\n \n    \n    \n\n\\subsection{Dataset}\nThe neural network model in our approach needs to be trained with a large number of history patches. To create this training set, we crawled Java projects created on GitHub~\\cite{github} between March 2011 and March 2018, and downloaded 1,083,185 commits where the commit message contains at least one word from the following two groups, respectively: (1) \\textit{fix, solve}; (2) \\textit{bug, issue, problem, error}. Commits were filtered to include only patches that modify one single statement or insert one new statement, corresponding to two types of edits that our approach currently supports. To avoid data leak, we further discarded patches where (1) the project is a clone to Defects4J project or a program repair project using Defects4J, or (2) the method modified by the patch is the same as the method modified by any patch in Defects4J v1.2 or v2.0, based on AST comparison. There are 103,585 valid patches left after filtering, \nwhich are further split into two parts: 80\\% for training and 20\\% for validation.\n\nWe used four benchmarks to measure the performance of \\techname. The first one contains 395 bugs from \\textit{Defects4J} v1.2~\\cite{defects4j}, which is a commonly used benchmark for automatic program repair research. The second one contains 420 additional bugs from \\textit{Defects4J v2.0}~\\cite{defects4j}. %It is a newly published benchmark, and none of the existing APR approaches has evaluated on the new version.\nDefects4J v2.0 introduces 438 new bugs compared with Defects4J v1.2. However, GZoltar~\\cite{GZoltar}, the fault localization approach used by our implementation as well as two baselines (TBar and SimFix), failed to finish on the project {\\it Gson}, so we excluded 18 bugs in {\\it Gson} from our benchmark. \\modify{}{The third one contains 40 bugs from \\textit{QuixBugs}~\\cite{quixbugs}, which is a benchmark\nwith 40 buggy algorithmic programs specified by test cases. The last one, \\textit{IntroClassJava}~\\cite{durieux:hal-01272126}, consists of 297 buggy Java programs generated from the \\textit{IntroClass}~\\cite{LeGoues15tse} benchmark for C.}\n\n\\subsection{Fault Localization}\nIn our experiment, two settings for fault localization are used. In the first setting, the faulty location of a bug is unknown to APR tools, and they rely on existing fault localization approaches to localize the bug. \\techname uses Ochiai~\\cite{ochiai} (implemented in GZoltar~\\cite{GZoltar}), which is widely used in existing APR tools~\\cite{liu2019tbar, 2018Shaping}. In the second setting, the actual faulty location is given to APR tools. This is to measure the capability of patch generation without the influence of a specific fault localization tool, as suggested and adopted in previous studies~\\citeN{coconut, codit, 9000077}.\n\n\\subsection{Baselines}\nWe selected existing APR approaches as the baselines for comparison. Since \\techname generates only single-hunk patches (patches that only change a consecutive code fragment), we chose 10 traditional single-hunk APR approaches that are often used as baselines in existing studies: {jGenProg}~\\cite{genprog}, {HDRepair}~\\cite{le2016history}, {Nopol}~\\cite{xuan2016nopol}, {CapGen}~\\cite{capgen}, {SketchFix}~\\cite{hua2018sketchfix}, {TBar}~\\cite{liu2019tbar},\n{FixMiner}~\\cite{koyuncu2020fixminer}, {SimFix}~\\cite{2018Shaping}, {PraPR}~\\cite{prapr}, {AVATAR}~\\cite{liu2019avatar}. In particular, TBar correctly repairs the highest number of bugs on Defects4J v1.2 as far as we know. We also selected DL-based APR approaches that adopt the encoder-decoder architecture to generate patches and have been evaluated on Defects4J as baselines. Four approaches have been chosen based on this criteria, namely, {SequenceR}~\\cite{9000077}, {CODIT}~\\cite{codit}, {DLFix}~\\cite{dlfix}, and {CoCoNuT}~\\cite{coconut}. \n\nFor Defects4J v1.2, the performance data of the baselines are collected from existing papers~\\cite{liu2019tbar, liu2020efficiency}.\nFor additional bugs from Defects4J v2.0, two best-performing single-hunk APR approaches on Defects4J v1.2, TBar and SimFix, are adapted and executed for comparison. \\modify{}{For QuixBugs and IntroClassJava, we directly choosed the APR tools used in RepairThemAll~\\cite{RepairThemAll2019} and DL-based APR tools which have experimented on these two benchmarks as baselines: jGenProg~\\cite{genprog}, RSRepair~\\cite{PatchPlausibility}, Nopol~\\cite{xuan2016nopol}, and CoCoNuT~\\cite{coconut}. We also directly used the result reported in the original papers~\\cite{RepairThemAll2019, coconut}}.\n\n\\subsection{Correctness of Patches}\nTo check the correctness of the patches, we manually examined every patch if it is the same with or semantically equivalent to the patch provided by Defects4J, as in previous works~\\cite{liu2019tbar, 2018Shaping, dlfix, coconut, prapr}. To reduce possible errors made in this process, every patch is examined by two of the authors individually and is considered correct only if both authors consider it correct. The kappa score of the experiment is 0.98.\nFurthermore, we also publish all the patches generated by \\techname for public judgment\\footnote{The source code of \\techname, generated patches, and an online demo are available at https://github.com/pkuzqh/Recoder}.\n\n\\subsection{Implementation Details}\nOur approach is implemented based on PyTorch~\\cite{pytorch}, with parameters set to $N_1 = 5, N_2 = 9, N_3 = 2$, i.e., the code reader contains a stack of 5 blocks, the AST reader contains a stack of 9 blocks, and the decoder contains a stack of 2 blocks, respectively. Embedding sizes for all embedding vectors are set to 256, and all hidden sizes are set following the configuration of TreeGen~\\cite{treegen}. During training, dropout~\\cite{dropout} is used to prevent overfitting, with the drop rate of 0.1. The model is optimized by Adam~\\cite{adam} with learning rate 0.0001. These hyper-parameters and parameters for our model are chosen based on the performance on validation set. \n\nWe set a 5-hour running-time limit for \\techname, following existing studies~\\cite{2018Shaping, dlfix, coconut,saha2019harnessing}. \n\n\\begin{table*}\n\\caption{Comparison without Perfect Fault Localization}                            \\vspace{-2mm}\n\n\\begin{threeparttable}\n  \\begin{tabular}{l|c|c|c|c|c|c|c|c|c|\n  c|c|c}\n    \\toprule\n    Project&jGenProg&HDRepair&Nopol&CapGen&SketchFix&FixMiner&SimFix&TBar&DLFix&PraPR&AVATAR&\\techname\\\\\n    \\midrule\n    Chart & 0/7 & 0/2 & 1/6 & 4/4 & 6/8&5/8&4/8&\\textbf{9/14}&5/12&4/14&5/12&8/14\\\\\n    Closure & 0/0 & 0/7 & 0/0 & 0/0 & 3/5 & 5/5 & 6/8 & 8/12 & 6/10 &12/62&8/12& \\textbf{15/31}\\\\\n    Lang & 0/0 & 2/6 & 3/7 & 5/5 & 3/4 & 2/3 & \\textbf{9/13}&5/14&5/12& 3/19 &{5/11}&\\textbf{9/15}\\\\\n    Math & 5/18 & 4/7 & 1/21 & 12/16 & 7/8 & 12/14 & 14/26 & \\textbf{18/36} & 12/28 &6/40&{6/13}& {15/30}\\\\\n    Time & 0/2 & 0/1 & 0/1 & 0/0 & 0/1 & 1/1 & 1/1 & 1/3 & 1/2&0/7& {1/3}&\\textbf{2/2}\\\\\n    Mockito & 0/0 & 0/0 & 0/0 & 0/0& 0/0 & 0/0 & 0/0&1/2 & 1/1 &1/6&\\textbf{2/2} & \\textbf{2/2}\\\\\n    \\midrule\n    Total & 5/27 & 6/23 & 5/35 & 21/25 & 19/26& 25/31 & 34/56 & 42/81 & 30/65&26/148&27/53 & \\textbf{51/94}\\\\\n    \\midrule\n    P(\\%) & 18.5 & 26.1 & 14.3 & \\textbf{84.0} & 73.1 & 80.6 & 60.7 & 51.9 & 46.2 &17.6&50.9& 54.3\\\\\n  \\bottomrule  \n\\end{tabular}\n \\begin{tablenotes}\n \\footnotesize\n \\item In the cells, x/y:x denotes the number of correct patches, and y denotes the number of patches that can pass all the test cases.\n \\end{tablenotes}\n\\end{threeparttable}\n\\label{tab:result}\n\\end{table*}\n\n\\begin{table}\n\\caption{Comparison with Perfect Fault Localization}\\vspace{-2mm}\n  \\resizebox{\\linewidth}{!}{\n  \\begin{tabular}{l|c|c|c|c|c|c}\n    \\toprule\n    Project&SequenceR&CODIT&DLFix&CoCoNuT&TBar&\\techname\\\\\n    \\midrule\n    Chart & 3 & 4 & 5 & 7&\\textbf{11} & {10}\\\\\n    Closure & 3 & 3 & 11 & 9&17 & \\textbf{23}\\\\\n    Lang & 3 & 3 & 8 & 7&\\textbf{13} & 10\\\\\n    Math & 4 & 6 & 13 & 16&\\textbf{22} & 18\\\\\n    Time & 0 & 0 & 2 & 1& 2& \\textbf{3} \\\\\n    Mockito & 0 & 0 & 1& \\textbf{4} & 3 & {2}\\\\\n    \\midrule\n        Total & 13 & 16 & 40 & 44 & \\textbf{68}&{66}\\\\\n  \\bottomrule  \n\\end{tabular}\n}\n\\label{tab:result2}\n\\end{table}\n\n\\section{Experimental Results}\n\\subsection{Performance of \\techname(RQ1)}\n\\subsubsection{Results without Perfect Fault Localization}\nWe first compare \\techname with the baselines in the setting where no faulty location is given. Results as Table~\\ref{tab:result} shown only include baselines that have been evaluated under this setting. As shown, \\techname correctly repairs 51 bugs and outperforms all of the previous single-hunk APR techniques on Defects4J v1.2. In particular, \\techname repairs 21.4\\% (9 bugs) more bugs than the previous state-of-the-art APR tool for single-hunk bugs, TBar. Within our knowledge, \\techname is the first DL-based APR approach that has outperformed the traditional APR approaches. \n\nWe show a few example patches that are possibly generated with the help of the novel techniques in \\techname.\nAs shown in Figure~\\ref{fig:unique2}, Chart-8 is a bug that DLFix fails to fix. The correct patch only changes a parameter of the method invocation while DLFix needs to generate the whole expression. %Thus, \\techname adopts the modify operation defined in the extended grammar over the expression to fix the defect.\nBy contrast, \\techname generates a {\\it modify} operation that changes only one parameter.\nFigure~\\ref{fig:unique} shows a bug only repaired by \\techname. This patch relies on a project-specific method, ``\\texttt{isNoType}'', and thus cannot be generated by many of the existing approaches. However, \\techname  %Thus, the previous approaches cannot generate the corresponding method invocation based on the given context. Owing to the placeholder generation, \\techname \nfixes it correctly by generating a placeholder and then instantiating it with ``\\texttt{isNoType}''. \n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{picture/chart8.pdf}\n                            \\vspace{-7mm}\n    \\caption{Chart-8 - A bug fixed by \\techname with Modify operation}\n    \\vspace{-2mm}\n    \\label{fig:unique2}\n\\end{figure}\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=\\linewidth]{picture/closure104.pdf}\n                            \\vspace{-7mm}      \n\n  \\caption{Closure-104 - A bug fixed by \\techname with \\textit{placeholder} generation}\n    \\vspace{-2mm}\n  \\label{fig:unique}\n\\end{figure}\n\\vspace{-2mm}\n\\subsubsection{Results with Perfect Fault Localization}\nTable~\\ref{tab:result2} shows the result where the actual faulty location is provided. %of the comparative experiment based on correct fault localization.\nAs before, only baselines that have been evaluated under this setting are listed.\n\\techname still outperforms all of the existing APR approaches, including traditional ones. %Furthermore, \\techname fixed 71 bugs on Defects4J v1.2, which has a 61.3\\% improvement over the existing DL-based state-of-the-art approach, CoCoNut. \nAlso, compared with \\techname using Ochiai for fault localization, this model achieves a 35.3\\% improvement. The result implies that \\techname can achieve better performance with better fault localization techniques.\n\n\\subsubsection{Degree of Complementary}%Qualitative Analysis for \\techname}\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{picture/overlap.jpg}      \n    {\\footnotesize Project Names: C:Chart, CL:Closure, L:Lang, M:Math, Moc:Mockito, T:Time}                            \n    \\vspace{-3mm}\n    \\caption{Degree of Complementary.}\n    \\vspace{-6mm}\n    \\label{fig:overlap}\n\\end{figure}\n\nWe further investigate to what extent \\techname complements the three best-performing existing approaches for fixing single-hunk bugs, TBar, SimFix, and DLFix. Figure~\\ref{fig:overlap} reveals the overlaps of the bugs fixed by different approaches.\nAs shown, \\techname fixes 19 unique bugs when compared with three baselines. Moreover, \\techname fixes 34, 28, 27 unique bugs compared with SimFix, TBar, and DLFix, respectively. %This result shows the effectiveness of our approach. In particular, integrated with TBar, \\techname can fix 69 bugs on Defects4J v1.2 in total. These results prove the effectiveness of \\techname.\nThis result shows that \\techname is complementary to these best-performing existing approaches for single-hunk bugs.\n\\tcbset{colframe = black}\n\\subsection{Contribution of Each Component (RQ2)}\nTo answer RQ2, we conducted an ablation test on Defects4J v1.2 to figure out the contribution of each component. Since the ablation test requires much time, we only conducted the experiment based on Ochiai Fault Localization scenario.%, which aims to understand the contribution of each component more clear. \n\n\\begin{table}\n\\caption{Ablation Test for \\techname on Defects4J v1.2}\\vspace{-2mm}\n\\resizebox{\\linewidth}{!}\n{ \n  \\begin{tabular}{l|c|c|c|c|c}\n    \\toprule\n    Project&\\tt -modify&\\tt -subtreecopy&\\tt -insert&\\tt -placeholder&\\techname\\\\\n    \\midrule\n    Chart & 4 & 6 & 7 & 8 & 8\\\\\n    Closure & 6 & 12 & 12 & 11 & 15\\\\\n    Lang & 3 & 6 & 5 & 5 & 9\\\\\n    Math & 7 & 8 & 9 & 9 & 15\\\\\n    Time & 1 & 1 & 1 & 1 & 2 \\\\\n    Mockito & 2 & 1 & 1 & 1 & 2\\\\\n    \\midrule\n        Total & 23 & 34 & 35 & 35 & 51\\\\\n  \\bottomrule  \n\\end{tabular}\n}\n\\label{tab:result3}\n\\end{table}\n\nTable~\\ref{tab:result3} shows the results of the ablation test. We respectively removed three edit operations, {\\tt modify}, {\\tt copy}, and {\\tt insert}, as well as the generation of placeholders. As shown in the table, removing any of the components leads to a significant drop in performance. This result suggests that the two novel techniques proposed in \\techname are the key to its performance.\n\n\\subsection{Generalizability of \\techname (RQ3)}\n\n\\begin{table}\n\\vspace{-3mm}\n\\caption{Comparison on the 420 additional bugs}\\vspace{-2mm}\n\\label{tab:rede2}\n\\resizebox{\\linewidth}{!}{\n  \\begin{tabular}{l|c|c|c|c|c}\n    \\toprule\n    Project& \\# Used Bugs & Bug IDs & TBar & SimFix & \\techname\\\\\n    \\midrule\n    Cli & 39 & 1-5,7-40& 1/7 & 0/4 & \\textbf{3/3}\\\\\n    Clousre & 43 &134 - 176&0/5&\\textbf{1/5}&0/7\\\\\n    JacksonDatabind&112&1-112&0/0&0/0&0/0\\\\\n    Codec &18 & 1-18& \\textbf{2/6} & 0/2 & \\textbf{2/2}\\\\\n    Collections& 4 & 25-28& 0/1 &0/1 & 0/0 \\\\\n    Compress & 47 & 1-47 & 1/13 &0/6 & \\textbf{3/9}\\\\\n    Csv& 16 & 1-16  & 1/5 &0/2  & \\textbf{4/4} \\\\\n    JacksonCore& 26 & 1-26 & 0/6 &0/0 & 0/4 \\\\\n    Jsoup& 93 & 1-93 & 3/7 & 1/5 & \\textbf{7/13}\\\\\n    JxPath& 22 & 1-22 & 0/0 &0/0 & 0/4\\\\\n    \\midrule\n        Total& 420 & - & 8/50 &2/25 & \\textbf{19/46}\\\\\n  \\bottomrule  \n\\end{tabular}\n}\n\\end{table}\n\\modify{Additional Experiment on RepairThemAll}{The results on Defects4J v2.0, QuixBugs and IntroClassJava are shown in Table~\\ref{tab:rede2} and Table~\\ref{tab:rede3}.} As shown, on Defects4J v2.0, all three approaches repair a smaller proportion of bugs, suggesting that the additional bugs on Defects4J v2.0 are probably more difficult to repair. Nevertheless, \\techname still repairs most bugs compared with baselines, 19 in total, achieving 137.5\\% (11 bugs) improvement over TBar and 850.0\\% (17 bugs) improvement over SimFix. We believe that the considerable performance drops of TBar and SimFix are caused by their design: TBar is based on validated patterns on Defects4J v1.2, which may not generalize beyond the projects in Defects4J v1.2; SimFix relies on similar code snippets in the same project, but new projects in Defects4J v2.0 are much smaller, and thus the chance to find similar code snippets become smaller. On the other hand, \\techname is trained from a large set of patches collected from different projects and is thus more likely to generalize to new projects. \\modify{Additional Experiment on RepairThemAll}{On QuixBugs and IntroClassJava, \\techname also repaired 775\\% (31 bugs) and 30.8\\% (4 bugs) more bugs on IntroClassJava and QuixBugs over the baselines respectively, further confirming the effectiveness and generalizability of \\techname.}\n\\begin{table}\n\\caption{Comparison on IntroClassJava and QuixBugs}\\vspace{-2mm}\n\\label{tab:rede3}\n\\resizebox{\\linewidth}{!}{\n  \\begin{tabular}{l|c|c|c|c|c|c}\n    \\toprule\n    Project& \\# Used Bugs &jGenProg&RSRepair&Nopol&CoCoNuT & \\techname\\\\\n    \\midrule\n    IntroClassJava & 297 & 1/4 &4/22&3/32&- & \\textbf{35/56}\\\\\n    QuixBugs & 40 &0/3&2/4&1/4&13/20&\\textbf{17/17}\\\\\n    \\midrule\n        Total& 337 & 1/7 & 6/26 &4/36 &13/20& \\textbf{52/73}\\\\\n  \\bottomrule  \n\\end{tabular}\n}\n\\end{table}\n\\section{DISCUSSION}\n\\subsection{Adequacy of Dataset}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{picture/tradeoff.jpg}\n    \\vspace{-7mm}\n    \\caption{Adequacy of the Dataset.}\n    \\vspace{-2mm}\n    \\label{fig:tradeoff}\n\\end{figure}\n\\modify{tradeoff}{\nTo understand the adequacy of our training data, we trained \\techname on differently sized subsets of the original training dataset and calculate the loss over the single-hunk bugs in the Defects4J v1.2 dataset. For each subset, we train 5 models with different random seeds and report the average performance of these models.\nFigure~\\ref{fig:tradeoff} shows the sensitivity analysis of dataset size for \\techname. As shown, the loss and the diversity both decrease with the increase of training subset size. %For \\techname, more training data are beneficial to improving the performance. \nTherefore, the performance of \\techname may further increase if more training data are provided.\n}\n\\subsection{Limitations of \\techname}\n\\modify{limitations of \\techname}{\nRecoder shares limitations common to most APR approaches: 1) Bugs to be fixed should be reproducible by failing test cases. 2) Effective fault localization is needed to identify the faulty statement. Recoder also shares the limitations common to DL-based approaches: The performance would degrade if the training set and the testing set have different distributions.}\n\\section{Related Work}\n\\paragraph{\\textbf {DL-based APR Approaches}} APR has been approached using different techniques, such as heuristics or random search~\\cite{genprog,6227211,10.1145/2931037.2948705,10.1145/2568225.2568254}, semantic analysis~\\cite{liu2019avatar, mechtaev2015directfix,mechtaev2016angelix,mechtaev2018symbolic, gao2021beyond,chen2017contract,hua2018sketchfix,kaleeswaran2014minthint}, manually defined or automatically mined repair patterns~\\cite{6606623,2018Shaping,koyuncu2020fixminer,long2017automatic,jiang2019inferring,ghanbari2019practical,bader2019getafix,rolim2017learning}, and learning from source code~\\cite{2018Shaping,capgen,xiong2017precise,xiong2018learning,xin2017leveraging,long2016automatic}. For a thorough discussion of APR, existing surveys~\\cite{8089448,monperrus:hal-01956501,goues2019automated,Monperrus2015} are recommended to readers.\n\nA closely related series of work is APR based on deep learning. The mainstream approaches treat APR as a statistical machine translation that generates the fixed code with faulty code. DeepFix~\\cite{10.5555/3298239.3298436} learns the syntax rules via a sequence-to-sequence model to fix syntax errors. \\citet{6606623} and \\citet{9000077} also adopt a sequence-to-sequence translation model to generate the patch. They use sequence-to-sequence NMT with a copy mechanism. \\citet{codit} propose CODIT, which learns code edits by encoding code structures in an NMT model to generate patches. \\citet{dlfix} propose a Tree-LSTM to encode the abstract syntax tree of the faulty method to generate the patches. CoCoNuT, as proposed by \\citet{coconut}, adopts CNN to encode the faulty method and generates the patch token by token. Compared to them, our paper is the first work that aims to improve the decoder and employs a syntax-guided manner to generate edits, with the provider/decider architecture and placeholder generation. \n\n\\paragraph{\\textbf {Multi-Hunk APR}} Most of the above approaches generate single-hunk patches---patches that change one place of the program. Recently, \\citet{saha2019harnessing} propose Hercules to repair multi-hunk bugs by discovering similar code snippets and applying similar changes. Since lifting single-hunk repair to multiple-hunk repair is a generic idea and can also be applied to \\techname, we did not directly compare \\techname with the multi-hunk repair tools in our evaluation. Nevertheless, though \\techname only repairs single-hunk bugs, we notice that it still outperforms Hercules by repairing 5 more bugs on the Defects4J v1.0 (including all projects from v1.2 except for Mockito), the dataset Hercules has been evaluated on.\n\n\\paragraph{\\textbf{DL-based Code Generation}} Code generation aims to generate code from a natural language specification and has been intensively studied during recent years. With the development of deep learning, \\citet{ling2016latent} propose a neural machine translation model to generate the program token by token. Being aware that code has the constraints of grammar rules and is different from natural language, \\citet{DBLP:conf/acl/YinN17} and \\citet{DBLP:conf/acl/RabinovichSK17} propose to generate the AST of the program via expanding from a start node. To integrate the semantic of identifiers, OCoR~\\cite{ocor} proposes to encode the identifiers at character level. To alleviate the long dependency problem, a CNN decoder~\\cite{DBLP:conf/aaai/SunZMXLZ19} and TreeGen (a tree-based Transformer)~\\cite{treegen} are proposed to generate the program. In this paper, we significantly extend TreeGen to generate the edit sequence for program repair.\n\n\\paragraph{\\textbf{DL-based Code Edit Generation}}\nSeveral existing DL-based approaches also use the idea of generating edits on programs~\\cite{yasunaga2020graphbased,DBLP:journals/corr/abs-1911-01205,structuralChange,Dinella2020HOPPITY, cao2020}. \\citet{DBLP:journals/corr/abs-1911-01205} view a program as a sequence of tokens and generate a sequence of token-editing commands. \\citet{structuralChange} view a program as a tree and generate node-editing or subtree-editing commands. \\citet{Dinella2020HOPPITY} view a program as a graph and generate node-editing commands. Compared with our approach, there are three major differences. First, the existing approaches are not syntax-guided and treat an edit script as a sequence of tokens. As a result, they may generate syntactically incorrect edit scripts and do not ensure syntactic correctness of the edited program. On the other hand, our approach introduces the provider/decider architecture and successfully realizes the syntax-guided generation for edits. Second, none of the existing approaches support placeholder generation, and thus are ineffective in generating edits with project-specific identifiers. Third, the editing commands they use are atomic and are inefficient in representing large changes. For example, to insert a variable declaration, in our approach there is one {\\tt insert} operation: {\\tt insert(int var = 0;)}. However, the existing approaches have to represent this change as a sequence of 5 insertions, where each insertion inserts one token. \n\n\\section{THREATS TO VALIDITY}\n\\paragraph{\\textbf{Threats to external validity}} mainly lie in the evaluation dataset we used. First, though our approach applies to different programming languages, so far, we have only implemented and evaluated it on Java, so future work is needed to understand its performance on other programming languages. Second, though we have evaluated on Defects4J v2.0, QuixBugs, and IntroClassJava, it is yet unknown how our approach generalizes to different datasets~\\cite{JiangLNZH21}. This is a future work to be explored. \n\\paragraph{\\textbf{Threats to internal validity}} mainly lie in our manual assessment of patch correctness. To reduce this threat, two authors have independently checked the correctness of the patches, and a patch is considered correct only if both authors consider it correct. The generated patches also have been released for public assessment.\n\n\\section{CONCLUSION}\n\\modify{}{In this paper, we propose \\techname, a syntax-guided edit decoder with placeholder generation for automated program repair. %We first define a extended grammar and adopt AST-based generation to predict the edits. We also introduce placeholder generation to predict the locally-defined identifiers. \n\\techname uses a novel provider/decider architecture to ensure accurate generation and syntactic correctness of the edited program and generates placeholders for project-specific identifiers.\nIn the experiment, \\techname achieved 21.4\\% improvement (9 bugs) over the existing state-of-the-art APR approach for single-hunk bugs on Defects4J v1.2. Importantly, \\techname is the first DL-based APR approach that has outperformed traditional APR techniques on this benchmark. Further evaluation on three other benchmarks shows that \\techname has better generalizability than some state-of-the-art APR approaches.}\n\n\\section*{ACKNOWLEDGMENTS}\n\n\\modify{acknowledgment}{This work is sponsored by the National Key Research and Development Program of China under Grant No. 2017YFB1001803, National Natural Science Foundation of China under Grant Nos. 61922003, and a grant from ZTE-PKU Joint Laboratory for Foundation Software.}\n\n\\balance\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-1811.04211v1.tex",
        "arXiv-2105.04123v3.tex",
        "arXiv-2106.08253v6.tex"
    ],
    "group_id": "group_71",
    "response": "### Title: Advances in Automatic Program Repair Using Machine Learning Techniques\n\n### Introduction\nAutomatic Program Repair (APR) is a burgeoning field in software engineering that aims to reduce the manual effort required for fixing bugs in programs. APR techniques have evolved from traditional methods such as heuristics and random search to more sophisticated approaches like semantic analysis and deep learning. The history of APR can be traced back to early work on fault localization and pattern-based repair, which laid the groundwork for modern approaches. Fault localization techniques identify suspicious code locations, while pattern-based repair methods leverage historical bug fixes to suggest potential patches. However, these traditional methods often struggle with the complexity and diversity of real-world bugs, leading to a high rate of incorrect or non-compilable patches.\n\nRecent advancements in deep learning have introduced new paradigms for APR, particularly neural machine translation (NMT) and program synthesis. NMT-based APR approaches treat bug fixing as a translation problem from faulty code to correct code, using large datasets of historical bug fixes to train models. Program synthesis techniques, on the other hand, generate patches by encoding repair constraints into satisfiability problems (SMT) and solving them. These methods have shown promise in generating syntactically correct patches, but they still face significant challenges, such as low compilability rates and the inability to handle complex patches.\n\nThis summary focuses on three recent papers that explore different aspects of APR using machine learning techniques. The first paper, \"Nopol: Automatic Repair of Conditional Statement Bugs in Java Programs,\" proposes a method to automatically repair bugs in conditional statements (if-then-else) and missing preconditions. The second paper, \"Neural Program Repair with Execution-based Backpropagation,\" introduces RewardRepair, a neural program repair model that uses execution-based backpropagation to improve patch quality. The third paper, \"A Syntax-Guided Edit Decoder for Neural Program Repair,\" presents Recoder, a novel edit decoder that generates patches more efficiently and accurately by leveraging placeholders for project-specific identifiers.\n\n### Main Content of Each Paper\n\n#### Nopol: Automatic Repair of Conditional Statement Bugs in Java Programs\nNopol is an automatic repair system designed specifically for fixing bugs in conditional statements and missing preconditions in Java programs. The system operates in three major phases: fault localization, runtime trace collection, and patch synthesis. In the fault localization phase, Nopol ranks statements based on their suspiciousness using the Ochiai metric. The runtime trace collection phase gathers variable values and object-oriented features during test execution. Finally, the patch synthesis phase translates these collected data into an SMT problem and generates a feasible patch solution. Nopol evaluates its approach on a dataset of 22 real-world bugs, demonstrating that it can fix 17 of these bugs, including four bugs with manually added test cases. The system is publicly available for further replication and research.\n\nThe main contributions of Nopol include:\n1. A repair approach for conditional statement bugs.\n2. Techniques for angelic fix localization and SMT encoding.\n3. An evaluation on a dataset of real-world bugs.\n4. Analysis of the repair process with respect to fault localization.\n\n#### Neural Program Repair with Execution-based Backpropagation\nRewardRepair is a novel neural program repair model that addresses the limitations of existing NMT-based APR approaches by incorporating execution-based backpropagation. The model is trained in two phases: syntactic training and semantic training. Syntactic training uses cross-entropy loss to optimize the model based on a large corpus of buggy and fixed code pairs. Semantic training employs a discriminative model to assess the quality of generated patches based on compilation and test execution information, providing a reward signal to modulate the cross-entropy loss during backpropagation. This approach ensures that the generated patches are not only syntactically close to the ground truth but also compilable and semantically correct.\n\nRewardRepair's key innovations include:\n1. A discriminative model that evaluates patches based on execution information.\n2. A mixed learning objective combining syntactic and semantic training.\n3. The use of placeholders for project-specific identifiers.\n\nThe model is evaluated on four widely used benchmarks: Defects4J v1.2, Defects4J v2.0, Bugs.jar, and QuixBugs. RewardRepair correctly repairs 207 bugs across these benchmarks, outperforming the state-of-the-art in terms of compilability and correctness. It uniquely repairs 121 bugs that were never fixed by any other APR approaches in the literature.\n\n#### A Syntax-Guided Edit Decoder for Neural Program Repair\nRecoder is a DL-based APR approach that introduces a syntax-guided edit decoder with placeholder generation. Unlike existing approaches that generate a sequence of tokens to replace faulty code, Recoder generates edits that can be more efficiently represented and are guaranteed to be syntactically correct. The provider/decider architecture ensures that the generated edits adhere to the language's syntax and semantics. Additionally, Recoder generates placeholders for project-specific identifiers, which are then instantiated with feasible identifiers during patch validation.\n\nRecoder's main contributions are:\n1. A syntax-guided edit decoder that ensures syntactic correctness.\n2. Placeholder generation for project-specific identifiers.\n3. An encoder-decoder architecture that supports both insert and modify operations.\n\nRecoder is evaluated on four benchmarks: Defects4J v1.2, Defects4J v2.0, IntroClassJava, and QuixBugs. It repairs 51 bugs on Defects4J v1.2, achieving a 21.4% improvement over the previous state-of-the-art (TBar). On Defects4J v2.0, it repairs 19 bugs, which is 137.5% more than TBar and 850% more than SimFix. Recoder also outperforms existing APR tools on IntroClassJava and QuixBugs, demonstrating better generalizability.\n\n### Commonalities and Innovations\n\n#### Commonalities\nAll three papers focus on improving the quality and efficiency of automatic program repair using machine learning techniques. They share the goal of generating patches that are both compilable and semantically correct. Each approach leverages existing bug fix data to train models that can generate patches for new bugs. Furthermore, all three papers employ fault localization techniques to identify suspicious code locations, and they use test cases to validate the generated patches.\n\n#### Innovations\n- **Nopol** introduces angelic fix localization and SMT encoding to handle complex object-oriented features, such as nullness checks and method calls.\n- **RewardRepair** uses a discriminative model to provide execution-based feedback during training, modulating the cross-entropy loss to encourage compilable and semantically correct patches.\n- **Recoder** proposes a syntax-guided edit decoder that ensures syntactic correctness and generates placeholders for project-specific identifiers, making it more flexible and generalizable.\n\n### Comparison of Results and Discussion\nThe three papers present different performance metrics and benchmarks to evaluate their APR approaches. Nopol's evaluation is based on a dataset of 22 real-world bugs, demonstrating that it can fix 17 bugs, including four bugs with manually added test cases. RewardRepair repairs 207 bugs across four benchmarks, uniquely fixing 121 bugs that were never repaired before. Recoder repairs 51 bugs on Defects4J v1.2, outperforming traditional APR approaches, and uniquely repairs 19 bugs on Defects4J v2.0.\n\nThe differences in performance can be attributed to several factors:\n1. **Dataset and Benchmark Differences**: Nopol focuses on a specific type of bugs (conditional statements and preconditions) in two open-source Java projects, while RewardRepair and Recoder are evaluated on a broader range of bugs and projects.\n2. **Training and Inference Strategies**: RewardRepair uses a mixed learning objective with syntactic and semantic training, whereas Recoder introduces a novel edit decoder architecture and placeholder generation.\n3. **Execution-based Feedback**: RewardRepair incorporates execution-based feedback during training, which is not present in Nopol and Recoder. This feedback helps in generating patches that are not only syntactically close but also compilable and semantically correct.\n\n### Conclusion\nThe three papers demonstrate significant advancements in automatic program repair using machine learning techniques. Nopol provides a scalable and effective approach for handling conditional statement bugs and missing preconditions in Java programs. RewardRepair introduces execution-based backpropagation to improve the compilability and correctness of generated patches, while Recoder employs a syntax-guided edit decoder to ensure syntactic correctness and generate project-specific identifiers.\n\nThe main findings from these papers are:\n1. **Nopol** can fix 17 out of 22 real-world bugs, highlighting the effectiveness of angelic fix localization and SMT encoding.\n2. **RewardRepair** repairs 207 bugs across four benchmarks, uniquely fixing 121 bugs, showcasing the benefits of incorporating execution feedback during training.\n3. **Recoder** outperforms traditional APR approaches on Defects4J v1.2 and uniquely repairs 19 bugs on Defects4J v2.0, demonstrating better generalizability.\n\nFuture research in APR should focus on:\n1. **Improving Fault Localization**: Enhancing fault localization techniques to better identify the root cause of bugs.\n2. **Handling Complex Patches**: Developing methods to handle complex patches that involve multiple statements or non-trivial expressions.\n3. **Generalizability**: Evaluating APR approaches on a wider range of benchmarks and programming languages to understand their generalizability.\n4. **Execution-based Feedback**: Incorporating more sophisticated execution-based feedback into training to improve the quality of generated patches.\n\n### Acknowledgment\nThis summary is based on the contributions of multiple researchers and the advancements in the field of automatic program repair. The authors of the papers have provided valuable insights into the challenges and potential solutions for APR, and their work has significantly advanced the state-of-the-art in this area."
}