{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Generalized Preference Optimization:\nA Unified Approach to Offline Alignment}\n\n\\begin{document}\n\n\\maketitle\n\n\\section{Introduction}\n\nReinforcement learning from human feedback (RLHF) has been a canonical paradigm for aligning powerful AI systems along human values \\citep{christiano2017deep,ouyang2022training}, as demonstrated by recent advances in large language models (LLMs) \\citep{achiam2023gpt,team2023gemini}. RLHF consists of two steps: reward modeling, which trains a reward model $r_\\phi$ to capture human preferences from a dataset of pairwise comparison; and regularized policy optimization, which aligns the AI systems against the learned reward model, more formally as below \n\\begin{align*}\n    \\max_\\theta  \\underbrace{\\mathbb{E}_{y\\sim\\pi_\\theta}\\left[r_\\phi(y)\\right]}_{\\text{reward maximization}} - \\beta \\underbrace{\\mathbb{KL}(\\pi_\\theta,\\pi_\\text{ref})}_{\\text{regularization}}.\n\\end{align*}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.48\\textwidth]{plots/loss.pdf}\n    \\caption{\\small{Illustration of offline preference optimization losses $\\mathbb{E}_{(y_w,y_l)\\sim\\mu}\\left[f\\left(\\rho_\\theta\\right)\\right]$ as a function of the difference of log ratio $\\rho_\\theta=\\log \\pi_\\theta(y_w) / \\pi_\\text{ref}(y_w) - \\log \\pi_\\theta(y_l)/ \\pi_\\text{ref}(y_l)$. DPO  applies the (scaled) logistic loss $\\frac{1}{\\log 2}\\log(1+\\exp(-\\rho_\\theta))$, SLiC applies the hinge loss $\\max(0,1-\\rho_\\theta)$, while IPO applies the squared loss $(\\rho_\\theta-1)^2$. As a result, many popular offline losses can be understood as convex approximations to the 0-1 loss that measures the binary classification accuracy. Any other convex loss alternatives to the above examples provide offline preference optimization losses not in the existing literature, as we show in Table~\\ref{table:losses}.}}\n    \\label{fig:loss}\n\\end{figure}\n\nLately, directly aligning AI systems from pairwise comparison datasets has become increasingly common (e.g., \\citealp{rafailov2023direct,azar2023general,zhao2023SLiC}), as evidenced by progress in open source models (e.g., \\citealp{jiang2024mixtral}). Compared to canonical RL algorithms, such methods are more computationally efficient as they do not require expensive sampling from the models. They also avoid learning reward models altogether, and effectively replace RLHF with a supervised learning problem, which is convenient from various practical perspectives. We refer to such methods as \\emph{offline preference optimization}, as they seek to optimize human preferences using offline datasets. Here, \\emph{offline} stresses the fact that such datasets are not generated by interactive data collections from the learned model.\n\nOur first contribution is to provide a unifying view over notable existing offline preference optimization algorithms, such as DPO \\citep{rafailov2023direct}, IPO \\citep{azar2023general} and SLiC \\citep{zhao2023SLiC}. To this end, we propose GPO (Generalized Preference Optimization),\nwhich parameterizes preference optimization losses via a family of convex functions $f$, with DPO, IPO, and SLiC as special cases (see Figure~\\ref{fig:loss} for a preview of the instantiations). The central insight to our derivation is that one can interpret the problem of reward modeling as a supervised binary classification problem \\citep{hastie2009elements}. The rich literature on supervised binary classification paves the way to unifying existing offline preference optimization algorithms, and naturally introduces new algorithms not yet in the current literature. The GPO formulation also helps better understand the algorithmic trade-offs between different variants, particularly, the strength of regularization, which we further dive into.\n\nWith a unifying view over offline preference optimization algorithms, our second contribution is to dive into the regularization mechanism induced by offline losses. We see that the tail behavior of the convex function $f$, governs the effective strength of regularization induced between $\\pi_\\theta$ and $\\pi_\\text{ref}$, which offers insight on the choice of hyper-parameters such as $\\beta$. We identify the offline regularization, computed based on the offline dataset, and show how it generally differs from the KL divergence intended in the initial formulation. Our analysis and empirical results hint at some challenges to enforcing the KL divergence constraints with offline losses, revealing some of the subtleties of the `equivalence' arguments adopted in prior work to derive offline losses (see also Theorem~\\ref{thm:equivalence} for a more general version of the equivalence argument). \n\nThe paper is organized as follows:\n\\begin{itemize}\n    \\item In Section~\\ref{sec:gpo}, we present GPO, generalized policy optimization, which parameterizes offline preference optimization algorithms through a convex function. This recovers a few popular algorithms as special cases and offers insights to offline alignment algorithms in general.\n    \\item In Section~\\ref{sec:derivation}, we expand on the derivation of reward modeling as a binary classification problem. Our insight allows for connecting a rich literature on supervised classification to the designs of offline alignment, which paves the way to the GPO formulation.\n    \\item In Section~\\ref{sec:regularization}, we dive into how offline preference optimization induces regularization between $\\pi_\\theta$ and $\\pi_\\text{ref}$ during optimization. We identify an offline regularization loss, the effective regularization that offline algorithms enforce, and show how it differs from the KL divergence through analysis and experimental study. We also show how the design of $f$ introduces different strength of regularization, and how hyper-parameters should be chosen adaptive to $f$.\n    \\item In Section~\\ref{sec:exp}, we start with a controlled setting akin to \\citet{gao2023scaling} and show the regularization vs. performance trade-off for different GPO variants. By varying $\\beta$ and learning stages during training, the policy performance initially increases followed by decrease, as predicted by the \\emph{Goodhart's law}. We observe similar trade-offs across different GPO variants, though the best hyper-parameter can differ significantly due to different inherent strengths of the regularization, as suggested by theory. In a LLM summarization task, we also confirm similar performance across different GPO variants (up to tuning in $\\beta$).\n\\end{itemize}\n\n\\section{A general family of offline preference optimization losses} \\label{sec:gpo}\n\nIn the case of language model alignment, we optimize a policy $\\pi_\\theta$ that outputs response $y\\sim \\pi_\\theta(\\cdot|x)$ given prompt~$x$. Given two responses $y,y'\\in\\mathcal{Y}$, a human rater provides feedback by picking out the preferred response. This allows relabeling the two responses as $(y_w,y_l)$ corresponding to the win-loss responses. Such pairwise preference data is usually collected offline and can come from a variety of sources in practice, which we denote as a behavior policy $\\mu$. Henceforth, when the context is clear we remove the dependency on the prompt $x$ for simplicity.\n\nImportantly, we do not make any assumption on the preference structure $p(y\\succ y')$, e.g., it may not come from a Bradley-Terry (BT) model \\citep{bradley1952rank}, a common assumption made in prior work \\citep{rafailov2023direct}. \nBelow, we unify ways to derive various existing offline preference optimization losses for learning from pairwise human feedback.\n\n\\subsection{A recipe to derive preference optimization losses}\n\nAssuming access to a reward function $r_\\phi$, the regularized policy optimization objective \\citep{ouyang2022training} is\n\\begin{align}\n    \\max_{\\pi_\\theta} \\mathbb{E}_{y\\sim \\pi_\\theta}\\left[r_\\phi(y)\\right] - \\beta \\mathbb{KL}\\left(\\pi_\\theta,\\pi_\\text{ref}\\right).\\label{eq:po}\n\\end{align}\nTo be clear about the KL definition, we have for any two distributions $\\pi,\\pi'$: $\\mathbb{KL}\\left(\\pi,\\pi'\\right) \\coloneqq \\mathbb{E}_{y\\sim \\pi}\\left[\\log \\frac{\\pi(y)}{\\pi'(y)}\\right]$. The solution to the regularized objective above can be written analytically as $\\pi_\\theta^\\ast(y)\\propto \\pi_\\text{ref}(y)\\exp\\left(\\beta^{-1}r_\\phi(y)\\right)$.\n\nGiven a pair of responses $(y_w,y_l)$, we can train the reward model $r_\\phi$ through supervised learning. A convenient class of loss function is defined through the difference $r_\\phi(y_w)-r_\\phi(y_l)$: we can think of $r_\\phi(y_w)-r_\\phi(y_l)$ as predicting how likely $y_w$ is preferred to $y_l$. From the discussion above, we see that this difference is equivalent to the log ratio difference of the optimal policy to Eqn~\\eqref{eq:po}\n\\begin{align}\n    r_\\phi(y_w) - r_\\phi(y_l) = \\beta \\left( \\log \\frac{\\pi_\\theta^\\ast(y_w)}{\\pi_\\text{ref}(y_w)} - \\log \\frac{\\pi_\\theta^\\ast(y_l)}{\\pi_\\text{ref}(y_l)}\\right).\\label{eq:reward-logit-equivalence}\n\\end{align}\nHence intuitively, any loss defined through the reward difference $r_\\phi(y_w) - r_\\phi(y_l)$\ncan introduce a loss over $\\pi_\\theta$. \n\nA central insight of this work is framing reward learning as a supervised binary classification problem. We leave a more detailed derivation to Section~\\ref{sec:derivation}, which provides additional insights. Letting $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ be a scalar function, in general the reward learning loss (to be \\emph{minimized}) can be written as\n\\begin{align}\n    \\mathbb{E}_{(y_w,y_l)\\sim \\mu}\\left[f\\left(r_\\phi(y_w)-r_\\phi(y_l)\\right)\\right].\\label{eq:rm}\n\\end{align}\n\nBefore moving on, note that the difference $r_\\phi(y_w)-r_\\phi(y_l)$ is reminiscent of the BT model assumption. However, we argue that it is more sensible to relate this parametric form to the fact that the RLHF formulation (Eqn~\\ref{eq:po}) is a maximization problem, and hence imply that each response can be characterized as a single scalar $r_\\phi(y)$.  We provide a more detailed discussion in Section~\\ref{sec:derivation}.\n\nMany existing offline preference optimization losses can be cast in this general form by replacing the reward difference by the log ratio difference,\n\\begin{align}\n\\mathbb{E}_{(y_w,y_l)\\sim \\mu}\\left[f\\left(\\beta\\cdot\\left(\\log\\frac{\\pi_\\theta(y_w)}{\\pi_\\text{ref}(y_w)} - \\log\\frac{\\pi_\\theta(y_l)}{\\pi_\\text{ref}(y_l)}\\right)\\right)\\right].\\label{eq:offline-loss}\n\\end{align}\n\nHenceforth, we denote the log ratio difference as $\\rho_\\theta\\coloneqq \\log\\frac{\\pi_\\theta(y_w)}{\\pi_\\text{ref}(y_w)} - \\log\\frac{\\pi_\\theta(y_l)}{\\pi_\\text{ref}(y_l)}$ and the above loss can be rewritten as $\\mathbb{E}_{(y_w,y_l)\\sim\\mu}\\left[f\\left(\\beta\\rho_\\theta\\right)\\right]$. A general recipe to derive offline preference optimization losses is to start with a supervised learning loss function $f$ for reward learning, and replace the reward difference by $\\rho_\\theta$ (see, e.g., \\citealp{hastie2009elements} for a nice overview of such loss functions). We can identify the specific functions $f$ for the most common choices; see illustrations of the losses in Figure~\\ref{fig:loss} with $\\beta=1$.\n\\begin{itemize}\n    \\item DPO: $f(\\beta\\rho_\\theta) = -\\log \\sigma(\\beta\\rho_\\theta)$ with $\\sigma$ being the sigmoid function, applies the logistic loss \\citep{hastie2009elements}. The loss can also be written as  $\\log(1+\\exp(-\\beta\\rho_\\theta))$. \n    \\item IPO: $f(\\beta\\rho_\\theta) = \\left(\\beta\\rho_\\theta-1\\right)^2$, the squared function \\citep{rosasco2004loss}, can be understood as applying linear regression to the probability that $y_w$ is preferred \\citep{hastie2009elements}.\n    \\item SLiC: $f(\\beta\\rho_\\theta)=\\max(0,1-\\beta\\rho_\\theta)$ is the hinge loss function, stemming from the max-margin (support vector machine) paradigm \\citep{boser1992training,cortes1995support}. The original SliC algorithm \\citep{zhao2023SLiC} also includes a supervised learning component, which we do not discuss here.\n\\end{itemize}\n\n\\begin{table*}\n\\centering\n    \\caption{Side-by-side correspondence between existing offline preference optimization losses and convex supervised learning losses. Among a rich variety of convex supervised learning losses developed in the literature, logistic log loss \\citep{hastie2009elements}, hinge loss \\citep{cortes1995support} and squared loss \\citep{rosasco2004loss} have offline preference optimization algorithmic counterparts. Other notable losses, such as the exponential loss \\citep{freund1995desicion}, truncated quadratic loss \\citep{bartlett2006convexity} and Savage loss \\citep{masnadi2008design} can form novel offline preference optimization algorithms. \n    \\newline}\n\\begin{small}\n\\begin{sc}\n \\begin{tabular}{l|c|c}\\toprule[1.5pt]\n Supervised learning losses & $f(\\beta\\rho_\\theta)$ & Offline preference  optimization\n\\\\\\midrule\nLogistic log loss & \\bf $\\log\\left(1+\\exp(-\\beta\\rho_\\theta)\\right)$ & DPO \\citep{rafailov2023direct} \\\\\nHinge loss & \\bf $\\max\\left(0,1-\\beta\\rho_\\theta\\right)$ & SLiC \\citep{zhao2023SLiC} \\\\\nsquared loss & \\bf $\\left(\\beta\\rho_\\theta-1\\right)^2$ & IPO \\citep{azar2023general} \\\\\nExponential loss & \\bf $\\exp(-\\beta\\rho_\\theta)$ & N/A \\\\\nTruncated quadratic loss & \\bf $\\left(\\max(0,1-\\beta\\rho_\\theta)\\right)^2$ & N/A \\\\\nSavage loss & \\bf $1/(1+\\exp(\\beta\\rho_\\theta))^2$ & N/A \\\\\n\\bottomrule\n\\end{tabular}\n\\end{sc}\n\\end{small}\n\\vskip -0.1in\n\\label{table:losses}\n\\end{table*}\n\n\\subsection{GPO: A generalized family of offline preference optimization algorithms}\n\nBuilding on the discussion above,\nin general, any properly defined supervised learning loss $f$ for reward modeling can translate into a preference optimization objective $\\mathbb{E}_{(y_w,y_l)\\sim\\mu}\\left[f(\\beta\\rho_\\theta)\\right]$. We provide a table of a few notable supervised learning losses developed in the decades-old literature, each loss mapping into an offline preference optimization algorithm.\n\nAs discussed above, some of them have already translated into existing methods.\nWe note a few examples without offline preference optimization counterparts:\n\\begin{itemize}\n    \\item Exponential loss: $f(\\beta\\rho_\\theta)=\\exp(-\\beta\\rho_\\theta)$, the loss function for the AdaBoost algorithm \\citep{freund1995desicion}.\n    \\item Truncated quadratic: $f(\\beta\\rho_\\theta)=\\left(\\max\\left(0,1-\\beta\\rho_\\theta\\right)\\right)^2$ \\citep{bartlett2006convexity}, a truncated variant of the squared loss, is also a smooth approximation to the hinge loss.\n    \\item Savage loss: $f(\\beta\\rho_\\theta)=1/\\left(1+\\exp(\\beta\\rho_\\theta)\\right)^2$ \\citep{masnadi2008design} which have proved robust to outliers in data and found applications in boosting algorithms.\n\\end{itemize}\n\\citet{rosasco2004loss,bartlett2006convexity} give a more exhaustive list of convex supervised learning losses and their discussions.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.48\\textwidth]{plots/loss_all.pdf}\n    \\caption{\\small{Illustration of notable examples of binary classification loss functions, including both examples (logistic, squared and hinge) that have led to existing offline preference optimization algorithms, as well as others (exponential, truncated squared, Savage) that produce novel losses.}}\n    \\label{fig:loss-all}\n\\end{figure}\n\nA key motivating argument for the offline preference optimization algorithms \\citep{rafailov2023direct,azar2023general,zhao2023SLiC} is that minimizing the offline losses for the policy $\\pi_\\theta$ is equivalent to obtaining the optimal regularized policy against a loss minimizing reward model. We can extend the conclusion to this general family of offline preference optimization algorithms.\n\n\\begin{restatable}{theorem}{thmequivalence}\\label{thm:equivalence}\n    (\\textbf{Equivalence of optimal solutions}) Let $\\pi_\\theta^\\ast$ be the global minimizer of the offline preference optimization loss in Eqn~\\eqref{eq:offline-loss}. $\\pi_\\theta^\\ast$ is the same as the optimal regularized policy (according to Eqn~\\eqref{eq:po}) for a reward function that globally minimizes the loss Eqn~\\eqref{eq:rm}.\n\\end{restatable}\n\n\\section{Reward modeling viewed as a binary classification problem} \\label{sec:derivation}\n\nHere, we take a step back and dive into the derivation that converts reward modeling into a supervised binary classification problem. We provide a brief background on the basic setup, and how it relates to reward modeling (see, e.g., \\citealp{hastie2009elements} for a more comprehensive introduction).\n\nIn binary classification, given a pair of feature and label $(z, l)$ with $z\\in\\mathbb{R}^k$ and $l \\in\\{-1,1\\}$, the aim is to predict $\\hat{\\ell}(z)\\in\\mathbb{R}$ as a function of the feature, and use $\\text{sign}\\left(\\hat{\\ell}(z)\\right)$ as the classifier, in the hope that it can match the ground truth label $y$. The classification accuracy can be written as $\\frac{1}{2}\\mathbb{E}\\left[\\text{sign}\\left(\\hat{\\ell}(z)\\cdot l\\right)\\right] + \\frac{1}{2} \\in [0,1]$ and an equivalent loss function is\n\\begin{align}\n    \\mathbb{E}\\left[1-\\text{sign}\\left(\\hat{\\ell}(z)\\cdot \\ell\\right)\\right].\\label{eq:0-1}\n\\end{align}\nThe above loss, known as the 0-1 loss (see the dotted dark curve in Figure~\\ref{fig:loss}) is non-convex. Instead of directly optimizing it, we can take smooth convex functions $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ and approximate the loss as\n\\begin{align*}\n    \\mathbb{E}\\left[f\\left(\\hat{\\ell}(z)\\cdot \\ell\\right)\\right].\n\\end{align*}\n\nTaking this back to the case of reward modeling, given a pair of responses $(y_1,y_2)$, we construct a sample for binary classification by setting the label $\\ell=1$ if $y_1\\succ y_2$ and $\\ell=-1$ otherwise. \n\nThinking of $(y_1,y_2)$ as the feature from which to make prediction, in general the prediction would be a bi-variate function $\\hat{\\ell}(y_1,y_2)$ that can depend on both $y_1$ and $y_2$ in an arbitrary form. For a pointwise reward model that depends on a single response $r_\\phi:\\mathcal{Y}\\rightarrow\\mathbb{R}$, an intuitive parameterization would be to take the difference of two rewards $\\hat{\\ell}(y_1,y_2)=r_\\phi(y_1)-r_\\phi(y_2)$. The corresponding binary classification loss is\n\\begin{align*}\n    \\mathbb{E}_{y_1\\sim\\mu,y_2\\sim\\mu}\\left[\\mathbb{I}\\left[y_1\\succ y_2\\right] f\\left(r_\\phi(y_1)-r_\\phi(y_2)\\right)\\right] + \\mathbb{E}_{y_1\\sim\\mu,y_2\\sim\\mu}\\left[\\mathbb{I}\\left[y_2\\succ y_1\\right] f\\left(r_\\phi(y_2)-r_\\phi(y_1)\\right)\\right].\n\\end{align*}\nEquivalently, we can write the loss as in Eqn~\\eqref{eq:rm}\n\\begin{align*}\n    \\mathbb{E}_{(y_w,y_l)\\sim \\mu}\\left[f\\left(r_\\phi(y_w)-r_\\phi(y_l)\\right)\\right].\n\\end{align*}\n\nThe above result offers a number of interesting implications, which we expand on in the next section.\n\n\\subsection{Characterizing what the reward model learns}\n\nDrawing inspiration from the supervised learning literature, we can reason about properties of the reward models obtained by minimizing the convex loss function $f$. This can translate into effects on the downstream optimized policies due to the equivalence in Eqn~\\eqref{eq:reward-logit-equivalence}. Some discussions are in order below.\n\n\\paragraph{The Bradley-Terry assumption and analytic forms of reward models.} \n\nAs alluded to earlier, the design of the reward modeling loss as a function of the reward difference $r_\\phi(y_w)-r_\\phi(y_l)$ should be interpreted as a result of the reward maximization formulation of RLHF. Implicitly, the maximization formulation assumes that there is a total order on all the responses (i.e., they can be ranked in a monotonic order), which intuitively is captured by the BT assumption to a large extent. Meanwhile when there is no total order, the formulation Eqn~\\eqref{eq:po} would not be perfect, and one might need to resort to alternative solution concepts such as Nash equilibrium \\citep{munos2023nash,swamy2024minimaximalist}. \n\nIn general, one should train a pairwise preference model $\\hat{\\ell}(y_1,y_2)=r_\\phi(y_1,y_2)$ rather than pointwise reward models, for which there could be characterizations on the properties of the learned model that we discuss below. For pointwise models\nthe analytic forms are only available in a few special cases drawn from prior work. We discuss two notable examples: (1) the logistic loss, under the assumption that the ground truth preference satisfies a BT model $p(y_1\\succ y_2)=\\sigma\\left(r^\\ast(y_1)-r^\\ast(y_2)\\right)$, then the optimal reward obtained by minimizing Eqn~\\eqref{eq:rm} is a constant shift from $r^\\ast$ \\citep{rafailov2023direct}; (2) For the squared loss, where the optimal reward is a constant away from $p(y\\succ \\mu)=\\mathbb{E}_{y'\\sim\\mu}\\left[p(y\\succ y')\\right]$ without further assumptions on the ground truth preference. For interested readers, note that the discussion here also provides an alternative way to derive the IPO algorithm distinct from the original derivation in \\citet{azar2023general}.\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=0.95\\textwidth]{plots/plot_det_policy_exp.pdf}\n    \\caption{\\small{Bandit example \\citep{azar2023general} to illustrate the regularization effect of different GPO variants. Convex loss functions with a fast decaying tail or upwards tail (hinge, truncated quadratic and squared loss) will penalize response-level deviations from $\\pi_\\theta$ to $\\pi_\\text{ref}$, effectively enforcing a stronger regularization. Other convex losses we exhibit here generally have a slower decaying tail, and will more likely converge to deterministic policies in pathological cases (e.g., deterministic preference).}}\n\\end{figure*}\n\n\\paragraph{A case study of logistic loss vs. hinge loss.} \n\nConsidering the special case when the preferred and non-preferred samples are separable, the hinge loss will find the optimal separating hyperplane that maximizes the margin between the two sets of samples. Drawing inspiration from the classic comparison between logistic regression and support vector machine \\citep{hastie2009elements}, we note that the logistic loss will find a similar decision boundary (i.e., sign of the prediction), but it will try to increase the magnitude of the prediction $\\hat{\\ell}(y_w,y_l)$ to infinity. Such behavior is alluded to in the IPO work \\citep{azar2023general} as a failure case of DPO. In general, convex loss functions with a fast-decaying tail (e.g., hinge loss for SLiC) or upwards tail (e.g., squared loss for IPO) will alleviate such issues. In Section~\\ref{sec:regularization}, we will illustrate such insights in combination with policy optimization.\n\n\\paragraph{General requirement on the convex function $f$.} Not all convex functions $f$ can lead to valid loss functions for binary classification. For our study, we further assume $f'(0)<0$, i.e., $f$ locally decreases at $\\rho_\\theta=0$. This means that the minimizer of $f$ is obtained at some $\\rho_\\theta>0$, and intuitively would push the reward difference $r_\\phi(y_w)-r_\\phi(y_l)$ in the right direction. Intriguingly, this condition is related to Bayes consistency \\citep{rosasco2004loss,bartlett2006convexity}, i.e., under which condition can the prediction function $\\hat{\\ell}(y_1,y_2)$ recover the same sign as the preference probability $\\text{sign}\\left(2p(y_1\\succ y_2)-1\\right)$. We provide discussions for interested readers in Appendix~\\ref{appendix:bayes}.\n\n\\section{Understanding regularization in offline preference optimization} \\label{sec:regularization}\n\nIn this section, we seek to gain a better understanding of the regularization implicitly enforced by the offline preference optimization algorithms.\n\nThough in general it is challenging to characterize the full learning dynamics of the offline algorithms, we provide analysis from a few angles, which might shed light on how the regularization works. Recall that in the RLHF formulation (Eqn~\\ref{eq:po}), the KL regularization is a key element; we will see its connections to the offline regularization.\n\n\\subsection{How do offline losses enforce regularization}\n\nAs hinted at before, henceforth will we consider the class of convex loss functions that are locally decreasing at $\\rho_\\theta=0$, i.e., $f'(0)<0$. All the examples in Table~\\ref{table:losses} satisfy this property. \n\nTo shed light on how such loss functions entail preference optimization while enforcing regularizers, we consider the Taylor expansion around $\\rho_\\theta=0$, which is a valid approximation when $\\rho_\\theta$ is small, i.e., $\\pi_\\theta$ does not deviate much from $\\pi_\\text{ref}$.\n\\begin{align*}\n   \\underbrace{\\mathbb{E}_{(y_w,y_l)\\sim \\mu}\\left[f(\\beta\\rho_\\theta)\\right]}_{\\text{offline loss}} \\approx f(0) + \\underbrace{f'(0)\\beta\\cdot\\mathbb{E}_{(y_w,y_l)\\sim \\mu}\\left[\\rho_\\theta\\right]}_{\\text{preference optimization}} + \\underbrace{\\frac{f''(0)\\beta^2}{2}\\cdot\\mathbb{E}_{(y_w,y_l)\\sim \\mu}\\left[\\rho_\\theta^2\\right]}_{\\text{offline regularization}}, \n\\end{align*}\nThe expansion implies that when the approximation is valid, the offline algorithms all resemble the case where $f$ is the squared loss (i.e., the IPO loss \\citep{azar2023general}). We provide more discussion in Appendix~\\ref{appendix:proof}. Minimizing the Taylor-expanded objective achieves two purposes: preference optimization and regularization towards the reference policy. Indeed,  minimizing the first-order term \n\\begin{align*}\n    f'(0)\\beta\\cdot\\mathbb{E}_{(y_w,y_l)\\sim \\mu}[\\rho_\\theta]\n\\end{align*}\nencourages $\\pi_\\theta$ to place more weight on the preferred response $y_w$ over $y_l$, hence maximizing pairwise human preference. \n\nTo see the effect of the regularization, when $f''(0)>0$ observe that the second-order term \n\\begin{align}\n    f''(0)\\beta^2\\cdot \\mathbb{E}_{(y_w,y_l)\\sim \\mu}\\left[\\frac{1}{2}\\rho_\\theta^2\\right]\\label{eq:squared-regularization}\n\\end{align}\nis minimized at $\\rho_\\theta=0$, in which case $\\pi_\\theta(y)=\\pi_\\text{ref}(y)$ for all $y$ in the support of $\\mu$. In general, this loss will encourage $\\pi_\\theta$ to stay close to $\\pi_\\text{ref}$. We call the above \\textbf{$\\mu$-weighted squared loss}. Importantly, the global minimizer of the KL divergence between $\\pi_\\theta$ and $\\pi_\\text{ref}$ is also a minimizer of the $\\mu$-weighted squared loss (i.e., both minimized when $\\pi_\\theta=\\pi_\\text{ref}$).\n\nWhen the approximation is valid, the GPO problem with a regularizer $\\beta$ is corresponds to the IPO problem with regularizer $|f''(0)/f'(0)|\\cdot\\beta$,\nand this quantity determines the relative strength of the regularization.\nThe coefficient $|f''(0)/f'(0)|$ interestingly relates to how convex loss functions are theoretically built-in to be regularized for better generalization \\citep{masnadi2015view}. This may inform the design of offline preference optimization algorithms with another theoretical perspective.\n\n\\paragraph{Intuition about the full gradient update.} The Taylor expansion is only valid near $\\rho_\\theta=0$ and except for the special case of squared loss (IPO), drops higher order terms. For example, the expansion does not work natively for SLiC, which employs a non-smooth convex function. Though understanding the full learning dynamics is challenging, we can provide some intuitions about how the full gradient update enforces $\\pi_\\theta$ to stay close to $\\pi_\\text{ref}$: consider the gradient update for when $\\beta=1$,\n\\begin{align}\n    \\theta \\leftarrow \\theta - \\mathbb{E}_{(y_w,y_l)\\sim\\mu}\\left[f'(\\rho_\\theta)\\nabla_\\theta \\rho_\\theta\\right].\\label{eq:gradient-update}\n\\end{align}\nStarting from $0$, suppose $\\rho_\\theta$ takes a very high value. This means potentially $\\pi_\\theta$ places many more weights on certain responses than $\\pi_\\text{ref}$, which is what the KL divergence regularization seeks to prevent. For the offline update, since $f$ is convex, a few cases are possible: case I: $f'(\\rho_\\theta)<0$ (for logistic, exponential and Savage loss), $\\rho_\\theta$ will continue to increase but with a vanishing gradient; hence the regularization is still in place. Meanwhile for case II: $f'(\\rho_\\theta)\\leq 0$ (for hinge, smoothed quadratic and squared loss), $\\rho_\\theta$ will stop updating or be pushed downwards. As a result, in case  II the gradient update explicitly does not allow $\\pi_\\theta(y)$ to deviate from $\\pi_\\text{ref}(y)$ for individual responses $y$, effectively enforcing a stronger regularization with a fixed value of $\\beta$.\n\nIn Figure~\\ref{fig:bandit}, we illustrate the effect of strong regularization using the $3$-action bandit example presented in \\citep{azar2023general}, where a simple offline dataset with three pairs of examples are used for training softmax parameterized policies: $(y_1,y_2), (y_2,y_3), (y_1,y_3)$. Examples are uniformly sampled from the distribution. Since $y_1$ is the strongest response, we expect the algorithms to assign high weights to $\\pi_\\theta(y_1)$, causing deviation from $\\pi_\\text{ref}$ which is uniform.\nThe example is meant to illustrate the undesirable behavior of DPO, which tends to push up the probability of $y_1$, despite the intended regularization. See Appendix~\\ref{appendix:exp} for more details on the setup. \n\nWe generalize their observations by noting that for any given values of $\\beta$, case I losses will keep pushing up the probability of a winning action $y_1$, whereas case II losses enforce the constraint much more conservatively, preventing deterministic policies. In practice where preferences over responses are almost never deterministic, we will see that case I losses are also reasonably well behaved.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.48\\textwidth]{plots/gaussian_only.pdf}\n    \\caption{\\small{An example of $\\mu$-weighted squared loss and KL divergence for mixture of Gaussians. The squared loss has local minimizers different from the KL divergence. This means locally descending on the squared loss may not lead to decreases in the KL divergence, and may not find the global minimizer of the KL divergence. See Appendix~\\ref{appendix:exp} for the pdf of $\\pi_\\text{ref}$ and $\\mu$.}}\n    \\label{fig:gaussians}\n\\end{figure}\n\n\\paragraph{Choosing the right value for $\\beta$.} if we understand the tail behavior of the convex function as determining the \\emph{natural} regularization strength of the offline algorithm, the hyper-parameter $\\beta$ needs to chosen accordingly, if one desires a fixed level of regularization. For example, the logistic loss (i.e., DPO) requires a higher value of $\\beta$ to enforce the same level of regularization as the squared loss (i.e., IPO) and the hinge loss (i.e., SLiC), as also exemplified in Figure~\\ref{fig:bandit}.\n\n\\subsection{Offline regularization vs. KL regularization}\n\nHenceforth we will resort back to the offline regularization: $\\mu$-weighted squared loss, and understand its difference against the KL divergence regularization. We start with the gradient of the $\\mu$-weighted squared loss \n\\begin{align*}\n    \\mathbb{E}_{y\\sim \\mu}\\left[\\nabla_\\theta\\frac{1}{2}\\rho_\\theta^2\\right]\n\\end{align*}\nwhich seeks to decrease the squared error that measures the discrepancy between $\\pi_\\theta$ and $\\pi_\\text{ref}$, at samples generated by $\\mu$. For the KL divergence, we can show that its gradient is equivalent to the $\\mu$-weighted squared loss with  $\\mu=\\pi_\\theta$\n\\begin{align}\n\\nabla_\\theta \\mathbb{KL}(\\pi_\\theta,\\pi_\\text{ref})= \\mathbb{E}_{\\color{blue}y\\sim \\pi_\\theta}\\left[\\nabla_\\theta \\frac{1}{2}\\rho_\\theta^2\\right].\\label{eq:kl-mu}\n\\end{align}\nIn other words, we can understand the gradient to the KL divergence as minimizing the discrepancy with \\emph{on-policy} samples under $\\pi_\\theta$, rather than offline samples from $\\mu$. We detail the derivation in Appendix~\\ref{appendix:proof}; note a highly similar result was also derived in \\citep{richter2020vargrad}.\n\nIn summary, both losses enforce the squared penalty on samples from $\\mu$ vs. online samples from $\\pi_\\theta$. We can envision cases when the $\\mu$-weighted squared loss is being minimized, the KL divergence might not decrease as desired.\n\n\\paragraph{A mixture of Gaussians counterexample.} To show the fact that, during minimization of the squared loss, we may not necessarily observe global minimization of the KL divergence, we provide a low-dimensional toy counterexample using mixture of Gaussians. We set up an example where both $\\pi_\\text{ref}$ and $\\mu$ are mixtures of three Gaussians. The optimized policy $\\pi_\\theta$ is just a constant shift away from $\\pi_\\text{ref}$ with the shift being parameterized by a trainable parameter $c$. When $c=0$, we have $\\pi_\\text{ref}=\\pi_\\theta$ and both the squared loss and KL divergence are minimized to $0$. \n\nIn Figure~\\ref{fig:gaussians}, we show the KL divergence and the $\\mu$-weighted squared loss, both in log scales, as a function of $c\\in[-1,1]$. The squared loss has a few minima, with some of them being remote from $c=0$. This means gradient descent on the squared loss may not lead to smaller KL in general, though they are both globally minimized at $\\pi_\\theta=\\pi_\\text{ref}$ for $c=0$. See Appendix~\\ref{appendix:exp} for the plot of the pdfs of $\\mu$ and $\\pi$.\n\nThis example is meant to illustrate that the arguments used in prior work on offline preference optimization \\citep{rafailov2023direct}, which heavily rely on the global minimization of objectives, may not always be true in practice: locally minimizing the $\\mu$-weighted squared loss might not lead to decrease in the KL divergence. However, the silver lining is that near $\\pi_\\theta=\\pi_\\text{ref}$, the two losses are highly correlated; we will validate the observations on such a low-dimensional example with a language modeling study.\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=0.95\\textwidth]{plots/gpo_all.pdf}\n    \\caption{\\small{Tracing out KL divergence vs. $\\mu$-weighted squared loss during offline preference optimization. (Left) With $f$ being the squared function, we show the trajectories for a range of $\\beta$s. Importantly, the initial data point for which $\\pi_\\theta=\\pi_\\text{ref}$ is dropped for better visualization, see Appendix~\\ref{appendix:exp} for the complete plot. Note that as $\\beta$ increases, the algorithm maintains a better constraint on the $\\mu$-weighted squared loss, which also induces a constraint on the KL divergence. (Right) We pool over different $\\beta$s and show trajectories for different GPO variants. See Appendix~\\ref{appendix:exp} for individual plots for each variant. Overall, all algorithmic variants enjoy similar constraint properties, with most variants being slightly more stable than the logistic variant.}}\n    \\label{fig:gpo-all}\n\\end{figure*}\n\n\\subsection{Analyzing a language modeling example}\n\nIn the case of language modeling, where $\\pi_\\theta,\\pi_\\text{ref},\\mu$ are sequential categorical distributions, we measure the correlation between the KL divergence $\\mathbb{KL}\\left(\\pi_\\theta,\\pi_\\text{ref}\\right)$ and the $\\mu$-weighted squared loss $\\mathbb{E}_{y\\sim\\mu}\\left[\\frac{1}{2}\\rho_\\theta^2\\right]$ during offline training. We consider the summarization task similar to \\citep{roit2023factually}, where the offline dataset is an open source summarization dataset collected with human feedback labels \\citep{stiennon2020learning}. We give more details in Appendix~\\ref{appendix:exp}.\n\nFor each experiment, we choose a fixed value of regularization $\\beta$. Then, we initialize $\\pi_\\theta$ from $\\pi_\\text{ref}$ and minimize the offline preference losses over the dataset. As the training progresses, we record sample-based estimates of the KL divergence and $\\mu$-weighted squared loss over time, and trace them out in Figure~\\ref{fig:gpo-all} left plot for when $f$ is a squared function.  We show both loss functions in the log scale. \n\nImportantly, we have dropped from the plot the initial data point for which $\\pi_\\theta=\\pi_\\text{ref}$ and both losses are zero, otherwise the whole plot will look unbalanced (since $\\log 0\\approx -\\inf$). See the full plot in Appendix~\\ref{appendix:exp}. We make a few comments regarding the current plot.\n\n\\paragraph{Correlation between the two losses.} There appears to be two phases in Figure~\\ref{fig:gpo-all} left plot. When $\\beta$ is large, and when the $\\mu$-weighted squared loss is maintained at a lower level, we see a better correlation between the two losses. Meanwhile, when $\\beta$ is small and the $\\mu$-weighted squared loss grows quickly during optimization, its correlation with KL divergence becomes more elusive (see purple and blue data points on the left plot). Such observations echo the mixture of Gaussian examples, where in the vicinity of $\\pi_\\theta=\\pi_\\text{ref}$, the two losses have similar trends; the misalignment happens when we deviate too much from the origin.\n\nThough the correlation between the two losses seem to break when $\\pi_\\theta$ is too far away from $\\pi_\\text{ref}$, the silver lining is that for offline algorithms, the optimization always starts with the origin $\\pi_\\theta=\\pi_\\text{ref}$, and one may expect a better control over the KL divergence through the $\\mu$-weighted squared loss.\n\n\\paragraph{More variations in KL compared to $\\mu$-weighted loss.}\nFor Figure~\\ref{fig:gpo-all} left plot, in the regime where the KL divergence and $\\mu$-weighted squared loss are better correlated (areas inside the grey bounding box), we see an order of magnitude more drastic variations in the KL divergence ($10^{-0.5}\\rightarrow 10^{1.5}$) than the $\\mu$-weighted squared loss ($10^{-1.5} \\rightarrow 10^{0.5}$). \n\nThis hints at the challenge of maintaining the KL divergence constraint by controlling the $\\mu$-weighted squared loss. Indeed, since the offline preference optimization algorithms directly optimize for the $\\mu$-weighted squared loss in the vicinity of the origin $\\pi_\\theta=\\pi_\\text{ref}$, even small changes in the $\\mu$-weighted squared loss can induce much bigger changes in the KL divergence. This might become a source of instability during optimization. However, the degree to which such instability can be mitigated by other hyper-parameter choices such as learning rate, might vary case-by-case.\n\n\\paragraph{Comparison across different GPO variants.} In Figure~\\ref{fig:gpo-all} right plot we compare the constraint contours across different GPO variants listed in Table~\\ref{table:losses}. For each variant we sweep the $\\beta$s but for visualization we pool across results from all $\\beta$s, see Appendix~\\ref{appendix:exp} for individual plots. \n\nOverall, different variants follow a similar pattern, with most variants being slightly more robust compared to the logistic loss, which seems to induce slightly bigger variations in the KL divergence compared to other alternatives.\n\n\\section{Empirical study of GPO variants} \\label{sec:exp}\n\nWe now carry out a set of experimental comparison between different GPO algorithms, and to study their empirical behavior and validate theoretical insights.\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=0.95\\textwidth]{plots/gpo_goodhart.pdf}\n    \\caption{Left: Tracing KL divergence vs. golden win rate performance for different GPO variants. Each data point corresponds to a policy obtained during training with a particular value of $\\beta$ and convex function loss. For each loss variant, we pool data points across $\\beta$s and different stages of training. Overall, the trade-off curves of GPO variants look similar. Right: Tracing the trade-off for the logistic loss (DPO), grouped according to the regularization coefficient $\\beta$. As $\\beta$ increases, the regularization effect is larger and during training, and the policies tend to have smaller KL divergence against $\\pi_\\text{sft}$.}\n    \\label{fig:goodhart-all}\n\\end{figure*}\n\n\\subsection{Trade-offs between KL divergence and performance}\n\nAs the offline alignment optimization progresses, the policy $\\pi_\\theta$ starts to drift away from the initial anchor policy $\\pi_\\text{sft}$. When measured in terms of the \\emph{ground truth} performance, there is a trade-off between model performance and KL divergence from the initialization. We adopt a synthetic setting similar to \\citep{gao2023scaling} to study this trade-off. \n\nConcretely, we take the summarization task introduced above and train a XXL model (11 billion parameters) as the golden preference model, using similar training setting as \\citet{munos2023nash}. This preference model will be used as the \\emph{golden} judgement. Since the preference model carries out side by side comparison, we also train a golden policy as the fixed baseline to compare against. We provide more technical details in Appendix~\\ref{appendix:exp}. For each fixed convex loss function, we sweep over values of the regularization coefficient $\\beta$. For each $\\beta$, we train the model for $2\\cdot 10^4$ steps with a constant learning rate ($10^{-5}$ and $3\\cdot 10^{-5}$). We evaluate checkpoints every $2k$ steps for a total of $20k$ training steps.\n\nIn Figure~\\ref{fig:goodhart-all} (left), we trace the performance of trained checkpoints over time, plotting their golden evaluation performance against the golden policy. Each dot corresponds to a checkpoint evaluation, for a particular value of $\\beta$, learning rate and convex function loss. We group the results by the convex function loss. A few observations are in order: (1) We observe the over-optimization effect compatible with \\emph{Goodhart's law} \\citet{gao2023scaling}, wherein as the KL divergence increases, the golden performance evaluation first increases and then decreases as a result of over-optimization. The key difference is that \\citep{gao2023scaling} is for online RLHF, while our case is offline optimization; (2) For different loss functions, the overall trade-off curves look similar. Concretely, the peak performance is similar and is obtained at a similar level of KL divergence. This suggests that for any choice of the convex loss function, a choice of $\\beta$ and training step can lead to a specified level of performance. \n\nIn Figure~\\ref{fig:goodhart-all} (right), we break down the trade-off curve with respect to the regularization coefficient $\\beta$. We show the case for the logistic loss, though other losses have a similar breakdown (see Appendix~\\ref{appendix:exp} for full results). For each $\\beta$ (with a unique color), different data points correspond to different stage of training for the same experiment and hence tracing out a trend of KL divergence vs. win rate. We make a few observations: (1) Data points seem to piece together seamlessly at the \\emph{soft} boundaries between $\\beta$s, this means given a fixed value of $\\beta$, one can probably obtain a specified level of KL divergence and win rate performance, by training the policy for a certain number of steps. However, different $\\beta$s are not equal: in the case of logistic loss, $\\beta \\sim 1$ seems to obtain the best overall performance across training, while $\\beta=0.01$ can easily train the policy to have large KL divergence, resulting in degraded performance; meanwhile, $\\beta=100$ puts a larger constrain the policy near $\\pi_\\text{sft}$, making it difficult to obtain the best performance across training.\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.95\\textwidth]{plots/gpo_betas.pdf}\n    \\caption{Left: $90\\%$-th percentile performance during training for different values of $\\beta$s. We use the $90\\%$-th percentile as an estimate of the best possible performance under a fixed $\\beta$. Different GPO variants seem to peak at different values of $\\beta$: noticeably, squared loss and truncated squared loss peak at about $\\beta=1$ while others mostly peak at slightly larger values $\\beta\\sim 10$. Right: Median values of KL divergence during training, as a function of $\\beta$ for different GPO variants. When $\\beta$ is small, different variants have little distinction; when $\\beta$ is large (strong regularization) and fixed, squared and truncated squared loss tend to incur smaller KL divergence compared to other variants.}\n    \\label{fig:goodhart-betas}\n\\end{figure*}\n\n\\paragraph{Impact of $\\beta$.} We now closely investigate the impact that $\\beta$ has on the performance and KL regularization dynamics of various GPO variants. Figure~\\ref{fig:goodhart-betas} (left) shows the peak performance of various algorithms as a function of $\\beta$. As seen from the plot, the peak performance of squared and truncated squared loss is obtained at generally lower $\\beta\\sim 1$, whereas the peak performance for other variants are obtained at higher $\\beta\\sim 10$. There is some variations of the peak win rate (e.g., exponential seems to be slightly better than others) but this might not be statistically significant.\n\nWhile the observation suggests the fact that different algorithms require different values of $\\beta$s to perform the best, it can be explained by the fact that different loss functions induce distinct strengths of regularization as a function of $\\beta$, as predicted by theory. In Figure~\\ref{fig:goodhart-betas} (right) we show the median KL divergence during training as a function of $\\beta$, for different convex loss functions. When $\\beta$ is small and regularization is weak, there is little distinction between different variants. This is compatible with the results in Figure~\\ref{fig:gpo-all}: the offline algorithm enforces regularization through the weighted squared loss, and its correlation with KL divergence is weak when the regularization is small. At large values of $\\beta$s, the correlation between offline regularization and KL divergence is much stronger. And indeed, we see squared and truncated squared loss enforce stronger regularization than other variants, with logistic, exponential and Savage being in the same league and hinge loss in the middle.\n\n\\subsection{Model-based side by side evaluation}\n\nThe synthetic setting has provided many insights into the trade-offs between regularization and policy performance, and how they are modulated by choices of $\\beta$ and convex loss functions. We now carry out a final set of experiments on the summarization task, using settings described in prior work \\citep{munos2023nash,calandriello2024human}.\n\nWe consider the side-by-side comparison metric used by \\citet{munos2023nash}, where we compare the checkpoint performance against a fixed opponent $\\pi_\\text{ref}$. The comparison is made by a prompted PaLM-2 model \\citep{anil2023palm} over an evaluation set of $2000$ summary samples. The prompted model judges which response is of higher quality. See Appendix~\\ref{appendix:exp} for evaluation details.\n\nExamining the performance across $\\beta$s, we see that when $\\beta$ is small, the optimization tends to be more effective, achieving the best performance at about $\\beta\\in[0.1,1]$ across the board, with similar peak performance. The performance experiences a bigger drop when $\\beta$ becomes large. When making pairwise comparison across different GPO variants, we see that their performance is generally on par with one another; choosing the right $\\beta$ appears more critical. Due to space limits, we present these comparisons in Appendix~\\ref{appendix:exp}.\n\n\\section{Discussions and conclusion}\n\nWe have presented GPO, a generalized approach to deriving offline preference optimization losses for LLM alignment. GPO presents a continuous spectrum of loss functions, encompassing DPO, IPO and SLiC as special instances. By deriving GPO through the rich literature on binary classification, we have presented a more unified way to reason about the strength of regularization and what the optimized policy seeks to capture.\n\nWe have shown the connections between the offline regularization and the KL regularization, which the RLHF formulation seeks to enforce. The two types of regularization are different in general. However, optimizing from the origin, we see empirical evidence that the two losses are correlated, alluding to the fact that enforcing KL divergence through offline optimization is possible though maybe more challenging. \n\nWe have also showed the regularization vs. performance trade-off between different GPO variants. Overall, the regularization vs. performance trade-off is similar for different algorithms. As predicted by theory, different convex loss variants induce inherently distinct strengths for regularization, which impacts the optimal value of $\\beta$ for each algorithm (i.e., squared loss needs a smaller $\\beta$ than logistic loss).\n\nOur results have a number of limitations and provide avenues for future work. Our framework is based on the reward maximization formulation of RLHF, and hence still encounters theoretical issues when the ground truth preference structure is complex. A future direction would be to connect GPO with alternative solution concepts for alignment such as Nash equilibrium \\citep{munos2023nash}. Our framework also only deals with offline losses with a contrastive form, and does not handle supervised learning based losses \\citep{zhao2023SLiC}.\n\n\\paragraph{Acknowledgements.} We thank Ivo Danihelka for providing very valuable feedback to an earlier draft of the paper. We are thankful to the Google DeepMind teams that build the infrastructure which facilitates the research in this work.\n\n\\newpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{sDPO: Don't Use Your Data All at Once}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\n\\blfootnote{$^\\dagger$ Corresponding Author}\nAs large language models (LLMs) continue to advance, aligning them with human preferences has become a critical objective. In this paper, we introduce stepwise DPO (\\method), an innovative extension of the recently popularized Direct Preference Optimization (DPO) technique for alignment tuning. \\method systematically partitions the available preference datasets and applies them incrementally, rather than utilizing the entire dataset simultaneously. This stepwise manner enables the integration of progressively more aligned reference models within the DPO training framework. Our empirical results demonstrate that \\method not only enhances the alignment precision of reference models but also significantly improves the overall performance of the final model, surpassing other prominent LLMs with larger parameter counts.\n\\end{abstract}\n\n\\section{Introduction}\nLarge language models (LLMs) have revolutionized the field of natural language processing (NLP) by undergoing pre-training, supervised fine-tuning, and alignment tuning, with the latter ensuring the safety and usefulness of the model. \nReinforcement learning (RL) techniques~\\cite{christiano2017deep, bai2022constitutional}, such as proximal policy optimization (PPO)~\\cite{schulman2017proximal}, are generally used in this alignment phase.\n\nTo address the complicated nature of RL in LLM training, direct preference optimization (DPO)~\\cite{rafailov2023direct} have been popularized for its simplicity and effectiveness. DPO involves curating preference datasets using human or strong AI (\\textit{e.g.,} GPT-4~\\cite{openai2023gpt4}) judgement to select chosen and rejected responses from a pool of multiple answers to a given question. Then, the model being trained (\\textit{i.e.}, target model) and a separate reference model compute log probabilities of chosen and rejected responses. Finally, the target model is trained by maximizing the difference of the log probability ratios of the target and the reference models for the chosen and rejected answers. However, obtaining these probabilities can be challenging if one wants to use proprietary models like GPT-4 as the reference model, since they do not offer log probabilities for inputs.\n\n\\begin{table}[t!]\n\\centering\n\\resizebox{1.00\\linewidth}{!}{\n\\begin{tabular}{lcc}\n\\toprule\nModel & Reference Model & H4\\\\ \\midrule\nMistral-7B-OpenOrca & N/A & 65.84 \\\\\nMistral-7B-OpenOrca + DPO & SFT Base & 68.87\\\\\nMistral-7B-OpenOrca + DPO & SOLAR-0-70B & 67.86\\\\\nMistral-7B-OpenOrca + DPO & Intel-7B-DPO & {\\bf 70.13}\\\\ \\midrule\nOpenHermes-2.5-Mistral-7B & N/A & 66.10 \\\\\nOpenHermes-2.5-Mistral-7B + DPO & SFT Base & 68.41\\\\\nOpenHermes-2.5-Mistral-7B + DPO & SOLAR-0-70B & 68.90\\\\\nOpenHermes-2.5-Mistral-7B + DPO & Intel-7B-DPO & {\\bf 69.72}\\\\\n\\bottomrule \n\\end{tabular}\n}\n\\caption{DPO results in terms of H4 scores for Mistral-7B-OpenOrca and OpenHermes-2.5-Mistral-7B with different reference models. The best results for each SFT base model are shown in bold.}\n\\label{tab:mistral_openorca}\n\\end{table}\n\nThus, in practice, the reference model is simply set as the base SFT model~\\cite{tunstall2023zephyr, intel2023orcadpo, ivison2023camels}, which is a much weaker alternative with potentially misaligned preferences.\nThrough Eq.~\\ref{eq:dpo_loss}, we show that the reference model acts as {\\it a lower bound} in DPO, \\textit{i.e.,} the target model is optimized to be at least as aligned as the reference model.\nThus, we argue that a reference model that is already more aligned will serve as a better lower bound for DPO training, which would be beneficial for the alignment tuning.\nOne option would be to utilize the plethora of open source models~\\cite{tunstall2023zephyr, ivison2023camels} that have already undergone alignment tuning.\n\n\\begin{figure*}[t!]\n    \\centering\n    \\resizebox{1.0\\linewidth}{!}{\n\\includegraphics{images/sDPO.pdf}\n    }\n    \\caption{Overview of \\method where preference datasets are divided to be used in multiple steps. The aligned model from the previous step is used as the reference and target models for the current step. The reference model is used to calculate the log probabilities and the target model is trained using the preference loss of DPO at each step.}\n    \\label{fig:method}\n\\end{figure*}\n\nNote that the above approach may not be feasible due to the absence of such aligned models, or the fact that it renounces control over the reference model, which could lead to safety concerns.\nInstead, we propose `stepwise DPO', named \\method, where we use the preference datasets (or subsets of a preference dataset) in {\\it a step-by-step manner} rather than all at once when undergoing DPO training.\nThe aligned model in the previous step is used as the reference model for the current step, which results in utilizing a more aligned reference model ({\\it i.e.}, a better lower bound).\nEmpirically, we show that using \\method results in a more performant final aligned model as well.\n\nWhile concurrent works~\\citep{yuan2024self} that focus on an iterative pipeline of generating \\textit{new} preference data have been proposed, \nour method focuses on utilizing the \\textit{currently available} preference datasets.\nThus, our approach is complementary as \\method can be easily applied to any preference data and further combination with concurrent works would be an exciting future direction.\n\n\\section{Related Work}\n\n\\subsection{Large Language Models}\nRecent research has highlighted a \"scaling law\" in the field of context-based language models~\\cite{kaplan2020scaling, hernandez2021scaling, anil2023palm}, showing a proportional relationship between the size of the model plus the training data and the resulting performance improvements. Consequently, this has led to the advent of LLMs. In contrast to earlier models, LLMs can perform in-context learning, which includes abilities such as zero-shot learning~\\cite{radford2019language} and few-shot learning~\\cite{brown2020language}, allowing them to adapt and perform tasks without the need for weight adjustments. These emergent abilities of LLMs, absent in their smaller counterparts, signal a significant evolution in language model capabilities~\\cite{wei2022emergent}.\n\n\\subsection{Alignment Tuning}\nLLMs have been recognized to produce text that may seem linguistically inconsistent to human interpreters because their pretraining is based not on an understanding of human intentions but on a broad spectrum of domain-specific knowledge, as indicated in~\\cite{ziegler2019fine}. In an effort to rectify this issue and better mirror human intentions, prior research~\\cite{ziegler2019fine} has suggested the adoption of Reinforcement Learning with Human Feedback (RLHF). RLHF seeks to refine the LLM's output by constructing a reward model that aligns with human preferences and applying reinforcement learning to direct the LLM towards selections that garner the most favorable reward metrics. This approach is intended to bolster the safety, decorum, and general excellence of the responses produced by the LLM. Nonetheless, despite showing promising results, RLHF is confronted with challenges, such as the intricate handling of an extensive set of hyperparameters and the necessity to amalgamate several models (policy, value, reward, and reference models).\n\nTo address these issues, there have been proposals for supervised fine-tuning methodologies such as RRHF~\\cite{yuan2023rrhf}, RAFT~\\cite{dong2023raft}, and DPO~\\cite{rafailov2023direct}. These methods circumvent the intricacies inherent in reinforcement learning and have been shown to yield empirical results on par with RLHF. Notably, the DPO technique straightforwardly encourages the LLM to favor positive responses and discourage negative ones. DPO has been observed to yield performant learning outcomes, in spite of its uncomplicated training procedure.\n\nConcurrent to our work, \\citet{yuan2024self} have developed an iterative framework for generating \\textit{new} preference datasets and performing DPO training on the resulting datasets. They empirically demonstrated the superiority of their iterative framework in terms of AlpacaEval 2.0.\nIn contrast, our work is complementary to the above in the sense that we focus on utilizing the \\textit{current} preference data and does not undergo new data generation. \nThus, our method can also be applied to \\citet{yuan2024self} by changing the DPO training part to using sDPO instead. \nAdditionally, the evaluation used in \\citet{yuan2024self} is also different to ours as we utilize tasks from Open LLM Leaderboard~\\cite{open-llm-leaderboard}, EQ Bench~\\cite{paech2023eq} and MT Bench~\\cite{zheng2023judging} whereas \\citet{yuan2024self} uses AlpacaEval 2.0.\n\n\\section{Methodology}\n\n\\subsection{Preliminary Investigation on Reference Models}\nTo gauge the importance of using a well-aligned reference model in DPO, we perform preliminary experiments of DPO training with the Ultrafeedback dataset~\\cite{cui2023ultrafeedback} on Mistral-7B-OpenOrca~\\cite{lian2023mistralorca1} and OpenHermes-2.5-Mistral-7B~\\cite{openhermes} as the SFT base model, owing to their excellent performance and small size. We compare the following reference models: i) the SFT base model itself, same as the conventional DPO setup; ii) SOLAR-0-70B~\\cite{solar70}, a larger and much more performant model; and iii) Intel-7B-DPO~\\cite{intel7b}, an already aligned reference model. The results are summarized in Table~\\ref{tab:mistral_openorca}.\n\nAs the table shows, using Intel-7B-DPO as the reference model results in the best performance, even better than using SOLAR-0-70B, which is a much larger and performant model.\nThus, whether the reference model is pre-aligned or not plays an important role in the resulting aligned model's performance.\nUnfortunately, it is not always possible to use a open sourced pre-aligned model as the reference model due to technical and safety concerns. For instance, such a model may not exist yet or can be susceptible to various domain-specific harmfulness and fairness criteria along with potential data contamination issues.\nTo circumvent the above, we propose \\method, which does not require an external pre-aligned model but uses more aligned reference models, built from the SFT base model, as a part of the training framework.\n\n\\subsection{Stepwise DPO}\n\\label{sec:sdpo}\nIn \\method, we propose to use the available preference datasets in a stepwise manner instead of using them all at once.\nEssentially, we partition the preference data into $T$ chunks and perform DPO training $T$ times.\nThe trained model from the previous step is used as the reference and target models, which means that each of the $T$ DPO training steps function in a similar manner to the conventional DPO setup.\nIn doing so, we create and utilize intermediary reference models that are more aligned than those thar are used in conventional DPO.\nThe comparison of the overall flow of DPO and sDPO is presented in Figure~\\ref{fig:method}.\n\\paragraph{Reference model.}\nThe reference model is used to calculate the log probabilities of the preference dataset.\nFor each step, only a subset of the total data is used and the reference model is initialized as $M_{t-1}$, \\textit{i.e,} the aligned model from the previous step.\nThe initial reference model is set as $S$, the SFT base model.\nThis results in using a more aligned reference model than conventional DPO.\n\n\\paragraph{Target model.}\nFor $t > 1$, the target model which is trained using the preference loss of DPO in each step of \\method is also initialized as $M_{t-1}$ instead of $S$.\nThis ensures that the final model trained with \\method has been directly trained with the same amount data as a model trained with DPO.\n\n\\begin{table*}[t!]\n\\centering\n\\resizebox{0.85\\linewidth}{!}{\n\\begin{tabular}{lccccccc}\n\\toprule\nModel & Size & Type  & H4 (Avg.) & ARC & HellaSwag & MMLU & TruthfulQA \\\\ \\midrule\n \\cellcolor{lp!60}SOLAR 10.7B + SFT + \\method &  \\cellcolor{lp!60}$\\sim$ 11B&  \\cellcolor{lp!60}Alignment-tuned& \\cellcolor{lp!60}{\\bf 74.31}  & \\cellcolor{lp!60}{\\bf 71.33} & \\cellcolor{lp!60}88.08& \\cellcolor{lp!60}65.39& \\cellcolor{lp!60}{\\bf 72.45} \\\\\n SOLAR 10.7B + SFT + DPO & $\\sim$ 11B & Alignment-tuned & 72.67 & 69.62 & 87.16 & 66.00 & 67.90 \\\\ \\midrule\n    Mixtral 8x7B-Instruct-v0.1 & $\\sim$ 47B& Alignment-tuned&73.40  &70.22 &87.63&71.16&64.58\\\\\n      SOLAR-0-70B-16bit &  $\\sim$ 70B&  Instruction-tuned& 72.93  & 71.08 & 87.89& 70.58&62.25 \\\\\n  Qwen 72B & $\\sim$ 72B& Pretrained&72.17  &65.19 &85.94&{\\bf 77.37}&60.19\\\\\n      Yi 34B & $\\sim$ 34B& Pretrained&70.72  &64.59 &85.69&76.35&56.23\\\\\n \\cellcolor{lp!60}SOLAR 10.7B + SFT&  \\cellcolor{lp!60}$\\sim$ 11B&  \\cellcolor{lp!60}Instruction-tuned& \\cellcolor{lp!60} 69.51 & \\cellcolor{lp!60}67.32 & \\cellcolor{lp!60}85.96& \\cellcolor{lp!60}65.95& \\cellcolor{lp!60}58.80 \\\\ \nMistral 7B-Instruct-v0.2& $\\sim$ 7B& Instruction-tuned&69.27  &63.14 &84.88&60.78&68.26\\\\\n         Falcon 180B & $\\sim$ 180B& Pretrained&68.57  &69.45 &{\\bf 88.86}&70.50&45.47\\\\\n  Mixtral 8x7B-v0.1 & $\\sim$ 47B& Pretrained&67.78  &66.04 &86.49&71.82&46.78\\\\\n  Llama 2 70B & $\\sim$ 70B& Pretrained&67.35  &67.32 &87.33&69.83&44.92\\\\\nZephyr & $\\sim$ 7B& Alignment-tuned & 66.36 & 62.03 & 84.52 & 61.44 & 57.44 \\\\\nQwen 14B & $\\sim$ 14B& Pretrained&64.85  &58.28 &83.99&67.70&49.43\\\\\n \\cellcolor{lp!60} SOLAR 10.7B &  \\cellcolor{lp!60}$\\sim$ 11B&  \\cellcolor{lp!60}Pretrained& \\cellcolor{lp!60}64.27  & \\cellcolor{lp!60}61.95 & \\cellcolor{lp!60}84.60& \\cellcolor{lp!60}65.48& \\cellcolor{lp!60}45.04 \\\\\n\nMistral 7B& $\\sim$ 7B& Pretrained&62.40  &59.98 &83.31&64.16&42.15\\\\\n\\bottomrule \n\\end{tabular}\n}\n\\caption{Performance comparison of applying \\method or DPO to SOLAR 10.7B + SFT against various top performing models. Size is shown in units of billions of parameters and type is reported as one of \\{`Pretrained', `Instruction-tuned', `Alignment-tuned'\\}. Models based on SOLAR 10.7B are shown in purple color. The best scores in each column are shown in bold.}\n\\label{tab:main_result}\n\\end{table*}\n\n\\paragraph{Mathematical explanation.}\nTo gain a deeper understanding of sDPO, we rearrange the DPO loss from ~\\cite{rafailov2023direct}, as follows:\n\\begin{equation}\n\\resizebox{0.85\\linewidth}{!}{%\n\\begin{math}\n\\begin{aligned}\n     & \\mathcal{L}_{\\text{DPO}}(\\pi_\\theta, \\pi_{ref}) \\\\\n     & = - \\mathbb{E}_{(x, y_w, y_l)\\sim \\mathcal{D}}\\left[\\log{\\sigma\\left(\\beta\\log{{\\pi_\\theta(y_w|x) \\over \\pi_{ref}(y_w|x)}} - \\beta \\log{{\\pi_\\theta(y_l|x) \\over \\pi_{ref}(y_l|x)}}\\right)}\\right] \\\\\n     & = - \\mathbb{E}_{(x, y_w, y_l)\\sim \\mathcal{D}}\\left[\\log{\\sigma\\left(\\beta\\cdot (\\gamma_{\\pi_\\theta}(x, y_w, y_l) - \\gamma_{\\pi_{ref}}(x, y_w, y_l)\\right)}\\right],\n\\end{aligned}\n\\end{math}\n}\n\\label{eq:dpo_loss}\n\\end{equation}\nwhere $D$ is the preference dataset, $x$ is the question, $y_w$ and $y_l$ are the chosen and rejected answers respectively, $\\theta$ is the learnable parameters of the model, and $\\gamma_\\pi(x, y_w, y_l) = \\log{{\\pi(y_w|x) \\over \\pi(y_l|x)}}$, \\textit{i.e.,} the logratio of the chosen and rejected samples w.r.t. the policy $\\pi$.\nAs $\\log\\sigma(\\cdot)$ is a monotonically increasing function and $\\gamma_{\\pi_{ref}}$ is fixed before training, the minimization of $\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta, \\pi_{ref})$ leads to $\\gamma_{\\pi_{\\theta}} > \\gamma_{\\pi_{ref}}$ on average.\nThus, $\\gamma_{\\pi_{ref}}$ can be understood as a lower bound defined by the reference model, of which the target model is trained such that $\\gamma_{\\pi_{\\theta}} > \\gamma_{\\pi_{ref}}$.\nIn sDPO, $\\gamma_{\\pi_{ref}}$ increases as the steps progress because the reference model that defines it is more and more aligned.\nHence, $\\gamma_{\\pi_{ref}}$ becomes a stricter lower bound as the steps pass, inducing a {\\it curriculum learning} from easy to hard optimization tasks.\nThus, the target model is being trained to learn stricter preferences as the steps progress in \\method.\n\n\\paragraph{Data partitioning strategy.}\nThe method for partitioning the preference data into $T$ chunks is also important in \\method.\nOne option would be to pool all the data from different dataset sources and perform random sampling.\nHowever, we argue that partitioning the data such that earlier chunks are comprised of easier preference data would be more aligned with inducing a curriculum learning of easy to hard optimization in \\method.\n\nTo that end, we propose to use easy-to-hard data partitioning by the following method.\nUsing $M_0$, the initial target model, we calculate the reward accuracy, \\textit{i.e.,} the percentage of samples in which the target model scores higher rewards for preferred samples, for the different dataset sources.\nThe dataset sources are sorted in descending order of the reward accuracy, which are then used as the $T$ chunks in \\method.\nThus, if we have $N$ dataset sources, we would have a total of $N$ chunks, where earlier chunks would contain easier samples as measured by the reward accuracy.\n\n\\section{Experiments}\n\\subsection{Experimental Setup}\n\\paragraph{Training details.} We use a supervised fine-tuned SOLAR 10.7B~\\cite{kim2023solar} as our SFT base model $S$ as it delivers excellent performance with its uncommon yet relatively small 10.7B size.\nNote that we do not need a separate reference model as it is initialized as $M_{t-1}$, the final trained model from the previous step. \nWe use OpenOrca~\\cite{mukherjee2023orca} ($\\sim12K$ samples) and Ultrafeedback Cleaned ($\\sim60K$ samples)~\\cite{cui2023ultrafeedback, ivison2023camels} as our preference datasets.\nThe training hyper-parameters follow that of~\\citet{tunstall2023zephyr}.\nUsing the easy-to-hard partitioning, we use OpenOrca as dataset $D_1$ and Ultrafeedback Cleaned as dataset $D_2$.\n\n\\paragraph{Evaluation.} We mainly utilize four log-probability tasks in the HuggingFace Open LLM Leaderboard~\\cite{open-llm-leaderboard}: ARC~\\cite{clark2018think}, HellaSWAG~\\cite{zellers2019hellaswag}, MMLU~\\cite{hendrycks2020measuring}, TruthfulQA~\\cite{lin2022truthfulqa}.\nWe also report the average scores for the four tasks, which is denoted as H4.\nNote that these tasks do not require the model to actually generate a new answer to the question.\nRather, the log-probability of a pre-defined answer is measured instead.\n\nTo augment the above potential downside of log-probability benchmarks, we also incorporate generation benchmarks such as EQ Bench~\\cite{paech2023eq} and MT Bench~\\cite{zheng2023judging}, where a model is prompted to generate an answer to a question.\nAs such, MT Bench and EQ Bench both strongly correlate with the Chatbot Arena ELO~\\cite{zheng2023judging,chiang2024chatbotarenaopenplatform}, one of the most widely recognized open-world LLM evaluation system.\n\n\\subsection{Main Results}\nEvaluation results for applying \\method to the SFT base model, along with results for other top-performing models are shown in Table~\\ref{tab:main_result}.\nApplying \\method on SOLAR 10.7B + SFT further increases the H4 score to $74.31$, an improvement of $+4.80$.\nNotably, `SOLAR 10.7B + SFT + \\method' outperforms other larger models such as Mixtral 8x7B-Instruct-v0.1, despite the smaller number of parameters.\nThis highlights that effective alignment tuning could be the key to unlocking next level performance for smaller LLMs.\nFurther, applying \\method results in substantially higher score of $72.45$ for TruthfulQA, which demonstrates the effectiveness of the alignment tuning process. \nWe also present additional results in Table~\\ref{tab:additional_main_result} of Section~\\ref{sec:additional_results} on the EQ Bench~\\cite{paech2023eq}, which is a generation task with high correlation with the Chatbot Arena ELO~\\cite{zheng2023judging}.\nThe additional results indicate the superiority of \\method over DPO in improving generation task performance as well.\n\n\\begin{figure}[t!]\n    \\centering\n    \\resizebox{1.0\\linewidth}{!}{\n    \\includegraphics{images/logratio.pdf}\n    }\n    \\caption{Mean $\\gamma_{\\pi_{ref}}$ on Ultrafeedback Cleaned dataset for different reference models $S,M_1,$ and $M_2$. Note that the x-axis is in log scale.}\n    \\label{fig:logratio}\n\\end{figure}\n\\subsection{Ablation Studies Against DPO}\nWe also report evaluation results for ablating \\method with traditional DPO in Table~\\ref{tab:main_result}.\n`SOLAR 10.7B + SFT + DPO' uses all the DPO data at once, \\textit{i.e.,} $D_1 + D_2$, same as the conventional DPO training setup.\n\nWe can see that using \\method over DPO results in a higher H4 score overall, with noticeable improvements in ARC and TruthfulQA scores.\nTherefore, we believe \\method could function as a drop-in replacement for DPO training with better performance.\n\n\\subsection{Reference Models in \\method}\n\n\\begin{figure}[t!]\n    \\centering\n    \\resizebox{1.0\\linewidth}{!}{\n    \\includegraphics{images/loss.pdf}\n    }\n    \\caption{Loss curve comparison in step 2 of \\method for different initializations of the target model.}\n    \\label{fig:init}\n\\end{figure}\n\n\\paragraph{Effectiveness of \\method in terms of alignment tuning.}\nIn Sec.~\\ref{sec:sdpo}, we explain that the reference models in \\method are more aligned, resulting in higher $\\gamma_{\\pi_{ref}}$, \\textit{i.e.,} a stricter lower bound.\nWe verify the above empirically in Figure~\\ref{fig:logratio} by comparing the mean $\\gamma_{\\pi_{ref}}$ on the Ultrafeedback Cleaned dataset for the reference models in steps 1 and 2 of \\method, \\textit{i.e.,} $S$ and $M_1$.\nNote that these two models have not been trained on the aforementioned dataset.\nUsing the SFT base model $S$ as the reference model, the mean of $\\gamma_{\\pi_{ref}}$ is $-38.60$.\nOn the other hand, using the aligned model $M_1$ from step 1 of \\method as the reference model, the mean of $\\gamma_{\\pi_{ref}}$ is $-25.10$, an increase of $13.50$ in \\textit{log scale}.\nThus, a single step of \\method greatly increases $\\gamma_{\\pi_{ref}}$, which results in a more performant aligned model as seen in Table~\\ref{tab:main_result}.\n\n\\paragraph{Adopting open source models as reference models could be dangerous.}\nWe also show mean $\\gamma_{\\pi_{ref}}$ of $M_2$, the aligned model from step 2 of \\method.\nUnlike $S$ and $M_1$, $M_2$ is trained on the Ultrafeedback Cleaned dataset, \\textit{i.e.,} $M_2$ is used as a reference model on data that was {\\it already used to train it}.\nNote that such a case could happen commonly when adopting various open source models as reference models.\nThis is because the datasets that were used in training those models are often unclear and could overlap with the preference datasets unintentionally.\nMean $\\gamma_{\\pi_{ref}}$ of $M_2$ is $84.35$, which is staggeringly higher than either $S$ or $M_1$. \nThe strikingly high value for $M_2$ likely points to overfitting of $M_2$ to the Ultrafeedback Cleaned dataset.\nNote that utilizing such an absurdly high value of $\\gamma_{\\pi_{ref}}$ as the lower bound in DPO training may be undesirable.\nThis result highlights the potential danger of merely adopting open source models as reference models instead of using \\method.\n\n\\begin{table*}[t!]\n\\centering\n\\resizebox{0.8\\linewidth}{!}{\n\\begin{tabular}{lccccc}\n\\toprule\nModel & H4 (Avg.) & ARC & HellaSwag & MMLU & TruthfulQA \\\\ \\midrule\n \\cellcolor{lp!60} SOLAR 10.7B + SFT + \\method & \\cellcolor{lp!60}{\\bf 74.31}  & \\cellcolor{lp!60}{\\bf 71.33} & \\cellcolor{lp!60}{\\bf88.08}& \\cellcolor{lp!60}65.39& \\cellcolor{lp!60}{\\bf 72.45} \\\\\n SOLAR 10.7B + SFT + \\method Rand. & 72.56 & 69.20 & 87.27 & {\\bf65.96} & 67.81 \\\\\n\\bottomrule \n\\end{tabular}\n}\n\\caption{Comparison between the easy-to-hard and random partitioning strategies. `SOLAR 10.7B + SFT + \\method' uses the easy-to-hard partitioning whereas `SOLAR 10.7B + SFT + \\method Rand.' denotes \\method with random partitioning instead. Easy-to-hard partitioning is better than random partitioning. The best scores are shown in bold.}\n\\label{tab:easy2hard}\n\\end{table*}\n\\subsection{Target Model Initialization in \\method}\nOne option for target model initialization in \\method is to use $S$, the initial SFT base model, for {\\it all steps}.\nHowever, such initialization results in the final model trained with \\method seeing less data than using DPO instead.\nFurther, the target model and the reference model become more and more different as the steps progress, which is a deviation from the original DPO setup and risks losing the theoretical benefits of DPO.\n\nTo concretely investigate such potential issues, we visualize the loss curves for initializing the target model as $S$ in Figure~\\ref{fig:init}.\nWe observe that the initial loss value is much higher when compared to initializing the target model as $M_{t-1}$, \\textit{i.e.,} the same as the reference model and adhering to the DPO convention.\nAs using $M_{t-1}$ the target model means that each {\\it step} of \\method is using the same setup as DPO, the loss curves are much more stable and desirable.\nThus, for stable training, initializing the target model as $M_{t-1}$ was chosen for \\method.\n\n\\subsection{Easy-to-Hard Data Partitioning}\n\nThe effectiveness of the easy-to-hard data partitioning used in \\method is demonstrated in Table~\\ref{tab:easy2hard}.\nNote that we use OpenOrca as $D_1$ and Ultrafeedback Cleaned as $D_2$.\nAs `SOLAR 10.7B + SFT + \\method', which uses the easy-to-hard partitioning, is more performant than`SOLAR 10.7B + SFT + \\method Rand.', which uses random partitioning, the proposed easy-to-hard data partitioning is more effective for \\method.\n\n\\subsection{Additional Results on Generation Tasks}\n\\label{sec:additional_results}\n\n\\begin{table}[t!]\n\\centering\n\\resizebox{1.0\\linewidth}{!}{\n\\begin{tabular}{lccc}\n\\toprule\nModel & EQ Bench & MT Bench\\\\ \\midrule\n \\cellcolor{lp!60}SOLAR 10.7B + SFT + \\method & \\cellcolor{lp!60}{\\bf 68.83} &\\cellcolor{lp!60}{\\bf  7.43}\\\\\n SOLAR 10.7B + SFT + DPO & 61.02 & 7.35\\\\ \\midrule\n SOLAR 10.7B + SFT&  60.48 & 7.14\\\\\n\\bottomrule \n\\end{tabular}\n}\n\\caption{Additional results on EQ Bench~\\cite{paech2023eq} and MT Bench~\\cite{zheng2023judging}, both of which are generation tasks that highly correlate with the Chatbot Arena ELO~\\cite{zheng2023judging,chiang2024chatbotarenaopenplatform}. The best scores for both benchmarks are shown in bold.}\n\\label{tab:additional_main_result}\n\\end{table}\n\nIn Table~\\ref{tab:additional_main_result}, we also report results for EQ Bench~\\cite{paech2023eq} and MT Bench~\\cite{zheng2023judging} for the SFT base model and the models obtained by applying DPO and \\method on the SFT base model.\n\nFor EQ Bench, we use the version without the revision prompt.\nWe note that the EQ Bench requires the models to generate an answer that can be parsed with a pre-defined template for evaluation, which could be said to measure distinct capabilities of LLMs from the log-probability benchmarks shown in Table~\\ref{tab:main_result}.\nWhile applying DPO only mildly improves the performance from the SFT base model, applying \\method improves the performance significantly by over $+8\\%$, indicating the effectivenss in which \\method improves the generation capabilities compared to DPO.\n\nAs for MT Bench, we note that using \\method achieves the best score of 7.43 amongst the compared models.\nNotably, applying \\method to the SFT base model improves the MT Bench score by a non-trivial margin of $+0.29$.\nApplying DPO to the SFT base model also improves the MT Bench score, but not by more than that of applying \\method.\n\n\\section{Conclusion}\n\nWe propose \\method, an extension of DPO for aligning LLMs. Unlike traditional DPO, \\method employs a stepwise approach, using subsets of preference data sequentially. This method leverages the aligned model from the previous step as the reference for the current step, ensuring progressively better alignment. Our experiments demonstrate that \\method significantly outperforms conventional DPO in terms of both log-probability benchmarks such as ARC, HellaSWAG, MMLU, and TruthfulQA, as well as generation benchmarks such as EQ Bench and MT Bench. Additionally, \\method enhances model alignment, as indicated by higher mean $\\gamma_{\\pi_{ref}}$ values, showing improved alignment with human preferences. The stepwise nature of \\method simplifies the training process and aligns with curriculum learning principles, facilitating a structured optimization path. By using existing preference datasets more effectively, \\method results in higher performance and better-aligned language models. This approach has the potential to transform alignment tuning, offering a robust framework for future research in LLMs.\n\n\\section*{Limitations}\nWhile we have demonstrated the effectiveness of employing easy-to-hard data partitioning of different datasets in distinct stages of \\method, identifying a more performant strategy for segmenting more intricate preference data collections remains an area for further exploration.\n\nFurthermore, our experiments predominantly utilized SOLAR 10.7B models, driven by the state-of-the-art performance at the time of experimentation along with its relatively 10.7 billion parameter size.\nAlthough as SOLAR 10.7B models are also based on the Llama-2 architecture with our results likely to transfer to other similar decoder only transformer models, more experiments using other models would be beneficial.\n\nAdditionally, as with most research on LLMs, we operated within our limitations in computational resources. Although this focus has yielded significant insights, expanding our experimental framework to incorporate a broader range of Large Language Models (LLMs) could potentially unveil more comprehensive understanding of the strengths and limitations of \\method. Such an expansion would allow for a more robust comparison across different model architectures and sizes, further enriching our findings.\n\nEvaluating the efficacy of LLMs is an evolving challenge in the field. In our study, we primarily employed tasks from the Huggingface Open LLM Leaderboard as benchmarks for evaluation along with EQ Bench and MT Bench. While this provided comparative results, future research could benefit from incorporating a wider array of tasks and benchmarks. These could include tasks that judge actual human or strong AI preference alignment. Such additional evaluation would not only enhance the validity of our findings but also contribute to the broader discourse on LLM assessment methodologies.\n\n\\section*{Ethics Statement}\nIn this study, we strictly adhered to ethical standards in the conduct of our research. Our experiments were based entirely on open models and open datasets, ensuring transparency and accessibility. We took meticulous care to avoid any biases or data contamination, thereby maintaining the integrity of our research process. The experimental environment was rigorously designed to be objective, ensuring that all comparisons conducted were fair and impartial. This approach reinforces the reliability and validity of our findings, contributing positively to the field while upholding the highest ethical standards. We confirmed that all the data used in our experiments were free of licensing issues.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{A Novel Approach to Direct Preference Optimization: Dynamic $\\beta$ and Quality-Aware Data Filtering}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nDirect Preference Optimization (DPO) has emerged as a compelling approach for training Large Language Models (LLMs) to adhere to human preferences. However, the performance of DPO is sensitive to the fine-tuning of its trade-off parameter $\\beta$, as well as to the quality of the preference data. We analyze the impact of $\\beta$ and data quality on DPO, uncovering that optimal $\\beta$ values vary with the informativeness of pairwise data. Addressing the limitations of static $\\beta$ values, we introduce a novel framework that dynamically calibrates $\\beta$ at the batch level, informed by data quality considerations. Additionally, our method incorporates $\\beta$-guided data filtering to safeguard against the influence of outliers. Through empirical evaluation, we demonstrate that our dynamic $\\beta$ adjustment technique significantly improves DPO's performance across a range of models and datasets, offering a more robust and adaptable training paradigm for aligning LLMs with human feedback. The code is available at \\url{https://github.com/junkangwu/beta-DPO}.\n\\end{abstract}\n\n\\section{Introduction}\n\\label{Introduction}\nThe alignment of Large Language Models (LLMs) with human feedback, as explored in works like GPT-4 and LLaMA-2 \\cite{GPT4,llama2,GPT4_2}, has marked a significant advancement in generating responses that are more helpful, factual, and ethical \\cite{instructGPT}. Among the various alignment strategies, Reinforcement Learning from Human Feedback (RLHF) \\cite{instructGPT} is a notable method that refines LLMs using the Proximal Policy Optimization (PPO) algorithm \\cite{PPO}. This approach employs a KL divergence penalty to ensure minimal deviation of the model from its original configuration, ensuring the retention of its initial characteristics while improving alignment.\n\nDespite the effectiveness, RLHF's instability and computational requirements often limit its practical applications, prompting the exploration of alternatives like Direct Preference Optimization (DPO) \\cite{DPO}. DPO circumvents the reinforcement learning loop by exploiting the inherent connection between reward functions and optimal policies, thereby simplifying the policy model training. It encourages the model to favor the response that aligns with human preferences ($\\yb_{w}$) over the dispreferred ($\\yb_{l}$), \\wjc{implying DPO's sensitivity to the quality of pairwise data}. \nThe balance between maintaining the original reference model ($\\pi_{\\text{ref}}$) and incorporating new preferences ($\\pi_{\\btheta}$) is controlled by a $\\beta$ hyperparameter, whose lower values advocate for aggressive updates and higher values support more conservative adjustments:\n\\begin{align*}\n    \\ell_{\\text{DPO}}(\\btheta)=\\EE_{\\xb,\\yb_{w},\\yb_{l}} [-\\log \\sigma \\big(\n    \\beta \\big[\n    \\log \\big(\\frac{\\pi_{\\btheta}(\\yb_{w}|\\xb)}{\\pi_{\\text{ref}}(\\yb_{w}|\\xb)}\\big)\n    -\n    \\log \\big(\\frac{\\pi_{\\btheta}(\\yb_{l}|\\xb)}{\\pi_{\\text{ref}}(\\yb_{l}|\\xb)}\\big)\n    \\big]\n    \\big)].\n\\end{align*}\nHowever, the current DPO literature has largely overlooked the joint influence of $\\beta$ selection and pairwise data $(\\yb_{w}, \\yb_{l})$'s quality on the alignment performance.\n\\wjc{To bridge this gap, we conduct a preliminary experiment to investigate how different $\\beta$ selections influence the model performance under two distinct data pair gap conditions, as shown in Figure \\ref{fig_1_2}.}\nIn \\emph{low gap} scenarios (\\cf Figure \\ref{fig_1_1}),\nwhere the difference between preferred and dispreferred samples is minor, an increase in $\\beta$ (\\eg from 0.1 to 0.5) corresponds with a decline in win rate (\\eg from 42\\% to 33\\%).\nConversely, in \\emph{high gap} situations (\\cf Figure \\ref{fig_1_1})\nwhere a significant difference exists, an increase in $\\beta$ tends to improve DPO performance.\n\\wjc{Such contrasting outcomes highlight the necessity to tailor the $\\beta$ value contingent upon the data quality, especially in the presence of outliers \\cite{outlier_dataset}.}\n\\begin{figure}\n    \\centering\n    \\begin{subfigure}{.65\\textwidth}\n      \\centering\n      \\includegraphics[width=\\linewidth]{figs/first_fig_a.pdf}\n      \\caption{} % 你可以在这里添加子图标题\n      \\label{fig_1_1}\n    \\end{subfigure}%\n    \\hfill % 这会在子图之间添加一些空间\n    \\begin{subfigure}{.3\\textwidth}\n      \\centering\n      \\includegraphics[width=\\linewidth]{figs/NeurIPS_first_fig.pdf}\n      \\caption{} % 同样，为第二个子图添加标题\n      \\label{fig_1_2}\n    \\end{subfigure}\n    \\caption{\n    \\textbf{(\\ref{fig_1_1}) Pairwise Data: Low vs. High Gap}: ``Low gap'' denotes cases where the chosen and rejected examples are closely similar, typically indicating high-quality, informative pairs. ``High gap'' signifies pairs with larger differences, implying lower-quality data.\n    \\textbf{(\\ref{fig_1_2}) Influence of Data Quality on $\\beta$ Selection:} Pythia-1.4B's performance on the HH dataset reveals a distinct trend: for ``Low gap'', a higher $\\beta$ reduces win rate, whereas for ``High gap'', an increased $\\beta$ improves it.}\n    \\label{fig:combined}\n\\end{figure}\n\nBuilding upon these insights and recognizing the mixture nature of diverse quality in practically collected data \\cite{mixture_data},\nwe propose a dynamic $\\beta$ selection strategy for DPO. This strategy adaptively adjusts $\\beta$ in response to the quality of pairwise data and robustifies DPO against data variability. \nIntuitively, one straightforward solution is personalizing $\\beta$ for each pair, rather than fixing it across the population of all pairs. While conceptually appealing, (1) instance-level $\\beta$ personalization can lead to optimization instabilities, particularly when dealing with a vast array of human preference instances \\wjc{(\\cf Section \\ref{sec_batch_level})}. This issue underscores the challenge of balancing $\\beta$ updates with stable and scalable DPO. In this light, we propose to explore a \\textit{batch-level} adaptation of $\\beta$, aiming to balance update aggressiveness and training stability. Moreover, (2) the frequent occurrence of outliers necessitates a strategy for accurately adjusting the batch-level $\\beta$. \nThe dataset notably features outliers, a challenge underscored by the significant reward discrepancy variations within the training samples of the dataset (\\cf Section \\ref{motivation_sec}). \nSuch conditions impede the model's ability to accurately estimate the batch-level $\\beta$, thereby undermining the effectiveness of batch-level $\\beta$ calibration.\nTo this end, we propose a simple yet effective dual-component approach:\n\n\\begin{itemize}[leftmargin=*]\n  \\item \\textbf{Dynamic $\\beta$ Calibration at Batch-Level} (for Challenge 1): To mitigate optimization instabilities, we dynamically calibrate $\\beta$ within each batch. Specifically, this batch-level adjustment is based on data quality, with $\\beta$ being adaptively decreased for high-quality, closely-matched pairwise data (\\ie \\emph{low gap} data) to facilitate assertive updates. While for easily-discriminated pairs (\\ie \\emph{high-gap} data), $\\beta$ is increased to promote cautious updates, preventing overfitting to noise.\n  This targeted batch-level calibration enables stable and responsive optimization.\n  \\item \\textbf{$\\beta$-Guided Data Filtering} (for Challenge 2): We implement a $\\beta$-guided data filtering approach to tackle the frequent occurrence of outliers. By establishing a benchmark $\\beta$ value for filtering incoming data at the batch level, we maintain the fidelity of $\\beta$ estimation by prioritizing the most reliable and representative samples. As a result, it diminishes the impact of outliers that might otherwise derail the optimization process, thus enhancing the precision and robustness of the batch-level $\\beta$ calibration.\n\\end{itemize}\n\nOur contributions are threefold: (1) We investigate a pioneering study on the joint influence of $\\beta$ selection and pairwise data quality on the DPO performance. (2) We introduce a simple yet effective $\\beta$-dynamic strategy for DPO, adaptively balancing the update aggressiveness and training stability. (3) Through empirical evaluations, our approach demonstrates marked improvements in performance across diverse conditions and model sizes\n(\\eg achieving improvements exceeding 10\\% on models of various sizes, including Pythia-410M, 1.4B, and 2.8B).\n\n\\section{Related Work}\n\\label{Related Work}\n\\textbf{Reinforcement Learning from Human Feedback.}\nDespite RLHF's effectiveness in aligning language models (LMs) with human values \\cite{ChristianoLBMLA17, Bai2022training, llama2, instructGPT, fangjf2024neuron, FangZW00LWD024}, its complexity and resource demands have spurred the exploration of alternatives. RAFT \\cite{RAFT} selects optimal training samples via an existing reward model, whereas RRHF \\cite{RRHF} employs a simpler ranking loss, retaining PPO's efficiency. Diverse from these, DPO \\cite{DPO} directly optimizes LMs using a preference-based loss function, showcasing enhanced training stability in comparison to traditional RLHF. Innovatively, SLiC-HF \\cite{SLiC-HF} and KTO \\cite{KTO} devise loss functions rooted in human decision-making, focusing on preference calibration and utility optimization, respectively. Dr. DPO \\citep{drdpo} consider robust settings where safety or group information is known at training time. Further, RSO \\cite{RSO} and ORPO \\cite{ORPO} introduce efficient preference modeling and optimization, with ORPO uniquely combining supervised fine-tuning and preference alignment. These advancements reflect the ongoing shift towards more efficient, nuanced RL methods.\n\n\\textbf{Data Quality in LLM's Alignment.}\nRecent studies have increasingly recognized the significance of data quality in the alignment of LLMs. For example, LIMA \\cite{LIMA} leverages heuristics such as post scores, response lengths, formatting, and topics to manually craft 1000 high-quality datasets from StackExchange, wikiHow, and Reddit for superficial alignment. In a similar vein, \\citet{Bai2022training} prioritize data points based on user engagement levels for dataset assembly. Rejection Sampling (RS) and Best-of-$N$ (BoN) techniques, as evidenced in the works of \\citet{WebGPT} and \\citet{Gao23ICML}, involve selecting the optimal candidate from $N$ generated possibilities through the application of a reward model. To enhance preference optimization, RSO \\cite{RSO} uses statistical weightings to differentiate outcomes from an optimal policy and a base SFT policy. Besides, fDPO \\cite{morimura2024filtered} employs a Reward Model to filter out low-quality data, effectively addressing dataset quality concerns.\n\n\\label{Preliminaries}\nGiven a text sequence (commonly referred to as a prompt) $\\xb$, a sequence $\\yb = [y_1, y_2, \\dots y_N]$ is generated as a response to the prompt $\\xb$.  \nAn autoregressive language model $\\pi$, when provided with the prompt $\\xb$, can generate the response sequence $\\yb$ following the probability decomposition:\n\\begin{equation}\n    \\pi(\\yb|\\xb) = \\prod_{t=1}^{N}\\pi(y_i|\\xb, \\yb_{<t}),\n\\end{equation}\nwhere $\\yb_{<t}$ denotes the preceding tokens in the response sequence.\nNow, given a preference dataset $\\Set{D} = \\{ (\\xb^{(i)}, \\yb_w^{(i)}, \\yb_l^{(i)}) \\}_{i=1}^{M}$, wherein each triplet consists of a prompt $\\xb$ with two responses $\\yb_w\\in\\Sigma^{*}$ and $\\yb_l\\in\\Sigma^{*}$, with $\\Sigma^{*}$ representing the alphabet, a preference oracle --- either a human annotator or a language model --- provides preference feedback $o(\\yb_w \\succ \\yb_l |\\xb) \\in \\{0,1\\}$, indicating whether $\\yb_w$ is preferred over $\\yb_l$. We denote $\\mathbb{P}(\\yb_w \\succ \\yb_l| \\xb) = \\mathbb{E}[o(\\yb_w \\succ \\yb_l|\\xb)]$ the probability of $\\yb_w$ ``winning the duel'' over $\\yb_l$. The Kullback-Leibler (KL) divergence between two probability distributions with densities $p$ and $q$ is defined as $\\mathrm{KL}(p \\| q) = \\mathbb{E}_{\\yb \\sim p(\\yb)} \\Big[\\log \\frac{p(\\yb)}{q(\\yb)} \\Big]$.\n\n\\textbf{RLHF with Reward Models.} \\citet{ChristianoLBMLA17} pioneer the learning of a reward function $r(\\yb ;\\xb)$ based on the Bradley-Terry model~\\citep{bradley1952rank}. This model is deployed for \nthe triplet of a prompt ($\\xb$) and two responses ($\\yb_w,\\yb_l$), establishing the likelihood of preference for $\\yb_w$ over $\\yb_l$ as:\n\\begin{align}\n    \\mathbb{P}(\\yb_w \\succ \\yb_l | \\xb)\n    & = \n    \\frac{\\exp(r(\\yb_w; \\xb))}{\\exp(r(\\yb_w; \\xb)) + \\exp(r(\\yb_l; \\xb))}\n    =\n    \\sigma \\big(r(\\yb_w; \\xb)-r(\\yb_l; \\xb)\n    \\big),\n\\end{align}\nwhere $\\sigma(x) = e^x / (e^x + 1)$ represents the logistic function. The approach for estimating the reward function within the Bradley-Terry framework is to maximize the log-likelihood $\\log \\mathbb{P}(\\yb_w \\succ \\yb_l | \\xb)$. Assuming accurate estimation of the true reward function $r(\\yb; \\xb)$, \\citet{ChristianoLBMLA17} propose to solve the following problem with policy optimization algorithms in RL such as PPO \\citep{PPO}:\n\n\\begin{align}\n    \\max_{\\btheta} \n    \\EE_{\\xb \\sim \\cX, \\yb \\sim \\pi_{\\btheta}(\\cdot|\\xb)}\n    [\n    r(\\yb; \\xb)\n    ]\n    -\n    \\beta\n    \\mathbb{E}_{\\xb \\sim \\cX}\n    [\\mathrm{KL}(\\pi_{\\btheta}(\\cdot|\\xb) \\| \\pi_{\\text{ref}}(\\cdot|\\xb))],\n\\end{align}\nwhere $\\cX$ represents the prompt distribution,\n$r(\\yb; \\xb)$ denotes the reward function learned using the Bradley-Terry model on the preference dataset,\n$\\pi_{\\text{ref}}$ is the fixed reference model (typically selected to be the one post supervised fine-tuning), and $\\beta$ serves as the penalty coefficient of the KL divergence.\n\n\\textbf{Directed Preference Optimization (DPO).}\n\\citet{DPO} identify that the optimization problem above has a closed-form solution such that for any $\\yb$,\n\\begin{align*}\n    \\pi^*(\\yb|\\xb)\n    \\propto \n    \\pi_{\\text{ref}}(\\yb|\\xb)\n    \\exp( r(\\yb; \\xb) / \\beta),\n\\end{align*}\nwhich can be further converted to the DPO loss for any triplet $(\\xb, \\yb_{w}, \\yb_{l})$:\n\\begin{equation}\n    \\ell_{\\text{DPO}}(\\xb, \\yb_{w}, \\yb_{l}; \\btheta; \\pi_{\\text{ref}})\n    = \n    -\\log \\sigma \\Bigg(\n    \\beta \\bigg[\n    \\log \\bigg(\\frac{\\pi_{\\btheta}(\\yb_{w}|\\xb)}{\\pi_{\\text{ref}}(\\yb_{w}|\\xb)}\\bigg)\n    -\n    \\log \\bigg(\\frac{\\pi_{\\btheta}(\\yb_{l}|\\xb)}{\\pi_{\\text{ref}}(\\yb_{l}|\\xb)}\\bigg)\n    \\bigg]\n    \\Bigg).\n    \\label{eq:DPO2}\n\\end{equation}\n\\section{Method}\n\\label{our_work}\nIn this section, we investigate the critical connection between the parameter $\\beta$ and the quality of pairwise data in optimizing DPO. We present empirical evidence demonstrating the effect of $\\beta$ settings on DPO performance across datasets of varying quality. Our proposed method, $\\beta$-DPO, introduces dynamic calibration of $\\beta$ and a data filtering mechanism tailored to improve DPO's effectiveness across diverse data conditions.\n\\subsection{Motivation: The Impact of Pairwise Data Quality on $\\beta$ Selection}\n\\label{motivation_sec}\nScrutinizing Equation \\eqref{eq:DPO2}, we argue that DPO's effectiveness critically hinges on two factors: the choice of $\\beta$ and the quality of pairwise data. Here, we conduct experiments to demonstrate the influence of variations in $\\beta$ and data quality on DPO, pivotal for its effective real-world application.\n\n\\textbf{Datasets.} \nWe utilize the Anthropic HH dataset \\cite{Bai2022training} for our experimental analysis, which contains approximately 170,000 dialogues between humans and an automated assistant. In this dataset, a human inquiry, denoted as $\\xb$, is paired with two responses $(\\yb_w, \\yb_l)$, where $\\yb_w$ represents the response favored by the human annotator, while $y_l$ is the alternate response. Notably, the alternate response $y_l$ retains informational value, making this dataset high-quality with minimal discrepancies between the response pairs, which we classify as a \\emph{low gap} dataset.\nTo further explore the impact of data quality on DPO, we construct a synthetic dataset, referred to as the \\emph{high gap} dataset. This dataset differs from the \\emph{low gap} dataset by introducing a greater disparity between responses. Specifically, the alternative response $y_l$ is generated by a Supervised FineTuned (SFT) Pythia-2.8B model, while the preferred response $y_w$ remains consistent with the original dataset.\nWe also combine the two datasets in equal proportion to create a \\emph{mixed gap} dataset, with each contributing 50\\%, to incorporate the characteristics of both the \\emph{low gap} and \\emph{high gap} datasets.\n\n \\begin{figure}[t]\n    \\vspace{-10pt}\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{figs/low_gap_mix_high_gap.pdf}\n    \\caption{Win rate performance of DPO across different $\\beta$ settings on the \\emph{low gap}, \\emph{mixed gap}, and \\emph{high gap} datasets.}\n    \\label{fig_low_high_gap}\n    \\vspace{-10pt}\n \\end{figure}\n \n\\textbf{Models and Metrics.} \nOur study evaluates various model sizes, specifically Pythia-410M, Pythia-1.4B, and Pythia-2.8B \\cite{pythia}, to ensure a comprehensive assessment. Following the established protocol in DPO \\cite{DPO}, each model iteration undergoes a single epoch with a batch size of 64. This setup provides a uniform basis for evaluation across different models.\nWe adopt the evaluation strategy from DPO \\cite{DPO} to calculate the \\textit{win rate}, a metric that measures how often the GPT-4 model prefers a response generated by our models over the default chosen response on the subset of the test dataset.\n\n\\textbf{Findings}: \n\\textbf{(1) The optimal value of $\\beta$ varies with data quality, reflecting divergent performance patterns across datasets.}\nIn Figure \\ref{fig_low_high_gap}, we present the win rate results across three levels of pairwise data gap, each evaluated under varying $\\beta$ parameters. As can be observed,\nwith \\emph{low gap} pairwise data, a smaller $\\beta$ value is preferable for optimizing performance. This is likely because the informative content of such data allows a lower $\\beta$ to facilitate more substantial updates, thereby enhancing alignment accuracy.\nConversely, for \\emph{high gap} pairwise data, maintaining a low $\\beta$ may lead to overfitting, which significantly undermines the alignment process.\nThe \\emph{mixed gap} dataset --- a combination of both \\emph{low gap} and \\emph{high gap} datasets --- exhibits a more nuanced performance pattern, suggesting the necessity for a dynamic $\\beta$ calibration strategy to adapt to varying data quality.\nConsequently, adhering to a fixed $\\beta$ value, \\ie configuring $\\beta$ at the population level, might be inadequate for the dynamic and varied nature of real-world datasets.\n\\begin{wrapfigure}{r}{0.3\\textwidth}\n    \\centering\n    \\vspace{-0.45cm}\n    \\!\\!\\!\\!\\!\\!\\!\\! \\includegraphics[width=0.32\\textwidth]{figs/gap_distribution.pdf}\n    \\vspace{-0.3cm}\n    \\caption{The distribution of individual reward discrepancy ($r(\\yb_w^{(i)};\\xb^{(i)})-r(\\yb_l^{(i)};\\xb^{(i)})$) on the training dataset of HH.}\n    \\vspace{-0.5cm}\n    \\label{fig:gap_distribution}\n  \\end{wrapfigure}\n\\textbf{(2) The dataset exhibits notable outliers.} In Figure \\ref{fig:gap_distribution}, utilizing the Pythia-2.8B model, we evaluate the data quality by examining the distribution of reward discrepancy for each triplet (which we will define as ``individual reward discrepancy'' later) within the HH dataset's training samples. \nThe tails of the density plot extend beyond the highlighted percentiles, suggesting the existence of data samples with significantly higher or lower reward discrepancies.\nNotably, cases with significantly higher rewards for positive samples over negative ones suggest low informational value, as these discrepancies likely do not contribute meaningfully to the model's learning process. Whereas the opposite cases hint at potential labeling errors. Both cases deviate from an expected rational distribution range and are thus classified as outliers. For further details on outliers, kindly refer to Appendix \\ref{low_gap_high_gap}.\n\n\\subsection{Method: Dynamic $\\beta$ Calibration in DPO}\n\nThrough our empirical analysis, we highlight the sensitivity of DPO to $\\beta$ selections and the frequent occurrence of outliers. Hence, \ndetermining the optimal $\\beta$ value requires careful consideration of the quality of pairwise data while also addressing the influence of outliers. This prompts the question: \\textit{what criteria define a superior choice of $\\beta$?} In response, we propose the following guiding principles:\n\n\\textbf{Principle 1:} \\textit{The optimal $\\beta$ value should be responsive to pairwise data's quality.}\n\n\\textbf{Principle 2:} \\textit{The selection of $\\beta$ value should minimize the influence of outliers.}\n\n\\subsubsection{Dynamic $\\beta$ Calibration at Batch-Level}\nWe begin by introducing the concept termed `\\textit{individual reward discrepancy}', which represents the difference between the rewards of winning and losing for each triplet, serving as a measurement for pairwise data quality. Formally, for a triplet $(\\xb^{(i)}, \\yb_w^{(i)}, \\yb_l^{(i)}) \\in \\Set{D}$, the individual reward discrepancy is defined as \n$M_i = r(\\yb_w^{(i)};\\xb^{(i)}) - r(\\yb_l^{(i)};\\xb^{(i)}).$\nWhile our primary analysis utilizes the implicit reward model induced by the policy trained using DPO, we also conducted comparative experiments with an explicit reward model. The details of these experiments with the explicit RM can be found in Appendix \\ref{sec:appendix_explicit_rm}. For the DPO-based implicit reward model, the reward discrepancy is expressed as:\n\n$$ M = \\beta_0 \\log \\left( \\frac{\\pi_\\theta (y_w \\mid x) }{\\pi_{\\text{ref}}(y_w \\mid x)} \\right) - \\beta_0 \\log \\left( \\frac{\\pi_\\theta (y_l \\mid x) }{\\pi_{\\text{ref}}(y_l \\mid x)} \\right). $$\n\nHere, \\(\\pi_\\theta\\) represents the policy being optimized, and \\(\\pi_{\\text{ref}}\\) denotes the reference policy. This formulation captures the difference in the log-probabilities of the winning and losing outcomes, weighted by the parameter \\(\\beta\\).\nMotivated by our guiding principles, a straightforward approach is to assign a distinct $\\beta$ to each triplet, allowing each $\\beta$ to serve as a parameter tailored to its respective triplet. This instance-level dynamic $\\beta$ adaption can be formulated as follows:\n\n\\begin{eqnarray}\n    \\begin{aligned}\n        \\beta_i = \\beta_0 + \\alpha \\large(M_i - M_0 \\large) \\beta_0 = [1 + \\alpha(M_i-M_0)]\\beta_0,\n    \\end{aligned}\n    \\label{eq:beta}\n\\end{eqnarray}\n\nwhere $\\beta_0$ represents the benchmark hyperparameter intrinsic to DPO, typically set to 0.1. The term $M_0$ denotes a predetermined threshold, and the coefficient $\\alpha$ is a scaling factor within the interval $[0, 1]$ that adjusts the influence of $M_i$ on $\\beta_i$. Specifically, when $\\alpha = 0$, $\\beta_i$ remains constant at $\\beta_0$, thus maintaining the standard DPO framework without modification.\n\nEquation \\eqref{eq:beta} illustrates that $\\beta_i$ increases monotonically with $M_i$, allowing the model to adjust the $\\beta$ value based on the running reward differential between paired samples.\\\nNevertheless, such instance-level adjustments may introduce instabilities during training. Prior studies have shown that a minibatch approach can help avoid saddle points or local minima \\cite{ge2015escaping}, as well as mitigate the impact of noise \\cite{robbins1951stochastic, bottou2010large}. Drawing inspiration from these benefits, we propose a batch-level dynamic estimation methodology for $\\beta$:\n\n\\begin{equation}\n    \\beta_{\\text{batch}} = [1 + \\alpha( \\EE_{i \\sim \\text{batch}}[M_i]-M_0)]\\beta_0.\n\\end{equation}\nIn practical applications, the threshold $M_0$ can be estimated by employing the global mean of $M_i$ with a moving average updating scheme \\cite{mae}:\n\\begin{equation}\n    M_0  \\leftarrow m M_0 + (1-m)  \\EE_{i \\sim \\text{batch}}[M_i],\n    \\label{mom}\n\\end{equation}\nwhere $m\\in[0,1)$ is a momentum coefficient.\nSuch a batch-level calibration method introduces only one new parameter, $\\alpha$, to control the scale of $\\beta$ adjustment.\nThe calculation of $\\EE_{i \\sim \\text{batch}}[M_i]$ is straightforward within DPO procedures, thereby incurring no additional computational overhead.\n\n\\subsubsection{$\\beta$-Guided Data Filtering }\nTo mitigate the adverse impact of outliers on the $\\beta$ selection process, we introduce a $\\beta$-guided data filtering mechanism. Informed by $3\\sigma$ confidence criterion \\cite{3sigma}, this strategy employs a probabilistic model to assess the significance of each triplet $(\\xb^{(i)}, \\yb_w^{(i)}, \\yb_l^{(i)})$ based on its individual reward discrepancy $M_i$, which is defined as:\n\\begin{equation}\n    p(M_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(M_i-M_0)^2}{2\\sigma^2}\\right),\n    \\label{sampling_p}\n\\end{equation}\nwhere $M_0$ and $\\sigma$ represent the mean and standard deviation of $M_i$ across the training dataset, respectively.\nSimilar to the updating scheme of $M_0$ in Equation \\eqref{mom}, we dynamically estimate the value of $\\sigma$ using the moving average method:\n\\begin{equation}\n     \\sigma \\leftarrow m \\sigma + (1-m)  \\sqrt{\\mathbb{V}_{i \\sim\\text{batch}}[M_i]}.\n     \\label{mom2}\n\\end{equation}\nThis probabilistic weighting discerns the relative importance of each sample, guiding the selection of $|\\text{batch}| \\times \\rho$ samples (without replacement) based on their calculated probabilities $p(M_i)$. Here, $\\rho$ denotes the selection ratio, defaulting to 0.8, a choice validated by preliminary experiments aimed at optimizing training efficiency and model accuracy.\n\nThis process is iterated for each training batch, ensuring that the training data is continuously updated to reflect the most informative samples. The introduction of the $\\beta$-guided data filtering strategy is instrumental in fortifying the model against outliers, thereby facilitating the accurate estimation of the $\\beta$ value.\n\n\\textbf{Highlights:}\nWe underline the following key features of our proposed $\\beta$-DPO framework:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Simplicity}: $\\beta$-DPO is extremely straightforward and quick to implement. It merely involves a dynamic $\\beta$ adjustment at the batch level and a $\\beta$-guided data filtering mechanism, both of which are predicated upon the reward discrepancy denoted by $M_i$.\n    \\item \\textbf{Efficiency}: Unlike other methodologies \\cite{RSO,morimura2024filtered,pruthi2020estimating,f-dpo} that necessitate an additional gold model for data filtering, our method leverages the running reward discrepancy $M_i$ within the DPO framework. Moreover, our empirical studies indicate that $\\beta$-DPO exhibits insensitivity to the hyperparameters $\\rho$. A default setting of $\\rho=0.8$ typically yields satisfactory performance.\n    \\item \\textbf{Model-agnostic}: As a variant of the traditional DPO, the proposed $\\beta$-DPO can function as a plug-and-play module. It allows straightforward integration of future enhancements and extensions within the DPO framework. Our empirical investigations corroborate this assertion.\n\\end{itemize}\n\n\\subsection{Discussion with Previous Studies}\n\\textbf{Relations to Data Selection.} An increasing volume of works \\cite{RSO,morimura2024filtered,LIMA,pruthi2020estimating, xia2024less} have underscored the impact of data quality on the performance of LLM's alignment. A common practice among these efforts involves employing a so-called ``gold model'' for data selection. This approach, however, introduces significant computational demands and the choice of the gold model directly influences the resultant system's performance. The focus of this work, it should be noted, is not to propose a superior strategy for data selection. Instead, we aim to enhance adaptability to the quality of data by dynamically adjusting the $\\beta$ parameter. This adjustment facilitates improved $\\beta$ estimation by selecting data based on the reward. Moreover, Section \\ref{exp_adaptation} illustrates the compatibility of dynamic $\\beta$ adjustment with other data selection methodologies.\n\n\\textbf{Relations to Recent Temperature Schemes.}\nDynamic temperature frameworks have been introduced in the realm of contrastive learning, motivated by various objectives, such as addressing out-of-distribution tasks \\cite{Uncertainty} or accommodating long-tail data distributions \\cite{long_tail}. The work most closely related to ours, MACL \\cite{MACL}, has indeed proposed an alignment-adaptive strategy; however, its primary aim was to navigate the uniformity-tolerance dilemma. Hence, the integration of dynamic temperature mechanisms with LLM's alignment remains an underexplored area against this backdrop.\\section{Experiments}\n\\label{Experiments}\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.49\\textwidth]{figs/win_rate_hh.pdf}\n    \\includegraphics[width=0.49\\textwidth]{figs/win_rate_tldr.pdf}\n    \\caption{\n    \\textbf{Left.} \n    The win rates computed by GPT-4 evaluations for the Anthropic-HH one-step dialogue; $\\beta$-DPO consistently outperforms across all sampling temperatures.\n    \\textbf{Right.} \n    In the comparison of TL;DR summarization win rates versus chosen summaries with GPT-4 as the evaluator, $\\beta$-DPO is distinguished as the only strategy achieving a win rate over 50\\% across different sampling temperatures.\n    }\n    \\label{fig:dialogue-main}\n\\end{figure}\n\nIn this section, we commence by conducting an empirical evaluation of $\\beta$-DPO on two specific tasks: dialogue generation and summarization. Subsequently, we analyze the various adaptations of the proposed method $\\beta$-DPO. Concluding this section, we underscore the imperative need for batch-level dynamic $\\beta$ calibration, highlighting its significance in the context of our study.\n\n\\subsection{Empirical Evaluation of $\\beta$-DPO on Dialogue Generation and Summarization}\n\\textbf{Datasets and Setup.} Our experiments are conducted on the Anthropic HH dataset \\cite{Bai2022training} and Reddit TL;DR summarization dataset \\cite{tldr_dataset}. The training configuration follows from \\citet{DPO}. The goals of these experiments are to study: 1) How $\\beta$-DPO performs on single-turn dialogue generation and summarization tasks; 2) How the sampling temperature affects the performance of $\\beta$-DPO; 3) How $\\beta$-DPO works with different model sizes. For detailed experimental settings, please refer to Appendix \\ref{setup_exp}.\n\n\\textbf{Baselines.} In our comparison, we examine the performance of $\\beta$-DPO relative to its counterparts: the standard DPO, DPO implemented with a dynamic $\\beta$ yet devoid of $\\beta$-guided data filtering, and DPO complemented by data filtering with $\\beta$ fixed at 0.1.\n\n\\textbf{Win Rate Across different Sampling Temperature.} An analysis of win rates derived from GPT-4 evaluations on the Anthropic-HH one-step dialogue demonstrates that $\\beta$-DPO consistently outperforms across all sampling temperatures, as depicted in Figure \\ref{fig:dialogue-main} (Left). Furthermore, for the TL;DR summarization task, $\\beta$-DPO stands out as the only approach achieving win rates above 50\\% for diverse sampling temperatures, which is visually represented in Figure \\ref{fig:dialogue-main} (Right). The data also suggests that while both dynamic $\\beta$ and data filtering enhance DPO's effectiveness, the impact of data filtering is especially pronounced in the summarization task, likely due to the inherently greater noise present in the Reddit TL;DR summarization dataset. Notably, $\\beta$-DPO exhibits a remarkable degree of robustness to variations in sampling temperature. As the temperature incrementally escalates from 0.0 to 1.0, the win rate for standard DPO plunges to a mere 25\\%, whereas $\\beta$-DPO maintains a commendable performance level with a win rate of 54\\%.\n\n\\textbf{Win Rate Across Different Model Sizes.} We further evaluate the performance of $\\beta$-DPO on the Anthropic HH dataset with Pythia-410M, -1.4B, and -2.8B models. The results are summarized in Table \\ref{tab:ablation}. We observe that $\\beta$-DPO consistently outperforms DPO, DPO with dynamic $\\beta$, and DPO with data filtering across all model sizes.\nWe observe that in a smaller model, the improvement of data filtering is more significant, while in a larger model, the improvement of dynamic $\\beta$ is more significant. We attribute this to the fact that the larger model has more capacity to learn the optimal policy, while the smaller model needs more help from the data filtering.\n\n\\begin{table}\n    \\centering\n    \\caption{\n    Win rate comparison of Pythia-410M, -1.4B, and -2.8B models on the Anthropic HH dataset, evaluated using GPT-4.\n    }\n    \\begin{tabular}{l|l|l|l}\n        \\toprule\n        \\textbf{Method} & \\textbf{410M} & \\textbf{1.4B} & \\textbf{2.8B} \\\\\n        \\midrule\n        DPO & $26.19$ & $42.78$ & $51.51$ \\\\\n        DPO + Dynamic $\\beta$ & $27.15^{\\color{+}+ 3.67\\%}$ & $43.51^{\\color{+}+ 1.71\\%}$ & $55.19^{\\color{+}+ 7.14\\%}$ \\\\\n        DPO + Data Filtering & $29.03^{\\color{+}+ 10.84\\%}$ & $46.99^{\\color{+}+ 9.84\\%}$ & $53.42^{\\color{+}+ 3.71\\%}$ \\\\\n        $\\beta$-DPO & $30.18^{\\color{+}+ 15.23\\%}$ & $48.67^{\\color{+}+ 13.77\\%}$ & $57.07^{\\color{+}+ 10.79\\%}$ \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\label{tab:ablation}\n\\end{table}\n\n\\subsection{Adaptations of $\\beta$-DPO}\n\\label{exp_adaptation}\nIn this section, our inquiry is twofold: first, we aim to understand the performance of $\\beta$-DPO when applied across various filtering strategies; second, we examine its efficacy across different adaptations of the DPO framework. In terms of filtering strategies, prevailing studies \\cite{pruthi2020estimating, xia2024less} in the domain largely employ a gradient-based approach. We propose to extend this methodology into three distinct scenarios. This involves arranging the gradients of pairwise data within a batch and consequently: (1) Excluding the top 20\\% of samples, hereby referred to as \\textbf{Filter Head},\n(2) Excluding the bottom 20\\% of samples, hereby referred to as \\textbf{Filter Tail},\n(3) Excluding both the top and bottom 10\\% of samples, a method we denote as \\textbf{Filter Tail \\& Head}.\nFor a fair comparison, we maintain the amount of data excluded at 20\\% for the above strategies.\nSecond, we integrate three variants of DPO into our analysis: the IPO \\cite{ipo}, a novel approach that facilitates learning directly from preferences without the need for the Bradley-Terry (BT) model. Additionally, we consider the KTO \\cite{KTO}, which focuses on discerning whether a preference is desirable or undesirable and SPPO \\cite{sppo}, which approximates the Nash equilibrium. For detailed settings, we refer the reader to the supplementary material.\n\n\\textbf{Selective filtering of the top 20\\% of samples markedly enhances model performance. } This approach, detailed in Figure \\ref{fig:gradient} (Left), not only surpasses other filtering strategies but also suggests that these samples, which exhibit the smallest discrepancies between positive and negative pairs, are particularly prone to flipped noise. By excluding them, the model's learning efficacy is appreciably improved.\n\n\\textbf{Dynamic $\\beta$ adapts to and improves upon existing filtering strategies.} Figure \\ref{fig:gradient} (Left) corroborates our stance that a static $\\beta$ proves insufficient within the DPO framework. We contend that the application of our dynamic $\\beta$-DPO could markedly reshape the DPO field by fostering the development of advanced filtering techniques.\n\n\\textbf{Dynamic $\\beta$ Enhancement across DPO Variants.}\nWe introduce dynamic $\\beta$-DPO, a novel strategy enhancing DPO and its variants: IPO, KTO, and SPPO in Figure \\ref{fig:gradient} (Middle). Our results on the Anthropic HH dataset demonstrate that while IPO initially leads in performance, the integration of dynamic $\\beta$ substantially elevates all variants, notably increasing $\\beta$-IPO's efficiency by 17.9\\%. This underscores dynamic $\\beta$-DPO's capability to significantly enhance model training through adaptable improvements, solidifying its value in advancing language models via human feedback.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.32\\textwidth]{figs/filter_gradient.pdf}\n    \\includegraphics[width=0.32\\textwidth]{figs/kto-sppo.pdf}\n    \\includegraphics[width=0.32\\textwidth]{figs/gap_batch_instance.pdf}\n    \\caption{\n    \\textbf{Left:} Win rates from GPT-4 evaluations on Anthropic-HH single-turn dialogues, showcasing $\\beta$-DPO's adaptability to diverse filtering strategies. \\textbf{Middle:} Win rates of $\\beta$-DPO across various DPO variants as evaluated by GPT-4. \\textbf{Right:} Distribution of individual reward discrepancies following fine-tuning through batch-level and instance-level calibration.\n    }\n    \n    \\label{fig:gradient}\n\\end{figure}\n\n\\subsection{Necessity of Batch-Level Dynamic $\\beta$ Calibration}\n\\label{sec_batch_level}\nIn this section, we aim to underscore the pivotal role of batch-level tuning in calibrating the parameter $\\beta$. To this end, we compare the performance of our $\\beta$-DPO algorithm under two distinct regimes: one employing batch-level dynamic $\\beta$ calibration, and the other utilizing instance-level dynamics. To emulate the diverse data disparity scenarios encountered in practical applications, we adopt the methodology outlined in Section \\ref{motivation_sec}, meticulously blending datasets characterized by both \\emph{low gap} and \\emph{high gap} attributes at varying ratios.\n\n\\textbf{Batch-level calibration surpasses both instance-level and population-level approaches.} The results presented in Table \\ref{tab:noise} illustrate that batch-level dynamic $\\beta$ calibration yields superior performance compared to instance-level dynamics and the baseline population-level approach (referred to as vanilla DPO) across a range of mixture ratios. This improvement can be credited to the batch-level calibration's ability to adjust to the varying data quality present within a batch, thus refining the model's learning process.\nConversely, instance-level dynamics can provoke excessively vigorous model updates, precipitating a decline in performance particularly noticeable at a mixture ratio of 40\\%, a scenario in which outliers exert a significant negative impact.\n\n\\textbf{Instance-level calibration magnifies the impact of outliers.}\nAs demonstrated in Figure \\ref{fig:gradient} (Right), instance-level calibration can inadvertently widen the range of reward discrepancy distribution. This broadened range suggests that instance-level calibration might be leading to excessively high or low $\\beta$ values for the model. Such disparities in $\\beta$ values consequently cause disproportionate update rates for certain samples, further intensifying the extremities in the distribution. \n\\begin{table}\n    \\centering\n    \\caption{\n    Comparison of win rates across varying mixture ratios on the Anthropic HH dataset, with each ratio indicating the proportion of \\emph{high-gap} to \\emph{low-gap} datasets, e.g., a 40\\% mixture ratio reflects a blend of 40\\% \\emph{high-gap} and 60\\% \\emph{low-gap}.\n    }\n    \\begin{tabular}{l|l|l|l|l}\n        \\toprule\n        \\textbf{Mixture Ratio} & \\textbf{10\\%} & \\textbf{20\\%} & \\textbf{30\\%} & \\textbf{40\\%}\\\\\n        \\midrule\n        Vanilla DPO & $50.17$ & $50.56$ & $47.95$ & $29.15$ \\\\\n        + Instance-level calibration & $49.18^{\\color{-}- 1.97\\%}$ & $49.82^{\\color{-}- 1.46\\%}$ & $44.42^{\\color{-}- 7.36\\%}$ & $16.82^{\\color{-}- 42.30\\%}$ \\\\\n        + Batch-level calibration & $57.68^{\\color{+}+ 14.69\\%}$ & $56.15^{\\color{+}+ 11.06\\%}$ & $51.25^{\\color{+}+ 6.88\\%}$ & $34.92^{\\color{+}+ 19.79\\%}$ \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\label{tab:noise}\n\\end{table}\n\n\\textbf{Our $\\beta$-calibration strategy consistently outperforms baseline methods.} To expand our approach to more diverse datasets and model sizes, we follow the current state-of-the-art models SimPO \\cite{SimPO2024}. We perform $\\beta$-DPO with two families of models, Llama3-8B \\citep{llama3modelcard}, Mistral2-7B \\citep{Jiang2023Mistral7}, on the \\href{https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k}{UltraChat-200k} dataset~\\cite{Ding2023EnhancingCL}. For comparison with baselines, we assess our models using one of the most popular open-ended instruction-following benchmarks: AlpacaEval 2. All settings are consistent with SimPO \\cite{SimPO2024}.\n\\begin{wraptable}[11]{r}{7.7cm}\n\\centering\n\\vspace{-0.6em}\n\\caption{Performance comparison of different models}\n \\small\n\\resizebox{7.7cm}{!}{\n\\begin{tabular}{ccccc}\n\\toprule\n\\multirow{2}{*}{Model} & \\multicolumn{2}{c}{Mistral-Instruct (7B)} & \\multicolumn{2}{c}{Llama3-Instruct (8B)} \\\\\n\\cmidrule(lr){2-3} \\cmidrule(lr){4-5}\n & LC (\\%) & WR (\\%) & LC (\\%) & WR (\\%) \\\\\n\\midrule\nDPO & 20.98 & \\textbf{21.60} & 40.44 & 37.38 \\\\\n$\\beta$-DPO & \\textbf{23.56} & 20.42 & \\textbf{43.38} & \\textbf{38.21} \\\\\n\\midrule\nSimPO & 28.50 & 30.56 & 44.38 & 38.97 \\\\\n$\\beta$-SimPO & \\textbf{30.48} & \\textbf{32.13} & \\textbf{46.03} & \\textbf{40.18} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\label{tab:performance_comparison_lc}\n\\end{wraptable}\nTable \\ref{tab:performance_comparison_lc} presents the AlpacaEval 2 results under the Mistral-Instruct (7B) and Llama3-Instruct (8B) settings. LC and WR denote length-controlled and raw win rate, respectively. Regardless of whether we use Llama3-8B or Mistral-7B, and whether the loss function is DPO or SimPO, our $\\beta$-{D, Sim}PO strategy consistently demonstrates significant performance improvements. This thoroughly showcases the method's strong generalization ability and excellent scalability.\n\n\\section{Conclusion and Future Work}\n\\label{conclusion}\nThis paper introduces $\\beta$-DPO, a novel framework designed to optimize DPO by dynamically adjusting the $\\beta$ parameter in response to the variability in the informativeness of pairwise data. Our approach, which incorporates $\\beta$-guided data filtering and batch-level dynamic $\\beta$ calibration, has demonstrated significant improvements in DPO's performance across a range of models and datasets. The empirical evaluations indicate that $\\beta$-DPO offers an adaptable training paradigm for LLMs with human feedback.\n\n\\textbf{Limitations and Future Work.}\nOur work on $\\beta$-DPO showcases a promising framework for LLM optimization, albeit with room for advancement. Future endeavors should explore:\nAdaptive $\\beta$ in Self-Play: Extending $\\beta$-DPO to self-play scenarios \\cite{spin,sppo} where negative samples dynamically adapt, necessitating iterative $\\beta$ adjustments, to foster the evolution of superior model strategies.\nEnhanced Evaluation Standards: Development of sophisticated metrics and use of advanced evaluators beyond win rates, capitalizing on advancements like GPT-4+, to comprehensively gauge model performance.\nScalability Investigation: Examining $\\beta$-DPO's scalability to ultra-large models surpassing 7B parameters, and its integration into diverse DPO-inspired architectures, is pivotal for practical impact.\nAutomated Parameter Tuning: Pursuing automation in parameter tuning, alleviating manual intervention for $\\beta$, to streamline the training pipeline and broaden accessibility.\n\\begin{ack}\nThis research is supported by the National Science and Technology Major Project (2023ZD0121102), National Natural Science Foundation of China (92270114, 62302321). This research was also supported by the advanced computing resources provided by the Supercomputing Center of the USTC.\n\\end{ack}\n\n\\newpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2402.05749v2.tex",
        "arXiv-2403.19270v2.tex",
        "arXiv-2407.08639v2.tex"
    ],
    "group_id": "group_24",
    "response": "### Title: Advances in Offline Preference Optimization for Large Language Model Alignment\n\n### Introduction\nThe field of Large Language Model (LLM) alignment with human preferences has seen significant advancements, particularly through the use of Reinforcement Learning from Human Feedback (RLHF) techniques. RLHF, as introduced by \\citet{christiano2017deep} and further developed by \\citet{ouyang2022training}, aims to align AI systems with human values by training a reward model based on human preferences and then optimizing the policy of the AI system against this model. However, RLHF is computationally expensive and requires interactive data collection, leading to the development of more efficient methods such as Direct Preference Optimization (DPO) \\citep{rafailov2023direct}, Indirect Preference Optimization (IPO) \\citep{azar2023general}, and Supervised Learning with Indirect Comparison (SLiC) \\citep{zhao2023SLiC}. These methods, collectively referred to as Offline Preference Optimization (OPO), seek to optimize human preferences using offline datasets without the need for interactive data collection. This summary explores recent research in OPO, focusing on three papers that introduce novel approaches to improve the alignment of LLMs with human preferences.\n\nThe history of LLM alignment can be traced back to early work on reward modeling and policy optimization \\citep{christiano2017deep}. RLHF has been pivotal in aligning powerful AI systems along human values, as demonstrated by recent advancements in LLMs \\citep{achiam2023gpt, team2023gemini}. However, the computational and practical challenges associated with RLHF have led to the exploration of OPO methods, which are more efficient and convenient. Despite these advancements, OPO methods still face challenges, such as the need for careful parameter tuning and the potential impact of outliers in the preference dataset.\n\n### Main Content of Each Paper\n\n#### Paper 1: Generalized Preference Optimization (GPO)\nThe first paper introduces GPO, a generalized framework for deriving offline preference optimization losses for LLM alignment. GPO parameterizes preference optimization losses via a family of convex functions $f$, with DPO, IPO, and SLiC as special cases. The authors argue that the tail behavior of the convex function $f$ governs the effective strength of regularization induced between the policy $\\pi_\\theta$ and the reference policy $\\pi_\\text{ref}$. This insight allows for a more unified understanding of the regularization mechanisms in OPO algorithms, providing a theoretical basis for choosing the hyper-parameter $\\beta$.\n\nThe paper's main contribution is a unifying view over notable OPO algorithms, which are derived from supervised binary classification problems. The authors propose GPO, which can be seen as a continuous spectrum of loss functions, with DPO, IPO, and SLiC as special instances. By framing reward learning as a binary classification problem, the authors provide a theoretical foundation for understanding the strengths and weaknesses of different OPO algorithms. They also highlight the importance of the convex function $f$ in determining the regularization strength and the optimal value of $\\beta$.\n\n#### Paper 2: Stepwise Direct Preference Optimization (sDPO)\nThe second paper introduces sDPO, an extension of DPO that employs a stepwise approach to aligning LLMs with human preferences. Unlike traditional DPO, which uses the entire preference dataset simultaneously, sDPO partitions the dataset into subsets and applies them incrementally. This method leverages the aligned model from the previous step as the reference for the current step, ensuring progressively better alignment. The authors demonstrate that sDPO significantly outperforms conventional DPO in terms of both log-probability benchmarks and generation benchmarks.\n\nThe authors argue that using a more aligned reference model can improve the alignment of the target model. They propose to use the aligned model from the previous step as the reference model for the current step, which results in a more conservative regularization effect. This approach simplifies the training process and aligns with curriculum learning principles, facilitating a structured optimization path. The authors also highlight the importance of the regularization coefficient $\\beta$ in controlling the strength of regularization, and suggest that different values of $\\beta$ can lead to different performance outcomes.\n\n#### Paper 3: Dynamic $\\beta$ and Quality-Aware Data Filtering in DPO ($\\beta$-DPO)\nThe third paper investigates the impact of the regularization coefficient $\\beta$ and the quality of pairwise data on the performance of DPO. The authors introduce a dynamic calibration of $\\beta$ and a data filtering mechanism to improve DPO's effectiveness across diverse data conditions. The proposed $\\beta$-DPO framework dynamically adjusts $\\beta$ at the batch level and filters out low-quality data samples, thereby enhancing the robustness and adaptability of DPO.\n\nThe authors conduct empirical evaluations on two specific tasks: dialogue generation and summarization. They demonstrate that $\\beta$-DPO consistently outperforms standard DPO and other variants, such as IPO and KTO, across various model sizes and datasets. The paper also highlights the importance of batch-level dynamic $\\beta$ calibration in mitigating the impact of outliers and improving the model's learning process. The authors argue that their approach offers a more robust and adaptable training paradigm for LLMs with human feedback.\n\n### Commonalities and Innovations\nAll three papers address the challenges of aligning LLMs with human preferences using OPO methods. They share a common goal of improving the alignment process by refining the training procedures and hyperparameters. However, each paper introduces unique innovations:\n\n- **Paper 1** provides a theoretical framework for understanding the regularization mechanisms in OPO algorithms. It introduces GPO, a generalized approach that encompasses DPO, IPO, and SLiC as special instances. The paper highlights the importance of the convex function $f$ in determining the regularization strength and the optimal value of $\\beta$.\n- **Paper 2** proposes sDPO, a stepwise approach to aligning LLMs with human preferences. This method leverages the aligned model from the previous step as the reference model for the current step, ensuring progressively better alignment. The authors argue that sDPO can improve the alignment of the target model by using a more aligned reference model.\n- **Paper 3** introduces $\\beta$-DPO, a dynamic calibration of $\\beta$ and a data filtering mechanism to improve DPO's effectiveness across diverse data conditions. The authors argue that their approach offers a more robust and adaptable training paradigm for LLMs with human feedback. They demonstrate that $\\beta$-DPO consistently outperforms standard DPO and other variants, such as IPO and KTO, across various model sizes and datasets.\n\n### Comparison of Results and Discussion\nThe empirical results from the three papers indicate significant improvements in the alignment of LLMs with human preferences. However, the specific improvements and performance metrics differ across the papers:\n\n- **Paper 1** demonstrates that the regularization vs. performance trade-off is similar for different GPO variants, with different convex loss variants inducing inherently distinct strengths for regularization. The authors observe that squared and truncated squared loss peak at generally lower $\\beta$ values than other variants, suggesting that different loss functions require different values of $\\beta$ to achieve optimal performance.\n- **Paper 2** shows that sDPO significantly outperforms conventional DPO in terms of both log-probability benchmarks and generation benchmarks. The authors attribute this improvement to the stepwise nature of sDPO, which allows for the integration of progressively more aligned reference models within the DPO training framework. The results indicate that sDPO can enhance the alignment precision of reference models and improve the overall performance of the final model.\n- **Paper 3** reports that $\\beta$-DPO consistently outperforms standard DPO and other variants across various model sizes and datasets. The authors argue that their approach offers a more robust and adaptable training paradigm for LLMs with human feedback. They demonstrate that $\\beta$-DPO can significantly enhance model training through adaptable improvements, solidifying its value in advancing language models via human feedback.\n\n### Conclusion\nThe main findings of the three papers highlight the importance of refining the training procedures and hyperparameters in OPO methods to improve the alignment of LLMs with human preferences. GPO provides a theoretical framework for understanding the regularization mechanisms in OPO algorithms, sDPO introduces a stepwise approach to aligning LLMs with human preferences, and $\\beta$-DPO offers a dynamic calibration of $\\beta$ and a data filtering mechanism to improve DPO's effectiveness across diverse data conditions.\n\nFuture research directions include:\n- **Adaptive $\\beta$ in Self-Play**: Extending $\\beta$-DPO to self-play scenarios where negative samples dynamically adapt, necessitating iterative $\\beta$ adjustments.\n- **Enhanced Evaluation Standards**: Developing sophisticated metrics and using advanced evaluators beyond win rates to comprehensively gauge model performance.\n- **Scalability Investigation**: Examining $\\beta$-DPO's scalability to ultra-large models surpassing 7B parameters and its integration into diverse DPO-inspired architectures.\n- **Automated Parameter Tuning**: Pursuing automation in parameter tuning to streamline the training pipeline and broaden accessibility.\n\nThese advancements will contribute to the ongoing efforts to improve the alignment of LLMs with human preferences, making them more helpful, factual, and ethical."
}