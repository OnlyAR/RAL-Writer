{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nFor natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset.\nIn pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks.\nBy including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks.\nGLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models.\nWe evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems. \n\\end{abstract}\n\n\\section{Introduction}\\label{sec:intro}\n\nThe human ability to understand language is \\emph{general}, \\textit{flexible}, and \\textit{robust}. \nIn contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. \nIf we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, \nthen it is critical to develop a more unified model that can learn to execute a range of different linguistic tasks in different domains.\n\nTo facilitate research in this direction, we present the General Language Understanding Evaluation \n(GLUE)\nbenchmark: a collection of NLU tasks including question answering, sentiment analysis, and textual entailment, and an associated online platform for model evaluation, comparison, and analysis.\nGLUE does not place any constraints on model architecture beyond the ability to process single-sentence and sentence-pair inputs and to make corresponding predictions. \nFor some GLUE tasks, training data is plentiful, but for others it is limited or fails to match the genre of the test set. GLUE therefore favors models that can learn to represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. \nNone of the datasets in GLUE were created from scratch for the benchmark; we rely on preexisting datasets because they have been implicitly agreed upon by the NLP community as challenging and interesting.\nFour of the datasets feature privately-held test data, which will be used to ensure that the benchmark is used fairly.\\footnote{To evaluate on the private test data, users of the benchmark must submit to \\texttt{\\href{gluebenchmark.com}{gluebenchmark.com}}}\n\nTo understand the types of knowledge learned by models and to encourage linguistic-meaningful solution strategies, GLUE also includes a set of hand-crafted analysis examples for probing trained models. \nThis dataset is designed to highlight common challenges, such as the use of world knowledge and logical operators, that we expect models must handle to robustly solve the tasks.\n\nTo better understand the challenged posed by GLUE, we conduct experiments with simple baselines and state-of-the-art sentence representation models.\nWe find that unified multi-task trained models slightly outperform comparable models trained on each task separately.\nOur best multi-task model makes use of ELMo \\citep{peters2018deep},\na recently proposed pre-training technique.\nHowever, this model still achieves a fairly low absolute score.\nAnalysis with our diagnostic dataset reveals that our baseline models deal well with strong lexical signals but struggle with deeper logical structure.\n\nIn summary, we offer: (i) A suite of nine sentence or sentence-pair NLU tasks, built on established annotated datasets and selected to cover a diverse range of text genres, dataset sizes, and degrees of difficulty. (ii) An online evaluation platform and leaderboard, based primarily on privately-held test data. The platform is model-agnostic, and can evaluate any method capable of producing results on all nine tasks. (iii) An expert-constructed diagnostic evaluation dataset. (iv) Baseline results for several major existing approaches to sentence representation learning.\n\n\\begin{table*}[t]\n\\centering \\small\n\\begin{tabular}{lrrlll}\n \\toprule\n\\textbf{Corpus} & \\textbf{$|$Train$|$} & \\textbf{$|$Test$|$} & \\textbf{Task} & \\textbf{Metrics} & \\textbf{Domain} \\\\\n\\midrule\n\\multicolumn{6}{c}{Single-Sentence Tasks}\\\\\n\\midrule\nCoLA & 8.5k & \\textbf{1k} & acceptability & Matthews corr.& misc. \\\\ % SB: Changed from 'linguistics literature'. That could be misleading, as few of the sentences are actually in the style of academic writing, and many are found in the wild.\nSST-2 & 67k & 1.8k & sentiment & acc. & movie reviews \\\\\n\\midrule\n\\multicolumn{6}{c}{Similarity and Paraphrase Tasks}\\\\\n\\midrule\nMRPC & 3.7k & 1.7k & paraphrase & acc./F1 & news \\\\\nSTS-B & 7k & 1.4k & sentence similarity & Pearson/Spearman corr. & misc. \\\\\nQQP & 364k & \\textbf{391k} & paraphrase & acc./F1 & social QA questions \\\\\n\\midrule\n\\multicolumn{6}{c}{Inference Tasks} \\\\\n\\midrule\nMNLI & 393k & \\textbf{20k} & NLI & matched acc./mismatched acc. & misc. \\\\\nQNLI & 105k & 5.4k & QA/NLI & acc. & Wikipedia \\\\\nRTE & 2.5k & 3k & NLI & acc. & news, Wikipedia \\\\\nWNLI & 634 & \\textbf{146} & coreference/NLI & acc. & fiction books \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Task descriptions and statistics. All tasks are single sentence or sentence pair classification, except STS-B, which is a regression task. MNLI has three classes; all other classification tasks have two. Test sets shown in bold use labels that have never been made public in any form.\n}\n\\label{tab:tasks}\n\\end{table*}\n\n\\section{Related Work}\n\\label{sec:related}\n\\citet{collobert2011natural} used a multi-task model with a shared sentence understanding component to jointly learn POS tagging, chunking, named entity recognition, and semantic role labeling.\nMore recent work has explored using labels from core NLP tasks to supervise training of lower levels of deep neural networks \\citep{sogaard2016deep,hashimoto2016joint} and \nautomatically learning cross-task sharing mechanisms for multi-task learning \\citep{ruder2017sluice}.\n\nBeyond multi-task learning, much work in developing general NLU systems has focused on sentence-to-vector encoders \\citep[][ i.a.]{pmlr-v32-le14,kiros2015skip}, leveraging unlabeled data \n\\citep{hill2016learning,peters2018deep}, labeled data \\citep{conneau2018senteval,mccann2017learned}, and combinations of these \\citep{collobert2011natural,subramanian2018large}.\nIn this line of work, a standard evaluation practice has emerged, recently codified as SentEval \\citep{DBLP:conf/emnlp/ConneauKSBB17,conneau2018senteval}.\nLike GLUE, SentEval relies on a set of existing classification tasks involving either one or two sentences as inputs. Unlike GLUE, SentEval only evaluates sentence-to-vector encoders, making it well-suited for evaluating models on tasks involving sentences \\emph{in isolation}.\nHowever, cross-sentence contextualization and alignment are instrumental in achieving state-of-the-art performance on tasks such as machine translation \\citep{bahdanau2014neural,vaswani2017attention}, question answering \\citep{seo2016bidirectional}, and natural language inference \\citep{rocktaschel2015reasoning}.\nGLUE is designed to facilitate the development of these methods: It is model-agnostic, allowing for any kind of representation or contextualization, including models that use no explicit vector or symbolic representations for sentences whatsoever.\n\nGLUE also diverges from SentEval in the selection of evaluation tasks that are included in the suite. Many of the SentEval tasks are closely related to sentiment analysis, such as MR \\citep{pang2005seeing}, SST \\citep{socher2013recursive}, CR \\citep{hu2004mining}, and SUBJ \\citep{pang2004sentimental}. Other tasks are so close to being solved that evaluation on them is relatively  uninformative, such as MPQA \\citep{wiebe2005annotating} and TREC question classification \\citep{voorhees1999trec}. In GLUE, we attempt to construct a benchmark that is both diverse and difficult.\n\n\\citet{McCann2018decaNLP} introduce decaNLP, which also scores NLP systems based on their performance on multiple datasets. Their benchmark recasts the ten evaluation tasks as question answering, converting tasks like summarization and text-to-SQL semantic parsing into question answering using automatic transformations. That benchmark lacks the leaderboard and error analysis toolkit of GLUE, but more importantly, we see it as pursuing a more ambitious but less immediately practical goal: While GLUE rewards methods that yield good performance on a circumscribed set of tasks using methods like those that are currently used for those tasks, their benchmark rewards systems that make progress toward their goal of unifying all of NLU under the rubric of question answering.\n\n\\section{Tasks}\\label{sec:tasks}\n\nGLUE is centered on nine English sentence understanding tasks, which  cover a broad range of domains, data quantities, and difficulties.\nAs the goal of GLUE is to spur development of generalizable NLU systems, we design the benchmark such that good performance should require a model to share substantial knowledge (e.g., trained parameters) across all tasks, while still maintaining some task-specific components.\nThough it is possible to train a single model for each task with no pretraining or other outside sources of knowledge and evaluate the resulting set of models on this benchmark, \nwe expect that our inclusion of several data-scarce tasks will ultimately render this approach uncompetitive.\nWe describe the tasks below and in \\autoref{tab:tasks}.  Appendix \\ref{sec:apdx_data} includes additional details. Unless otherwise mentioned, tasks are evaluated on accuracy and are balanced across classes.\n\n\\subsection{Single-Sentence Tasks}\n\n\\paragraph{CoLA}\nThe Corpus of Linguistic Acceptability \\citep{warstadt2018neural}\nconsists of English acceptability judgments drawn from books and journal articles on linguistic theory.\nEach example is a sequence of words annotated with whether it is a grammatical English sentence. \nFollowing the authors, we use Matthews correlation coefficient \\citep{matthews1975comparison} as the evaluation metric, which evaluates performance on unbalanced binary classification and ranges from -1 to 1, with 0 being the performance of uninformed guessing.\nWe use the standard test set, for which we obtained private labels from the authors.\nWe report a single performance number on the combination of the in- and out-of-domain sections of the test set.\n\n\\paragraph{SST-2}\nThe Stanford Sentiment Treebank \\citep{socher2013recursive} consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. We use the two-way (\\textit{positive}/\\textit{negative}) class split, and use only sentence-level labels.\n\n\\subsection{Similarity and Paraphrase Tasks}\n\n\\paragraph{MRPC}\nThe Microsoft Research Paraphrase Corpus \\citep{dolan2005automatically} is a corpus of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent. Because the classes are imbalanced (68\\% positive), we follow common practice and report both accuracy and F1 score.\n\n\\paragraph{QQP}\nThe Quora Question Pairs\\footnote{ \\href{https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs}{\\texttt{data.quora.com/\\allowbreak First-\\allowbreak Quora-\\allowbreak Dataset-\\allowbreak Release-Question-Pairs}}} dataset is a collection of question pairs from the community question-answering website Quora. The task is to determine whether a pair of questions are semantically equivalent. As in MRPC, the class distribution in QQP is unbalanced (63\\% negative), so we report both accuracy and F1 score. We use the standard test set, for which we obtained private labels from the authors. We observe that the test set has a different label distribution than the training set.\n\n\\paragraph{STS-B}\nThe Semantic Textual Similarity Benchmark \\citep{cer2017semeval} is a collection of sentence pairs drawn from news headlines, video and image captions, and natural language inference data. \nEach pair is human-annotated with a similarity score from 1 to 5; the task is to predict these scores.\nFollow common practice, we evaluate using Pearson and Spearman correlation coefficients.\n\n\\subsection{Inference Tasks}\n\n\\paragraph{MNLI}\nThe Multi-Genre Natural Language Inference Corpus \\citep{DBLP:journals/corr/WilliamsNB17} is a crowd-sourced collection of sentence pairs with textual entailment annotations. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (\\textit{entailment}), contradicts the hypothesis (\\textit{contradiction}), or neither (\\textit{neutral}). The premise sentences are gathered from ten different sources, including transcribed speech, fiction, and government reports. We use the standard test set, for which we obtained private labels from the authors, and evaluate on both the \\textit{matched} (in-domain) and \\textit{mismatched} (cross-domain) sections. We also use and recommend the SNLI corpus \\citep{bowman2015large} as 550k examples of auxiliary training data. %\\citep{chen2017recurrent,gong2018nli}.\n\n\\paragraph{QNLI}\nThe Stanford Question Answering Dataset (\\citealt{rajpurkar2016squad}) is a question-answering dataset consisting of question-paragraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question (written by an annotator). We convert the task into sentence pair classification by forming a pair between each question and each sentence in the corresponding context, and filtering out pairs with low lexical overlap between the question and the context sentence. The task is to determine whether the context sentence contains the answer to the question. This modified version of the original task removes the requirement that the model select the exact answer, but also removes the simplifying assumptions that the answer is always present in the input and that lexical overlap is a reliable cue.\nThis process of recasting existing datasets into NLI is similar to methods introduced in \\citet{white2017inference} and expanded upon in \\citet{demszky2018transforming}.\nWe call the converted dataset QNLI (Question-answering NLI).\\footnote{An earlier release of QNLI had an artifact where the task could be modeled and solved as an easier task than we describe here. We have since released an updated version of QNLI that removes this possibility.}\n\n\\paragraph{RTE}\nThe Recognizing Textual Entailment (RTE) datasets come from a series of annual textual entailment challenges. We combine the data from RTE1 \\citep{dagan2006pascal}, RTE2 \\citep{bar2006second}, RTE3 \\citep{giampiccolo2007third}, and RTE5 \\citep{bentivogli2009fifth}.\\footnote{RTE4 is not publicly available, while RTE6 and RTE7 do not fit the standard NLI task.} Examples are constructed based on news and Wikipedia text. \nWe convert all datasets to a two-class split, where for three-class datasets we collapse \\textit{neutral} and \\textit{contradiction} into \\textit{not\\_entailment}, for consistency.\n\n\\begin{table*}[t]\n\\small\n\\centering\n\\begin{tabular}{ll}\n\\toprule\n\\textbf{Coarse-Grained Categories} & \\textbf{Fine-Grained Categories} \\\\\n\\midrule\n\\multirow{2}{*}{Lexical Semantics} & Lexical Entailment, Morphological Negation, Factivity, \\\\ & Symmetry/Collectivity, Redundancy, Named Entities, Quantifiers \\\\\n\\midrule\n\\multirow{3}{*}{Predicate-Argument Structure} & Core Arguments, Prepositional Phrases, Ellipsis/Implicits, \\\\ & Anaphora/Coreference\nActive/Passive, Nominalization, \\\\ & Genitives/Partitives, Datives, Relative Clauses, \\\\\n& Coordination Scope, Intersectivity, Restrictivity \\\\\n\\midrule\n\\multirow{2}{*}{Logic} & Negation, Double Negation, Intervals/Numbers, Conjunction, Disjunction, \\\\ & Conditionals, Universal, Existential, Temporal, Upward Monotone, \\\\ & Downward Monotone, Non-Monotone \\\\\n\\midrule\nKnowledge & Common Sense, World Knowledge\\\\\n\n\\bottomrule\n\\end{tabular}\n\\caption{The types of linguistic phenomena annotated in the diagnostic dataset, organized under four major categories. For a description of each phenomenon, see \\autoref{sec:apdx_diagnostic}.}\n\\label{tab:analysis-categories}\n\\end{table*}\n\n\\paragraph{WNLI}\nThe Winograd Schema Challenge \\citep{levesque2011winograd} is a reading comprehension task in which a system must read a sentence with a pronoun and select the referent of that pronoun from a list of choices. \nThe examples are manually constructed to foil simple statistical methods: Each one is contingent on contextual information provided by a single word or phrase in the sentence. \nTo convert the problem into sentence pair classification, we construct sentence pairs by replacing the ambiguous pronoun with each possible referent.\nThe task is to predict if the sentence with the pronoun substituted is entailed by the original sentence.\nWe use a small evaluation set consisting of new examples derived from fiction books\\footnote{See similar examples at \n\\href{https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html}{\\tt cs.nyu.edu/\\allowbreak faculty/\\allowbreak davise/\\allowbreak papers/\\allowbreak WinogradSchemas/\\allowbreak WS.html}} that was shared privately by the authors of the original corpus. \nWhile the included training set is balanced between two classes,  the test set is imbalanced between them (65\\% not entailment). Also, due to a data quirk, the development set is \\textit{adversarial}: hypotheses are sometimes shared between training and development examples, so if a model memorizes the training examples, they will predict the wrong label on corresponding development set example. As with QNLI, each example is evaluated separately, so there is not a systematic correspondence between a model's score on this task and its score on the unconverted original task.\nWe call converted dataset WNLI (Winograd NLI).\n\n\\begin{table*}[t]\n\\small\n\\centering\n\\begin{tabularx}{\\textwidth}{XXXll}\n\\toprule\n \\textbf{Tags} & \\textbf{Sentence 1} & \\textbf{Sentence 2} & \\textbf{Fwd} & \\textbf{Bwd} \\\\\n\\midrule\n\\it Lexical Entailment (Lexical Semantics), Downward Monotone (Logic) & The timing of the meeting has not been set, according to a Starbucks spokesperson. & The timing of the meeting has not been considered, according to a Starbucks spokesperson. & N & E \\\\ \\midrule\n\\it Universal Quantifiers (Logic) & Our deepest sympathies are with all those affected by this accident. & Our deepest sympathies are with a victim who was affected by this accident. & E & N \\\\ \\midrule\n\\it Quantifiers (Lexical Semantics), Double Negation (Logic) & I have never seen a hummingbird not flying. & I have never seen a hummingbird. & N & E \\\\\n\\bottomrule\n\\end{tabularx}\n\\caption{Examples from the diagnostic set. \\textit{Fwd} (resp. \\textit{Bwd}) denotes the label when sentence 1 (resp. sentence 2) is the premise. Labels are \\textit{entailment} (E), \\textit{neutral} (N), or \\textit{contradiction} (C).\nExamples are tagged with the phenomena they demonstrate, and each phenomenon belongs to one of four broad categories (in parentheses).\n}\n\\label{tab:analysis-examples}\n\\end{table*}\n\n\\subsection{Evaluation}\nThe GLUE benchmark follows the same evaluation model as SemEval and Kaggle. To evaluate a system on the benchmark, one must run the system on the provided test data for the tasks, then upload the results to the website \\href{https://gluebenchmark.com}{\\tt gluebenchmark.com} for scoring. \nThe benchmark site shows per-task scores and a macro-average of those scores to determine a system's position on the leaderboard.\nFor tasks with multiple metrics (e.g., accuracy and F1), we use an unweighted average of the metrics as the score for the task when computing the overall macro-average.\nThe website also provides fine- and coarse-grained results on the diagnostic dataset. See Appendix \\ref{sec:apdx_website} for details.\n\n\\section{Diagnostic Dataset}\n\nDrawing inspiration from the FraCaS suite \\citep{cooper96fracas} and the recent Build-It-Break-It competition \\citep{ettinger2017towards}, we include a small, manually-curated test set for the analysis of system performance. While the main benchmark mostly reflects an application-driven distribution of examples, \nour diagnostic dataset highlights a pre-defined set of phenomena that we believe are interesting and important for models to capture. We show the full set of phenomena in \\autoref{tab:analysis-categories}.\n\nEach diagnostic example is an NLI sentence pair with tags for the phenomena demonstrated.\nThe NLI task is well-suited to this kind of analysis, as it can easily evaluate the full set of skills involved in (ungrounded) sentence understanding, from resolution of syntactic ambiguity to pragmatic reasoning with world knowledge.\nWe ensure the data is reasonably diverse by producing examples for a variety of linguistic phenomena and basing our examples on naturally-occurring sentences from several domains (news, Reddit, Wikipedia, academic papers).\nThis approaches differs from that of FraCaS, which was designed to test linguistic theories with a minimal and uniform set of examples.\nA sample from our dataset is shown in \\autoref{tab:analysis-examples}. \n\n\\paragraph{Annotation Process} \nWe begin with a target set of phenomena, based roughly on those used in the FraCaS suite \\citep{cooper96fracas}.\nWe construct each example by locating a sentence that can be easily made to demonstrate a target phenomenon, and editing it in two ways to produce an appropriate sentence pair.\nWe make minimal modifications so as to maintain high lexical and structural overlap within each sentence pair and limit superficial cues.\nWe then label the inference relationships  between the sentences, considering each sentence alternatively as the premise, producing two labeled examples for each pair (1100 total).\nWhere possible, we produce several pairs with different labels for a single source sentence, to have minimal sets of sentence pairs that are lexically and structurally very similar but correspond to different entailment relationships.\nThe resulting labels are 42\\% \\textit{entailment}, 35\\% \\textit{neutral}, and 23\\% \\textit{contradiction}.\n\n\\paragraph{Evaluation}\nSince the class distribution in the diagnostic set is not balanced, we use \\(R_3\\) \\citep{gorodkin2004Rk}, a three-class generalization of the Matthews correlation coefficient, for evaluation.\n\nIn light of recent work showing that crowdsourced data often contains artifacts which can be exploited to perform well without solving the intended task \n\\citep[][ i.a.]{schwartz17cloze,poliak2018hypothesis,TSUCHIYA18.786},\nwe audit the data for such artifacts.\nWe reproduce the methodology of \\citet{gurudipta18artifacts},\ntraining two fastText classifiers \\citep{joulin2016bag} to predict entailment labels on SNLI and MNLI using only the hypothesis as input. \nThe models respectively get near-chance accuracies of 32.7\\% and 36.4\\% on our diagnostic data, showing that the data does not suffer from such artifacts. \n\nTo establish human baseline performance on the diagnostic set, we have six NLP researchers annotate 50 sentence pairs (100 entailment examples) randomly sampled from the diagnostic set. Inter-annotator agreement is high, with a Fleiss's \\(\\kappa\\) of 0.73.\nThe average \\(R_3\\) score among the annotators is 0.80, much higher than any of the baseline systems described in Section \\ref{sec:baselines}. \n\n\\paragraph{Intended Use}\nThe diagnostic examples are hand-picked to address certain phenomena, and NLI is a task with no natural input distribution, so we do not expect performance on the diagnostic set to reflect overall performance or generalization in downstream applications. Performance on the analysis set should be compared between models but not between categories. The set is provided not as a benchmark, but as an analysis tool for error analysis, qualitative model comparison, and development of adversarial examples.\n\n\\section{Baselines}\\label{sec:baselines}\n\nFor baselines, we evaluate a multi-task learning model trained on the GLUE tasks, as well as several variants based on recent pre-training methods.\nWe briefly describe them here. See Appendix \\ref{sec:apdx_baselines} for details.\nWe implement our models in the AllenNLP library \\citep{Gardner2017AllenNLPAD}.\nOriginal code for the baselines is available at \\texttt{\\href{https://github.com/nyu-mll/GLUE-baselines}{https://github.com/nyu-mll/GLUE-baselines}} \nand a newer version is available at \\texttt{\\href{https://github.com/jsalt18-sentence-repl/jiant}{https://github.com/jsalt18-sentence-repl/jiant}}.\n\n\\paragraph{Architecture}\n\nOur simplest baseline architecture is based on sentence-to-vector encoders, and sets aside GLUE's ability to evaluate models with more complex structures.\nTaking inspiration from \\citet{DBLP:conf/emnlp/ConneauKSBB17}, the model uses a two-layer, 1500D (per direction) BiLSTM with max pooling and 300D GloVe word embeddings \\citep[840B Common Crawl version;][]{pennington2014glove}.\nFor single-sentence tasks, we encode the sentence and pass the resulting vector to a classifier.\nFor sentence-pair tasks, we encode sentences independently to produce vectors $u, v$, and pass $[u; v; |u - v|; u * v]$ to a classifier.\nThe classifier is an MLP with a 512D hidden layer.\n\nWe also consider a variant of our model which for sentence pair tasks uses an attention mechanism inspired by \\citet{seo2016bidirectional} between all pairs of words, followed by a second BiLSTM with max pooling.\nBy explicitly modeling the interaction between sentences, these models fall outside the sentence-to-vector paradigm.\n\n\\paragraph{Pre-Training} We augment our base model with two recent methods for pre-training: ELMo and CoVe. \nWe use existing trained models for both.\n\nELMo uses a pair of two-layer neural language models trained on the Billion Word Benchmark \\citep{chelba2013one}. \nEach word is represented by a contextual embedding, produced by taking a linear combination of the corresponding hidden states of each layer of the two models. \nWe follow the authors' recommendations\\footnote{\\href{https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md}{\\tt github.com/\\allowbreak allenai/\\allowbreak allennlp/\\allowbreak blob/\\allowbreak master/\\allowbreak tutorials/\\allowbreak how\\textunderscore to/\\allowbreak elmo.md}} and use ELMo embeddings in place of any other embeddings.\n\nCoVe \\citep{mccann2017learned} uses a two-layer BiLSTM encoder originally trained for English-to-German translation. \nThe CoVe vector of a word is the corresponding hidden state of the top-layer LSTM. \nAs in the original work, we concatenate the CoVe vectors to the GloVe word embeddings. \n\n\\paragraph{Training}\n\nWe train our models with the BiLSTM sentence encoder and post-attention BiLSTMs shared across tasks, and classifiers trained separately for each task.\nFor each training update, we sample a task to train with a probability proportional to the number of training examples for each task.\nWe train our models with Adam \\citep{kingma2014adam} with initial learning rate $10^{-4}$ and batch size 128.\nWe use the macro-average score as the validation metric and stop training when the learning rate drops below $10^{-5}$ or performance does not improve after 5 validation checks.\n\nWe also train a set of single-task models, which are configured and trained identically, but share no parameters. To allow for fair comparisons with the multi-task analogs, we do not tune parameter or training settings for each task, so these single-task models do not generally represent the state of the art for each task.\n \n\\paragraph{Sentence Representation Models}\n\nFinally, we evaluate the following trained sentence-to-vector encoder models using our benchmark: average bag-of-words using GloVe embeddings (CBoW), Skip-Thought \\citep{kiros2015skip}, InferSent \\citep{DBLP:conf/emnlp/ConneauKSBB17}, DisSent \\citep{nie2017dissent}, and GenSen \\citep{subramanian2018large}. \nFor these models, we only train task-specific classifiers on the representations they produce. \n\n\\begin{table*}[t]\n\\centering \\fontsize{8.4}{10.1}\\selectfont \\setlength{\\tabcolsep}{0.5em}\n\\begin{tabular}{lrrrrrrrrrr}\n\n\\toprule\n\n&& \\multicolumn{2}{c}{\\textbf{Single Sentence}} & \\multicolumn{3}{c}{\\textbf{Similarity and Paraphrase}} & \\multicolumn{4}{c}{\\textbf{Natural Language Inference}} \\\\\n\\textbf{Model} & \\multicolumn{1}{c}{\\textbf{Avg}} & \\multicolumn{1}{c}{\\textbf{CoLA}} & \\multicolumn{1}{c}{\\textbf{SST-2}} & \\multicolumn{1}{c}{\\textbf{MRPC}} & \\multicolumn{1}{c}{\\textbf{QQP}} & \\multicolumn{1}{c}{\\textbf{STS-B}} & \\multicolumn{1}{c}{\\textbf{MNLI}} & \\multicolumn{1}{c}{\\textbf{QNLI}} & \\multicolumn{1}{c}{\\textbf{RTE}} & \\multicolumn{1}{c}{\\textbf{WNLI}} \\\\\n\\midrule\n\\multicolumn{11}{c}{Single-Task Training} \\\\\n\\midrule\n\nBiLSTM & 63.9 & 15.7 & 85.9 & 69.3/79.4 & 81.7/61.4 & 66.0/62.8 & 70.3/70.8 & 75.7 & 52.8 & \\textbf{\\underline{65.1}} \\\\\n\n~~+ELMo & 66.4 & \\textbf{\\underline{35.0}} & \\underline{90.2} & 69.0/80.8 & 85.7/65.6 & 64.0/60.2 & 72.9/73.4 & 71.7 & 50.1 & \\textbf{\\underline{65.1}} \\\\\n\n~~+CoVe & 64.0 & 14.5 & 88.5 & \\underline{73.4}/\\underline{81.4} & 83.3/59.4 & \\underline{67.2}/\\underline{64.1} & 64.5/64.8 & 75.4 & \\underline{53.5} & \\textbf{\\underline{65.1}} \\\\\n\n~~+Attn & 63.9 & 15.7 & 85.9 & 68.5/80.3 & 83.5/62.9 & 59.3/55.8 & 74.2/73.8 & \\underline{77.2} & 51.9 & \\textbf{\\underline{65.1}} \\\\\n\n~~+Attn, ELMo & \\underline{66.5} & \\textbf{\\underline{35.0}} & \\underline{90.2} & 68.8/80.2 & \\textbf{\\underline{86.5}}/\\textbf{\\underline{66.1}} & 55.5/52.5 & \\textbf{\\underline{76.9}}/\\textbf{\\underline{76.7}} & 76.7 & 50.4 & \\textbf{\\underline{65.1}} \\\\\n\n~~+Attn, CoVe & 63.2 & 14.5 & 88.5 & 68.6/79.7 & 84.1/60.1 & 57.2/53.6 & 71.6/71.5 & 74.5 & 52.7 & \\textbf{\\underline{65.1}} \\\\\n\n\\midrule\n\\multicolumn{11}{c}{Multi-Task Training} \\\\\n\\midrule\n\nBiLSTM & 64.2 & 11.6 & 82.8 & 74.3/81.8 & 84.2/62.5 & 70.3/67.8 & 65.4/66.1 & 74.6 & 57.4 & \\textbf{\\underline{65.1}} \\\\\n\n~~+ELMo & 67.7 & 32.1 & 89.3 & \\textbf{\\underline{78.0}}/\\textbf{\\underline{84.7}} & 82.6/61.1 & 67.2/67.9 & 70.3/67.8 & 75.5 & 57.4 & \\textbf{\\underline{65.1}} \\\\\n\n~~+CoVe & 62.9 & 18.5 & 81.9 & 71.5/78.7 & \\underline{84.9}/60.6 & 64.4/62.7 & 65.4/65.7 & 70.8 & 52.7 & \\textbf{\\underline{65.1}} \\\\\n\n~~+Attn & 65.6 & 18.6 & 83.0 & 76.2/83.9 & 82.4/60.1 & 72.8/70.5 & 67.6/68.3 & 74.3 & 58.4 & \\textbf{\\underline{65.1}} \\\\\n\n~~+Attn, ELMo & \\textbf{\\underline{70.0}} & \\underline{33.6} & \\textbf{\\underline{90.4}} & \\textbf{\\underline{78.0}}/84.4 & 84.3/\\underline{63.1} & \\underline{74.2}/\\underline{72.3} & \\underline{74.1}/\\underline{74.5} & \\textbf{\\underline{79.8}} & \\underline{58.9} & \\textbf{\\underline{65.1}} \\\\\n\n~~+Attn, CoVe & 63.1 & 8.3 & 80.7 & 71.8/80.0 & 83.4/60.5 & 69.8/68.4 & 68.1/68.6 & 72.9 & 56.0 & \\textbf{\\underline{65.1}} \\\\\n\n\\midrule\n\\multicolumn{11}{c}{Pre-Trained Sentence Representation Models} \\\\\n\\midrule\n\nCBoW & 58.9 & 0.0 & 80.0 & 73.4/81.5 & 79.1/51.4 & 61.2/58.7 & 56.0/56.4 & 72.1 & 54.1 & \\textbf{\\underline{65.1}} \\\\\n\nSkip-Thought & 61.3 & 0.0 & 81.8 & 71.7/80.8 & 82.2/56.4 & 71.8/69.7 & 62.9/62.8 & 72.9 & 53.1 & \\textbf{\\underline{65.1}} \\\\\n\nInferSent & 63.9 & 4.5 & \\underline{85.1} & 74.1/81.2 & 81.7/59.1 & 75.9/75.3 & 66.1/65.7 & 72.7 & 58.0 & \\textbf{\\underline{65.1}} \\\\\n\nDisSent & 62.0 & 4.9 & 83.7 & 74.1/81.7 & 82.6/59.5 & 66.1/64.8 & 58.7/59.1 & 73.9 & 56.4 & \\textbf{\\underline{65.1}} \\\\\n\nGenSen & \\underline{66.2} & \\underline{7.7} & 83.1 & \\underline{76.6}/\\underline{83.0} & \\underline{82.9}/\\underline{59.8} & \\textbf{\\underline{79.3}}/\\textbf{\\underline{79.2}} & \\underline{71.4}/\\underline{71.3} & \\underline{78.6} & \\textbf{\\underline{59.2}} & \\textbf{\\underline{65.1}} \\\\\n\n\\bottomrule\n\\end{tabular}\n\\caption{Baseline performance on the GLUE task test sets. \nFor MNLI, we report accuracy on the matched and mismatched test sets. For MRPC and Quora, we report accuracy and F1. For STS-B, we report Pearson and Spearman correlation. For CoLA, we report Matthews correlation. For all other tasks we report accuracy. All values are scaled by 100.\nA similar table is presented on the online platform. % SB: The appendix shows this.\n}\n\\label{tab:benchmark}\n\\end{table*}\n\n\\section{Benchmark Results}\\label{sec:experiments}\n\nWe train three runs of each model and evaluate the run with the best macro-average development set performance (see Table \\ref{tab:benchmark-dev} in Appendix \\ref{sec:apdx_dev}). For single-task and sentence representation models, we evaluate the best run for each individual task. We present performance on the main benchmark tasks in \\autoref{tab:benchmark}. \n\nWe find that multi-task training yields better overall scores over single-task training amongst models using attention or ELMo.\nAttention generally has negligible or negative aggregate effect in single task training, but helps in multi-task training.\nWe see a consistent improvement in using ELMo embeddings in place of GloVe or CoVe embeddings, particularly for single-sentence tasks.\nUsing CoVe has mixed effects over using only GloVe.\n\nAmong the pre-trained sentence representation models, we observe fairly consistent gains moving from CBoW to Skip-Thought to Infersent and GenSen.\nRelative to the models trained directly on the GLUE tasks, InferSent is competitive and GenSen outperforms all but the two best.\n\nLooking at results per task, we find that the sentence representation models substantially underperform on CoLA compared to the models directly trained on the task. \nOn the other hand, for STS-B, models trained directly on the task lag significantly behind the performance of the best sentence representation model.\nFinally, there are tasks for which no model does particularly well. On WNLI, no model exceeds most-frequent-class guessing (65.1\\%) and we substitute the model predictions for the most-frequent baseline. On RTE and in aggregate, even our best baselines leave room for improvement.\nThese early results indicate that solving GLUE is beyond the capabilities of current models and methods.\n\n\\begin{table*}[t]\n\\centering \\fontsize{8.4}{10.1}\\selectfont \\setlength{\\tabcolsep}{0.5em}\n\\begin{tabular}{lr@{\\hskip 3em}rrrr@{\\hskip 3em}rrrrrr}\n \\toprule\n && \\multicolumn{4}{l}{\\textbf{~~Coarse-Grained}} & \\multicolumn{6}{c}{\\textbf{Fine-Grained}}\\\\ % SB: Something odd is going on with centering. Fixing manually for now.\n\\textbf{Model} & \\textbf{All} & \\textbf{LS} & \\textbf{PAS} & \\textbf{L} & \\textbf{K} & \\textbf{UQuant} & \\textbf{MNeg} & \\textbf{2Neg} & \\textbf{Coref} & \\textbf{Restr} & \\textbf{Down} \\\\\n\\midrule\n\\multicolumn{12}{c}{Single-Task Training} \\\\\n\\midrule\nBiLSTM & 21 & 25 & 24 & 16 & 16 & 70 & \\underline{53} & 4 & 21 & -15 & \\textbf{\\underline{12}} \\\\\n~~+ELMo & 20 & 20 & 21 & 14 & 17 & 70 & 20 & \\textbf{\\underline{42}} & 33 & -26 & -3 \\\\\n~~+CoVe & 21 & 19 & 23 & 20 & \\underline{18} & 71 & 47 & -1 & 33 & -15 & 8 \\\\\n~~+Attn & 25 & 24 & 30 & 20 & 14 & 50 & 47 & 21 & \\underline{38} & -8 & -3 \\\\\n~~+Attn, ELMo & \\textbf{\\underline{28}} & \\textbf{\\underline{30}} & \\textbf{\\underline{35}} & \\textbf{\\underline{23}} & 14 & \\textbf{\\underline{85}} & 20 & \\textbf{\\underline{42}} & 33 & -26 & -3 \\\\\n~~+Attn, CoVe & 24 & 29 & 29 & 18 & 12 & 77 & 50 & 1 & 18 & \\underline{-1} & \\textbf{\\underline{12}} \\\\\n\n\\midrule\n\\multicolumn{12}{c}{Multi-Task Training} \\\\\n\\midrule\n\nBiLSTM & 20 & 13 & 24 & 14 & 22 & \\underline{71} & 17 & -8 & 31 & -15 & 8 \\\\\n~~+ELMo & 21 & \\underline{20} & 21 & \\underline{19} & 21 & \\underline{71} & \\textbf{\\underline{60}} & 2 & 22 & 0 & \\textbf{\\underline{12}} \\\\\n~~+CoVe & 18 & 15 & 11 & 18 & \\textbf{\\underline{27}} & 71 & 40 & \\underline{7} & \\textbf{\\underline{40}} & 0 & 8 \\\\\n~~+Attn & 18 & 13 & 24 & 11 & 16 & \\underline{71} & 1 & -12 & 31 & -15 & 8 \\\\\n~~+Attn, ELMo & \\underline{22} & 18 & \\underline{26} & 13 & 19 & 70 & 27 & 5 & 31 & -26 & -3 \\\\\n~~+Attn, CoVe & 18 & 16 & 25 & 16 & 13 & \\underline{71} & 26 & -8 & 33 & \\textbf{\\underline{9}} & 8 \\\\\n\n\\midrule\n\\multicolumn{12}{c}{Pre-Trained Sentence Representation Models} \\\\\n\\midrule\nCBoW & 9 & 6 & 13 & 5 & 10 & 3 & 0 & \\underline{13} & 28 & \\underline{-15} & -11 \\\\\nSkip-Thought & 12 & 2 & 23 & 11 & 9 & 61 & 6 & -2 & \\underline{30} & \\underline{-15} & 0 \\\\\nInferSent & 18 & 20 & 20 & \\underline{15} & 14 & 77 & 50 & -20 & 15 & \\underline{-15} & -9 \\\\\nDisSent & 16 & 16 & 19 & 13 & \\underline{15} & 70 & 43 & -11 & 20 & -36 & -09 \\\\\nGenSen & \\underline{20} & \\underline{28} & \\underline{26} & 14 & 12 & \\underline{78} & \\underline{57} & 2 & 21 & \\underline{-15} & \\textbf{\\underline{12}} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Results on the diagnostic set. We report \\(R_3\\) coefficients between gold and predicted labels, scaled by 100.\nThe coarse-grained categories are \\textit{Lexical Semantics} (\\textbf{LS}),\n\\textit{Predicate-Argument Structure} (\\textbf{PAS}),\n\\textit{Logic} (\\textbf{L}),\nand \\textit{Knowledge and Common Sense} (\\textbf{K}). Our example fine-grained categories are \\textit{Universal Quantification} (\\textbf{UQuant}),\n\\textit{Morphological Negation} (\\textbf{MNeg}),\n\\textit{Double Negation} (\\textbf{2Neg}),\n\\textit{Anaphora/Coreference} (\\textbf{Coref}), \\textit{Restrictivity} (\\textbf{Restr}), and \\textit{Downward Monotone} (\\textbf{Down}).}\n\\label{tab:diagnostic}\n\\end{table*}\n\n\\section{Analysis}\n\nWe analyze the baselines by evaluating each model's MNLI classifier on the diagnostic set to get a better sense of their linguistic capabilities.\nResults are presented in \\autoref{tab:diagnostic}.\n\n\\paragraph{Coarse Categories}\nOverall performance is low for all models: The highest total score of 28 still denotes poor absolute performance.\nPerformance tends to be higher on Predicate-Argument Structure and lower on Logic, though numbers are not closely comparable across categories.\nUnlike on the main benchmark, the multi-task models are almost always outperformed by their single-task counterparts.\nThis is perhaps unsurprising, since with our simple multi-task training regime, there is likely some destructive interference between MNLI and the other tasks.\nThe models trained on the GLUE tasks largely outperform the pretrained sentence representation models, with the exception of GenSen.\nUsing attention has a greater influence on diagnostic scores than using ELMo or CoVe, which we take to indicate that attention is especially important for generalization in NLI.\n\n\\paragraph{Fine-Grained Subcategories}\n\nMost models handle universal quantification relatively well.\nLooking at relevant examples, it seems that relying on lexical cues such as ``all'' often suffices for good performance.\nSimilarly, lexical cues often provide good signal in morphological negation examples.\n\nWe observe varying weaknesses between models.\nDouble negation is especially difficult for the GLUE-trained models that only use GloVe embeddings. \nThis is ameliorated by ELMo, and to some degree CoVe.\nAlso, attention has mixed effects on overall results, and models with attention tend to struggle with downward monotonicity.\nExamining their predictions, we found that the models are sensitive to hypernym/hyponym substitution and word deletion as a signal of entailment, but predict it in the wrong direction (as if the substituted/deleted word were in an upward monotone context).\nThis is consistent with recent findings by \\citet{mccoy2019subsequence} that these systems use the subsequence relation between premise and hypothesis as a heuristic shortcut.\nRestrictivity examples, which often depend on nuances of quantifier scope, are especially difficult for almost all models.\n\nOverall, there is evidence that going beyond sentence-to-vector representations, e.g. with an attention mechanism, might aid performance on out-of-domain data, and that transfer methods like ELMo and CoVe encode linguistic information specific to their supervision signal.\nHowever, increased representational capacity may lead to overfitting, such as the failure of attention models in downward monotone contexts.\nWe expect that our platform and diagnostic dataset will be useful for similar analyses in the future, so that model designers can better understand their models' generalization behavior and implicit knowledge. \n\n\\section{Conclusion}\\label{sec:conclusion}\n\nWe introduce GLUE, \na platform and collection of resources for evaluating and analyzing natural language understanding systems. \nWe find that, in aggregate, models trained jointly on our tasks see better performance than the combined performance of models trained for each task separately.\nWe confirm the utility of attention mechanisms and transfer learning methods such as ELMo in NLU systems, which combine to outperform the best sentence representation models on the GLUE benchmark, but still leave room for improvement.\nWhen evaluating these models on our diagnostic dataset, we find that they fail (often spectacularly) on many linguistic phenomena, suggesting possible avenues for future work.\nIn sum, the question of how to design general-purpose NLU models remains unanswered, and we believe that GLUE can provide fertile soil for addressing this challenge.\n\n\\section*{Acknowledgments}\nWe thank Ellie Pavlick, Tal Linzen, Kyunghyun Cho, and Nikita Nangia for their comments on this work at its early stages, and we thank Ernie Davis, Alex Warstadt, and Quora's Nikhil Dandekar and Kornel Csernai for providing access to private evaluation data. \nThis project has benefited from financial support to SB by Google, Tencent Holdings, and Samsung Research, and to AW from AdeptMind and an NSF Graduate Research Fellowship.\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nWe present a large-scale dataset, \\ReCoRD,\nfor machine reading comprehension requiring commonsense reasoning.\nExperiments on this dataset demonstrate that the performance of state-of-the-art MRC systems fall far behind human performance.\n\\ReCoRD represents a challenge for future research to bridge the gap between human and machine commonsense reading comprehension. \n\\ReCoRD is available at \\url{http://nlp.jhu.edu/record}.\n\\kev{How about emphasize more the motivation of common sense more in the abstract?}\n\\end{abstract}\n\n\\section{Introduction}\n\\ben{It is a little weird that RECORD is not spelled out in the abstract, but especially odd that it isn't spelled out in the Introduction.  I would remove the footnote, put that content in the Introduction}\n\n\\ben{@kev agree.  ... Human and Machine Commonsense Reading Comprehension}\n\n\\ben{Methods in machine reading comprehension (MRC) are driven by the datasets available -- such as curated by \\newcite{deepmind-cnn-dailymail}, \\newcite{cbt}, \\newcite{squad}, \\newcite{newsqa}, and \\newcite{msmarco} -- where an MRC task is commonly defined as answering a question given some passage. However ...}\n\nMachine reading comprehension (MRC) is a central task in natural language understanding, with techniques lately driven by a surge of large-scale datasets~\\cite{deepmind-cnn-dailymail,cbt,squad,newsqa,msmarco}, usually formalized as a task of answering questions given a passage. An increasing number of analyses~\\cite{adversarial-squad,squad-v2,how-much-reading} have revealed that a large portion of questions in these datasets can be answered by simply matching the patterns between the question and the answer sentence in the passage.\nWhile systems may match or even outperform humans on these datasets, our intuition suggests that there are at least some instances in human reading  comprehension that require more than what existing challenge tasks are emphasizing.\n\\ben{This \"thus\" claim is far too strong.  You haven't cited anything that says humans *don't* rely on simple pattern matching, you just rely on an implicit assumption that 'surely humans must be doing something complicated when they read'.  If a system performs as well as a human on a task, the conclusion shouldn't immediately be that the task is too easy, it should more subtly be that new datasets are then needed to see if the inference mechanisms hold up, where the creation of the datasets can be based based on an explicitly stated intuition that humans \\emph{may} rely on more than pattern matching.  It is a hypothesis at this point in the Introduction, that systems doing well on earlier datasets won't also do well on yours.  You expect they will fail, and even design the dataset specifically around their failure cases. }\n\\ben{I would say: While systems may match or even outperform humans on these datasets, our intuition suggests that there are at least some instances in human reading  comprehension that require more than what existing challenge tasks are stressing.}\nOne primary type of questions these datasets lack are the ones that require reasoning over common sense or understanding across multiple sentences in the passage~\\cite{squad,newsqa}.\n\\ben{This statement is given without citation: why do you claim that common sense is missing?  Do you provide an analysis later in this paper that supports it?  If so, provide a forward reference.  If you can cite earlier work, do so.  Otherwise, remove or soften this statement, e.g., \"We hypothesize that one type of question ...\".  And then in next sentence, rather than \"To overcome this limitation\", which you haven't proven yet actually exists, you would say: \"To help evaluate this question, we introduce ...\"}\n\\ben{rather than \"most of which require\", say \"most of which seem to require some aspect of reasoning beyond immediate pattern matching\".  The SWAG / BERT case should be fresh in your mind as you write this introduction, and where-ever you are tempted to declare things in absolute terms.   The more you go on the record as THIS DATASET REQUIRES COMMONSENSE then the more you look silly later if someone finds a 'trick' to solve it.  A more honest and safer way to put this is to exactly reference the SWAG/BERT issue at some point in this paper, acknowledging that prior claims to have constructed commonsense datasets have been shown to either be false, or to imply that commonsense reasoning can be equated to large scale language modeling.  You can cite Rachel's Script Induction as Language Modeling paper, JOCI,  and the reporting bias article, perhaps all in a footnote, when commenting that researchers have previously raised concerns about the idea that all of common sense can be derived from corpus co-occurrence statistics.}\n\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=0.48\\textwidth]{fig/better-example.pdf}\n\\caption{An example from \\ReCoRD. \nThe \\textbf{passage} is a snippet from a news article followed by some bullet points which summarize the news event. Named entities highlighted in the passage are possible answers to the query.\nThe \\textbf{query} is a statement that is factually supported by the passage.\n    $\\mathbf{X}$ in the statement indicates a missing named entity.\n    The goal is to find the correct entity in the passage that best fits $\\mathbf{X}$.\\label{fig:example}}\n\\end{figure}\n\nTo overcome this limitation, we introduce a large-scale dataset for reading comprehension, \\ReCoRD (\\textipa{[\"rEk@rd]}), which consists of over 120,000 examples, most of which require deep commonsense reasoning.\n\\ReCoRD is an acronym for the \\textbf{Re}ading \\textbf{Co}mprehension with \\textbf{Co}mmonsense \\textbf{R}easoning \\textbf{D}ataset. \n\n\\Cref{fig:example} shows a \\ReCoRD example: \nthe passage describes a lawsuit claiming that the band  ``\\emph{Led Zeppelin}'' had plagiarized the song ``\\emph{Taurus}'' to their most iconic song, ``\\emph{Stairway to Heaven}''.\nThe cloze-style query asks what does ``\\emph{Stairway to Heaven}'' sound similar to. \nTo find the correct answer, we need to understand from the passage that ``\\emph{a copyright infringement case alleges that `Stairway to Heaven' was taken from `Taurus'}'', and from the bullet point that ``\\emph{these two songs are claimed similar}''. \nThen based on the commonsense knowledge that ``\\emph{if two songs are claimed similar, it is likely that (parts of) these songs sound almost identical}'', we can reasonably infer that the answer is ``\\emph{Taurus}''.\n\\kev{This example is good, but you might need to make sure the reader reads the whole passage first or else it may be hard to follow. Maybe add a few more sentences to explain Figure 1 in the paragraph here.} \n\nDiffering from most of the existing MRC datasets, all queries and passages in \\ReCoRD are automatically mined from news articles, which maximally reduces the human elicitation bias \\cite{reporting-bias,vision-reporting-bias,joci},\nand the data collection method we propose is cost-efficient.\n\\kev{You should have one of these comparison tables that lists multiple MRC datasets and compares different features}\nFurther analysis shows that\na large portion of \\ReCoRD requires commonsense reasoning.\n\nExperiments on \\ReCoRD demonstrate that human readers are able to achieve a high performance at 91.69 F1, whereas the state-of-the-art MRC models fall far behind at 46.65 F1.\nThus, \\ReCoRD presents a real challenge for future research to bridge the gap between human and machine commonsense reading comprehension.\n\\ben{this is a bulky URL: I will pay the small fee to register some domain name that is more slick than this}\n\\ben{about the leaderboard on the website: I think it a little misleading to have Google Brain and IBM Watson, etc. as the names on the leaderboard, if it is really you running their code.  Better would be \"JHU (modification of Google Brain system)\", \"JHU (modification of IBM Watson system)\", ... .}\n\n\\section{Task Motivation}\n\\label{sec:task}\n\n\\begin{quote}\n  {\\emph{A program has common sense if it automatically deduces for itself a sufficiently wide class of immediate consequences of anything it is told and what it already knows.} --~\\newcite{mccarthy59}}\n\\end{quote}\n\n\\noindent\\textbf{Commonsense Reasoning in MRC}\nAs illustrated by the example in \\Cref{fig:example}, the commonsense knowledge ``\\emph{if two songs are claimed similar, it is likely that (parts of) these songs sound almost identica}'' is not explicitly described in the passage, but is necessary to acquire in order to generate the answer.\nHuman is able to infer the answer because the commonsense knowledge is commonly known by nearly all people.\nOur goal is to evaluate whether a machine is able to learn such knowledge.\nHowever, since commonsense knowledge is massive and mostly implicit, defining an explicit free-form evaluation is challenging~\\cite{wsc}.\nMotivated by \\newcite{mccarthy59}, we instead evaluate a machine's ability of commonsense reasoning -- a reasoning process requiring commonsense knowledge; that is, if a machine has common sense, it can deduce for itself the likely consequences or details of anything it is told and what it already knows rather than the unlikely ones.\nTo formalize it in MRC, given a passage $\\mathbf{p}$ (i.e., ``\\emph{anything it is told}'' and ``\\emph{what it already knows}''),\nand a set of consequences or details $\\mathcal{C}$ which are factually supported by the passage $\\mathbf{p}$ with different likelihood,\nif a machine $\\mathbf{M}$ has common sense, it can choose the most likely consequence or detail $\\mathbf{c}^*$ from $\\mathcal{C}$, i.e., \n\\begin{equation}\n\\label{eq:csr-in-mrc}\n    \\mathbf{c}^* = \\argmax_{\\mathbf{c} \\in \\mathcal{C}}P(\\mathbf{c}\\mid\\mathbf{p},\\mathbf{M}).\n\\end{equation}\n\n\\kev{What are the properties of $o$? What can be a consequence? Be more specific or give examples.}\n\n\\noindent\\textbf{Task Definition} With the above discussion, we propose a specific task to evaluate a machine's ability of commonsense reasoning in MRC: as shown in \\Cref{fig:example}, given a passage $\\mathbf{p}$ describing an event, a set of text spans $\\mathbf{E}$ marked in $\\mathbf{p}$, and a cloze-style query $Q(\\mathbf{X})$ with a missing text span indicated by $\\mathbf{X}$,\na machine $\\mathbf{M}$ is expected to act like human, reading the passage $\\mathbf{p}$ and then using its hidden commonsense knowledge to choose a text span $\\mathbf{e}\\in\\mathbf{E}$ that best fits $\\mathbf{X}$, i.e., \n\\begin{equation}\n\\label{eq:task}\n    \\mathbf{e}^* = \\argmax_{\\mathbf{e} \\in \\mathbf{E}}P(Q(\\mathbf{e})\\mid\\mathbf{p},\\mathbf{M}).\n\\end{equation}\n\nOnce the cloze-style query $Q(\\mathbf{X})$ is filled in by a text span $\\mathbf{e}$, the resulted statement $Q(\\mathbf{e})$ becomes a consequence or detail $\\mathbf{c}$ as described in \\Cref{eq:csr-in-mrc}, which is factually supported by the passage with certain likelihood.\n\n\\kev{There's a disconnect between this paragraph and the previous one. How do you jump from $o$ to Q(e) and the ineqality to argmax? Also, I'm not sure if \"cloze\" is defined anywhere: you might need a one-sentence explanation in case the reader is not familiar.}\n\n\\section{Data Collection}\n\n\\kev{First add motivation about general philosophy of data collection} \nWe describe the framework for automatically generating the dataset, \\ReCoRD, for our task defined in \\Cref{eq:task},\nwhich consists of passages with text spans marked, cloze-style queries, and reference answers.\nWe collect \\ReCoRD in four stages as shown in Figure~\\ref{fig:flow-chart}:\n(1) curating CNN/Daily Mail news articles,\n(2) generating passage-query-answers triples based on the news articles,\n(3) filtering out the queries that can be easily answered by state-of-the-art MRC models,\nand (4) filtering out the queries ambiguous to human readers.\n\n\\begin{figure}[!ht]\n\\centering\n\\includegraphics[width=0.45\\textwidth]{fig/flow-chart.pdf}\n\\caption{The overview of data collection stages.\\label{fig:flow-chart}}\n\\end{figure}\n\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=0.98\\textwidth]{fig/running-example.pdf}\n\\caption{Passage-query-answers generation from a CNN news article.\\label{fig:example-for-stage2}}\n\\end{figure*}\n\n\\subsection{News Article Curation}\n\\label{sec:news-curation}\nWe choose to create \\ReCoRD by exploiting news articles, because the structure of news makes it a good source for our task:\nnormally, the first few paragraphs of a news article summarize the news event, which can be used to generate passages of the task;\nand the rest of the news article provides consequences or details of the news event, which can be used to generate queries of the task.\nIn addition, news providers such as CNN and Daily Mail supplement their articles with a number of bullet points~\\cite{Ranknet,story-highlight-generation,deepmind-cnn-dailymail}, which outline the highlights of the news and hence form a supplemental source for generating passages.\n\nWe first downloaded CNN and Daily Mail news articles using the script\\footnote{\\url{https://github.com/deepmind/rc-data}} provided by~\\citet{deepmind-cnn-dailymail}, \nand then sampled 148K articles from CNN and Daily Mail.\nIn these articles, named entities and their coreference information have been annotated by a Google NLP pipeline, and will be used in the second stage of our data collection.\nSince these articles can be easily downloaded using the public script, we are concerned about potential cheating if using them as the source for generating the dev./test datasets.\nTherefore, we crawled additional 22K news articles from the CNN and Daily Mail websites.\nThese crawled articles have no overlap with the articles used in~\\citet{deepmind-cnn-dailymail}.\nWe then ran the state-of-the-art named entity recognition model~\\cite{elmo} and the end-to-end coreference resolution model~\\cite{end-to-end-coref} provided by AllenNLP~\\cite{allennlp} to annotate the crawled articles.\nOverall, we have collected 170K CNN/Daily Mail news articles with their named entities and coreference information annotated.\n\n\\subsection{Passage-Query-Answers Generation}\nAll passages, queries and answers in \\ReCoRD were automatically generated from the curated news articles.\n\\Cref{fig:example-for-stage2} illustrates the generation process. (1) we split each news article into two parts as described in \\Cref{sec:news-curation}:\nthe first few paragraphs which summarize the news event,\nand the rest of the news which provides the details or consequences of the news event.\nThese two parts make a good source for generating passages and queries of our task respectively.\n(2) we enriched the first part of news article with the bullet points provided by the news editors.\nThe first part of news article, together with the bullet points, is considered as a candidate passage.\nTo ensure that the candidate passages are informative enough, we required the first part of news article to have at least 100 tokens and contain at least four different entities.\n(3) for each candidate passage, the second part of its corresponding news article was split into sentences by Stanford CoreNLP \\cite{corenlp}.\nThen we selected the sentences that satisfy the following conditions as potential details or consequences of the news event described by the passage: \n\\begin{itemize}[itemsep=0pt,topsep=6pt,leftmargin=10pt]\n    \\item Sentences should have at least 10 tokens, as longer sentences contain more information and thus are more likely to be inferrable details or consequences.\n    \\item Sentences should not be questions, as we only consider details or consequences of a news event, not questions.\n    \\item Sentences should not have 3-gram overlap with the corresponding passage, so they are less likely to be paraphrase of sentences in the passage.\n    \\item Sentences should have at least one named entity, so that we can replace it with $\\mathbf{X}$ to generate a cloze-style query.\n    \\item All named entities in sentences should have precedents in the passage according to coreference, so that the sentences are not too disconnected from the passage, and the correct entity can be found in the passage to fill in $\\mathbf{X}$. \n\\end{itemize}\nFinally, we generated queries by replacing entities in the selected sentences with $\\mathbf{X}$.  \nWe only replaced one entity in the selected sentence each time, and generated one cloze-style query.\nBased on coreference, the precedents of the replaced entity in the passage became reference answers to the query. \nThe passage-query-answers generation process matched our task definition in \\Cref{sec:task}, and therefore created queries that require some aspect of reasoning beyond immediate pattern matching.\nIn total, we generated 770k (passage, query, answers) triples.\n\n\\subsection{Machine Filtering}\nAs discussed in \\citet{adversarial-squad,squad-v2,adversarial-training,how-much-reading},\nexisting MRC models mostly learn to predict the answer by simply paraphrasing questions into declarative forms, and then matching them with the sentences in the passages.\nTo overcome this limitation, we filtered out triples whose queries can be easily answered by\nthe state-of-the-art MRC architecture, Stochastic Answer Networks (SAN) \\cite{san}.\nWe choose SAN because it is competitive on existing MRC datasets, and it has components widely used in many MRC architectures such that low bias was anticipated in the filtering (which is confirmed by evaluation in \\Cref{sec:evaluation}). \nWe used SAN to perform a five-fold cross validation on all 770k triples.\nThe SAN models correctly answered 68\\% of these triples. \nWe excluded those triples, and only kept 244k triples that could not be answered by SAN. \nThese triples contain queries which could not be answered by simple paraphrasing, and other types of reasoning such as commonsense reasoning and multi-sentence reasoning are needed.\n\\kev{Briefly mention why you use SAN, i.e. it's competitive on current benchmarks like SQuAD. Also mention whether this may cause some bias in the filtering, compared to using some other system, and why your methodology is still ok.}\n\n\\subsection{Human Filtering}\n\\label{sec:human-filtering}\nSince the first three stages of data collection were fully automated,\nthe resulted triples could be noisy and ambiguous to human readers.\nTherefore, we employed crowdworkers to validate these triples.\nWe used Amazon Mechanical Turk for validation.\nCrowdworkers were required to: 1) have a 95\\% HIT acceptance rate, 2) a minimum of 50 HITs, 3) be located in the United States, Canada, or Great Britain, and 4) not be granted the qualification of poor quality (which we will explain later in this section).\nWorkers were asked to spend at least 30 seconds on each assignment, and paid \\$3.6 per hour on average.\n\n\\begin{figure}[!ht]\n\\centering\n\\includegraphics[width=0.49\\textwidth]{fig/hit-screenshot.png}\n\\caption{The crowdsourcing web interface.\\label{fig:hit}}\n\\end{figure}\n\n\\Cref{fig:hit} shows the crowdsourcing web interface.\nEach HIT corresponds to a triple in our data collection.\nIn each HIT assignment, we first showed the expandable instructions for first-time workers, to help them better understand our task (see the \\Cref{sec:hit-instructions}).\nThen we presented workers with a passage in which the named entities are highlighted and clickable.\nAfter reading the passage, workers were given a supported statement with a placeholder (i.e., a cloze-style query) indicating a missing entity.\nBased on their understanding of the events that might be inferred from the passage, workers were asked to find the correct entity in the passage that best fits the placeholder. \nIf workers thought the answer is not obvious, they were allowed to guess one, and were required to report that case in the feedback box.\nWorkers were also encouraged to write other feedback.\n\nTo ensure quality and prevent spamming, we used the reference answers in the triples to compute workers' average performance after every 1000 submissions. \nWhile there might be coreference or named entity recognition errors in the reference answers, \nas reported in \\citet{exam-of-cnn-dailymail} (also confirmed by our analysis in \\Cref{sec:data-analysis}), they only accounted for a very small portion of all the reference answers. Thus, the reference answers could be used for comparing workers' performance. \nSpecifically, if a worker's performance was significantly lower than the average performance of all workers, we blocked the worker by granting the qualification of poor quality.\nIn practice, workers were able to correctly answer about 50\\% of all queries. \nWe blocked workers if their average accuracy was lower than 20\\%, and then republished their HIT assignments.\nOverall, 2,257 crowdworkers have participated in our task, and 51 of them have been granted the qualification of poor quality. \n\n\\noindent\\textbf{Train\\,/\\,Dev.\\,/\\,Test Splits}\nAmong all the 244k triples collected from the third stage, we first obtained one worker answer for each triple. \nCompared to the reference answers, workers correctly answered queries in 122k triples.\nWe then selected around 100k correctly-answered triples as the training set,\nrestricting the origins of these triples to the news articles used in \\citet{deepmind-cnn-dailymail}.\nAs for the development and test sets, we solicited another worker answer to further ensure their quality.\nTherefore, each of the rest 22k triples has been validated by two workers.\nWe only kept 20k triples that were correctly answered by both workers.\nThe origins of these triples are either articles used in \\citet{deepmind-cnn-dailymail} or articles crawled by us (as described in \\Cref{sec:news-curation}), with a ratio of 3:7.\nFinally, we randomly split the 20k triples into development and test sets, with 10k triples for each set.\n\\Cref{tab:statistics} summarizes the statistics of our dataset, \\ReCoRD.\n\n\\begin{table}[!ht]\n\\small\n\\begin{tabular}{@{}l|r|r|r|r@{}}\n\\toprule\n                     & \\multicolumn{1}{c|}{Train}   & \\multicolumn{1}{c|}{Dev.}   & \\multicolumn{1}{c|}{Test}   & \\multicolumn{1}{c}{Overall} \\\\ \\midrule\nqueries          & 100,730                      & 10,000                      & 10,000                      & 120,730                     \\\\\nunique passages    & 65,709                       & 7,133                       & 7,279                       & 80,121                      \\\\ \\midrule\npassage vocab.       & 352,491 & 93,171 & 94,386 & 395,356 \\\\\nquery vocab.      & 119,069 & 30,844 & 31,028 & 134,397 \\\\ \\midrule\ntokens\\,/\\,passage   & 169.5                        & 168.6                       & 168.1                       & 169.3                       \\\\\nentities\\,/\\,passage & 17.8                         & 17.5                        & 17.3                        & 17.8                        \\\\\ntokens\\,/\\,query  & 21.3                         & 22.1                        & 22.2                        & 21.4                        \\\\ \\bottomrule\n\\end{tabular}\n\\caption{Statistics of \\ReCoRD}\n\\label{tab:statistics}\n\\end{table}\n\n\\begin{table*}[!t]\n\\centering\n\\small\n\\begin{tabular}{@{}m{2cm}m{4.8cm}m{7cm}r@{}}\n\\toprule\n\\multicolumn{1}{c}{Reasoning} & \\multicolumn{1}{c}{Description}                                                                                                                                                                                                             & \\multicolumn{1}{c}{Example} & \\multicolumn{1}{r}{\\%} \\\\ \\midrule\nParaphrasing                  & The answer sentence can be found by paraphrasing the query with some syntactic or lexical variation.                                                                                                               &    \n\\noindent\\textbf{P:} \\ldots\\underline{\\textcolor{blue}{Ralph Roberts}}\\dots  then acquired other cable systems, changed the name of the company to \\underline{\\textcolor{blue}{Comcast}} and ran the company until he was aged 82\n\n\\noindent\\textbf{Q:} \\textcolor{red}{$\\mathbf{X}$} began acquiring smaller cable systems and built the company into the nation's fifth-largest by 1988.\n\n\\noindent\\textbf{A:} [Ralph Roberts]\n& 3\\%                            \\\\ \\midrule\nPartial Clue                  & Although a complete semantic match cannot be found between the query and the passage, the answer can be inferred through partial clues, such as some word/concept overlap. &     \n\\noindent\\textbf{P:}\\ldots\n\\underline{\\textcolor{blue}{Hani Al}}-\\underline{\\textcolor{blue}{Sibai}} says he has `severe mobility problems' to get disability cash\\ldots\n\n\\noindent\\textbf{Q:} However the photographs caught \\textcolor{red}{$\\mathbf{X}$}-Sibai walking with apparent ease in the sunshine.\n\n\\noindent\\textbf{A:} [Hani Al]\n& 10\\%                           \\\\ \\midrule\nMulti-sentence Reasoning      & It requires anaphora, or higher-level fusion of multiple sentences to find the answer.                                                                                                                                                      &      \n\\noindent\\textbf{P:} \\underline{\\textcolor{blue}{Donald Trump}} is officially a \\$10 billion man\\ldots HIs campaign won't release a copy of the financial disclosure even though the \\underline{\\textcolor{blue}{FEC}} says it can do so on its own\\ldots\n\n\\noindent\\textbf{Q:} The \\textcolor{red}{$\\mathbf{X}$} campaign did provide a one-page summary of the billionaire's investment portfolio, which is remarkably modest for a man of his means.\n\n\\noindent\\textbf{A:} [Donald Trump]\n& 6\\%                            \\\\ \\midrule\nCommonsense Reasoning        & It requires inference drew on common sense as well as multi-sentence reasoning to find the answer.                                                                                                &         \n\\noindent\\textbf{P:} \n\\ldots\\underline{\\textcolor{blue}{Daniela Hantuchova}} knocks \\underline{\\textcolor{blue}{Venus Williams}} out of \\underline{\\textcolor{blue}{Eastbourne}} 6-2 5-7 6-2 \\ldots\n\n\\noindent\\textbf{Q:} Hantuchova breezed through the first set in just under 40 minutes after breaking Williams' serve twice to take it 6-2 and led the second 4-2 before \\textcolor{red}{$\\mathbf{X}$} hit her stride.\n\n\\noindent\\textbf{A:} [Venus Williams]\n& 75\\%                           \\\\ \\midrule\nAmbiguous                     & The passage is not informative enough, or the query does not have a unique answer.                                                                                                                                                       &         \n\\noindent\\textbf{P:} The supermarket wars have heated up with the chief executive of \\underline{\\textcolor{blue}{Wesfarmers}} suggesting successful rival \\underline{\\textcolor{blue}{Aldi}} may not be paying its fair share of tax in \\underline{\\textcolor{blue}{Australia}}\\ldots\n\n\\noindent\\textbf{Q:} \\textcolor{red}{$\\mathbf{X}$}'s average corporate tax rate for the last three years was almost 31 per cent of net profit, and in 2013 it paid \\$81.6 million in income tax.\n\n\\noindent\\textbf{A:} [Aldi]\n& 6\\%                            \\\\ \\bottomrule\n\\end{tabular}\n\\caption{An analysis of types of reasoning needed in 100 random samples from the dev. set of \\ReCoRD.}\n\\label{tab:reason-types}\n\\end{table*}\n\n\\section{Data Analysis}\n\\label{sec:data-analysis}\n\\ReCoRD differs from other reading comprehension datasets due to its unique requirement for reasoning more than just paraphrasing.\nIn this section, we provide a qualitative analysis of \\ReCoRD which highlights its unique features.\n\n\\noindent\\textbf{Reasoning Types} We sampled 100 examples from the development set, and then manually categorized them into types shown in \\cref{tab:reason-types}.\nThe results show that significantly different from existing datasets such as SQuAD~\\cite{squad}, and NewsQA~\\cite{newsqa}, \\ReCoRD requires commonsense reasoning to answer 75\\% of queries.\nOwing to the machine filtering stage, only 3\\% queries could be answered by paraphrasing.  \nThe small percentage (6\\%) of ambiguous queries demonstrate the benefit of the human filtering stage.\nWe also noticed that 10\\% queries can be answered through partial clues.\nAs the example shows, some of partial clues were caused by the incompleteness of named entity recognition in the stage of news article curation.\n\n\\noindent\\textbf{Types of Commonsense Reasoning} \nFormalizing the commonsense knowledge needed for even simple reasoning problems is a huge undertaking.\nBased on the observation of the sampled queries that required commonsense reasoning, we roughly categorized them into the following four coarse-gained types:\n\n\\begin{itemize}[itemsep=1pt,topsep=1pt,leftmargin=8pt]\n    \\item[]\\textbf{Conceptual Knowledge}: the presumed knowledge of properties of concepts~\\cite{wordnet,conceptnet,class-attributes,joci}.\n    \\item[]\\textbf{Causal Reasoning}: the causal bridging inference invoked between two events, which is validated against common sense~\\cite{bridging-inference,copa}. \n    \\item[]\\textbf{Na\\\"ive Psychology}: the predictable human mental states in reaction to events~\\cite{naive-psychology}.\n    \\item[]\\textbf{Other}: Other types of common sense, such as social norms, planning, spatial reasoning, etc.\n\\end{itemize}\n\nWe annotated one or more types to each of these queries, and computed the percentage of them in these queries as shown in \\Cref{tab:commonsense-types}.\n\n\\begin{table*}[!t]\n\\centering\n\\small\n\\begin{tabular}{@{}m{2cm}m{12cm}r@{}}\n\\toprule\n\\multicolumn{1}{l}{Reasoning} & \\multicolumn{1}{c}{Example} & \\multicolumn{1}{r}{\\%} \\\\ \\midrule\nConceptual Knowledge  &    \n\\noindent\\textbf{P:} Suspended hundreds of feet in the air amid glistening pillars of ice illuminated with ghostly lights from below, this could easily be a computer-generated scene from the latest sci-fi blockbuster movie. But in fact these ethereal photographs were taken in real life\\ldots\ncaptured by photographer \\underline{\\textcolor{blue}{Thomas Senf}} as climber \\underline{\\textcolor{blue}{Stephan Siegrist}}, 43, scaled frozen waterfall\\ldots\n\n\\noindent\\textbf{Q:} With bright lights illuminating his efforts from below, Mr \\textcolor{red}{$\\mathbf{X}$} appears to be on the set of a sci-fi movie.\n\n\\noindent\\textbf{A:} [Stephan Siegrist]\n\n\\noindent\\textbf{Commonsense knowledge:} \\emph{Scenes such as ``a person suspended hundreds of feet in the air amid glistening pillars of ice illuminated with ghostly lights from below'' tend to be found in sci-fi movies.}\n& 49.3\\%                            \\\\ \\midrule\nCausal \n\nReasoning   &     \n\\noindent\\textbf{P:} \\ldots\n\\underline{\\textcolor{blue}{Jamie Lee Sharp}}, 25, stole keys to \\pounds 40,000 \\underline{\\textcolor{blue}{Porsche Boxster}} during raid\\ldots\nHe filmed himself boasting about the car before getting behind the wheel\n\n\\noindent\\textbf{Q:} \\textcolor{red}{$\\mathbf{X}$} was  jailed for four years after pleading guilty to burglary, aggravated vehicle taking, driving whilst disqualified, drink-driving and driving without insurance.\n\n\\noindent\\textbf{A:} [Jamie Lee Sharp]\n\n\\noindent\\textbf{Commonsense knowledge:} \\emph{If a person steals a car, the person may be arrested and jailed.}\n& 32.0\\%                           \\\\ \\midrule\nNa\\\"ive \n\nPsychology &      \n\\noindent\\textbf{P:} \\underline{\\textcolor{blue}{Uruguay}} star \\underline{\\textcolor{blue}{Diego Forlan}} said Monday that he is leaving \\underline{\\textcolor{blue}{Atletico Madrid}} and is set to join \\underline{\\textcolor{blue}{Serie A}} \\underline{\\textcolor{blue}{Inter Milan}}\\ldots \\underline{\\textcolor{blue}{Forlan}} said ``\\ldots At the age of 33, going to a club like \\underline{\\textcolor{blue}{Inter}} is not an opportunity that comes up often\\ldots\"\n\n\\noindent\\textbf{Q:} ``I am happy with the decision that I have taken, it is normal that some players come and others go,\" \\textcolor{red}{$\\mathbf{X}$} added.\n\n\\noindent\\textbf{A:} [Diego Forlan, Forlan]\n\n\\noindent\\textbf{Commonsense knowledge:} \\emph{If a person has seized an valuable opportunity, the person will feel happy for it.}\n& 28.0\\%                            \\\\ \\midrule\nOther &         \n\\noindent\\textbf{P:} \nA \\underline{\\textcolor{blue}{British}} backpacker who wrote a romantic note to locate a handsome stranger after spotting him on a \\underline{\\textcolor{blue}{New Zealand}} beach has finally met her \\underline{\\textcolor{blue}{Romeo}} for the first time. \\underline{\\textcolor{blue}{Sarah Milne}}, from \\underline{\\textcolor{blue}{Glasgow}}, left a handmade poster for the man, who she saw in \\underline{\\textcolor{blue}{Picton}} on Friday\\ldots\nShe said she would return to the same spot in \\underline{\\textcolor{blue}{Picton}}, \\underline{\\textcolor{blue}{New Zealand}}, on Tuesday in search for him\\ldots\n \\underline{\\textcolor{blue}{William Scott Chalmers}} revealed himself as the man and went to meet her\\ldots\n\n\\noindent\\textbf{Q:} Mr Chalmers, who brought a bottle of champagne with him, walked over to where Milne was sitting and said ``Hello, I'm \\textcolor{red}{$\\mathbf{X}$}, you know you could have just asked for my number.''\n\n\\noindent\\textbf{A:} [William Scott Chalmers]\n\n\\noindent\\textbf{Commonsense knowledge:} \\emph{When two people meet each other for the first time, they will likely first introduce themselves.}\n& 12.0\\%                            \\\\ \\bottomrule\n\\end{tabular}\n\\caption{An analysis of specific types of commonsense reasoning in 75 random sampled queries illustrated in \\Cref{tab:reason-types} which requires common sense reasoning. A query may require multiple types of commonsense reasoning.}\n\\label{tab:commonsense-types}.\n\\end{table*}\n\n\\section{Evaluation}\n\\label{sec:evaluation}\n\nWe are interested in the performance of existing MRC architectures on \\ReCoRD.\nAccording to the task definition in~\\Cref{sec:task}, \\ReCoRD can be formalized as two types of machine reading comprehension (MRC) datasets: passages with cloze-style queries, or passages with queries whose answers are spans in the passage.\nTherefore, we can evaluate two types of MRC models on \\ReCoRD, and compare them with human performance.\nAll the evaluation is carried out based on the train\\,/dev.\\,/test split as illustrated in \\Cref{tab:statistics}.\n\n\\subsection{Methods}\n\n\\noindent\\textbf{DocQA}\\footnote{\\url{https://github.com/allenai/document-qa}}~\\cite{docqa} is a strong baseline model for queries with extractive answers.\nIt consists of components such as bi-directional attention flow~\\cite{bidaf} and self attention which are widely used in MRC models.\nWe also evaluate DocQA with ELMo~\\cite{elmo} to analyze the impact of largely pre-trained encoder on our dataset.\n\n\\noindent\\textbf{QANet}\\footnote{The official implementation of QANet is not released. We use the implementation at \\url{https://github.com/NLPLearn/QANet}.}~\\cite{qanet} is one of the top MRC models for SQuAD-style datasets.\nIt is different from many other MRC models due to the use of transformer~\\cite{transformer}.\nThrough QANet, we can evaluate the reasoning ability of transformer on our dataset.\n\n\\noindent\\textbf{SAN}\\footnote{\\url{https://github.com/kevinduh/san_mrc}}~\\cite{san} is also a top-rank MRC model.\nIt shares many components with DocQA, and employs a stochastic answer module.\nSince we used SAN to filter out easy queries in our data collection, it is necessary to verify that the queries we collect is hard for not only SAN but also other MRC architectures.\n\n\\noindent\\textbf{ASReader}\\footnote{\\url{https://github.com/rkadlec/asreader}}~\\cite{asreader} is a strong baseline model for cloze-style datasets such as~\\cite{deepmind-cnn-dailymail,cbt}.\nUnlike other baseline models which search among all text spans in the passage, ASReader directly predicts answers from the candidate named entities. \n\n\\noindent\\textbf{Language Models}\\footnote{\\url{https://github.com/tensorflow/models/tree/master/research/lm_commonsense}} (LMs)~\\cite{google-lms} trained on large corpora recently achieved the state-of-the-art scores on the Winograd Schema Challenge~\\cite{wsc}.\nFollowing in the same manner, we first concatenate the passage and the query together as a long sequence, and substitute $\\mathbf{X}$ in the long sequence with each candidate entity; we use LMs to compute the probability of each resultant sequence and the substitution that results in the most probable sequence will be the predicted answer.\n\n\\noindent\\textbf{Random Guess} acts as the lower bound of the evaluated models. It considers the queries in our dataset as cloze style, and randomly picks a candidate entity from the passage as the answer.\n\n\\subsection{Human Performance}\nAs described in \\Cref{sec:human-filtering}, we obtained two worker answers for each query in the development and test sets, and confirmed that each query has been correctly answered by two different workers. \nTo get human performance, we obtained an additional worker answer for each query, and compare it with the reference answers.\n\n\\subsection{Metrics}\nWe use two evaluation metrics similar to those used by SQuAD~\\cite{squad}.\nBoth ignore punctuations and articles (e.g., \\emph{a, an, the}).\n\n\\noindent\\textbf{Exact Match} (EM) measures the percentage of predictions that match any one of the reference answers exactly.\n\n\\noindent(Macro-averaged) \\textbf{F1} measures the average overlap between the prediction and the reference answers. \nWe treat the prediction and the reference answer as bags of tokens, and compute their F1.\nWe take the maximum F1 over all of the reference answers for a given query, and then average over all of the queries.\n\n\\subsection{Results}\nWe show the evaluation results in \\Cref{tab:performance}.\nHumans are able to get 91.31 EM and 91.69 F1 on the set, with similar results on the development set.\nIn contrast, the best automatic method -- DocQA with ELMo --  achieves 45.44 EM and 46.65 F1 on the test set, illustrating a significant gap between human and machine reading comprehension on \\ReCoRD.\nAll other methods without ELMo get EM/F1 scores significantly lower than DocQA with ELMo,\nwhich shows the positive impact of ELMo (see in \\Cref{sec:result-analysis}).\nWe also note that SAN leads to a result comparable with other strong baseline methods. \nThis confirms that since SAN shares general components with many MRC models, using it to do machine filtering does help us filter out queries that are relatively easy to all the methods we evaluate. \nFinally, to our surprise, the unsupervised method (i.e., LM) which achieved the state-of-the-art scores on the Winograd Schema Challenge only leads to a result similar to the random guess baseline: a potential explanation is the lack of domain knowledge on our dataset.\nWe leave this question for future work.\n\n\\begin{table}[!t]\n\\centering\n\\small\n\\begin{tabular}{@{}lrr|rr@{}}\n\\toprule\n\\multirow{2}{*}{}                 & \\multicolumn{2}{c}{Exact Match}                      & \\multicolumn{2}{|c}{F1}                             \\\\ \\cmidrule(l){2-5} \n                                  & \\multicolumn{1}{c}{Dev.} & \\multicolumn{1}{c|}{Test} & \\multicolumn{1}{c}{Dev.} & \\multicolumn{1}{c}{Test} \\\\ \\midrule\n\\multicolumn{1}{l|}{Human}        & \\textbf{91.28}           & \\textbf{91.31}            & \\textbf{91.64}           & \\textbf{91.69}           \\\\ \\midrule\\midrule\n\\multicolumn{1}{l|}{DocQA w/ ELMo}        & 44.13                    & 45.44                     & 45.39                    & 46.65                    \\\\ \n\\multicolumn{1}{l|}{DocQA w/o ELMo}  & 36.59                    & 38.52                     & 37.89                    & 39.76                    \\\\ \\midrule\n\\multicolumn{1}{l|}{SAN}          & 38.14                    & 39.77                     & 39.09                    & 40.72                    \\\\ \\midrule\n\\multicolumn{1}{l|}{QANet}        & 35.38                    & 36.51                     & 36.75                    & 37.79                    \\\\ \\midrule\\midrule\n\\multicolumn{1}{l|}{ASReader}     & 29.24                    & 29.80                     & 29.80                    & 30.35                    \\\\ \\midrule\n\\multicolumn{1}{l|}{LM}           & 16.73                    & 17.57                     & 17.41                    & 18.15                    \\\\ \\midrule\n\\midrule\n\\multicolumn{1}{l|}{Random Guess} & 18.41                    & 18.55                     & 19.06                    & 19.12                    \\\\ \\bottomrule\n\\end{tabular}\n\\caption{Performance of various methods and human.}\n\\label{tab:performance}\n\\end{table}\n\\kev{Are you shrinking Table 4 too much? Make sure you don't violate submission rules}\n\n\\subsection{Analysis}\n\\label{sec:result-analysis}\n\n\\noindent\\textbf{Human Errors}\nAbout 8\\% dev./test queries have not been correctly answered in the human evaluation.\nWe analyzed samples from these queries, and found that in most queries human was able to narrow down the set of possible candidate entities, but not able to find a unique answer. \nIn many cases, two candidate entities equally fit $\\mathbf{X}$ unless human has the specific background knowledge. \nWe show an example in the~\\Cref{sec:case-study}.\n\nFor the method analysis, we mainly analyzed the results of three representative methods: DocQA w/ ELMo, DocQA, and QANet.\n\n\\begin{figure}[!ht]\n\\centering\n\\includegraphics[width=0.40\\textwidth]{fig/venn.pdf}\n\\caption{The Venn diagram of correct predictions from various methods and human on the development set.\\label{fig:venn}}\n\\end{figure}\n\n\\noindent\\textbf{Impact of ELMo}\nAs shown in \\Cref{fig:venn}, among all three methods the correct predictions of DocQA w/ ELMo have the largest overlap\n(92.6\\%) with the human predictions.\nAs an ablation study, we analyzed queries which were only correctly answered after ELMo was added.\nWe found that in some cases ELMo helped the prediction by incorporating the knowledge of language models.\nWe show an example in the~\\Cref{sec:case-study}.\n\n\\noindent\\textbf{Predictions of QANet}\n\\Cref{fig:venn} shows that QANet correctly answered some ambiguous queries, which we think was due to the randomness of parameter initialization and did not reflect the true reasoning ability. \nSince QANet uses the transformer-based encoder and DocQA uses the LSTM-based encoder, \nwe see a significant difference of predictions between QANet and DocQA.\n\n\\begin{table}[!ht]\n\\centering\n\\begin{tabular}{@{}lr@{}}\n\\toprule\n\\multicolumn{1}{c}{Method} & \\multicolumn{1}{c}{OOC Rate} \\\\ \\midrule\nDocQA w/ ELMo              & 6.27\\%                                \\\\\nDocQA                      & 6.37\\%                                \\\\\nQANet                      & 6.41\\%                                \\\\ \\bottomrule\n\\end{tabular}\n\\caption{The out-of-candidate-entities (OOC) rate of three analyzed methods.}\n\\label{tab:ooc}\n\\end{table}\n\n\\noindent\\textbf{Impact of Cloze-style Setting}\nExcept ASReader, all the MRC models were evaluated under the extractive setting, which means the information of candidate named entities was not used.\nInstead, extractive models searched answers from all possible text spans in passages.\nTo show the potential benefit of using the candidate entities in these models, we computed the percentage of model predictions that could not be found in the candidate entities. \nAs shown in \\Cref{tab:ooc}, all three methods have about 6\\% OOC predictions. \nMaking use of the candidate entities would potentially help them increase the performance by 6\\%.\n\nIn \\Cref{sec:data-analysis}, we manually labeled 100 randomly sampled queries with different types of reasoning.\nIn \\Cref{fig:baselines-eval-on-question-types} and \\ref{fig:baselines-eval-on-commonsense-types}, we show the performance of three analyzed methods on these queries. \n\n\\begin{figure}[!ht]\n\\centering\n\\includegraphics[width=0.49\\textwidth]{fig/category-barchart.pdf}\n\\caption{Performance of three analyzed methods on the 100 random samples with reasoning types labeled.(CSR stands for commonsense reasoning, and MSR stands for multi-sentence reasoning.)\\label{fig:baselines-eval-on-question-types}}\n\\end{figure}\n\n\\Cref{fig:baselines-eval-on-question-types} shows that three methods performed poorly on queries requiring commonsense reasoning, multi-sentence reasoning and partial clue.\nCompared to DocQA, QANet performed better on multi-sentence reasoning queries probably due to the use of transformer.\nAlso, QANet outperformed DocQA on paraphrased queries probably because we used SAN to filtering queries and SAN has an architecture similar to DocQA.\nAs we expect, ELMo improved the performance of DocQA on paraphrased queries.\n\n\\begin{figure}[!ht]\n\\centering\n\\includegraphics[width=0.49\\textwidth]{fig/type-barchart.pdf}\n\\caption{Performance of three analyzed methods on 75\\% of the random samples with specific commonsense reasoning types labeled.\\label{fig:baselines-eval-on-commonsense-types}}\n\\end{figure}\n\nAmong the 75\\% sampled queries that require commonsense reasoning, we see that ELMo significantly improved the performance of commonsense reasoning with presumed knowledge.\nFor all other types of commonsense reasoning, all three methods have relatively poor performance.\n\n\\section{Related Datasets}\n\\label{sec:related-datasets}\n\n\\ReCoRD relates to two strands of research in datasets: data for reading comprehension, and that for commonsense reasoning.\n\n\\noindent\\textbf{Reading Comprehension}\n\\emph{The CNN/Daily Mail Corpus} \\cite{deepmind-cnn-dailymail}, \\emph{The Children's Book Test} (CBT)~\\cite{cbt}, and LAMBADA~\\cite{lambada} are closely related to \\ReCoRD:\n(1) \\emph{The CNN/Daily Mail Corpus} constructed queries from the bullet points, most of which required limited reasoning ability~\\cite{exam-of-cnn-dailymail}.\n(2) CBT is a collection of 21 consecutive sentences from book excerpts, with one\nword randomly removed from the last sentence.\nSince CBT has no machine or human filtering to ensure quality,\nonly a small portion of the CBT examples really probes machines' ability to understand the context.\n(3) Built in a similar manner to CBT, LAMBADA was filtered to be human-guessable in the broader context only.\nDiffering from \\ReCoRD, LAMBADA was designed to be a language modeling problem where contexts were not required to be event summaries, and answers were not necessarily in the context.\n\nSince all candidate answers were extracted from in the passage, \\ReCoRD can also be formalized as a extractive MRC dataset,\nsimilar to SQuAD~\\cite{squad} and NewsQA~\\cite{newsqa}.\nThe difference is that questions in these datasets were curated from crowdworkers.\nSince it is hard to control the quality of crowdsourced questions, a large portion of questions in these datasets can be answered by word matching or paraphrasing~\\cite{adversarial-squad,squad-v2,adversarial-training}. \nThere are other large-scale datasets~\\cite{msmarco,triviaqa,race,searchqa,narrativeqa,coqa,quac,hotpotqa} targeting different aspects of reading comprehension. See \\cite{gaosurvey} for a recent survey.\n\n\\noindent\\textbf{Commonsense Reasoning}\nROCStories Corpus~\\cite{rocstories}, SWAG~\\cite{swag}, and \\emph{The Winograd Schema Challenge}~(WSC) \\cite{wsc} are related \\ReCoRD:\n(1) ROCStories assesses commonsense reasoning in story understanding by choosing the correct story ending from only two candidates. Stories in the corpus were all curated from crowdworkers, which could suffer from human elicitation bias~\\cite{reporting-bias,vision-reporting-bias,joci}.\n(2) SWAG unifies commonsense reasoning and natural language inference. \nIt selects an ending from multiple choices which is most likely to be anticipated from the situation describe in the premise.\nThe counterfactual endings in SWAG were generated using language models with adversarial filtering.\n(3) WSC foucses on intra-sentential pronoun disambiguation problems that require commonsense reasoning.\nThere are other datasets~\\cite{copa,joci,naive-psychology-in-stories,event2mind} targeting different aspects of commonsense reasoning.\n\n\\section{Conclusion}\nWe introduced \\ReCoRD, a large-scale reading comprehension dataset requiring commonsense reasoning.\nUnlike existing machine reading comprehension (MRC) datasets, \\ReCoRD contains a large portion of queries that require commonsense reasoning to be answered.\nOur baselines, including top performers on existing MRC datasets, are no match for human competence on \\ReCoRD.\nWe hope that \\ReCoRD will spur more research in MRC with commonsense reasoning.%, which is a key to bridging the gap between human and machine reading comprehension.\n\n\\clearpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n\nIn the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard.\nSuperGLUE is available at \\href{https://super.gluebenchmark.com/}{\\tt super.gluebenchmark.com}.\n\n\\end{abstract}\n\n\\section{Introduction}\n\nRecently there has been notable progress across many natural language processing (NLP) tasks, led by methods such as ELMo \\citep{peters2018deep}, OpenAI GPT \\citep{radford2018improving}, and BERT \\citep{devlin2018bert}. The unifying theme of these methods is that they couple self-supervised learning from massive unlabelled text corpora with effective adapting of the resulting model to target tasks. The tasks that have proven amenable to this general approach include question answering, textual entailment, and parsing, among many others \\citep[][i.a.]{devlin2018bert,kitaev2018multilingual}. \n\nIn this context, the GLUE benchmark \\citep{wang2018glue} has become a prominent evaluation framework for research towards general-purpose language understanding technologies. \nGLUE is a collection of nine language understanding tasks built on existing public datasets, together with private test data, an evaluation server, a single-number target metric, and an accompanying expert-constructed diagnostic set. GLUE was designed to provide a general-purpose evaluation of language understanding that covers a range of training data volumes, task genres, and task formulations. We believe it was these aspects that made GLUE particularly appropriate for exhibiting the transfer-learning potential of approaches like OpenAI GPT and BERT.\n\nThe progress of the last twelve months has eroded headroom on the GLUE benchmark dramatically. \nWhile some tasks (Figure~\\ref{fig:benchmark-trends}) and some linguistic phenomena (Figure~\\ref{fig:diagnostic-trends} in Appendix~\\ref{ax:diagnostics}) measured in GLUE remain difficult, \nthe current state of the art GLUE Score as of early July 2019\n\\citep[88.4 from][]{yang2019xlnet}\nsurpasses human performance \\citep[87.1 from][]{nangia2019human} by 1.3 points, and in fact exceeds this human performance estimate on four tasks.\nConsequently, while there remains substantial scope for improvement towards GLUE's high-level goals, the original version of the benchmark is no longer a suitable metric for quantifying such progress.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.97\\textwidth]{images/leaderboard.pdf}\n    \\caption{GLUE benchmark performance for submitted systems, rescaled to set human performance to 1.0, shown as a single number score, and broken down into the nine constituent task performances. For tasks with multiple metrics, we use an average of the metrics. More information on the tasks included in GLUE can be found in \\citet{wang2018glue} and in \\citet[CoLA]{warstadt2018neural}, \\citet[SST-2]{socher2013recursive}, \\citet[MRPC]{dolan2005automatically}, \\citet[STS-B]{cer-etal-2017-semeval}, and \\citet[MNLI]{williams2018broad}, and \\citet[the original data source for QNLI]{rajpurkar2016}.}\n    \\label{fig:benchmark-trends}\n\\end{figure}\n\nIn response, we introduce SuperGLUE, a new benchmark designed to pose a more rigorous test of language understanding. SuperGLUE has the same high-level motivation as GLUE: to provide a simple, hard-to-game measure of progress toward general-purpose language understanding technologies for English. We anticipate that significant progress on SuperGLUE should require substantive innovations in a number of core areas of machine learning, including sample-efficient, transfer, multitask, and unsupervised or self-supervised learning.\n\nSuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around eight language understanding tasks, drawing on existing data, accompanied by a single-number performance metric, and an analysis toolkit. \nHowever, it improves upon GLUE in several ways:\n\n\\textbf{More challenging tasks:} SuperGLUE retains the two hardest tasks in GLUE. The remaining tasks were identified from those submitted to an open call for task proposals and were selected based on difficulty for current NLP approaches.%\\footnote{\\href{http://bit.ly/glue2cfp}{\\tt bit.ly/glue2cfp}} \n\n\\textbf{More diverse task formats:} The task formats in GLUE are limited to sentence- and sentence-pair classification. We expand the set of task formats in SuperGLUE to include coreference resolution and question answering (QA). \n\n\\textbf{Comprehensive human baselines:} We include human performance estimates for all benchmark tasks, which verify that substantial headroom exists between a strong BERT-based baseline and human performance.\n\n\\textbf{Improved code support:} SuperGLUE is distributed with a new, modular toolkit for work on pretraining, multi-task learning, and transfer learning in NLP, built around standard tools including PyTorch \\citep{paszke2017automatic} and AllenNLP \\citep{Gardner2017AllenNLP}.\n\n\\textbf{Refined usage rules:} The conditions for inclusion on the SuperGLUE leaderboard have been revamped to ensure fair competition, an informative leaderboard, and full credit assignment to data and task creators.\n\nThe SuperGLUE leaderboard, data, and software tools are available at \\href{https://super.gluebenchmark.com/}{\\tt super.gluebenchmark.com}.\n\n\\section{Related Work}\n\nMuch work prior to GLUE demonstrated that training neural models with large amounts of available supervision can produce representations that effectively transfer to a broad range of NLP tasks \\citep{collobert2008unified,dai2015semisupervised,kiros2015skip,hill2016learning,conneau2018senteval,mccann2017learned,peters2018deep}.\nGLUE was presented as a formal challenge affording straightforward comparison between such task-agnostic transfer learning techniques.\nOther similarly-motivated benchmarks include SentEval~\\citep{conneau2018senteval}, which specifically evaluates fixed-size sentence embeddings, and DecaNLP~\\citep{mccann2018decanlp}, which recasts a set of target tasks into a general question-answering format and prohibits task-specific parameters. In contrast, GLUE provides a lightweight classification API and no restrictions on model architecture or parameter sharing, which seems to have been well-suited to recent work in this area.\n\nSince its release, GLUE has been used as a testbed and showcase by the developers of several influential models, including GPT~\\citep{radford2018improving} and BERT~\\citep{devlin2018bert}. As shown in Figure~\\ref{fig:benchmark-trends}, progress on GLUE since its release has been striking.\nOn GLUE, GPT and BERT achieved scores of 72.8 and 80.2 respectively, relative to 66.5 for an ELMo-based model \\citep{peters2018deep} and 63.7 for the strongest baseline with no multitask learning or pretraining above the word level. \nRecent models \\citep{liu2019mt,yang2019xlnet} have clearly surpassed estimates of non-expert human performance on GLUE \\citep{nangia2019human}. \nThe success of these models on GLUE has been driven by ever-increasing model capacity, compute power, and data quantity, as well as innovations in model expressivity (from recurrent to bidirectional recurrent to multi-headed transformer encoders) and degree of contextualization (from learning representation of words in isolation to using uni-directional contexts and ultimately to leveraging bidirectional contexts).\n\nIn parallel to work scaling up pretrained models, several studies have focused on complementary methods for augmenting performance of pretrained models. \\citet{phang2018sentence} show that BERT can be improved using two-stage pretraining, i.e., fine-tuning the pretrained model on an intermediate data-rich supervised task before fine-tuning it again on a data-poor target task.\n\\citet{liu2019mt, liu2019improving} and \\citet{snorkel:2018} get further improvements respectively via multi-task finetuning and using massive amounts of weak supervision.\n\\citet{clark2019bam} demonstrate that knowledge distillation \\citep{hinton2015distilling,furlanello2018born} can lead to student networks that outperform their teachers.\nOverall, the quantity and quality of research contributions aimed at the challenges posed by GLUE underline the utility of this style of benchmark for machine learning researchers looking to evaluate new application-agnostic methods on language understanding.\n\nLimits to current approaches are also apparent via the GLUE suite. \nPerformance on the GLUE diagnostic entailment dataset, at 0.42 $R_3$, falls far below the average human performance of 0.80 $R_3$ reported in the original GLUE publication, with models performing near, or even below, chance on some linguistic phenomena (Figure~\\ref{fig:diagnostic-trends}, Appendix~\\ref{ax:diagnostics}). While some initially difficult categories saw gains from advances on GLUE (e.g., double negation), others remain hard (restrictivity) or even adversarial (disjunction, downward monotonicity). This suggests that even as unsupervised pretraining produces ever-better statistical summaries of text, it remains difficult to extract many details crucial to semantics without the right kind of supervision. \nMuch recent work has made similar observations about the limitations of existing pretrained models\n\\citep{jia2017adversarial, naik2018stresstest, mccoy2019nonentailed, mccoy2019wrongreasons,  liu2019transferability, liu2019inoculation}.\n \n\n\\section{SuperGLUE Overview}\n\n\\begin{table*}[t]\n\\caption{The tasks included in SuperGLUE.  \n\\textit{WSD} stands for word sense disambiguation, \\textit{NLI} is natural language inference, \\textit{coref.} is coreference resolution, and \\textit{QA} is question answering. For MultiRC, we list the number of total answers for 456/83/166 train/dev/test questions. \n}\n\\centering \\small\n\\begin{tabular}{lrrrlll}\n \\toprule\n\\textbf{Corpus} & \\textbf{$|$Train$|$} & \\textbf{$|$Dev$|$} & \\textbf{$|$Test$|$} & \\textbf{Task} & \\textbf{Metrics} & \\textbf{Text Sources} \\\\\n\\midrule \nBoolQ & 9427 & 3270 & 3245 & QA & acc. & Google queries, Wikipedia \\\\\nCB & 250 & 57 & 250 & NLI & acc./F1 & various \\\\\nCOPA & 400 & 100 & 500 & QA & acc. & blogs, photography encyclopedia\\\\\nMultiRC & 5100 & 953 & 1800 & QA & F1$_a$/EM & various \\\\\nReCoRD & 101k & 10k & 10k & QA & F1/EM & news (CNN, Daily Mail) \\\\\nRTE & 2500 & 278 & 300 & NLI & acc. & news, Wikipedia \\\\\nWiC & 6000 & 638 & 1400 & WSD & acc. & WordNet, VerbNet, Wiktionary \\\\\nWSC & 554 & 104 & 146 & coref. & acc. & fiction books \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:tasks}\n\\end{table*}\n\n\\begin{table*}[t]\n\\caption{Development set examples from the tasks in SuperGLUE. \\textbf{Bold} text represents part of the example format for each task. Text in \\textit{italics} is part of the model input. \\underline{\\textit{Underlined}} text is specially marked in the input. Text in a \\texttt{monospaced font} represents the expected model output.}\n\\label{tab:examples}\n\\centering \\footnotesize\n\\begin{tabular}{p{0.005\\textwidth}p{0.93\\textwidth}}\n\n \\toprule\n \\parbox[t]{1mm}{\\multirow{2}{*}{\\rotatebox[origin=c]{90}{{\\textbf{BoolQ}}}}} &\n\\textbf{Passage:} \\textit{Barq's -- Barq's is an American soft drink. Its brand of root beer is notable for having caffeine. Barq's, created by Edward Barq and bottled since the turn of the 20th century, is owned by the Barq family but bottled by the Coca-Cola Company. It was known as Barq's Famous Olde Tyme Root Beer until 2012.} \\\\ & \\textbf{Question:} \\textit{is barq's root beer a pepsi product} \\quad \\textbf{Answer:} \\texttt{No}\\\\\n\n\\midrule\n\\parbox[t]{1mm}{\\multirow{2}{*}{\\rotatebox[origin=c]{90}{{\\textbf{CB}}}}} &\n\\textbf{Text:} \\textit{B: And yet, uh, I we-, I hope to see employer based, you know, helping out. You know, child, uh, care centers at the place of employment and things like that, that will help out. A: Uh-huh. B: What do you think, do you think we are, setting a trend?} \\\\ & \\textbf{Hypothesis:} \\textit{they are setting a trend} \\quad \\textbf{Entailment:} \\texttt{Unknown}\\\\\n\n\\midrule\n\\parbox[t]{1mm}{\\multirow{2}{*}{\\rotatebox[origin=c]{90}{{\\textbf{COPA}}}}} & \\textbf{Premise:} \\textit{My body cast a shadow over the grass.}\\quad\n\\textbf{Question:} \\textit{What’s the CAUSE for this?}\\\\\n&\\textbf{Alternative 1:} \\textit{The sun was rising.}\\quad\n\\textbf{Alternative 2:} \\textit{The grass was cut.}\\\\\n&\\textbf{Correct Alternative:} \\texttt{1}\\\\\n\n\\midrule\n\\parbox[t]{1mm}{\\multirow{2}{*}{\\rotatebox[origin=c]{90}{{\\textbf{MultiRC}}}}} &\n\\textbf{Paragraph:} \\textit{Susan wanted to have a birthday party. She called all of her friends. She has five friends. Her mom said that Susan can invite them all to the party. Her first friend could not go to the party because she was sick. Her second friend was going out of town. Her third friend was not so sure if her parents would let her. The fourth friend said maybe. The fifth friend could go to the party for sure. Susan was a little sad. On the day of the party, all five friends showed up. Each friend had a present for Susan. Susan was happy and sent each friend a thank you card the next week}\\\\\n& \\textbf{Question:} \\textit{Did Susan's sick friend recover?} \\textbf{Candidate answers:} \n\\textit{Yes, she recovered} (\\texttt{T}), \n\\textit{No} (\\texttt{F}), \n\\textit{Yes} (\\texttt{T}), \n\\textit{No, she didn't recover} (\\texttt{F}), \n\\textit{Yes, she was at Susan's party} (\\texttt{T})\\\\\n\n\\midrule\n\\parbox[t]{1mm}{\\multirow{2}{*}{\\rotatebox[origin=c]{90}{{\\textbf{ReCoRD}}}}} & \n\\textbf{Paragraph:} \\textit{(\\underline{CNN}) \\underline{Puerto Rico} on Sunday overwhelmingly voted for statehood. But Congress, the only body that can approve new states, will ultimately decide whether the status of the \\underline{US} commonwealth changes. Ninety-seven percent of the votes in the nonbinding referendum favored statehood, an increase over the results of a 2012 referendum, official results from the \\underline{State Electorcal Commission} show. It was the fifth such vote on statehood. \"Today, we the people of \\underline{Puerto Rico} are sending a strong and clear message to the \\underline{US Congress}  ... and to the world ... claiming our equal rights as \\underline{American} citizens, \\underline{Puerto Rico} Gov. \\underline{Ricardo Rossello} said in a news release. @highlight \\underline{Puerto Rico} voted Sunday in favor of \\underline{US} statehood}\\\\\n&\\textbf{Query} For one, they can truthfully say, ``Don't blame me, I didn't vote for them, '' when discussing the <placeholder> presidency \\quad \\textbf{Correct Entities:} \\texttt{US} \\\\\n\n\\midrule\n\\parbox[t]{1mm}{\\multirow{2}{*}{\\rotatebox[origin=c]{90}{{\\textbf{RTE}}}}} &\n\\textbf{Text:} \\textit{Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44, according to the Christopher Reeve Foundation.}\\\\\n& \\textbf{Hypothesis:} \\textit{Christopher Reeve had an accident.} \\quad\n\\textbf{Entailment:} \\texttt{False}\\\\\n\n\\midrule\n\\parbox[t]{1mm}{\\multirow{2}{*}{\\rotatebox[origin=c]{90}{{\\textbf{WiC}}}}} &\n\\textbf{Context 1:} \\textit{Room and \\underline{board}.} \\quad\n\\textbf{Context 2:} \\textit{He nailed \\underline{boards} across the windows.} \\\\\n& \\textbf{Sense match:} \\texttt{False}\\\\\n\n\\midrule\n\\parbox[t]{1mm}{\\multirow{1}{*}{\\rotatebox[origin=c]{90}{{\\textbf{WSC}}}}} & \n\\textbf{Text:} \\textit{Mark told \\underline{Pete} many lies about himself, which Pete included in his book. \\underline{He} should have been more truthful.} \\quad \\textbf{Coreference:} \\texttt{False}\\vspace{0.25em}\\\\\n\\bottomrule\n\\end{tabular}\n\n\\end{table*}\n\n\\subsection{Design Process}\n\nThe goal of SuperGLUE is to provide a simple, robust evaluation metric of any method capable of being applied to a broad range of language understanding tasks. To that end, in designing SuperGLUE, we identify the following desiderata of tasks in the benchmark:\n\n\\textbf{Task substance:} Tasks should test a system's ability to understand and reason about texts in English. \n\n\\textbf{Task difficulty:} Tasks should be beyond the scope of current state-of-the-art systems, but solvable by most college-educated English speakers. We exclude tasks that require domain-specific knowledge, e.g. medical notes or scientific papers.\n\n\\textbf{Evaluability:} Tasks must have an automatic performance metric that corresponds well to human judgments of output quality. Some text generation tasks fail to meet this criteria due to issues with automatic metrics like ROUGE and BLEU \\citep[][i.a.]{callison2006re,liu2016not}.\n\n\\textbf{Public data:} We require that tasks have \\textit{existing} public training data in order to minimize the risks involved in newly-created datasets. We also prefer tasks for which we have access to (or could create) a test set with private labels.\n\n\\textbf{Task format:} We prefer tasks that had relatively simple input and output formats, to avoid incentivizing the users of the benchmark to create complex task-specific model architectures. Still, while GLUE is restricted to tasks involving single sentence or sentence pair inputs, for SuperGLUE we expand the scope to consider tasks with longer inputs. This yields a set of tasks that requires understanding individual tokens in context, complete sentences, inter-sentence relations, and entire paragraphs.\n\n\\textbf{License:} Task data must be available under licences that allow use and redistribution for research purposes.\n\nTo identify possible tasks for SuperGLUE, we disseminated a public call for task proposals to the NLP community, and received approximately 30 proposals.\nWe filtered these proposals according to our criteria.\nMany proposals were not suitable due to licensing issues, complex formats, and insufficient headroom;\nwe provide examples of such tasks in Appendix~\\ref{ax:excluded}. For each of the remaining tasks, we ran a BERT-based baseline and a human baseline, and filtered out tasks which were either too challenging for humans without extensive training or too easy for our machine baselines. \n\n\\subsection{Selected Tasks}\nFollowing this process, we arrived at eight tasks to use in SuperGLUE. See Tables \\ref{tab:tasks} and \\ref{tab:examples} for details and specific examples of each task.\n \n\\textbf{BoolQ} \\citep[Boolean Questions,][]{clark2019boolq} is a QA task where each example consists of a short passage and a yes/no question about the passage. The questions are provided anonymously and unsolicited by users of the Google search engine, and afterwards paired with a paragraph from a Wikipedia article containing the answer. Following the original work, we evaluate with accuracy.\n\n\\textbf{CB} \\citep[CommitmentBank,][]{demarneffe:cb} is a \ncorpus of short texts in which at least one sentence contains an embedded clause. \nEach of these embedded clauses is annotated with the degree to which it appears the person who wrote the text is \\textit{committed} to the truth of the clause. The resulting task framed as three-class textual entailment on examples that are drawn from the Wall Street Journal, fiction from the British National Corpus, and Switchboard.\nEach example consists of a premise containing an embedded clause and the corresponding hypothesis is the extraction of that clause. \nWe use a subset of the data that had inter-annotator agreement above $80\\%$.\nThe data is imbalanced (relatively fewer \\textit{neutral} examples), so we evaluate using accuracy and F1, where for multi-class F1 we compute the unweighted average of the F1 per class.\n\n\\textbf{COPA} \\citep[Choice of Plausible Alternatives,][]{roemmele2011choice} is a causal reasoning task in which a system is given a premise sentence and must determine either the cause or effect of the premise from two possible choices. \nAll examples are handcrafted and focus on topics from blogs and a photography-related encyclopedia. Following the original work, we evaluate using accuracy.\n\n\\textbf{MultiRC} \\citep[Multi-Sentence Reading Comprehension,][]{khashabi2018looking} is a QA task where each example consists of a context paragraph, a question about that paragraph, and a list of possible answers. The system must predict which answers are true and which are false. While many QA tasks exist, we use MultiRC because of a number of desirable properties: (i)~each question can have multiple possible correct answers, so each question-answer pair must be evaluated independent of other pairs, (ii)~the questions are designed such that answering each question requires drawing facts from multiple context sentences, and (iii)~the question-answer pair format more closely matches the API of other tasks in SuperGLUE than the more popular span-extractive QA format does.\nThe paragraphs are drawn from seven domains including news, fiction, and historical text.\nThe evaluation metrics are F1 over all answer-options (F1$_a$) and exact match of each question's set of answers (EM).\n\n\\textbf{ReCoRD} \\citep[Reading Comprehension with Commonsense Reasoning Dataset,][]{zhang2018record} is a multiple-choice QA task.\nEach example consists of a news article and a Cloze-style question about the article in which one entity is masked out. The system must predict the masked out entity from a list of possible entities in the provided passage, where the same entity may be expressed with multiple different surface forms, which are all considered correct.\nArticles are from CNN and Daily Mail. We evaluate with max (over all mentions) token-level F1 and exact match (EM).\n\n\\textbf{RTE} (Recognizing Textual Entailment) datasets come from a series of annual competitions on textual entailment. %\\footnote{Textual entailment is also known as natural language inference, or NLI}\nRTE is included in GLUE, and we use the same data and format as GLUE: We merge data from RTE1 \\citep{dagan2006pascal}, RTE2 \\citep{bar2006second}, RTE3 \\citep{giampiccolo2007third}, and RTE5 \\citep{bentivogli2009fifth}. %\\footnote{RTE4 is not publicly available, while RTE6 and RTE7 do not conform to the standard NLI task.} \nAll datasets are combined and converted to two-class classification: \\textit{entailment} and \\textit{not\\_entailment}.\nOf all the GLUE tasks, RTE is among those that benefits from transfer learning the most, with performance jumping from near random-chance ($\\sim$56\\%) at the time of GLUE's launch to 86.3\\% accuracy \\citep{liu2019mt,yang2019xlnet} at the time of writing.\nGiven the nearly eight point gap with respect to human performance, however, the task is not yet solved by machines, and we expect the remaining gap to be difficult to close.\n\n\\textbf{WiC} \\citep[Word-in-Context,][]{pilehvar2018wic} is a word sense disambiguation task cast as binary classification of sentence pairs. \nGiven two text snippets and a polysemous %(sense-ambiguous) \nword that appears in both sentences, the task is to determine whether the word is used with the same sense in both sentences. \nSentences are drawn from WordNet \\citep{miller1995wordnet}, VerbNet \\citep{schuler2005verbnet}, and Wiktionary.\nWe follow the original work and evaluate using accuracy.\n\n\\textbf{WSC} \\citep[Winograd Schema Challenge,][]{levesque2012winograd} is a coreference resolution task in which examples consist of a sentence with a pronoun and a list of noun phrases from the sentence.\nThe system must determine the correct referrent of the pronoun from among the provided choices.\nWinograd schemas are designed to require everyday knowledge and commonsense reasoning to solve.\n\nGLUE includes a version of WSC recast as NLI, known as WNLI.\nUntil very recently, no substantial progress had been made on WNLI, with many submissions opting to submit majority class predictions.\\footnote{WNLI is especially difficult due to an adversarial train/dev split: Premise sentences that appear in the training set often appear in the development set with a different hypothesis and a flipped label. \nIf a system memorizes the training set, which was easy due to the small size of the training set, it could perform far \\textit{below} chance on the development set. We remove this adversarial design in our version of WSC by ensuring that no sentences are shared between the training, validation, and test sets.}\nIn the past few months, several works \\citep{kocijan2019surprisingly,liu2019mt} have made rapid progress via a hueristic data augmentation scheme, raising machine performance to 90.4\\% accuracy.\nGiven estimated human performance of $\\sim$96\\%, there is still a gap between machine and human performance, which we expect will be relatively difficult to close.\nWe therefore include a version of WSC cast as binary classification, where each example consists of a sentence with a marked pronoun and noun, and the task is to determine if the pronoun refers to that noun.\nThe training and validation examples are drawn from the original WSC data \\citep{levesque2012winograd}, as well as those distributed by the affiliated organization \\textit{Commonsense Reasoning}.\\footnote{\\url{http://commonsensereasoning.org/disambiguation.html}} \nThe test examples are derived from fiction books and have been shared with us by the authors of the original dataset.\nWe evaluate using accuracy.\n\n\\subsection{Scoring} As with GLUE, we seek to give a sense of aggregate system performance over all tasks by averaging scores of all tasks.\nLacking a fair criterion with which to weight the contributions of each task to the overall score, we opt for the simple approach of weighing each task equally, and for tasks with multiple metrics, first averaging those metrics to get a task score.\n\n\\subsection{Tools for Model Analysis}\n\n\\paragraph{Analyzing Linguistic and World Knowledge in Models}\nGLUE includes an expert-constructed, diagnostic dataset that automatically tests models for a broad range of linguistic, commonsense, and world knowledge.\nEach example in this broad-coverage diagnostic is a sentence pair labeled with a three-way entailment relation (\\textit{entailment}, \\textit{neutral}, or \\textit{contradiction})\nand tagged with labels that indicate the phenomena that characterize the relationship between the two sentences. Submissions to the GLUE leaderboard are required to include predictions from the submission's MultiNLI classifier on the diagnostic dataset, and analyses of the results were shown alongside the main leaderboard.\nSince this diagnostic task has proved difficult for top models, we retain it in SuperGLUE.\nHowever, since MultiNLI is not part of SuperGLUE, we collapse \\textit{contradiction} and \\textit{neutral} into a single \\textit{not\\_entailment} label, and request that submissions include predictions on the resulting set from the model used for the \\textit{RTE} task.\nWe estimate human performance following the same procedure we use for the benchmark tasks (Section~\\ref{ax:instruct}). %(Section~\\ref{sec:human}).\nWe estimate an accuracy of 88\\% and a Matthew's correlation coefficient (MCC, the two-class variant of the $R_3$ metric used in GLUE) of 0.77.\n\n\\paragraph{Analyzing Gender Bias in Models}\nRecent work has identified the presence and amplification of many social biases in data-driven machine learning models\n\\citep[][ i.a.]{lu-2018-gender, zhao-2018-gender}. To promote the detection of such biases, we include Winogender \\citep{rudinger-winogender} as an additional diagnostic dataset. Winogender is designed to measure gender bias in coreference resolution systems. We use the Diverse Natural Language Inference Collection \\citep{poliak-2018-dnc} version that casts Winogender as a textual entailment task.%\\footnote{We filter out 23 examples where the labels are ambiguous}\nEach example consists of a premise sentence with a male or female pronoun and a hypothesis giving a possible antecedent of the pronoun.\nExamples occur in \\textit{minimal pairs}, where the only difference between an example and its pair is the gender of the pronoun in the premise.\nPerformance on Winogender is measured with accuracy and the \\textit{gender parity score}: the percentage of minimal pairs for which the predictions are the same.\nA system can trivially obtain a perfect gender parity score by guessing the same class for all examples, so a high gender parity score is meaningless unless accompanied by high accuracy.\nWe collect non-expert annotations to estimate human performance, and observe an accuracy of 99.7\\% and a gender parity score of 0.99.\n\nLike any diagnostic, Winogender has limitations. It offers only positive predictive value: A poor bias score is clear evidence that a model exhibits gender bias, but a good score does not mean that the model is unbiased. More specifically, in the DNC version of the task, a low gender parity score means that a model's prediction of textual entailment can be changed with a change in pronouns, all else equal. It is plausible that there are forms of bias that are relevant to target tasks of interest, but that do not surface in this setting \\citep{gonen-goldberg-2019-lipstick}. \nAlso, Winogender does not cover all forms of social bias, or even all forms of gender. For instance, the version of the data used here offers no coverage of gender-neutral \\textit{they} or non-binary pronouns. Despite these limitations, we believe that Winogender's inclusion is worthwhile in providing a coarse sense of how social biases evolve with model performance and for keeping attention on the social ramifications of NLP models.\n\n\\section{Using SuperGLUE}\n\n\\paragraph{Software Tools}\nTo facilitate using SuperGLUE, we release \\texttt{jiant} \\citep{wang2019jiant},\\footnote{\\url{https://github.com/nyu-mll/jiant}} a modular software toolkit, built with PyTorch \\citep{paszke2017automatic}, components from AllenNLP \\citep{Gardner2017AllenNLP}, and the \\texttt{transformers} package.\\footnote{\\url{https://github.com/huggingface/transformers}}\n\\texttt{jiant} implements our baselines and supports the evaluation of custom models and training methods on the benchmark tasks.\nThe toolkit includes support for existing popular pretrained models such as OpenAI GPT and BERT, as well as support for multistage and multitask learning of the kind seen in the strongest models on GLUE.\n\n\\paragraph{Eligibility} Any system or method that can produce predictions for the SuperGLUE tasks is eligible for submission to the leaderboard, subject to the data-use and submission frequency policies stated immediately below. There are no restrictions on the type of methods that may be used, and there is no requirement that any form of parameter sharing or shared initialization be used across the tasks in the benchmark. \nTo limit overfitting to the private test data, users are limited to a maximum of two submissions per day and six submissions per month.\n\n\\paragraph{Data} \nData for the tasks are available for download through the SuperGLUE site and through a download script included with the software toolkit. \nEach task comes with a standardized training set, development set, and \\textit{unlabeled} test set.\nSubmitted systems may use any public or private data when developing their systems, with a few exceptions: Systems may only use the SuperGLUE-distributed versions of the task datasets, as these use different train/validation/test splits from other public versions in some cases. Systems also may not use the unlabeled test data for the tasks in system development in any way, may not use the structured source data that was used to collect the WiC labels (sense-annotated example sentences from WordNet, VerbNet, and Wiktionary) in any way, and may not build systems that share information across separate \\textit{test} examples in any way.\n\nTo ensure reasonable credit assignment, because we build very directly on prior work, we ask the authors of submitted systems to directly name and cite the specific datasets that they use, \\textit{including the benchmark datasets}. We will enforce this as a requirement for papers to be listed on the leaderboard.\n\n\\section{Experiments}\n\n\\subsection{Baselines}\n\n\\paragraph{BERT}\n\n\\begin{table*}[t]\n\\footnotesize\n\n\\caption{Baseline performance on the SuperGLUE test sets and diagnostics. \nFor CB we report accuracy and macro-average F1. For MultiRC we report F1 on all answer-options and exact match of each question's set of correct answers. AX$_b$ is the broad-coverage diagnostic task, scored using Matthews' correlation (MCC). AX$_g$ is the Winogender diagnostic, scored using accuracy and the gender parity score (GPS). All values are scaled by 100. The \\textit{Avg} column is the overall benchmark score on non-AX$_*$ tasks.\nThe bolded numbers reflect the best machine performance on task.\n*MultiRC has multiple test sets released on a staggered schedule, \nand these results evaluate on an installation of the test set that is a subset of ours.\n}\n\n\\centering %\\small \n\\fontsize{8.4}{10.1}\\selectfont\n\\setlength{\\tabcolsep}{0.3em}\n\n\\begin{tabular}{lccc@{/}ccc@{/}cc@{/}cccccc@{/}c}\n\n\\toprule\n\n\\textbf{Model} & \\multicolumn{1}{c}{\\textbf{Avg}} & \\multicolumn{1}{c}{\\textbf{BoolQ}} & \\multicolumn{2}{c}{\\textbf{CB}} &  \\multicolumn{1}{c}{\\textbf{COPA}} & \\multicolumn{2}{c}{\\textbf{MultiRC}} & \\multicolumn{2}{c}{\\textbf{ReCoRD}} & \\multicolumn{1}{c}{\\textbf{RTE}} & \\multicolumn{1}{c}{\\textbf{WiC}} & \\multicolumn{1}{c}{\\textbf{WSC}} & \n\\multicolumn{1}{c}{\\textbf{AX$_b$}} &\n\\multicolumn{2}{c}{\\textbf{AX$_g$}} \\\\\n\\textbf{Metrics} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{\\textbf{Acc.}} & \\multicolumn{2}{c}{\\textbf{F1/Acc.}} &  \\multicolumn{1}{c}{\\textbf{Acc.}} & \\multicolumn{2}{c}{\\textbf{F1$_a$/EM}} & \\multicolumn{2}{c}{\\textbf{F1/EM}} & \\multicolumn{1}{c}{\\textbf{Acc.}} & \\multicolumn{1}{c}{\\textbf{Acc.}} & \\multicolumn{1}{c}{\\textbf{Acc.}} &\n\\multicolumn{1}{c}{\\textbf{MCC}} & \n\\multicolumn{1}{c}{\\textbf{GPS}} & \n\\multicolumn{1}{c}{\\textbf{Acc.}} \\\\\n\\midrule\n\nMost Frequent & 47.1 & 62.3 & 21.7 & 48.4 & 50.0 & 61.1 & 0.3 & 33.4 & 32.5 & 50.3 & 50.0 & 65.1 & 0.0 & 100.0 & 50.0 \\\\\nCBoW & 44.3 & 62.1 & 49.0  & 71.2 &  51.6 & 0.0 & 0.4 & 14.0 & 13.6 & 49.7 & 53.0 & 65.1 & -0.4 & 100.0 & 50.0 \\\\\nBERT & 69.0 & 77.4 & 75.7 & 83.6 & 70.6 & 70.0 & 24.0 & 72.0 & 71.3 & 71.6 & \\textbf{69.5} & \\textbf{64.3} & 23.0 & 97.8 & 51.7 \\\\\nBERT++ & \\textbf{71.5} & 79.0 & \\textbf{84.7} & \\textbf{90.4} & 73.8 & 70.0 & 24.1 & 72.0 & 71.3 & 79.0 & \\textbf{69.5} & \\textbf{64.3} & 38.0 & 99.4 & 51.4 \\\\\n\nOutside Best & - & \\textbf{80.4} & - &  - & \\textbf{84.4} & \\textbf{70.4}* & \\textbf{24.5}* & \\textbf{74.8} & \\textbf{73.0} & \\textbf{82.7} & - & - & - & - & - \\\\\n\n\\midrule\nHuman (est.) & 89.8 & 89.0 & 95.8 & 98.9 & 100.0 & 81.8* & 51.9* & 91.7 & 91.3 & 93.6 & 80.0 & 100.0 & 77.0 & 99.3 & 99.7 \\\\\n\n\\bottomrule\n\\end{tabular}\n\n\\label{tab:benchmark}\n\\end{table*}\n\nOur main baselines are built around BERT, variants of which are among the most successful approach on GLUE at the time of writing. Specifically, we use the \\texttt{bert-large-cased} variant.\nFollowing the practice recommended in \\citet{devlin2018bert}, for each task, we use the simplest possible architecture on top of BERT. We fine-tune a copy of the pretrained BERT model separately for each task, and leave the development of multi-task learning models to future work. For training, we use the procedure specified in \\citet{devlin2018bert}:\nWe use Adam \\citep{kingma2014adam} with an initial learning rate of $10^{-5}$ and fine-tune for a maximum of 10 epochs.\n\nFor classification tasks with sentence-pair inputs (BoolQ, CB, RTE, WiC), we concatenate the sentences with a \\textsc{[sep]} token, feed the fused input to BERT, and use a logistic regression classifier that sees the representation corresponding to \\textsc{[cls]}.\nFor WiC, we also concatenate the representation of the marked word. % to the \\textsc{[cls]} representation.\nFor COPA, MultiRC, and ReCoRD, for each answer choice, we similarly concatenate the context with that answer choice and feed the resulting sequence into BERT to produce an answer representation.\nFor COPA, we project these representations into a scalar, and take as the answer the choice with the highest associated scalar.\nFor MultiRC, because each question can have more than one correct answer, we feed each answer representation into a logistic regression classifier.\nFor ReCoRD, we also evaluate the probability of each candidate independent of other candidates, and take the most likely candidate as the model's prediction.\nFor WSC, which is a span-based task, we use a model inspired by \\citet{tenney2018you}.\nGiven the BERT representation for each word in the original sentence, we get span representations of the pronoun and noun phrase via a self-attention span-pooling operator \\citep{lee2017end}, before feeding it into a logistic regression classifier. \n\n\\paragraph{BERT++} We also report results using BERT with additional training on related datasets before fine-tuning on the benchmark tasks, following the STILTs style of transfer learning \\citep{phang2018sentence}. Given the productive use of MultiNLI in pretraining and intermediate fine-tuning of pretrained language models \\citep[][i.a.]{conneau2017supervised,phang2018sentence}, for CB, RTE, and BoolQ, we use MultiNLI as a transfer task by first using the above procedure on MultiNLI. Similarly, given the similarity of COPA to SWAG \\citep{zellers2018swag}, we first fine-tune BERT on SWAG. These results are reported as BERT++. For all other tasks, we reuse the results of BERT fine-tuned on just that task.\n\n\\paragraph{Other Baselines} We include a baseline where for each task we simply predict the majority class,\\footnote{For ReCoRD, we predict the entity that has the highest F1 with the other entity options.} as well as a bag-of-words baseline where each input is represented as an average of its tokens' GloVe word vectors \\citep[the 300D/840B release from][]{pennington2014glove}. \nFinally, we list the best known result on each task as of May 2019, except on tasks which we recast (WSC), resplit (CB), or achieve the best known result (WiC). \nThe outside results for COPA, MultiRC, and RTE are from  \\citet{sap2019socialiqa}, \\citet{trivedi2019repurposing}, and \\citet{liu2019mt} respectively.\n\n\\paragraph{Human Performance}\n\\citet{pilehvar2018wic}, \\citet{khashabi2018looking}, \\citet{nangia2019human}, and \\citet{zhang2018record} respectively provide estimates for human performance on WiC, MultiRC, RTE, and ReCoRD. \nFor the remaining tasks, including the diagnostic set, we estimate human performance by hiring crowdworker annotators through Amazon's Mechanical Turk platform to reannotate a sample of each test set.\nWe follow a two step procedure where a crowd worker completes a short training phase before proceeding to the annotation phase, modeled after the method used by \\citet{nangia2019human} for GLUE. \nSee Appendix~\\ref{ax:instruct} for details.\n\n\\subsection{Results}\n\nTable~\\ref{tab:benchmark} shows results for all baselines. \nThe most frequent class and CBOW baselines do not perform well overall, achieving near chance performance for several of the tasks.\nUsing BERT increases the average SuperGLUE score by 25 points,\nattaining significant gains on all of the benchmark tasks, particularly MultiRC, ReCoRD, and RTE. On WSC, BERT actually performs worse than the simple baselines, likely due to the small size of the dataset and the lack of data augmentation.\nUsing MultiNLI as an additional source of supervision for BoolQ, CB, and RTE leads to a 2-5 point improvement on all tasks.\nUsing SWAG as a transfer task for COPA sees an 8 point improvement.\n\nOur best baselines still lag substantially behind human performance.\nOn average, there is a nearly 20 point gap between \\textsc{BERT++} and human performance. The largest gap is on WSC, with a 35 point difference between the best model and human performance. \nThe smallest margins are on BoolQ, CB, RTE, and WiC, with gaps of around 10 points on each of these.\nWe believe these gaps will be challenging to close: On WSC and COPA, human performance is perfect. On three other tasks, it is in the mid-to-high 90s. \nOn the diagnostics, all models continue to lag significantly behind humans.\nThough all models obtain near perfect gender parity scores on Winogender, this is due to the fact that they are obtaining accuracy near that of random guessing.\n\n\\section{Conclusion}\n\nWe present SuperGLUE, a new benchmark for evaluating general-purpose language understanding systems.\nSuperGLUE updates the GLUE benchmark by identifying a new set of challenging NLU tasks, as measured by the difference between human and machine baselines.\nThe set of eight tasks in our benchmark emphasizes diverse task formats and low-data training data tasks, with nearly half the tasks having fewer than 1k examples and all but one of the tasks having fewer than 10k examples.\n\nWe evaluate BERT-based baselines and find that they still lag behind humans by nearly 20 points.\nGiven the difficulty of SuperGLUE for BERT, we expect that further progress in multi-task, transfer, and unsupervised/self-supervised learning techniques will be necessary to approach human-level performance on the benchmark.\nOverall, we argue that SuperGLUE offers a rich and challenging testbed for work developing new general-purpose machine learning methods for language understanding.\n\n\\section{Acknowledgments}\n\nWe thank the original authors of the included datasets in SuperGLUE for their cooperation in the creation of the benchmark, as well as those who proposed tasks and datasets that we ultimately could not include.\nThis work was made possible in part by a donation to NYU from Eric and Wendy Schmidt made by recommendation of the Schmidt Futures program. We gratefully acknowledge the support of the NVIDIA Corporation with the donation of a Titan V GPU used at NYU for this research, and funding from DeepMind for the hosting of the benchmark platform.\nAW is supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE 1342536. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.\nThis project is partly supported by Samsung Advanced Institute of Technology (Next Generation Deep Learning: from Pattern Recognition to AI) and Samsung Electronics (Improving Deep Learning using Latent Structure).\n\n\\clearpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-1804.07461v3.tex",
        "arXiv-1810.12885v1.tex",
        "arXiv-1905.00537v3.tex"
    ],
    "group_id": "group_8",
    "response": "### Title: Evaluating General-Purpose Language Understanding Systems: GLUE, ReCoRD, and SuperGLUE\n\n### Introduction\n\nThe field of Natural Language Processing (NLP) has seen significant advancements in recent years, largely driven by the development of deep learning models and pretraining techniques. These models have been able to achieve impressive performance on a variety of tasks, including question answering, sentiment analysis, and natural language inference. However, the performance of these models is often task-specific and may struggle with out-of-domain data, indicating a need for more generalizable systems that can understand language in a broader context. This summary focuses on three research papers that introduce benchmarks to evaluate the performance of such general-purpose language understanding systems: GLUE, ReCoRD, and SuperGLUE. Each paper aims to assess the capabilities of these models in handling diverse and challenging linguistic phenomena, thereby providing a platform for researchers to develop and test more robust and flexible NLP systems.\n\nGLUE (General Language Understanding Evaluation) was introduced in 2018 as a benchmark to evaluate the performance of models across a diverse set of existing NLU tasks. ReCoRD (Reading Comprehension with Commonsense Reasoning Dataset) was presented in 2018 as a large-scale dataset for machine reading comprehension that requires commonsense reasoning. SuperGLUE, introduced in 2019, builds on the foundation of GLUE but includes more challenging tasks and diagnostic sets to push the boundaries of current NLP models. These benchmarks are crucial for understanding the strengths and weaknesses of current models and for guiding future research towards more general and robust language understanding systems.\n\n### Main Content of Each Paper\n\n#### GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\n\nGLUE is a benchmark designed to evaluate the performance of models across a diverse set of NLU tasks. The benchmark includes nine tasks, such as question answering, sentiment analysis, and textual entailment, and is built on existing datasets to ensure that the tasks are challenging and relevant. GLUE is model-agnostic, allowing for any kind of representation or contextualization, and includes a diagnostic dataset for detailed linguistic analysis. The authors find that multi-task training on all tasks performs better than training separate models for each task, but the low absolute performance of their best model suggests that there is still room for improvement in general NLU systems.\n\n**Tasks Covered:**\n- **Single-Sentence Tasks:** CoLA (acceptability), SST-2 (sentiment).\n- **Similarity and Paraphrase Tasks:** MRPC (paraphrase), QQP (paraphrase), STS-B (sentence similarity).\n- **Inference Tasks:** MNLI (natural language inference), QNLI (question answering and natural language inference), RTE (textual entailment), WNLI (winograd schema challenge).\n\n**Innovations:**\n- **Multi-Task Training:** GLUE encourages models to share general linguistic knowledge across tasks, which is beneficial for handling out-of-domain data.\n- **Diagnostic Dataset:** A hand-crafted dataset to probe models for specific linguistic phenomena, providing insights into their strengths and weaknesses.\n\n#### ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension\n\nReCoRD is a large-scale dataset designed to evaluate machine reading comprehension systems that require commonsense reasoning. Unlike other MRC datasets, ReCoRD contains a significant portion of queries that necessitate reasoning beyond immediate pattern matching, such as understanding coreference and entailment across multiple sentences. The dataset is automatically mined from news articles, reducing human elicitation bias and ensuring a cost-efficient collection method. Human performance on ReCoRD is significantly higher than that of state-of-the-art MRC models, indicating a substantial gap that needs to be addressed.\n\n**Tasks Covered:**\n- **Reading Comprehension:** ReCoRD is a QA task where the system must predict the correct entity to fill a placeholder in a cloze-style query based on a news article.\n\n**Innovations:**\n- **Automatic Data Collection:** The dataset is automatically generated from news articles, which helps in reducing bias and making the data collection process cost-efficient.\n- **Human and Machine Filtering:** Both human and machine filtering are used to ensure that the queries require commonsense reasoning and are not easily answerable by pattern matching.\n\n#### SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems\n\nSuperGLUE is an updated benchmark that includes more challenging tasks than GLUE, designed to push the boundaries of current NLP models. The tasks in SuperGLUE cover a broader range of formats, including coreference resolution and multi-sentence reading comprehension, and are built on existing public datasets. The benchmark includes human performance estimates for all tasks and diagnostic sets, providing a clear target for researchers to aim for. SuperGLUE aims to be a robust evaluation metric for any method capable of being applied to a broad range of language understanding tasks.\n\n**Tasks Covered:**\n- **QA and NLI Tasks:** BoolQ (boolean questions), CB (commitmentBank), COPA (causal reasoning), MultiRC (multi-sentence reading comprehension), ReCoRD (reading comprehension with commonsense reasoning), RTE (textual entailment), WiC (word-in-context), WSC (winograd schema challenge).\n\n**Innovations:**\n- **More Challenging Tasks:** SuperGLUE includes tasks that are beyond the scope of current state-of-the-art systems, such as MultiRC and ReCoRD.\n- **Diagnostic Sets:** AX$_b$ (a diagnostic set for entailment) and AX$_g$ (Winogender, a diagnostic set for gender bias) are included to provide a more comprehensive evaluation of model performance.\n\n### Commonalities and Innovations\n\n**Commonalities:**\n- All three benchmarks aim to evaluate the performance of NLP models across a diverse set of tasks.\n- They include tasks that test the ability of models to understand and reason about texts in English.\n- Each benchmark provides a platform for model evaluation, comparison, and analysis.\n- They all include a diagnostic dataset to probe models for specific linguistic phenomena.\n\n**Innovations:**\n- GLUE introduces a multi-task training regime and a diagnostic dataset to evaluate models on specific linguistic phenomena.\n- ReCoRD is unique in its focus on commonsense reasoning and its automatic data collection method.\n- SuperGLUE expands the scope of tasks to include more challenging formats and introduces human performance estimates for all tasks, providing a clear target for researchers.\n\n### Comparison of Results\n\nThe results of the three benchmarks highlight the current limitations of NLP models in handling diverse and challenging tasks. GLUE shows that multi-task training yields better overall scores compared to single-task training, but the best models still underperform on certain tasks, such as WNLI. ReCoRD demonstrates a significant gap between human and machine performance, with humans achieving 91.69 F1 score while the best models only achieve 46.65 F1 score. SuperGLUE further emphasizes this gap, with BERT-based models lagging behind human performance by nearly 20 points on average. The diagnostic datasets in GLUE and SuperGLUE reveal that models struggle with deeper logical structures and commonsense reasoning, indicating that current models are not yet capable of fully understanding and reasoning about language in a human-like manner.\n\n### Conclusion\n\nThe GLUE, ReCoRD, and SuperGLUE benchmarks provide valuable tools for evaluating and analyzing the performance of NLP models across a diverse set of tasks. They highlight the need for more generalizable and robust models that can handle out-of-domain data and commonsense reasoning. The results from these benchmarks suggest that while significant progress has been made, there is still a substantial gap between human and machine performance. Future research should focus on developing models that can better capture and utilize linguistic knowledge, as well as methods for improving transfer learning and multi-task training. The diagnostic datasets included in these benchmarks will be instrumental in guiding this research by providing insights into the specific areas where current models are lacking.\n\n### Future Research Directions\n\n- **Generalizable Models:** Develop models that can handle a wide range of tasks and domains without requiring task-specific training.\n- **Commonsense Reasoning:** Improve models' ability to reason about language using commonsense knowledge, as seen in the ReCoRD and SuperGLUE benchmarks.\n- **Multi-Task Learning:** Enhance multi-task training methods to better share knowledge across tasks and improve overall performance.\n- **Diagnostic Analysis:** Use diagnostic datasets to analyze and improve models' understanding of specific linguistic phenomena, such as logical structure and world knowledge.\n- **Bias Mitigation:** Address social biases in NLP models, as highlighted by the Winogender diagnostic in SuperGLUE.\n\nThese benchmarks and their diagnostic datasets offer fertile ground for addressing the challenge of developing general-purpose language understanding systems that can match human performance."
}