{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Direct Preference Optimization:\\\\ Your Language Model is Secretly a Reward Model}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nWhile large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training.\nExisting methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF).\nHowever, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model.\n\\rev{In this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be \\emph{optimized exactly} with a single stage of policy training, essentially solving a classification problem on the human preference data.}{In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss.}\nThe resulting algorithm, which we call \\textit{Direct Preference Optimization} (DPO), is stable, performant, and computationally lightweight, eliminating the need for \\rev{fitting a reward model,}{} sampling from the LM during fine-tuning or performing significant hyperparameter tuning.\nOur experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.\n\n\\end{abstract}\n\n\\section{Introduction}\nLarge unsupervised language models (LMs) trained on very large datasets\nacquire surprising capabilities~\\citep{chowdhery2022palm, brown2020language, touvron2023llama,bubeck2023sparks}. However, these models are trained on data generated by humans with a wide variety of goals, priorities, and skillsets. Some of these goals and skillsets may not be desirable to imitate; for example, while we may want our AI coding assistant to \\textit{understand} common programming mistakes in order to correct them, nevertheless, when generating code, we would like to bias our model toward the (potentially rare) high-quality coding ability present in its training data. Similarly, we might want our language model to be \\textit{aware} of a common misconception believed by 50\\% of people, but we certainly do not want the model to claim this misconception to be true in 50\\% of queries about it! In other words, selecting the model's \\emph{desired responses and behavior} from its very wide \\textit{knowledge and abilities} is crucial to building AI systems that are safe, performant, and controllable \\citep{ouyang2022training}. While existing methods typically steer LMs to match human preferences using reinforcement learning (RL), we will show that the RL-based objective used by existing methods can be optimized exactly with a simple binary cross-entropy objective, greatly simplifying the preference learning pipeline.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.999\\textwidth]{figures/diagrams/teaser.png}\n    \\caption{\\textbf{{\\methodac} optimizes for human preferences while avoiding reinforcement learning.} Existing methods for fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and human preferences over pairs of responses, and then use RL to find a policy that maximizes the learned reward. In contrast, {\\methodac} directly optimizes for the policy best satisfying the preferences with a simple classification objective, \\rev{without an explicit standalone reward model or RL}{fitting an \\textit{implicit} reward model whose corresponding optimal policy can be extracted in closed form}.}\n    \\vspace{-2mm}\n    \\label{fig:teaser}\n\\end{figure}\n\nAt a high level, existing methods instill the desired behaviors into a language model using curated sets of human preferences representing the types of behaviors that humans find safe and helpful. This preference learning stage occurs after an initial stage of large-scale unsupervised pre-training on a large text dataset. While the most straightforward approach to preference learning is supervised fine-tuning on human demonstrations of high quality responses, the most successful class of methods is reinforcement learning from human (or AI) feedback (RLHF/RLAIF; \\citep{christiano2017deep,bai2022constitutional}). RLHF methods fit a reward model to a dataset of human preferences and then use RL to optimize a language model policy to produce responses assigned high reward without drifting excessively far from the original model. While RLHF produces models with impressive conversational and coding abilities, the RLHF pipeline is considerably more complex than supervised learning, involving training multiple LMs and sampling from the LM policy in the loop of training, incurring significant computational costs.\n\nIn this paper, we show how to directly optimize a language model to adhere to human preferences, without explicit reward modeling or reinforcement learning.\nWe propose\n\\textit{{\\methodfull} (\\methodac)}, an algorithm that implicitly optimizes the same objective as existing RLHF algorithms (reward maximization with a KL-divergence constraint) but is simple to implement and straightforward to train. Intuitively, the {\\methodac} update increases the relative log probability of preferred to dispreferred responses, but it incorporates a dynamic, per-example importance weight that prevents the model degeneration that we find occurs with a naive probability ratio objective. Like existing algorithms, {\\methodac} relies on a theoretical preference model (such as the Bradley-Terry model; \\cite{bradley1952rankanalysis}) that measures how well a given reward function aligns with empirical preference data. However, while existing methods use the preference model to define a preference loss to train a reward model and then train a policy that optimizes the learned reward model, {\\methodac} uses a change of variables to define the preference loss as a function of the policy directly. Given a dataset of human preferences over model responses, {\\methodac} can therefore optimize a policy using a simple binary cross entropy objective, \\rev{without learning an explicit, standalone reward model or sampling from the policy during training}{producing the optimal policy to an implicit reward function fit to the preference data}.\n\nOur main contribution is {\\methodfull} (\\methodac), a simple RL-free algorithm for training language models from preferences. Our experiments show that {\\methodac} is at least as effective as existing methods, including PPO-based RLHF, for learning from preferences in tasks such as sentiment modulation, summarization, and dialogue, using language models with up to 6B parameters.\n\n\\section{Related Work}\n\nSelf-supervised language models of increasing scale learn to complete some tasks zero-shot \\citep{radford2019language} or with few-shot prompts \\citep{gpt3,megatron,chowdhery2022palm}. However, their performance on downstream tasks and alignment with user intent can be significantly improved by fine-tuning on datasets of instructions and human-written completions \\citep{mishra-etal-2022-cross,sanh2022multitask,chung2022scaling,thoppilan2022lamda}. This `instruction-tuning' procedure\nenables LLMs to generalize to instructions outside of the instruction-tuning set and generally increase their usability \\citep{chung2022scaling}. Despite the success of instruction tuning, \\textit{relative} human judgments of response quality are often easier to collect than expert demonstrations, and thus subsequent works have fine-tuned LLMs with datasets of human preferences, improving proficiency in translation \\citep{kreutzer-etal-2018-reliability}, summarization \\citep{stiennon2022learning,ziegler2020finetuning}, story-telling \\citep{ziegler2020finetuning}, and instruction-following \\citep{ouyang2022training,ramamurthy2023is}. These methods first optimize a neural network reward function for compatibility with the dataset of preferences under a preference model such as the Bradley-Terry model \\citep{bradley1952rankanalysis}, then fine-tune a language model to maximize the given reward using reinforcement learning algorithms, commonly REINFORCE \\citep{williams1992reinforce}, proximal policy optimization (PPO; \\cite{schulman2017proximal}), or variants \\citep{ramamurthy2023is}. A closely-related line of work leverages LLMs fine-tuned for instruction following with human feedback to generate additional synthetic preference data for targeted attributes such as safety or harmlessness \\citep{bai2022constitutional}, using only weak supervision from humans in the form of a text rubric for the LLM's annotations. These methods represent a convergence of two bodies of work: one body of work on training language models with reinforcement learning for a variety of objectives~\\citep{Ranzato2015SequenceLT,paulus2018a,wu2018learning} and another body of work on general methods for learning from human preferences \\citep{christiano2017deep,kupcsik2018learning}. \nDespite the appeal of using relative human preferences, fine-tuning large language models with reinforcement learning remains a major practical challenge; this work provides a theoretically-justified approach to optimizing relative preferences without RL.\n\nOutside of the context of language, learning policies from preferences has been studied in both bandit and reinforcement learning settings, and several approaches have been proposed. Contextual bandit learning using preferences or rankings of actions, rather than rewards, is known as a contextual dueling bandit (CDB; \\cite{yue2012karmed,dudik2015contextual}). In the absence of absolute rewards, theoretical analysis of CDBs substitutes the notion of an optimal policy with a \\textit{von Neumann winner}, a policy whose expected win rate against \\textit{any} other policy is at least 50\\% \\citep{dudik2015contextual}. However, in the CDB setting, preference labels are given online, while in learning from human preferences, we typically learn from a fixed batch of offline preference-annotated action pairs \\citep{yan2022human}. Similarly, \\textit{preference-based RL} (PbRL) learns from binary preferences generated by an \\textit{unknown} `scoring' function rather than rewards \\citep{BusaFekete2014,ruiz2023dueling}. Various algorithms for PbRL exist, including methods that can reuse off-policy preference data, but generally involve first explicitly estimating the latent scoring function (i.e. the reward model) and subsequently optimizing it \\citep{jain2013learning,BusaFekete2014,christiano2017deep,sadigh2017active,kupcsik2018learning}. We instead present a single stage policy learning approach that directly optimizes a policy to satisfy preferences.\n\n\\section{Preliminaries}\\label{section:prelims}\n\nWe review the RLHF pipeline in \\citeauthor{ziegler2020finetuning} (and later \\citep{stiennon2022learning, bai2022training, ouyang2022training}). It usually includes three phases: 1) supervised fine-tuning (SFT); 2) preference sampling and reward learning and 3) RL optimization.\n\n\\textbf{SFT}: RLHF typically begins by fine-tuning a pre-trained LM with supervised learning on high-quality data for the downstream task(s) of interest (dialogue, summarization, etc.), to obtain a model $\\pisft$. \n\n\\textbf{Reward Modelling Phase}: In the second phase the SFT model is prompted with prompts $x$ to produce pairs of answers $(y_1, y_2)\\sim \\pisft(y \\mid x)$. These are then presented to human labelers who express preferences for one answer, denoted as $y_w\\succ y_l \\mid x$ where $y_w$ and $y_l$ denotes the preferred and dispreferred completion amongst $(y_1, y_2)$ respectively. The preferences are assumed to be generated by some latent reward model $r^*(y, x)$, which we do not have access to. There are a number of approaches used to model preferences, the Bradley-Terry (BT) \\cite{bradley1952rankanalysis} model being a popular choice (although more general Plackett-Luce ranking models \\citep{plackett1975analysis, luce2012individual} are also compatible with the framework if we have access to several ranked answers). The BT model stipulates that the human preference distribution $p^*$ can be written as:\n\\begin{equation}\\label{eq:bradley-terry}\n    p^*(y_1\\succ y_2 \\mid x)=\\frac{\\exp\\left(r^*(x, y_1)\\right)}{\\exp\\left(r^*(x, y_1)\\right) + \\exp\\left(r^*(x, y_2)\\right)}.\n\\end{equation}\nAssuming access to a static dataset of comparisons $\\mathcal{D}=\\bigl\\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\\bigr\\}_{i=1}^N$ sampled from $p^*$, we can parametrize a reward model $r_{\\phi}(x, y)$ and estimate the parameters via maximum likelihood. Framing the problem as a binary classification we have the negative log-likelihood loss:\n\\begin{equation}\\label{eq:reward_model}\n    \\mathcal{L}_R(r_{\\phi}, \\mathcal{D}) = -\\mathbb{E}_{(x, y_w, y_l)\\sim \\mathcal{D}}\\bigl[\\log \\sigma(r_{\\phi}(x, y_w)- r_{\\phi}(x, y_l))\\bigr]\n\\end{equation}\nwhere $\\sigma$ is the logistic function. In the context of LMs, the network $r_{\\phi}(x, y)$ is often initialized from the SFT model $\\pisft(y \\mid x)$ with the addition of a linear layer on top of the final transformer layer that produces a single scalar prediction for the reward value \\cite{ziegler2020finetuning}. To ensure a reward function with lower variance, prior works normalize the rewards, such that  $\\mathbb{E}_{x,y\\sim \\mathcal{D}}\\left[r_\\phi(x, y)\\right] = 0$ for all $x$.\n\n\\textbf{RL Fine-Tuning Phase}: During the RL phase, the learned reward function is used to provide feedback to the language model. Following prior works~\\citep{jaques2017sequence, jaques2020human}, the optimization is formulated as\n\\begin{equation}\\label{eq:RL}\n\\max_{\\pi_{\\theta}}  \\mathbb{E}_{x\\sim \\mathcal{D}, y\\sim \\pi_{\\theta}(y \\mid x)}\\bigl[r_{\\phi}(x, y)\\bigr] - \\beta\\mathbb{D}_{\\textrm{KL}}\\bigl[\\pi_{\\theta}(y\\mid x)\\mid \\mid \\piref(y\\mid x)\\bigr],\n\\end{equation}\nwhere $\\beta$ is a parameter controlling the deviation from the base reference policy $\\piref$, namely the initial SFT model $\\pisft$. \nIn practice, the language model policy $\\pi_\\theta$ is also initialized to $\\pisft$. The added constraint is important, as it prevents the model from deviating too far from the distribution on which the reward model is accurate, as well as maintaining the generation diversity and preventing mode-collapse to single high-reward answers. Due to the discrete nature of language generation, this objective is not differentiable and is typically optimized with reinforcement learning. The standard approach \\citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training} has been to construct the reward function ${r(x, y) = r_{\\phi}(x, y) -\\beta (\\log \\pi_{\\theta}(y\\mid x) - \\log \\piref(y\\mid x))}$, and maximize using PPO \\cite{schulman2017proximal}. \n\n\\section{Direct Preference Optimization}\\label{sec:DPO}\n\nMotivated by the challenges of applying reinforcement learning algorithms on large-scale problems such as fine-tuning language models, our goal is to derive a simple approach for policy optimization using preferences directly. Unlike prior RLHF methods, which learn a reward and then optimize it via RL, our approach \\rev{bypasses the reward modeling step and directly optimizes a language model using preference data}{leverages a particular choice of reward model parameterization that enables extraction of its optimal policy in closed form, without an RL training loop}. \nAs we will describe next in detail, our key insight is to leverage an analytical mapping from reward functions to optimal policies, which enables us to transform a loss function over reward functions into a loss function over policies.\nThis change-of-variables approach \\rev{allows us to skip the explicit reward modeling step}{avoids fitting an explicit, standalone reward model}, while still optimizing under existing models of human preferences, such as the Bradley-Terry model. In essence, the policy network represents both the language model and the \\rev{}{(implicit)} reward.\n\n\\textbf{Deriving the DPO objective.} We start with the same RL objective as prior work, Eq.~\\ref{eq:RL}, under a general reward function $r$. Following prior work~\\citep{peters2007reinforcement, peng2019advantage, korbak2022reinforcement, go2023aligning}, it is straightforward to show that the optimal solution to the KL-constrained reward maximization objective in Eq.~\\ref{eq:RL} takes the form:\n\\begin{equation}\\label{eq:op_policy}\n    \\pi_r(y\\mid x) = \\frac{1}{Z(x)}\\piref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right),\n\\end{equation}%\nwhere $Z(x) =\\sum_{y}\\piref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)$ is the partition function. See Appendix \\ref{app:derivation1} for a complete derivation. Even if we use the MLE estimate $r_{\\phi}$ of the ground-truth reward function $r^*$, it is still expensive to estimate the partition function $Z(x)$ \\citep{korbak2022reinforcement, go2023aligning}, which makes this representation hard to utilize in practice. However, we can rearrange Eq.~\\ref{eq:op_policy} to express the reward function in terms of its corresponding optimal policy $\\pi_r$, the reference policy $\\piref$, and the unknown partition function $Z(\\cdot)$. Specifically, we first take the logarithm of both sides of Eq.~\\ref{eq:op_policy} and then with some algebra we obtain:\n\\begin{equation}\\label{eq:main_eq}\n    r(x,y) =\\beta \\log \\frac{\\pi_r(y\\mid x)}{\\piref(y\\mid x)} + \\beta \\log Z(x).\n\\end{equation}\nWe can apply this reparameterization to the ground-truth reward $r^*$ and corresponding optimal model $\\pi^*$. Fortunately, the Bradley-Terry model depends only on the difference of rewards between two completions, i.e., ${p^*(y_1 \\succ y_2 \\mid x) = \\sigma(r^*(x, y_1) - r^*(x, y_2))}$. Substituting the reparameterization in Eq.~\\ref{eq:main_eq} for $r^*(x,y)$ into the preference model Eq.~\\ref{eq:bradley-terry}, the partition function cancels, and we can express the human preference probability in terms of only the optimal policy $\\pi^*$ and reference policy $\\piref$. Thus, the optimal RLHF policy $\\pi^*$ under the Bradley-Terry model satisfies the preference model:\n\\begin{equation}\\label{eq:objective}\n    p^*(y_1\\succ y_2 \\mid x)=\\frac{1}{1 + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2\\mid x)}{\\piref(y_2\\mid x)} - \\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\piref(y_1\\mid x)}\\right)}\n\\end{equation}\nThe derivation is in Appendix~\\ref{app:derivation2}. While Eq.~\\ref{eq:objective} uses the Bradley-Terry model, we can similarly derive expressions under the more general Plackett-Luce models~\\citep{plackett1975analysis, luce2012individual}, shown in Appendix~\\ref{app:plackett_luce_models}.\n\nNow that we have \nthe probability of human preference data in terms of the optimal policy rather than the reward model, we can formulate a maximum likelihood objective for a parametrized policy $\\pi_\\theta$. Analogous to the reward modeling approach (i.e. Eq.~\\ref{eq:reward_model}), our policy objective becomes:\n\\begin{equation}\\label{eq:optimum_model}\n    \\mathcal{L}_\\text{DPO}(\\pi_{\\theta}; \\piref) = -\\mathbb{E}_{(x, y_w, y_l)\\sim \\mathcal{D}}\\left[\\log \\sigma \\left(\\beta \\log \\frac{\\pi_{\\theta}(y_w\\mid x)}{\\piref(y_w\\mid x)} - \\beta \\log \\frac{\\pi_{\\theta}(y_l\\mid x)}{\\piref(y_l\\mid x)}\\right)\\right].\n\\end{equation}\n\\rev{This way, we simultaneously bypass the explicit reward modeling step while also avoiding the need to perform reinforcement learning optimization.}{This way, we fit an implicit reward using an alternative parameterization, whose optimal policy is simply $\\pi_\\theta$.} Moreover, since our procedure is equivalent to fitting a reparametrized Bradley-Terry model, it enjoys certain theoretical properties, such as consistencies under suitable assumption of the preference data distribution \\cite{bong2022generalized}. In Section~\\ref{sec:theory}, we further discuss theoretical properties of DPO in relation to other works.\n\n\\textbf{What does the DPO update do?} For a mechanistic understanding of DPO, it is useful to analyze the gradient of the loss function $\\mathcal{L}_\\text{DPO}$. The gradient with respect to the parameters $\\theta$ can be written as:\n\\begin{multline*}\\label{eq:gradient}\n    \\nabla_\\theta \\mathcal{L}_\\text{DPO}(\\pi_\\theta;\\piref) = \\\\ -\\beta\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\bigg[\\underbrace{\\sigma(\\hat{r}_\\theta(x, y_l) - \\hat{r}_\\theta (x, y_w))}_\\text{higher weight when reward estimate is wrong}\\bigg[\\underbrace{\\nabla_\\theta\\log \\pi(y_w \\mid x)}_\\text{increase likelihood of $y_w$} - \\underbrace{\\nabla_\\theta\\log\\pi(y_l \\mid x)}_\\text{decrease likelihood of $y_l$}\\bigg]\\bigg],\n\\end{multline*}\nwhere $\\hat{r}_\\theta(x, y) = \\beta \\log \\frac{\\pi_\\theta(y \\mid x)}{\\piref(y \\mid x)}$ is the reward implicitly defined by the language model $\\pi_\\theta$ and reference model $\\piref$ (more in Section~\\ref{sec:theory}). Intuitively, the gradient of the loss function $\\mathcal{L}_\\text{DPO}$ increases the likelihood of the preferred completions $y_w$ and decreases the likelihood of dispreferred completions $y_l$. Importantly, the examples are weighed by how much higher the implicit reward model $\\hat{r}_\\theta$ rates the dispreferred completions, scaled by $\\beta$, i.e, how incorrectly the implicit reward model orders the completions, accounting for the strength of the KL constraint. Our experiments suggest the importance of this weighting, as a na\\\"ive version of this method without the weighting coefficient can cause the language model to degenerate (Appendix Table~\\ref{tab:unlikelihood_generations}).\n\n\\textbf{DPO outline.} \nThe general DPO pipeline is as follows: 1) Sample completions $y_1, y_2 \\sim \\piref(\\cdot \\mid x)$ for every prompt $x$, label with human preferences to construct the offline dataset of preferences $\\mathcal{D} = \\{x^{(i)}, y_w^{(i)}, y_l)^{(i)}\\}_{i=1}^N$ and 2) optimize the language model $\\pi_\\theta$ to minimize $\\mathcal{L}_\\text{DPO}$ for the given $\\piref$ and $\\mathcal{D}$ and desired $\\beta$. \nIn practice, one would like to reuse preference datasets publicly available, rather than generating samples and gathering human preferences. Since the preference datasets are sampled using $\\pisft$, we initialize $\\piref = \\pisft$ whenever available. However, when $\\pisft$ is not available, we initialize $\\piref$ by maximizing likelihood of preferred completions ${(x, y_w)}$, that is, ${\\piref = \\argmax_{\\pi}\\mathbb{E}_{x, y_w \\sim \\mathcal{D}}\\left[\\log \\pi(y_w \\mid x)\\right]}$. This procedure helps mitigate the distribution shift between the true reference distribution which is unavailable, and $\\piref$ used by DPO. Further details related to the implementation and hyperparameters can be found in Appendix~\\ref{app:implementation}.\n\n\\section{Theoretical Analysis of DPO}\nIn this section, we give further interpretation of the DPO method, provide theoretical backing, and relate advantages of DPO to issues with actor critic algorithms used for RLHF (such as PPO~\\cite{schulman2017proximal}).\n\n\\label{sec:theory}\n\n\\subsection{Your Language Model Is Secretly a Reward Model} DPO is able to bypass both \\rev{explicit reward estimation}{fitting an explicit reward} and performing RL to learn the policy using a single maximum likelihood objective. Note the optimization objective Eq. \\ref{eq:main_eq} is equivalent to a Bradley-Terry model with a reward parameterization $r^*(x, y) = \\beta \\log\\frac{\\pi^*_\\theta(y \\mid x)}{\\piref(y \\mid x)}$ and we optimize our parametric model $\\pi_{\\theta}$, equivalently to the reward model optimization in Eq. \\ref{eq:reward_model} under the change of variables. In this section we will build the theory behind this reparameterization, show that it does not constrain the class of learned reward models, and allows for the exact recovery of the optimal policy. We begin with by defining an equivalence relation between reward functions. \n\n\\begin{definition}\nWe say that two reward functions $r(x, y)$ and $r'(x, y)$ are equivalent iff ${r(x, y)-r'(x, y) = f(x)}$ for some function $f$.     \n\\end{definition}\nIt is easy to see that this is indeed an equivalence relation, which partitions the set of reward functions into classes. We can state the following two lemmas:\n\n\\begin{lemma}\\label{lemma:same_prefrence} Under the Plackett-Luce, and in particular the Bradley-Terry, preference framework, two reward functions from the same class induce the same preference distribution.\n\\end{lemma}\n\n\\begin{lemma}\\label{lemma:same_policy}\n    Two reward functions from the same equivalence class induce the same optimal policy under the constrained RL problem.\n\\end{lemma}\nThe proofs are straightforward and we defer them to Appendix \\ref{app:lemma1}. The first lemma is a well-known under-specification issue with the Plackett-Luce family of models \\cite{plackett1975analysis}. Due to this under-specification, we usually have to impose additional identifiability constraints to achieve any guarantees on the MLE estimates from Eq. \\ref{eq:reward_model} \\cite{bong2022generalized}. The second lemma states that all reward functions from the same class yield the same optimal policy, hence for our final objective, we are only interested in recovering an arbitrary reward function from the optimal class. We prove the following Theorem in Appendix~\\ref{app:thm1}:\n\\begin{theorem}\\label{thm:main}\n    Under mild assumptions, all reward classes consistent with the Plackett-Luce (and Bradley-Terry in particular) models can be represented with the reparameterization ${r(x, y) = \\beta \\log \\frac{\\pi(y\\mid x)}{\\piref(y\\mid x)}}$ for some model $\\pi(y\\mid x)$ and a given reference model $\\piref(y \\mid x)$.\n\\end{theorem}\n\\begin{sproof}\n    Consider any reward function $r(x, y)$, which induces a corresponding optimal model $\\pi_r(y \\mid x)$, specified by Eq. \\ref{eq:op_policy}. We will show that a reward function from the equivalence class of $r$ can be represented using the reparameterization given above. We define the projection $f$ as  \n\\begin{equation}\n    f(r; \\piref, \\beta)(x, y) = r(x, y) - \\beta\\log\\sum_{y}\\piref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)\n\\end{equation}\nThe operator $f$ simply normalizes the reward function with the logarithm of the partition function of $\\pi_r$. Since the added normalization term is only a function of the prefix $x$, $f(r; \\piref, \\beta)(x, y) $ is a reward function in the equivalence class of $r(x, y)$. Finally, replacing $r$ with the RHS of Eq.~\\ref{eq:main_eq} (which holds for any reward function), we have $f(r; \\piref, \\beta)(x, y) = \\beta \\log \\frac{\\pi_r(y\\mid x)}{\\piref(y\\mid x)}$. That is, the projection $f$ produces a member of the equivalence class of $r$ with the desired form, and we do not lose any generality in our reward model from the proposed reparameterization.\n\\end{sproof}\nWe can alternatively view Theorem~\\ref{thm:main} as specifying exactly which reward function within each equivalence class the DPO reparameterization selects, that is, the reward function satisfying:\n\\begin{equation}\\label{eq:lag_p}\n     \\sum_{y}\\underbrace{\\piref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)}_{=\\pi(y\\mid x)\\text{, using Thm.~\\ref{thm:main} reparam.}} = 1,\n\\end{equation}\ni.e., $\\pi(y\\mid x)$ is a valid distribution (probabilities are positive and sum to 1).\nHowever, following Eq.~\\ref{eq:op_policy}, we can see that Eq.~\\ref{eq:lag_p} is the partition function of the optimal policy induced by the reward function $r(x, y)$.\nThe key insight of the DPO algorithm is that we can impose certain constraints on the under-constrained Plackett-Luce (and Bradley-Terry in particular) family of preference models, such that we preserve the class of representable reward models, but explicitly make the optimal policy in Eq. \\ref{eq:op_policy} analytically tractable for all prompts $x$.\n\n\\subsection{Instability of Actor-Critic Algorithms}\nWe can also use our framework to diagnose instabilities with standard actor-critic algorithms used for the RLHF, such as PPO. We follow the RLHF pipeline and focus on the RL fine-tuning step outlined in Section \\ref{section:prelims}. We can draw connections to the control as inference framework \\cite{levine2018reinforcement} for the constrained RL problem outlined in \\ref{eq:RL}. We assume a parameterized model $\\pi_{\\theta}(y\\mid x)$ and minimize $\\mathbb{D}_{\\text{KL}}[\\pi_{\\theta}(y|x) \\mid \\mid \\pi^*(y\\mid x)]$ where $\\pi^*$ is the optimal policy from Eq. \\ref{eq:optimum_model} induced by the reward function $r_{\\phi}(y, x)$. With some algebra this leads to the optimization objective:\n\\begin{equation}\\label{eq:AC}\n    \\max_{\\pi_{\\theta}}\\mathbb{E}_{\\pi_{\\theta}(y\\mid x)}\\bigg[\\underbrace{r_{\\phi}(x, y) -\\beta\\log\\sum_{y}\\piref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r_{\\phi}(x, y)\\right)}_{f(r_{\\phi}, \\piref, \\beta)} - \\underbrace{\\beta\\log\\frac{\\pi_{\\theta}(y\\mid x)}{\\piref(y\\mid x)}}_{\\text{KL}}\\bigg]\n\\end{equation}\nThis is the same objective optimized in prior works \n\\citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training} using the DPO-equivalent reward for the reward class of $r_{\\phi}$. In this setting, we can interpret the normalization term in $f(r_{\\phi}, \\piref, \\beta)$ as the soft value function of the reference policy $\\piref$. While this term does not affect the optimal solution, without it, the policy gradient of the objective could have high variance, making learning unstable. We can accommodate for the normalization term using a learned value function, but that can also be difficult to optimize. Alternatively, prior works have normalized rewards using a human completion baseline, essentially a single sample Monte-Carlo estimate of the normalizing term. In contrast the DPO reparameterization yields a reward function that does not require any baselines. \n\n\\section{Experiments}\nIn this section, we empirically evaluate DPO's ability to train policies directly from preferences. First, in a well-controlled text-generation setting, we ask: how efficiently does DPO trade off maximizing reward and minimizing KL-divergence with the reference policy, compared to common preference learning algorithms such as PPO? Next, we evaluate DPO's performance on larger models and more difficult RLHF tasks, including summarization and dialogue. We find that with almost no tuning of hyperparameters, DPO tends to perform as well or better than strong baselines like RLHF with PPO as well as returning the best of $N$ sampled trajectories under a learned reward function. Before presenting these results, we describe the experimental set-up; additional details are in Appendix~\\ref{app:exp_details}.\n\n\\textbf{Tasks.} Our experiments explore three different open-ended text generation tasks. For all experiments, algorithms learn a policy from a dataset of preferences $\\mathcal{D}=\\bigl\\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\\bigr\\}_{i=1}^N$. In \\textbf{controlled sentiment generation}, $x$ is a prefix of a movie review from the IMDb dataset \\cite{maas-EtAl:2011:ACL-HLT2011}, and the policy must generate $y$ with positive sentiment. In order to perform a controlled evaluation, for this experiment we \\textit{generate} preference pairs over generations using a pre-trained sentiment classifier, where $p(\\text{positive}\\mid x,y_w)>p(\\text{positive}\\mid x,y_l)$. For SFT, we fine-tune GPT-2-large until convergence on reviews from the train split of the IMDB dataset (further details in App~\\ref{app:sentiment_details}). In \\textbf{summarization}, $x$ is a forum post from Reddit; the policy must generate a summary $y$ of the main points in the post. Following prior work, we use the Reddit TL;DR summarization dataset \\citep{volske-etal-2017-tl} along with human preferences gathered by \\citeauthor{stiennon2022learning}. We use an SFT model fine-tuned on human-written forum post summaries\\footnote{\\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}} with the TRLX \\citep{leandro_von_werra_2023_7790115} framework for RLHF. The human preference dataset was gathered by \\citeauthor{stiennon2022learning} on samples from a different, but similarly-trained, SFT model. Finally, in \\textbf{single-turn dialogue}, \n$x$ is a human query, which may be anything from a question about astrophysics to a request for relationship advice. A policy must produce an engaging and helpful response $y$ to a user's query; we use the Anthropic Helpful and Harmless dialogue dataset \\citep{bai2022training}, containing 170k dialogues between a human and an automated assistant. Each transcript ends with a pair of responses generated by a large (although unknown) language model along with a preference label denoting the human-preferred response. In this setting, no pre-trained SFT model is available; we therefore fine-tune an off-the-shelf language model on only the preferred completions to form the SFT model.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.50\\textwidth]{figures/results/frontier.pdf}\n    \\includegraphics[width=0.49\\textwidth]{figures/results/tldr_winrate_vs_temp.pdf}\n    \\caption{\\textbf{Left.} The frontier of expected reward vs KL to the reference policy. DPO provides the highest expected reward for all KL values, demonstrating the quality of the optimization. \\textbf{Right.} TL;DR summarization win rates vs. human-written summaries, using GPT-4 as evaluator. DPO exceeds PPO's best-case performance on summarization, while being more robust to changes in the sampling temperature.}\n    \\vspace{-2mm}\n    \\label{fig:frontier-tldr-main}\n\\end{figure}\n\n\\textbf{Evaluation.} Our experiments use two different approaches to evaluation. In order to analyze the effectiveness of each algorithm in optimizing the constrained reward maximization objective, in the controlled sentiment generation setting we evaluate each algorithm by its frontier of achieved reward and KL-divergence from the reference policy; this frontier is computable because we have acccess to the ground-truth reward function (a sentiment classifier). However, in the real world, the ground truth reward function is not known; therefore, we evaluate algorithms with their \\textit{win rate} against a baseline policy, using GPT-4 as a proxy for human evaluation of summary quality and response helpfulness in the summarization and single-turn dialogue settings, respectively. For summarization, we use reference summaries in the test set as the baseline; for dialogue, we use the preferred response in the test dataset as the baseline. While existing studies suggest LMs can be better automated evaluators than existing metrics \\citep{Chen2023ExploringTU}, we conduct a human study to justify our usage of GPT-4 for evaluation in Sec.~\\ref{sec:human-judgments}. We find GPT-4 judgments correlate strongly with humans, with human agreement with GPT-4 typically similar or higher than inter-human annotator agreement. \n\n\\textbf{Methods.} In addition to DPO, we evaluate several existing approaches to training language models to adhere to human preferences. Most simply, we explore zero-shot prompting with \\textbf{GPT-J} \\citep{gpt-j} in the summarization task and 2-shot prompting with \\textbf{Pythia-2.8B} \\citep{biderman2023pythia} in the dialogue task. In addition, we evaluate the \\textbf{SFT} model as well as \\textbf{Preferred-FT}, which is a model fine-tuned with supervised learning on the chosen completion $y_w$ from either the SFT model (in controlled sentiment and summarization) or a generic LM (in single-turn dialogue). Another pseudo-supervised method is \\textbf{Unlikelihood}~\\citep{welleck2019neural}, which simply optimizes the policy to maximize the probability assigned to $y_w$ and \\textit{minimize} the probability assigned to $y_l$; we use an optional coefficient $\\alpha\\in[0,1]$ on the `unlikelihood' term. We also consider \\textbf{PPO} \\citep{schulman2017proximal} using a reward function learned from the preference data and \\textbf{PPO-GT}, which is an oracle that learns from the ground truth reward function available in the controlled sentiment setting. In our sentiment experiments, we use two implementations of PPO-GT, one of-the-shelf version \\cite{leandro_von_werra_2023_7790115} as well as a modified version that normalizes rewards and further tunes hyperparameters to improve performance (we also use these modifications when running `normal' PPO with learned rewards). Finally, we consider the \\textbf{Best of $N$} baseline, sampling $N$ responses from the SFT model (or Preferred-FT in dialogue) and returning the highest-scoring response according to a reward function learned from the preference dataset. This high-performing method decouples the quality of the reward model from the PPO optimization, but is computationally impractical even for moderate $N$ as it requires sampling $N$ completions for every query at test time.\n\n\\subsection{How well can DPO optimize the RLHF objective?}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.50\\textwidth]{figures/results/dialogue_winrate_vs_temp.pdf}\n    \\includegraphics[width=0.49\\textwidth]{figures/results/dialogue_winrate_vs_steps.pdf}\n    \\caption{\\textbf{Left.} Win rates computed by GPT-4 for Anthropic-HH one-step dialogue; DPO is the only method that improves over chosen summaries in the Anthropic-HH test set. \\textbf{Right.} Win rates for different sampling temperatures over the course of training. DPO's improvement over the dataset labels is fairly stable over the course of training for different sampling temperatures.}\n    \\vspace{-2mm}\n    \\label{fig:dialogue-main}\n\\end{figure}\n\nThe KL-constrained reward maximization objective used in typical RLHF algorithms balances exploitation of reward while restricting the policy from deviating far from the reference policy. Therefore, when comparing algorithms, we must take into account both reward achieved as well as the KL discrepancy; achieving slightly higher reward but with much higher KL is not necessarily desirable. Figure~\\ref{fig:frontier-tldr-main} shows the reward-KL frontier for various algorithms in the sentiment setting. We execute multiple training runs for each algorithm, using a different hyperparameter for policy conservativeness in each run (target KL $\\in\\{3,6,9,12\\}$ for PPO, $\\beta \\in \\{0.05,0.1,1,5\\}$, $\\alpha\\in\\{0.05,0.1,0.5,1\\}$ for unlikelihood, random seeds for preferred-FT). This sweep includes 22 runs in total. After each 100 training steps until convergence, we evaluate each policy on a set of test prompts, computing the average reward under the true reward function as well as the average sequence-level KL\\footnote{That is, the sum of the per-timestep KL-divergences.} with the reference policy $\\text{KL}\\left(\\pi\\mid \\mid \\piref\\right)$. We find that DPO produces by far the most efficient frontier, achieving the highest reward while still achieving low KL. This result is particularly notable for multiple reasons. First, DPO and PPO optimize the same objective, but DPO is notably more efficient; DPO's reward/KL tradeoff strictly dominates PPO. Second, DPO achieves a better frontier than PPO, \\emph{even when PPO can access ground truth rewards} (PPO-GT).\n\n\\subsection{Can DPO scale to real preference datasets?}\n\\label{sec:dpo-real-datasets}\nNext, we evaluate fine-tuning performance of DPO on summarization and single-turn dialogue. For summarization, \nautomatic evaluation metrics such as ROUGE can be poorly correlated with human preferences~\\citep{stiennon2022learning}, and prior work has found that fine-tuning LMs using PPO on human preferences to provide more effective summaries. We evaluate different methods by sampling completions on the test split of TL;DR summarization dataset, and computing the average win rate against reference completions in the test set. The completions for all methods are sampled at temperatures varying from 0.0 to 1.0, and the win rates are shown in Figure~\\ref{fig:frontier-tldr-main} (right). DPO, PPO and Preferred-FT all fine-tune the same GPT-J SFT model\\footnote{\\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}}. We find that DPO has a win rate of approximately 61\\% at a temperature of 0.0, exceeding the performance of PPO at ~57\\% at its optimal sampling temperature of 0.0. DPO also achieves a higher maximum win rate compared to the best of $N$ baseline. We note that we did not meaningfully tune DPO's $\\beta$ hyperparameter, so these results may underestimate DPO's potential. Moreover, we find DPO to be much more robust to the sampling temperature than PPO, the performance of which can degrade to that of the base GPT-J model at high temperatures. Preferred-FT does not improve significantly over the SFT model. We also compare DPO and PPO head-to-head in human evaluations in Section~\\ref{sec:human-judgments}, where DPO samples at temperature 0.25 were preferred 58\\% times over PPO samples at temperature 0.\n\nOn single-turn dialogue, we evaluate the different methods on the subset of the test split of the Anthropic HH dataset \\citep{bai2022training} with one step of human-assistant interaction. GPT-4 evaluations use the preferred completions on the test as the reference to compute the win rate for different methods. As there is no standard SFT model for this task, we start with a pre-trained Pythia-2.8B, use Preferred-FT to train a reference model on the chosen completions such that completions are within distribution of the model, and then train using DPO. We also compare against the best of 128 Preferred-FT completions (we found the Best of $N$ baseline plateaus at 128 completions for this task; see Appendix Figure~\\ref{fig:best-of-n}) and a 2-shot prompted version of the Pythia-2.8B base model, finding DPO performs as well or better for the best-performing temperatures for each method. We also evaluate an RLHF model trained with PPO on the Anthropic HH dataset \\footnote{\\url{https://huggingface.co/reciprocate/ppo_hh_pythia-6B}} from a well-known source \\footnote{\\url{https://github.com/CarperAI/trlx/tree/main/examples/hh}}, but are unable to find a prompt or sampling temperature that gives performance better than the base Pythia-2.8B model. Based on our results from TL;DR and the fact that both methods optimize the same reward function, we consider Best of 128 a rough proxy for PPO-level performance. Overall, DPO is the only computationally efficient method that improves over the preferred completions in the Anthropic HH dataset, and provides similar or better performance to the computationally demanding Best of 128 baseline. Finally, Figure~\\ref{fig:dialogue-main} shows that DPO converges to its best performance relatively quickly.\n\n\\subsection{Generalization to a new input distribution}\n\n\\begin{wraptable}{r}{0.375\\textwidth}\n    \\small\n    \\vspace{-10mm}\n    \\begin{tabular}{ccc}\n        \\toprule\n        & \\multicolumn{2}{c}{\\textbf{Win rate vs. ground truth}} \\\\\n        \\cmidrule(lr){2-3}\n        \\textbf{Alg.} & Temp $0$ & Temp $0.25$ \\\\\n        \\midrule\n        DPO & 0.36 & 0.31 \\\\\n        PPO & 0.26 & 0.23 \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{GPT-4 win rates vs. ground truth summaries for out-of-distribution CNN/DailyMail input articles.}\n    \\vspace{-3mm}\n    \\label{tab:ood}\n\\end{wraptable}\n\nTo further compare the performance of PPO and DPO under distribution shifts, we evaluate the PPO and DPO policies from our Reddit TL;DR summarization experiment on a different distribution, news articles in the test split of the CNN/DailyMail dataset \\citep{nallapati-etal-2016-abstractive}, using the best sampling temperatures from TL;DR (0 and 0.25). The results are presented in Table~\\ref{tab:ood}. We computed the GPT-4 win rate against the ground-truth summaries in the datasets, using the same GPT-4 (C) prompt we used for Reddit TL;DR, but replacing the words ``forum post'' with ``news article''. For this new distribution, DPO continues to outperform the PPO policy by a significant margin. This experiment provides initial evidence that DPO policies can generalize similarly well to PPO policies, even though DPO does not use the additional unlabeled Reddit TL;DR prompts that PPO uses.\n\n\\subsection{Validating GPT-4 judgments with human judgments}\n\\label{sec:human-judgments}\nWe conduct a human study to verify the reliability of GPT-4's judgments, using the results of the TL;DR summarization experiment and two different GPT-4 prompts. The \\textbf{GPT-4 (S)} (simple) prompt simply asks for which summary better-summarizes the important information in the post. The \\textbf{GPT-4 (C)} (concise) prompt also asks for which summary is more concise; we evaluate this prompt because we find that GPT-4 prefers longer, more repetitive summaries than humans do with the \\textbf{GPT-4 (S)} prompt. See Appendix~\\ref{app:prompts} for the complete prompts. We perform three comparisons, using the highest (DPO, temp. 0.25), the lowest (PPO, temp. 1.0), and a \\begin{wraptable}{r}{0.47\\textwidth}\n    \\centering\n    \\small\n    \\vspace{-1.5mm}\n    \\begin{tabular}{lccc}\n    \\toprule\n        & \\textbf{DPO} & \\textbf{SFT} & \\textbf{PPO-1} \\\\\n        \\cmidrule(lr){2-4}\n        N respondents & 272 & 122 & 199 \\\\\n        \\midrule\n        GPT-4 (S) win \\% & 47 & 27 & 13 \\\\\n        GPT-4 (C) win \\% & 54 & 32 & 12 \\\\\n        Human win \\% & 58 & 43 & 17 \\\\\n        \\midrule\n        GPT-4 (S)-H agree & 70 & 77 & 86 \\\\\n        GPT-4 (C)-H agree & 67 & 79 & 85 \\\\\n        H-H agree & 65 & - & 87 \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\vspace{-1mm}\n    \\caption{Comparing human and GPT-4 win rates and per-judgment agreement on TL;DR summarization samples. \\textbf{Humans agree with GPT-4 about as much as they agree with each other.} Each experiment compares a summary from the stated method with a summary from PPO with temperature 0.}\n    \\vspace{-5mm}\n    \\label{tab:human_results}\n\\end{wraptable}middle-performing (SFT, temp. 0.25) method with the aim of covering a diversity of sample qualities; all three methods are compared against greedily-sampled PPO (its best-performing temperature). We find that with both prompts, GPT-4 tends to agree with humans about as often as humans agree with each other, suggesting that GPT-4 is a reasonable proxy for human evaluations (due to limited human raters, we only collect multiple human judgments for the DPO and PPO-1 comparisons). Overall, the \\textbf{GPT-4 (C)} prompt generally provides win rates more representative of humans; we therefore use this prompt for the main results in Section~\\ref{sec:dpo-real-datasets}. For additional details about the human study, including the web interface presented to raters and the list of human volunteers, see Appendix~\\ref{app:human-study}.\n\n\\section{Discussion}\nLearning from preferences is a powerful, scalable framework for training capable, aligned language models. We have introduced DPO, a simple training paradigm for training language models from preferences without reinforcement learning. Rather than coercing the preference learning problem into a standard RL setting in order to use off-the-shelf RL algorithms, DPO identifies a mapping between language model policies and reward functions that enables training a language model to satisfy human preferences \\textit{directly}, with a simple cross-entropy loss, without reinforcement learning or loss of generality. With virtually no tuning of hyperparameters, DPO performs similarly or better than existing RLHF algorithms, including those based on PPO; DPO thus meaningfully reduces the barrier to training more language models from human preferences.\n\n\\textbf{Limitations \\& Future Work.} Our results raise several important questions for future work. How does the DPO policy generalize out of distribution, compared with learning from an explicit reward function? Our initial results suggest that DPO policies can generalize similarly to PPO-based models, but more comprehensive study is needed. For example, can training with self-labeling from the DPO policy similarly make effective use of unlabeled prompts? On another front, how does reward over-optimization manifest in the direct preference optimization setting, and is the slight decrease in performance in Figure~\\ref{fig:dialogue-main}-right an instance of it? Additionally, while we evaluate models up to 6B parameters, exploration of scaling DPO to state-of-the-art models orders of magnitude larger is an exciting direction for future work. Regarding evaluations, we find that the win rates computed by GPT-4 are impacted by the prompt; future work may study the best way to elicit high-quality judgments from automated systems. Finally, many possible applications of DPO exist beyond training language models from human preferences, including training generative models in other modalities.\n\n\\section*{Acknowledgements}\nEM gratefully acknowledges funding from a Knight-Hennessy Graduate Fellowship. CF and CM are CIFAR Fellows. This work was supported in part by the Stanford Accelerator for Learning (SAL) and Stanford Institute for Human-Centered Artificial Intelligence (HAI) \\textit{Generative AI for the Future of Learning} seed grant program. The Stanford Center for Research on Foundation Models (CRFM) provided part of the compute resources used for the experiments in this work. This work was supported in part by ONR grant N00014-20-1-2675.\n\n\\newpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{KTO: Model Alignment as Prospect Theoretic Optimization}\n\n\\begin{document}\n\n\\twocolumn[\n\\icmltitle{KTO: Model Alignment as Prospect Theoretic Optimization}\n\n\\icmlsetsymbol{equal}{*}\n\n\\begin{icmlauthorlist}\n\\icmlauthor{Kawin Ethayarajh}{stanford}\n\\icmlauthor{Winnie Xu}{contextual}\n\\icmlauthor{Niklas Muennighoff}{contextual}\n\\icmlauthor{Dan Jurafsky}{stanford}\n\\icmlauthor{Douwe Kiela}{stanford,contextual}\n\\end{icmlauthorlist}\n\n\\icmlaffiliation{stanford}{Stanford University (first author was an intern at Contextual AI)}\n\\icmlaffiliation{contextual}{Contextual AI}\n\n\\icmlcorrespondingauthor{Kawin Ethayarajh}{kawin@stanford.edu}\n\n\\icmlkeywords{Machine Learning, ICML}\n\n\\vskip 0.3in\n]\n\n\\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution\n\n\\begin{abstract}\nKahneman \\& Tversky's \\emph{prospect theory} tells us that humans perceive random variables in a biased but well-defined manner \\citeyearpar{tversky1992advances}; for example, humans are famously loss-averse.\nWe show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases---the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call \\emph{human-aware losses} (HALOs).\nHowever, the utility functions these methods attribute to humans still differ from those in the prospect theory literature.\nUsing a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. \nWe call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable.\nMore broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration.\n\\end{abstract}\n\n\\section{Introduction}\n\nAligning generative models with human feedback has been successfully used to make generations more helpful, factual, and ethical, among other desiderata \\citep{ouyang2022training,tian2023fine}.\nFor LLMs, alignment methods such as RLHF and DPO have consistently proven to be more beneficial than doing supervised finetuning (SFT) alone. \nHowever, human feedback is often discussed only in the context of preferences (e.g., output $y_w \\succ y_l$ for input $x$), even though it can take many forms (e.g., approval/disapproval of $y$ given $x$).\nThis is because preferences, despite being a kind of data that is relatively scarce and expensive to collect in practice \\citep{casper2023open}, are required by the alignment methods shown to work best---RLHF~\\citep{christiano2017deep} and DPO~\\citep{rafailov2023direct}.\n\nTo understand why these methods work so well, and whether feedback needs to be in preference form, we frame alignment through the lens of \\textit{prospect theory}~\\cite{kahneman1979prospect,tversky1992advances}.\nProspect theory explains why humans make decisions about uncertain events that do not maximize their expected value.\nIt formalizes how humans perceive random variables in a biased but well-defined manner; for example, relative to some reference point, humans are more sensitive to losses than gains, a property called \\textit{loss aversion}.\nWe show that popular alignment methods such as DPO and PPO-Clip \\citep{schulman2017proximal} implicitly model some of these biases, helping explain their success independently of the data used (\\S\\ref{ssec:halos}).\nWe then propose a more general class of such loss functions called \\textit{human-aware losses} (HALOs).\\footnote{We use the term \\emph{human-aware} to draw an analogy with how \\emph{hardware-aware} methods benefit from being designed around hardware limitations \\citep{dao2022flashattention}, not to claim that a simple loss function is fully aware of human behavior.}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.9\\columnwidth]{figures/utility}\n    \\vspace{-5pt}\n    \\caption{The utility that a human gets from the outcome of a random variable, as implied by different human-aware losses (HALOs).\n    Notice that the implied value functions share properties such as loss aversion with the canonical human value function in prospect theory \\citep{tversky1992advances}.\n     }\n    \\label{fig:utility}\n\\end{figure}\n\nAlthough it is impossible to say that HALOs are categorically better than non-HALOs, we find that among existing methods, those that meet the definition of a HALO work better than those that do not (\\S\\ref{ssec:do_halos_matter}).\nWe find that DPO performance can even be matched at most scales by running an offline PPO variant on dummy +1/-1 rewards, suggesting that preference data might not be needed if the inductive bias in the loss function is good enough.\nHowever, despite the surprising success of this simple baseline, it significantly lags behind DPO at the 30B LLM scale and suffers from hyperparameter sensitivity, making it difficult to use.\n\nTaking a more principled approach, we derive a HALO using the model of human utility that Kahneman \\& Tversky proposed to describe how humans make decisions about uncertain monetary outcomes \\citep{tversky1992advances}.\nThis approach, which we call Kahneman-Tversky Optimization (KTO), directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as most current methods do (\\S\\ref{ssec:kto}).\nKTO only requires a binary signal of whether an output is desirable or undesirable for an input.\nThis data is more abundant, cheaper, and faster to collect in the real world,  making it easier to scale alignment in production and rapidly iterate on models.\nWe find that:\n\\begin{itemize}\n    \\item KTO matches or exceeds DPO performance at  scales from 1B to 30B parameters (\\S\\ref{ssec:experiments}).\\footnote{Our code is available on \\href{https://github.com/ContextualAI/HALOs}{Github}; models are on \\href{https://huggingface.co/collections/ContextualAI/archangel-65bd45029fa020161b052430}{Huggingface}.}\n    That is, breaking up $n$ preferences meant for DPO into $2n$ examples for KTO can yield better LLM generations, as determined by closed-ended tasks such as mathematical reasoning and open-ended judgments from humans and \\texttt{GPT-4}. \n    \n    \\item KTO can handle extreme data imbalances, matching DPO performance while using up to 90\\% fewer desirable examples (i.e., examples of good generations).\n    Its success cannot be ascribed to the alignment data being sourced from a preference dataset.\n\n    \\item When the pretrained model is sufficiently good, one can skip supervised finetuning (SFT) and go straight to KTO without a loss in generation quality, whereas SFT is always needed for best results with DPO.\n\\end{itemize}\nThe intent behind KTO was that even if the model learns from a weaker signal, we could compensate with the higher volume of data that could be accessed in practice; the fact that KTO can match and even outperform DPO on the same data is thus surprising.\nWe conclude by discussing some theoretical explanations for this phenomenon (\\S\\ref{ssec:theory}).\nDespite the success of KTO in our experiments, our work ultimately suggests that there is no one HALO that is universally superior; the best HALO depends on the inductive biases appropriate for a given setting, and this choice should be made deliberately instead of defaulting to any one loss.\n\n\\section{Background}\n\\label{sec:background}\nFor a detailed survey, please see Appendix \\ref{sec:appendix_related_work}.\nIn brief, LLMs are traditionally trained in three stages \\citep{ouyang2022training}:\n\n\\paragraph{Pretraining} Given a large corpus, train the model to maximize the log-likelihood of the next token conditioned on the preceding text.\nLet $\\pi_0$ denote the pretrained model.\n\n\\paragraph{Supervised Finetuning (SFT)} Finetune the model to predict the next token on data that is more relevant to the downstream task.\nOften, such data will comprise instructions and an appropriate response (i.e., instruction finetuning).\nLet $\\pi_\\text{ref}$ denote the finetuned model.\n\n\\paragraph{RLHF} Given a dataset $\\mathcal{D}$ of preferences $(x, y_w, y_l)$---where $x$ is an input, $y_w, y_l$ are the preferred and dispreferred outputs (i.e., $y_w \\succ y_l$ for $x$), and $r^*$ is the “true” reward function underlying the preferences---it is first assumed that the probability that $y_w$ is preferred to $y_l$ can be captured with a specific function class, typically a Bradley-Terry model \\citep{bradley1952rank}. Where $\\sigma$ is the logistic function:\n\\begin{equation}\np^*(y_w \\succ y_l|x) = \\sigma(r^*(x,y_w) - r^*(x,y_l))\n\\label{eq:bradley}\n\\end{equation}\nSince getting the true reward from a human would be intractably expensive, a reward model $r_\\phi$ learns to serve as a proxy, done by minimizing the negative log-likelihood of the human preference data:\n$$\\mathcal{L}_R(r_\\phi) = \\mathbb{E}_{x,y_w,y_l \\sim D}[- \\log \\sigma(r_\\phi(x, y_w) - r_\\phi(x, y_l))]$$\n\nBut solely maximizing the reward might come at the expense of desiderata such as generating grammatical text. \nTo avoid this, a KL divergence penalty is introduced to restrict how far the language model can drift from $\\pi_\\text{ref}$. \nWhere $\\pi_\\theta$ is the model we are optimizing, the optimal model $\\pi^*$ is that which maximizes\n\\begin{equation}\n\\begin{split}\n    \\mathbb{E}_{x \\in D, y \\in \\pi_\\theta} [r_\\phi(x,y)] \\\n    & - \\beta D_{\\text{KL}}(\\pi_\\theta(y|x) \\| \\pi_{\\text{ref}}(y|x)) \n    \\label{eq:rlhf_obj}\n\\end{split}\n\\end{equation}\nwhere $\\beta > 0$ is a hyperparameter. \nSince this objective is not differentiable, we need to use an RL algorithm like PPO \\citep{schulman2017proximal}.\n\nHowever, RLHF is often slow (largely because of having to sample generations) and quite unstable in practice (especially in a distributed setting).\nFor this reason, recent work has focused on designing closed-form losses that maximize the margin between the preferred and dispreferred generations.\nIn particular, Direct Preference Optimization (DPO) \\citep{rafailov2023direct} has emerged as a popular alternative as it allows the same optimal policy as in RLHF to be recovered under certain conditions:\n\\begin{equation}\n\\begin{split}\n    & \\mathcal{L}_\\text{DPO}(\\pi_\\theta, \\pi_\\text{ref}) = \\mathbb{E}_{x,y_w,y_l \\sim D} \\\\ & \\left[  -\\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_\\text{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_\\text{ref}(y_l|x)} \\right) \\right]\n\\end{split}\n\\label{eq:dpo}\n\\end{equation}\n\n\\section{A Prospect Theoretic View of Alignment}\n\nTo understand why alignment methods work so well, we now frame them through the lens of \\textit{prospect theory} \\citep{tversky1992advances}.\nProspect theory explains why, when faced with an uncertain event, humans make decisions that do not maximize their expected value.\nFor example, because humans are loss-averse, given a gamble that returns \\$100 with 80\\% probability and \\$60 with 20\\% probability, a person might accept \\$60 to avoid the gamble, despite their \\textit{certainty equivalent} of \\$60 being less than the expected value of \\$80.\n\n\\subsection{Prospect Theory}\n\nIn prospect theory, human utility depends on a \\textit{value function} and a \\textit{weighting function}:\\footnote{\\textit{Cumulative prospect theory} is the full name of the expanded theory we dicuss here \\citep{tversky1992advances}.}\n\n\\begin{definition}\nA \\textit{value function} $v: \\mathcal{Z} \\to \\mathbb{R}$ maps an outcome $z$, relative to some reference point $z_0$, to its perceived (or subjective) value.\nFor example, these functions capture the fact that humans tend to be more sensitive to relative losses than relative gains of the same magnitude.\n\\end{definition}\n\n\\begin{definition}\nA \\textit{weighting function} $\\omega$ is the derivative of a \\textit{capacity function} that maps cumulative probabilities to perceived cumulative probabilities.\nThese functions capture, for example, the fact that humans tend to overestimate the chance of rare events.\nLet $\\omega_z$ denote the weight placed on outcome $z$.\n\\end{definition}\n\n\\begin{definition}\nThe \\textit{utility of a random variable} $Z$ is a function of its outcomes: $u(Z) \\triangleq \\sum_{z \\in Z} \\omega_z v(z - z_0)$.\n\\end{definition}\n\nHowever, because humans do not see the full probability distribution of an LLM, weighting functions are not salient to this discussion; we will focus only on value functions.\nUsing experiments that presented real humans with monetary gambles and asked for their certainty equivalent, \\citet{tversky1992advances} proposed the following functional form for human value:\n\\begin{equation}\n    v(z; \\lambda, \\alpha,  z_0) = \\begin{cases}\n(z - z_0)^\\alpha & \\text{if } z \\geq z_0\\\\\n-\\lambda(z_0 - z)^\\alpha & \\text{if } z < z_0 \\\\\n    \\end{cases}\n    \\label{eq:human_value}\n\\end{equation}\nwhere the median value of hyperparameter $\\alpha = 0.88$ and $\\lambda = 2.25$ across individuals.\n$\\alpha$ controls the curvature of the function, which reflects risk aversion; $\\lambda$ controls its steepness, which reflects loss aversion.\nWhile the shape of the median Kahneman-Tversky value function is illustrated in Figure \\ref{fig:utility}, it should be noted that it varies across individuals \\citep{tversky1992advances}.\nThere are also other functional forms for the value function that have been proposed in later work \\citep{gurevich2009decision}. \nThe salient qualities of a value function are: the existence of a reference point that is used to get the \\emph{relative} gain or loss; concavity in relative gains (i.e., diminishing sensitivity away from $z_0$); and loss aversion (i.e., greater sensitivity to losses).\n\n\\subsection{HALOs}\n\\label{ssec:halos}\n\n\\begin{definition}[\\textbf{HALOs}]\n    Let $\\theta$ denote the trainable parameters of the model $\\pi_\\theta: \\mathcal{X} \\to \\mathcal{P}(\\mathcal{Y})$ being aligned, $\\pi_\\text{ref}$ the reference model, $l: \\mathcal{Y} \\to \\mathbbm{R}^+$ a normalizing factor, and $r_\\theta(x,y) = {l(y)} \\log [\\pi_\\theta(y|x) / \\pi_\\text{ref}(y|x)]$ the implied reward.\n    Where $Q(Y'|x)$ is a reference point distribution over $\\mathcal{Y}$  and $v: \\mathbbm{R} \\to \\mathbbm{R}$ is non-decreasing everywhere and concave in $(0, \\infty)$, the \\textit{human value} of $(x,y)$ is \n    \\begin{equation}\n        v(r_\\theta(x,y) - \\mathbb{E}_{Q}[r_\\theta(x,y')])\n    \\end{equation}\n    A function $f$ is a \\textit{human-aware loss} for $v$ if $\\exists\\ a_{x,y} \\in \\{-1, +1\\}$ such that:\n    \\begin{equation}\n    \\begin{split}\n        f(& \\pi_\\theta, \\pi_\\text{ref}) = \\\\  & \\mathbb{E}_{x,y\\sim\\mathcal{D}}[ a_{x,y} v(r_\\theta(x,y) - \\mathbb{E}_{Q}[r_\\theta(x, y')])] + C_\\mathcal{D}\n    \\end{split}\n    \\end{equation}\n    where $\\mathcal{D}$ is the feedback data and $C_\\mathcal{D} \\in \\mathbb{R}$ is a data-specific constant. \n\\end{definition}\nIn a classic prospect theory experiment, $r_\\theta$ would be the dollar amount assigned to each outcome; here, $r_\\theta$ is measured in nats, as the decrease in conditional surprisal when going from $\\pi_\\text{ref}$ to $\\pi_\\theta$, normalized according to $l$.\nThis follows naturally from the next-token prediction objective used to pretrain and finetune LLMs.\nAs $\\pi_\\theta$ is aligned, we would expect $r_\\theta$ to grow increasingly positive for desirable outputs and increasingly negative for undesirable outputs.\n\nAnother perspective on the reward comes from the RLHF objective in (\\ref{eq:rlhf_obj}).\nThe policy that maximizes this objective has a closed-form expression \\citep{peng2019advantage,peters2007reinforcement}; where $Z(x)$ is the partition function:\n$$\\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y|x) \\exp\\left(\\frac{1}{\\beta} r^*(x,y)\\right)$$\nLetting $l(\\cdot) = \\beta$, we get\n\\begin{equation}\n    r_{\\theta^*}(x, y) = r^*(x,y) - \\beta \\log Z(x)\n    \\label{eq:opt_reward}\n\\end{equation}\nUnder $\\theta^*$, the HALO-defined reward is just the optimal reward shifted by an input-specific term, meaning that $r_{\\theta^*}$ is in the same equivalence class as $r^*$ and would also induce the optimal policy $\\pi^*$ (Lemma 1, \\citet{rafailov2023direct}).\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/halos_vs_nonhalos}\n    \\vspace{-15pt}\n    \\caption{HALOs (DPO, offline PPO variant) outperform non-HALOs (SLiC, CSFT), as measured by the \\texttt{GPT-4-0613}-judged winrate of the aligned model's generations against a hard-to-beat baseline: the outputs that would have been used as the targets for SFT.\n    The $y$-axis here plots the winrate above chance (i.e., the winrate -- 50\\%).\n    The difference between methods is only significant $(p < 0.05)$ at 13B+ parameters, and only the HALO-aligned Llama-\\{13B, 30B\\} models are able to match the baseline and yield a winrate at or above chance.}\n    \\label{fig:halos_vs_nonhalos}\n\\end{figure*}\n\nThe reference point in a HALO is the expected reward from the human's perspective, where $Q(Y'|x)$ describes the examples that are used to construct a baseline.\nWe require that the value function be concave in gains but not necessarily convex in losses---unlike the canonical Kahneman-Tversky value function---because a minority of individuals are risk-averse in both the gain and loss regime, resulting in concavity everywhere \\citep{kahneman1979prospect}.\n\n\\begin{theorem}\n\\label{theorem:halos}\n    DPO and PPO-Clip are human-aware losses.\n\\end{theorem}\nThe proof is deferred to Appendix \\ref{sec:appendix_proofs}.\nIn Figure \\ref{fig:utility}, we can see this more intuitively by plotting the value function (i.e., the implied human utility).\n\n\\subsection{Does being a HALO matter?}\n\\label{ssec:do_halos_matter}\n\nIt is difficult to answer whether being a HALO is useful, since both HALOs and non-HALOs are diverse function classes, but we attempt to do so by comparing popular methods that qualify as a HALO with those that do not:\n\\begin{enumerate}\n    \\item \\textbf{CSFT}: Conditional SFT is a simple alignment method where a control token is prepended to the output during training; then, at inference, the control token corresponding to desirable generations is appended to the input to induce good generations \\citep{korbak2023pretraining}.\n    This is not a HALO, since that would demand that $- \\log \\pi_\\text{ref}(y|x)$ always equal the reference point; however, since the reference point is fixed for a given $x$, this is not possible when $\\pi_\\text{ref}$ is a non-uniform distribution.\n\n    \\item \\textbf{SLiC}: Sequence Likelihood Calibration \\citep{zhao2023slic}, which combines a max-margin loss for preferences with a language modeling loss, is not a HALO (for the same reasons given above for CSFT):\n    \\begin{equation*}\n    \\begin{split} \n        & \\mathcal{L}_\\text{cal}(\\pi_\\theta) = \\mathbb{E}_{x,y_w,y_l \\sim D}\\left[ \\max \\left( 0, \\delta - \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_\\theta(y_l|x)} \\right) \\right] \\\\\n        & \\mathcal{L}_\\text{reg}(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathbb{E}_{x \\sim D, y \\sim \\pi_\\text{ref}(x)}[- \\log \\pi_\\theta(y|x)] \\\\\n        & \\mathcal{L}_\\text{SLiC}(\\pi_\\theta, \\pi_{\\text{ref}}) = \\mathcal{L}_\\text{cal}(\\pi_\\theta) + \\lambda_\\text{reg} L_\\text{reg}(\\pi_\\theta, \\pi_{\\text{ref}}) \\\\\n    \\end{split}\n    \\end{equation*}\n\n    \\item \\textbf{DPO}: DPO is a HALO (Theorem \\ref{theorem:halos}).\n\n    \\item \\textbf{PPO (offline)}: The standard RLHF objective in (\\ref{eq:rlhf_obj}) is typically optimized with PPO-Clip, which works by “clipping” how far $\\pi_\\theta$ can drift from the version $\\pi_\\text{old}$ at the previous step:\n    \\begin{equation*}\n    \\begin{split}\n        \\mathcal{L}_\\text{PPO (offline)} =& -\\mathbb{E}_{x,y,t \\sim D}[\\min(q_\\theta A(x{:}y_{<t}, y_t), \\\\\n        &\\text{clip}(q_\\theta, 1 - \\epsilon, 1 + \\epsilon) A(x{:}y_{<t}, y_t))]\n    \\end{split}\n    \\end{equation*}\n    where $q_\\theta = \\frac{\\pi_\\theta (y_t|x{:}y_{<t})}{\\pi_\\text{old}(y_t|x{:}y_{<t})}$ and $A(x{:}y_{<t}, y_t)$ is the per-token advantage (i.e., the surplus benefit from producing a given token in a given state).\n    \n    PPO is an online algorithm---generations are sampled from the current model, judged by a reward model, and then used to update the current version. \n    However, for a fair comparison with offline methods, we consider a variant of PPO that is only fed offline data.\n    Because RLHF is also quite unstable in a distributed setting, we never update $\\pi_\\text{old}$ and keep it as $\\pi_\\text{ref}$, instead clipping less conservatively than we traditionally would.\n    \\citet{baheti2023improving} found that these changes, along with treating the entire output sequence as a single action, greatly improves stability. \n    However, since RLHF traditionally uses token-level advantages, we omit the third change and only preserve the first two.\n    The PPO-Clip loss is unchanged and remains a HALO (Theorem \\ref{theorem:halos}).\n    \n    Calling this method PPO is somewhat imprecise, because it is offline and takes only one step, but to avoid introducing too many new terms, we will call this \\textit{PPO (offline)}.\n    Instead of using learned rewards, we simplify even further and use dummy +1/-1 rewards\\footnote{Note that +1/-1 do not refer to the HALO-implied rewards, but rather the rewards used for the advantage $A(x{:}y_{<t}, y_t)$ calculation.} for $y_w$ and $y_l$ instead.\n    Further details on the implementation of this method can be found in Appendix \\ref{sec:appendix_impl}.\n\\end{enumerate}\n\n\\paragraph{Experimental Setup} We compare these baselines on two model families, Pythia-\\{1.4B, 2.8B, 6.9B, 12B\\} \\citep{biderman2023pythia} and Llama-\\{7B, 13B, 30B\\} \\citep{touvron2023llama}.\nThis permits us to see how LLM alignment scales within a model family (Llama-2 lacks a 30B model, hence our use of Llama).\nLater experiments (\\S\\ref{ssec:experiments}) are done on Mistral-7B derivatives \\citep{jiang2023mistral} and Llama-3 \\citep{dubey2024llama}.\nThe models are trained on a combination of Anthropic-HH \\citep{ganguli2022red}, OpenAssistant \\citep{kopf2023openassistant}, and SHP \\citep{pmlr-v162-ethayarajh22a}.\n\nAll models are aligned under identical settings on the same data, save for hyperparameters unique to them.\nSimilar to \\citet{rafailov2023direct}, the target sequences for SFT are a subset of $\\{y_w\\}$.\nWe use \\texttt{GPT-4-0613} to judge whether the aligned model’s response is better than the SFT target for a given test input with respect to helpfulness, harmlessness, and conciseness, a now standard practice \\citep{zheng2023judging,alpaca_eval}.\\footnote{We validate that GPT-4 judgments concur with human judgments in Appendix \\ref{sec:human_eval}.}\nNote that while the SFT target is considered a desirable output for $x$, it is by no means the \\textit{best} output, meaning that an aligned model can certainly achieve a winrate above 50\\%, although this is difficult given that many of the target sequences are human-written.\n\nIn Figure \\ref{fig:halos_vs_nonhalos}, we see the results of this analysis:\n\\begin{itemize}\n    \\item \\textbf{HALOs either match or outperform non-HALOs at every scale}, though the gap is only significant $(p < 0.05)$ at 13B+ model sizes after correcting for multiple comparisons \\citep{holm1979simple}.\n    In fact, only the HALO-aligned Llama-\\{13B, 30B\\} models match or exceed a win rate of 50\\% (i.e., are able to match or exceed the generation quality of the SFT targets in the test data).\n    \n    \\item \\textbf{Up to a scale of 7B parameters, alignment provides virtually no gains over SFT alone.}\n    However, it is worth noting that if the base models were more performant, or if the SFT data distribution were less similar to the preference data, then the gains from the alignment stage would ostensibly be greater.\n\n    \\item \\textbf{Despite only using dummy +1/-1 rewards, our offline PPO variant performs as well as DPO} for all models except Llama-30B.\n    This challenges conventional wisdom, which places heavy emphasis on reward learning \\cite{casper2023open}, and suggests that even the simplest rewards can prove useful when used in a loss function that has the right inductive bias.\n    Despite its success, our offline PPO baseline still suffers from hyperparameter sensitivity and training instability, albeit not to the same extent as traditional RLHF. \n\\end{itemize}\n\n\\section{Kahneman-Tversky Optimization}\n\nThe surprising success of offline PPO with dummy +1/-1 rewards suggests that---with the right inductive biases---a binary signal of good/bad generations may be sufficient to reach DPO-level performance, even if the offline PPO approach itself was unable to do so past a certain scale (\\S\\ref{ssec:do_halos_matter}).\nTaking a more principled approach, we now derive a HALO using the Kahneman-Tversky model of human value, which allows us to directly optimize for utility instead of maximizing the log-likelihood of preferences.\n\n\\begin{table*}[t]\n    \\small \n    \\centering\n    \\caption{\n        Recommended hyperparameter settings for different losses and models when aligned on UltraFeedback, evaluated on the benchmarks discussed in \\S\\ref{ssec:experiments}.\n        The hyperparameter sweeps were done with AdamW, an effective batch size of 32, and $\\lambda_D = \\lambda_U = 1$. \n        Depending on your task and ratio of desirable:undesirable examples, the optimal choice of $\\lambda_D, \\lambda_U$ might be significantly different.\n    }\n    \\begin{tabular*}{\\textwidth}{@{\\extracolsep{\\fill}}llccccc@{}}\n        \\toprule\n        Model & Method & LR & $\\beta$ & AlpacaEval (LC) $\\uparrow$ & BBH $\\uparrow$ & GSM8K (8-shot) $\\uparrow$ \\\\\n        \\midrule\n        Llama-3 8B & SFT+KTO & 5e-6 & 0.05 & 10.59 & 65.15 & 60.20 \\\\\n        Llama-3 8B & KTO & 5e-6 & 0.10 & 11.25 & 65.26 & 57.92 \\\\\n        Qwen2.5 3B Instruct & SFT+KTO & 5e-6 & 0.10 & 13.01 & 32.39 & 61.11 \\\\\n        Qwen2.5 3B Instruct & KTO & 5e-6 & 0.50 & 16.63 & 20.41 & 60.35 \\\\\n        \\bottomrule\n    \\end{tabular*}\n    \\label{tab:hparams}\n\\end{table*}\n\n\\subsection{Derivation}\n\\label{ssec:kto}\n\nThe canonical Kahneman-Tversky value function (\\ref{eq:human_value}) suffers from numerical instability during optimization due to the exponent $a$, so we replace it with the logistic function $\\sigma$, which is also concave in gains and convex in losses.\n\nTo control the degree of risk aversion, we introduce a hyperparameter $\\beta \\in \\mathbbm{R}^+$ as part of the value function.\nThe greater $\\beta$ is, the more quickly the value saturates, meaning the human is simultaneously more risk-averse in gains and more risk-seeking in losses.\nIn practice, this has a similar effect as $\\beta$ in the DPO loss, which controls how far $\\pi_\\theta$ drifts from $\\pi_\\text{ref}$, though we introduce it here explicitly to control risk aversion; in DPO, it carries over from the KL constraint in the RLHF objective (\\ref{eq:rlhf_obj}) and is part of the reward.\n\nWe replace the loss aversion coefficient $\\lambda$ in the original Kahneman-Tversky value function (\\ref{eq:human_value}) with $\\{\\lambda_D, \\lambda_U\\}$, where $\\lambda_D$ and $\\lambda_U$ are hyperparameters for desirable and undesirable outputs respectively; more complex schemes could also be used for importance sampling.\n\nRather than having just one dispreferred generation serve as the reference point $z_0$, as in DPO, we assume that humans judge the quality of $y|x$ in relation to all possible outputs.\nThis implies that $Q(Y'|x)$ is the policy and that the reference point is the KL divergence $\\text{KL}(\\pi_{\\theta}(y'|x)\\|\\pi_\\text{ref}(y'|x))$.\nHowever, as we discuss further below, we end up taking a biased estimate of this term in practice.\n\nWhere $\\lambda_y$ denotes $\\lambda_D(\\lambda_U)$ when $y$ is desirable(undesirable) respectively, the default KTO loss is:\\footnote{An earlier draft separated out $\\lambda_D, \\lambda_U$ into a function $w$. For an easier comparison with (\\ref{eq:human_value}), they---along with $\\beta$---have been moved into the value function itself. Note that $\\lambda_y$ exists solely to make the loss non-negative and can be removed. The use of multiple $x'$ to estimate $z_0$ in practice was included in the loss definition originally, but has now been moved outside for clarity.}\n\\begin{equation}\n    L_\\text{KTO}(\\pi_\\theta, \\pi_\\text{ref}) = \n     \\mathbb{E}_{x,y \\sim D} [ \\lambda_y - v(x, y) ]\n    \\label{eq:kto_loss}\n\\end{equation}\nwhere \n\\begin{equation*}\n\\begin{split}\n    r_\\theta(x, y) &= \\log \\frac{\\pi_\\theta(y|x)}{\\pi_\\text{ref}(y|x)} \\\\\n    z_0 &= \\text{KL}(\\pi_{\\theta}(y'|x)\\|\\pi_\\text{ref}(y'|x)) \\\\\n    v(x, y) &=\n    \\begin{cases}\n    \\lambda_D \\sigma(\\beta(r_\\theta(x,y) - z_0)) \\ \\text{if } y \\sim y_\\text{desirable}|x \\\\\n    \\lambda_U \\sigma(\\beta(z_0 - r_\\theta(x,y))) \\ \\text{if } y \\sim y_\\text{undesirable}|x\\\\\n    \\end{cases} \\\\\n\\end{split}\n\\end{equation*}\nFor more stable training, we do not backpropagate through $z_0$; it exists purely to control the loss saturation.\n\nIntuitively, KTO works as follows: if the model increases the reward of a desirable example in a blunt manner, then the KL penalty also rises and no progress is made.\nThis forces the model to learn exactly what makes an output desirable, so that the reward can be increased while keeping the KL term flat (or even decreasing it).\nThe argument works in the other direction as well, though the non-negativity of the KL term allows faster saturation in the loss regime.\n\n\\paragraph{KL Estimate}\nIn practice, estimating $z_0$ as it is defined above is impractical because sampling from $\\pi_\\theta$ is slow.\nInstead, we take a biased but convenient estimate by shifting outputs in the same microbatch to induce mismatched pairs $\\{ (x_1, y_2), (x_2, y_3), ..., (x_m, y_0) \\}$, then estimating a shared reference point $z_0$ for all examples in the same microbatch as follows.\nWhere $j = (i+1) \\text{ mod } m$,\n$$\\hat{z}_\\text{0} = \\max\\left(0, \\frac{1}{m} \\sum_{1 \\leq i < m} \\log \\frac{ \\pi_\\theta(y_{j}|x_i)}{\\pi_\\text{ref}(y_{j}|x_i)}\\right)$$\n\nBecause of clamping, our estimator has a positive bias but lower variance than the standard unbiased estimator.\nAlthough it costs an additional forward pass, we use a mismatched output $y_j$ instead of the corresponding $y_i$ because the latter have often been deliberately chosen to be canonically good or bad outputs, and thus have unrepresentative high-magnitude rewards.\nIt is worth noting that although our estimator is biased, so would the human-perceived reference point, since humans do not perceive the full distribution induced by $\\pi_\\theta$ and would employ an “availability heuristic” that would overweight outputs for which they have recently given feedback, regardless of whether those outputs are a good continuation of $x$ \\citep{tversky1973availability}.\n\nIf KTO is preceded by SFT done on the same data that is used as desirable feedback and the SFT model is used as $\\pi_\\text{ref}$, then the KL estimate will quickly approach zero.\nHaving already learned what is desirable during SFT, the policy will tend to scatter the mass placed on undesirable examples, leading to minimal divergence.\nAlso, because the policy may learn to place less mass on undesirable $y_i$ regardless of whether it is preceded by $x_i$, $\\hat{z}_0$ might actually be an under-estimate.\nIn such cases, one can avoid the extra computation and set $\\hat{z}_0 = 0$.\nHowever, when KTO is not preceded by SFT, or when the SFT data is not a subset of the KTO data, estimating $\\hat{z}_0$ is necessary.\n\n\\paragraph{Data} \nIf the alignment data is naturally binary, every positive example can be assumed to be drawn from $y_\\text{desirable}|x$ and every negative example from $y_\\text{undesirable}|x$.\nHowever, the canonical feedback datasets in academic research (HH, SHP, OASST) are in preference format, since the methods that have worked best up until now are preference-based.\nIn our experiments, we convert preference data $y_w \\succ y_l$ by assuming that $y_w$ is drawn from the desirable distribution and $y_l$ from the undesirable one.\nThis is a naive assumption, made for the sake of simplicity, and a more complex deconstruction of preferences into binary feedback would likely yield better results, which we leave for future work.\nTo show that KTO can be used with non-preference data, we also subsample exactly one $y$ per $x$ for some experiments (denoted one-$y$-per-$x$), removing any trace of paired preferences at the cost of reducing the data volume.\n\nIf human feedback is in the form of scores or ratings, the simplest means of incorporating it into KTO is to construct a weighting function such that high-magnitude data is weighed more and that examples with scores above(below) some threshold are desirable(undesirable).\nIt is also possible to construct score-based HALOs from first principles, but we leave the design of such losses to future work. \n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/dpo_vs_kto}\n    \\vspace{-15pt}\n    \\caption{KTO is as good or better than DPO at all scales,  as measured by the \\texttt{GPT-4-0613}-judged winrate of the aligned model's generations against the outputs that would have been used for SFT. \n    In fact, for the Llama models, KTO alone matches the performance of SFT+DPO and is significantly better than DPO alone.\n    Error bars denote a 90\\% binomial confidence interval.}\n    \\label{fig:dpo_vs_kto}\n\\end{figure*}\n\n\\subsection{Hyperparameters} \n\nIn Table \\ref{tab:hparams}, we provide recommended hyperparameter settings for Llama-3 8B \\citep{dubey2024llama} and Qwen2.5 3B Instruct \\citep{yang2024qwen2} based on benchmarks such as MMLU (0-shot) \\citep{hendryckstest2021}, GSM8K (8-shot, chain-of-thought) \\citep{cobbe2021gsm8k}, HumanEval (0-shot) \\citep{chen2021evaluating}, and BigBench-Hard (3-shot chain-of-thought) \\citep{srivastava2022beyond}.\n\n\\paragraph{Learning Rate} \nWe find that that the performance of an aligned model is more sensitive to the learning rate than any other hyperparameter.\nThe optimal learning rate for KTO is usually 2x to 10x the optimal learning rate for DPO; since the reference-adjusted reward tends to be much smaller in magnitude for KTO, one needs to use a more aggressive learning rate to compensate.\nFor example, the default learning rate for DPO is 5e-7 \\citep{rafailov2023direct}, but we find that a default of 5e-6 works better for KTO.\nIn our experiments, we use the default DPO learning rate with RMSProp for all methods to ensure an apples-to-apples comparison with \\citet{rafailov2023direct}, but when using KTO in practice, we recommend starting at 5e-6 with AdamW and adjusting the learning rate as needed.\n\n\\paragraph{Batch Size} KTO needs a microbatch size $\\geq$ 2 to estimate the reference point in a single step.\nThe experiments in this paper all use an effective batch size of 32, and in general we recommend using a batch size between 8 and 128.\n\n\\paragraph{Risk Aversion} \nThe degree of risk aversion/seeking is controlled by $\\beta$; the greater $\\beta$ is, the greater the risk aversion in gains and risk seeking in losses.\nIn practice, lower values of $\\beta$ in the range [0.01, 0.10] work better for larger models that have already undergone SFT; higher values of $\\beta$ in the range [0.10, 1.00] work better for smaller models undergoing KTO directly, without SFT prior.\n\n\\paragraph{Loss Aversion} The default weighting function controls the degree of loss aversion with $\\lambda_D, \\lambda_U$, which are both set to 1 by default.\nIn general, where $n_D$ and $n_U$ refer to the number of desirable and undesirable examples respectively, we find that it is generally best to set $\\lambda_D, \\lambda_U$ such that \n\\begin{equation}\n    \\frac{\\lambda_D n_D}{\\lambda_U n_U} \\in \\left[ 1, \\frac{3}{2} \\right]\n    \\label{eq:weights}\n\\end{equation}\nFor example, if there were a 1:10 ratio of desirable to undesirable examples, we would set $\\lambda_U = 1, \\lambda_D \\in [10, 15]$.\nThis interval was determined empirically, and implies that---after adjusting for class imbalances---gain sensitivity yields better performance than loss sensitivity, ostensibly because producing good outputs is more important than avoiding bad outputs for success on most benchmarks.\nThis is not a hard rule, however.\nIn tasks where minimizing the downside is more important, like toxicity prevention, setting $\\lambda_D, \\lambda_U$ such that\n$\\lambda_D n_D < \\lambda_U n_U$ may work better.\nUnless otherwise stated, we use $\\lambda_D = \\lambda_U = 1$ in our experiments.\n\nThe current configuration permits different sensitivities to desirable and undesirable examples based on the premise that upon convergence, all undesirable outputs will yield negative rewards and all desirable outputs will yield positive ones, in which case $\\lambda_U, \\lambda_D$ would directly correspond to $\\lambda$ in the original Kahneman-Tversky value function.\nHowever, during training itself, a desirable output may have a negative reward (and vice-versa), yet $\\lambda_D$ would be the same regardless of whether $r_\\theta(x,y) - z_0$ were positive or negative.\nWe may want to change this so that not only is asymmetry possible upon convergence, but even during training within the same class of output: in this case, using a higher $\\lambda_D$ when $r_\\theta(x,y) - z_0$ is negative than when it is positive.\nWe leave the design of dynamic hyperparameter selection schemes as directions for future work.\n\n\\begin{table}[t]\n\\centering\n\\caption{(top) Results from aligning Zephyr-$\\beta$-SFT \\citep{tunstall2023zephyr} on UltraFeedback for exactly 1 epoch.\nEven when only one of the two outputs in each preference is seen by KTO, it still outperforms DPO, despite this reducing the volume of data by half (one-$y$-per-$x$).\n(middle) Changing the structure of the KTO loss, even in subtle ways, makes the aligned model worse, supporting our design choices. (bottom) Fixing $\\lambda_U = 1$, we try different levels of loss and risk aversion by changing $\\lambda_D$ and $\\beta$ respectively (see Appendix \\ref{sec:appendix_impl} for more results). \n}\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{l|llll}\n\\toprule\nDataset ($\\rightarrow$) & MMLU & GSM8k & HumanEval & BBH \\\\\nMetric ($\\rightarrow$)  & EM & EM & pass@1 & EM \\\\\n\\midrule\nSFT & 57.2 & 39.0 & 30.1 & 46.3 \\\\ \nDPO & 58.2 & 40.0 & 30.1 & 44.1 \\\\\nORPO ($\\lambda = 0.1$) & 57.1 & 36.5 & 29.5 & 47.5 \\\\\nKTO ($\\beta = 0.1$, $\\lambda_D = 1$) & \\textbf{58.6} & \\textbf{53.5} & \\textbf{30.9} & \\textbf{52.6}  \\\\\nKTO (one-$y$-per-$x$) & 58.0 & 50.0 & 30.7 & 49.9 \\\\\n\\midrule\nKTO (no $z_0$) & 58.5 & 49.5 & 30.7 & 49.0 \\\\\nKTO (concave, $v = \\log \\sigma$) & 58.3 & 42.5 & 30.6 & 43.2 \\\\\nKTO (risk-neutral, $v(\\cdot) = \\cdot$) & 57.3 & 42.0 & 28.8 & 6.1 \\\\\nKTO (no $\\pi_\\text{ref}$, $\\lambda_D = 1.75$) & 57.5 & 47.5 & 29.5 & 51.6  \\\\\n\\midrule\nKTO ($\\beta = 0.01$) & 57.3 & 54.0 & 33.2 & 49.8 \\\\\nKTO ($\\beta = 0.50$) & 58.3 & 45.0 & 32.7 & 51.8 \\\\\nKTO ($\\lambda_D = 0.50$) & 57.4 & 47.0 & 32.2 & 30.5 \\\\\nKTO ($\\lambda_D = 0.75$) & 58.3 & 52.0  & 30.3 & 50.6 \\\\\nKTO ($\\lambda_D = 1.33$) & 58.5 & 52.0 & 32.8 & 45.4 \\\\\nKTO ($\\lambda_D = 2.00$) & 58.1 & 43.0 & 32.1 & 45.3 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\label{tab:zephyr}\n\\end{table}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{figures/lengths}\n    \\vspace{-15pt}\n    \\caption{Without doing SFT first, DPO-aligned models tend to ramble and hallucinate entire conversations.\n    KTO does not suffer from this issue.\n    }\n    \\label{fig:lengths}\n\\end{figure}\n\n\\subsection{Experiments}\n\\label{ssec:experiments}\n\n\\paragraph{KTO $\\geq$ DPO} As seen in Figure \\ref{fig:dpo_vs_kto}, when rerunning the winrate evaluation in \\S\\ref{ssec:do_halos_matter}, SFT+KTO is competitive with SFT+DPO at scales from 1B to 30B, despite the model learning from a weaker signal.\nKTO alone is better than DPO alone for the Llama-\\{7B, 13B, 30B\\} models, and this gap is significant ($p < 0.01$) at 7B and 30B even after correcting for multiple comparisons \\citep{holm1979simple}.\nAmong the Pythia models, there is no significant difference between the two, suggesting that a minimum model capacity is needed for these differences to emerge.\nKTO also fares better than DPO and other baselines on generative benchmarks (Table \\ref{tab:zephyr}).\nThis is most pronounced for certain tasks: on GSM8K, a mathematical reasoning dataset, just swapping DPO for KTO when aligning Zephyr-$\\beta$-SFT \\cite{tunstall2023zephyr} on UltraFeedback \\citep{cui2023ultrafeedback} improves performance by 13.5 points.\n\n\\paragraph{At sufficient scale, KTO does not need SFT.}\nA KTO-aligned Llama-\\{13B, 30B\\} model is competitive with its SFT+KTO counterpart despite not undergoing SFT first, and is the only alignment method of the ones we tested to show this behavior.\nThis is perhaps due to KTO alone keeping the average response length roughly the same, while running DPO without SFT  prior causes the response length to increase dramatically (Figure \\ref{fig:lengths}).\n\n\\paragraph{KTO data \\underline{need not} come from preferences.}\nMight KTO be secretly benefiting from its $2n$ examples in the previous experiment coming from $n$ preference pairs instead of a naturally unpaired data distribution?\nTo test this, we randomly discard increasingly large fractions of the desirable data before KTO-aligning a Llama-7B model.\nFor example, if we discard 90\\% of the desirable data while leaving the undesirable data untouched, then the ratio of desirable:undesirable examples goes from 1:1 to 1:10 and the vast majority of undesirable examples no longer have a preferred counterpart.\nWe handle such imbalances by changing $\\lambda_D, \\lambda_U$ to satisfy the criteria in (\\ref{eq:weights}); when we drop 90\\% of the desirable data, we set $\\lambda_u = 1, \\lambda_D = 13.33$ for example.\nFor Llama-7B, we find that up to 90\\% of the desirable data can in fact be discarded while still outperforming DPO (Figure \\ref{fig:fracdata}).\n\nWe further verify this claim by aligning Mistral-7B on OpenAssistant using DPO (on $n$ pairs), standard KTO (on all $2n$ outputs), and KTO where only one $y$ per $x$ is used.\nSince the output of one $y$ in OpenAssistant is not conditioned on the other $y$ for the same $x$, the latter captures the setting where the data is from an inherently unpaired distribution.\nDespite the one-$y$-per-$x$ setup decreasing the amount of training data by 72\\%, the KTO-aligned model still outperforms both its DPO counterpart and the official instruction-tuned Mistral-7B \\citep{jiang2023mistral}, as seen in Table \\ref{tab:mistral}.\n\n\\paragraph{Changing the design of KTO makes it significantly worse.}\nFor one, removing the reference point $z_0$---which is necessary for KTO to qualify as a HALO---causes a 3.6 and 4.0 point drop on BBH and GSM8K respectively (Table \\ref{tab:zephyr}, middle).\nEven changes that allow KTO to remain a HALO are typically suboptimal.\nFor example, removing the symmetry of the value function---going from $1 - \\sigma(\\cdot)$ to $- \\log \\sigma(\\cdot)$ (i.e., making the value function concave everywhere, as in DPO)---causes a 9.4 and 11.0 point drop on BBH and GSM8K respectively.\nMaking the value function risk-neutral by setting it to the identity function leads to a total collapse in BBH performance.\nChanging the curvature and slope via the risk and loss aversion hyperparameters can, depending on the task, improve or degrade performance (Table \\ref{tab:zephyr}, bottom).\n\n\\paragraph{KTO works without a reference model or SFT, but not as well as standard KTO.}\nIf we can avoid storing the reference model in memory, then we can do much more memory-efficient alignment.\nThe naive way to do this is to assume that $\\pi_\\text{ref}$ returns a uniform distribution over outputs for all $x$, which simplifies $r_\\theta - z_0$ to $\\log \\pi_\\theta(y|x) - H(\\pi_\\theta(y'|x))$, where $H$ denotes the entropy.\nAs seen in Table \\ref{tab:zephyr} (middle), if we set $\\lambda_D = 1.75$, this memory-efficient variant of KTO is better than DPO on some tasks and worse on others, though it still trails standard KTO.\nWe find that it is also more sensitive to the loss aversion hyperparameters; setting $\\lambda_D \\in \\{1.5, 2.0\\}$ reduces performance on GSM8K and BBH by several points.\nStill, it strictly outperforms ORPO \\citep{hong2024reference}, a recently-proposed reference-free method, while also using less memory than all existing approaches, since $\\pi_\\text{ref}$ need never be loaded into memory and a batch of $m$ KTO outputs is smaller than a batch of $m$ preferences (i.e., $2m$ outputs) used for ORPO/DPO.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1.02\n    \\columnwidth]{figures/full_comp}\n    \\vspace{-15pt}\n    \\caption{A KTO-aligned Llama-7B model can match or exceed the performance of its DPO-aligned counterpart while aligned on a smaller and highly imbalanced version of the same dataset, even with as few as 0.1 positive/desirable examples for every negative/undesirable one.\n    }\n    \\label{fig:fracdata}\n\\end{figure}\n\n\\subsection{Theoretical Analysis}\n\\label{ssec:theory}\nKTO was designed with the motivation that even if binary feedback were weaker, one could compensate with sheer volume, as such data is much more abundant, cheaper, and faster to collect than preferences.\nSo why does KTO perform as well or better than DPO on the same preference data (that has been broken up)?\nGreater data efficiency helps, but it is not the only answer, given that even after adjusting for this factor in the one-$y$-per-$x$ setup, KTO still outperforms.\n\nIn this section, we will discuss two theoretical explanations for this phenomenon: (1) preference likelihood can be maximized without necessarily maximizing underlying human utility; (2) KTO implicitly ignores noisy and intransitive data, which is common in real-world feedback.\n\n\\begin{proposition}\n    As the reward implied by the current policy tends to $\\pm \\infty$, the KTO update of $\\pi_\\theta$ tends to zero.\n    \\label{prop:gradient}\n\\end{proposition}\n\nThis means that if $(x,y)$ is implied by the current policy $\\pi_\\theta$ to be too difficult or too easy to learn from, then it is effectively ignored.\nIn some cases, this may be a blessing in disguise: since real-world feedback is very noisy \\citep{hoeffler1999constructing}, the reason a desirable example has a highly negative implied reward may be because it is mislabelled.\nBy avoiding this hard-to-learn data, KTO avoids fitting to noise.\nHowever, this also means that KTO could end up ignoring data that is hard-to-learn but necessary to recover $r^*$, potentially resulting in underfitting to complex distributions.\nSuch underfitting may be mitigated by aligning the model with lower $\\beta$ and for more epochs.\n\n\\begin{theorem}\n    \\label{theorem:omission}\n    Assuming the value function is logistic, for a reward function $r^*_a$ that maximizes (\\ref{eq:rlhf_obj}), there exists a reward function in its equivalence class (i.e., $r^*_b(x,y) = r^*_a(x,y) + h(x)$ for some $h(x)$) that induces the same optimal policy $\\pi^*$ and the same Bradley-Terry preference distribution but a different human value distribution.\n\\end{theorem}\n\nA key insight from \\citet{rafailov2023direct} is that reward functions in the same equivalence class (i.e., differing only in an input-specific component) induce the same optimal policy under (\\ref{eq:rlhf_obj}) and the same Bradley-Terry preference distribution.\nHowever, we show under mild assumptions that the value distribution---i.e., human utility---is affected by such input-specific changes, so maximizing preference likelihood does not mean one is maximizing human utility.\nThis helps explain why the margin between KTO and DPO is even bigger in human evaluations than it is in automated LLM-as-a-judge evaluations (Appendix \\ref{sec:human_eval}). \n\n\\begin{table}[t]\n\\tiny\n\\centering\n\\caption{In aligning Mistral-7B on the OpenAssistant dataset, we find that using KTO with only one output per input still outperforms DPO, despite this restriction reducing the amount of training data by 72\\%. A 90\\% binomial confidence interval is given.}\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{lc}\n\\toprule\nMethod & Winrate vs.\\ SFT Target \\\\\n\\midrule\nMistral-7B (unaligned) & 0.525 $\\pm$ 0.037 \\\\\nMistral-7B + DPO & 0.600 $\\pm$ 0.037 \\\\\nMistral-7B + KTO (all $y$ per $x$) & \\bf 0.652 $\\pm$ 0.036 \\\\\nMistral-7B + KTO (one $y$ per $x$) & 0.631 $\\pm$ 0.036 \\\\\nMistral-7B-Instruct & \\ 0.621 $\\pm$ 0.031 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\label{tab:mistral}\n\\end{table}\n\n\\begin{theorem}\n\\label{theorem:intransitive}\nFor input $x$ with outputs $\\{y_a, y_b\\}$, let dataset $D$ comprise contradictory preferences $y_a \\succ y_b$ and $y_b \\succ y_a$ in proportion $p \\in (0.5, 1)$ and $(1 - p) \\in (0, 0.5)$ respectively. \nIf $p^{1/\\beta} \\pi_\\text{ref}(y_a|x) < (1 - p)^{1/\\beta} \\pi_\\text{ref}(y_b|x)$, then the optimal DPO policy is more likely to produce the minority-preferred $y_b$; the optimal KTO policy will strictly produce the majority-preferred $y_a$ for a loss-neutral value function ($\\lambda_D = \\lambda_U$). \n\\end{theorem}\n\nInformally, say there are two contradictory preferences over the output for $x$, with the majority $p$ preferring $y_a$ and the minority $1 - p$ preferring $y_b$.\nIn the worst-case, when $p$ is sufficiently low and the reference model is sufficiently unaligned, the optimal DPO policy is more likely to produce the minority-preferred output $y_b$ even though the implied reward $r_\\theta(x, y_b) > r_\\theta(x, y_a)$.\nIn contrast, the optimal KTO policy will deterministically produce the majority-preferred $y_a$ if the value function is loss-neutral ($\\lambda_D = \\lambda_U$), which is the default setting.\nThis suggests that KTO has better worst-case outcomes when handling feedback intransitivity.\n\n\\subsection{KTO vs.\\ DPO -- when to use which? }\nWhen human feedback is in a binary format, and especially when there is an imbalance between the number of desirable and undesirable examples, KTO is the natural choice.\nWhen your data is in the form of preferences, the choice is less clear.\nPutting aside the greater data efficiency of KTO, our theoretical analysis suggests that if your preference data has sufficiently little noise and sufficiently little intransitivity, then DPO will work better, since there is some risk of KTO underfitting; this risk can be mitigated by using a lower value of $\\beta$ and aligning with KTO for more epochs.\nBut if there is enough noise and intransitivity, then the better worst-case guarantees of KTO will win out.\nMost publicly available preference datasets (e.g., SHP, OpenAssistant) contain noisy feedback from many different humans whose preferences likely contradict to some extent, which explains why KTO was able to match or exceed DPO performance in our experiments.\nEven synthetic feedback can be noisy and intransitive, which helps explain why KTO outperforms DPO when aligning with UltraFeedback. \n\n\\section{Future Work}\n\nThe existence of HALOs raises many questions. For one, KTO is based on the Kahneman-Tversky value function for monetary gambles, which is almost certainly different from how humans perceive the relative goodness of text. \nWhat value functions and reference point distributions best describe how humans perceive language, and how do they vary across domains and individuals?\nHow can we identify the best HALO for each individual and setting instead of using one default loss?\n\nOn a more technical level, important directions include developing HALOs that: (1) incorporate granular feedback, such as a score, especially when optimizing for multiple desiderata; (2) work for other modalities (e.g. images) and model classes (e.g., diffusion models), especially models that do not produce an explicit distribution over the output space; (3) can resolve contradictions in feedback according to different definitions of fairness; (4) are designed to be used with online data, where the direction of feedback is implied by $r_\\theta$ or some external reward data.\n\nEcologically valid evaluation \\citep{de2020towards}, where the aligned models are deployed in real-world settings, are also needed to judge the merits of different HALOs.\n\n    \n\\section{Conclusion}\n\nAlthough model alignment has historically been reward-centric, we found that the inductive biases of alignment objectives are critical to their success.\nMoreover, these inductive biases have analogs in the prospect theory literature, suggesting that they work in part because they reflect human biases in decision-making.\nWe abstracted these insights into a family of alignment objectives called \\textit{human-aware losses} (HALOs).\nWe then proposed a HALO called Kahneman-Tversky Optimization (KTO) for directly maximizing the utility of generations instead of maximizing the likelihood of preferences, as existing methods do.\nDespite only learning from a binary signal of whether an output is (un)desirable, KTO was as good or better than preference-based methods in our experiments.\nMore broadly, our work suggests that akin to how there is no one reward model that is universally superior, there is no one loss function either---the best HALO depends on the inductive biases that are most appropriate for a given setting, and much work remains to be done in identifying the best HALO for each context.\n\n\\section*{Acknowledgements}\n\nWe thank Percy Liang, Dilip Arumugam, Arya McCarthy, and Nathan Lambert for feedback.\nWe thank Stas Bekman and Gautam Mittal for cluster assistance and Alex Manthey for helping with human evaluation.\n\n\\section*{Impact Statement}\n\nThe methods discussed in this paper have the potential to make LLMs more helpful and safer, which is often needed for models deployed in production.\nIt is possible that in making models more helpful, we increase the utility of one person at the expense of broader society.\nIn aligning models with human feedback, one may also---without even fully recognizing it---be aligning to an unrepresentative subset of the population, which may hinder the ability of individuals outside that subset to benefit equally from using the model.\n\nThe data used for LLM alignment, including the datasets used in this paper (e.g., SHP, HH, OASST) contain preferences of groups that are not representative of the broader population.\nBiases in this data have the potential to be propagated downstream when used to align models with methods like KTO, especially when no efforts are made to adjust for the different population.\nKTO in particular implicitly resolves contradictions in feedback by taking the majority-preferred outcome for a loss-neutral value function, which does not comport with many theories of fairness (e.g., Rawlsianism).\nSince user preferences are, in turn, affected by the models they interact with, this also risks the homogenization of preferences and utility functions when KTO-aligned models are deployed at scale.\nThe design of HALOs that resolve contradictions in more diverse ways is an important direction for future work.\n\nOn the other hand, because KTO works with binary feedback, which is more abundant, cheaper, and faster to collect in the real world, it significantly lowers the barrier to data collection.\nThis makes it easier to collect feedback from traditionally under-represented groups and serve different models to different users, instead of just one monolithic model being served to everyone.\n\n\\newpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{\\texttt{ORPO}: Monolithic Preference Optimization without Reference Model}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nWhile recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, \\texttt{ORPO}, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with \\texttt{ORPO} on the UltraFeedback alone surpasses the performance of state-of-the-art language models with more than 7B and 13B parameters: achieving up to 12.20\\% on $\\text{AlpacaEval}_{2.0}$ (Figure \\ref{fig:first}), 66.19\\% on IFEval (instruction-level loose, Table \\ref{tab:ifeval}), and 7.32 in MT-Bench (Figure \\ref{fig:mtbench}). We release code\\footnote{\\url{https://github.com/xfactlab/orpo}} and model checkpoints for Mistral-\\texttt{ORPO}-$\\alpha$ (7B)\\footnote{\\url{https://huggingface.co/kaist-ai/mistral-orpo-alpha}} and Mistral-\\texttt{ORPO}-$\\beta$ (7B).\\footnote{\\url{https://huggingface.co/kaist-ai/mistral-orpo-beta}}\n\\end{abstract}\n\n\\setlength{\\jot}{10pt}\n\\section{Introduction}\nPre-trained language models (PLMs) with vast training corpora such as web texts \\citep{Gokaslan2019OpenWeb, penedo2023refinedweb} or textbooks \\citep{li2023textbooks} have shown remarkable abilities in diverse natural language processing (NLP) tasks \\citep{NEURIPS2020_1457c0d6, zhang2022opt, touvron2023llama, jiang2023mistral, almazrouei2023falcon}. \n\\begin{figure}[hbt]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{figure/first_fig.pdf}\n    \\caption{$\\text{AlpacaEval}_{2.0}$ result of Llama-2 (7B) and Mistral (7B) fine-tuned with \\texttt{ORPO} (blue) in comparison to the state-of-the-art models. Notably, Mistral-\\texttt{ORPO}-$\\alpha$ \\& $\\beta$ surpasses Zephyr $\\beta$ and Llama-2-Chat (13B) with a single epoch training exclusively on the UltraFeedback.}\n    \\label{fig:first}\n\\end{figure}However, the models must undergo further tuning to be usable in general-domain applications, typically through processes such as \\textit{instruction tuning} and \\textit{preference alignment}. \n\nInstruction-tuning \\citep{wei2022finetuned, alpaca, wang2023far, zhou2023lima} trains models to follow task descriptions given in natural language, which enables models to generalize well to previously unseen tasks. However, despite the ability to follow instructions, models may generate harmful or unethical outputs \\citep{carlini2021extracting, gehman-etal-2020-realtoxicityprompts, pryzant-etal-2023-automatic}.  To further align these models with human values, additional training is required with pairwise preference data using techniques such as reinforcement learning with human feedback \\citep[RLHF]{ziegler2020finetuning, stiennon2022learning} and direct preference optimization \\citep[DPO]{rafailov2023direct}.\n\n\\begin{figure*}[hbt!]\n    \\centering\n    \\includegraphics[width=\\textwidth]{figure/orpo_main.pdf}\n    \\caption{Comparison of model alignment techniques. \\texttt{ORPO} aligns the language model \\textit{without a reference model} in a single-step manner by assigning a weak penalty to the rejected responses and a strong adaptation signal to the chosen responses with a simple log odds ratio term appended to the negative log-likelihood loss.}\n    \\label{fig:main}\n\\end{figure*}\n\nPreference alignment methods have demonstrated success in several downstream tasks beyond reducing harm. For example, improving factuality \\citep{tian2023finetuning, cheng2024ai, chen2024grath}, code-based question answering \\citep{gorbatovski2024reinforcement}, and machine translation \\citep{ramos2023aligning}. The versatility of alignment algorithms over a wide range of downstream tasks highlights the necessity of understanding the alignment procedure and further improving the algorithms in terms of efficiency and performance. However, existing preference alignment methods normally consist of a multi-stage process, as shown in Figure \\ref{fig:main}, typically requiring a second reference model and a separate warm-up phase with supervised fine-tuning (SFT) \\citep{ziegler2020finetuning, rafailov2023direct, wu2023pairwise}. \n\nIn this paper, we study the role and impact of SFT in pairwise preference datasets for model alignment in Section \\ref{sec:role} and propose a simple and novel monolithic alignment method, odds ratio preference optimization (\\texttt{ORPO}), which efficiently penalizes the model from learning undesired generation styles during SFT in Section \\ref{sec:method}. In contrast to previous works, our approach requires neither an SFT warm-up stage nor a reference model, enabling resource-efficient development of preference-based aligned models. \n\nWe demonstrate the effectiveness of our method with the evaluation of model alignment tasks and popular leaderboards in Section \\ref{subsec:instruct} and \\ref{subsec:multi} by fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with \\texttt{ORPO}. Then, we conduct controlled experiments comparing \\texttt{ORPO} against established methods for model alignment, RLHF, and DPO for different datasets and model sizes in Section \\ref{subsec:rmwin}. Along with the post-hoc analysis of generation diversity in Section \\ref{subsec:div}, we expound on the theoretical, empirical, and computational justification of utilizing the odds ratio in monolithic preference alignment in Section \\ref{subsec:compute}. We release the training code and the checkpoints for Mistral-\\texttt{ORPO}-$\\alpha$ (7B) and Mistral-\\texttt{ORPO}-$\\beta$ (7B). These models achieve 7.24 and 7.32 in MT-Bench, 11.33\\% and 12.20\\% on $\\text{AlpacaEval}_{2.0}$, and 61.63\\% and 66.19\\% in IFEval instruction-level loose accuracy, respectively. % in AlpacaEval.\n\n\\section{Related Works}\n\\paragraph{Alignment with Reinforcement Learning} Reinforcement learning with human feedback (RLHF) commonly applies the Bradley-Terry model \\citep{19ff28b9-64f9-3656-ba40-08326a05748e} to estimate the probability of a pairwise competition between two independently evaluated instances. An additional reward model is trained to score instances. Reinforcement learning algorithms such as proximal policy optimization (PPO) \\citep{schulman2017proximal} are employed to train the model to maximize the score of the reward model for the chosen response, resulting in language models that are trained with human preferences \\citep{ziegler2020finetuning, stiennon2022learning}. Notably, \\citet{ouyang2022training} demonstrated the scalability and versatility of RLHF for instruction-following language models. Extensions such as language model feedback (RLAIF) could be a viable alternative to human feedback \\citep{bai2022constitutional, lee2023rlaif, pang2023language}. However, RLHF faces challenges of extensive hyperparameter searching due to the instability of PPO \\citep{rafailov2023direct, wu2023pairwise} and the sensitivity of the reward models \\citep{gao2022scaling, wang2024secrets}. Therefore, there is a crucial need for stable preference alignment algorithms.\n\n\\paragraph{Alignment without Reward Model} Several techniques for preference alignment mitigate the need for reinforcement learning \\citep{rafailov2023direct, song2023preference, azar2023general, ethayarajh2023halos}. \\citet{rafailov2023direct} introduce direct policy optimization (DPO), which combines the reward modeling stage into the preference learning stage. \\citet{azar2023general} prevented potential overfitting problems in DPO through identity preference optimization (IPO). \\citet{ethayarajh2023halos} and \\citet{Cai2023ULMAUL} proposed Kahneman-Tversky Optimisation (KTO) and Unified Language Model Alignment (ULMA) that does not require the pair-wise preference dataset, unlike RLHF and DPO. \\citet{song2023preference} further suggests incorporation of the softmax value of the reference response set in the negative log-likelihood loss to merge the supervised fine-tuning and preference alignment. \n\n\\paragraph{Alignment with Supervised Fine-tuning} Preference alignment methods in reinforcement learning (RL) often leverage supervised fine-tuning (SFT) to ensure the stable update of the active policy in relation to the old policy \\citep{schulman2017proximal}. This is because the SFT model is the old policy in the context of RLHF \\citep{ziegler2020finetuning}. Furthermore, empirical findings indicate that, even in non-RL alignment methods, the SFT model is crucial for achieving convergence to desired results \\citep{rafailov2023direct, tunstall2023zephyr}. \n\nIn contrast, there have been approaches to build human-aligned language models by conducting SFT only with filtered datasets \\citep{zhou2023lima, li2023selfalignment, haggerty2024selfsupervised, Zhou2023LoBaSSGL}. \\citet{zhou2023lima} demonstrated that SFT with a small amount of data with fine-grained filtering and curation could be sufficient for building helpful language model assistants. Furthermore, \\citet{li2023selfalignment} and \\citet{haggerty2024selfsupervised} proposed an iterative process of fine-tuning the supervised fine-tuned language models with their own generations after fine-grained selection of aligned generations and \\citet{Zhou2023LoBaSSGL} suggested that a curated subset of preference dataset is sufficient for alignment. While these works highlight the impact and significance of SFT in the context of alignment, the actual role of SFT and the theoretical background for incorporating preference alignment in SFT remains understudied.\n\n\\section{The Role of Supervised Fine-tuning}\\label{sec:role}\nWe study the behavior of supervised fine-tuning (SFT) as an initial stage of preference alignment methods \\citep{ziegler2020finetuning, rafailov2023direct} through analysis of the loss function in SFT and empirical demonstration of the preference comprehension ability of the trained SFT model. SFT plays a significant role in tailoring the pre-trained language models to the desired domain \\citep{zhou2023lima,dong2024abilities} by increasing the log probabilities of pertinent tokens. Nevertheless, this inadvertently increases the likelihood of generating tokens in undesirable styles, as illustrated in Figure \\ref{fig:sft}. Therefore, it is necessary to develop methods capable of preserving the domain adaptation role of SFT while concurrently discerning and mitigating unwanted generation styles.\n\n\\paragraph{Absence of Penalty in Cross-Entropy Loss} The goal of cross-entropy loss model fine-tuning is to penalize the model if the predicted logits for the reference answers are low, as shown in Equation \\ref{eq:celoss}. \n\\begin{align}\n\\mathcal{L} &= -\\frac{1}{m} \\sum_{k=1}^{m} \\log P(\\mathbf{x}^{(k)}, \\mathbf{y}^{(k)})\\\\\n&= -\\frac{1}{m}\\sum_{k=1}^{m}\\sum_{i=1}^{|V|}y_i^{(k)} \\cdot \\log(p_i^{(k)})\\label{eq:celoss}\n\\end{align}\nwhere $y_i$ is a boolean value that indicates if $i$th token in the vocabulary set $V$ is a label token, $p_i$ refers to the probability of $i$th token, and $m$ is the length of sequence. Using cross-entropy alone gives no direct penalty or compensation for the logits of non-answer tokens \\citep{lin2017focal} as  $y_i$ will be set to 0. While cross-entropy is generally effective for domain adaptation  \\citep{mao2023crossentropy}, there are no mechanisms to penalize rejected responses when compensating for the chosen responses. Therefore, the log probabilities of the tokens in the rejected responses increase along with the chosen responses, which is not desired from the viewpoint of preference alignment.\n\n\\paragraph{Generalization over Both Response Styles} We conduct a pilot study to empirically demonstrate the miscalibration of chosen and rejected responses with supervised fine-tuning alone. We fine-tune OPT-350M \\citep{zhang2022opt} on \\textit{the chosen responses only} from the HH-RLHF dataset \\citep{bai2022constitutional}. Throughout the training, we monitor the log probability of rejected responses for each batch and report this in Figure \\ref{fig:sft}. Both the log probability of chosen and rejected responses exhibited a simultaneous increase. This can be interpreted from two different perspectives. First, the cross-entropy loss effectively guides the model toward the intended domain (e.g., dialogue). However, the absence of a penalty for unwanted generations results in rejected responses sometimes having even higher log probabilities than the chosen ones.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{figure/sft_logit.pdf}\n    \\caption{Log probabilities for chosen and rejected responses during OPT-350M model fine-tuning on HH-RLHF dataset. Despite only chosen responses being used for supervision, rejected responses show a comparable likelihood of generation.}\n    \\label{fig:sft}\n\\end{figure}\n\n\\paragraph{Penalizing Undesired Generations} Appending an unlikelihood penalty to the loss has demonstrated success in reducing unwanted degenerative traits in models  \\citep{welleck2019neural,li-etal-2020-dont}. For example, to prevent \\textit{repetitions}, an unwanted token set of previous contexts, $k \\in \\mathcal{C}_{recent}$, is disfavored by adding the following term to $(1-p_i^{(k)})$ to the loss (such as Equation \\ref{eq:celoss}) which penalizes the model for assigning high probabilities to recent tokens. Motivated by SFT ascribing high probabilities to rejected tokens (Figure \\ref{fig:sft}) and the effectiveness of appending penalizing unwanted traits, we design a monolithic preference alignment method that dynamically penalizes the disfavored response for each query without the need for crafting sets of rejected tokens.\n\n\\section{Odds Ratio Preference Optimization}\\label{sec:method}\nWe introduce a novel preference alignment algorithm, Odds Ratio Preference Optimization (\\texttt{ORPO}), which incorporates an odds ratio-based penalty to the conventional negative log-likelihood (NLL) loss for differentiating the generation styles between favored and disfavored responses.\n\n\\subsection{Preliminaries}\nGiven an input sequence $x$, the average log-likelihood of generating the output sequence $y$, of length $m$ tokens, is computed as Equation \\ref{eq:likeli}. The odds of generating the output sequence $y$ given an input sequence $x$ is defined in Equation \\ref{eq:odds}:\n\\begin{equation}\n    \\log P_\\theta(y|x) = \\frac{1}{m} \\sum_{t=1}^m \\log P_\\theta(y_t|x, y_{<t})\\label{eq:likeli}\n\\end{equation}\n\\begin{equation}\n    \\textbf{odds}_\\theta(y|x) = \\frac{P_\\theta(y|x)}{1 - P_\\theta(y|x)}\\label{eq:odds}\n\\end{equation}\nIntuitively, $\\textbf{odds}_\\theta(y|x) = k$ implies that it is $k$ times more likely for the model $\\theta$ to generate the output sequence $y$ than not generating it. Thus, the odds ratio of the chosen response $y_w$ over the rejected response $y_l$, $\\textbf{OR}_\\theta(y_w, y_l)$, indicates how much more likely it is for the model $\\theta$ to generate $y_w$ than $y_l$ given input $x$, defined in Equation \\ref{eq:or}.\n\\begin{equation}\n    \\textbf{OR}_\\theta(y_w, y_l) = \\frac{\\textbf{odds}_\\theta(y_w|x)}{\\textbf{odds}_\\theta(y_l|x)}\\label{eq:or}\n\\end{equation}\n\n\\subsection{Objective Function of \\texttt{ORPO}}\n\nThe objective function of \\texttt{ORPO} in Equation~\\ref{eq:main} consists of two components: 1) supervised fine-tuning (SFT) loss ($\\mathcal{L}_{SFT}$); 2) relative ratio loss ($\\mathcal{L}_{OR}$).\n\\begin{equation}\n    \\mathcal{L}_{ORPO} = \\mathbb{E}_{(x, y_w, y_l)}\\left[ \\mathcal{L}_{SFT} + \\lambda \\cdot \\mathcal{L}_{OR} \\right]\n    \\label{eq:main}\n\\end{equation}\n$\\mathcal{L}_{SFT}$ follows the conventional causal language modeling negative log-likelihood (NLL) loss function to maximize the likelihood of generating the reference tokens as previously discussed in Section~\\ref{sec:role}.\n$\\mathcal{L}_{OR}$ in Equation \\ref{eq:ratio} maximizes the odds ratio between the likelihood of generating the disfavored response $y_w$ and the disfavored response $y_l$. We wrap the log odds ratio with the log sigmoid function so that $\\mathcal{L}_{OR}$ could be minimized by increasing the log odds ratio between $y_w$ and $y_l$.\n\\begin{equation}\n    \\mathcal{L}_{OR} = -\\log \\sigma \\left( \\log \\frac{\\textbf{odds}_\\theta(y_w|x)}{\\textbf{odds}_\\theta(y_l|x)} \\right) \\label{eq:ratio} \n\\end{equation}\nTogether, $\\mathcal{L}_{SFT}$ and $\\mathcal{L}_{OR}$ weighted with $\\lambda$ tailor the pre-trained language model to adapt to the specific subset of the desired domain and disfavor generations in the rejected response sets.\n\n\\subsection{Gradient of \\texttt{ORPO}}\\label{subsec:grad}\n\nThe gradient of $\\mathcal{L}_{Ratio}$ further justifies using the odds ratio loss. It comprises two terms: one that penalizes the wrong predictions and one that contrasts between chosen and rejected responses, denoted in Equation \\ref{eq:simp}\\footnote{The full derivation for $\\nabla_\\theta \\mathcal{L}_{OR}$ is in Appendix \\ref{ap:derivative}.} for $d=(x, y_l, y_w)\\sim D$.\n\\begin{equation}\n    \\nabla_\\theta \\mathcal{L}_{OR} = \\delta(d) \\cdot h(d)\\label{eq:simp}\n\\end{equation}\n\\begin{align}\n    \\delta(d) &= \\left[ 1 + \\frac{\\textbf{odds}_\\theta P(y_w|x)}{\\textbf{odds}_\\theta P(y_l|x)} \\right]^{-1}\\label{eq:delta} \\\\\n    h(d) &= \\frac{\\nabla_\\theta \\log P_\\theta(y_w|x)}{1 - P_\\theta(y_w|x)} - \\frac{\\nabla_\\theta \\log P_\\theta(y_l|x)}{1 - P_\\theta(y_l|x)}\\label{eq:grad}\n\\end{align}\nWhen the odds of the favored responses are relatively higher than the disfavored responses, $\\delta(d)$ in Equation \\ref{eq:delta} will converge to 0. This indicates that the $\\delta(d)$ will play the role of a penalty term, accelerating the parameter updates if the model is more likely to generate the rejected responses. \n\nMeanwhile, $h(d)$ in Equation \\ref{eq:grad} implies a weighted contrast of the two gradients from the chosen and rejected responses. Specifically, $1-P(y|x)$ in the denominators amplifies the gradients when the corresponding side of the likelihood $P(y|x)$ is low. For the chosen responses, this accelerates the model's adaptation toward the distribution of chosen responses as the likelihood increases.\n\n\\section{Experimental Settings}\n\n\\subsection{Training Configurations}\n\\paragraph{Models} We train a series of OPT models \\citep{zhang2022opt} scaling from 125M to 1.3B parameters comparing supervised fine-tuning (SFT), proximal policy optimization (PPO), direct policy optimization (DPO), and compare these to our \\texttt{ORPO}. PPO and DPO models were fine-tuned with TRL library \\citep{vonwerra2022trl} on top of SFT models trained for a single epoch on the chosen responses following \\citet{rafailov2023direct} and \\citet{tunstall2023zephyr}. We notate this by prepending \"+\" to each algorithm (e.g., +DPO). Additionally, we train Phi-2 (2.7B) \\citep{Javaheripi_Bubeck_2023}, a pre-trained language model with promising downstream performance \\citep{open-llm-leaderboard}, as well as Llama-2 (7B) \\citep{touvron2023llama} and Mistral (7B) \\citep{jiang2023mistral}.\nFurther training details for each method are in Appendix \\ref{ap:detail}.\n\n\\paragraph{Datasets} We test each training configuration and model on two datasets: 1) Anthropic's HH-RLHF \\citep{bai2022training}, 2) Binarized UltraFeedback \\citep{tunstall2023zephyr}. We filtered out instances where $y_w=y_l$ or where $y_w=\\emptyset$ or where $y_l=\\emptyset$.\n\n\\paragraph{Reward Models} We train OPT-350M and OPT-1.3B on each dataset for a single epoch for reward modeling with the objective function in Equation \\ref{eq:rm} \\citep{ziegler2020finetuning}. The OPT-350M reward model was used for PPO, and OPT-1.3B reward model was used to assess the generations of fine-tuned models. We refer to these reward models as RM-350M and RM-1.3B in Section \\ref{sec:result}.\n\\begin{equation}\n    -\\mathbb{E}_{(x, y_l, y_w)} \\left[ \\log \\sigma \\left(r(x, y_w) - r(x, y_l) \\right) \\right]\n    \\label{eq:rm}\n\\end{equation}\n\n\\subsection{Leaderboard Evaluation} In Section~\\ref{subsec:instruct},  we evaluate the models using the $\\text{AlpacaEval}_{1.0}$ and $\\text{AlpacaEval}_{2.0}$ \\citep{alpaca_eval} benchmarks, comparing \\texttt{ORPO} to other instruction-tuned models reported in the official leaderboard,\\footnote{\\url{https://tatsu-lab.github.io/alpaca_eval/}} including Llama-2 Chat (7B) and (13B) \\citep{touvron2023llama}, and Zephyr $\\alpha$ and $\\beta$ \\citep{almazrouei2023falcon}. Similarly, in Section \\ref{subsec:multi}, we evaluate the models with MT-Bench \\citep{zheng_judging_2023} and report the results and the scores of the same models reported in the official leaderboard.\\footnote{\\url{https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard}}\nUsing GPT-4 \\citep{achiam2023gpt} as an evaluator in $\\text{AlpacaEval}_{1.0}$, we assess if the trained model can be preferred over the responses generated from \\texttt{text-davinci-003}. For $\\text{AlpacaEval}_{2.0}$, we used GPT-4-turbo\\footnote{\\url{https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo}} as an evaluator following the default setting. We assess if the generated responses are favored over those generated from GPT-4. Finally, using GPT-4 as an evaluator in MT-Bench, we check if the models can follow the instructions with hard answers in a multi-turn conversation.\n\n\\section{Results and Analysis}\\label{sec:result}\n\nFirst, we assess the general instruction-following abilities of the models by comparing the preference alignment algorithms in Sections \\ref{subsec:instruct} and \\ref{subsec:multi}. Second, we measure the win rate of OPT models trained with \\texttt{ORPO} against other alignment methods training OPT 1.3B as a reward model in Section \\ref{subsec:rmwin}. Then, we measure the lexical diversity of the models trained with \\texttt{ORPO} and DPO in Section \\ref{subsec:div}.\n\n\\subsection{Single-turn Instruction Following}\\label{subsec:instruct}\n\n\\begin{table*}[hbt!]\n\\setlength{\\tabcolsep}{12pt}\n\\centering\n\\resizebox{350pt}{!}{%]\n\\begin{tabular}{l|c|c|c}\n\\hline \\hline\n\\textbf{Model Name}       & \\textbf{Size} &  $\\textbf{AlpacaEval}_{\\textbf{1.0}}$    &$\\textbf{AlpacaEval}_{\\textbf{2.0}}$   \\\\ \\hline\n\nPhi-2 + SFT                 &  2.7B  & 48.37\\% (1.77)          & 0.11\\% (0.06) \\\\\nPhi-2 + SFT + DPO           &  2.7B  &   50.63\\% (1.77)           & 0.78\\% (0.22) \\\\\nPhi-2 + \\texttt{ORPO} \\textit{(Ours)}    &  2.7B  &  \\textbf{71.80\\% (1.59)}  & \\textbf{6.35\\% (0.74)} \\\\  \\hline  \nLlama-2 Chat *              &  7B    &  71.34\\% (1.59)          & 4.96\\% (0.67) \\\\  \nLlama-2 Chat *              &  13B   &  81.09\\% (1.38)          & 7.70\\% (0.83) \\\\ \nLlama-2 + \\texttt{ORPO} \\textit{(Ours)}     &  7B    &  \\textbf{81.26\\% (1.37)}          & \\textbf{9.44\\% (0.85)}  \\\\ \\hline\nZephyr ($\\alpha$) *         &  7B    &  85.76\\% (1.23)          & 8.35\\% (0.87) \\\\  \nZephyr ($\\beta$) *          &  7B    &  90.60\\% (1.03)          & 10.99\\% (0.96) \\\\ \nMistral-\\texttt{ORPO}-$\\alpha$ \\textit{(Ours)}     &  7B    &   87.92\\% (1.14)   & 11.33\\% (0.97)  \\\\ \nMistral-\\texttt{ORPO}-$\\beta$ \\textit{(Ours)}     &  7B    &   \\textbf{91.41\\% (1.15)}   & \\textbf{12.20\\% (0.98)}  \\\\ \\hline \\hline\n\n\\end{tabular}%\n}\n\\caption{Table of instruction-following abilities of each checkpoint measured through AlpacaEval. While clearly showing the improvements in instruction-following abilities after training with \\texttt{ORPO}, it is notable that \\texttt{ORPO} models exceed RLHF or DPO models of Llama-2 and Mistral (* indicates the results from the official leaderboard.)}\n\\label{tab:result}\n\\end{table*}\n\\paragraph{Phi-2 (2.7B)} \\texttt{ORPO} improved pre-trained Phi-2 to exceed the performance of the Llama-2 Chat instruction-following language model by \\textit{only using UltraFeedback} as the instruction-tuning dataset, as shown in Table \\ref{tab:result}. $\\lambda$ of 0.25 was applied for Phi-2, resulting in 71.80\\% and 6.35\\% in AlpacaEval.\n\n\\paragraph{Llama-2 (7B)} Notably, UltraFeedback and \\texttt{ORPO} with $\\lambda$ of 0.2 on Llama-2 (7B) resulted in higher AlpacaEval scores than the chat versions of both 7B and 13B scale trained with RLHF, eventually showing 81.26\\% and 9.44\\% in both AlpacaEvals.\n\nIn contrast, in our controlled experimental setting of conducting one epoch of SFT and three epochs of DPO following \\citet{tunstall2023zephyr} and \\citet{rafailov2023direct}, Llama-2 + SFT and Llama-2 + SFT + DPO yielded models with outputs that could not be evaluated. This supports the efficacy of \\texttt{ORPO}, in which the model can rapidly learn the desired domain and the preference with limited data. This aligns with the $h(d)$ examination in the gradient of our method studied in Section \\ref{subsec:grad}.\n\n\\paragraph{Mistral-\\texttt{ORPO}-$\\alpha$ (7B)} Furthermore, fine-tuning Mistral (7B) with single-turn conversation dataset, UltraFeedback, and \\texttt{ORPO} with $\\lambda$ of 0.1 outperforms Zephyr series, which are the Mistral (7B) models fine-tuned with SFT on 20K UltraChat \\citep{ding2023enhancing} and DPO on the full UltraFeedback. As shown in Table \\ref{tab:result}, Mistral-\\texttt{ORPO}-$\\alpha$ (7B) achieves 87.92\\% and 11.33\\%, which exceeds Zephyr $\\alpha$ by 1.98\\% and Zephyr $\\beta$ by 0.34\\% in $\\text{AlpacaEval}_{2.0}$. The sample responses and corresponding references from GPT-4 can be found in Appendix \\ref{ap:generation}.\n\n\\paragraph{Mistral-\\texttt{ORPO}-$\\beta$ (7B)} Using the same configuration of Mistral-\\texttt{ORPO}-$\\alpha$ (7B), we additionally compare fine-tuning Mistral on the cleaned version of the UltraFeedback\\footnote{\\url{https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned}} to demonstrate the effect of the data quality \\citep{notus2023}. While the actual sizes of datasets are similar, \\texttt{ORPO} gains further advantages from the dataset quality by scoring over 91\\% and 12\\% on AlpacaEval, as shown in Table \\ref{tab:result}. Further instruction-following evaluation on two Mistral-based models with IFEval \\citep{zhou2023instructionfollowing} is reported in the Appendix \\ref{ap:ifeval}.\n\n\\subsection{Multi-turn Instruction Following}\\label{subsec:multi}\n\nWith our best model, Mistral-\\texttt{ORPO}-$\\alpha$ (7B) and Mistral-\\texttt{ORPO}-$\\beta$ (7B), we also assess the multi-turn instruction-following skills with deterministic answers (e.g., math) through MT-Bench. \n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{figure/mtbench_paper.pdf}\n    \\caption{MT-Bench result of Mistral-\\texttt{ORPO}-$\\alpha$ (7B) and Mistral-\\texttt{ORPO}-$\\beta$ (7B) by the category. Further comparison can be found in the Appendix \\ref{ap:mtbench}.}\n    \\label{fig:mtbench-main}\n\\end{figure}\n\nAs shown in Figure \\ref{fig:mtbench-main}, \\texttt{ORPO}-Mistral (7B) series achieve comparable results to either larger or the proprietary models, including Llama-2-Chat (70B) and Claude. Eventually, Mistral-\\texttt{ORPO}-$\\alpha$ (7B) and Mistral-\\texttt{ORPO}-$\\beta$ (7B) scored 7.23 and 7.32 in MT-Bench without being exposed to the multi-turn conversation dataset during training.\n\n\\begin{figure*}[hbt!]\n    \\centering\n    \\includegraphics[width=\\textwidth]{figure/uf_reward.pdf}\n    \\caption{Reward distribution comparison between OPT-125M (left), OPT-350M (middle), and OPT-1.3B (right) trained with SFT (blue), RLHF (green), DPO (orange), and \\texttt{ORPO} (red) on the test set of UltraFeedback using the RM-1.3B. While the rewards of the trained models are roughly normal and preference optimization algorithms (RLHF, DPO, and \\texttt{ORPO}) tend to move the reward distribution in the positive direction, \\texttt{ORPO} is on par or better than RLHF and DPO in increasing the expected reward. The same plot for the HH-RLHF dataset is in Appendix \\ref{ap:hh}.}\n    \\label{fig:foobar}\n\\end{figure*}\n\n\\subsection{Reward Model Win Rate}\\label{subsec:rmwin}\n\nWe assess the win rate of \\texttt{ORPO} over other preference alignment methods, including supervised fine-tuning (SFT), PPO, and DPO, using RM-1.3B to understand the effectiveness and scalability of \\texttt{ORPO} in Tables \\ref{tab:rm_win_hh} and \\ref{tab:rm_win_uf}. Additionally, we visually verify that \\texttt{ORPO} can effectively enhance the expected reward compared to SFT in Figure \\ref{fig:foobar}.\n\n\\paragraph{HH-RLHF} In Table \\ref{tab:rm_win_hh}, \\texttt{ORPO} outperforms SFT and PPO across all model scales. The highest win rate against SFT and PPO across the size of the model was 78.0\\% and 79.4\\%, respectively. Meanwhile, the win rate over DPO was correlated to the model's size, with the largest model having the highest win rate: 70.9\\%. \n\n\\begin{table}[hbt!]\n\\resizebox{\\columnwidth}{!}{%\n\\begin{tabular}{c|ccc}\n\\hline \\hline\n\\textbf{\\texttt{ORPO} vs}  & \\textbf{SFT}    & \\textbf{+DPO}      & \\textbf{+PPO}      \\\\ \\hline\n\\textbf{OPT-125M} & 84.0 (0.62)     & 41.7 (0.77)       & 66.1 (0.26) \\\\\n\\textbf{OPT-350M} & 82.7 (0.56)     & 49.4 (0.54)       & 79.4 (0.29)  \\\\\n\\textbf{OPT-1.3B} & 78.0 (0.16)     & 70.9 (0.52)       & 65.9 (0.33)    \\\\ \\hline \\hline\n\\end{tabular}%\n}\n\\caption{Average win rate (\\%) and its standard deviation of \\texttt{ORPO} and standard deviation over other methods on \\textbf{HH-RLHF} dataset for three rounds. Sampling decoding with a temperature of 1.0 was used on the test set.}\n\\label{tab:rm_win_hh}\n\\end{table}\n\n\\paragraph{UltraFeedback} The win rate in UltraFeedback followed similar trends to what was reported in HH-RLHF, as shown in Table \\ref{tab:rm_win_uf}. \\texttt{ORPO} was preferred over SFT and PPO for maximum 80.5\\% and 85.8\\%, respectively. While consistently preferring \\texttt{ORPO} over SFT and PPO, the win rate over DPO gradually increases as the size of the model increases. The scale-wise trend exceeding DPO will be further shown through 2.7B models in Section \\ref{subsec:instruct}.\n\\begin{table}[hbt!]\n\\resizebox{\\columnwidth}{!}{%\n\\begin{tabular}{c|ccc}\n\\hline \\hline\n\\textbf{\\texttt{ORPO} vs}  & \\textbf{SFT}    & \\textbf{+DPO}      & \\textbf{+PPO}      \\\\ \\hline\n\\textbf{OPT-125M} & 73.2 (0.12)     & 48.8 (0.29)       & 71.4 (0.28) \\\\\n\\textbf{OPT-350M} & 80.5 (0.54)     & 50.5 (0.17)       & 85.8 (0.62)  \\\\\n\\textbf{OPT-1.3B} & 69.4 (0.57)     & 57.8 (0.73)       & 65.7 (1.07)  \\\\ \\hline \\hline\n\\end{tabular}%\n}\n\\caption{Average win rate (\\%) and its standard deviation of \\texttt{ORPO} and standard deviation over other methods on \\textbf{UltraFeedback} dataset for three rounds. Sampling decoding with a temperature of 1.0 was used.}\n\\label{tab:rm_win_uf}\n\\end{table}\n\n\\paragraph{Overall Reward Distribution} In addition to the win rate, we compare the reward distribution of the responses generated with respect to the test set of the UltraFeedback dataset in Figure \\ref{fig:foobar} and HH-RLHF dataset in Appendix \\ref{ap:hh}. Regarding the SFT reward distribution as a default, PPO, DPO, and \\texttt{ORPO} shift it in both datasets. However, the magnitude of reward shifts for each algorithm differs.\n\nIn Figure \\ref{fig:foobar}, RLHF (i.e., SFT + PPO) has some abnormal properties of the distribution with a low expected reward. We attribute this to empirical evidence of the instability and reward mismatch problem of RLHF \\citep{rafailov2023direct, gao2022scaling, shen-etal-2023-loose} as the RLHF models were trained with RM-350M and assessed with RM-1.3B. Meanwhile, it is notable that the \\texttt{ORPO} distribution (red) is mainly located on the very right side of each subplot, indicating higher expected rewards. Recalling the intent of preference alignment methods, the distributions in Figure \\ref{fig:foobar} indicate that \\texttt{ORPO} tends to fulfill the aim of preference alignment for all model sizes.\n\n\\subsection{Lexical Diversity}\\label{subsec:div}\n\nThe lexical diversity of the preference-aligned language models was studied in previous works \\citep{kirk2024understanding}. We expand the concept of per-input and across-input diversity introduced in \\citet{kirk2024understanding} by using Gemini-Pro \\citep{team2023gemini} as an embedding model, which is suitable for assessing the diversity of instruction-following language models by encoding a maximum of 2048 tokens. The diversity metric with the given set of sampled responses is defined as Equation \\ref{eq:distance}.\n\\begin{equation}\n    \\mathcal{O}_\\theta^i := \\{y_j \\sim \\theta(y|x_i) | j = 1, 2, ..., K \\}\\label{eq:set}\n\\end{equation}\n\\begin{equation}\n    D(\\mathcal{O}^i_\\theta) = \\frac{1}{2} \\cdot \\frac{\\sum_{i=1}^{N-1} \\sum_{j=i+1}^{N} \\cos(h_i, h_j)}{N \\cdot (N-1)}\\label{eq:distance}\n\\end{equation}\n\nwhere $\\cos(h_i, h_j)$ refers to the cosine similarity between the embedding $h_i$ and $h_j$. 5 different responses are sampled with a temperature of 1.0 to 160 queries in AlpacaEval (i.e., $K=5, N=160$) using Phi-2 and Llama-2 trained with \\texttt{ORPO} and DPO. We report the results in Table \\ref{tab:diversity}.\n\n\\paragraph{Per Input Diversity (PID)} We average the input-wise average cosine similarity between the generated samples with Equation \\ref{eq:pid} to assess the per-input diversity. In Table \\ref{tab:diversity}, \\texttt{ORPO} models have the highest average cosine similarity in the first column for both models, which implies the lowest diversity per input. This indicates that \\texttt{ORPO} generally assigns high probabilities to the desired tokens, while DPO has a relatively smoother logit distribution.\n\n\\begin{equation}\n    \\text{PID}_D(\\theta) = \\frac{1}{N}\\sum_{i=1}^N D(\\mathcal{O}^i_\\theta)\\label{eq:pid}\n\\end{equation}\n\n\\paragraph{Across Input Diversity (AID)} Using 8 samples generated per input, we sample the first item for each input and examine their inter cosine similarity with Equation \\ref{eq:aid} for across-input diversity. Unlike per-input diversity, it is noteworthy that Phi-2 (\\texttt{ORPO}) has lower average cosine similarity in the second row of Table \\ref{tab:diversity}. We can infer that \\texttt{ORPO} triggers the model to generate more instruction-specific responses than DPO.\n\n\\begin{equation}\n    \\text{AID}_D(\\theta) = D \\left( \\bigcup\\limits_{i=1}^{N}\\mathcal{O}^i, _{\\theta, j=1} \\right)\\label{eq:aid}\n\\end{equation}\n\n\\begin{table}[hbt!]\n\\centering\n\\resizebox{\\columnwidth}{!}{%\n\\begin{tabular}{l|cc}\n\\hline \\hline\n\\multicolumn{1}{c|}{}       & \\textbf{Per Input}$\\downarrow$ & \\textbf{Across Input}$\\downarrow$ \\\\ \\hline\nPhi-2 + SFT + DPO    & \\textbf{0.8012}             & 0.6019                \\\\\nPhi-2 + ORPO         & 0.8909             & \\textbf{0.5173}                \\\\ \\hline \nLlama-2 + SFT + DPO  & \\textbf{0.8889}             & 0.5658               \\\\\nLlama-2 + ORPO       & 0.9008             & \\textbf{0.5091}                \\\\ \\hline \\hline\n\\end{tabular}%\n}\n\\caption{Lexical diversity of Phi-2 and Llama-2 fine-tuned with DPO and \\texttt{ORPO}. Lower cosine similarity is equivalent to higher diversity. The highest value in each column within the same model family is bolded.}\n\\label{tab:diversity}\n\\end{table}\n\n\\section{Discussion}\n\nIn this section, we expound on the theoretical and computational details of \\texttt{ORPO}. The theoretical analysis of $\\texttt{ORPO}$ is studied in Section \\ref{subsec:why}, which will be supported with the empirical analysis in Section \\ref{subsec:odds_trend}. Then, we compare the computational load of DPO and \\texttt{ORPO} in Section \\ref{subsec:compute}.\n\n\\subsection{Comparison to Probability Ratio}\\label{subsec:why}\n\nThe rationale for selecting the odds ratio instead of the probability ratio lies in its stability. The probability ratio for generating the favored response $y_w$ over the disfavored response $y_l$ given an input sequence $x$ can be defined as Equation \\ref{eq:pr}. \n\\begin{equation}\n    \\textbf{PR}_\\theta(y_w, y_l) = \\frac{P_\\theta(y_w|x)}{P_\\theta(y_l|x)}\\label{eq:pr}\n\\end{equation}\n\nWhile this formulation has been used in previous preference alignment methods that precede SFT \\citep{rafailov2023direct, azar2023general}, the odds ratio is a better choice in the setting where the preference alignment is incorporated in SFT as the odds ratio is more sensitive to the model's preference understanding. In other words, the probability ratio leads to more extreme discrimination of the disfavored responses than the odds ratio.\n\nWe visualize this through the sample distributions of the log probability ratio $\\log \\textbf{PR}(X_2|X_1)$ and log odds ratio $\\log \\textbf{OR}(X_2|X_1)$. We sample 50,000 samples each with Equation \\ref{eq:unif} and plot the log probability ratio and log odds ratio in Figure \\ref{fig:compare}. We multiply $\\beta$ for the probability ratio as it is practiced in the probability ratio-based methods and report the cases where $\\beta=0.2$ and $\\beta=1.0$.\n\\begin{gather}\n    X_1, X_2 \\sim \\text{Unif}(0, 1)\\label{eq:unif} \\\\\n    Y \\sim \\beta \\left( \\log X_1 - \\log X_2 \\right)\\label{eq:sample_prob} \\\\\n    Y \\sim \\log \\frac{X_1}{1 - X_1} - \\log \\frac{X_2}{1 - X_2}\\label{eq:sample_odds}\n\\end{gather}\n\\begin{figure}[hbt!]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{figure/ratio.pdf}\n    \\caption{Sampled distribution of $\\log \\textbf{PR}(X_2|X_1)$ and $\\log \\textbf{OR}(X_2|X_1)$. $\\log \\textbf{OR}(X_2|X_1)$ has a wider range given the same input probability pairs $(X_1, X_2)$.}\n    \\label{fig:compare}\n\\end{figure}\n\nRecalling that the log sigmoid function is applied to the log probability ratio and log odds ratio, each ratio's scale determines the expected margin between the likelihood of the favored and disfavored styles when the loss is minimized. In that sense, the contrast should be relatively extreme to minimize the log sigmoid loss when $\\textbf{PR}(X_2|X_1)$ is inputted instead of $\\textbf{OR}(X_2|X_1)$ to the log sigmoid function, regarding the sharp distribution of $\\log \\textbf{PR}(X_2|X_1)$ in Figure \\ref{fig:compare}. This results in overly suppressing the logits for the tokens in the disfavored responses in the setting where SFT and preference alignment are incorporated, as the model is not adapted to the domain. We empirically support this analysis through the ablation study in Appendix \\ref{ap:rpor}. Therefore, the odds ratio is a better choice when the preference alignment is done with SFT  due to the mild discrimination of disfavored responses and the prioritizing of the favored responses to be generated.\n\nThroughout fine-tuning, minimizing the log sigmoid loss leads to either $\\textbf{PR}(X_2|X_1)$ or $\\textbf{OR}(X_2|X_1)$ to be larger. This is equivalent to the rejected responses' token-wise likelihood, which will generally get smaller. In this context, it is essential to avoid an overly extreme contrast. This precaution is especially important given the sharp distribution of $\\log \\textbf{PR}(X_2|X_1)$ depicted in Figure \\ref{fig:compare}. The excessive margin could lead to the unwarranted suppression of logits for tokens in disfavored responses within the incorporated setting, potentially resulting in issues of degeneration.\n\n\\subsection{Minimizing $\\mathcal{L}_{OR}$}\\label{subsec:odds_trend}\nWe demonstrate that models trained with \\texttt{ORPO} learned to reflect the preference throughout the training process. We monitored the log probabilities of the chosen and rejected responses and the log odds ratio with $\\lambda = 1.0$. With the same dataset and model as Figure \\ref{fig:sft}, Figure \\ref{fig:log_odds} shows that the log probability of rejected responses is diminishing while that of chosen responses is on par with Figure \\ref{fig:sft} as the log odds ratio increases. This indicates that \\texttt{ORPO} is successfully preserving the domain adaptation role of SFT while the penalty term $L_{OR}$ induces the model to lower the likelihood of unwanted generations. We discuss the effect of $\\lambda$ in Equation \\ref{eq:main} in Appendix \\ref{ap:ablation}, studying the proclivity of the log probability margin between the favored and disfavored responses with respect to $\\lambda$.\n\\begin{figure}[hbt!]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{figure/training2_new.pdf}\n    \\caption{Average log-likelihood for chosen and rejected responses and log odds ratio per batch. The odds consistently increase during training with \\texttt{ORPO}.}\n    \\label{fig:log_odds}\n\\end{figure}\n\n\\subsection{Computational Efficiency}\\label{subsec:compute}\n\nAs depicted in Figure \\ref{fig:main}, \\texttt{ORPO} does not require a reference model, unlike RLHF and DPO. In that sense, \\texttt{ORPO} is computationally more efficient than RLHF and DPO in two perspectives: 1) memory allocation and 2) fewer FLOPs per batch.\n\nThe reference model ($\\pi_{SFT}$) in the context of RLHF and DPO denotes the model trained with supervised fine-tuning (SFT), which will be the baseline model for updating the parameters with RLHF or DPO \\citep{ziegler2020finetuning,rafailov2023direct}. Thus, two $\\pi_{SFT}$s, a frozen reference model and the model undergoing tuning, are required during training. Furthermore, in theory, two forward passes should be calculated for each model to acquire the logits for the chosen and rejected responses. In other words, four forward passes happen in total for a single batch. \nOn the other hand, a reference model is not required in \\texttt{ORPO} as $\\pi_{SFT}$ is directly updated. This leads to half the number of forward passes required for each batch during training.\n\n\\section{Conclusion}\n\nIn this paper, we introduced a reference-free monolithic preference alignment method, odds ratio preference optimization (\\texttt{ORPO}), by revisiting and understanding the value of the supervised fine-tuning (SFT) phase in the context of preference alignment. \\texttt{ORPO} was consistently preferred by the fine-tuned reward model against SFT and RLHF across the scale, and the win rate against DPO increased as the size of the model increased. Furthermore, we validate the scalability of \\texttt{ORPO} with 2.7B and 7B pre-trained language models by exceeding the larger state-of-the-art instruction-following language models in AlpacaEval. Specifically, Mistral-\\texttt{ORPO}-$\\alpha$ and Mistral-\\texttt{ORPO}-$\\beta$ achieved 11.33\\% and 12.20\\% in $\\text{AlpacaEval}_{2.0}$, 7.23 and 7.32 in MT-Bench, thereby underscoring the efficiency and effectiveness of \\texttt{ORPO}. We release fine-tuning code and model checkpoints for Mistral-\\texttt{ORPO}-$\\alpha$ and Mistral-\\texttt{ORPO}-$\\beta$ to aid reproducibility.\n\n\\section*{Limitations}\n\nWhile conducting a comprehensive analysis of the diverse preference alignment methods, including DPO and RLHF, we did not incorporate a more comprehensive range of preference alignment algorithms. We leave the broader range of comparison against other methods as future work, along with scaling our method to over 7B models. In addition, we will expand the fine-tuning datasets into diverse domains and qualities, thereby verifying the generalizability of our method in various NLP downstream tasks. Finally, we would like to study the internal impact of our method on the pre-trained language model, expanding the understanding of preference alignment procedure to not only the supervised fine-tuning stage but also consecutive preference alignment algorithms.\n\n\\setlength{\\jot}{10pt}\n\n\\onecolumn\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2305.18290v3.tex",
        "arXiv-2402.01306v4.tex",
        "arXiv-2403.07691v2.tex"
    ],
    "group_id": "group_10",
    "response": "### Title: Advances in Preference-Based Alignment of Large Language Models\n\n### Introduction\n\nThe field of large language model (LLM) alignment focuses on tailoring the behavior of these models to better align with human preferences, ensuring they generate helpful, ethical, and factually accurate content. Historically, LLMs have been trained using supervised fine-tuning (SFT) on specific datasets, followed by reinforcement learning from human feedback (RLHF) to optimize their behavior based on human preferences. However, RLHF is complex and often unstable, necessitating a more straightforward and efficient approach. Recent advancements have introduced methods like Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO) to streamline the alignment process. This summary will explore three papers that present innovative approaches to preference-based alignment, each offering a unique perspective on how to optimize LLMs to adhere to human preferences more effectively.\n\nThe first paper, \"Direct Preference Optimization: Your Language Model is Secretly a Reward Model,\" proposes a method that bypasses the need for explicit reward modeling and reinforcement learning, simplifying the alignment process. The second paper, \"KTO: Model Alignment as Prospect Theoretic Optimization,\" leverages Kahneman and Tversky's prospect theory to optimize LLMs based on human biases, using a binary signal of whether an output is desirable. The third paper, \"ORPO: Monolithic Preference Optimization without Reference Model,\" introduces a monolithic approach to preference optimization that does not require a separate reference model, further simplifying the alignment pipeline. Each paper addresses the challenges of aligning LLMs with human preferences, aiming to make the process more stable, efficient, and scalable.\n\n### Main Content of Each Paper\n\n#### Direct Preference Optimization (DPO)\n\nThe paper \"Direct Preference Optimization: Your Language Model is Secretly a Reward Model\" introduces a novel method for fine-tuning LLMs to align with human preferences without the need for explicit reward modeling or reinforcement learning. The authors leverage a theoretical framework that maps reward functions to optimal policies, enabling the optimization of a policy directly from preference data. The key insight is that the optimal policy can be extracted in closed form, allowing for a single-stage training process that is both stable and computationally lightweight. The DPO algorithm optimizes a policy using a simple binary cross-entropy objective, effectively bypassing the need for a separate reward model and sampling from the policy during training.\n\nThe authors conduct experiments on sentiment modulation, summarization, and dialogue tasks, demonstrating that DPO can match or exceed the performance of PPO-based RLHF. For instance, in the controlled sentiment generation task, DPO achieves the highest expected reward for all KL values, outperforming PPO even when PPO has access to ground truth rewards. In summarization and dialogue tasks, DPO matches or improves response quality while being simpler to implement and train. The paper also highlights the robustness of DPO to sampling temperatures, indicating that it can generalize well across different settings.\n\n#### Kahneman-Tversky Optimization (KTO)\n\nIn \"KTO: Model Alignment as Prospect Theoretic Optimization,\" the authors present a method that aligns LLMs with human preferences by directly optimizing the utility of generations rather than maximizing the log-likelihood of preferences. This approach is grounded in Kahneman and Tversky's prospect theory, which explains human decision-making biases such as loss aversion. The authors define a class of alignment objectives called human-aware losses (HALOs), which incorporate these biases into the optimization process. KTO is a specific instance of a HALO that uses a binary signal to indicate whether an output is desirable or undesirable, making it easier to scale in real-world applications.\n\nThe KTO method is derived from the canonical Kahneman-Tversky value function, which is adjusted for numerical stability and risk aversion. The authors show that KTO can match or exceed the performance of DPO across various scales, from 1B to 30B parameters. They also demonstrate that KTO can handle extreme data imbalances, using up to 90% fewer desirable examples compared to DPO. Additionally, KTO can achieve good performance without a supervised fine-tuning (SFT) phase, which is often required for DPO. The authors emphasize that the success of KTO can be attributed to its ability to reflect human biases in decision-making, leading to more effective and stable alignment.\n\n#### Odds Ratio Preference Optimization (ORPO)\n\nThe paper \"ORPO: Monolithic Preference Optimization without Reference Model\" introduces a monolithic approach to preference optimization that does not require a separate reference model. ORPO incorporates an odds ratio-based penalty into the conventional negative log-likelihood (NLL) loss, enabling the model to distinguish between favored and disfavored response styles. This method is designed to be computationally efficient, requiring fewer forward passes and less memory allocation compared to DPO and RLHF.\n\nThe authors conduct experiments on various datasets, including Anthropic's HH-RLHF and UltraFeedback, and demonstrate that ORPO can outperform SFT, PPO, and DPO across different scales. For example, on the UltraFeedback dataset, ORPO-trained models achieve higher win rates against SFT and PPO models, with performance gains increasing as the model size increases. ORPO also shows better lexical diversity, generating more instruction-specific responses while maintaining diversity across inputs. The authors attribute the success of ORPO to its ability to effectively penalize disfavored responses without overly suppressing the likelihood of favored responses, thereby preserving the domain adaptation role of SFT.\n\n### Commonalities and Innovations\n\nAll three papers address the challenge of aligning LLMs with human preferences, but they do so using different methodologies. DPO and KTO both leverage theoretical frameworks to optimize policies directly from preference data, bypassing the need for explicit reward modeling. However, DPO uses a binary cross-entropy objective, while KTO incorporates human biases into its optimization process through a value function derived from prospect theory. ORPO, on the other hand, integrates preference optimization into the SFT phase, using an odds ratio-based penalty to differentiate between favored and disfavored response styles.\n\nThe common thread among these methods is the desire to streamline the alignment process, making it more stable and computationally efficient. DPO and KTO both eliminate the need for reinforcement learning, which is often unstable and resource-intensive. ORPO further simplifies the process by removing the need for a separate reference model, making it more memory-efficient and easier to implement. Each method offers a unique perspective on how to optimize LLMs, with DPO focusing on a direct policy optimization approach, KTO incorporating human biases into its loss function, and ORPO integrating preference optimization into SFT.\n\n### Comparison of Results and Discussion\n\nThe experiments conducted in each paper demonstrate the effectiveness of their respective methods in aligning LLMs with human preferences. DPO shows superior performance in sentiment modulation and matches or improves response quality in summarization and dialogue tasks. KTO, on the other hand, matches or exceeds DPO performance across various scales, from 1B to 30B parameters. ORPO outperforms SFT, PPO, and DPO in win rates and lexical diversity, especially for larger models. \n\nOne significant difference is the computational efficiency and stability of each method. DPO is noted for its simplicity and stability, avoiding the need for reinforcement learning and explicit reward modeling. KTO is computationally efficient and can handle extreme data imbalances, but it requires a more complex value function and hyperparameter tuning. ORPO is the most efficient, requiring no separate reference model and showing better performance in win rates and lexical diversity, especially when aligned with larger models.\n\n### Conclusion\n\nThe main findings of these papers highlight the importance of inductive biases in preference-based alignment methods and suggest that there is no one-size-fits-all approach to aligning LLMs with human preferences. DPO offers a stable and straightforward method for optimizing policies directly from preference data, while KTO leverages human biases to optimize utility directly. ORPO integrates preference optimization into SFT, offering a monolithic approach that is both efficient and effective. These methods collectively underscore the need for more efficient and stable alignment techniques, which can be crucial for deploying LLMs in real-world applications.\n\nFuture research directions include expanding the application of these methods to other modalities and tasks, studying the impact of different value functions and reference point distributions on alignment performance, and exploring the generalizability of these methods across diverse datasets and domains. Additionally, there is a need to address the potential biases in preference datasets and ensure that alignment methods do not inadvertently propagate these biases. The development of more robust and diverse HALOs could also be an important area of future work, as different tasks and domains may require different inductive biases for optimal performance."
}