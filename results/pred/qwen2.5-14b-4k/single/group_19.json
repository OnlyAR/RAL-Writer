{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Bridging the Gap: A Unified Video Comprehension Framework \\\\for Moment Retrieval and Highlight Detection}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nVideo Moment Retrieval (MR) and Highlight Detection (HD) have attracted significant attention due to the growing demand for video analysis.\nRecent approaches treat MR and HD as similar video grounding problems and address them together with transformer-based architecture.\nHowever, we observe that the emphasis of MR and HD differs, with one necessitating the perception of local relationships and the other prioritizing the understanding of global contexts.\nConsequently, the lack of task-specific design will inevitably lead to limitations in associating the intrinsic specialty of two tasks.\nTo tackle the issue, we propose a \\textbf{U}nified \\textbf{V}ideo \\textbf{COM}prehension framework (UVCOM) to bridge the gap and jointly solve MR and HD effectively.\nBy performing progressive integration on intra and inter-modality across multi-granularity, UVCOM achieves the comprehensive understanding in processing a video.\nMoreover, we present multi-aspect contrastive learning to consolidate the local relation modeling and global knowledge accumulation via well aligned multi-modal space.\nExtensive experiments on QVHighlights, Charades-STA, TACoS, YouTube Highlights and TVSum datasets demonstrate the effectiveness and rationality of UVCOM which outperforms the state-of-the-art methods by a remarkable margin.\nCode is available at \\href{https://github.com/EasonXiao-888/UVCOM}{https://github.com/EasonXiao-888/UVCOM}\n\n\\end{abstract}\\section{Introduction}\n\\label{sec:intro}\nVideo has emerged as a highly favored multi-medium format on the internet with its diverse content.\nThis significant surge in online video encourages users to adjust their strategies for accessing desired video contents.\nInstead of spending time-consuming efforts inspecting the whole video, they are more inclined to directly obtain particular clips of interest through language descriptions.\nThis shift in user preference gives rise to two significant research topics: Video Moment Retrieval~\\cite{ctrl,early_mr_1,man,recent_mr_1,recent_mr_2}, focuses on locating the specific moment, and Highlight Detection~\\cite{video2gif,Yao_2016,cvs,LIM-s,joint_va}, is dedicated to identifying segments of high saliency.\n\nActually, it is apparent that two tasks share many common characteristics, \\textit{e.g.}, identifying relevant video segments in response to textual expressions.   \nIn light of the above, Lei \\textit{et.al.}~\\cite{momentdetr} first proposes a novel dataset named QVHighlights and a basic framework called Moment-DETR to jointly solve both tasks.\nUMT~\\cite{umt} incorporates extra audio modality and QD-DETR~\\cite{qddetr} produces text-query dependent video representation to achieve better performance.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{fig/teaser.pdf}\n    \\caption{\\textbf{Illustration of the intrinsic characteristics of Moment Retrieval and Highlight Detection.} We visualize the attention map of the same video under two tasks. \n    The attention map for MR takes on strip-like patterns, indicating the emphasis of local relations. In contrast, it is in band-like format for HD, which signifies the focus on global information.}\n    \\label{fig:teaser}\n    \\vspace{-10pt}\n\\end{figure}\nThe above-mentioned methods simply model MR and HD as a multi-task problem and mainly concentrate on utilizing non-specific strategy to solve them. \nIn particular, they all adopt a straightforward way to train and optimize both tasks together with general design, \\textit{e.g.}, transformer-based models.\nHowever, we revisit the characteristics of MR/HD and discover that there exists a gap between them as illustrated in \\cref{fig:teaser}. \nwhich leads to  \nthe challenge in consistent performance on both tasks, \\textit{i.e.}, achieving precise moment localization and accurate highlight-ness estimation simultaneously.\n\nTherefore, we consider that the design of framework should follow two principles to alleviate the above weakness: 1) \\textit{\\textbf{Local Relation Activation:}} MR necessitates the understanding of local relationships within the video to accurately localize specific segments. 2) \\textit{\\textbf{Global Knowledge Accumulation:}} The objective of HD is to fit the saliency distribution of the entire video, emphasizing the importance of global context (in \\cref{fig:teaser}). Based on the principles, we propose a \\textbf{U}nified \\textbf{V}ideo \\textbf{Com}prehension Framework (UVCOM) to seamlessly integrate the emphasis of MR and HD, which effectively bridges the gap and achieves great performance on both tasks consistently. \nSpecifically, we first design a novel Comprehensive Integration Module (CIM) to progressively facilitate the integration on intra and inter-modality across multi-granularity.\nCIM first efforts to propagate the aggregated semantic phrases from the text into the visual feature to realize local relationship perception.\nThen, it accumulates global information from video by utilizing the moment-awareness feature as an intermediary.\nWith a comprehensive view of the entire video, CIM facilitates the understanding of particular intervals and highlight contents, which is beneficial to identify the desired moment and non-related ones. \nFurthermore, we introduce a multi-aspect contrastive learning which incorporates clip-text alignment to consolidate the local relation modeling, and video-linguistic discrimination to enhance the quality of accumulated global information.\n\nWe conduct extensive experiments on five popular MR/HD benchmarks  \nto validate the effectiveness of our framework and the results show that UVCOM notably outperforms existing methods for all benchmarks.\n\nOverall, our contributions are summarized as follows:\n\\begin{itemize}\n    \\item Based on our investigation into the emphasis of Moment Retrieval and Highlight Detection, we present two principles for framework design. Guided by them, we propose a Unified Video Comprehension Framework called UVCOM to effectively bridge the gap between two tasks.\n    \\item In UVCOM, a Comprehensive Integration Module (CIM) is designed to perform progressive intra and inter-modality interaction across multi-granularity, which achieves locality perception of temporal and multi-modal relationships as well as global knowledge accumulation of the entire video.\n    \\item Without bells and whistles, our method outperforms all existing state-of-the-art methods by a remarkable margin, \\textit{e.g.}, $+5.97$\\% in R1@0.7 for MR than UniVTG~\\cite{univtg} on TACoS~\\cite{tacos} and $+3.31$\\% in HIT@1 for HD than QD-DETR~\\cite{qddetr} on QVHighlights~\\cite{momentdetr}.\n\\end{itemize}\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=\\linewidth]{fig/framework.pdf}\n    \\caption{\\textbf{Overview of UVCOM.} Based on the exploration of MR and HD, we propose a unified video comprehension framework guided by the design principles. Specifically, the model takes a video with language description as input. After encoding and early-fusion process, we design a Comprehensive Integration Module (CIM) to achieve subsequent progressive integration on intra and inter-modality across multi-granularity. Finally, the multi-task heads output the moment spans for MR and saliency scores for HD.}\n    \\label{fig:framework}\n    \\vspace{-10pt}\n\\end{figure*}\n\\section{Related Work}\n\\label{sec:related_work}\n\\paragraph{Moment Retrieval.} Moment Retrieval is a task that aims at retrieving the target moment, \\textit{i.e.}, one~\\cite{ctrl} or many~\\cite{tvr} continuous intervals in a video given the text description. Generally, the model will focus more on the relationship across adjacent frames for better localization. Previous works retrieve video intervals into two perspectives: proposal-based and proposal-free. The proposal-based methods~\\cite{hendricks, bpn, ctrl, mpn} follow the propose-then-rank pipeline, where they first generate candidate proposals then rank them based on matching scores. Liu \\textit{et al.}~\\cite{liumeng-2018} and Hendricks \\textit{et al.}~\\cite{hendricks} utilize sliding windows to scan the entire video for candidate proposals generation and calculate the similarity with textual embedding for selection. \nTo enhance temporal awareness, MAN~\\cite{man} and Gao \\textit{et al.}~\\cite{gao_2021} both introduce graph convolution network to capture temporal dependency across adjacent moments. \nOn the other hand, the proposal-free methods~\\cite{yuan_2019, mun_2020, rodriguez_2020, VSLNET, li_2021, Tang_2022} directly regress the start and end timestamp via video-text interaction. Yuan \\textit{et al.}~\\cite{yuan_2019} and Mun \\textit{et al.}~\\cite{mun_2020} generate the temporal coordinates of sentence by multi-modal co-attention mechanism. Furthermore, Rodriguez \\textit{et al.}~\\cite{rodriguez_2020} utilizes a simple dynamic filter instead of cross attention to match video and text embedding. \n\n\\vspace{-10pt} \n\\paragraph{Highlight Detection.} Highlight Detection aims to identify highlights or important segments with high potential appeal in a video. Compared with Moment Retrieval, It is necessary for the model to associate the whole video content for fitting saliency distribution of each clip. Many prior works~\\cite{Yao_2016, video2gif, Yao_2016, LIM-s, cvs} adopt ranking formulation where they rank the important segments with higher score. Video2Gif~\\cite{video2gif} trains a generic highlight predictor to produce GIF from videos. Rochan \\textit{et al.}~\\cite{Rochan_2020} designs a task-specific highlight detectors to automatically create highlights from the user history. They all use the fine-grained annotations for training which are labor-intensive and expansive. Therefore, Xiong \\textit{et al.}~\\cite{LIM-s} proposes to learn highlights only with video-level label. Recently, Badamdorj \\textit{et al.}~\\cite{joint_va} elaborates on fusing visual and audio content to generate better video representations.\n\nMR and HD share many similar properties. Moment-DETR~\\cite{momentdetr} puts forward a novel dataset which first includes two tasks and provides a simple DETR-based~\\cite{detr} network. To improve the query quality, UMT~\\cite{umt} proposes to adopt audio, visual and text content for query generation. Furthermore, QD-DETR~\\cite{qddetr} exploits the textual information by involving video-text pair negative relationship learning, achieving greater performance. However, previous methods simply train and optimize two tasks without considering the different emphasis of each task. To address this issue, we propose a novel unified framework UVCOM that effectively associates the speciality of MR and HD to achieve comprehensive understanding. \n\\section{Method}\n\nGiven a video of $L$ clips $\\{v_1, v_2, \\dots, v_l\\}$ and a textual expression of $N$ words $\\{e_1, e_2, \\dots, e_n\\}$, the goal of MR is to localize the most relevant moment with the center coordinate and duration, while HL is to generate the saliency score distribution for the whole video.\n\n\\subsection{Visual-Text Encoding}\n\\label{sec:visual-text encoding}\n\\paragraph{Visual Encoder.} Following previous works~\\cite{momentdetr,umt,qddetr,univtg}, we utilize the pretrained backbone, \\textit{e.g.}, SlowFast~\\cite{slowfast}, video encoder of CLIP~\\cite{clip} and I3D~\\cite{i3d} to extract visual features $\\mathcal{F}_v \\in \\mathbb{R}^{L \\times D}$ of the video. Note that D demotes the channel.\n\\vspace{-12pt}\n\\paragraph{Language Encoder.} Simultaneously, text encoder of CLIP is adopted to encode the linguistic expression into the textual embedding $\\mathcal{F}_{t} \\in \\mathbb{R}^{N \\times D}$.\n \nWith the visual and textual features, we apply a bidirectional transformer-based encoder to perform the early fusion.\nIt coarsely encodes features in different modalities and outputs preliminary aligned visual and textual representations.  \n\n\\subsection{Comprehensive Integration Module}\n\\label{sec:cim}\nAfter getting the visual and textual representations, we design a Comprehensive Integration Module (CIM) to perform progressive intra and inter-modality integration across multi-granularity. \nSpecifically, we leverage Expectation-Maximum (EM) Attention~\\cite{em} on associating inner-modality content to generate the moment-wise visual features and phrase-wise textual features, respectively. \nThen we propose Local Relation Perception (LRP) module to unify temporal relationship modeling and inter-modality fusion, which reformulates the temporal and modality inter-connection to enhance the locality perception.\nFinally, we utilize a standard encoder to produce the video-wise feature by integrating the correlation between moment and clip-wise visual features.\n\\vspace{-10pt}\n\\paragraph{Dual Branches Intra-Modality Aggregation.} A video usually contains more than one event and irrelevant background scenes.\nThe same scenario happens in textual descriptions where insignificant words and unconstrained expressions may cause potential ambiguity. \nTo tackle the problem, we propose to utilize RBF-kernel based EM Attention~\\cite{em,rskp} to aggregate the clip/word-level features. \nAs shown in \\cref{fig:framework}, it is a dual-branches structure.\nThe clip-to-moment branch aims at incorporating the relationship of each clip to enhance the desired event representations while suppressing the background noise.\nMeanwhile, the word-to-phrase branch is to emphasize the referred moment description by accumulating contextual information.\n\nSpecifically, we fit the distribution of $\\mathcal{F}_v$ and $\\mathcal{F}_t$ by a separated Gaussian Mixture Model~\\cite{gmm} to generate the compact moment and phrase-level representations via the centroid of Gaussians.\nTaking $\\mathcal{F}_v$ as an example,\nwe utilize a linear superposition of $n_v$ Gaussians to capture the statistics of $f_v^i \\in \\mathbb{R}^D$ (the $i$-th snippet of $\\mathcal{F}_v$):\n\\begin{equation}\np(f_v^i)=\\sum_{k=1}^{n_v} z_{k}^v\\mathcal{N} (f_v^i|\\mu_k, \\Sigma _k),\n\\end{equation}\nwhere $z_{k}^v \\in \\mathbb{R}$, $\\mu_k \\in \\mathbb{R}^D$ and $\\Sigma_k \\in \\mathbb{R}^{D\\times D}$ denote the weight, mean and covariance of $k$-th Gaussian basis for the clip-to-moment branch.\nWe substitute the covariance with an identity matrix $I$ for simplification and employ the radial basis function (RBF Kernel) $\\mathcal{K}(f_v^i,\\mu_k)$ to estimate the posterior probability $\\mathcal{N} (f_v^i|\\mu _k, \\mathcal{I} )$:\n\\begin{equation}\n    \\mathcal{K}(f_v^i,\\mu_k) = exp(- \\lambda \\left \\| f_v^i -\\mu_k \\right \\| _2^2),\n\\end{equation}\nwhere $\\lambda > 0$ is an adjustable hyper-parameter to control the distribution.\nAfterwards, at $t$-th iteration, we update the weight $Z^{(t)} \\in \\mathbb{R}^{L\\times n_v}$ in the E Step and re-estimate $\\mu^{(t)} \\in \\mathbb{R}^{n_v\\times D}$ in the M step, which can be formulated as:\n\\begin{equation}\n\\label{t-step}\n    \\mu^{(t)} = \\mathrm{Norm_1}(Z^{(t)})^TF_v,\\quad t\\in \\left \\{  1,\\dots,T\\right \\}.\n\\end{equation}\nFurthermore, in contrast to conventional cluster methods that only involve iterative update, the initialized means $\\mu^{(0)}$ we set are learnable.\nTherefore, they can effectively capture the feature distribution of the dataset through the standard back-propagation.\n\nAfter $t$ iterations, we obtain the fine-grained moment-wise representation $\\mathcal{F}_m$ from $\\mu^{(t)}$ which fully aggregates the contextual information.\nSimilarly, we operate the above steps on word-to-phrase branch to generate the phrase-level linguistic feature $\\mathcal{F}_p \\in \\mathbb{R}^{n_t \\times D}$, where $n_t$ indicates the number of Gaussian basis in word-to-phrase branch. \n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{fig/lrp_module.pdf}\n    \\caption{\\textbf{Illustration of Local Relation Perception (LRP) module.} We first process the visual feature $\\mathcal{F}_v$ with a Conv1D Block. Then we develop a Bidirectional Modality Random Walk (BMRW) algorithm to exploit the power of fine-grained multi-modal interaction. The affinity $\\mathcal{Z}$ is generates by scaled dot product: $\\mathcal{Z}=\\lambda_{z}\\mathcal{F'}_v^{(0)} (\\mathcal{F}_p^{(0)})^{\\top }$.}\n    \\label{fig:lrp_module}\n    \\vspace{-10pt}\n\\end{figure}\n\\vspace{-10pt}\n\\paragraph{Local Relation Perception.}\nPrevious methods~\\cite{momentdetr, umt, qddetr} directly perform cross-modal fusion between clip and word-level features, disregarding the temporal relation and valuable semantic interaction across different granularities. Without the information from adjacent clips, the simple and coarse clip-word fusion will easily deviate the model from focusing on the relevant boundary clips, causing incorrect localization. \nTo address the aforementioned weakness, we design a Local Relation Perception (LRP) module to excavate both temporal and inter-modality relationships.\nAs shown in \\cref{fig:framework}, we first utilize a temporal convolution block to improve the locality perception of clip-level features, which can be formulated as:\n\\begin{equation}\n    \\mathcal{F'}_{v} = \\mathrm{Conv}\\left(\\mathcal{F}_v\\right) + \\mathcal{F}_v.\n\\end{equation}\nSince simply incorporate clip-level relation may introduce local redundancy, we leverage fine-grained inter-modal interaction to re-calibrate the attention for activating the relevant moments. \nIntuitively, a straightforward approach is to utilize cross-attentive mechanism~\\cite{qddetr, umt} to perform inter-modal interaction.\nNevertheless, the complex scenario in an untrimmed video, \\textit{e.g.}, footage transitions and irrelevant events, will increase the likelihood of attention drift which leads to the undesirable local activation.\nMoreover, although phrase-wise linguistic features specify referred moment description and alleviate the impact of noise in contrast to word-wise one, it may potentially contribute to attention drift due to the irrelevant accumulated words.\nTherefore, inspired by~\\cite{rskp,Giannis2020,Grady2006}, we design a  \nbidirectional modality random walk (BMRW) algorithm to mitigate the mentioned drawbacks and fully exploit the power of the fine-grained multi-modal interaction.\nIt propagates the textual prior into the visual features for highlighting the corresponding local context and suppressing unrelated ones.\nSimultaneously, linguistic features are refined through the incorporation of updated visual content. \nAs shown in \\cref{fig:lrp_module}, there are multiple iterations in BMRW where two modalities features learn collaboratively in visual-linguistic shared embedding space until convergence.\n\nFormally, we first define the $\\mathcal{F'}_v$, $\\mathcal{F}_p$ as initial features $\\mathcal{F'}_v^{(0)}$, $\\mathcal{F}_p^{(0)}$ at $0$-th iteration and formulate affinity $\\mathcal{Z}$ by scaled dot product: $\\mathcal{Z}=\\lambda_{z}\\mathcal{F'}_v^{(0)} (\\mathcal{F}_p^{(0)})^{\\top }$, where $\\lambda_{z}$ is the scaling factor.\nAt $t$-th iteration, the phrase-wise linguistic feature $\\mathcal{F}_p^{(t)}$ is updated by the original feature $\\mathcal{F}_p^{(0)}$ and the visual output $\\mathcal{F'}_v^{(t-1)}$ from previous iteration: \n\\begin{equation}\n    \\label{txt_t_iter}\n    \\mathcal{F}_p^{(t)}=\\omega \\mathrm{Norm1}(\\mathcal{Z})^{\\top }\\mathcal{F'}_v^{(t-1)}+(1-\\omega )\\mathcal{F}_p^{(0)},\n\\end{equation}\nSubsequently, it is projected into the temporal-awareness feature $\\mathcal{F'}_{v}^{(t)}$:\n\\begin{equation}\n    \\label{vid_t_iter}\n    \\mathcal{F'}_v^{(t)}=\\omega \\mathcal{Z}\\mathcal{F}_p^{(t)} + (1-\\omega )\\mathcal{F'}_v^{(0)},\n\\end{equation}\nwhere $\\omega \\in (0,1)$ is the factor which controls the degree of modalities fusion.\nThen, we substitute $\\mathcal{F}_p^{(t)}$ into \\cref{vid_t_iter} to derive the iterative update formula of $\\mathcal{F'}_v^{(t)}$:\n\\begin{equation}\n    \\mathcal{F'}_v^{(t)} = (\\omega^2A)^{t} \\mathcal{F'}_v^{(0)} + (1-\\omega )\\sum_{i=0}^{t-1} (\\omega^2A)^{i}(\\omega \\mathcal{Z}\\mathcal{F}_p^{(0)}+\\mathcal{F'}_v^{(0)}),\n\\end{equation}\nwhere $A$ denotes $\\mathcal{Z}\\mathrm{Norm1}(\\mathcal{Z})^{\\top}$. Intuitively, the moment-specific regions of visual features can be fully activated by the guidance of textual features after multiple iterations.\nMoreover, to avoid the potential issue of unexpected gradient and high computation cost, we use an approximate inference function based on Neumann Series~\\cite{neuman} when $t \\to \\infty$:\n\\begin{equation}\n    \\label{new_vid}\n    \\mathcal{F'}_v^{(\\infty )}=(1-\\omega )(I-\\omega ^2 A)^{-1}(\\omega \\mathcal{Z} \\mathcal{F}_p^{(0)}+ \\mathcal{F'}_v^{(0)}).\n\\end{equation}\n\nIn this manner, the model realizes a synergistic temporal and inter-modality relation integration and generates a more comprehensive visual representation $\\mathcal{F}_v^{new}$, \\textit{i.e.}, $\\mathcal{F'}_v^{(\\infty )}$ in~\\cref{new_vid}.\n\n\\vspace{-10pt}\n\\paragraph{Global Knowledge Accumulation.}\nAs illustrated in \\cref{fig:teaser}, Highlight Detection prioritizes global information of videos.\nQD-DETR~\\cite{qddetr} uses a saliency token to capture general information. \nHowever, the input-agonist design might cause the inferior perception of the text-related intervals due to the non-referential search area.\nTo mitigate the concern, we propose to use the moment-aware feature as intermediate guidance to accumulate the global knowledge of a video.  \nSpecifically, we derive the most relevant snippet $\\mathcal{F'}_m$ by measuring the similarity between the moment-wise $\\mathcal{F}_m $ and phrase-wise embeddings $\\mathcal{F}_p$. Then, a stack of transformer encoder layers~\\cite{transformer} are utilized to excavate the correlation between $\\mathcal{F'}_m$ and $\\mathcal{F}_v^{new}$. The overall process is:\n\\begin{equation}\n    \\mathcal{F}_v^{g}, \\mathcal{F}_v^{l} = Encoder(Concat[ \\mathcal{F'}_m,\\mathcal{F}_v^{new}]).\n\\end{equation}\nConsequently, the semantic snippet is obliged to focus on the referred moment and suppress the non-target response, which eventually produces the video-wise feature $\\mathcal{F}_v^{g} \\in \\mathbb{R}^{1\\times D}$. In addition, $\\mathcal{F}_v^{l} \\in \\mathbb{R}^{L\\times D}$ is greatly enriched by the supplement of global information.  \n\n\\subsection{Multi-Aspect Contrastive Learning}\n\\label{sec:MACL}\nAs discussed in \\cref{sec:cim}, CIM can better accomplish local relation enhancement in temporal and inter-modality as well as global knowledge accumulation of a video. It is anticipated that the explicit supervision of each objective will further consolidate the effectiveness. To this end, we introduce multi-aspect contrastive learning, which is in two folds:  \n\\paragraph{Clip-Text Alignment.} This loss bridges the semantic gap between the textual expression and the clip-level features, which further improves the quality of local relation modeling. Specifically, we first average $\\mathcal{F}_t$ to get the sentence-level textual embedding $\\mathcal{F'}_t \\in \\mathbb{R}^{1 \\times D}$ and then measure the relevance with clip-level visual representation $\\mathcal{F}_v^{new}$:\n\\begin{equation}\n    S_{ct} = \\frac{\\mathcal{F}_v^{new}\\cdot \\mathcal{F'}_t^{\\top } }{\\left \\| \\mathcal{F}_v^{new} \\right \\|\\cdot \\left \\|  \\mathcal{F'}_t \\right \\| }. \n\\label{cta}\n\\end{equation}\nFinally, we compute the contrastive loss by matrix multiplication:\n\\begin{equation}\n    \\mathcal{L}_{cta} = -\\mathrm{LogSoftmax}\\left(S_{ct}\\right)\\cdot G_{ct},\n\\label{cta_loss}\n\\end{equation}\nwhere $G_{ct}$ is annotated to $1$ for relevant clips and $0$ for others.\n\\paragraph{Video-Linguist Discrimination.} It aims at constructing the fine-grained multi-modal joint space where video-level visual feature $\\{\\mathcal{F}_{v(i)}^g\\}_{i=1}^{B}$ closens relevant sentence-level textual representation $\\{\\mathcal{F'}_{t(i)}\\}_{i=1}^{B}$ while distances unrelated ones within a batch $B$.\nSimilar to~\\cite{clip}, the whole process can be formulated as:  \n\\begin{equation}\n    \\mathcal{L}_{vld} = -\\sum_{i=1}^{B}\\mathrm{Log}\\frac{exp\\left(\\mathcal{F}_{v(i)}^g \\cdot \\mathcal{F'}_{t(i)}^{\\top}\\right)}{\\sum_{j=1}^B exp\\left(\\mathcal{F}_{v(j)}^g \\cdot \\mathcal{F'}_{t(j)}^{\\top}\\right)}.\n\\end{equation}\n\n\\subsection{Prediction Heads and Loss Function}\n\\label{sec:phlf}\n\\paragraph{Multi-Task Prediction Heads.}\nAs depicted in~\\cref{fig:framework}, there are two simple heads built on top of the Comprehensive Integration Module for Moment Retrieval and Highlight Detection respectively.\nSimilar to~\\cite{momentdetr,qddetr,eatr}, Moment Retrieval Head comprises a standard transformer decoder where we leverage $\\mathcal{F}_t$ as the query to generate a series of moment spans $P_m$.\nHighlight Detection Head consists of two groups of single fully-connected layer for linear projection.\nAccordingly, we get the prediction saliency scores $P_{s} \\in \\mathbb{R}^{L\\times 1} $:\n\\begin{equation}\n    P_s = \\frac{\\mathcal{F}_v^{g} w_g^{\\top } \\cdot\\mathcal{F}_v^{l} w_l^{\\top }}{\\sqrt[]{d} },\n\\end{equation}\nwhere $w_{g}$ and $w_{l} \\in \\mathbb{R}^{d\\times D}$ are learnable weights of two Layers.\n\n\\paragraph{Total Loss.}\nWe supervise our framework by four groups of training objective functions.\nFor MR, $L1$ loss and $GIoU$ loss are adopted to measure the disparity between GT moment $G_m$ and prediction spans $P_m$ : \n\\begin{equation}\n    \\mathcal{L}_{MR} = \\lambda_{gIoU}\\mathcal{L}_{gIoU}(P_m,G_m) + \\lambda_{L1}\\mathcal{L}_{L1}(P_m,G_m).\n\\end{equation}\nMoreover, the loss functions for HD consist of margin ranking loss $\\mathcal{L}_{margin}$ and rank-aware loss $\\mathcal{L}_{rank}$ following~\\cite{qddetr}. Both losses work in tandem to ensure the predicted saliency scores $P_s$ conform to the ground truth scores $G_s$ :\n\\begin{equation}\n    \\mathcal{L}_{HD} = \\lambda_{HD}\\left[\\mathcal{L}_{margin}(P_s,G_s) + \\mathcal{L}_{rank}(P_s,G_s)]\\right.\n\\end{equation}\nInspired by~\\cite{qddetr, dong2017}, we involve hard samples into training process for diversifying the formulations of local and global relationships of different video-text pairs. Briefly, we categorize the lowest relevance between video and text as hard samples and suppress their saliency scores $P_s^{hard}$ during training:   \n\\begin{equation}\n    \\mathcal{L}_{hard} = -\\lambda_{hard}\\mathrm{Log}(1-P_s^{hard})\n\\end{equation}\n\n\\noindent In addition, the objective of multi-aspect contrastive learning promotes semantic associations between text descriptions and visual contents of multi-granularity:\n\\begin{equation}\n    \\mathcal{L}_{con} = \\lambda_{cta}\\mathcal{L}_{cta} + \\lambda_{vld}\\mathcal{L}_{vld}.\n\\end{equation}\nGenerally, the total loss is expressed as:\n\\begin{equation}\n    \\mathcal{L}_{total} = \\mathcal{L}_{HD} + \\mathcal{L}_{MR} + \\mathcal{L}_{hard} + \\mathcal{L}_{con}.\n\\end{equation}\nThe $\\lambda$ above are hyper-parameters for balancing the losses.\\section{Experiments}\n\n\\subsection{Datasets and Evaluation Metrics}\n\\paragraph{Datasets.} We evaluate our model on five prevalent MR/HD benchmarks: QVHighlights~\\cite{momentdetr}, Charades-STA~\\cite{cha}, TaCoS~\\cite{tacos}, TVSum~\\cite{tvsum} and YouTube Highlights~\\cite{youtubehl}. Due to the space limitation, the details of each datasets are included in the~\\cref{sup:datasets}.\n\n\\vspace{-7pt}\n\\paragraph{Metrics.} \nFollowing~\\cite{momentdetr,umt,detr}, we measure the performance of our model by the same criteria for QVhighlights, Charades-STA, TACoS, YouTube Highlights and TVSum. For descriptions of the metrics corresponding to datasets, please see the~\\cref{sup:metrics}.\n\n\\subsection{Implementation Details}\n\\paragraph{Pre-extracted Features.} For a fair comparison, we take the same features of video, text and audio from corresponding pretrained feature extractors, \\textit{e.g.}, SlowFast~\\cite{slowfast}, CLIP~\\cite{clip}, PANN~\\cite{pann}. For more details please refer to~\\cref{sup:features}.\n\n\\begin{table}[t]\n\\footnotesize\n\\setlength{\\tabcolsep}{0pt}\n\\begin{tabularx}{\\linewidth}{@{\\hspace{0.1cm}}p{2.2cm}p{0.85cm}<{\\centering}p{0.85cm}<{\\centering}p{1.0mm}<{\\centering}p{0.85cm}<{\\centering}p{0.85cm}<{\\centering}p{0.85cm}<{\\centering}p{1.0mm}<{\\centering}p{0.85cm}<{\\centering}p{0.8cm}<{\\centering}}\n\\toprule\n& \\multicolumn{6}{c}{\\textbf{MR}} & & \\multicolumn{2}{c}{\\textbf{HD}} \\\\\n\\cmidrule{2-7} \\cmidrule{9-10}\n& \\multicolumn{2}{c}{R$1$} & & \\multicolumn{3}{c}{mAP} & & \\multicolumn{2}{c}{$\\geq$ Very Good} \\\\\n\\cmidrule{2-3} \\cmidrule{5-7} \\cmidrule{9-10}\n\\vspace{-0.73cm}\\hspace{0.1cm}\\textbf{Method} & @$0.5$ & @$0.7$ & & @$0.5$ & @$0.75$ & Avg. & & mAP & HIT@$1$ \\\\\n\\midrule\nM-DETR \\cite{momentdetr} & $52.89$ & $33.02$ & & $54.82$ & $29.40$ & $30.73$ & & $35.69$ & $55.60$ \\\\\n{UMT}$\\dagger$~\\cite{umt} & $56.23$ & $41.18$ & & $53.83$ & $37.01$ & $36.12$ & & $38.18$ & $59.99$ \\\\\n{UniVTG}~\\cite{univtg} & $58.86$ & $40.86$ & & $57.60$ & $35.59$ & $35.47$ & & $38.20$ & $60.96$ \\\\\n{MH-DETR}~\\cite{mhdetr} & $60.05$ & $42.28$ & & $60.75$ & $38.13$ & $38.38$ & & $38.22$ & $60.51$ \\\\\n{QD-DETR$\\dagger$}~\\cite{qddetr} & $63.06$ & $45.10$ & & $63.04$ & $40.1$ & $40.19$ & & $39.04$ & $62.87$ \\\\\n{EaTR}~\\cite{eatr} & $61.36$ & $45.79$ & & $61.86$ &  $41.91$ & $41.74$ & & $37.15$ & $58.65$ \\\\\n\\rowcolor{gray!10}\n{UVCOM} & $\\mathbf{63.55}$ & $\\mathbf{47.47}$ & & $\\mathbf{63.37}$ & $\\mathbf{42.67}$ & $\\mathbf{43.18}$ & & $\\mathbf{39.74}$ & $\\mathbf{64.20}$ \\\\\n\\rowcolor{gray!10}\n{UVCOM $\\dagger$} & $\\mathbf{63.81}$ & $\\mathbf{48.70}$ & & $\\mathbf{64.47}$ & $\\mathbf{44.01}$ & $\\mathbf{43.27}$ & & $\\mathbf{39.79}$ & $\\mathbf{64.79}$ \\\\\n\\midrule\n\\multicolumn{8}{c}{\\hspace{2cm}\\textit{With ASR Captions Pretrain}} \\\\\n\\midrule\nM-DETR \\cite{momentdetr} & $59.78$ & $40.33$ & & $60.51$ & $35.36$ & $36.14$ & & $37.43$ & $60.17$ \\\\\n{UMT}~\\cite{umt} & $60.83$ & $43.26$ & & $57.33$ & $39.12$ & $38.08$ & & $39.12$ & $62.39$ \\\\\n{QD-DETR}~\\cite{qddetr} & $64.10$ & $46.10$ & & $64.30$ & $40.50$ & $40.62$ & & $38.52$ & $62.27$ \\\\\n\\rowcolor{gray!10}\n{UVCOM } & $\\mathbf{64.53}$ & $\\mathbf{48.31}$ & & $\\mathbf{64.78}$ & $\\mathbf{43.65}$ & $\\mathbf{43.80}$ & & $\\mathbf{39.98}$ & $\\mathbf{65.58}$ \\\\\n\\bottomrule\n\\end{tabularx}\n\\captionsetup{font={small}}\n\n\\caption{\\textbf{Jointly MR and HD results on QVHighlights test split.} $\\dagger$ indicates training with audio modality. \\textit{With ASR Caption Pretrain} denotes models pretrained on ASR captions~\\cite{momentdetr}.}\n\n\\vspace{-2pt}\n\\label{tab:qvhl}\n\\end{table}\n\\footnotesize\n     \\setlength{\\tabcolsep}{0pt}\n    \\begin{tabularx}{\\linewidth}{@{\\hspace{0.1cm}}p{2.0cm}|@{\\hspace{0.1cm}}p{1.47cm}\n<{\\centering}p{1.47cm}<{\\centering}p{0.1cm}|@{\\hspace{0.1cm}}p{1.47cm}\n<{\\centering}p{1.47cm}\n<{\\centering}p{1.47cm}\n<{\\centering}}\n    \\toprule\n    \\multirow{2}{*}{\\vspace{-0.1cm}\\textbf{Method}}  & \\multicolumn{2}{c}{\\textbf{Charades-STA}} &  & \\multicolumn{2}{c}{\\textbf{TACoS}}\n    \\\\\n     \\cmidrule{2-3} \\cmidrule(l{0.2cm}){4-6}\n      & R1@0.5 & R1@0.7  & & R1@0.5 & R1@0.7 \\\\\n    \\midrule\n    2D TAN~\\cite{2d-tan}  & $46.02$ & $27.50$  & & $27.99$ & $12.92$ \\\\\n    VSLNet~\\cite{VSLNET} & $42.69$ & $24.14$  & & $23.54$ & $13.15$ \\\\\n    M-DETR~\\cite{momentdetr} & $53.63$ & $31.37$ & & $24.67$ & $11.97$ \\\\\n    QD-DETR~\\cite{qddetr} & $57.31$ & $32.55$  & & -- & -- \\\\\n    UniVTG~\\cite{univtg} & $58.01$ & $35.65$ & & $34.97$ & $17.35$ \\\\\n    \\midrule\n    \\rowcolor{gray!10}\n    UVCOM & $\\mathbf{59.25}$ & $\\mathbf{36.64}$ & & $\\mathbf{36.39}$ & $\\mathbf{23.32}$ \\\\\n    \\bottomrule\n    \\end{tabularx}\n    \\makeatother\\caption{\\textbf{MR results on Charades-STA test split and TACoS test split}. The pre-extracted features are from SlowFast~\\cite{slowfast} and CLIP~\\cite{clip}.}\n    \\label{tab:tacos_cha}\n\\vspace{-4mm}\n\\end{table}\\begin{figure*}[t]\n \\begin{minipage}[t]{0.4\\textwidth}\n  \\centering\n  \\scriptsize\n\\renewcommand\\tabcolsep{0pt}\n\\footnotesize\n\\begin{tabularx}{\\linewidth}{@{\\hspace{1mm}}p{2cm}|@{\\hspace{0.5mm}}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}}\n\\toprule\n\\textbf{Method} & {Dog} & {Gym.} & {Par.} & {Ska.} & {Ski.} & {Sur.} & \\textbf{Avg.} \\\\\n\\midrule\nGIFs \\cite{video2gif} & $30.8$ & $33.5$ & $54.0$ & $55.4$ & $32.8$ & $54.1$ & $46.4$ \\\\\nLSVM \\cite{youtubehl} & $60.0$ & $41.0$ & $61.0$ & $62.0$ & $36.0$ & $61.0$ & $53.6$ \\\\\nLIM-S \\cite{LIM-s} & $57.9$ & $41.7$ & $67.0$ & $57.8$ & $48.6$ & $65.1$ & $56.4$ \\\\\nSL-Module \\cite{sl_module} & ${70.8}$ & ${53.2}$ & $77.2$ & ${72.5}$ & ${66.1}$ &${76.2}$ & ${69.3}$ \\\\\nMINI-Net$\\dagger$~\\cite{mn} & $58.2$ & $61.7$ & $70.2$ & $72.2$ & $58.7$ & $65.1$ & $64.4$ \\\\\nTCG$\\dagger$ \\cite{TCG} & $55.4$ & $62.7$ & $70.9$ & $69.1$ & $60.1$ & $59.8$ & $63.0$ \\\\\nJoint-VA$\\dagger$ \\cite{joint_va} & $64.5$ & $71.9$ & $80.8$ & $62.0$ & $73.2$ & $78.3$ & $71.8$ \\\\\n{UMT}$\\dagger$\\cite{umt} & $65.9$ & $75.2$ & ${81.6}$ & $71.8$ & $72.3$ & ${82.7}$ & $74.9$ \\\\ \n{UniVTG}~\\cite{univtg} &  ${71.8}$ & ${76.5}$ & $73.9$ & ${73.3}$ & ${73.2}$ & ${82.2}$ & ${75.2}$ \\\\ \n\\midrule\n\\rowcolor{gray!10}\n{UVCOM$^{1}$} &  $\\mathbf{73.8} $ & $\\mathbf{77.1}$ & ${75.7}$ & $\\mathbf{75.3}$ & $\\mathbf{74.0}$ & $\\mathbf{82.7}$ & $\\mathbf{76.4}$  \\\\\n\\rowcolor{gray!10}\n{UVCOM$^{2}$} &  $\\mathbf{66.5} $ & $\\mathbf{77.4}$ & $\\mathbf{82.8}$ & $\\mathbf{78.7}$ & $\\mathbf{74.2}$ & $\\mathbf{84.6}$ & $\\mathbf{77.4}$  \\\\\n\\bottomrule\n\\end{tabularx}\n\\vspace{-1em}\n\\captionsetup{font={small}}\n\\makeatletter\\def\\@captype{table}\\makeatother\\caption{\\small{\\textbf{HD results of mAP on YouTube HL.} $\\dagger$ denotes using audio modality.} $1$ and $2$ indicate using the same visual and textual features of UniVTG and UMT.}\n\\label{tab:youtube}\n  \\end{minipage}\n  \\hspace{0.65cm}\n  \\begin{minipage}[t]{0.558\\textwidth}\n   \\centering\n   \\scriptsize\n\\renewcommand\\tabcolsep{0pt}\n\\footnotesize\n\\begin{tabularx}{1\\linewidth}{@{\\hspace{1mm}}p{1.9cm}|@{\\hspace{0.4mm}}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}p{0.7cm}<{\\centering}}\n\\toprule\n\\textbf{Method} &  {VT} & {VU} & {GA} & {MS} & {PK} & {PR} & {FM} & {BK} & {BT} & {DS} & \\textbf{Avg.} \\\\\n\\midrule\nsLSTM \\cite{sLSTM} & ${41.1}$ & ${46.2}$ & $46.3$ & $47.7$ & $44.8$ & $46.1$ & $45.2$ & $40.6$ & $47.1$ & $45.5$ & $45.1$ \\\\\nLIM-S \\cite{LIM-s} & $55.9$ & $42.9$ & $61.2$ & $54.0$ & $60.4$ & $47.5$ & $43.2$ & $66.3$ & $69.1$ & $62.6$ & $56.3$ \\\\\nTrailer \\cite{trailer} &$61.3$ &$54.6$ & $65.7$ & $60.8$ & $59.1$ & ${70.1}$ & $58.2$ & $64.7$ & $65.6$ & ${68.1}$ & $62.8$ \\\\\nSL-Module \\cite{sl_module} & ${86.5}$ & ${68.7}$ & ${74.9}$ & $\\underline{86.2}$ & ${79.0}$ & $63.2$ & ${58.9}$ & ${72.6}$ & ${78.9}$ & $64.0$ & ${73.3}$ \\\\\nMINI-Net$\\dagger$ \\cite{mn} & $80.6$ & $68.3$ & $78.2$ & $81.8$ & $78.1$ & $65.8$ & $57.8$ & $75.0$ & $80.2$ & $65.5$ & $73.2$ \\\\\nTCG$\\dagger$ \\cite{TCG} & $85.0$ & $71.4$ & $81.9$ & $78.6$ & $80.2$ & $75.5$ & $71.6$ & $77.3$ & $78.6$ & $68.1$ & $76.8$ \\\\\nJoint-VA$\\dagger$ \\cite{joint_va} & $83.7$ & $57.3$ & $78.5$ & ${86.1}$ & $80.1$ & $69.2$ & $70.0$ & $73.0$ & $\\mathbf{97.4}$ & $67.5$ & $76.3$ \\\\\n{UniVTG}~\\cite{univtg}& $83.9$ & $85.1$ & $\\underline{89.0}$ & ${80.1}$ & ${84.6}$ & ${81.4}$ & ${70.9}$ & $\\underline{91.7}$ & ${73.5}$ & ${69.3}$ & ${81.0}$\\\\\n{UMT}$\\dagger$\\cite{umt} & ${87.5}$ & ${81.5}$ & ${88.2}$ & $78.8$ & ${81.5}$ & $\\mathbf{87.0}$ & ${76.0}$ & ${86.9}$ & ${84.4}$ & $\\mathbf{79.6}$ &\n${83.1}$ \\\\\nQD-DETR \\cite{qddetr} & $\\mathbf{88.2}$ & $\\underline{87.4}$ & $85.6$ & $85.0$ & $\\underline{85.8}$ & $86.9$ & $\\underline{76.4}$ & $91.3$ & $\\underline{89.2}$ & $73.7$ & $\\underline{85.0}$ \\\\\n\\midrule\n\\rowcolor{gray!10}\n{UVCOM} &  $\\underline{87.6}$ & $\\mathbf{91.6}$ & $\\mathbf{91.4}$ & $\\mathbf{86.7}$ & $\\mathbf{86.9}$ & $\\underline{86.9}$ & $\\mathbf{76.9}$ & $\\mathbf{92.3}$ & ${87.4}$ & $\\underline{75.6}$ & $\\mathbf{86.3}$\\\\\n\\bottomrule\n\\end{tabularx}\n\\vspace{-1em}\n\\captionsetup{font={small}}\n\\makeatletter\\def\\@captype{table}\\makeatother\\caption{\\small{\\textbf{HD results of Top-5 mAP on TVSum.} $\\dagger$ denotes using audio modality. The $2$-nd performance values are highlighted by \\underline{underline}.}}\n\\label{tab:tvsum}\n\\end{minipage}\n\\vspace{-1em}\n\\end{figure*}\\begin{table*}[t]\n\\vspace{-12pt}\n\\begin{minipage}[c]{\\textwidth}\n    \\begin{minipage}{0.44\\textwidth}\n    \\makeatletter\\def\\@captype{table}\n    \\centering\n    \\footnotesize\n    \\setlength{\\tabcolsep}{4.2pt}\n    \\begin{tabular}{c c c@{\\hspace{0.4cm}} c c c c c c}\n    \\toprule\n    \\multirow{3}{*}{\\vspace{-0.2cm}\\textbf{CIM}} & &\\multirow{3}{*}{\\vspace{-0.2cm}\\textbf{MCL}} & \\multicolumn{3}{c}{\\textbf{MR}} & & \\multicolumn{2}{c}{\\textbf{HD}}\n    \\\\\n     \\cmidrule{4-6} \\cmidrule{8-9}\n    & & & R1 & R1 & mAP & & \\multirow{2}{*}{mAP} & \\multirow{2}{*}{HIT@1} \\\\\n    & & & @0.5 & @0.7 & Avg. & & & \\\\\n    \\midrule\n    & & & $61.55$ & $44.84$ & $40.08$ & & $37.10$ & $62.0$ \\\\\n    \\checkmark & & & $62.84$ & $48.77$ & $43.6$ & & $39.33$ & $62.97$ \\\\\n     & & \\checkmark  & $60.77$ & $44.06$ & $40.48$ & & $38.81$ & $62.06$ \\\\\n    \\checkmark & & \\checkmark & $\\mathbf{65.10}$ & $\\mathbf{51.81}$ & $\\mathbf{45.79}$ & & $\\mathbf{40.03}$ & $\\mathbf{63.29}$ \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Effectiveness of the proposed modules.}}\n    \\label{tab:ablation_modules}\n    \\end{minipage}\n    \\begin{minipage}{0.53\\textwidth}\n    \\vspace{10pt}\n    \\makeatletter\\def\\@captype{table}\n    \\centering\n    \\footnotesize\n    \\setlength{\\tabcolsep}{4.3pt}\n    \\hspace{-2mm}\n    \\begin{tabular}{c c c c c c c c c}\n    \\toprule\n    \\multirow{3}{*}{\\vspace{-0.2cm}\\textbf{DBIA}} & \\multirow{3}{*}{\\vspace{-0.2cm}\\textbf{LRP}} & \\multirow{3}{*}{\\vspace{-0.2cm}\\textbf{GKA}}& \\multicolumn{3}{c}{\\textbf{MR}} & & \\multicolumn{2}{c}{\\textbf{HD}}\n    \\\\\n     \\cmidrule{4-6} \\cmidrule{8-9}\n    & & & R1 & R1 & mAP & & \\multirow{2}{*}{mAP} & \\multirow{2}{*}{HIT@1} \\\\\n    & & & @0.5 & @0.7 & Avg. & & & \\\\\n    \\midrule\n    &  &  & $60.77$ & $44.06$ & $40.48$ & & $38.81$ & $62.06$ \\\\\n     &  &\\checkmark & $62.32$ & $46.71$ & $41.03$ & & $38.73$ & $62.58$ \\\\\n      & \\checkmark & & $62.06$ & $46.45$ & $41.42$ & & $38.57$ & $62.45$ \\\\\n    \\checkmark & & \\checkmark & $63.74$ & $49.16$ & $43.45$ & & $39.54$ & $\\mathbf{64.26}$ \\\\\n    \\checkmark& \\checkmark &  & $64.71$ & $50.0$ & $43.69$ & & $39.69$ & $63.16$ \\\\\n     & \\checkmark &\\checkmark & $64.84$ & $50.0$ & $44.02$ & & $39.58$ & $64.13$ \\\\\n    \\checkmark& \\checkmark & \\checkmark & $\\mathbf{65.10} $ & $\\mathbf{51.81} $ & $\\mathbf{45.79} $ & & $\\mathbf{40.03}$ & $63.29 $ \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\hspace{-0.05\\textwidth}\n    \\caption{\\textbf{Effects of the components designed of proposed CIM module.}}\n    \\label{tab:ablation_CIM}\n    \\end{minipage}\n    \\begin{minipage}{0.45\\textwidth}\n    \\makeatletter\\def\\@captype{table}\n    \\centering\n    \\footnotesize\n    \\setlength{\\tabcolsep}{4.2pt}\n    \\vspace{-2.5mm}\n    \\begin{tabular}{c@{\\hspace{0.4cm}} c c@{\\hspace{0.5cm}} c c c c c c}\n    \\toprule\n    & \\multirow{3}{*}{\\vspace{-0.2cm}\\textbf{Method}} & & \\multicolumn{3}{c}{\\textbf{MR}} & & \\multicolumn{2}{c}{\\textbf{HD}}\n    \\\\\n     \\cmidrule{4-6} \\cmidrule{8-9}\n      & & & R1 & R1 & mAP & & \\multirow{2}{*}{mAP} & \\multirow{2}{*}{HIT@1} \\\\\n      & & & @0.5 & @0.7 & Avg. & & & \\\\\n    \\midrule\n    & Average & & $63.48$ & $49.87$ & $44.10$ & & $39.81$ & $63.16$ \\\\\n     & K-Means & & $62.58$ & $48.39$ & $43.13$ & & $39.47$ & $62.26$ \\\\\n    & EM-Att & & $64.32$ & $50.26$ & $44.49$ & & $39.82$ & $\\mathbf{64.0}$ \\\\\n    & EM-Att$\\dagger$ & & $\\mathbf{65.10}$ & $\\mathbf{51.81} $ & $\\mathbf{45.79}$ & & $\\mathbf{40.03}$ & $63.29$ \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Impact of various aggregation methods.} $\\dagger$ indicates the EM Attention module with RBF kernel. }\n    \\vspace{5pt}\n    \\label{tab:cluster}\n    \\end{minipage}\n    \\hspace{0.025\\textwidth}\n    \\begin{minipage}{0.51\\textwidth}\n    \\makeatletter\\def\\@captype{table}\n    \\centering\n    \\footnotesize\n    \\vspace{-1.5mm}\n    \\setlength{\\tabcolsep}{4.2pt}\n    \\begin{tabular}{c@{\\hspace{0.4cm}} c c@{\\hspace{0.4cm}} c c c c c c}\n    \\toprule\n    & \\multirow{3}{*}{\\vspace{-0.2cm}\\textbf{Method}} & & \\multicolumn{3}{c}{\\textbf{MR}} & & \\multicolumn{2}{c}{\\textbf{HD}}\n    \\\\\n     \\cmidrule{4-6} \\cmidrule{8-9}\n      & & & R1 & R1 & mAP & & \\multirow{2}{*}{mAP} & \\multirow{2}{*}{HIT@1} \\\\\n      & & & @0.5 & @0.7 & Avg. & & & \\\\\n    \\midrule\n    & Cross Attention & & $63.03$ & $49.87$ & $43.79$ & & $39.63$ & $\\mathbf{63.94}$ \\\\\n    & BMRW & & $\\mathbf{65.10} $ & $\\mathbf{51.81} $ & $\\mathbf{45.79} $ & & $\\mathbf{40.03}$ & $63.29 $ \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Comparison of different modality interaction strategies.}}\n    \\label{tab:ablation_lrp}\n    \\end{minipage}\n\\end{minipage}\n\\vspace{-20pt}\n\\end{table*}\n\\vspace{-7pt}\n\\paragraph{Training Settings.} Our model is trained with AdamW optimizer where the learning rate is $1 \\times 10^{-4}$ and weight decay is $1 \\times 10^{-4}$ by default. The encoder of Global Knowledge Accumulation and the decoder of Moment Retrieval Head compose of three layers of transformer blocks.\nThe coefficients for losses are set to \n$\\lambda_{cta}=0.5,\\lambda_{hard}=1,\\lambda_{vld}=0.5,\\lambda_{HD}=1,\\lambda_{gIoU}=1,\\lambda_{L1}=10$ in default. \nDue to space limitations, please see~\\cref{sup:training details} for more training details.\n\n\\subsection{Main Result}\n\\paragraph{QVHighlights.} We compare our method to previous methods on QVHighlights in \\cref{tab:qvhl}.\nBenefiting from the comprehensive understanding of the video, our UVCOM achieves new state-of-the-art performance on different settings and shows a significant margin across all metrics.\nSpecifically, our approach outperforms EaTR~\\cite{eatr} by $2.25$\\% on the average of all metrics.\nIncorporating with video and audio modality, UVCOM yields a clear improvement of $3.6\\%$ in R1@0.7, $4\\%$ in mAP@0.75 for MR and $2\\%$ in HID@1 for HD compared to QD-DETR~\\cite{qddetr}.\nFurthermore, with ASR caption pretraining, UVCOM achieves the greatest performance on more stringent metrics, \\textit{e.g.}, $43.8\\%$ in Avg. mAP for MR and $39.98$\\% in Avg. mAP for HD, demonstrating the effectiveness of our method. \n\n\\vspace{-10pt}\n\\paragraph{Charades-STA \\& TACoS.}\nIn order to evaluate the performance of our method in precise moment localization, we report the results on Charades-STA and TACoS benchmarks.\nAs depicted in \\cref{tab:tacos_cha}, UVCOM outperforms QD-DETR~\\cite{qddetr} by about $4\\%$ R1@0.7 using SlowFast and CLIP features in Charades-STA dataset while boosts $6\\%$ R1@0.7 than UniVTG~\\cite{univtg} in TaCoS. It is worth noting that we also validate our model surpasses the existing SOTA methods using VGG features (see in~\\cref{sup:result_cha_vgg}).\n\n\\vspace{-10pt}\n\\paragraph{YouTube Highlights \\& TVSum.}\nFor Video Highlight Detection, we conduct experiments on TVSum and YouTube Highlights. Considering the fact that the scale and scoring criteria of TVSum is small and inconsistent, our method gains incoherently among domains.\nHowever, in~\\cref{tab:tvsum}, it still boost an improvement of $1.3\\%$ in Avg. mAP compared with the SOTA methods. As shown in \\cref{tab:youtube}, our method achieves $76.4$\\% and $77.4$\\% in Avg. mAP without audio source under different settings. \nNote that the features used in UniVTG~\\cite{univtg} and UMT~\\cite{umt} on YouTube Highlights are different. Therefore, we follow the same protocol of each for a fair comparison.\n\n\\begin{figure*}[h]\n\\vspace{3mm}\n    \\centering\n    \\includegraphics[width=\\linewidth]{fig/case.pdf}\n    \\caption{\\textbf{Visullization comparison on MR and HD.} QD indicates previous state-of-the-art method QD-DETR~\\cite{qddetr}}\n    \\label{fig:case_show}\n    \\vspace{-10pt}\n\\end{figure*}\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{fig/ablation_lrp.pdf}\n    \\caption{\\textbf{Illustration of different modality interaction strategies.}\n    The red bounding box indicates the relevant interval and the white bounding box denotes the start and end clips.}\n    \\label{fig:ablation-lrp}\n    \\vspace{-15pt}\n\\end{figure}\n\n\\subsection{Ablation Study}\nIn this section, we conduct a series of analysis experiments on the val split of QVHighlights benchmark and train the model from scratch without audio modality.\n\n\\paragraph{Component Analysis.}\nWe first verify the effectiveness of the proposed Comprehensive Integration Module (CIM) and Multi-Aspect Contrastive Learning (MCL).\nAs illustrated in \\cref{tab:ablation_modules}, both of them brings improvement and their combination contributes to better performance ,\\textit{i.e.}, $+5.71$\\% in Avg. mAP, which demonstrates the effectiveness of the comprehensive understanding.\nTo further investigate the validity of three modules involved in CIM, we provide additional experiments on Dual Branches Intra-Modality Aggregation (DBIA), Local Relation Perception (LRP) and Global Knowledge Accumulation (GKA).  \nAs shown in \\cref{tab:ablation_CIM}, since GKA facilitate the understanding of global context, the ablation of it leads to inferior performance on HD, \\textit{i.e.}, $-1.1\\%$ in HIT@1. Moreover, LRP brings a clear improvement of $+2.34\\%$ in Avg. mAP on MR, proving the enhancement on locality perception.  \n\n\\vspace{-12pt}\n\\paragraph{Aggregation Method.}\nWe study the impacts on various aggregation methods utilized in DBIA module.\nAs illustrated in \\cref{tab:cluster}, we believe the superiority of our RBF kernel based EM-Attention derives from two aspects: 1) Compared with ``Average\" and K-Means, our method\nenhances the desired moment representation while suppresses noises.\n2) RBF kernel maps features into a high-dimensional latent space while modeling the relationship within it, which is beneficial for the subsequent aggregation.\n\n\\vspace{-12pt}\n\\paragraph{Modality Interaction Strategy.}\nWe investigate the effects of different modality interaction strategies in Local Relation Perception. As shown in \\cref{tab:ablation_lrp}, replacing BMRW by cross attention mechanism results in $2\\%$ performance degradation, which demonstrates the effectiveness of BMRW.\nFurthermore, we provide visualization of features to prove the rationality of LRP. It can be seen in \\cref{fig:ablation-lrp} that the utilization of cross-attentive mechanism leads to the emergence of attention drift. In contrast, through iterative multi-modal learning in shared space, BMRW mitigates the issue, thereby facilitating more precise localization.\nMoreover, LRP achieves the local relation perception evidenced by clearer strip-like attention patterns in~\\cref{fig:ablation-lrp}.\n\n\\paragraph{Grounding Consistency.}\nBenefiting from the task-specific design, our method yields greater consistency in the joint solution of MR and HD.\nTo quantify the performance coherence, on one hand, we count the videos with accurate hightlight-ness estimation ($mAP_{\\text{HD}} > 0.8$) and calculate MR mAP for those videos as shown in~\\cref{fig:gap} (a). \nOn the other hand, we measure the HD mAP and quantities of videos with precise moment spans ($mAP_{\\text{MR}} > 0.8$) as shown in \\cref{fig:gap} (b). \nThe results demonstrate that UVCOM effectively bridges the gap between two tasks for which our method is superior on all statistics, \\textit{i.e.}, MR and HD precision as well as quantity.\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{fig/gap.pdf}\n    \\caption{\\textbf{Illustration of grounding consistency of MR and HD.} (a) indicates the videos collected by $mAP_{\\text{HD}} > 0.8$. (b) indicates the videos collected by $mAP_{\\text{MR}} > 0.8$.}\n    \\label{fig:gap}\n    \\vspace{-10pt}\n\\end{figure}\n\n\\subsection{Qualitative Results}\nAs shown in \\cref{fig:case_show}, The local-global enhancement and comprehensive understanding allows our method to accurately model the saliency distribution and localize timestamps of the moment precisely. Comparatively, without the explicit association of characteristics of two tasks, QD-DETR~\\cite{qddetr} struggles to handle simultaneously in complex scenarios.  \n\n\\section{Conclusion}\nIn light of the different emphasis on MR and HD, we propose a unified video comprehension framework called UVCOM under the guidance of design principles to effectively bridge the gap between two tasks.\nBy performing progressive intra and inter-modality interaction across multi-granularity, UVCOM achieves locality perception of temporal and multi-modal relationship as well as global knowledge accumulation of the entire video.\nMoreover, we introduce multi-aspect contrastive learning to provide the explicit supervision of above two objectives. \nExtensive studies validate our models comprehensive understanding of videos and show our UVCOM remarkably outperforms the existing state-of-the-art methods.\n\n\\noindent \\textbf{Limitations.} Our UVCOM has achieved comprehensive understanding of the untrimmed video and superior performance.\nHowever, there are some potential limitations, \\textit{e.g.}, unable to process auditory descriptions well.\nSince we just use a simple way to handle audio features instead of specific design, we think that the explicit design for audio features is an interesting future direction.\n\n{\n    \\small\n        }\n\\clearpage\n\\setcounter{page}{1}\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and Highlight Detection}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n    Video moment retrieval (MR) and highlight detection (HD) based on natural language queries are two highly related tasks, which aim to obtain relevant moments within videos and highlight scores of each video clip. Recently, several methods have been devoted to building DETR-based networks to solve both MR and HD jointly. These methods simply add two separate task heads after multi-modal feature extraction and feature interaction, achieving good performance. Nevertheless, these approaches underutilize the reciprocal relationship between two tasks. In this paper, we propose a task-reciprocal transformer based on DETR (TR-DETR) that focuses on exploring the inherent reciprocity between MR and HD. Specifically, a local-global multi-modal alignment module is first built to align features from diverse modalities into a shared latent space. Subsequently, a visual feature refinement is designed to eliminate query-irrelevant information from visual features for modal interaction. Finally, a task cooperation module is constructed to refine the retrieval pipeline and the highlight score prediction process by utilizing the reciprocity between MR and HD. Comprehensive experiments on QVHighlights, Charades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing state-of-the-art methods. Codes are available at \\url{https://github.com/mingyao1120/TR-DETR}.\\footnote{Note: This is a pre-print version of the paper. The final, copyrighted version of the paper can be accessed through the AAAI Digital Library.}\n\\end{abstract}\n\n\\section{Introduction}\nWith the ubiquity of digital devices and the expansion of the Internet, the number and variety of videos are rapidly increasing~\\cite{Foo_2023_CVPR}. How to quickly search out the desired moments from massive videos (called moment retrieval, MR)~\\cite{gao2017tall} and efficiently browse videos (called highlight detection, HD)~\\cite{DBLP:conf/mm/MolinoG18} according to the needs of users has attracted widespread attention. In practical applications, user needs can be expressed in natural language queries~\\cite{DBLP:conf/aaai/00010WLW22}. Due to the complexity of video content as well as the diversity of user needs, MR\\&HD based on user-provided natural language queries is extremely challenging.\n\nThe goal of MR is to precisely search for semantically related moments from whole videos guided by natural language queries~\\cite{DBLP:conf/cvpr/0006XQZT0YZW22}. The common pipeline of MR involves several steps. Firstly, pre-trained networks are utilized to extract features from the input video and text. Subsequently, cross-modal interaction is performed based on the extracted features to obtain the query relevance score of the candidate moment or the frame-level start-end probability of the relevant moment~\\cite{DBLP:journals/pami/ZhangSJZ23}. HD based on queries strives to assign highlight scores to each video clip based on considering the user needs~\\cite{DBLP:journals/tmm/GuoZJWLY22}. Existing methods~\\cite{liu2022umt,xiong2023dual} utilize transformers~\\cite{vaswani2017attention} or graph neural networks~\\cite{scarselli2008graph} to perform single-modal feature encoding or cross-modal interaction.\n\nDue to the task similarity between MR and HD based on queries, and the commonality between their methods involving multi-modal feature extraction, feature interaction, etc., some works~\\cite{lei2021detecting, DBLP:journals/corr/abs-2307-16715} have devoted to designing various multi-task networks for joint MR\\&HD. For example, Moment-DETR~\\cite{lei2021detecting} pioneers the application of DETR~\\cite{DBLP:conf/eccv/CarionMSUKZ20} for joint MR\\&HD. QD-DETR~\\cite{moon2023query} introduces a query-dependent video representation module, making moment predictions reliant on user queries. MH-DETR~\\cite{xu2023mh} introduces a pooling operation into the encoder and incorporates a cross-modality interaction module to fuse visual and query features. In these methods, two isolated task heads are added after the shared multi-modal feature extraction and feature interaction modules for joint MR\\&HD.\nThese methods generally focus on improving the discrimination of multi-modal feature extraction and feature interaction through a multi-task learning scheme, achieving good performance. However, the reciprocity between MR and HD tasks is ignored.\n\nFor MR, the highlight scores from HD based on user-provided queries can be utilized to assist in eliminating query-irrelevant clips, thereby boosting moment retrieval accuracy. In turn, for HD based on queries, the results of moment retrieval can be used to improve the understanding of videos and user needs. Therefore, MR and HD based on queries are reciprocal.\n\nTo fully exploit the reciprocal relationship between the two tasks, we propose a task-reciprocal transformer based on DETR, named TR-DETR, for joint MR\\&HD. Firstly, visual features and textual features are extracted from user-provided videos and queries through pre-trained networks. Then, we introduce a local-global multi-modal alignment module to perform local and global semantic alignment before modal interaction, respectively. This module encourages the model to distinguish video clips that are semantically similar but irrelevant to the query. Subsequently, we propose a visual feature refinement module for modal interaction, which employs aligned textual features to filter out query-irrelevant information in visual features to avoid it interfering with joint features. Finally, to exploit the complementarities between MR and HD, we propose a task cooperation module consisting of HD2MR and MR2HD. The former explicitly infuses highlight score information into the moment retrieval process, enhancing localization accuracy. The latter exploits localization outcomes to derive clip-level relevant scores, offering visual support for highlight detection.\nExtensive experiments on QVHighlights~\\cite{lei2021detecting}, Charades-STA~\\cite{gao2017tall} and TVSum~\\cite{song2015tvsum} demonstrate that the proposed TR-DETR outperforms the state-of-the-art methods. The contributions of this paper are summarized as follows:\n\n\\begin{itemize}\n    \\item We highlight the reciprocity between MR and HD. In addition, we introduce an innovative TR-DETR network that leverages this reciprocity between tasks to optimize performance.\n    \\item We introduce the local and global alignment regulators. These regulators are designed to facilitate semantic alignment between video clips and the query, which serves to generate discriminative joint representations.\n    \\item To explore the intrinsic complementarity between the two tasks, we construct a task cooperation module. This module explicitly exploits the complementarity between MR and HD by injecting highlight scores into the moment retrieval pipeline and using the retrieved moments to refine the initial highlight distribution.\n\\end{itemize}\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=1\\textwidth]{Figure/method_clipped.pdf} % Replace with the actual image file name and path\n\\caption{The proposed TR-DETR involves several key steps. Initially, two frozen pre-trained networks are employed to extract visual and textual features from videos and queries. Subsequently, a local-global multi-modal alignment module is constructed to effectively align the extracted visual and textual features. Then, the visual features are refined under the guidance of textual features for obtaining discriminative joint features. Finally, a task cooperation module is implemented to enhance prediction outcomes based on task reciprocity. Additionally, two multi-head self-attention components share weights.}\n\\label{method}\n\\end{figure*}\n\n\\section{Related Works}\n\\subsection{MR and HD}\n\nVideo moment retrieval is originally introduced by the literature~\\cite{gao2017tall}, with the objective of retrieving moments from a video based on a given natural language query. Moment retrieval typically includes two types of methods: proposal-based and proposal-free methods. In the proposal-based methods, candidate moments are initially generated through techniques such as sliding windows~\\cite{gao2017tall}, proposal generation networks~\\cite{xu2019multilevel}, or 2D-Maps~\\cite{zhang2020learning}. These candidates are subsequently ranked based on the similarity scores to the query, where the candidate with the highest score is used as the result. Although these methods have high accuracy, they necessitate additional pre- and post-processing steps, introducing computational redundancy. Moreover, their performance heavily relies on the quality of candidate moments. On the other hand, proposal-free methods~\\cite{ghosh2019excl, zhang2021natural,mun2020local} directly predict start-end probabilities for target moments within a video, which eliminates the need to rank a large number of candidate moments, thereby improving training efficiency.\n\nIn contrast, highlight detection concentrates on measuring the significance of each clip within a given video. Slightly different from moment retrieval, highlight detection initially is proposed as a single-modal task and does not rely on text queries. However, highlight determination is often a subjective matter and users' preferences should be taken into account. Therefore, the literature~\\cite{kudi2017words} proposes to integrate text queries as supplementary information for highlight detection. Nonetheless, this work relies solely on text ranking algorithms to rank video descriptions in the text domain to guide video clip ranking. It does not entail a direct alignment of text and highlights. Subsequently, in video thumbnail generation, which closely parallels highlight detection, Yuan \\emph{et al.}~\\cite{DBLP:conf/mm/YuanM019} delves into text queries and uses graph convolutional networks to model the interaction between each clip and text.\n\nConventionally, moment retrieval and highlight detection are addressed in isolation, lacking an integrated framework for joint learning. Recent research~\\cite{lei2021detecting} constructs the QVHighlights dataset to facilitate joint learning of MR\\&HD and proposes a baseline model based on DETR. Building upon this, Liu \\emph{et al.}~\\cite{liu2022umt} incorporates audio modality into the model, catering to scenarios for missing queries. Additionally, Moon \\emph{et al.}~\\cite{moon2023query} prioritizes full integration of provided query information into the joint representation, enabling the text to guide both moment retrieval and highlight detection. Different from previous methods, this paper focuses on exploiting the natural reciprocity between two tasks.\n\n\\subsection{Multi-Modal Alignment}\nRecently, researchers in the multimodal field have focused on constructing contrastive losses to fit the interactions and correspondences between different modalities~\\cite{luo2020univl,sun2019learning,miech2020end, yan2023CAAI}. For example, the literature~\\cite{ging2020coot} introduces a cycle consistency loss to align video clip-level features and query word-level features. Similarly, the literature ~\\cite{zhang2022video} introduces a multi-level contrast loss to capture multi-granular interactive alignment details within queries and videos, enhancing the performance of moment retrieval.\nAlthough these methods share similarities with the multi-modal alignment in our approach, they do not explicitly align the semantic information of different modalities before modality interaction, resulting in insufficient discrimination of joint features.\n\n\\section{Method}\nThe overview of TR-DETR is shown in  Figure~\\ref{method}.\nTR-DETR comprises four core modules: feature extraction, local-global multi-modal alignment, visual feature refinement for modal interaction, and task cooperation.\nDetails are introduced as follows.\n\n\\subsection{Feature Extraction}\n\\subsubsection{Visual Features.}\nFollowing the literature~\\cite{lei2021detecting}, the video is first divided into non-overlapping clips according to a certain time interval, such as 2s. Then the pre-trained ViT-B/32 in CLIP~\\cite{DBLP:conf/icml/RadfordKHRGASAM21} and SlowFast~\\cite{DBLP:journals/corr/abs-1812-03982} are utilized to extract clip-level visual features $ F_{v}=\\left[f_{v}^{1},f_{v}^{2},\\ldots,f_{v}^{L}\\right]\\in \\mathbb{R}^{L\\times d_{v}}$, where $L$ and $d_v$ are the number of clips and the visual feature dimension, respectively. Following the way that UMT~\\cite{liu2022umt} uses audio information, we use the pre-trained audio feature extractor to extract the audio features $F_a \\in \\mathbb{R}^{L\\times d_{a}} $, and then splice them behind the visual features $F_{v}$. See the experimental settings for details.\n\\subsubsection{Textual Features.}\nFor a natural language query, we use the textual encoder in the pre-trained CLIP to extract textual features $ F_{t}=\\left[f_{t}^{1},f_{t}^{2},\\ldots,f_{t}^{N}\\right]\\in \\mathbb{R}^{N\\times d_{t}}$, where $N$ and $d_t$  are the number of words and the textual feature dimension, respectively.\n\n\\subsection{Local-Global Multi-Modal Alignment}\nExisting methods~\\cite{moon2023query,lei2021detecting,liu2022umt} for joint MR\\&HD directly input the extracted visual and textual features into the modal interaction module to obtain joint features. However, there is a natural information mismatch between visual features and textual features, resulting in insufficient discrimination of joint features~\\cite{10123038}. In this study, to reduce the modal gap, we propose a local-global multi-modal alignment module, comprising local and global regularization components. The local regulator helps the model distinguish semantically similar but undesired clips, while the global regulator ensures that both modalities share a unified semantic space. Integrating these alignment regulators can significantly promote multimodal associations and facilitate subsequent modal interactions.\n\nGiven the clip-level visual features  $F_v$ of the video and the word-level textual features $F_t$  of the query, we first map them into the same dimension $d$ by using three-layer multilayer perceptions (MLP).\n\\begin{align}\n    \\widehat{F}_v & = \\text{MLP}_v(F_v), \\\\\n    \\widehat{F}_t & = \\text{MLP}_t(F_t).\n\\end{align}\nFor the local regulator, we calculate the cosine similarity between each clip and each word by using the following formula, obtaining a similarity matrix $S_{loc} \\in\\mathbb{R}^{L\\times N}$.\n\\begin{align}\n    S_{loc} = \\sigma \\left( \\frac{\\widehat{F}_{v} \\, \\widehat{F}_{t}^{\\mathsf{T}}}{\\|\\widehat{F}_{v}\\|_{2} \\|\\widehat{F}_{t}\\|_{2}} \\right),\n\\end{align}\nwhere $\\sigma$ is the sigmoid function. We employ mean-pooling to get $\\widehat{S}_{loc} = \\text{MeanPooling}(S_{loc}) \\in \\mathbb{R}^L$,  which measures the similarity between each video clip and the global textual features. Then, a local regular loss $\\mathcal{L}_{loc}$ is used to encourage distinguishing video clips that are irrelevant to the query.\n\\begin{align}\n\\begin{aligned}\n    \\mathcal{L}_{local} = -\\sum_{i=1}^{L} \\left( C^{i} \\log(\\widehat{S}^{i}_{loc}) + (1 - C^{i}) \\log(1 - \\widehat{S}^{i}_{loc}) \\right),\n\\end{aligned}\n\\end{align}\nwhere $\\widehat{S}^i_{loc}$ is the similarity score between the $i$-th video clip and the global textual features, and $C^i$ indicates whether the $i$-th video clip and the query are actually relevant. Specifically, according to ground truth in MR, if the $i$-th clip is relevant to the query,  $C^i$ is 1, otherwise 0.\nFor the global regulator, a multi-modal contrastive loss~\\cite{li2021align} is employed to promote the similarity of global representations of paired videos and queries.\n\\begin{align}\n    \\mathcal{L}_{global} = -\\frac{1}{B}\\sum_{i=1}^{B}\\log\\frac{\\exp( (G_v^i) \\, (G_t^i)^{\\mathsf{T}})}{\\sum_{i=1}^{B}\\sum_{j=1}^{B}\\exp((G_v^i) \\, (G_t^j)^{\\mathsf{T}})},\n\\end{align}\nwhere $B$ is the batch size,  $G_v^i \\in \\mathbb{R}^d$ and $G_t^i \\in \\mathbb{R}^d$ are the global feature of the $i$-th video and the $i$-th query in a training batch, respectively. Specifically, $G_v^i$ is obtained by averaging all clip features $\\widehat{F}_v$ within the $i$-th video, and $G_t^i$ is derived by averaging word-level features $\\widehat{F}_t$ in the $i$-th query.\n\n\\subsection{Visual Feature Refinement for Modal Interaction}\nThe goal of modal interaction is to generate discriminative joint features from visual and textual features~\\cite{lei2021detecting}, which play a key role in joint MR\\&HD.\nIn the literature~\\cite{lei2021detecting}, visual and textual features are simply concatenated for modal interaction. However, videos generally contain a large number of clips irrelevant to the textual query, which may cause the model to pay too much attention to these irrelevant contents, resulting in ignoring the really important clips.\n\nTo suppress the interference of query-irrelevant information in visual features, we introduce a query-guided visual feature refinement module inspired by the literature~\\cite{xiong2016dynamic} for modal interaction. This module employs the textual query as a guide to refine clip-level visual features to effectively suppress irrelevant information present in the video and retain temporal cues. The similarity matrix between aligned clip-level visual features and word-level textual features is calculated as:\n\\begin{align}\n    A = \\frac{\\text{Linear}(\\widehat{F}_{v}) \\, \\text{Linear}(\\widehat{F}_{t})^{\\mathsf{T}}}{\\sqrt{d}},\n\\end{align}\nwhere $A \\in \\mathbb{R}^{L \\times N}$ is the similarity matrix and Linear($\\cdot$) represents the linear projection layer. Then the similarity matrix is used to weigh and sum the query and video features respectively to obtain preliminary refinement features.\n\\begin{align}\n    F_{v2q} &= A_r \\, \\widehat{F}_t, \\\\\n    F_{q2v} &= A_{r} \\, A_{c}^{\\mathsf{T}} \\, \\widehat{F}_{v},\n\\end{align}\nwhere $A_r$ and $A_c$ represent the results after row softmax normalization and column softmax normalization of $A$, $F_{v2q}$ and $F_{q2v}$ are the clip-level textual features and word-level visual features, respectively. Finally, to further use text queries to optimize clip-level visual features $\\widehat{F}_v$, we perform the following feature concatenation and obtain the final refined clip features $\\overline{F}_v$ through linear projection.\n\\begin{align}\n    F^{Cat}_v &= \\left[\\widehat{F}_v\\| F_{v2q}\\| \\widehat{F}_v \\odot F_{v2q}\\| \\widehat{F}_v \\odot F_{q2v}\\| F_t^G \\right], \\\\\n    \\overline{F}_v &= \\text{Linear}(F^{Cat}_v),\n\\end{align}\nwhere $F_t^G \\in \\mathbb{R}^{L \\times d}$ is a matrix formed by copying and splicing the text global features obtained through the pooling operation, $\\left[ \\cdot\\|\\cdot\\right]$ means concatenation, and $\\odot$ is the Hadamard product.\nThen, modality fusion is performed using a cross-attention layer to further incorporate query features into the joint features, where textual features are from the refined clip feature $Q_{v} = \\text{Linear}_q(\\overline{F}_v)$, key and value features are from the textual features $K_t=\\text{Linear}_k(\\widehat{F}_t)$ and $V_t=\\text{Linear}_v(\\widehat{F}_t)$.\n\\begin{align}\n    Z = \\operatorname{Attention}(Q_v, K_t, V_t) = \\text{Softmax}\\left(\\frac{Q_v K_t^{\\mathsf{T}}}{\\sqrt{d}}\\right) V_t,\n\\end{align}\nwhere $Z \\in\\mathbb{R}^{L\\times d} $ represents joint features through modal interaction between refined visual features and textual features.\n\n\\subsection{Task Cooperation}\nAlthough previous methods~\\cite{lei2021detecting,liu2022umt,moon2023query}  have attempted to jointly solve MR and HD, these methods usually focus on optimizing the shared multi-modal feature extraction and feature interaction modules to improve the discrimination of joint features using a multi-task learning framework. However, the inherent complementarity between MR and HD tasks is underutilized.\n\nIn essence, video clips with high highlight scores are often strong candidates for MR. Because highlight-worthy clips tend to possess enhanced visual significance and attraction. Additionally, clips within the moment relevant to the current query probably cover the highlights, too.\nThis is because query-relevant moments also contain visual expressions of user needs, which helps to refine the highlight score distribution from the visual perspective. Given these insights, we propose a task cooperation module consisting of HD2MR and MR2HD components.\n\n\\subsubsection{HD2MR}\nMR can leverage the highlight scores obtained by HD to empower the exclusion of irrelevant or less attractive video clips. We first use the multi-head attention mechanism and a linear layer to obtain clip-level highlight scores from the joint features $Z$.\n\\begin{align}\n    H &= \\text{Linear}(\\text{MHA}(Z)),\n\\end{align}\nwhere MHA($\\cdot$) represents multi-head attention that is employed to model video temporal information and $H \\in\\mathbb{R}^L$ is the predicted highlight scores.\n\nTo filter out non-highlight information in $Z$ and explicitly inject highlight scores information into the MR pipeline, we multiply the clip-level highlight scores $H$ with the joint features $Z$  to obtain the enhanced joint features $\\overline{Z} \\in \\mathbb{R}^{L \\times d}$.  Then, $\\overline{Z}$ is input into the MHA again for joint features encoding.\n\\begin{align}\n\\begin{aligned}\n    \\overline{Z} &= \\text{Softmax}(H) \\odot Z,\\\\\n    \\widehat{Z} &= \\text{MHA}\\left(Z + \\overline{Z}\\right),\n\\end{aligned}\n\\end{align}\nwhere $\\widehat{Z}$ is the joint features of the perceived highlight scores.\nFinally, these enhanced features $\\widehat{Z}$ are fed into the transformer decoder and prediction head from the literature~\\cite{DBLP:conf/iclr/LiuLZYQSZZ22} to obtain the ultimate retrieved moments.\n\n\\subsubsection{MR2HD}\nHD, in turn, gains a deeper understanding of video content and user needs by leveraging the text query and retrieved moments from MR. We employ the gated recurrent unit (GRU)~\\cite{chung2014empirical} to effectively capture global information from the retrieved moments.\n\\begin{align}\n\\begin{aligned}\n    F^M_v &= \\text{GRU}(m),\n\\end{aligned}\n\\end{align}\n where $m$ represents the clip feature vectors in $\\widehat{F}_v$ of the retrieved moments from HD2MR and $F^M_v \\in \\mathbb{R}^d$ is the global feature vector of these retrieved moments. To use the visual information of the retrieved moments to refine highlight scores prediction, we calculate similarity scores between $F^M_v$ and visual features $\\widehat{F}_v$.\n\\begin{align}\n\\begin{aligned}\n    S_{ref} &= \\frac{F^M_v \\, \\widehat{F}_v^\\mathsf{T}}{\\|F^M_v\\|_2 \\, \\|\\widehat{F}_v\\|_2},\n\\end{aligned}\n\\end{align}\n where $S_{ref}\\in\\mathbb{R}^L$ is the correlation between clips and $F^M_v$.\n The highlight score refinement process involves multiplying the clip-level correlation scores by $\\widehat{Z}$, then adding them to $Z$, and finally obtaining refined scores by linear projection. The formulation is as follows:\n \\begin{align}\n    \\begin{aligned}\\overline{H}&=\\text{Linear}(Z+\\text{Softmax}(S_{ref})\\odot \\widehat{Z}),\\end{aligned}\n \\end{align}\nwhere $\\overline{H} \\in\\mathbb{R}^L$ is the refined highlight scores.\n\n\\subsection{Objective Losses}\nThe objective losses of TR-DETR include three parts: MR loss $\\mathcal{L}_{mom}$, HD loss $\\mathcal{L}_{high}$, regulators losses $\\mathcal{L}_{local}$ and $\\mathcal{L}_{global}$.\n\\begin{align}\n    \\begin{aligned}\n        \\mathcal{L}_{total}=\\mathcal{L}_{mom}+\\mathcal{L}_{high}+\\lambda_{lg}(\\mathcal{L}_{local}+\\mathcal{L}_{global}),\n    \\end{aligned}\n\\end{align}\nwhere $\\lambda_{lg}$ is the coefficient of local-global regulators losses. $\\mathcal{L}_{mom}$ and $\\mathcal{L}_{high}$ are consistent with QD-DETR~\\cite{moon2023query}.\n\n\\section{Experiment}\n\\begin{table*}[!ht]\n    \\centering\n    \\begin{tabular}{lcccccccc}\n        \\toprule  % \n\n    \\multirow{4}{*}{\\textbf{Method}} & \\multirow{4}{*}{\\textbf{Src}} & \\multicolumn{5}{c}{\\textbf{Moment Retrieval}}                           & \\multicolumn{2}{c}{\\textbf{HD}}                       \\\\ \\cmidrule(r){3-7} \\cmidrule(lr){8-9}\n                            &                      & \\multicolumn{2}{c}{R1} & \\multicolumn{3}{c}{mAP} & \\multicolumn{2}{c}{$\\ge$Very Good} \\\\ \\cmidrule(r){3-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-9}\n                            &                      & @0.5       & @0.7      & @0.5   & @0.75  & Avg.  & mAP                   & HIT@1                \\\\\n     \\midrule\n    BeautyThumb~\\cite{DBLP:conf/cikm/SongRVJ16}      & V                    & -          & -         & -      & -      & -     & 14.36                 & 20.88                \\\\\n    DVSE~\\cite{DBLP:conf/cvpr/LiuMZCL15}            & V                    & -          & -         & -      & -      & -     & 18.75                 & 21.79                \\\\\n    MCN~\\cite{DBLP:conf/emnlp/HendricksWSSDR18}             & V                    & 11.41      & 2.72      & 24.94  & 8.22   & 10.67 & -                     & -                    \\\\\n    CAL~\\cite{DBLP:journals/corr/abs-1907-12763}             & V                    & 25.49      & 11.54     & 23.40   & 7.65   & 9.89  & -                     & -                    \\\\\n    XML~\\cite{DBLP:conf/eccv/LeiYBB20}            & V                    & 41.83      & 30.35     & 44.63  & 31.73  & 32.14 & 34.49                 & 55.25                \\\\\n    XML+~\\cite{lei2021detecting}            & V                    & 46.69      & 33.46     & 47.89  & 34.67  & 34.90  & 35.38                 & 55.06                \\\\\n    MDETR~\\cite{lei2021detecting}          & V                    & 52.89      & 33.02     & 54.82  & 29.40   & 30.73 & 35.69                 & 55.60                 \\\\\n    QD-DETR~\\cite{moon2023query}        & V                    & \\underline{62.40}     & \\underline{44.98}     & \\underline{62.62}  & \\underline{39.88}  & \\underline{39.86} & \\underline{38.64}                 & \\underline{62.40}                 \\\\\n    UniVTG~\\cite{DBLP:journals/corr/abs-2307-16715}          & V                    & 58.86      & 40.86     & 57.60   & 35.59  & 35.47 & 38.20                  & 60.96                \\\\\n    \\rowcolor[rgb]{0.78,0.78,0.78}  TR-DETR                 & V                    & \\textbf{64.66}     & \\textbf{48.96}     & \\textbf{63.98}  & \\textbf{43.73}  & \\textbf{42.62} & \\textbf{39.91}                 & \\textbf{63.42}                \\\\\n    \\midrule\n    UMT~\\cite{liu2022umt}            & V+A                  & 56.23      & 41.18     & 53.38  & 37.01  & 36.12 & 38.18                 & 59.99                \\\\\n    QD-DETR~\\cite{moon2023query}                & V+A                  & \\underline{63.06}      & \\underline{45.10}      & \\underline{63.04}  & \\underline{40.10}   & \\underline{40.19} & \\underline{39.04}                 & \\underline{62.87}                \\\\\n    \\rowcolor[rgb]{0.78,0.78,0.78} TR-DETR                 & V+A                  & \\textbf{65.05}      & \\textbf{47.67}     & \\textbf{64.87}  & \\textbf{42.98}  & \\textbf{43.10}  & \\textbf{39.90}                  & \\textbf{63.88}                \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Experimental results on the QVHighlights \\emph{test} set. HD represents the results of highlight detection. `V' and `A' represent using video and audio features, respectively. Bold letters indicate the best results, while underlined results are suboptimal.}\n    \\label{results_QV}\n\\end{table*}\n\n\\begin{table}[!ht]\n    \\centering\n    \\small\n    \\begin{tabular}{lccc}\n        \\toprule\n        \\textbf{Method} & \\textbf{Feat} & R1@0.5 & R1@0.7 \\\\\n        \\midrule\n        SAP~\\cite{DBLP:conf/aaai/ChenJ19a} & VGG & 27.42 & 13.36 \\\\\n        TripNet~\\cite{DBLP:conf/bmvc/HahnKRG20} & VGG & 36.61 & 14.50 \\\\\n        MAN~\\cite{DBLP:conf/cvpr/ZhangDWWD19} & VGG & 41.24 & 20.54 \\\\\n        2D-TAN~\\cite{zhang2020learning} & VGG & 40.94 & 22.85 \\\\\n        FVMR~\\cite{li2021align} & VGG & 42.36 & 24.14 \\\\\n        UMT~\\cite{liu2022umt} & VGG & 48.31 & 29.25 \\\\\n        QD-DETR~\\cite{moon2023query} & VGG & 52.77 & 31.13 \\\\\n        QD-DETR~\\cite{moon2023query} & VGG & \\textbf{55.51} & \\textbf{34.17} \\\\\n        \\rowcolor[rgb]{0.78,0.78,0.78} TR-DETR & VGG & 53.47 & 30.81 \\\\\n        \\rowcolor[rgb]{0.78,0.78,0.78} TR-DETR & VGG & \\underline{54.49} & \\underline{32.37} \\\\\n        \\midrule\n        CTRL~\\cite{gao2017tall} & C3D & 23.63 & 8.89 \\\\\n        ACL~\\cite{DBLP:conf/wacv/GeGCN19} & C3D & 30.48 & 12.20 \\\\\n        MAN~\\cite{DBLP:conf/cvpr/ZhangDWWD19} & C3D & 46.53 & 22.72 \\\\\n        DEBUG~\\cite{DBLP:conf/emnlp/LuCTLX19} & C3D & 37.39 & 17.69 \\\\\n        VSLNet~\\cite{zhang2021natural} & I3D & 47.31 & 30.19 \\\\\n        QD-DETR~\\cite{moon2023query} & I3D & \\underline{50.67} & \\underline{31.02} \\\\\n        \\rowcolor[rgb]{0.78,0.78,0.78} TR-DETR & I3D & \\textbf{55.51} & \\textbf{33.66} \\\\\n        \\midrule\n        QD-DETR~\\cite{moon2023query} & SF+C & 57.31 & 32.55 \\\\\n        UniVTG~\\cite{DBLP:journals/corr/abs-2307-16715} & SF+C & \\textbf{58.01} & \\textbf{35.65} \\\\\n        \\rowcolor[rgb]{0.78,0.78,0.78} TR-DETR & SF+C & \\underline{57.61} & \\underline{33.52} \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{Experimental results on the Charades-STA \\emph{test} set. `' represents using audio features.}\n    \\label{results_CHA}\n\\end{table}\n\n\\subsection{Datasets}\n\n\\begin{table*}[!ht]\n    \\centering\n    \\begin{tabular}{lcccccccccccc}\n        \\toprule\n        \\textbf{Method} & \\textbf{Src} & VT & VU & GA & MS & PK & PR & FM & BK & BT & DS & Avg \\\\\n        \\midrule\n        sLSTM~\\cite{DBLP:conf/eccv/ZhangCSG16} & V & 41.1 & 46.2 & 46.3 & 47.7 & 44.8 & 46.1 & 45.2 & 40.6 & 47.1 & 45.5 & 45.1 \\\\\n        SG~\\cite{DBLP:journals/tmm/YuanTLF20} & V & 42.3 & 47.2 & 47.5 & 48.9 & 45.6 & 47.3 & 46.4 & 41.7 & 48.3 & 46.6 & 46.2 \\\\\n        LIM-S~\\cite{xiong2019less} & V & 55.9 & 42.9 & 61.2 & 54.0 & 60.3 & 47.5 & 43.2 & 66.3 & 69.1 & 62.6 & 56.3 \\\\\n        Trailer~\\cite{DBLP:conf/eccv/WangLPM20} & V & 61.3 & 54.6 & 65.7 & 60.8 & 59.1 & 70.1 & 58.2 & 64.7 & 65.6 & 68.1 & 62.8 \\\\\n        SL-Module~\\cite{DBLP:conf/iccv/XuWNZSW21} & V & 86.5 & 68.7 & 74.9 & \\textbf{86.2} & 79.0 & 63.2 & 58.9 & 72.6 & 78.9 & 64.0 & 73.3 \\\\\n        QD-DETR~\\cite{moon2023query} & V & \\underline{88.2} & \\underline{87.4} & 85.6 & 85.0 & \\underline{85.8} & 86.9 & \\underline{76.4} & \\underline{91.3} & \\underline{89.2} & \\underline{73.7} & \\underline{85.0} \\\\\n        UniVTG~\\cite{DBLP:journals/corr/abs-2307-16715} & V & 83.9 & 85.1 & \\underline{89.0} & 80.1 & 84.6 & \\underline{87.0} & 70.9 & \\textbf{91.7} & 73.5 & 69.3 & 81.0 \\\\\n        \\rowcolor[rgb]{0.78,0.78,0.78} TR-DETR & V & \\textbf{89.3} & \\textbf{93.0} & \\textbf{94.3} & \\underline{85.1} & \\textbf{88.0} & \\textbf{88.6} & \\textbf{80.4} & \\underline{91.3} & \\textbf{89.5} & \\textbf{81.6} & \\textbf{88.1} \\\\\n        \\midrule\n        MINI-Net~\\cite{DBLP:conf/eccv/HongHLZ20} & V+A & 80.6 & 68.3 & 78.2 & 81.8 & 78.1 & 65.8 & 75.8 & 75.0 & 80.2 & 65.5 & 73.2 \\\\\n        TCG~\\cite{DBLP:conf/iccv/YeSGWBL021} & V+A & 85.0 & 71.4 & 81.9 & 78.6 & 80.2 & 75.5 & 71.6 & 77.3 & 78.6 & 68.1 & 76.8 \\\\\n        Joint-VA~\\cite{DBLP:conf/iccv/BadamdorjRWC21} & V+A & 83.7 & 57.3 & 78.5 & \\underline{86.1} & 80.1 & 69.2 & 70.0 & 73.0 & \\textbf{97.4} & 67.5 & 76.3 \\\\\n        UMT~\\cite{liu2022umt} & V+A & 87.5 & 81.5 & 88.2 & 78.8 & 81.4 & \\underline{87.0} & 76.0 & 86.9 & 84.4 & \\underline{79.6} & 83.1 \\\\\n        QD-DETR~\\cite{moon2023query} & V+A & \\underline{87.6} & \\underline{91.7} & \\underline{90.2} & \\textbf{88.3} & \\underline{84.1} & \\textbf{88.3} & \\underline{78.7} & \\underline{91.2} & 87.8 & 77.7 & \\underline{86.6} \\\\\n        \\rowcolor[rgb]{0.78,0.78,0.78} TR-DETR & V+A & \\textbf{90.6} & \\textbf{92.4} & \\textbf{91.7} & 81.3 & \\textbf{86.9} & 85.5 & \\textbf{79.8} & \\textbf{93.4} & \\underline{88.3} & \\textbf{81.0} & \\textbf{87.1} \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{Experimental results on the TVSum \\emph{val} set. `V' and `A' represent using video and audio features, respectively.}\n    \\label{results_TV}\n\\end{table*}\n\n\\begin{figure*}[!ht]\n\\centering\n\\includegraphics[width=0.95\\textwidth]{Figure/QV_clipped.pdf} % Replace with\n\\caption{Qualitative results of TR-DETR on QVHighlights \\emph{val} set.}\n\\label{qualative_QV}\n\\end{figure*}\n\n\\begin{figure*}[!ht]\n\\centering\n\\includegraphics[width=0.95\\textwidth]{Figure/TVsum_clipped.pdf} % Replace\n\\caption{Qualitative results of TR-DETR on TVSum \\emph{val} set.}\n\\label{qualative_TV}\n\\end{figure*}\n\n\\begin{figure}[!ht]\n\\centering\n\\includegraphics[width=0.45\\textwidth]{Figure/LG_loss_cof_clipped.pdf} % Replace with the actual image file name and path\n\\caption{The impact of local-global alignment loss and $\\lambda_{lg}$ based on QVHighlights \\emph{val} set, introducing audio features. }\n\\label{cof_abi}\n\\end{figure}\nQVHighlights dataset~\\cite{lei2021detecting} comprises 10,148 content-rich videos from YouTube. Each video is accompanied by at least one manually annotated text query, where the highlight clips are located within the corresponding moment. The evaluation process of this dataset is particularly fair as the annotations of the test set are inaccessible. The prediction results of the model need to be uploaded to the QVHighlights server's CodaLab competition platform\\footnote{https://codalab.lisn.upsaclay.fr/competitions/6937} for impartial performance assessment.\n\nCharades-STA dataset~\\cite{gao2017tall} contains 9,848 videos capturing daily indoor activities and 16,128 human-tagged query texts. Following QD-DETR\\cite{moon2023query}, we allocate 12,408 samples for training while the remaining 3,720 samples are for testing.\n\nTVSum dataset~\\cite{song2015tvsum} is a benchmark dataset for HD. It contains 10 different categories of videos, and each category comprises 5 videos. To ensure consistency with QD-DETR~\\cite{moon2023query},  80\\% of the dataset is utilized for training and the remaining for testing.\n\n\\subsection{Metrics and Experimental Settings}\nWe use common metrics from recent studies like Moment-DETR, UMT, QD-DETR, and MH-DETR. For QVHighlights, we calculate Recall@1 with IoU $\\in \\{0.5, 0.7\\}$ and mean average precision (mAP) with IoU $\\in \\{0.5, 0.75\\}$. Following Lei \\emph{et al.}~\\cite{lei2021detecting}, we also uniformly sample 10 IoU thresholds from $\\{0.5, 0.95\\}$ to calculate mAP, and take the average as the average mAP metric. For highlight detection, we use mAP and HIT@1. Charades-STA involves Recall@1 with IoU $\\in \\{0.5, 0.7\\}$, while for TVSum, top-5 mAP is the main metric.\n\nIn addition, we introduce implementation details and hyperparameters as follows. The hidden layer dimension $d$ is 256, and $\\lambda_{lg}$ is set to 0.3. We use PANN~\\cite{DBLP:journals/taslp/KongCIWWP20} trained on the AudioSet dataset~\\cite{DBLP:conf/icassp/GemmekeEFJLMPR17} to extract audio features.\nFor QVHighlights, we use SlowFast~\\cite{DBLP:journals/corr/abs-1812-03982} and CLIP to extract visual features and the text encoder in CLIP to extract textual features. The training phase involves 200 epochs, a batch size of 32, and a learning rate of 1e-4.\nFor TVSum, we use the I3D pre-trained on Kinetics-400 for visual features and CLIP for textual features. Training spans 2000 epochs with a batch size of 4 and a learning rate of 1e-3.\nIn Charades-STA, we extract visual features with VGG~\\cite{DBLP:journals/corr/SimonyanZ14a}, I3D~\\cite{DBLP:conf/cvpr/CarreiraZ17}, SlowFast, and CLIP, and use GLoVe~\\cite{DBLP:conf/emnlp/PenningtonSM14}  for textual features. The training phase includes 100 epochs, a batch size of 8, and a learning rate of 1e-4. Moreover, all our experiments are conducted on Nvidia RTX 4090 and Gen Intel(R) Core(TM) i7-12700 CPU.\n\n\\subsection{Comparison with Other Methods}\nTable~\\ref{results_QV} reports the TR-DETR's performance on joint moment retrieval and highlight detection tasks. Meanwhile, Tables~\\ref{results_CHA} and~\\ref{results_TV} list the results of different methods on moment retrieval and highlight detection, respectively.\n\nIn Table~\\ref{results_QV}, we evaluate the performance of moment retrieval and highlight detection simultaneously based on the QVHighlights dataset. For a fair comparison, we compare the performance with UniVTG~\\cite{DBLP:journals/corr/abs-2307-16715} without pre-training. As shown in Table~\\ref{results_QV}, our TR-DETR method outperforms the current best approach on all metrics.  Especially with visual features only, TR-DETR exhibits a significant increase in performance under more stringent metrics and high IOU thresholds.  Compared with previous methods, TR-DETR improves R1@0.7 and mAP@0.75 by 3.98\\% and 3.75\\%, respectively.\nIn addition, after introducing audio information, the performance of a few indicators decreases. This may be because the audio features are spliced directly behind the video features, causing misaligned multi-modal features to be combined and thus impairing modal interactions.\n\nIn Table~\\ref{results_CHA}, we use VGG, C3D, and SF+C features to comprehensively evaluate the performance of TR-DETR on the Charades-STA dataset. For each feature of VGG, C3D and SF+C, we follow the data preparation settings of UMT~\\cite{liu2022umt}, VSLNet~\\cite{zhang2021natural}, and Moment-DETR~\\cite{lei2021detecting}, respectively.  As shown in Table~\\ref{results_CHA}, our TR-DETR shows comparable performance on VGG and SF+C features.  Also, performance on some metrics degrades with the introduction of audio, possibly due to insufficient modal interaction. Compared with using only VGG features, the performance of the proposed method is slightly different from UniVTG when using SF+C features. We believe the reasons are as follows: semantic information of features extracted by different-scale feature extractors (\\emph{e.g.} VGG and PANN) varies greatly. In our method, the local-global multi-modal alignment module is used to force the alignment of the visual features of VGG, the audio features of PANN, and the text features of GLoVe, which is challenging and results in relatively weak performance. However, when text, visual and audio features are all derived from large models, such as CLIP, our method shows excellent performance on the  QVHighlights dataset.\n\nConsistent with previous work on highlight detection, we evaluate the performance of the proposed TR-DETR on each video category and calculate the top-5 mAP scores. The results are shown in Table~\\ref{results_TV}. In addition, to comprehensively evaluate the overall performance of TR-DETR, we calculate the average value of top-5 mAP on 10 categories. The proposed TR-DETR exceeds the previous method by approximately 3.1\\% when using only video features, which demonstrates the powerful performance of TR-DETR in solving HD alone.\n\n\\begin{table*}[t]\n    \\centering\n    \\small\n    \\begin{tabular}{ccccclllllll}\n        \\toprule\n        \\multirow{4}{*}{\\textbf{Setting}} & \\multirow{4}{*}{LGAM} & \\multirow{4}{*}{VFR} & \\multirow{4}{*}{MR2HD} & \\multirow{4}{*}{HD2MR} & \\multicolumn{5}{c}{\\textbf{Moment Retrieval}}                           & \\multicolumn{2}{c}{\\textbf{HD}}                        \\\\ \\cmidrule(r){6-10} \\cmidrule(lr){11-12}\n                             &                     &                              &                        &                        & \\multicolumn{2}{c}{R1} & \\multicolumn{3}{c}{mAP} & \\multicolumn{2}{c}{$\\ge$Very Good} \\\\ \\cmidrule(r){6-7} \\cmidrule(r){8-10} \\cmidrule(lr){11-12}\n                             &                     &                              &                        &                        & @0.5       & @0.7      & @0.5   & @0.75  & Avg.  & mAP                    & HIT@1                \\\\\n        \\midrule\n        (a)                      &                     &                              &                        &                        & 57.72      & 42.35     & 59.10  & 38.16  & 38.03 & 36.76                  & 57.44                \\\\\n        \\midrule\n        (b)                      & \\checkmark                 &                              &                        &                        & 63.10       & 44.97     & 63.13  & 40.22  & 40.47 & 39.92                  & 63.87                \\\\\n        (c)                      &                     & \\checkmark                             &                        &                        & 64.19      & 47.61     & 63.50 & 42.90  & 41.74 & 39.71                  & 64.13                \\\\\n        (d)                      &                     &                              & \\checkmark                       &                        & 58.39      & 42.71     & 59.28  & 39.19  & 38.76 & 37.80                   & 58.8                 \\\\\n        (e)                      &                     &                              &                        & \\checkmark                       & 59.61      & 42.26     & 60.91  & 39.28  & 39.26 & 37.67                  & 58.45                \\\\\n        \\midrule\n        (f)                      &                     &                              & \\checkmark                       & \\checkmark                       & 59.81      & 44.71     & 60.25  & 39.33  & 39.80  & 37.86                  & 57.94                \\\\\n        (g)                      & \\checkmark                    &                              & \\checkmark                       & \\checkmark                       & 62.13      & 47.16     & 62.00     & 42.79  & 41.21 & 39.76                  & 62.65                \\\\\n        (h)                      &                     & \\checkmark                             & \\checkmark                       & \\checkmark                       & 63.23      & 46.90      & 63.30   & 42.47  & 41.64 & 38.12                  & 59.55                \\\\\n        (i)                      & \\checkmark                    & \\checkmark                             &                        &                        & 66.32      & 50.71     & 65.71  & 44.82  & 43.95 & 40.35                  & 64.90                 \\\\\n        \\midrule\n        (j)                      & \\checkmark                    & \\checkmark                             & \\checkmark                       & \\checkmark                       & 67.10       & 51.48     & 66.27  & 46.42  & 45.09 & 40.55                  & 64.77               \\\\\n        \\bottomrule\n    \\end{tabular}\n     \\caption{Comparison with the baseline (Moment-DETR with cross-attention module and DAB-DETR's decoder~\\cite{DBLP:conf/iclr/LiuLZYQSZZ22}) with different module combinations on QVHighlights \\emph{val} set.  LGAM represents the local-global alignment module, and VFR is the visual feature refinement module.}\n     \\label{results_abi}\n\\end{table*}\n\n\\subsection{Visualization}\n\nIn Figures~\\ref{qualative_QV} and~\\ref{qualative_TV}, we visualize the qualitative analysis results of TR-DETR on the QVHighlights and TVSum datasets, respectively. In Figure~\\ref{qualative_QV}, compared with QD-DETR, TR-DETR shows more reasonable and accurate results in terms of retrieved accuracy and highlight score distribution. In Figure~\\ref{qualative_TV} a), the proposed TR-DETR can accurately fit the highlight score distribution. We believe that these performance improvements are due to the combination of the proposed modules. In addition, in Figure~\\ref{qualative_TV} b), it may be that the model only noticed the concept of ``puppy dog\", resulting in unreasonable high highlight scores in the middle of the result.\n\n\\subsection{Ablation}\n\nTo verify the effect of each module in the proposed TR-DETR, we conduct a comprehensive ablation experiment, and the results are listed in Table~\\ref{results_abi}. Settings (b) to (e) show the performance of each component on the baseline model compared to setting (a). Setting (f) demonstrates the existence of task reciprocity. Compared with setting (c), the reason for the performance degradation in setting (h) may be the semantic mismatch between modalities, resulting in mutual degradation of tasks. Setting (i) shows the huge performance improvement of the proposed local-global alignment loss combined with visual feature refinement.\n\nTo further verify the effect of the proposed local-global alignment loss, we also conduct ablation experiments on its coefficients. As shown in Figure~\\ref{cof_abi}, after adding the local global regularization term, the model's performance has been significantly improved by about 5\\%. In addition, as the value of the hyperparameter $\\lambda_{lg}$ gradually increases, the performance improvement becomes more significant. When $\\lambda_{lg}$ is set to 0.3, the model performance reaches its peak and then begins to decline slowly. Comparing the hyperparameter values of 0 and 0.3, the model performance has been improved by about 7\\% in total, confirming the significant role of the local-global alignment regulators.\n\n\\section{Conclusion}\n\nThis paper proposes a TR-DETR to explore the reciprocity between HD and MR tasks.\nFirst, local-global alignment regulators are designed to align visual and textual features. Then, a visual feature refinement module is constructed to obtain discriminative joint features. Finally, a task-reciprocal module is proposed to inject highlight score information into the moment retrieval pipeline and optimize highlight score prediction by utilizing retrieved moments.  Extensive experiments on several datasets demonstrate the effectiveness of TR-DETR.\nHowever, TR-DETR cannot efficiently utilize data from the audio modality. In the future, we will study novel multi-modal feature interaction networks to coordinate information from multiple modalities.\n\n\\section{Acknowledgments}\nThis work was supported in part by National Natural Science Foundation of China under Grant 62201222 and 62377026, in part by Hubei Provincial Natural Science Foundation of China under Grant 2022CFB954, in part by Knowledge Innovation Program of Wuhan-Shuguang Project under Grant 2023010201020377 and 2023010201020382, in part by self-determined research funds of CCNU from the colleges' basic research and operation of MOE under Grant CCNU22QN014, CCNU22JC007 and CCNU22XJ034, and in part by Hubei Provincial Key Laboratory of Artificial Intelligence and Smart Learning NO. 2023AISL003 and 2023AISL010.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Length-Aware DETR for Robust Moment Retrieval}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n    Video Moment Retrieval (MR) aims to localize moments within a video based on a given natural language query.\n    Given the prevalent use of platforms like YouTube for information retrieval, the demand for MR techniques is significantly growing.\n    Recent DETR-based models have made notable advances in performance but still struggle with accurately localizing short moments.\n    Through data analysis, we identified limited feature diversity in short moments, which motivated the development of MomentMix. MomentMix employs two augmentation strategies: ForegroundMix and BackgroundMix, each enhancing the feature representations of the foreground and background, respectively. Additionally, our analysis of prediction bias revealed that short moments particularly struggle with accurately predicting their center positions of moments. To address this, we propose a Length-Aware Decoder, which conditions length through a novel bipartite matching process.\n    Our extensive studies demonstrate the efficacy of our length-aware approach, especially in localizing short moments, leading to improved overall performance.\n    Our method surpasses state-of-the-art DETR-based methods on benchmark datasets, achieving the highest R1 and mAP on QVHighlights and the highest R1@0.7 on TACoS and Charades-STA (such as a 2.46\\% gain in R1@0.7 and a 2.57\\% gain in mAP average for QVHighlights). The code is available at \\url{https://github.com/sjpark5800/LA-DETR}.\n    \n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:introduction}\n\nAs vast amounts of video content are created and shared on the internet daily~\\cite{cheng2013understanding_YouTube}, the need for effective filtering has become more critical. Text-based search algorithms~\\cite{boyer1977fast_search_matching} have emerged as one of the most effective solutions, enabling rapid and accurate retrieval of videos that match user queries. \nTo improve user experience and search efficiency, moment retrieval (MR)~\\cite{anne2017localizing,gao2017tall_tall} has gained significant attention. MR identifies the specific moments within a video that best align with a given query. Specifically, this task involves localizing the start and end points in the video relevant to the textual query, offering a more fine-grained understanding of video content.\n\n\\begin{table}[tb]    \n    \\caption{\n        Mean Average Precision (mAP) of previous moment retrieval (MR) DETR-based methods on \\textsc{QVHighlights} \\textit{val} set by the lengths of the moment. Notice that the mAP drops significantly in capturing short-moment, where MR can be best utilized.\n    }\n    \\centering\n\n    \\scalebox{0.80}{\n    \n    \\tabcolsep=0.11cm\n    \n    \\begin{tabular}{l r >{\\columncolor{yellow}}c c c c}\n        \\hline\n\n        \n        \\multirow{2}{*}{Method} &  & Short & Middle & Long & All \\\\\n\n        \\cline{3-3}\n        \\cline{4-4}\n        \\cline{5-5}\n        \\cline{6-6}\n\n        \\multirow{1}{*}{}\n        & \\multirow{1}{*}{{$_{\\textrm{(seconds)}}$}}\n        & \\multirow{1}{*}{{$_{(000,010]}$}}\n        & \\multirow{1}{*}{{$_{(010,030]}$}}\n        & \\multirow{1}{*}{{$_{(030,150]}$}}\n        & \\multirow{1}{*}{{$_{(000,150]}$}} \\\\\n        \n        \\hline\n\n        \n        \\multicolumn{2}{l}{QD-DETR {\\footnotesize\\color{gray}(CVPR 2023)}} & 8.34 & 43.54 & 47.80 & 41.24 \\\\\n        \\multicolumn{2}{l}{TR-DETR {\\footnotesize\\color{gray}(AAAI 2024)}} & 9.93 & 46.91 & 51.71 & 45.09 \\\\\n        \\multicolumn{2}{l}{UVCOM {\\footnotesize\\color{gray}(CVPR 2024)}}   & 12.65 & 49.04 & 49.39 & 45.80 \\\\\n        \\hline \\\\\n    \\end{tabular}\n}\n\\label{tab:table_performance_moment_length}\n\\end{table}\n\nFor the MR task, previous approaches leveraging DETR for its efficiency and flexibility~\\cite{lei2021detecting_Moment-DETR,moon2023query_QD-DETR,sun2024tr_TR-DETR, zhang2024temporally_TaskWeave, xiao2024bridging_UVCOM} have proposed methods to improve video-text feature representation, achieving impressive performance. \nHowever, our empirical findings indicate that these DETR-based models suffer from a significant drop in performance when handling short moments as highlighted in \\Cref{tab:table_performance_moment_length}.\nFor example, UVCOM~\\cite{xiao2024bridging_UVCOM} shows an average mAP of 49.04 for middle-length moments (1030 seconds), \nwhile achieving only 12.65 for short moments (less than 10 seconds), revealing a substantial gap. As illustrated in \\Cref{fig:intro_dataset_info}, moments are uniformly distributed across different lengths. However, due to the inherently short duration of short moments, they appear more frequently within a single video sample. Performance metrics are calculated by averaging the performance across all moments within a sample and then averaging these sample performances to obtain the overall performance. Therefore, the overall performance does not sufficiently reveal the performance on short moments, leading to a lack of attention to the challenges associated with short moment retrieval.\n\nRetrieving short moments within videos is a crucial task because videos often contain a significant amount of redundant or irrelevant information, while essential content is frequently condensed in short moments. This aligns with the importance of MR, where improving the accuracy of short-moment retrieval enables the precise extraction of the most relevant information. \nSuch improvements can significantly reduce the time and effort required for video exploration.\nFor example, highlights in sports and news, as well as key scenes in movies and dramas, often involves short moments. This emphasizes the importance of accurately retrieving short moments in practical scenarios.\n\n\\begin{figure}[tb]\n    \\centering\n    \\includegraphics[width=\\linewidth]{assets/intro_dataset_info.pdf}\n    \\caption{In \\textsc{QVHighlights} dataset, a single video can contain multiple moments of varying lengths. The yellow bars represent the count of each \\textbf{moment} length across all videos. In contrast, the blue bars categorize a \\textbf{video sample} by a specific length if it includes at least one moment of that length.\n    }\n    \\label{fig:intro_dataset_info}\n\\end{figure}\n\nIn this study, we analyzed the challenges associated with existing methods in short-moment retrieval from both data and model perspectives. From a data perspective, we examined the feature distribution of short moments compared to other moments.  As shown in \\Cref{fig:method_feature_diversity}, the features of short moments tend to be more concentrated around the mean feature, with 42.9\\% of samples falling within one standard deviation. In contrast, for other moments, 26.6\\% of samples fall within one standard deviation of the mean. These observations show that short moments exhibit relatively simple and less diverse feature distributions. On the model side, we analyzed the trends in prediction accuracy by breaking down the models final output into center and length components, as illustrated in \\Cref{fig:method_predictions}. Although conceptually, moments are defined by (start, end), existing models predict them in the format of (center, length). Interestingly, we found that the accuracy of center predictions for short moments was significantly lower than for other moment types.\n\nBased on the above analysis, we propose a new DETR-based MR framework that addresses the performance degradation when retrieving short moments. Our framework consists of two novel techniques: a data augmentation technique called \\textbf{MomentMix} and a length-aware decoder (\\textbf{LAD}). Through data analysis, we identified a key limitation in the feature diversity of short moments. It prompts us to design MomentMix, which comprises two types of mix-based data augmentation strategies: ForegroundMix and BackgroundMix. In a video sample, we define the temporal moments relevant to the text query as the foreground and the unrelated moments as the background. ForegroundMix enhances the diversity of foreground features by combining rich foreground elements from different moments to create new short foregrounds. These newly created short foregrounds enable the model to detect short moments more robustly. Conversely, BackgroundMix utilizes portions of other videos as backgrounds, forming varied foreground-background combinations. This enables the model to better differentiate between foreground and background, ultimately improving the quality of video features across all moment lengths, not just short ones. \n\nOur analysis of model outputs revealed that center prediction errors significantly contribute to performance drops in short-moment retrieval. To address this, we introduce a length-aware decoder, which structures the decoder to focus more effectively on center prediction by conditioning it on length. Specifically, we predefine length classes (e.g., short, middle, long) and uniformly assign each decoder query to these length-specific classes. Additionally, we modify the bipartite matching process so that queries are matched with ground-truth moments within the same length class. This approach improves the accuracy of both center and length predictions for short moments.\n\nIn this work, we tackled the significant challenge of short-moment retrieval by proposing MomentMix augmentation and Length-Aware Decoder.\nOur approach significantly improved performance across various datasets, including \\textsc{QVHighlights}, \\textsc{TACoS}, and \\textsc{Charades-STA}.\nOur contributions are summarized as follows:\n\\begin{enumerate}\n    \\item We identify the root causes of performance degradation in short-moment retrieval for MR from both data and model perspectives.\n    \n    \\item To address the issue of limited feature diversity in short moments, we propose a novel augmentation strategy based on two types of mixing methods, specifically tailored for video moment retrieval.\n    \n    \\item To enhance both center and length prediction for short moments, we introduced length conditioning into DETR-based MR methods for the first time, effectively creating  ``length-wise expert'' queries with length-wise matching.\n    \n    \\item Our approach notably enhanced performance on the various MR datasets, resulting in significant improvement of mAP in \\textsc{QVHighlights} (9.36\\%; 41.22 $\\rightarrow$ 45.08) and across other datasets.\n\\end{enumerate}\n\n\\section{Related Work}\n\\label{sec:related_work}\n\n\\subsection{Moment Retrieval}\nThe moment retrieval (MR) task involves identifying the start and end timestamps of a specific moment within a video given a text query.\nTraditional methods approach MR through either \\textbf{two-stage}~\\cite{anne2017localizing_mcn, hendricks2018localizing_MLLC, zhang2019exploiting_TCMN, zhang2019cross_QSPN, gao2021fast_SAP} or \\textbf{one-stage}~\\cite{chen2018temporally_tgn, wang2020temporally_cbp, otani2020uncovering_SCDM, Zhang_2019_CVPR_man, hu2021video_CMHN, liu2018temporal_tmn, zhang2020learning_2d_tan} framework, which often rely on cumbersome processes such as proposal generation, non-maximum suppression (NMS), and human priors.\nA significant paradigm shift was introduced with Moment-DETR~\\cite{lei2021detecting_Moment-DETR}, which framed MR as a direct set prediction task using \\textbf{Detection Transformer} (DETR)~\\cite{carion2020end_DETR}. \nThis approach eliminates these dependencies, thereby making MR more efficient and flexible. Subsequent models, including QD-DETR~\\cite{moon2023query_QD-DETR}, and MESM~\\cite{liu2024towards_MESM}, have further optimized DETR-based MR by addressing cross-modal alignment and refining prediction accuracy. \nRecent works, such as TR-DETR~\\cite{sun2024tr_TR-DETR}, TaskWeave~\\cite{zhang2024temporally_TaskWeave}, and UVCOM~\\cite{xiao2024bridging_UVCOM}, leverage task-specific features to bridge Moment Retrieval and Highlight Detection, enhancing robustness across both tasks. Additionally, there are researches to develop boundary-aware notations for moments~\\cite{lee2025bam-detr} and to explore advanced feature in MR~\\cite{liu2024r2-tuning, wang2024internvideo2}. \nConsidering the efficiency and advancements of DETR-based architectures, our research specifically targets the challenges within DETR-based methods.\n\nDifferent from the previous works, to the best of our knowledge, we are the first to address the critical limitation of performance degradation on short moments, a common challenge across existing DETR-based methods. Our approach specifically targets the accurate retrieval of short-duration moments, thereby filling a significant gap in the current landscape of DETR-based MR research.\n\n\\begin{figure}[tb]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{assets/method_features_diversity.pdf}\n    \\caption{ \\textbf{Data perspective analysis}. t-SNE~\\cite{van2008tsne} visualization of visual features for 50 sampled short moments and 50 non-short moments from \\textsc{QVHighlights} \\textit{train} set. Each sampled moment is taken from a distinct video. This plot demonstrates that short moments exhibit fewer and significantly sparser visual features compared to non-short moments, highlighting the limited visual information inherent to their short duration.\n    }\n    \\label{fig:method_feature_diversity}\n\\end{figure}\n\n\\subsection{Mixing-based Augmentation}\nMixing-based augmentations have been explored in both image and video tasks, each adapting spatial or temporal mixing based on task requirements.\nIn image classification, Mixup~\\cite{zhang2017mixup} and CutMix~\\cite{yun2019cutmix} create new image samples by interpolating or combining patches, promoting diverse feature representations. Copy-Paste~\\cite{ghiasi2021copypaste} augments data for detection and segmentation by inserting objects from one image into another, increasing object and scene variety.\nIn video understanding tasks, VideoMix~\\cite{yun2020videomix} inserts randomly selected video cuboid patches from one video into another, thereby introducing both spatial and temporal diversity. Similarly, VIPriors~\\cite{kim2020VIPriors} extends traditional image-based mixing augmentations to the temporal dimension, which strengthens temporal feature representations and improves model robustness against temporal fluctuations.\n\nHowever, these approaches primarily focus on modifying spatial features and are not directly applicable to the Moment Retrieval framework, which relies solely on frame-level features without spatial dimensions. To address this limitation, we introduce a novel data augmentation technique specifically designed for moment retrieval that enhances the diversity of frame-level features. \n\n\\section{Method}\n\\label{sec:method}\n\n\\begin{figure}[tb]\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{assets/method_center_length.pdf}\n    \\caption{ \\textbf{Model perspective analysis}.     We analyze the top-1 predictions of QD-DETR on the \\textsc{QVHighlights} \\textit{val} set.     \n    [Left] Percentage of predictions where the predicted center falls within the ground truth. Only 37\\% of center predictions for short moments have their centers within the ground truth, indicating substantial errors in center prediction.\n    [Right] Confusion matrix displaying predicted moment lengths (x-axis) versus ground-truth moment lengths (y-axis) across various durations. Short moments (highlighted in red) show a high rate of length prediction errors. \n    }\n    \\label{fig:method_predictions}\n\\end{figure}\n\n\\subsection{Motivation}\n\n\\begin{figure*}[tb]\n    \\centering\n    \\includegraphics[width=0.95\\linewidth]{assets/method_overview.pdf}\n    \\caption{\\textbf{Overview.} In \\Cref{subsec:MomentMix_Augmentation}, we propose MomentMix, which comprises two types of mix-based data augmentations: ForegroundMix and BackgroundMix. ForegroundMix implementation involves cutting a long moment into shorter segments, which are then shuffled to generate new short-moment data. The BackgroundMix preserves the foregrounds unchanged and replaces the backgrounds with temporal segments randomly cropped from other video samples. In \\Cref{subsec:Length_Aware_Decoder}, We introduce a Length-Aware Decoder, which utilizes a length-wise bipartite matching process to create length-wise expert decoder queries.\n    }\n    \\label{fig:method_overview}\n\\end{figure*}\n\n\\noindent\\textbf{Background.} Suppose that a video consists of $\\mathcal{N}_v$ clips, ${\\{v_i\\}}_{i=1}^{\\mathcal{N}_v}$, and a text query of $\\mathcal{N}_t$ words, ${\\{t_i\\}}_{i=1}^{\\mathcal{N}_t}$.\nThe objective of moment retrieval (MR) is to predict a set of $\\mathcal{N}_m$ moments, $\\{m_i\\}_{i=1}^{\\mathcal{N}_m}$, corresponding to video clips relevant to the text query.\nEach moment $m_i$ is defined by its center coordinate $c_i$ and length (span) $\\sigma_i$, representing a contiguous subset of video clips. In this paper, we classify moments based on the following criteria: 1) A temporal moment within the video is defined as \\textit{foreground} if it is relevant to the text query, and as \\textit{background} if it is not. 2) Moments are categorized as short (less than 10 seconds), middle (10 to 30 seconds), or long (over 30 seconds) based on their temporal duration, consistent with the classification used in the previous method~\\cite{lei2021detecting_Moment-DETR}.\n\n\\vspace{1mm}\n\\noindent\\textbf{Performance limit of DETR-based methods.} Recent approaches actively employ DETR for the MR task and achieve impressive performances.\nWe selected representative DETR-based models and analyzed their performance according to the length of target moments. Despite achieving strong performance, these models exhibited significant performance drops in retrieving short moments, as shown in \\Cref{tab:table_performance_moment_length}. \nSpecifically, QD-DETR, TR-DETR, and UVCOM experienced mAP declines of - 79.8\\%, 78.0\\%, and 72.4\\% for short moments. These results indicate a consistently significant decrease relative to overall performance. To address this degradation, we investigated the underlying causes from both data-centric and model-centric perspectives.\n \n\nFor the data-centric analysis, we examined the statistical characteristics of short moments. As shown in \\Cref{fig:intro_dataset_info}, while the total number of short moments is comparable to that of other types of moments, the number of videos containing short moments is clearly limited. This led us to hypothesize that short moments might lack diverse contextual representation and could exhibit a narrow distribution in the training data. To test this hypothesis, we compared the feature distribution of short moments to that of other moments using feature visualization. We randomly sampled 50 short moments and 50 non-short moments from the training set and applied t-SNE~\\cite{van2008tsne} to visualize their visual features. As shown in \\Cref{fig:method_feature_diversity}, the distribution of visual features for short moments was concentrated, indicating a significant lack of diversity. This observation suggests that the training data for short moments does not capture a wide range of visual features, leading to suboptimal generalization performance during testing.\n \n\nFor the model-centric analysis, we evaluated the model's prediction tendencies by separately assessing the center and length predictions of short moments compared to other types of moments. As illustrated in \\Cref{fig:method_predictions}, only 37\\% of center predictions for short moments have their center within the ground truth, while 74\\% for middle moment and 82\\%  for long moments. This revealed that inaccuracies in center prediction are a significant source of overall error.\n\nTo overcome these limitations, we propose two novel techniques, MomentMix and Length-Aware Decoder that can be easily integrated into other DETR-based models. Our overall architecture is illustrated in \\Cref{fig:method_overview}, following the design of the common baseline, QD-DETR.\n\n\\subsection{MomentMix: Leveraging ForegroundMix and BackgroundMix}\n\\label{subsec:MomentMix_Augmentation}\n\nWe propose MomentMix, a data augmentation strategy designed to address the issue of low feature diversity for short moments. MomentMix consists of two components: (1) ForegroundMix, which enhances the diversity of foreground features, and (2) BackgroundMix, which increases the variety of background features. To the best of our knowledge, this is the first data augmentation approach specifically tailored for video moment retrieval.\n\n\\vspace{1mm}\n\\noindent\\textbf{ForegroundMix augmentation.} The goal of ForegroundMix is to increase the visual diversity of foreground features in short moments, enabling more generalized prediction. To achieve this, we randomly extract and mix rich foreground features from longer samples to create augmented short moments. Visual features within a single video naturally exhibit higher similarity compared to those from different videos. By exploiting features from other video clips, our method allows the model to generalize diverse contexts, ensuring it can detect short moments reliably even in challenging or ambiguous frames.\n\nGiven an existing video training sample \\(X = \\{v_i\\}_{i=0}^{\\mathcal{N}_v}\\) that contains a long foreground (moment) \\( f_\\text{source} = \\{v_i\\}_{i=s}^{e} \\),  this foreground can be divided into sub-foregrounds \\( f_1, f_2, \\dots, f_n \\) as follows:\n\n\\begin{equation}\nf_\\text{source} = \\bigcup_{i=1}^{n} f_i, \\:\\: \\text{where} \\:\\:  f_i \\cap f_j = \\emptyset \\:\\: \\text{for all} \\:\\: i \\neq j.\n\\end{equation}\n\nHere, \\( n = \\frac{\\texttt{len}(f_\\text{source})}{{\\varepsilon}_\\text{cut}} \\), where \\(\\varepsilon_\\text{cut}\\) is a hyperparameter determining the extent to which each sub-foreground is shortened relative to the original long foreground.\n\nThese sub-regions represent segments of the foreground, uniformly sampled as \\( f_i^s, f_i^e \\sim \\text{Unif}(s, e) \\), where $s$ and $e$ indicate start and end of \\(f_\\text{source}\\).\nSimilarly, the background region, $b_\\text{source} = b_\\text{front} \\cup b_\\text{back}$, is divided into \\( n+1 \\) sub-regions denoted as \\( b_0, b_1, \\dots, b_{n} \\), representing the segments of the background as:\n\\begin{equation}\nb_\\text{source} = \\bigcup_{i=0}^{n} b_i \\:\\: \\text{where} \\:\\: b_i \\cap b_j = \\emptyset \\:\\: \\text{for all} \\:\\: i \\neq j.\n\\end{equation}\nThe original foreground, $\\{f_i\\}_{i=1}^{n}$, and backgrounds, $\\{b_i\\}_{i=0}^{n}$, are then shuffled as\n\\begin{equation}\n\\begin{gathered}\n    \\pi: \\{f_1, f_2, \\dots, f_n\\} \\rightarrow \\{f_1', f_2', \\dots, f_n'\\}, \\\\\n    \\pi: \\{b_0, b_1, \\dots, b_n\\} \\rightarrow \\{b_0', b_1', \\dots, b_n'\\}, \\\\\n\\end{gathered}\n\\end{equation} where $\\pi$ be a random permutation function.\n\nEach shuffled foregrounds $\\{f'_i\\}_{i=1}^{n}$ is then paired with backgrounds $\\{b'_i\\}_{i=0}^{n}$ to form the following augmented samples:\n\\begin{equation}\n\\begin{gathered}\n    X' = b'_0 \\cup \\bigcup_{i=1}^{n} (f'_i \\cup b'_{i}).\n\\end{gathered}\n\\end{equation}\n\n\\noindent\\textbf{BackgroundMix augmentation.} The goal of BackgroundMix is to improve the diversity of the visual background features, thereby strengthening the association between foreground visual features and the text query. To achieve this, we keep the original foreground features while replacing the background with features from different videos. This method provides the model with richer training signals, allowing it to learn various boundaries more effectively.\n\nA given the \\(k\\)-th video training sample \\( X^k \\), consists of \\(\\mathcal{N}^k_f\\) foreground segments \\( f^k = \\{f^k_i\\}_{i=1}^{\\mathcal{N}^k_f} \\) and \\(\\mathcal{N}^k_b\\) background segments \\( b^k = \\{b^k_i\\}_{i=1}^{\\mathcal{N}^k_b} \\). All segments within the video are defined as follows:\n\n\\begin{equation}\na^k =  f^k \\cup b^k = \\{a^k_i\\}_{i=1}^{\\mathcal{N}^k_a},\n\\text{where } \\mathcal{N}^k_a = \\mathcal{N}^k_f + \\mathcal{N}^k_b.\n\\end{equation}\n\nTo increase feature diversity, we replace each background segment \\(b^k_i\\) of the \\(k\\)-th sample with a randomly cropped segment from a different training sample \\(X^m\\) (\\(m \\neq k\\)). Specifically, for each \\(b^k_i\\), a segment \\(a^m_j\\) is randomly selected from \\(X^m\\) and cropped to match the duration of \\(b^k_i\\). The replacement is performed as follows:\n\n\\[\nb^k_i \\leftarrow \\text{Crop}(a^m_j, \\lvert b^k_i \\rvert)\n\\]\n\nThis approach ensures that while the backgrounds of the \\(k\\)-th sample are augmented with diverse background features, the original foreground remains intact.\n\n\\subsection{Length-Aware Decoder}\n\\label{subsec:Length_Aware_Decoder}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=1.00\\linewidth]{assets/method_matching.pdf}\n    \\caption{\n    [Left] Group-DETR~\\cite{chen2023group_Group_DETR} employs one-to-many matching, where the same labels are utilized across all groups.\n    [Right] Our length-wise matching is one-to-one, and it operates within each length class.\n    By matching only the predictions and ground truths that belong to the same class, this approach enables the creation of length-wise expert queries.\n    }\n    \\label{fig:matching}\n\\end{figure}\n\n\\noindent \n\nIn our previous analysis, we identified that the model struggles to accurately predict both the center and the length of short moments. To address this issue, we propose a length-aware decoder that conditions the moment length, enabling the model to focus more effectively on center prediction. We categorize moment lengths into distinct classessuch as short, middle, and longby analyzing a cumulative mAP graph and identifying inflection points as boundaries. (Detailed information can be found in the supplementary materials.) The decoder queries are trained using a length-wise matching approach based on these length categories. This categorization creates length-wise expert queries that better handle the specific characteristics of different moment lengths.\n\n\\vspace{1mm}\n\\noindent\\textbf{Decoder queries with class-pattern.} We define $\\mathcal{N}_c$ as the number of length classes for assigning roles to decoder queries. Drawing inspiration from Anchor-DETR~\\cite{wang2109anchor_Anchor_DETR}, we interpret \\textit{pattern} in pattern embedding as a length category and create \\textit{class-pattern embeddings} \\(Q_c\\):\n\\begin{equation}\n    Q_c = \\texttt{Embedding}(\\mathcal{N}_c, d) \\in \\mathbb{R}^{\\mathcal{N}_c \\times d}.\n\\end{equation}\nBy replicating each class-pattern embedding $\\mathcal{N}_q$ times (the number of queries per length), we obtain class-specific queries \\(Q \\in \\mathbb{R}^{\\mathcal{N}_c \\mathcal{N}_q \\times d}\\). This approach ensures that decoder queries share the same class embedding within each length category, enabling each query to perform roles tailored to its specific length class.\n\n\\vspace{1mm}\n\\noindent\\textbf{Length-wise matching.}\nTo create the length-wise expertise within class-pattern embeddings, we revised the bipartite matching approach to operate on a per-class basis.\nThis method ensures that class-specific queries are matched and trained only with ground truth moments of the corresponding length class.\nBy categorizing ground truth moments into length classes and performing length-class-wise matching, we ensure precise alignment.\nAlthough this may resemble group-wise matching in object detection~\\cite{chen2023group_Group_DETR}, it differs significantly. As shown in \\Cref{fig:matching}, existing methods use the same labels across all groups, resulting in one-to-many label assignments.\nIn contrast, our approach assigns a unique subset of labels to each length class, enabling one-to-one assignments and effectively creating a \"length-wise expert\" for matching.\n\nWe denote $\\hat y = \\{\\hat y_i \\}^{\\mathcal{N}_c \\mathcal{N}_q}_{i=1}$ as all the predicted moments from the decoder head, where $\\mathcal{N}_c$ and $\\mathcal{N}_q$ are a number of classes and a number of queries for each class $k \\in \\texttt{length-classes}$, respectively.\nThen, the predictions belonging to class $k$ can be denoted as:\n\\begin{equation}\n    \\label{equation_ground_truth}\n    \\hat y^{(k)}  = \\{\\hat y_i \\: \\mid \\: i^{th}\\ query \\in \\texttt{class}\\ k \\}, \\\\\n\\end{equation}\nWhen all ground truth moments are denoted as \\(y = \\{y_i\\}^{\\mathcal{N}_y}_{i=1}\\), ground truth moments belonging to a specific class $k$ can be defined as:\n\\begin{equation}\n    \\label{equation_ground_truth2}\n    y^{(k)} = \\{ y_i \\: \\mid \\:  \\texttt{length}(y_i)  \\: \\in \\: \\texttt{class} \\: k \\: \\}. \\\\\n\\end{equation}\nFor bipartite matching, by applying background \\(\\varnothing\\) padding to make each set size $\\mathcal{N}_q$, the final ground truth set becomes $\\tilde y^{(k)} = \\{{y_i^{(k)}}\\}_{i=1}^{\\mathcal{N}_q}$. \nThe bipartite matching for each class $k$ is determined by finding the lowest cost among permutations of $\\mathcal{N}_q$ elements, denoted as $\\sigma \\in {\\mathfrak{S}}_{\\mathcal{N}_q}$.\n\\begin{equation}\n    \\label{equation_ground_truth3}\n    \\hat \\sigma^{(k)} = \\argmin_{\\sigma \\in {\\mathfrak{S}}_{\\mathcal{N}_q}} \\sum_{i}^{\\mathcal{N}_q} \\{ {{\\mathbb{C}_{match}} (\\tilde y^{(k)}_i, \\hat y^{(k)}_{\\sigma(i)})} \\},\n\\end{equation}  where \\({\\mathbb{C}_{match}}\\) is a \\textit{matching cost} between ground truth and prediction. The matching cost function is set identically to that of the previous method~\\cite{lei2021detecting_Moment-DETR}. \n\nThis approach of class-wise matching aids in explicitly determining the length classes that were implicitly carried by the moment queries. By combining all the results from the bipartite matching for each class, we achieve an efficient matching that considers moment length.\n\n\\section{Experiments}\n\\label{sec:experiments}\n\n\\subsection{Experimental Setup}\n\\label{sec:experiment_setup}\n\n\\noindent\\textbf{Datasets.}\nWe utilized three datasets, {\\textsc{QVHighlights}} \\cite{lei2021detecting_Moment-DETR}, \\textsc{Charades-STA} \\cite{gao2017tall_dataset_charades}, and \\textsc{TACoS}, \\cite{regneri2013grounding_dataset_tacos}) for evaluation.\n\\textsc{QVHighlights} consists of over 10k YouTube videos covering various topics such as everyday activities, travel, social activities, and political activities.\nIt contains moments of various lengths distributed evenly and allows for testing our intended aspects effectively, as multiple moments appear within a single video.\nConsidering the diversity and complexity of the dataset, it covers the most realistic and challenging scenario. \n\\textsc{Charades-STA} focuses on daily indoor activities, comprising 9,848 videos with 16,128 annotated queries.\nThe lengths of moments are mostly below 20 seconds.\n\\textsc{TACoS} primarily features activities in the cooking domain, consisting of 127 videos with 18,818 queries.\nThe video lengths vary from very short to nearly 800 seconds, with most moments being shorter than 30 seconds.\nUnlike \\textsc{QVHighlights} and \\textsc{TACoS}, which encompass moments of varying lengths, \\textsc{Charades-STA} is predominantly composed of shorter samples (under 30 seconds), thus it does not fully align with the problem we aim to address. Nevertheless, we include it in our evaluation to assess the model's generalization performance.\n\n\\vspace{1mm}\n\\noindent\\textbf{Evaluation metrics.}\nFollowing the metrics of existing methods, we use mean average precision (mAP) with Intersection of Union (IoU) thresholds of 0.5 and 0.75, as well as the average mAP over multiple IoU thresholds [0.5: 0.05: 0.95].\nAdditionally, we report the standard metric Recall@1 (R1) metric, commonly used in single-moment retrieval, with IoU thresholds of 0.5 and 0.7. Also, we report the average R1 over multiple IoU thresholds [0.5: 0.05: 0.95].\n\n\\vspace{1mm}\n\\noindent\\textbf{Implementation details.}\nWe divided the classes based on the point where the change in performance was most significant in the validation results of each baseline model. To achieve this, we plotted the cumulative mAP graph with respect to length and identified the inflection points.\nThe thresholds for class division were then determined by calculating the k-means centers of these inflection points.\nAs a result, we set the thresholds for \\textsc{QVHighlights} as [12, 36, 65, \\(\\inf\\)], for \\textsc{Charades-STA} as [5.67, 14, \\(\\inf\\)], and for \\textsc{TACoS} as [10, 19, 38, \\(\\inf\\)], using UVCOM as baseline.\n\nIn ForegroundMix, we set \\(\\varepsilon_\\text{cut}=5\\) for \\textsc{QVHighlights} and \\textsc{TACoS} and  \\(\\varepsilon_\\text{cut}=10\\) for \\textsc{Charades-STA} to create shorter moments. In Length-Aware Decoder, the number of queries per class $ \\mathcal{N}_q$ was set to 10. \n\nFor a fair comparison, we utilize the same features that previous works used. \nOn \\textsc{QVHighlights} and \\textsc{TACoS}, the video features are extracted from SlowFast~\\cite{feichtenhofer2019slowfast_slowfast} and CLIP visual encoder~\\cite{radford2021learning_CLIP}.\nOn \\textsc{Charades-STA}, we use two feature types as in previous works.\nThe first type is the video features from SlowFast and CLIP visual encoder, and text features extracted from the CLIP text encoder.\nThe second type is video features extracted from VGG \\cite{simonyan2014very_vgg} and text features extracted from GloVe \\cite{pennington2014glove_glove}. \n\nThe model is trained for 200 epochs on all datasets with learning rates 1e-4. The batch size is 32 for \\textsc{QVHighlights}, 8 for \\textsc{Charades-STA}, and 16 for \\textsc{TACoS}, following previous methods. We kept all the baseline parameters.\n\n\\begin{table}[ht]\n\n\\setlength{\\tabcolsep}{0.3em}\n\\centering\n\n\\caption{Performance gains of our method on the \\textsc{QVHighlights} \\textit{test} set across different moment lengths.\n}\n\n\\begin{small}\n\\setlength{\\tabcolsep}{2pt}\n\n\\resizebox{\\linewidth}{!}\n{\n\n    \\begin{tabular}{\n        l    c c   c c   c c    c c\n    }\n\n    \n    \\toprule\n    \n\n    \\multirow{3}{*}{Method} & \\multicolumn{2}{c}{\\textbf{Short}} & \\multicolumn{2}{c}{Middle} & \\multicolumn{2}{c}{Long}  & \\multicolumn{2}{c}{All} \\\\\n\n    \\cmidrule(l){2-3}  \\cmidrule(l){4-5}  \\cmidrule(l){6-7}  \\cmidrule(l){8-9}\n\n    \n      & R1 & mAP & R1 & mAP & R1 & mAP  & R1 & mAP \\\\    \n\n    \\midrule\n\nQD-DETR~\\cite{moon2023query_QD-DETR}\t & 3.95\t & 6.98\t & 37.39\t & 41.12\t & 42.86\t & 46.95\t & 40.01\t & 39.84\t\\\\\n\\rowcolor{gray!10} ~ +Ours\t & 13.47\t & 15.36\t & 39.57\t & 44.93\t & 46.15\t & 52.75\t & 45.03\t & 46.03\t\\\\\n\t& \\gainp{+9.52} \t& \\gainp{+8.38} \t& \\gainp{+2.18} \t& \\gainp{+3.81} \t& \\gainp{+3.29} \t& \\gainp{+5.80} \t& \\gainp{+5.03} \t& \\gainp{+6.19} \t\\\\\n\\hline\n    \nTR-DETR~\\cite{sun2024tr_TR-DETR}\t & 4.95\t & 8.22\t & 40.08\t & 43.27\t & 47.63\t & 50.80\t & 43.70\t & 42.62\t\\\\\n\\rowcolor{gray!10} ~ +Ours\t & 13.40\t & 15.45\t & 41.59\t & 46.42\t & 47.51\t & 53.01\t & 46.76\t & 47.23\t\\\\\n\t& \\gainp{+8.44} \t& \\gainp{+7.23} \t& \\gainp{+1.51} \t& \\gainp{+3.15} \t& \\gainm{-0.12} \t& \\gainp{+2.21} \t& \\gainp{+3.06} \t& \\gainp{+4.61} \t\\\\\n\\hline\n\n    \nUVCOM~\\cite{xiao2024bridging_UVCOM}\t & 5.28\t & 10.67\t & 41.81\t & 44.90\t & 44.95\t & 48.37\t & 43.85\t & 43.18\t\\\\\n\\rowcolor{gray!10} ~ +Ours\t & 12.54\t & 16.36\t & 42.40\t & 48.30\t & 46.08\t & 52.22\t & 46.51\t & 47.93\t\\\\\n\t& \\gainp{+7.26} \t& \\gainp{+5.69} \t& \\gainp{+0.59} \t& \\gainp{+3.40} \t& \\gainp{+1.13} \t& \\gainp{+3.85} \t& \\gainp{+2.66} \t& \\gainp{+4.75} \t\\\\\n\n    \\bottomrule\n    \\end{tabular}\n}\n\n\\end{small}\n\n\\label{tab:exp_length_test}\n\n\\end{table}\n\n\\begin{table}[t]\n\n\\setlength{\\tabcolsep}{0.3em}\n\\centering\n\n\\caption{Performance comparison on \\textsc{QVHighlights} \\textit{test} set. {\\dag} indicates training with additional audio features.\n}\n\n\\begin{small}\n\\setlength{\\tabcolsep}{2pt}\n\n\\resizebox{\\linewidth}{!}\n{\n\n    \\begin{tabular}{\n         l  c c c  c c c  c c\n    }\n\n    \n    \\toprule\n    \n\n\\multirow{3}{*}{Method} & \\multicolumn{6}{c}{\\textbf{MR}}  & \\multicolumn{2}{c}{\\textbf{HD}} \\\\\n\n\\cmidrule(l){2-7} \\cmidrule(l){8-9}\n\n& \\multicolumn{3}{c}{R$1$}  & \\multicolumn{3}{c}{mAP} & \\multicolumn{2}{c}{$\\geq$ Very Good} \\\\\n\n\\cmidrule(l){2-4} \\cmidrule(l){5-7} \\cmidrule(l){8-9}\n\n & @$0.5$ & @$0.7$ & Avg. & @$0.5$ & @$0.75$ & Avg. & mAP & HIT@$1$ \\\\\n\n    \\midrule\n\nM-DETR~\\cite{lei2021detecting_Moment-DETR}\t & 52.89\t & 33.02\t& -\t & 54.82\t & 29.40\t & 30.73\t & 35.69\t & 55.60\t\\\\\nUMT $\\dagger$~\\cite{liu2022umt_Umt}\t & 56.23\t & 41.18\t& -\t & 53.83\t & 37.01\t & 36.12\t & 38.18\t & 59.99\t\\\\\nEaTR~\\cite{jang2023knowing_EaTR}\t & 57.98\t & 42.41\t& -\t & 59.95\t & 39.29\t & 39.00\t& -\t& -\t\\\\\nUniVTG~\\cite{lin2023univtg_Univtg}\t & 58.86\t & 40.86\t& -\t & 57.60\t & 35.59\t & 35.47\t & 38.20\t & 60.96\t\\\\\nMomentDiff~\\cite{li2024momentdiff_momentdiff}\t & 57.42\t & 39.66\t& -\t & 54.02\t & 35.73\t & 35.95\t& -\t& -\t\\\\\nMESM~\\cite{liu2024towards_MESM}\t & 62.78\t & 45.20\t& -\t & 62.64\t & 41.45\t & 40.68\t& -\t& -\t\\\\\nTaskWeave~\\cite{zhang2024temporally_TaskWeave}\t & 61.87\t & 46.24\t & 42.45\t & 63.75\t & 43.63\t & 43.33\t & 37.87\t & 59.08\t\\\\\nBAM-DETR~\\cite{lee2025bam-detr}\t & 62.71\t & 48.64\t& -\t & 64.57\t & 46.33\t & 45.36\t& -\t& -\t\\\\\n\n\\hline\nQD-DETR~\\cite{moon2023query_QD-DETR}\t & 61.22\t & 44.49\t & 40.01\t & 62.31\t & 39.45\t & 39.84\t & 39.01\t & 62.13\t\\\\\n\\rowcolor{gray!10} ~ +Ours\t & 64.01\t & 48.90\t & 45.03\t & 64.69\t & 47.23\t & 46.03\t & 40.32\t & \\textbf{66.99}\t\\\\\n\t& \\gainp{+2.79} \t& \\gainp{+4.41} \t& \\gainp{+5.03} \t& \\gainp{+2.38} \t& \\gainp{+7.78} \t& \\gainp{+6.19} \t& \\gainp{+1.31} \t& \\gainp{+4.86} \t\\\\\n\n\\hline\n    \nTR-DETR~\\cite{sun2024tr_TR-DETR}\t & \\underline{64.66}\t & 48.96\t & 43.70\t & 63.98\t & 43.73\t & 42.62\t & 39.91\t & 63.42\t\\\\\n\\rowcolor{gray!10} ~ +Ours\t & \\textbf{65.43}\t & \\underline{50.13}\t & \\textbf{46.76}\t & \\textbf{65.66}\t & \\underline{47.89}\t & \\underline{47.23}\t & \\textbf{41.41}\t & \\underline{65.63}\t\\\\\n\t& \\gainp{+0.77} \t& \\gainp{+1.17} \t& \\gainp{+3.06} \t& \\gainp{+1.68} \t& \\gainp{+4.16} \t& \\gainp{+4.61} \t& \\gainp{+1.50} \t& \\gainp{+2.21} \t\\\\\n\n\\hline\n    \nUVCOM~\\cite{xiao2024bridging_UVCOM}\t & 63.55\t & 47.47\t & 43.85\t & 63.37\t & 42.67\t & 43.18\t & 39.74\t & 64.20\t\\\\\n\\rowcolor{gray!10} ~ +Ours\t & 63.94\t & \\textbf{51.10}\t & \\underline{46.51}\t & \\underline{65.65}\t & \\textbf{49.44}\t & \\textbf{47.93}\t & \\underline{40.39}\t & 65.50\t\\\\\n\t& \\gainp{+0.39} \t& \\gainp{+3.63} \t& \\gainp{+2.66} \t& \\gainp{+2.28} \t& \\gainp{+6.77} \t& \\gainp{+4.75} \t& \\gainp{+0.65} \t& \\gainp{+1.30} \t\\\\\n    \n    \\bottomrule\n    \\end{tabular}\n}\n\n\\end{small}\n\n\\label{tab:exp_qv_test}\n\n\\end{table}\n\n\\begin{table}[ht]\n\n\\centering\n\n\\caption{Results on \\textsc{Charades-STA} and \\textsc{TaCoS} \\textit{test} set. {\\ddag} indicates training with VGG features and GloVe features.}\n\n\\setlength{\\tabcolsep}{0.05pt}\n\n\\begin{small}\n\n\\resizebox{1\\linewidth}{!}\n{\n\n    \\begin{tabular}{\n        l c   c c   c c   c c\n    }\n    \\toprule\n    \\multirow{2}{*}{Method} & \\multicolumn{2}{c}{\\textsc{Charades-STA}} & \\multicolumn{2}{c}{\\textsc{TACoS}} & \\multicolumn{2}{c}{\\textsc{Charades-STA}$^{\\ddag}$}\\\\\n    \\cmidrule(l){2-3}  \\cmidrule(l){4-5}  \\cmidrule(l){6-7}\n    & R1@0.5 & R1@0.7 & R1@0.5 & R1@0.7 & R1@0.5 & R1@0.7\\\\\n\n    \\midrule\nSAP~\\cite{gao2021fast_SAP}          & -              & -              & -          & -          & 27.42          & 13.36 \\\\\nSM-RL~\\cite{wang2019language_sm_rl}        & -              & -              & -          & -          & 24.36          & 11.17 \\\\\nMAN~\\cite{Zhang_2019_CVPR_man}          & -              & -              & -          & -          & 41.24          & 20.54 \\\\\n2D-TAN~\\cite{zhang2020learning_2d_tan}       & 46.02          & 27.50          & 27.99      & 12.92      & 40.94          & 22.85 \\\\\nVSLNet~\\cite{zhang2020span_vslnet}       & 42.69          & 24.14          & 23.54      & 13.15      & -              & - \\\\\nM-DETR~\\cite{lei2021detecting_Moment-DETR}       & 53.63          & 31.37          & 24.67      & 11.97      & -              & - \\\\\nQD-DETR~\\cite{moon2023query_QD-DETR}      & 57.31          & 32.55          & -          & -          & 52.77          & 31.13 \\\\\nUniVTG~\\cite{lin2023univtg_Univtg}       & 58.01          & 35.65          & 34.97      & 17.35      &                &  \\\\\nMESM~\\cite{liu2024towards_MESM}         & \\underline{61.24} & {38.04}          & -          & -          & \\textbf{56.69}          & {35.99} \\\\\nTR-DETR~\\cite{sun2024tr_TR-DETR}      & 57.61          & 33.52          & -          & -          & 53.47          & 30.81 \\\\\nTaskWeave~\\cite{zhang2024temporally_TaskWeave}    & {56.51}          & 33.66          & -          & -          & \\underline{56.51}          & 33.66 \\\\\n\nBAM-DETR~\\cite{lee2025bam-detr}        & 59.83          & \\underline{39.83}          & \\underline{41.54}      & \\underline{26.77}      & -  & - \\\\\n\n\\hline\n\nUVCOM~\\cite{xiao2024bridging_UVCOM}        & 59.25          & 36.64          & 36.39      & 23.32      & 54.57          & \\underline{34.13} \\\\\n\n\\rowcolor{gray!10}~+Ours & \\textbf{61.45}    & \\textbf{40.22}   & \\textbf{42.31} & \\textbf{28.09}   & {56.16} & \\textbf{36.10} \\\\\n\n& \\gainp{+2.20} \t& \\gainp{+3.58} \t& \\gainp{+5.92} \t& \\gainp{+4.77} \t& \\gainp{+1.59} \t& \\gainp{+1.97} \\\\\n\n    \\bottomrule\n    \\end{tabular}\n}\n\n\\end{small}\n\n\\label{tab:exp_cha_tacos}\n\n\\end{table}\n\n\\begin{table*}[!t]\n    \\caption{\n        Performance comparison with baseline(QD-DETR) on \\textsc{QVHighlights} \\textit{val} set. FGMix, BGMix, and LAD indicate \\underline{F}ore\\underline{G}round \\underline{M}ix, \\underline{B}ack\\underline{G}round \\underline{M}ix and \\underline{L}ength-\\underline{A}ware \\underline{D}ecoder, respectively. \n    }\n    \\label{tab:exp_component_analysis}\n    \\centering\n\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{0.85\\linewidth}{!}\n{\n    \\begin{tabular}{\n        c c c   c c   c c   c c   c c c    c c c \n    }\n    \\toprule\n\n    \\multicolumn{3}{c}{\\multirow{3}{*}{Method}} & \\multicolumn{2}{c}{Short}  & \\multicolumn{2}{c}{Middle }  & \\multicolumn{2}{c}{Long } & \\multicolumn{6}{c}{All}  \\\\\n\n    \\cmidrule(l){4-5}  \\cmidrule(l){6-7} \\cmidrule(l){8-9} \\cmidrule(l){10-15} \n    \n    & & & R1 & mAP & R1 & mAP & R1 & mAP & \\multicolumn{3}{c}{R1} & \\multicolumn{3}{c}{mAP}  \\\\\n\n    \\cmidrule(l){4-5}  \\cmidrule(l){6-7} \\cmidrule(l){8-9} \\cmidrule(l){10-12}  \\cmidrule(l){13-15} \n    \n    FGMix & BGMix & LAD &\\multicolumn{2}{c}{Avg.} & \\multicolumn{2}{c}{Avg.} & \\multicolumn{2}{c}{Avg.} & @0.5 & @0.7 & Avg.  & @0.5 & @0.75 & Avg.  \\\\\n    \\midrule\n    \n    \\nomark   & \\nomark & \\nomark\t & 4.57\t & 7.77\t & 38.89\t & 43.10\t & 42.62\t & 47.44\t & 61.39\t & 46.18\t & 41.06\t & 61.68\t & 41.57\t & 41.00\t\\\\\n    \\yesmark   & \\nomark & \\nomark\t & 7.86\t & 12.21\t & 41.42\t & 45.28\t & 43.45\t & 47.69\t & 64.06\t & 48.97\t & 43.84\t & 64.15\t & 43.96\t & 43.32\t\\\\\n    \\nomark   & \\yesmark & \\nomark\t & 6.27\t & 11.07\t & 42.74\t & 46.51\t & 44.55\t & 48.57\t & 63.87\t & 49.03\t & 44.62\t & 63.64\t & 44.23\t & 43.86\t\\\\\n    \\nomark   & \\nomark & \\yesmark\t & 6.99\t & 10.26\t & 39.48\t & 44.68\t & 43.34\t & 51.23\t & 60.77\t & 46.71\t & 42.36\t & 62.89\t & 45.47\t & 43.91\t\\\\\n    \\yesmark   & \\yesmark & \\nomark\t & 7.60\t & 13.35\t & \\textbf{42.95}\t & 47.20\t & 45.23\t & 48.42\t & 63.87\t & 50.19\t & 45.37\t & 63.96\t & 46.21\t & 44.95\t\\\\\n    \\yesmark   & \\yesmark & \\yesmark\t & \\textbf{12.26}\t & \\textbf{16.44}\t & 42.89\t & \\textbf{47.69}\t & \\textbf{45.78}\t & \\textbf{52.75}\t & \\textbf{64.32}\t & \\textbf{50.90}\t & \\textbf{46.83}\t & \\textbf{65.49}\t & \\textbf{48.92}\t & \\textbf{47.84}\t\\\\\n\n        \\bottomrule\n    \\end{tabular}\n}\n\n\\end{table*}\n\n\\begin{table}[!t]\n    \\caption{\n        Results on the \\textsc{QVHighlights} \\textit{val} set using 50\\%, 20\\%, and 10\\% of the original training data.\n    }\n    \\label{tab:exp_semi}\n    \\centering\n\n\\setlength{\\tabcolsep}{4pt}\n\\resizebox{0.95\\linewidth}{!}\n{\n    \\begin{tabular}{\n        l   c c c   c c c \n    }\n    \\toprule\n    {\\multirow{3}{*}{Method}} & \\multicolumn{3}{c}{R1} & \\multicolumn{3}{c}{mAP}  \\\\\n\n    \\cmidrule(l){2-4}  \\cmidrule(l){5-7}  \n    \n     & @0.5 & @0.7 & Avg.  & @0.5 & @0.75 & Avg.  \\\\\n    \\midrule\n\n100\\% train data & 61.39\t & 46.18\t & 41.06\t & 61.68\t & 41.57\t & 41.00\t\\\\\n\\hline\n 50\\% train data & 57.23\t & 40.26\t & 36.10\t & 57.51\t & 35.63\t & 35.98\t\\\\\n\\rowcolor{gray!10} ~ + MomentMix\t & 63.16\t & 47.74\t & 43.36\t & 61.91\t & 41.90\t & 41.73\t\\\\\n\t& \\gainp{+5.93} \t& \\gainp{+7.48} \t& \\gainp{+7.26} \t& \\gainp{+4.40} \t& \\gainp{+6.27} \t& \\gainp{+5.75} \t\\\\\n\\hline\n    \n 20\\% train data& 46.84\t & 30.45\t & 26.58\t & 48.27\t & 25.35\t & 26.88\t\\\\\n\\rowcolor{gray!10} ~ + MomentMix\t & 52.45\t & 37.68\t & 33.69\t & 52.66\t & 34.25\t & 33.72\t\\\\\n\t& \\gainp{+5.61} \t& \\gainp{+7.23} \t& \\gainp{+7.11} \t& \\gainp{+4.39} \t& \\gainp{+8.90} \t& \\gainp{+6.84} \t\\\\\n\\hline\n    \n 10\\% train data& 32.45\t & 16.84\t & 15.90\t & 37.10\t & 15.37\t & 18.17\t\\\\\n\\rowcolor{gray!10} ~ + MomentMix\t & 43.10\t & 28.71\t & 25.61\t & 44.97\t & 26.12\t & 26.62\t\\\\\n\t& \\gainp{+10.65} \t& \\gainp{+11.87} \t& \\gainp{+9.71} \t& \\gainp{+7.87} \t& \\gainp{+10.75} \t& \\gainp{+8.45} \t\\\\\n\n        \\bottomrule\n    \\end{tabular}\n}\n\n\\end{table}\n\n\\subsection{Results}\\label{sec:results}\n\nWe applied our method to QD-DETR~\\cite{moon2023query_QD-DETR}, a common baseline in many studies.\nHowever, since our method can be easily added to other models, we further validated our method on three recent methods (TR-DETR~\\cite{sun2024tr_TR-DETR}, and UVCOM~\\cite{xiao2024bridging_UVCOM}) to demonstrate its effectiveness.\nWe compared our approach against existing moment retrieval methods, including the latest DETR-based models.\nWhile existing models report only overall performance, we also analyze the performance of each length.\n\n\\vspace{1mm}\n\\noindent\\textbf{Performance with respect to moment length on \\textsc{QVHighlights}.} \nIn \\Cref{tab:exp_length_test}, our method significantly improves short-moment performance across all baselines. Specifically, for QD-DETR, the R1 average and mAP average for short moments increased by +9.52\\% and +8.38\\%, respectively. Moreover, our approach consistently outperforms all baselines in mAP average across all lengths.\n\n\\vspace{1mm}\n\\noindent\\textbf{Overall performance on \\textsc{QVHighlights}.} \nIn \\Cref{tab:exp_qv_test}, our method yields significant improvements across all metrics, indicating enhanced overall performance across all baselines. Notably, while our primary objective was to improve the short moment performance in Moment Retrieval (MR) by enhancing feature diversity, we also observed substantial performance gains in Highlight Detection (HD). This demonstrates that enhancing feature diversity is an effective strategy that can positively impact other tasks as well.\n\n\\vspace{1mm}\n\\noindent\\textbf{Overall performance on \\textsc{Charades-STA} and \\textsc{TACoS}.} \nAs shown in \\Cref{tab:exp_cha_tacos}, our method achieved a substantial performance improvement of +5.92\\% in R1@0.5 on \\textsc{TACoS}, which encompasses moments with a broader range of lengths compared to \\textsc{QVHighlights}. This significant enhancement demonstrates the superior generalization capabilities of our length-aware approach.\n\nIn contrast to other datasets, \\textsc{Charades-STA} contains moments with highly restricted lengths, making it less representative of the problem we aim to address. Nevertheless, we included \\textsc{Charades-STA} in our evaluation to ensure the robustness of our approach. The results show that our method improved R1@0.7 by +3.58\\% when using SlowFast and CLIP features, +1.97\\% using VGG features. \n\n\\subsection{Ablation Studies and Discussions}\\label{sec:Ablation studies}\n\n\\noindent\\textbf{Component analysis.} In \\Cref{tab:exp_component_analysis}, we examined the impact of MomentMix and the Length-Aware Decoder on enhancing performance for short moments, observing overall gains.\nWhile each component individually improves performance, their combined application leads to even greater improvements.\nThis suggests that our two components, MomentMix and Length-Aware Decoder, each contribute effectively without redundancy, making their combined use the effective approach for tackling the challenge of short-moment retrieval.\n\n\\vspace{1mm}\n\\noindent\\textbf{Evaluation in few-shot scenarios.} To validate the effectiveness of MomentMix as a data augmentation technique, we conducted experiments using 50\\%, 20\\%, and 10\\% of the training data.\nAs shown in \\Cref{tab:exp_semi}, our method significantly outperformed the baseline (QD-DETR) with substantial performance gains. Specifically, utilizing only half of the training samples with our augmentation surpassed the baseline performance that employed the entire training dataset. Additionally, even in the extreme scenario of using just 10\\% of the training samples, our method achieved remarkable improvements of +9.71\\% in R1 average and +8.45\\% in mAP. These results indicate that MomentMix effectively generates new training samples by enhancing feature diversity.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{assets/exp_4427_final.png}\n    \\includegraphics[width=\\linewidth]{assets/exp_251_final.png}\n    \\caption{\n        A qualitative result on \\textsc{QVHighlights} \\textit{val} set. Existing models often fail to accurately distinguish between foreground and background, leading to unsuccessful predictions or missed detections of short moments. In contrast, our model is capable of predicting short moments with greater accuracy and robustness. \n    }\n    \\label{fig:exp_qualitative}\n\\end{figure}\n\n\\vspace{1mm}\n\\noindent\\textbf{Qualitative results.} We visualized predictions with confidence scores exceeding 0.7, using an alpha value of 0.5. As shown in \\Cref{fig:exp_qualitative}, by applying our method, short moments predicted as background in other methods can now be accurately captured. Also, predictions that merged multiple short instances into a single long instance can now be segmented into precise, fine-grained predictions.\n\n\\section{Limitation and Conclusion}\n\\label{sec:conclusion}\n\\noindent\\textbf{Limitation.} While we have incorporated length-awareness in the decoder, further investigation into enhancing the encoder is necessary. Moreover, our novel data augmentation method for MR improves performance but also increases training time. Thus, future research should focus on developing more efficient approaches.\n\n\\noindent\\textbf{Conclusion.}\nIn this study, we addressed the limitations of short-moment retrieval in existing DETR-based approaches from both data and model perspectives. To overcome the data-centric issue of limited feature diversity in short moments, we introduced MomentMix, which leverages two mix-based data augmentation strategies: ForegroundMix and BackgroundMix. These strategies enhance the feature representations of both foreground and background elements. On the model side, we identified inaccuracies in center predictions for short moments and proposed a Length-Aware Decoder with a novel bipartite matching process conditioned on moment length. This approach leverages length expert queries to improve center prediction accuracy. Extensive experiments demonstrate that our method surpasses state-of-the-art DETR-based moment retrieval models in terms of R1 and mAP on benchmark datasets. Furthermore, our methodology can be seamlessly integrated with other DETR-based models, paving the way for future advancements in the field.\n\n{\n    \\small\n        }\n\n\\clearpage\n\\newpage\n\\setcounter{page}{1}\n\\maketitlesupplementary\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2311.16464v1.tex",
        "arXiv-2401.02309v2.tex",
        "arXiv-2412.20816v1.tex"
    ],
    "group_id": "group_19",
    "response": "### Title: Advances in Video Moment Retrieval and Highlight Detection: A Comparative Analysis of Recent Frameworks\n\n### Introduction\nThe field of video analysis has seen significant advancements with the rise of deep learning techniques, particularly in tasks such as Video Moment Retrieval (MR) and Highlight Detection (HD). These tasks aim to identify specific moments within a video based on textual descriptions and to detect segments of high saliency, respectively. MR and HD are critical for enhancing user experience in video search and browsing, especially with the increasing volume of online video content. Historically, MR and HD were treated as separate tasks, each with its own set of challenges and methodologies. However, recent research has begun to bridge these tasks by leveraging common features and techniques, such as transformer-based architectures and multi-modal learning, to jointly address them. Despite these advancements, existing methods often struggle to balance the requirements of local and global understanding, which are crucial for accurate moment localization and saliency estimation. This summary explores three recent papers that tackle these challenges by proposing novel frameworks and techniques for MR and HD.\n\n#### Current Progress and Challenges\nCurrent progress in MR and HD has been driven by the development of transformer-based models, which excel in capturing complex relationships between video and textual content. However, these models often face challenges in handling the specific nuances of each task. MR requires a strong focus on local temporal relationships to accurately localize moments, while HD demands a broader understanding of the entire video context to estimate saliency scores effectively. Existing frameworks that treat MR and HD as a multi-task problem often lack task-specific design, leading to suboptimal performance in both tasks. Additionally, the integration of audio modality and the use of different feature extractors have shown promise but also introduce new challenges, such as misalignment and redundancy in feature extraction. This summary aims to highlight recent advancements and innovations in addressing these challenges through novel frameworks and techniques.\n\n### Main Content of Each Paper\n\n#### Paper 1: UVCOM - Unified Video COMprehension Framework\n**UVCOM** is a novel framework designed to address the joint MR and HD tasks by emphasizing the specific characteristics of each task. The authors observe that MR requires a strong focus on local temporal relationships, while HD prioritizes global context understanding. To bridge this gap, UVCOM introduces a Comprehensive Integration Module (CIM) that progressively integrates intra and inter-modality features across multiple granularities. CIM uses Expectation-Maximum (EM) Attention to aggregate clip-level and word-level features into moment-level and phrase-level representations, respectively. This process enhances the local relation modeling and global knowledge accumulation, allowing the framework to achieve a more comprehensive understanding of the video content.\n\nUVCOM also incorporates a Local Relation Perception (LRP) module that utilizes a Bidirectional Modality Random Walk (BMRW) algorithm to refine the attention mechanism, thereby improving the localization accuracy of MR. The BMRW algorithm iteratively updates the visual and textual features to highlight the relevant local context and suppress unrelated content. Furthermore, UVCOM employs a Global Knowledge Accumulation (GKA) module to accumulate global information from the video, using the moment-aware feature as an intermediary. This ensures that the model focuses on the relevant video segments while suppressing background noise.\n\nTo further consolidate the effectiveness of CIM, UVCOM introduces Multi-Aspect Contrastive Learning (MACL) that includes Clip-Text Alignment (CTA) and Video-Linguistic Discrimination (VLD). CTA measures the relevance between clip-level visual features and sentence-level textual embeddings, while VLD ensures that video-level features are closely aligned with relevant textual representations. These components work together to provide explicit supervision for both local and global understanding.\n\n**Prediction Heads and Loss Function**: UVCOM uses two simple heads for MR and HD, respectively. The MR head leverages a standard transformer decoder to generate moment spans, while the HD head employs fully-connected layers to predict saliency scores. The authors balance the losses using hyperparameters to ensure consistent performance across both tasks.\n\n#### Paper 2: TR-DETR - Task-Reciprocal DETR\n**TR-DETR** aims to exploit the natural reciprocity between MR and HD tasks to enhance joint learning. The authors propose a framework that includes a Local-Global Multi-Modal Alignment (LGMA) module, a Visual Feature Refinement (VFR) module, and a Task Cooperation (TC) module. LGMA aligns visual and textual features into a shared latent space, facilitating better multimodal associations. VFR refines the visual features by suppressing irrelevant information and retaining temporal cues, ensuring that the model focuses on the relevant video segments. TC explicitly infuses highlight scores into the MR pipeline and uses the retrieved moments to refine the initial highlight distribution, thereby leveraging the complementarity between the two tasks.\n\n**Local-Global Multi-Modal Alignment**: TR-DETR uses a local regulator to distinguish semantically similar but irrelevant clips and a global regulator to ensure that both modalities share a unified semantic space. The local regulator calculates cosine similarity between each clip and each word, while the global regulator employs a multi-modal contrastive loss to align global representations of paired videos and queries.\n\n**Visual Feature Refinement**: TR-DETR introduces a query-guided visual feature refinement module that uses the textual query to filter out irrelevant information in the visual features. This module employs a similarity matrix to weigh and sum the query and video features, obtaining preliminary refined features. These refined features are then further incorporated into the joint features using a cross-attention layer.\n\n**Task Cooperation**: TR-DETR constructs a task cooperation module consisting of HD2MR and MR2HD components. HD2MR uses the multi-head attention mechanism to obtain clip-level highlight scores and filters out non-highlight information in the joint features. MR2HD leverages the retrieved moments to refine the highlight score distribution, thereby improving the understanding of video content and user needs.\n\n#### Paper 3: LA-DETR - Length-Aware DETR\n**LA-DETR** focuses on addressing the specific challenge of retrieving short moments accurately, a common limitation in existing DETR-based models. The authors propose two novel techniques: MomentMix, a data augmentation strategy, and a Length-Aware Decoder (LAD), which conditions the model on moment length to improve center prediction accuracy. MomentMix consists of two components: ForegroundMix and BackgroundMix. ForegroundMix enhances the diversity of foreground features by mixing rich foreground elements from different moments, while BackgroundMix replaces the background with features from different videos, providing richer training signals.\n\n**Length-Aware Decoder**: LA-DETR introduces a length-wise bipartite matching process that categorizes moments into distinct classes based on their duration. This approach ensures that decoder queries share the same class embedding within each length category, enabling each query to perform roles tailored to its specific length class. The authors set the thresholds for class division by analyzing the cumulative mAP graph and identifying inflection points, leading to more precise alignment between predictions and ground truths.\n\n**Experimental Setup**: LA-DETR evaluates its performance on three datasets: QVHighlights, Charades-STA, and TACoS. The authors use mean average precision (mAP) and Recall@1 (R1) metrics to assess the performance of MR and HD tasks. LA-DETR is trained for 200 epochs with learning rates of 1e-4 and batch sizes of 32, 8, and 16 for QVHighlights, Charades-STA, and TACoS, respectively.\n\n### Commonalities and Innovations\nAll three papers aim to improve the joint performance of MR and HD tasks by leveraging transformer-based architectures and multimodal learning. However, they differ in their approaches to addressing the specific challenges of each task. UVCOM emphasizes the progressive integration of intra and inter-modality features across multiple granularities, using EM Attention and BMRW to enhance local and global understanding. TR-DETR focuses on the reciprocity between MR and HD tasks, introducing a local-global alignment module and a visual feature refinement module to improve multimodal associations and suppress irrelevant information. LA-DETR specifically targets the retrieval of short moments, proposing MomentMix and a Length-Aware Decoder to enhance feature diversity and improve center prediction accuracy.\n\n### Comparison of Results\nThe results from the three papers demonstrate significant improvements in MR and HD performance across various datasets. UVCOM outperforms existing state-of-the-art methods on QVHighlights, Charades-STA, and TACoS datasets, achieving notable gains in mAP and R1 metrics. TR-DETR shows comparable performance on QVHighlights and Charades-STA, with slight improvements in some metrics. LA-DETR achieves substantial performance gains in MR, particularly for short moments, on QVHighlights and TACoS datasets, surpassing other DETR-based models.\n\n**QVHighlights Dataset**:\n- **UVCOM**: Achieves new state-of-the-art performance, outperforming QD-DETR by 3.6% in R1@0.7 and 4% in mAP@0.75 for MR, and 2% in HIT@1 for HD.\n- **TR-DETR**: Outperforms QD-DETR by 3.98% in R1@0.7 and 3.06% in mAP@0.75 for MR, and 1.50% in HIT@1 for HD.\n- **LA-DETR**: Improves short-moment performance by +9.52% in R1 average and +8.38% in mAP average for QD-DETR, and +3.63% in mAP for UVCOM.\n\n**Charades-STA Dataset**:\n- **UVCOM**: Outperforms UniVTG by 6% in R1@0.7 using SlowFast and CLIP features.\n- **TR-DETR**: Achieves comparable performance on VGG and SF+C features, with slight improvements in some metrics.\n- **LA-DETR**: Improves R1@0.7 by +3.58% using SlowFast and CLIP features, and +1.97% using VGG features.\n\n**TACoS Dataset**:\n- **UVCOM**: Boosts performance by 6% in R1@0.7 compared to UniVTG.\n- **TR-DETR**: Shows a significant improvement of +5.92% in R1@0.5 and +4.77% in R1@0.7.\n- **LA-DETR**: Surpasses other DETR-based models in terms of R1 and mAP, achieving the highest performance on benchmark datasets.\n\n### Conclusion\nThe three papers present innovative frameworks and techniques to enhance the joint performance of MR and HD tasks. UVCOM introduces a CIM module to progressively integrate intra and inter-modality features, leading to improved local and global understanding. TR-DETR leverages the reciprocity between MR and HD tasks, using a local-global alignment module and a visual feature refinement module to improve multimodal associations. LA-DETR specifically addresses the challenge of short moment retrieval, proposing MomentMix and a Length-Aware Decoder to enhance feature diversity and improve center prediction accuracy.\n\n**Main Findings**:\n- **UVCOM**: Achieves the best performance on QVHighlights, Charades-STA, and TACoS datasets, demonstrating the effectiveness of its comprehensive integration and contrastive learning modules.\n- **TR-DETR**: Shows comparable performance on QVHighlights and Charades-STA, with slight improvements in some metrics, indicating the benefits of task reciprocity.\n- **LA-DETR**: Surpasses other DETR-based models in MR, particularly for short moments, highlighting the importance of addressing specific challenges within DETR-based architectures.\n\n**Future Research Directions**:\n- **UVCOM**: Future work could focus on improving the handling of auditory descriptions and developing more efficient audio feature extraction methods.\n- **TR-DETR**: Enhancing the utilization of audio modality and exploring novel multi-modal feature interaction networks could further improve performance.\n- **LA-DETR**: Investigating methods to improve feature diversity in the encoder and developing more efficient data augmentation techniques could enhance overall performance.\n\nThese advancements not only improve the accuracy and efficiency of MR and HD tasks but also pave the way for future research in multimodal video analysis, suggesting that task-specific design and novel data augmentation strategies are key to achieving robust and comprehensive video understanding."
}