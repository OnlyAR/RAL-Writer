{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{LongAlign: A Recipe for Long Context Alignment\\\\\nEncompassing Data, Training, and Evaluation}\n\n\\begin{document}\n\n\\maketitle\n\n\\renewcommand{\\thefootnote}{\\fnsymbol{footnote}}\n    \\footnotetext[2]{Work done when YB, JZ, and JQ interned at Zhipu.AI. %Email: \\texttt{bys22@mails.tsinghua.edu.cn}\n    }\n\\renewcommand{\\thefootnote}{\\arabic{footnote}}\n\n\\thispagestyle{plain}\n\\pagestyle{plain}\n\n\\begin{abstract}\n\nExtending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length. \nTo address this, we present LongAlign---a recipe of the instruction data, training, and evaluation for long context alignment. \nFirst, we construct a long instruction-following dataset using Self-Instruct. \nTo ensure the data diversity, it covers a broad range of tasks from various long context sources. \nSecond, we adopt the packing and sorted batching strategies to speed up supervised fine-tuning on data with varied length distributions. \nAdditionally, we develop a loss weighting method to balance the contribution to the loss across different sequences during packing training. \nThird, we introduce the LongBench-Chat  benchmark for evaluating instruction-following capabilities on queries of 10k-100k in length.\nExperiments show that LongAlign outperforms existing recipes for LLMs in long context tasks by up to 30\\%, while also maintaining their proficiency in handling short, generic tasks.\nThe code, data, and long-aligned models are open-sourced at \\url{https://github.com/THUDM/LongAlign}.\n\n\\end{abstract}\n\\section{Introduction}\n\\label{sec:intro}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/leaderboard.pdf}\n    \\caption{Test results on LongBench-Chat, which contains real-world queries of 10k-100k in length\\protect\\footnotemark.}\n    \\label{fig:leaderboard}\n\\end{figure}\n\nLarge language models (LLMs) with large context windows facilitate tasks such as summarization, question answering on long text and code~\\cite{bai2023longbench}. \nImportantly, they may form the foundational support for life-long conversations and complex agent scenarios~\\cite{xiao2023efficient,liu2023agentbench}. \nExisting works to build long-context LLMs predominantly focus on context extension~\\cite{chen2023extending,xiong2023effective,peng2023yarn}, that is, position encoding extension and continual training on long text. \n\nIn this work, we instead focus on the perspective of long context alignment, i.e., instruction fine-tuning LLMs to handle long user prompts. \nHowever, several challenges are required to address. \nFirst, there is an absence of long instruction-following datasets for supervised fine-tuning (SFT), and by extension the lack of methods for constructing such data. \nSecond, the varied length distribution of long-context data drastically reduces the training efficiency of traditional batching methods in a multi-GPU setup, as GPUs processing shorter inputs have to stay idle until those handling longer inputs complete their tasks. \nThird, there is a crucial need for a robust benchmark to evaluate LLMs' long-context capacities against real-world queries.\n\n\\footnotetext{LongAlign-6B-64k, LongAlign-7B-64k and LongAlign-13B-64k are trained based on ChatGLM3-6B, Llama-2-7B and Llama-2-13B, respectively.}\n\nTo address them, we present the \\textbf{LongAlign} recipe, covering data, efficient training, and evaluation, respectively. \n\\textit{Data-wise}, to construct a diverse long instruction-following dataset, we collect long sequences from nine sources and use Self-Instruct~\\cite{selfinstruct} to generate 10k instruction data of 8k-64k length. \n\n\\textit{Training-wise}, to address the inefficiency under uneven batching, we adopt the packing strategy~\\cite{krell2021efficient} that packs sequences together up to the maximum length before dispatching them to GPUs. \nHowever, we identified a bias in loss averaging during this packing training, as packs containing different numbers of sequences are assigned equal weight in the final loss calculation. \nTo mitigate this bias, we propose a loss weighting strategy to balance contributions to the loss across different sequences.\nIn addition, we introduce a sorted batching method that groups sequences of similar lengths to reduce the intra-batch idle time.\n\n\\textit{Evaluation-wise}, we develop LongBench-Chat, a benchmark compromising open-ended questions of 10k-100k length annotated by Ph.D. students. \nIt covers diverse aspects of instruction-following abilities such as reasoning, coding, summarization, and multilingual translation over long contexts. \nGPT-4~\\cite{GPT-4} is employed to score the machine-generated responses based on our annotated groundtruths and few-shot scoring examples. \n\nExtensive experiments show that LongAlign effectively aligns models to handle contexts of up to 64k tokens in length while maintaining their performance on general tasks without degradation. \nIn addition, we have the following findings: \n\\begin{itemize}[itemsep=0pt, leftmargin=*]\n    \\item \\textbf{Impact of Data Quantity and Diversity}: \n    Both the quantity and the diversity of the long instruction data significantly influence the aligned model's ability to handle long contexts, impacting final performance by up to 30\\%. \n    \n    \\item \\textbf{Benefits of Long Instruction Data}: \n    The amount of long instruction data positively affects the performance on long-context tasks while does not hurt the models' short-context handling capacities. \n    \n    \\item \\textbf{Effectiveness of Training Strategies}: \n    The packing and sorted batching strategies adopted can accelerate training by over 100\\% without  performance compromise. \n    Furthermore, the proposed loss weighting technique improves long context performance by 10\\%.\n\\end{itemize}\n\n\\section{Related Work}\n\\label{sec:related}\n\n\\xhdr{Long Context Scaling}\nLong context scaling aims to expand the limited context length of existing LLMs to support long context tasks~\\cite{xiong2023effective}. The current methods for long context scaling can be divided into two categories: those that require fine-tuning or continual training on longer sequences and those that do not.\nMethods that do not require fine-tuning often employ techniques such as sliding window attention~\\cite{han2023lm,xiao2023efficient} or neighboring token compression~\\cite{jiang2023longllmlingua,zhang2024soaring,jin2024llm} to handle the positional O.O.D. problem in attention computation for long contexts.\nThese methods, although capable of extending the context length of LLMs in a plug-and-play manner, still cannot match the performance of the fine-tuned approaches.\nProminent fine-tuned approaches for long context scaling~\\cite{chen2023extending,peng2023yarn,xiong2023effective,chen2023longlora,zhu2023pose,fu2023longdata} typically involve position encoding extension and continual pretraining on longer sequences.\n\n\\xhdr{LLM Alignment}\nFollowing the previous steps of long context scaling, it is vital to also align the model with instruction-following data to ensure that it can interact with various user requests in a chat interface~\\cite{wang2023how}.\nThis phase, often referred to as supervised fine-tuning or instruction-tuning, has been extensively studied in short context scenarios~\\cite{selfinstruct,alpaca,wang2023how,tunstall2023zephyr}.\nHowever, the introduction of long sequences presents unique challenges in terms of data, training methods, and evaluation for alignment.\n\\citet{xiong2023effective} proposes generating long instruction data by concatenating short instruction data, yet their dataset and model weight are not open-sourced.\nOn the other hand, while \\citet{chen2023longlora} has made their long instruction data, LongAlpaca-12k, available and employed LoRA~\\cite{hu2022lora} for efficient fine-tuning, it lacks in-depth discussion and comparative analysis of the influence of data and training methodologies.\nOur work aims to find an optimal solution for supervised (full parameter) fine-tuning on long context with full attention, by tuning data, training methods, and evaluating the aligned models on a wide range of tasks.\n\\section{LongAlign}\n\\label{sec:method}\nIn this section, we discuss the methodology in LongAlign, involving the data construction process, training method, and evaluation benchmark.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/instruction.pdf}\n    \\caption{Data construction example.}\n    \\label{fig:inst}\n\\end{figure}\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/method.pdf}\n    \\caption{Under a long-tailed data length distribution, packing or sorted batching can reduce idle time and speed up the training process. Loss weighting is required during packing to balance the loss contribution across sequences.}\n    \\label{fig:method}\n\\end{figure*}\n\n\\subsection{Preliminary}\nLarge language models can learn alignment by supervised fine-tuning on high-quality pairs of instruction $x$ and response $y$~\\cite{ouyang2022training,chung2022scaling}. During training, the instruction and response are typically concatenated to form a sequence $[x,y]$, which is then processed through an auto-regressive language model $\\pi$ to maximize the probability $P_\\pi(y|x)$.\nThe loss is similar to a language modeling loss, while only accounting for the loss associated with the tokens in $y$ (target tokens):\n\\begin{equation}\n    \\mathcal{L}([x, y]) = -\\sum_{i=1}^{|y|}\\log P_\\pi(y_i\\,|\\,[x, y_{<i}]).\n\\end{equation}\n\n\\subsection{Dataset Construction}\n\\label{sec:data}\nLong instruction data typically involves a long context material, such as a book, an extensive document, or a lengthy code, accompanied by a task query that requires summarizing, reasoning, or computing based on the material.\nDuring construction, we first collect long articles and documents from 9 varied sources, covering books, encyclopedias, academic papers, codes, etc.\nWe then employ Claude 2.1~\\cite{claude} to generate tasks and answers according to a given long context, as illustrated in Figure~\\ref{fig:inst}.\nTo foster a diverse range of generated tasks, we incorporate task type descriptions into the prompts, such as queries for summaries, information extraction, reasoning, etc. \nUsing this methodology, we create tasks and answers for 10k lengthy texts, yielding a total of 10k instances of supervised data, of which 10\\% is in Chinese.\nThe length of these data ranges from 8k to 64k, measured by ChatGLM tokenizer~\\cite{zeng2022glm} due to its higher compression rate for Chinese characters.\nDetails regarding the prompts and the data construction process can be found in Appendix~\\ref{app:dataset}.\n\n\\subsection{Efficient Long-Context Training}\nTo ensure that the model retains the ability to handle both long and short texts (general capability) after SFT, we mix the long instruction data with a general instruction dataset for training.\nThe mixture of a large amount of general short data with a relatively smaller amount of long instruction data results in a long-tail data length distribution. As shown in Figure~\\ref{fig:method} left, the majority of the data falls within the 0-8k length range, while the remaining data is fairly evenly distributed in the 8k-64k length interval.\nUnder this distribution, during training, a data batch typically contains mostly short data, yet these batches also include a few longer texts which necessitate much more computation times, resulting in considerable idle times.\nTo minimize these idle times, the most effective approach is to concatenate or sort the data in a manner that ensures a more uniform length and computational time within each batch.\nBearing this in mind, we explore two training methods, namely packing and sorted batching.\n\n\\xhdr{Packing} It involves concatenating data of varying lengths together until reaching the maximum length.\nThe resulting packed data, whose lengths are generally close to the maximum length, are then batched and processed on multi-GPUs.\nThis approach effectively minimizes the idle time within each batch, as depicted in the upper right of Figure~\\ref{fig:method}.\nAdditionally, to prevent cross-contamination between different sequences within the same pack during self-attention calculation, we pass a list containing the starting and ending positions of different sequences and utilize the \\texttt{flash\\_attn\\_varlen\\_func} from FlashAttention 2~\\cite{dao2022flashattention,dao2023flashattention2}, which supports efficient computation of block diagonal attention (see Appendix~\\ref{app:training} for more details).\nIt requires less computation and IO time compared to the traditional use of a 2D attention mask.\n\nHowever, we notice that the packing strategy leads to a bias towards longer sequences and sequences containing more target tokens.\nThis is because different packs, each contributing equally to the final loss, contain varying numbers of sequences with different numbers of target tokens. Consequently, when calculating the mean loss for each batch, sequences in packs with fewer sequences (typically the longer ones) or those containing more target tokens, have a greater influence on the final loss.\nFormally, consider $M$ sequences packed into a batch of $K$ packs where the $i$-th pack consists of the sequences with indices in $[P_{i-1}, P_i)$, thus it holds that $P_0=1, P_K=M+1$. Let $L_i$ denote the total summation of loss over $N_i$ target tokens in the $i$-th sequence. If we weigh each sequence equally, the loss should be\n\\begin{equation}\n    \\mathcal{L} = \\frac{1}{M}\\sum_{i=1}^{M}\\frac{L_i}{N_i},\n\\label{eq:loss}\n\\end{equation}\nwhile the loss calculated under packing is \n\\begin{equation}\n    \\mathcal{L}' = \\frac{1}{K}\\sum_{k=1}^{K}(\\sum_{i=P_{k-1}}^{P_{k}-1}L_i / \\sum_{i=P_{k-1}}^{P_{k}-1}N_i) \\neq \\mathcal{L}.\n\\label{eq:loss'}\n\\end{equation}\nCompared with Eq.~\\ref{eq:loss}, this equates to assigning a weight of $(N_j/\\sum_{i=P_{k-1}}^{P_{k}-1}N_i)$ to sequence $j$ in the loss, i.e., in favor of sequences with more target tokens and sequences in smaller packs.\nTo address this inequality, we propose to scale the loss in the $i$-th sequence by $K/(N_iM)$ and instead take the sum of the scaled loss on each pack, which results in an equal loss to Eq.~\\ref{eq:loss}:\n\\begin{equation}\n    \\mathcal{L}' = \\frac{1}{K}\\sum_{k=1}^{K}(\\sum_{i=P_{k-1}}^{P_{k}-1}\\frac{L_iK}{N_iM})\n    = \\frac{1}{K}\\sum_{i=1}^{M}\\frac{L_iK}{N_iM}=\\mathcal{L}.\n\\end{equation}\nAs demonstrated in our experimental section, the loss weighting strategy results in a 10\\% improvement in downstream tasks.\n\n\\xhdr{Sorted batching}\nWe also consider an efficient sorted batching strategy for training (lower right of Figure~\\ref{fig:method}). To ensure that the sequences within each batch are of similar lengths, we sort the data by length and select a random consecutive group of data for each batch, with no repetition.\nHowever, this strategy inevitably introduces a bias in the data distribution across different batches, where batches consist either of all long sequences or all short sequences. \nThis can be potentially disastrous for SGD optimization.\nIn our experiments, we observe that sorted batching significantly accelerates the process without a noticeable negative impact on performance. This might be attributed to our use of large gradient accumulation steps and the strong adaptability of the optimizer.\n\n\\subsection{LongBench-Chat}\nAlthough there are existing benchmarks for evaluating LLMs' long context understanding~\\cite{an2023leval,bai2023longbench,li2023loogle}, they do not focus on assessing their instruction-following capability under long context. Furthermore, their reliance on automatic metrics for evaluation limits the assessment of aligned models' longer and more diverse outputs to real-world queries, and how their responses align with human preference.\n\nTo this end, we propose LongBench-Chat, which includes 50 long context real-world queries ranging from 10k to 100k in length, covering various key user-intensive scenarios such as document QA, summarization, and coding. It consists of 40 tasks in English and 10 in Chinese. \nTo ensure the evaluation truly reflects the model's ability to follow long context instructions, we avoid using popular long texts that are likely to have been seen and memorized by the model during pretraining. \nWe also avoid posing questions that the model could answer without reading the long text. \n\nFor evaluation, following previous works that have shown the effectiveness of using LLM as an evaluator~\\cite{bai2023benchmarking,zheng2023judging,ke2023critiquellm}, we employ GPT-4~\\cite{GPT-4} to score the model's response in 1-10 based on a given human-annotated referenced answer and few-shot scoring examples for each question.\nWe only pass the short query (without the long document) to the evaluator, as currently there is no model capable of evaluating the quality of responses under long context inputs.\nTo ensure that the evaluator can make informed judgments based solely on the groundtruth and few-shot scoring examples, we steer clear of overly open-ended questions, such as ``Write a poem based on the preceding text''.\n\n\\begin{table}[t]\n\\centering\n\\resizebox{0.49\\textwidth}{!}{\n\\begin{tabular}{lccc}\n\\toprule\n& Human & GPT-4 & GPT-4+\\emph{Few-shot} \\\\\n\\midrule\nSpearman ($\\rho$) & 0.817 & 0.788 & \\textbf{0.844} \\\\\nKendall ($\\tau$) & 0.694 & 0.656 & \\textbf{0.716} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Inter-annotator correlations; correlations between GPT-4 (w/ and w/o Few-shot) and human.}\n\\label{tb:corr}\n\\end{table}\n\nTo validate the reliability of using GPT-4 as an evaluator on LongBench-Chat, we conduct a human evaluation study (more details in Appendix~\\ref{app:human}). In Table~\\ref{tb:corr}, we present the correlation between GPT-4's assessments using zero-shot prompting, which involves only the referenced answer, and its evaluations with additional few-shot scoring examples, compared to crowdsourced human judgments. We also show the inter-annotator correlation in the first column.\nWe find that with few-shot prompting, GPT-4's correlation with human annotations not only aligns but also surpasses the level of agreement among human annotators, proving the reliability of such a metric on LongBench-Chat.\nWe further discover that the overall average scores (1-10) obtained using GPT-4+\\emph{Few-shot} differ by an average of 0.1 or less from the scores given by human experts. \nAdditionally, we do not observe a significant bias in GPT-4's scoring towards the length of responses --- in fact, it even penalizes excessively lengthy responses.\n\n\\xhdr{Leaderboard}\nFigure~\\ref{fig:leaderboard} reports the test results of current long context (16k+) instruction fine-tuned models (chat models) and our most competent models trained with LongAlign on LongBench-Chat.\nWe include API-based Commercial models: GPT-4-1106-preview~\\cite{gpt-4-1106-preview} (GPT-4 Turbo), GLM-4-128k\\footnote{\\hyperlink{https://open.bigmodel.cn/pricing}{https://open.bigmodel.cn/pricing}}, and Claude-2.1~\\cite{claude}; as well as open-sourced models: InternLM2-7b-200k, InternLM2-20b-200k~\\cite{2023internlm}, ChatGLM3-6B-32k~\\cite{du2022glm,zeng2022glm}, Vicuna-7b-v1.5-16k~\\citep{zheng2023judging}, Orion-14b-LongChat~\\cite{orion2024}, LongChat-7b-v1.5-32k~\\cite{longchat2023}, and Mixtral-8x7b-Instruct-v0.2~\\cite{jiang2024mixtral}.\nNote that we employ middle truncation for inputs surpassing the model's context window.\nOur evaluation result reveals that the performance of current open-sourced models still significantly lags behind commercial models, which partially attributed to the scale difference between these models. Additionally, we observe that models with a context length of 32k or less tend to underperform on LongBench-Chat, indicating that a longer context window is necessary to complete these long tasks.\n\\section{Experiments}\n\\label{sec:experiments}\nIn this section, we aim to answer the following research questions through a series of experiments:\n\n\\xhdr{RQ1}\nDuring SFT, how does the quantity and diversity of the long instruction data influence the model's performance in downstream tasks.\n\n\\xhdr{RQ2}\nWhether incorporating long instruction data during training affects the model's general capabilities and their instruction-following / conversational abilities in short context scenarios.\n\n\\xhdr{RQ3}\nThe impact that the packing and sorted batching training methods have on the training efficiency and the final performance of the models.\n\n\\noindent\nWe also incorporate discussions on the scalability of LongAlign on model size and context length, and the learning curve in long context alignment.\n\n\\begin{table*}[t]\n\\centering  \n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|cccc|ccccc}\n\\toprule\n\\multicolumn{1}{c|}{\\textbf{Training Data}} & \\multicolumn{4}{c|}{\\textbf{Long Tasks}} & \\multicolumn{5}{c}{\\textbf{Short Tasks}} \\\\\n\\cmidrule(lr){2-5} \\cmidrule(lr){6-10} \n\\multicolumn{1}{c|}{\\textbf{(Long)}} & \\textbf{LongBench-Chat} & \\textbf{S-Doc QA} & \\textbf{M-Doc QA} & \\textbf{Summ} & \\textbf{MT-Bench} & \\textbf{ARC} & \\textbf{HellaSwag} & \\textbf{TruthfulQA} & \\textbf{MMLU} \\\\\n\\midrule\n\\emph{LongAlign-0k} & 3.73 & 58.7 & 41.1 & 38.4 & 5.34 & 50.3 & 74.7 & 51.6 & 45.5 \\\\\n\\emph{LongAlign-5k} & \\cellcolor[HTML]{9CC5DE}5.97 & \\cellcolor[HTML]{E5EFF6}61.8 & \\cellcolor[HTML]{F3F7FB}42.1 & \\cellcolor[HTML]{D0E3EF}42.0 & \\cellcolor[HTML]{EFF5F9}5.51 & \\cellcolor[HTML]{FFFFFF}50.3 & \\cellcolor[HTML]{FCFDFE}75.1 & \\cellcolor[HTML]{F6F9FC}52.5 & \\cellcolor[HTML]{F3F8FB}46.6\\\\\n\\emph{LongAlign-10k} & \\cellcolor[HTML]{92BFDB}6.21 & \\cellcolor[HTML]{D2E4F0}64.0 & \\cellcolor[HTML]{D7E7F1}44.4 & \\cellcolor[HTML]{B4D3E6}44.2 & \\cellcolor[HTML]{F0F6FA}5.5 & \\cellcolor[HTML]{FDFDFE}50.5 & \\cellcolor[HTML]{FDFEFE}74.9 & \\cellcolor[HTML]{F6F9FC}52.5 & \\cellcolor[HTML]{FFFFFF}45.5\\\\\n\\emph{LongAlpaca-12k} & \\cellcolor[HTML]{DEECF4}4.46 & \\cellcolor[HTML]{C3DCEB}65.8 & \\cellcolor[HTML]{C9DFED}45.6 & \\cellcolor[HTML]{B5D4E6}44.1 & \\cellcolor[HTML]{FDD7C5}4.93 & \\cellcolor[HTML]{F3F8FB}51.5 & \\cellcolor[HTML]{FAFCFD}75.4 & \\cellcolor[HTML]{EFF6F9}53.2 & \\cellcolor[HTML]{EDF4F9}47.1\\\\\n\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Performance of ChatGLM3-6B-64k after training on different quantities and types of long instruction data.}\n\\label{tb:exp1}\n\\end{table*}\n\n\\subsection{Experimental Setup}\n\\xhdr{Data}\nTo maintain the model's general capabilities and its proficiency in following short instructions, we utilize \\href{https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main/HTML_cleaned_raw_dataset}{ShareGPT}~\\cite{vicuna2023} (empty assistant responses are filtered out) as the source of short instruction data in our training data. \nTo compare the impact of different aspects of long instruction data on model training, we incorporate the following four suites of long instruction data in our experiment.\n`\\emph{LongAlign-0k}', `\\emph{LongAlign-5k}', and `\\emph{LongAlign-10k}': 0, 5k, and 10k instances of LongAlign data, constructed according to the procedure in Sec~\\ref{sec:data}; `\\emph{LongAlpaca-12k}': 12k data from the LongAlpaca dataset~\\cite{chen2023longlora}. LongAlpaca includes 9k long QA data and 3k short QA data, where the long QA data is generated based only on academic papers and books, offering less diversity compared to our LongAlign data. We use this dataset to compare the impact of the diversity of long instruction data on model training.\n\n\\xhdr{Model}\nWe include three model variants, namely ChatGLM3-6B~\\cite{du2022glm,zeng2022glm}, Llama-2-7B, and Llama-2-13B~\\cite{touvron2023llama} (all base models).\nGiven their 8k and 4k context windows, we first perform context extension to extend their context window to 64k, resulting in ChatGLM3-6B-64k, Llama-2-7B-64k, and Llama-2-13B-64k. This involves expanding the base frequency $b$ of the RoPE position encoding~\\cite{su2024roformer} by 200 times (from 10,000 to 2,000,000) and continual training on pretraining data with lengths under 64k, for a total of 10 billion tokens\\footnote{Continual training on 10B tokens is sufficient for context extension, as suggested in~\\citet{fu2023longdata}.}.\n\n\\xhdr{Training}\nAll models are trained with 8xA800 80G GPUs and DeepSpeed+ZeRO3+CPU offloading~\\cite{rasley2020deepspeed}.\nThe models can be trained with a maximum length of 64k tokens without GPU memory overflow. Consequently, we set the maximum length of the training data to 64k, with any data exceeding this length being truncated from the right.\nFor packing training, each pack consists of 12 sequences on average, we set the total batch size to 8, resulting in a global batch size of 96.\nFor a fair comparison, we set the batch size to 8, with a gradient accumulation step of 12 for other non-packing training methods.\nWe train 2 epochs on the training data (approximately 1500-2000 steps).\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/needle.pdf}\n    \\caption{1k-60k Needle test performance of Chat-GLM3-6B-64k trained on different suites of long data mixed with ShareGPT.}\n    \\label{fig:needle}\n\\end{figure}\n\n\\xhdr{Evaluation}\nWe involve both long context tasks and short context tasks in evaluation.\nIn both long and short scenarios, we consider tasks that evaluate the instruction-following and conversational abilities, as well as tasks that assess general capabilities. \nFor long context tasks, we use our proposed LongBench-Chat to evaluate the models' long context alignment proficiency and employ LongBench~\\cite{bai2023longbench} to test the model's general long context understanding abilities. \nLongBench is a bilingual, multi-task long context benchmark. We conduct evaluations on three types of tasks within it: Single-Doc QA, Multi-Doc QA, and Summarization. \nSince the aligned models typically produce longer responses, instead of using the original metrics (ROUGE, F1) to score the models' replies, we use GPT-4 to rate the model's outputs based on their alignment with the groundtruth answers on LongBench.\nFor short context tasks, we use MT-Bench~\\cite{zheng2023judging}, a multi-turn chat benchmark, to measure the models' ability to follow short instructions. We also evaluate on the general tasks on Open LLM Leaderboard~\\cite{openllm}, including ARC~\\cite{clark2018think}, HellaSwag~\\cite{zellers2019hellaswag}, Truthful QA~\\cite{lin2022truthfulqa}, and MMLU~\\cite{hendrycks2021measuring}.\nWe follow the evaluation settings in the Open LLM Leaderboard and utilize lm-evaluation-harness framework~\\cite{eval-harness} for evaluation on these tasks.\nTo ensure the most stable evaluation results, we use GPT-4 to score twice on LongBench-Chat and MT-Bench, and average these scores to obtain the final score.\n\n\\begin{table*}[t]\n\\centering  \n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|cccc|ccccc}\n\\toprule\n\\multirow{2}{*}{\\textbf{Training Method}} & \\multicolumn{4}{c|}{\\textbf{Long Tasks}} & \\multicolumn{5}{c}{\\textbf{Short Tasks}} \\\\\n\\cmidrule(lr){2-5} \\cmidrule(lr){6-10} \n& \\textbf{LongBench-Chat} & \\textbf{S-Doc QA} & \\textbf{M-Doc QA} & \\textbf{Summ} & \\textbf{MT-Bench} & \\textbf{ARC} & \\textbf{HellaSwag} & \\textbf{TruthfulQA} & \\textbf{MMLU} \\\\\n\\midrule\n\\emph{ChatGLM3-6B-64k} \\\\\nNaïve batching & 5.87 & 65.4 & 45.0 & 44.8 & 5.61 & 50.7 & 74.7 & 52.8 & 46.0 \\\\\nSorted batching & \\cellcolor[HTML]{FEE3D7}5.4 & \\cellcolor[HTML]{FAFCFD}66.2 & \\cellcolor[HTML]{F5F9FB}46.3 & \\cellcolor[HTML]{FEF6F2}43.7 & \\cellcolor[HTML]{F6F9FC}5.76 & \\cellcolor[HTML]{FBFCFD}51.3 & \\cellcolor[HTML]{FEFEFE}74.8 & \\cellcolor[HTML]{FEF9F6}51.9 & \\cellcolor[HTML]{FCFDFE}46.3\\\\\nPacking & \\cellcolor[HTML]{FEF8F5}5.76 & \\cellcolor[HTML]{FEFCFB}65.0 & \\cellcolor[HTML]{FEFEFE}45.1 & \\cellcolor[HTML]{FEEFE8}42.8 & \\cellcolor[HTML]{FDFDFE}5.64 & \\cellcolor[HTML]{FDFEFE}50.9 & \\cellcolor[HTML]{FEFEFE}74.8 & \\cellcolor[HTML]{FEF0E9}50.5 & \\cellcolor[HTML]{F6F9FC}47.2\\\\\nPacking+loss weighting & \\cellcolor[HTML]{ECF3F8}6.21 & \\cellcolor[HTML]{FEF7F4}64.0 & \\cellcolor[HTML]{FEFAF8}44.4 & \\cellcolor[HTML]{FEFAF8}44.2 & \\cellcolor[HTML]{FEF8F5}5.5 & \\cellcolor[HTML]{FEFDFD}50.5 & \\cellcolor[HTML]{FEFEFE}74.9 & \\cellcolor[HTML]{FEFDFC}52.5 & \\cellcolor[HTML]{FEFBF9}45.5\\\\\n\\midrule\n\\emph{Llama-2-7B-64k} \\\\\nNaïve batching & 5.95 & 62.8 & 42.7 & 41.6 & 5.52 & 48.9 & 74.8 & 45.3 & 43.6 \\\\\nSorted batching & \\cellcolor[HTML]{E7F1F7}6.38 & \\cellcolor[HTML]{FBFDFD}63.4 & \\cellcolor[HTML]{FEFAF9}42.2 & \\cellcolor[HTML]{FEFCFB}41.3 & \\cellcolor[HTML]{FEFEFE}5.51 & \\cellcolor[HTML]{FAFCFD}49.5 & \\cellcolor[HTML]{FFFFFF}74.8 & \\cellcolor[HTML]{EBF3F8}48.0 & \\cellcolor[HTML]{F9FBFD}44.3\\\\\nPacking & \\cellcolor[HTML]{FEFBF9}5.89 & \\cellcolor[HTML]{FEF8F6}61.7 & \\cellcolor[HTML]{FEECE4}40.4 & \\cellcolor[HTML]{FBFDFD}42.0 & \\cellcolor[HTML]{FBFCFD}5.58 & \\cellcolor[HTML]{FEF9F6}48.1 & \\cellcolor[HTML]{FEFEFE}74.9 & \\cellcolor[HTML]{F9FBFD}46.1 & \\cellcolor[HTML]{FCFDFE}43.9\\\\\nPacking+loss weighting & \\cellcolor[HTML]{F6FAFC}6.10 & \\cellcolor[HTML]{FEF4EF}60.8 & \\cellcolor[HTML]{FEF3EE}41.3 & \\cellcolor[HTML]{F3F8FB}43.1 & \\cellcolor[HTML]{FAFCFD}5.60 & \\cellcolor[HTML]{FEFBF9}48.4 & \\cellcolor[HTML]{FEFDFC}74.5 & \\cellcolor[HTML]{EFF6F9}47.4 & \\cellcolor[HTML]{FEFCFB}43.3\\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Performance of ChatGLM3-6B-64k and Llama-2-7B-64k under different training methods.}\n\\label{tb:exp2}\n\\end{table*}\n\n\\subsection{Influence of Data}\nWe conduct SFT on ChatGLM3-6B-64k using ShareGPT data mixed with different suites of long instruction data. \nAll models except \\emph{LongAlign-0k} are trained using the more efficient packing strategy with loss weighting.\nThe evaluation results are reported in Table~\\ref{tb:exp1}.\nFor LongBench-Chat and MT-Bench, the reported results are averaged over GPT-4's rating (1-10) across all test instances, while results on other datasets are normalized between 0-100. \nWe also conduct the ``Needle in A HayStack'' experiment\\footnote{\\hyperlink{https://github.com/gkamradt/LLMTest_NeedleInAHaystack}{https://github.com/gkamradt/LLMTest\\_NeedleInAHaystack}} (result visualization in Figure~\\ref{fig:needle}) to test the model's ability to utilize information from 10 different positions within long contexts of varying lengths between 1k-60k.\nSpecifically, this task asks for the model to retrieve a piece of fact (the `needle') that is inserted in the middle (positioned at a specified depth percent) of a long context window (the `haystack').\nWe summarize our key findings on the influence of data as follows.\n\n\\xhdr{1. More long instruction data enhances the performance in long tasks, and without compromising the performance in short tasks}\nComparing the performance of \\emph{LongAlign-0k}, \\emph{LongAlign-5k}, and \\emph{LongAlign-10k}, we observe that as the amount of long instruction data increases, there is a consistent improvement in the model's performance across all long tasks. Meanwhile, intriguingly, its performance on short tasks remains comparable to when it is trained solely on short instructions.\nAdditionally, given the inferior performance of \\emph{LongAlign-0k} in long tasks (especially on LongBench-Chat), this also indicates that merely performing context extension on the base model is insufficient to ensure good performance on downstream long tasks. It is necessary to incorporate a substantial amount of long data covering various lengths during SFT.\nMoreover, the needle test result also suggests that more long data enhances the model's ability to utilize information from different positions within long texts, resulting in a decrease of the model's retrieval error.\n\n\\xhdr{2. Diversity of long instruction data is beneficial for the model's instruction-following abilities}\n\\emph{LongAlign-10k} shows significantly better results in long and short instruction-following tasks (LongBench-Chat and MTBench), compared to \\emph{LongAlpaca-12k}. Meanwhile, \\emph{LongAlpaca-12k} slightly outperforms \\emph{LongAlign-10k} on LongBench. This is primarily due to its superior performance on the 2WikiMQA~\\cite{ho2020constructing} and NarrativeQA~\\cite{kovcisky2018narrativeqa} datasets, which are based on Wikipedia and novels, bearing more resemble to the source of the instruction data in LongAlpaca-12k.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/speed.pdf}\n    \\caption{Training time (hrs) on 8xA800 80G GPUs under different training methods.}\n    \\label{fig:efficiency}\n\\end{figure}\n\n\\subsection{Impact of Training Methods}\nWe compare different training methods on ChatGLM3-6B-64k and Llama-2-6B-64k, including naïve batching, packing (w/ and w/o loss weighting), and sorted batching, to assess their impact on training efficiency, as well as their influence on downstream task performance.\\footnote{Naïve batching and sorted batching consume more GPU memory compared to packing, due to their use of gradient accumulation. We truncate all data to 56k length for ChatGLM with these two methods to ensure no GPU memory overflow.}\nAll models are trained on \\emph{LongAlign-10k}.\nFigure~\\ref{fig:efficiency} displays a comparison of the training time required for each method. Table~\\ref{tb:exp2} presents the performance on downstream tasks.\nOur findings are as follows.\n\n\\begin{table*}[t]\n\\centering  \n\\resizebox{0.85\\textwidth}{!}{\n\\begin{tabular}{l|ccccc}\n\\toprule\n\\emph{Llama-2-13B-64k} & \\textbf{LongBench-Chat} & \\textbf{S-Doc QA} & \\textbf{M-Doc QA} & \\textbf{Summ} & \\textbf{MT-Bench} \\\\\n\\midrule\nPacking+loss weighting & 6.79 & 68.0 & 40.3 & 43.6 & 6.12 \\\\\nSorted batching & 7.02 & 66.1 & 43.9 & 45.3 & 6.02 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Scaling up: LongAlign on LLama-2-13B.}\n\\label{tb:exp3}\n\\end{table*}\n\n\\xhdr{1. Packing and sorted batching double the training efficiency while exhibiting good performance}\nFrom Figure~\\ref{fig:efficiency}, we can see that the training efficiency of packing and sorted batching is comparable, both requiring less than half the time needed under naïve batching. Additionally, according to table~\\ref{tb:exp2}, models trained with the two efficient methods perform comparably to those trained with naïve batching on both long and short tasks. \nWe also find that the effectiveness of these two training methods varies with different models. \nFor instance, the model trained on ChatGLM3-6B using packing+loss weighting shows significantly better performance on LongBench-Chat, whereas sorted batching performs the best for Llama-2-7B.\n\n\\xhdr{2. Loss weighting significantly improves performance on long instruction task for packing training}\nBy comparing the performance of models with and without loss weighting strategy during packing training, it's evident that incorporating the loss weighting strategy greatly improves the capability in LongBench-Chat (by about 5\\%$\\sim$10\\%), while having a minimal and variable impact on the performance of other tasks.\nWe believe that this is primarily because, without loss weighting in SFT data, different long instruction data contribute variably to the loss --- longer data tend to contribute more to the loss (refer to Eq.~\\ref{eq:loss'}). \nSuch an unnatural weighting bias is often detrimental to model training, potentially leading to training instability, deviating it from the optimal learning trajectory.\n\n\\subsection{Discussion}\n\\xhdr{Scalability of LongAlign}\nWe explore two scaling directions on our LongAlign framework: \\textbf{larger model size} and \\textbf{longer context window}.\nTo do so, we fine-tune Llama-2-13B-64k using \\emph{LongAlign-10k} dataset with the two efficient training methods, and the evaluation results are shown in Table~\\ref{tb:exp3}.\nCompared to the 7B-scale model, the 13B model shows a 10\\% improvement on LongBench-Chat, setting a new record among open-sourced models (LongAlign-13B-64k in Figure~\\ref{fig:leaderboard}).\nThis indicates that our alignment method scales effectively to larger-scale models.\nWe also construct SFT data up to 128k in length with human annotation and successfully align ChatGLM3-6B under 128k context window using packing training with loss weighting, resulting in ChatGLM3-6B-128k (performance shown in Figure~\\ref{fig:leaderboard}).\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{figs/curve.pdf}\n    \\caption{Relative performance on long and short tasks throughout the training process of ChatGLM3-6B-64k.}\n    \\label{fig:curve}\n\\end{figure}\n\n\\xhdr{Learning curve on long task v.s. short task}\nTo compare the learning processes of alignment under long context and short context, we present in Figure~\\ref{fig:curve} the relative performance curves on long and short instruction-following tasks (on LongBench-Chat and MT-Bench, respectively) during model training, illustrating how performance varies with the number of training steps.\nWe use exponential moving average to smooth the original performance curves (dotted lines), and display them as solid lines.\nWe observe that the trends of the two learning curves are strikingly similar --- both show rapid improvement between 0-500 steps, followed by a slow rise, and stabilize after 1000 steps. \nThis may imply a deeper connection between long and short alignment. \nThey might be jointly determined by shared latent factors, which are optimized during training to help the model align to both long and short instructions simultaneously.\n\nIn Appendix~\\ref{app:case}, we provide case analyses of different LongAlign-tuned models on out-of-distribution (OOD) long context query, that is, query that the models have not encountered in the long context SFT data.\nWe find that models trained with LongAlign can generalize to OOD long context queries, such as writing a review for a research paper, and that larger-scale models have stronger generalization capabilities.\n\\section{Conclusion}\n\\label{sec:conclusion}\n\nThis paper aims to find the best practice for long context alignment in the scope of data, training method, and evaluation.\nOur proposed solution, namely LongAlign, uses Self-Instruct to construct diverse long instruction data, and efficiently fine-tune the model with packing combined with loss weighting or sorted batching.\nMoreover, we introduce LongBench-Chat to facilitate reliable assessment of LLM's instruction-following ability on practical long context interactions.\nThrough controlled experiments, we find that the amount, diversity of data, as well as the correct training method, are crucial to the final performance.\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback}\n\n\\begin{document}\n\n\\maketitle\n\\renewcommand{\\thefootnote}{\\fnsymbol{footnote}}\n    \\footnotetext[1]{ZH and YN contributed equally.}\n\\renewcommand{\\thefootnote}{\\arabic{footnote}}\n\n\\begin{abstract}\n\nChatGLM is a free-to-use AI service powered by the ChatGLM family of large language models (LLMs). \nIn this paper, we present the ChatGLM-RLHF pipeline---a reinforcement learning from human feedback (RLHF) system---designed to enhance ChatGLM's alignment with human preferences. \nChatGLM-RLHF encompasses three major components: the collection of human preference data, the training of the reward model, and the optimization of policies. \nThroughout the process of integrating ChatGLM-RLHF into production, we encountered and addressed several unprecedented challenges. \nWe introduce the strategies to mitigate reward variance for stabilized large-scale training, implement model parallelism with fused gradient-descent, and design regularization constraints to avoid catastrophic forgetting in LLMs. \nExperiments show that  ChatGLM-RLHF brings significant improvements in alignment tasks compared to the supervised fine-tuned (SFT) version of ChatGLM. \nFor instance, it achieves on average 15\\% more wins against ChatGLM-SFT in Chinese alignment tasks. \nThe work presents our practices of aligning LLMs with human preferences, offering insights into the challenges and solutions in RLHF implementations. \n\n\\end{abstract}\n\n\\hide{\n\nThis paper introduces the systematic design and practices of Reinforcement Learning from Human Feedback (RLHF) pipeline in the ChatGLM system, focusing on improving large language models by aligning them with human preferences. We present a comprehensive framework that includes human preference data collection, rigorous training of reward and policy models, and various strategies to mitigate various types of biases in the data and models so as to enhance model stability. Experiment evaluations demonstrate significant improvements in alignment tasks through both automatic and human assessments. The research underscores the importance of human feedback in refining language models, offering insights into the challenges and solutions in RLHF implementations. This RLHF pipeline is a critical component in ChatGLM online system and serves huge number of users everyday.\n\n}\n\\hide{\n\\section{Introduction}\n\nLarge language models (LLMs) ~\\cite{brown2020language,zhang2022opt,workshop2022bloom,zeng2023glm,touvron2023llama} have remarkably advanced the capabilities of machines on language understanding and generation. Recent works~\\cite{ouyang2022training,wei2021finetuned,bai2022training} have made progress on aligning language models to act in accordance with the users' intention, i.e., responding to questions and executing instructions from user, and enable LLMs to adapt to a wide range of natural language processing tasks~\\cite{park2023generative,Frieder2023MathematicalCO}.\n\nFinetuning with human feedback has been proved to be an effective approach to encourage LLMs to produce more helpful, honest, and harmless responses and align with human preferences and values~\\cite{bai2022constitutional,achiam2023gpt,ouyang2022training}.  \nThese techniques generally treat human preference as a reward signal and employ reinforcement learning algorithms like Proximal Policy Optimization~\\cite{schulman2017proximal} to optimize language models (RLHF). \nBuilding a practical RLHF training framework faces many challenges. First, collecting and modeling reliable human preference is critical to the success of exploiting human feedback, yet biased~\\cite{cui2023ultrafeedback,longpre2023flan} and deceptive preference~\\cite{bai2022training} in human annotations are inevitable.\nSecond, a scalable and comprehensive training framework is necessary to ensure efficient and effective optimization of large language models across different scales, especially covering all kinds of corner cases for requirements of online deployment. Most existing reported efforts are based on parameter-efficient tuning~\\cite{sun2023salmon,yao2023deepspeed} or small-scale models~\\cite{zheng2023secrets,wu2023pairwise}. As LLMs often exhibit significantly different behaviors across model scales, there is very limited experience publicly available for references.\n\nIn this paper, we introduce our practices in aligning ChatGLM \\hnote{which paper should we cite here?} with human preferences with Reinforcement Learning from Human Feedback (RLHF). This pipeline serves as a critical component in the ChatGLM system to produce better responses that meet human preferences. We systematically introduce our approach to designing the pipeline, addressing the challenges associated with constructing preference data and optimizing the reward model and policies. %Through this process, we establish the RLHF pipeline as an indispensable element of the online ChatGLM system.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.48\\textwidth]{figures/rlhf-demo.pdf}\n    \\caption{Screenshot of ChatGLM webpage (left) and an example of use case (right).}\n    \\label{fig:enter-label}\n\\end{figure}\n\nFirst, we set up a system to collect human preference annotations and ensure consistency among labelers. \nWe employ a pairwise comparison mechanism, wherein each labeler is asked to select a preferred response from two options generated by our model \\hnote{our SFT model?}, given multiple responses for each human instruction \\hnote{What does it mean by select a preferred one from two and then given multiple responses?}.\nTo facilitate consistency among labelers, we establish a standard criterion \\hnote{What is this?}, as conflicting or contrary preferences could lead to undesirable noise and detrimentally affect the performance. The criteria provide different dimensions as reference points, including helpfulness, harmlessness, and fluency, to make informed comparisons. \nAdditionally, we also develop a post-filtering process to remove undesirable patterns, such as cyclic and tie preferences, during the preference data collection stage. \nAs a result, this pipeline enables us to compile a dataset of human-labeled comparisons between the outputs produced by our models.\n\nSecond, we then train a reward model on the collected preference dataset as a proxy about what responses an ordinary human user would prefer. To train a reliable proxy, we develop techniques to avoid the tendency of reward model to take shortcuts and learn unexpected bias, such as a preference for longer yet not really more helpful outputs~\\cite{singhal2023long,shen2023loose}. \n\nFinally, with the reward model as the proxy of human feedback, we employ reinforcement learning algorithms to train language models to act in accordance with the user’s preference and intention. \nBeyond performance improvement, a variety of cornercases have to be covered to support training scalable ChatGLM models and meet the requirements of online deployment of ChatGLM. \nWe implement a comprehensive training framework with various practical designs, including reward variance reduction to stabilize the training of large-scale models, model parallel strategy for efficient training with fused gradient-descent and generation, and regularization constraints to avoid capability forgetting. \n\nOverall speaking: \\todo{practices}\n\\begin{itemize}\n\\item Establishing criteria and details reference dimensions for annotation contributes to more reliable and consistent human preference.\n\\item Eliminating bias from the reward model can be a simple and effective approach to reflect genuine human preferences and prevent shortcuts.\n\\item Training stability can be improved by subtracting a baseline reward from the original reward \n\\item Incorporating SFT loss can reduce capability shifting in RLHF training. \n\\end{itemize}\n\n\\hide{\n\\begin{enumerate}\n    \\item During the annotation of human preference, it is important to explicitly instruct the annotators to pay attention to aspects such as instruction following, correctness, fluency, and safety, which ensures that the annotated preference is correlated to these factors.\n    \\item Reward bias reduction can aid in mitigating inherent biases associated with human preferences, such as preference for longer responses. A well-designed debiasing method can enable the reward model to focus more on the quality of the response rather than its length.\n    \\item Training stability can be improved by using a reference reward, which substracts a baseline reward from the original reward \n    \\item Incorporating SFT loss can reduce capability forgetting in improving human prefernce in RLHF training. \n\\end{enumerate}\n}\n\nWe conduct extensive experiments and report our results on ChatGLM-6B and ChatGLM-32B. Our evaluation results demonstrate that our framework can effectively improve the performance and empower ChatGLM to produce more helpful and human-preferred outputs. As of this writing, it is supporting online ChatGLM system and serving numerous users every day. \n\n}%end of hide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n\\section{Introduction}\n\nLarge language models (LLMs) ~\\cite{brown2020language,zhang2022opt,workshop2022bloom,zeng2023glm,touvron2023llama} have remarkably advanced the capabilities of machines in language understanding and generation. \nThe most notable example is ChatGPT~\\cite{achiam2023gpt}, which demonstrates strong capability in responding to users' queries and following their instructions.\nDifferent from the pre-trained GPT-3 model~\\cite{brown2020language}, ChatGPT (GPT-3.5, GPT-4~\\cite{achiam2023gpt}, and beyond) are further post-trained, with supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF)~\\cite{ouyang2022training}, to better align with human preferences.    \nWe have developed ChatGLM\n---a free-to-use AI service powered by the ChatGLM family of LLMs~\\cite{zeng2023glm}. \nSimilar to ChatGPT, ChatGLM was pre-trained for trillions of multilingual tokens and post-trained with both SFT and RLHF. \n\nThe goal of RLHF is to \nencourage LLMs to better align with human preferences and values, i.e., to produce more helpful, more accurate, and safer responses ~\\cite{bai2022constitutional,achiam2023gpt,ouyang2022training}. \nSpecifically, the RLHF techniques typically treat human preferences as rewards and employ reinforcement learning algorithms, such as Proximal Policy Optimization (PPO)~\\cite{schulman2017proximal}, to realize the goal of alignment. \nDespite the significant community efforts~\\cite{touvron2023llama,bai2023qwen,dai2023safe,sun2023salmon} in applying RLHF to perform LLMs alignment, since the inception of ChatGPT, our experience in constructing a practical RLHF training framework for ChatGLM has revealed unknown technical challenges.\n\nFirst, the collection and modeling of reliable human preferences is critical to the effective use of human feedback. \nHowever, biased~\\cite{cui2023ultrafeedback,longpre2023flan} and deceptive preferences~\\cite{bai2022training} in human annotations are often inevitable.\nSecond, the reward model is readily impacted by biased preferences, which can lead to the learning of shortcut features. \nThis issue may drastically compromise the precision and generalization of the reward model.\nThird, a scalable and robust training framework is required to ensure efficient and effective optimization of LLMs across different scales, as well as to cover all potential corner-cases to meet the requirements for online deployment. \nFor instance, applying PPO on LLMs of 32B parameters requires at least 8 or 16 A100 (8$\\times$80G) servers for 2-3 days, respectively. \nHowever, most existing efforts focus on parameter-efficient tuning~\\cite{sun2023salmon,yao2023deepspeed} or small-scale models~\\cite{zheng2023secrets,wu2023pairwise}. \n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.7\\textwidth]{figures/chatglm-intro-watermark.pdf}\n    \\caption{An illustrative example of ChatGLM \\textmd{(\\textcolor{blue}{\\url{chatglm.cn}} accessed on Feb. 8, 2024).}}\n    \\label{fig:enter-label}\n    \\vspace{-3mm}\n\\end{figure}\n\nIn this paper, we present the ChatGLM-RLHF pipeline, which has been used to improve ChatGLM's alignment with human preferences, with a particular focus on the Chinese language. \nThis pipeline serves as a critical component in improving  ChatGLM's performance to generate responses that more accurately reflect human values and expectations. \nWe detail our approach for designing and developing the pipeline---human preference annotation, reward model training, policy optimization, as well as practices of addressing the challenges associated with these steps. \n\nFirst, we set up a routine system to \\textit{collect human preference annotations}. \nTo ensure the consistency among annotators, we employ a pairwise comparison mechanism. \nSpecifically, each annotator is asked to select the preferred response from two outputs generated by our SFT model, guided by annotation guidelines that cover three key aspects of helpfulness, harmlessness, and fluency. \nWe also develop a post-filtering process to remove undesirable annotations, such as cyclic and tie preferences. %, during preference data collection stage. \nSecond, we train a \\textit{reward model} on the collected preference dataset as a proxy for the responses an average human user would favor. \nTo train a reliable proxy, we develop strategies to prevent the reward model from taking shortcuts or learning unexpected biases, such as a biased preference towards longer yet not really helpful outputs~\\cite{singhal2023long,shen2023loose}. \nFinally, by using the reward model as the proxy for human preferences, we apply an online RL algorithm \\textit{PPO} and an offline RL algorithm \\textit{DPO} \\cite{rafailov2023direct} to align the language model.  \n\n\\vpara{Best Practices.}\nTo support scalable RLHF training, we implement a robust training framework with important practical designs, including the strategies of reward variance reduction to stabilize large-scale training, model parallelism with fused gradient-descent, and regularization constraint to avoid catastrophic forgetting. \nIn particular, we summarize the following lessons learned over the course of developing ChatGLM-RLHF: \n\\begin{itemize}\n\\item Establishing criteria and detailed reference dimensions for annotation contributes to more reliable and consistent human preference.\n\\item Eliminating bias from the reward model can serve as an efficient and powerful approach to more accurately reflect genuine human preferences and reduce the influence of spurious correlation.\n\\item Training stability can be substantially improved by subtracting a baseline reward from the original reward during PPO training. \n\\item Incorporating next-token-prediction loss of SFT data can reduce capability shifting in RLHF training. \n\\end{itemize}\n\nExtensive experiments on ChatGLM-6B and ChatGLM-32B demonstrate that ChatGLM-RLHF can significantly improve the performance of ChatGLM, enabling it to produce more helpful, safe, and aligned responses. \nThe ChatGLM models, refined through the ChatGLM-RLHF pipeline, are supporting the online services available at \\textit{\\url{chatglm.cn}} and mobile applications on iOS and Android platforms. \n\n\\hide{\n\\begin{enumerate}\n    \\item During the annotation of human preference, it is important to explicitly instruct the annotators to pay attention to aspects such as instruction following, correctness, fluency, and safety, which ensures that the annotated preference is correlated to these factors.\n    \\item Reward bias reduction can aid in mitigating inherent biases associated with human preferences, such as preference for longer responses. A well-designed debiasing method can enable the reward model to focus more on the quality of the response rather than its length.\n    \\item Training stability can be improved by using a reference reward, which substracts a baseline reward from the original reward \n    \\item Incorporating next-token-prediction loss of SFT data can help reduce capability forgetting in improving human preference in RLHF training. \n\\end{enumerate}\n}\n\\section{Related Work}\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/overview-rlhf.pdf}\n    \\caption{The overall design of the ChatGLM-RLHF pipeline. \\textmd{We first set up a comprehensive system to collect human preferences over responses of ChatGLM and remove unexpected patterns and potential biases in the data. Then we train a reward model to predict human preferences and employ reinforcement learning to optimize ChatGLM to generate responses assigned with higer rewards.}}\n    \\label{fig:overview}\n\\end{figure*}\n\n\\vpara{Large Language Models.}\nLarge Language Models (LLMs), particularly those employing self-supervised learning techniques, have become a focal point in the field of Natural Language Processing (NLP) due to their ability to encode a vast array of knowledge within extensive parameter sets. This attribute contributes to their remarkable capabilities across a wide range of tasks. Prominent examples of such models include GPT-3~\\cite{brown2020language}, PaLM~\\cite{chowdhery2023palm}, OPT~\\cite{zhang2022opt}, BLOOM~\\cite{workshop2022bloom}, GLM-130B~\\cite{Du2021GLMGL,zeng2023glm} and LlaMA~\\cite{touvron2023llama1,touvron2023llama}. \n\nLLMs\ncan be prompted to perform a range of NLP tasks. They can effectively leverage given examples in the context~\\cite{brown2020language} to solve upcoming tasks yet without explicit finetuning. This in-context learning has primarily demonstrated LLM's ability to solve diverse and challenging tasks yet still requires elaborated few-shot exemplars as activation. \nHowever, these models often express unintended behaviors such as making up facts, generating biased or toxic text, or simply not following user instructions~\\cite{bommasani2021opportunities,kenton2021alignment,bender2021dangers}.\nTo enable LLMs to respond to human instructions in zero-shot setting, instruction tuning~\\cite{wei2021finetuned,chung2022scaling} trains LLMs with natural language instructions consisting of human-written instructions and responses.\nSuch targeted fine-tuning substantially augments the LLMs' ability to generalize to diverse tasks beyond the training dataset and significantly boosts the performance~\\cite{longpre2023flan}.\nRecently, Supervised Fine-Tuning (SFT)~\\cite{ouyang2022training}, which could be regarded as an extension of instruction tuning, is proposed to further enhance the alignment of LLM outputs with human expectations using high-quality and diverse human-written datasets. The quality of instruction data has been proved to be significantly critical~\\cite{zhou2023lima,touvron2023llama} in alignment. This approach activates and bolsters the LLMs' capabilities in various domains, including conversation, reasoning, and writing, among others. \n\n\\vpara{Aligning LLMs with Human Feedback.}\nLLMs as AI assistants are expected to follow human instructions and be helpful, honest, and harmless to align with human values and preferences~\\cite{ouyang2022training,bai2022constitutional}. Despite the effectiveness of instruction tuning and SFT, gathering accurate and expert annotations is costly and becoming more difficult with the increasingly stronger ability of LLMs, as higher quality data and stronger supervision are a must to direct model training in SFT. \nIn comparison, collecting human judgments on response quality is more feasible because it is easier for human to serve as a judge than to provide the gold answer, as demonstrated in discriminative-generative gap~\\cite{zheng2023revisiting}.\nPlenty of studies~\\cite{touvron2023llama, ouyang2022training,bai2022constitutional,bai2022training,ganguli2023capacity} have focused on refining LLMs using human preferences and found notable improvements in areas such as summarization and instruction-following. \nInstructGPT~\\cite{ouyang2022training} first trains a reward model to fit human preference and then optimizes LLMs to maximize rewards through reinforcement learning, i.e., Proximal Policy Optimization~\\cite{schulman2017proximal}, which becomes standard RLHF paradigm for following works~\\cite{touvron2023llama, bai2022constitutional}. Another line of works~\\cite{rafailov2023direct,zhao2023slic,song2023preference} is to directly optimize the policy model with human preference without training a reward model.\nDPO~\\cite{rafailov2023direct} derives the policy objective from Bradley-Terry~\\cite{bradley1952rank} reward preference. \nSLiC~\\cite{zhao2023slic} directly contrasts the positive and negative samples with a margin loss. \nPRO~\\cite{song2023preference} extends pairwise to listwise data and utilizes multiple negative samples. \nCompared to PPO, these works \nbehave similarly to off-policy reinforcement learning which \ncould be more efficient yet might sacrifice performance. \n\nDespite open-source practices~\\cite{yao2023deepspeed} and studies~\\cite{sun2023salmon,zheng2023secrets} on PPO optimization, almost all of them are conducted on small-scale models or parameter-efficient tuning with well-prepared academic benchmarks. In this work, we set up the RLHF framework from scratch and optimize the ChatGLM model, taking into account a range of practical constraints while placing \nemphasis on scalability.\n\n\\section{ChatGLM-RLHF}\nWe present the design of ChatGLM-RLHF, including human preference data collection, reward model training, and policy model optimization. \nUniquely, we introduce challenges encountered and practical solutions proposed for large-scale RLHF model training. % and deployment. %, as optimizing a large-scale model for online deployment requires careful considerations in addition to accuracy improvements. %, which is the major focus of most published work, as we have to cover plenty of corner-cases. \n\n\\subsection{Learning from Human Preferences}\nReinforcement Learning from Human Feedback (RLHF) for large language models (LLMs) typically initiates by fine-tuning a pre-trained language model with high-quality human-written data in a supervised finetuning (SFT) manner. This step yields a preliminary aligned model $\\pi_0$, called the SFT model, which can respond to human instructions yet may not behave helpfully and safely.\nTo further align the LM with human values and preferences, RLHF acts upon the SFT model.\nIt views human feedback (or preference) for model-generated texts as reward for maximization, which is different from training through next-token-prediction.\nFollowing the common setting in reinforcement learning, RLHF first trains a reward model $r_{\\phi}(x,y)$ as a proxy to model human preferences. Then the reward model is utilized to optimize policy model $\\pi_{\\theta}(y|x)$ to generate responses that are rewarded higher but are not drifting too far from the SFT model, where $(x, y)$ represents prompt and response pair and $\\{\\phi, \\theta\\}$ are learnable parameters.\n\nSubsequent parts of this section will delve into the details of the RLHF design in ChatGLM, including gathering human preference data, modeling human preferences, and fine-tuning the policy model to better align with human values.\n\n\\subsection{Data Collection and Processing}\n\n\\vpara{Prompt Collection}\nTo cover the diversity of human intents and preferences, it is important to collect a comprehensive prompt set for later response generation and comparison.\nWe collect prompts based on the demands of real-world application scenarios and apply quality filtering to select high-quality prompts.\nSpecifically, we establish a taxonomy for the possible intentions of prompts and ensure that each category of intentions is accompanied by enough training prompts.\nAfter that, a quality classifier is employed to score each prompt in three aspects:\n\\begin{itemize}[leftmargin=*]\n    \\item The \\textit{intention} of the prompt is clear, ambiguous, or totally unclear.\n    \\item The \\textit{semantic} is clear, guessable, or totally incomprehensible.\n    \\item The prompt is \\textit{answerable} or beyond models' capabilities, such as predicting lottery numbers.\n\\end{itemize}\nThe answerable prompts with clear intentions and semantics are utilized for training reward/policy models (see Table~\\ref{tab:prompt-quality-category} for examples).\nBesides, we also include several rule-based filters to select informative prompts, such as filtering out too short prompts.\n\n\\begin{table}[]\n\\centering\n\\caption{Examples of prompts with varied quality.}\n\\begin{tabular}{p{0.96\\linewidth}}\n\\toprule[1.2pt]\n\\textbf{Clear intention; clear semantics; answerable:}                                            \\\\\nAssist me in crafting a three-day travel itinerary to Beijing with a budget of under 5000. \\\\ \\hline\n\\textbf{Unclear intention:}                                                                       \\\\\nThe gentleman attended the meeting, dressed in formal attire.                        \\\\ \\hline\n\\textbf{Incomprehensible semantics:}                                                             \\\\\nChristmas, Reindeer, Christmas Tree                                                        \\\\ \\hline\n\\textbf{Unanswerable prompt:}                                                                              \\\\\nWhat is the winning lottery number for tomorrow? \\\\ \n\\bottomrule[1.2pt]\n\\end{tabular}\n\\label{tab:prompt-quality-category}\n\\end{table}\n\n\\vpara{Preference Annotation}\nReward model is trained with human feedback on the model-generated responses.\nTo collect human preference, each annotator is provided with one prompt and two responses and asked to determine which response is preferred. They are encouraged to make a decision based on \nhelpfulness and safety:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textit{Helpfulness} encompasses multiple aspects including the extent to which a response fulfills every requirement of the prompt, provides accurate and valuable information, and maintains logical consistency. Additionally, we posit that responses lacking linguistic fluency are disqualified from meeting the requirements of users. Therefore, instances of grammatical errors or unexpected language mixing, such as a mixture of Chinese and English, can detrimentally impact the perceived \\textit{helpfulness} of a response.\n    \\item The focus of \\textit{safety} is to ascertain whether responses contain harmful or toxic content, as well as content that could potentially provoke controversy.\n\\end{itemize}\nAnnotators, based on their assessments of helpfulness and safety, will decide \\textit{preference} from the perspective of overall quality. This evaluation, while inherently subjective, is crucial in the context of alignment. As the reward model is expected to assess diverse responses, various chat models and sampling strategies are utilized to generate diverse responses for annotation.\nTable~\\ref{tab:reward-model-statistic} illustrates the statistics of the preference data used for the reward model training.\n\n\\subsection{Reward Model Training}\n\nReward model is designed to assess the quality of responses, which serves as the training signal for policy improvement.\nAs annotators are asked to take helpfulness and safety into account during the annotation of preference, preference can thus reflect the overall quality of the responses in these aspects. Consequently, we exclusively utilize preference annotations for our reward model training.\n\nIn practice, the reward model is initialized with ChatGLM-SFT, and then trained on the preference data under the loss function\n\\begin{equation}\n    \\mathcal{L}_{RM} = -E_{(x,y_w,y_l)\\sim \\mathcal{D}_{RM}} [\\log (\\sigmoid (r_\\phi(x, y_w)-r_\\phi(x, y_l)))].\n\\end{equation}\nwhere $x$ denotes the prompt, $y_w$ is the preferred response to $y_l$ by the annotators. \nThe reward model $r_\\phi$ assigns a scalar value to each \\textit{(prompt, response)} pair.\n\n\\vpara{Length Bias Reduction}\nThere exist some inherent biases within the preference data, such as the inclination of humans to prefer longer, well-structured responses. Such biases may mislead the reward model to over-emphasize these secondary features, thereby overlooking the content quality of responses.\nTo alleviate the influence of length bias on the reward model, we devise a debiasing method named `Bucket-Based Length Balancing'.\n\nThe first step in our method involves calculating the length difference between the two responses in each preference pair, i.e. $d=abs(|\\mathrm{tokenize}(y_w)|-|\\mathrm{tokenize}(y_l)|)$.\nFollowing this, assign the preference pairs with similar length differences into the same buckets, each of which corresponds to a specific difference range.\nIn the final step, within each bucket, balance the number of examples where the better/worse response is longer.\n\nNote that the efficacy of this method in mitigating length bias is determined by the granularity of buckets. Finer ranges of buckets result in more effective length bias reduction.\n\n\\vpara{Stable Training}\nDuring the training of reward model, we observe substantial volatility in the distribution of scores predicted by the reward model, which suggests unstable training. To alleviate this problem, we introduce a new loss component\n\\begin{equation}\n    \\mathcal{L}_{REG} = r_\\phi(x, y_w)^2+r_\\phi(x, y_l)^2,\n\\end{equation}\nwhich resembles L2 regularization loss.\nThis loss term imposes a Gaussian prior with a mean of zero on the score distribution, thereby constraining the volatility of the score distribution.\n\n\\begin{table}[]\n\\centering\n\\caption{Stat. of human preference data for reward training. \n}\n\\scalebox{0.8}{\n\\begin{tabular}{ccccc}\n\\toprule[1.2pt]\n\\begin{tabular}[c]{@{}c@{}}Num. of\\\\ Comparisons\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Avg. \\# Turns\\\\ per Dialogue\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Avg. \\# Tokens\\\\ in History\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Avg. \\# Tokens\\\\ in Prompt\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Avg. \\# Tokens\\\\ in Response\\end{tabular} \\\\ \n\\midrule\n221,866                                                       & 2.4                                                                  & 314.1                                                               & 104.1                                                              & 267.7                                                                \\\\ \n\\bottomrule[1.2pt]\n\\end{tabular}}\n\\vspace{-2mm}\n\\label{tab:reward-model-statistic}\n\\end{table}\n\n\\subsection{Policy Model Training}\n\\vpara{Setting up Training Data} \nIn this stage, the aim is to leverage the reward model $r_{\\phi}$ to guide the optimization of the policy model $\\pi_{\\theta}$ to elicit responses of increased reward value. The model is expected to receive both positive and negative feedback and is capable of discerning disparate responses of varied quality, thereby progressively augmenting its response generation in line with human preferences. To realize this, it is crucial for the model to generate responses spanning a spectrum of quality and thus the model can learn and improve from the reward feedback. Consequently, prompts leading to almost the same responses must be systematically excluded to ensure the effectiveness of this process.\n\nMore specifically, we conduct a preprocessing step to filter out less valuable data by means of the reward model. For each prompt, the model generates $K$ responses. If the variance in rewards for these responses is smaller than a margin $\\epsilon$, it indicates that the prompt offers limited opportunity for exploration and improvement against the current model, and is therefore removed from the training dataset. Consequently, the final training dataset includes only those instances where significant variation and diversity are observed in the responses generated by the model.\n\n\\vpara{Proximal Policy Optimization (PPO)} is an online reinforcement learning framework for policy improvement. %It fuses both training and inference operation as it optimizes the model based on its instant actions. \nDuring PPO, we seek to optimize the policy model by maximizing the cumulative reward. \n\\begin{equation}\n    \\mathrm{argmax}_{\\pi_{\\theta}} \\mathbb{E}_{x\\sim \\mathcal{D},y_g\\sim \\pi_{\\theta}}[r_\\phi(x,y_g)]\n\\end{equation}\nwhere $\\pi_{\\theta}$ is the policy model and $\\theta$ is its learnable weights.  \nIn each training iteration, the policy model first generates responses $\\{y_{i}\\}$ for a set of prompt $\\{x_i\\}$. Then the reward model evaluates and assigns scores to each response, leading to $\\{r(x_i,y_i)\\}$, which is used to direct the gradient descent update of the policy model. \n\nSince the reward reflects approximate human preference and could be inaccurate, to avoid the policy diverging too far from the initial SFT model $\\pi_0$, a penalty term based on KL divergence is usually added as part of the reward as regularization and also helps maintain training stability. \n\\begin{equation}\n    \\begin{split}\n    r(x, y_g) &= r_\\phi(x,y_g) - \\beta D_{KL}(\\pi_{\\theta}(y_g|x) || \\pi_{0}(y_g|x)) \\\\\n    &= r_\\phi(x, y_g) - \\beta \\log \\frac{\\pi_{\\theta}(y_g|x)}{\\pi_{0}(y_g|x)}\n    \\end{split}\n\\end{equation}\n\n\\vpara{Reward Bias Reduction} In the previous section, we make efforts to mitigate potential biases in reward modeling, such as length bias. Despite these efforts, it is observed that biases could not be entirely eliminated. These biases predominantly stem from two factors:\n1) Value instability. The reward model utilizes a pairwise training loss; however, it does not impose explicit constraints regarding the numerical values assigned to responses in relation to their quality. Consequently, this lack of definitive guidance necessitates the implementation of reward or advantage normalization in the preliminary stages of PPO training to maintain stability.\n2) Task/sample bias. The objective to enhance the general capabilities of the policy model necessitates training data that encompasses a wide range of tasks and prompts, including creative writing, mathematical reasoning, and role-playing. This diversity leads to the reward model assigning highly variable rewards across different prompts and tasks. This variability is attributed to differences in response style, length, and the Discriminativeness of the reward model, among other factors.\nFigure ~\\ref{fig:reward_distribution} illustrates the significant disparities in reward scores both within a single task and across different tasks. Consequently, a response that receives a high reward cannot be reliably deemed as high-quality without considering the influence of its associated prompt and task.\n\n\\begin{wrapfigure}{r}{0pt}\n\\centering\n\\begin{minipage}[t]{0.5\\columnwidth}\n\\vspace{-3mm}\n\\includegraphics[width=\\linewidth]{figures/reward_distribution.png}\n\\vspace{-6mm}\n\\caption{Reward distributions on different tasks.}\n\\label{fig:reward_distribution}\n\\vspace{-3mm}\n\\end{minipage}\n\\end{wrapfigure}\n\nTo overcome the above drawbacks, we introduce a reference baseline to avoid the variability from absolute value. The intuition is that the target is to enable the policy model to generate a better response than the reference one as much as possible, no matter its absolute reward. This corresponds to the initial objective of human preference and reward model training which excels at pairwise comparison rather than pointwise scoring.\nTo be specific, given a prompt $x$, we generate a response $y_{ref}$ during pre-processing as the reference response. During policy training, we regard the reward difference between the generated response $y$ and the reference response $y_{ref}$ as the optimization target,\n\\begin{equation}\n    r(x, y) = \\left(r_\\phi(x, y_g) - r_\\phi(x, y_{ref})\\right) - \\beta D_{KL}\\left(\\pi_{\\theta}(y_g|x) || \\pi_{0}(y|x)\\right)\n\\end{equation}\nAt the warm-up training stage, $y$ and $y_{ref}$ are drawn from almost the same distribution, and thus $r(x,y)$ starts from near zero, which also helps stabilize the training even without other normalization tricks. ~\\cite{wu2023pairwise} adopts a similar idea to replace absolute feedback with relative feedback\n\n\\vpara{Capability Forgetting}\nThe RLHF is expected to increase the response quality and better align them with human preferences. However, as a post-training step subsequent to SFT, we also observed unexpected behaviors in the policy after the RLHF stage. The model shows a reduced capability in handling specific scenarios, including wrong self-identification and invalid JSON format, and shifts a little far from the original model unexpectedly.\nFor example, the model is trained to claim itself as \"ChatGLM\" after SFT yet it would describe itself as \"an AI assistant\" after RLHF if asked \\textit{\"who are you?\"}.\nThis behavior could be \nattributed to the problem of difference among data distributions or the inability of the reward model in such nuanced details. \nThe problem of capability forgetting (or shifting) gets little attention in previous studies. \nBut it is critical for online deployment,  especially for addressing diverse edge cases in real-world scenarios. \nFor example, output in JSON format is crucial for tasks involving semantic analysis or answer extraction in practical applications. The emergence of capability forgetting can be attributed to the variance in data distribution between SFT and RLHF training, which encapsulates different facets of a model's capabilities.\n\nTo overcome the issue of capability forgetting, we propose to incorporate an extra supervised next-token-prediction loss as anadditionaln regularization besides the KL divergence, when performing reward maximization. This is intended to preserve the pre-existing abilities of the SFT model. The high-level idea is to encourage the model's outputs to align with human preferences through RLHF and leveraging next-token prediction to capture more granular signals.\nTo be more specific, in each training step, the training involves two aspects of data and objective: 1) RLHF training with prompts and model-generated responses for preference optimization,\n$\\mathcal{L}_r=-\\sum_{x \\sim \\mathcal{D}} r(x,y_g)$; \n2) next-token-prediction with a small amount of human-annotated (prompt, response) pairs $\\mathcal{D}_{S}$, which are specific to particular tasks and serve as supervised regularizations,\n\\begin{equation}\n\\mathcal{L} = \\mathcal{L}_{r} - \\beta \\cdot \\sum_{(x,y)\\sim \\mathcal{D}_{S}} \\log \\pi_{\\theta}(y|x)\n\\end{equation}\n\n\\vpara{A simplified alternative: Direct Preference Optimization (DPO)} PPO has shown special advantages as an online algorithm, but it also brings additional technical difficulties in efficient parallel training in large language models as it involves both gradient decent update and response sampling during training.\nDirect Preference Optimization (DPO)~\\cite{rafailov2023direct} has been proven to be a simple yet effective alternative to PPO. DPO proposes to directly learn from annotated preference data without training a reward model. In our practice, we also implement DPO for comparison and find it is more flexible yet very effective. Different from the implementation reported in the original paper, we still train a reward model and regard it as a discriminator to help construct training data pairs with varied reward scores$(x, y_w, y_l)$. This way separates training data construction and data annotation and is thus more scalable. In DPO, the policy is trained to optimize the following objective:\n\\begin{equation}\n    \\mathcal{L}_{r} = -\\sum_{(x, y_w, y_l)\\sim \\mathcal{D}}\\log \\sigma(\\beta \\log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_0(y_w|x)} - \\beta \\log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_0(y_l|x)} )\n\\end{equation}\n\n\\para{Parallel Training}\nTo efficiently train large models, the policy is trained with a combination of data parallelism and model parallelism~\\cite{rasley2020deepspeed,narayanan2021efficient}. Model parallelism is further divided into tensor parallelism, which involves the division of tensors, and pipeline parallelism, which distributes different layers of a network across multiple GPUs.\nPipeline parallel can effectively reduce the communication overhead between GPUs during training with large batch size.\nTypically, both the pretraining phase and supervised finetuning leverage a three-dimensional parallelism strategy—combining data, tensor, and pipeline parallelism—to optimize efficiency.\nIn the context of DPO, which relies solely on pre-generated responses, its training aligns with that of SFT, allowing for the application of three-dimensional parallelism.\nBut things are different for PPO, which involves the fusion of both gradient descent and inference within its training process.\nEach generation comprises hundreds or thousands of forward operations and would incur a number of unexpected bubbles in the pipeline parallel and significantly slow down the generation. \nThe generation generally accounts for more than 80\\% training time, and it is crucial to avoid these bubbles for efficiency.\nConsequently, in PPO training, we adopt a two-dimensional parallelism approach, combining data and tensor parallelism, to circumvent these bottlenecks.\nThere are public implementations based on fully-sharded data parallel~\\cite{yao2023deepspeed, havrilla-etal-2023-trlx} but their efficiency is not satisfactory when finetuning full parameters of large-scale models due to the heavy communication cost. \n\n\\para{RLHF v.s. Rejection Sampling}\nPrevious studies~\\cite{touvron2023llama} have shown the effectiveness of Rejection-Sampling Fine-Tuning (RFT) as a post-SFT step in enhancing the performance of model alignment. RFT involves generating multiple outputs from the policy model and selecting the best candidates with the reward as the gold standard for further finetuning. The main difference between RFT and DPO is that DPO utilizes both positive and negative pairs for optimization, whereas RFT focuses on the positive instances. Both methods are constrained to pre-generated responses for training. PPO, an online algorithm, dynamically updates the model weights based on responses generated from the latest policy during training, which would generally produce better results.\n\n    \\centering\n    \\small\n    \\caption{Results of automatic evaluation on AlignBench (Chinese alignment) rated by gpt-4-0613. \\textit{Prof. Know.}: professional knowledge. \\textit{Fund. Lang.}: fundamental language. \\textit{Logic.}: logical reasoning.}\n    \\renewcommand\\tabcolsep{4pt}\n    \\begin{tabular}{l|cccccccc|c}\n    \\toprule[1.2pt]\n         & \\begin{tabular}{@{}c@{}}Prof. \\\\ Know.\\end{tabular} & \\begin{tabular}{@{}c@{}}Advanced \\\\ Chinese\\end{tabular} & \\begin{tabular}{@{}c@{}}Fund. \\\\ Lang.\\end{tabular} & Math & Writing & OpenQA & \\begin{tabular}{@{}c@{}}Role \\\\ Play\\end{tabular} & \\begin{tabular}{@{}c@{}}Logic.\\end{tabular} & Overall \\\\\n    \\midrule\n    GPT-3.5-turbo-0613$^1$ & 6.77 & 5.81 & 6.71 & 5.68 & 7.03 & 7.29 & 7.28 & 5.02 & 6.08 \\\\\n    GPT-4-0613$^1$ & 7.94 & 6.93 & 7.81 & 7.56 & 7.93 & 7.42 & 7.51 & 7.37 & 7.53 \\\\\n    GPT-4-1106-preview$^1$ & 8.65 & 7.33 & 7.99 & 7.80 & 8.67 & 8.61 & 8.47 & 7.66 & 8.01 \\\\\n    GLM-4 (0116) & 8.52 & 8.19 & 7.29 & 7.26 & 8.27 & 8.50 & 8.53 & 7.30 & 7.75 \\\\\n    \\midrule\n     Chinese-LLaMa-2-7B-Chat$^1$ & 4.13 & 4.26 & 4.31 & 2.29 & 4.63 & 4.50 & 4.91 & 3.07 & 3.57 \\\\ \n     Qwen-7B-Chat$^1$ & 5.66 & 5.74 & 6.40  & 3.62 & 6.31 & 6.26 & 6.19 & 3.83 & 4.91 \\\\\n     ChatGLM3-6B-SFT & 4.80 & 5.17 & 5.21 & 2.71 & 6.72 & 6.55 & 6.34 & 4.02 & 4.58 \\\\\n     ChatGLM3-6B-DPO & 5.64 & 4.60 & 4.94 & 3.41 & 6.85 & 7.29 & 6.66 & 3.81 & 4.80 \\\\\n     ChatGLM3-6B-PPO & 5.93 & 5.08 & 5.28 & 3.57 & 7.13 & 6.84 &  6.16 & 3.90 & 4.90 \\\\\n    \\midrule\n     DeepSeek-67B-Chat & 7.37 & 6.52 & 7.12 & 5.71 &\t7.20 & 7.58 & 6.91 & 5.79 & 6.43 \\\\\n     DeepSeek-67B-Chat-DPO\t & 7.71 & 7.47 & 7.29 & 6.13 & 7.51 & 7.82 & 7.83 & 5.41 & 6.69\t\\\\\n     ChatGLM-32B-SFT$^2$     & 7.47 & 6.62 & 6.53 & 5.87 & 7.45 & 7.13 & 7.38 & 5.42  & 6.37 \\\\\n     ChatGLM-32B-DPO & 7.27 & 6.41  & 6.87 & 6.12 & 7.97 & 8.34 & 8.17 & 5.60  & 6.68 \\\\\n     ChatGLM-32B-PPO & 7.27 &  6.97 & 7.06 & 5.85 & 7.92 & 8.18 & 7.96 &  6.02 & 6.75 \\\\\n    \\bottomrule[1.2pt]\n    \\end{tabular}\n     \\begin{tablenotes}\n        \\footnotesize\n        \\item $1$ The results are from the AlignBench paper, which employs varying generation temperatures for different tasks. Notably, all ChatGLM models produce responses in a greedy setting. In our experiments, the performance in the greedy setting is typically slightly lower than in the original setting but is stable.\n        \\item $2$ The version of ChatGLM is ChatGLM-32B-2310.\n    \\end{tablenotes}\n    \\label{tab:alignbench}\n\\end{table*}\n\n\\section{Experiments}\n\n\\subsection{Experimental Setup}\nWe conduct experiments based on ChatGLM~\\cite{zeng2023glm,Du2021GLMGL} and test the effectiveness of \nDPO and PPO as the post-training step after SFT, as RLHF is always implemented based on an SFT model. \nWe mainly focus on the improvement of the RLHF-aligned model over SFT model via both automatic evaluation and human evaluation.  \n\n\\vpara{Training Details} For PPO training, the policy and critics models share the same learning rate 1e-6 and are trained for around 600 to 800 iterations. Neither reward nor advantage normalization is adopted as reference rewards already lead to a stable training process in our experiments. At inference time, the policy top-p is set to 0.9 to generate diverse responses. \nIn this section, we report the results on ChatGLM-6B and ChatGLM-32B models.\n\n\\subsection{Automatic Evaluation}\n\\vpara{Setup} To evaluate the alignment performance on tasks in Chinese, we assessed the ability of our trained language models across various fields using the AlignBench test set~\\cite{liu2023alignbench}, a benchmark for open-ended questions. AlignBench comprises 8 main categories and 36 subcategories of tasks, covering a total of 683 questions. % including Fundamental Language Abilities, Chinese Advanced Understanding, Open-ended Questions, Writing Ability, Logical, Reasoning, Mathematics, Task-oriented Role Play, and Professional Knowledge. \nAlongside each question, AlignBench provides a reference answer and evaluation criteria, facilitating the automatic assessment of response quality by LLM-as-judge. We follow the original setting and use GPT-4 as the judge. The evaluation framework assembles the evaluation principles, the question, the model-generated response, and a reference answer as the input prompt to the LLM judge. The LLM-judge then performs the intended analytical evaluation and assigns a score ranging from 1 to 10 to the response.\n\nWe use the official code repository of AlignBench, available on GitHub\\footnote{\\url{https://github.com/THUDM/AlignBench}}. We generated responses to all questions with a greedy strategy to ensure the stability of the results and reduce variance, which is a little different from the original setting that sets different temperatures for different tasks.\n\n\\vpara{Results} The overall results are illustrated in Table~\\ref{tab:alignbench}. We compare with ChatGPT~\\cite{achiam2023gpt} and various open-source models~\\cite{bi2024deepseek,bai2023qwen,touvron2023llama}. The results of the ChatGLM-32B show that RLHF methods, including both DPO and PPO, substantially improve the SFT model across a wide range of tasks in Alignbench. Notably, tasks such as Writing, OpenQA, and Role-Play exhibit more significant improvements. This suggests that the reward model is particularly suited to tasks related to creative writing, whereas it demonstrates limitations in tasks requiring advanced reasoning abilities, such as math and logic. This observation aligns with the intuitive understanding that human preferences tend to lean more toward stylistic and formatting elements rather than deep critical thinking skills.\nFurthermore, PPO demonstrates a slight edge over DPO, with an average improvement margin of approximately 0.07. This outcome aligns with expectations, considering that PPO's more intricate design demands significantly greater resources during training compared to DPO.\n\n\\begin{table}[]\n    \\centering\n    \\caption{Results of human evaluation of ChatGLM-32B on internal test set. \\textmd{We report the proportion of wins and ties of the PPO and SFT model on different tasks.}}\n    \\begin{tabular}{c|ccc|c}\n    \\toprule[1.2pt]\n         &  PPO win & Tie & SFT win & $\\Delta$(PPO-SFT) \\\\\n    \\midrule\n        Multilingual & 0.32 & 0.44 & 0.24 & 0.08\\\\\n        Creative Writing & 0.40 & 0.52 & 0.08 & 0.32 \\\\\n        Role-play & 0.42 & 0.42 & 0.16 &  0.26 \\\\\n        Knowledge & 0.36 & 0.50 & 0.14 &  0.22 \\\\\n        Semantic Extraction & 0.30 & 0.50 & 0.20 & 0.10  \\\\\n        Math. & 0.16 & 0.70 & 0.14 & 0.02 \\\\\n        Programming & 0.22 & 0.72 & 0.06 & 0.16  \\\\\n        Logic & 0.20 & 0.64 & 0.16 & 0.04 \\\\\n    \\midrule\n        Overall & 0.30 & 0.55 & 0.15 & 0.15 \\\\\n    \\bottomrule[1.2pt]\n    \\end{tabular}\n    \\vspace{-2mm}\n    \\label{tab:human_eval}\n\\end{table}\n\n\\subsection{Human Evaluation}\n\\vpara{Setup.} \nIn addition to automatic evaluations, we also incorporate human evaluations to assess the effectiveness of RLHF. Human evaluation is important because they're less likely to be biased as long as the rules are clearly defined. We collect a diverse human evaluation dataset, consisting of 400 instructions in Chinese, spanning an extensive array of tasks such as creative writing, logical reasoning, semantic analysis, language comprehension, and mathematics. Each subject area was represented by 50 distinct samples. \nThe evaluation process employed a pairwise comparison method, where human annotators were tasked with selecting the more suitable response from a pair, with the option to declare a tie. And we have set very lenient requirements for determining whether two responses can be labeled as a tie, so that the \\textit{win} could denote a clear advantage of one response over the other.\nTo minimize variability, each sample is annotated by two evaluators. In this part, we mainly compare ChatGLM-32b-SFT and ChatGLM-32b-PPO, as PPO has shown an edge over DPO in automatic evaluation.\n\n\\vpara{Results} Table~\\ref{tab:human_eval} shows the results of human evaluation and we report the win-rate of two models. The results reveal that the PPO model maintains a distinct advantage over the SFT model, with a win-rate of 30\\% compared to 15\\%. Regarding task-specific performance, the findings align with the observations from the automatic evaluations. Notably, the improvement in mathematical reasoning tasks are minimal and barely significant, whereas tasks related to writing exhibit considerable improvements. Moreover, an unexpected yet significant advancement was observed in programming tasks. However, based on our experience, it remains challenging for the reward model to accurately identify errors within code snippets. The programming instructions primarily focus on practical guidance, such as \\textit{how to build an Anaconda in linux}. This diverges from conventional code-evaluation benchmarks like HumanEval~\\cite{chen2021evaluating}, requiring the model to solve practical programming problems. Additionally, a high tie rate of 55\\% was recorded, aligning with our anticipations.\n\n\\hide{\n\\vpara{Case Study} Through human evaluation, we also found that RLHF can improve ChatGLM to better understand and follow human instructions.\nwhich is not reflected in Table~\\ref{tab:alignbench} and ~\\ref{tab:human_eval}. \nFigure~\\ref{fig:case_study} shows a representative case, in which the PPO model can better capture the indication of \"romantic\" and the conversation between two people.\nThis aligns with the notion that the reward model can provide both positive and negative signals, discouraging undesired behaviors while promoting desired ones.\n}\n\n\\subsection{Evaluation on the Reward Model}\n\nWe conduct experiments to evaluate the accuracy of the reward model in predicting human preferences. Specifically, we create a test set comprising pairwise comparisons of tasks from human evaluation, each accompanied by carefully-examined human annotations. \nAt inference time, the reward model is capable of producing a scalar value for an individual response without the need for a paired sample. \nWe utilize the reward model to predict the reward for each response and assess whether it assigns higher reward scores to the responses preferred by humans.\n\nThe results are presented in Table~\\ref{tab:reward_model}. We also include the accuracy of Llama2 reward model as a baseline for comparison. Despite ChatGLM and Llama2 being evaluated using distinct internal datasets, their results are still comparable as both models can reflect the alignment between human preferences and the predictions of the reward model. The data indicates that ChatGLM-32B surpasses ChatGLM-6B in terms of accuracy, as expected. However, the highest accuracy achieved by ChatGLM-32B is 65\\%, which is on par with that of Llama2-70B. The marginal accuracy gain with increased model scale suggests a limit to the benefits of scaling. Nonetheless, the fact that the reward model can guide the training of RLHF algorithms with approximately 65\\% accuracy in mirroring human judgment is noteworthy and deserves further exploration.\n\n\\begin{table}[]\n    \\centering\n    \\caption{Consistency between reward model and human preference on the internal test set. \\textmd{The results of Llama2 are from~\\cite{touvron2023llama}. ChatGLM and Llama2 are not evaluated on the same test set. But the results could still be comparable in how accurately the reward model predicts human preferences. We report Accuracy. }}\n    \\begin{tabular}{l|cc}\n    \\toprule[1.2pt]\n         &  Training Acc & Test Acc \\\\\n    \\midrule\n        Llama2-7B &  - & $\\sim$0.61\\\\\n        Llama2-13B & - & $\\sim$0.61 \\\\\n        Llama2-70B & - & $\\sim$0.64 \\\\\n        ChatGLM-6B & 0.64 & 0.59 \\\\\n        ChatGLM-32B & 0.68 & 0.65 \\\\\n    \\bottomrule[1.2pt]\n    \\end{tabular}\n    \\label{tab:reward_model}\n\\end{table}\n\n\\begin{table}[]\n    \\centering\n    \\caption{Response length of different models (Tokens) . \\textmd{Both DPO and PPO significantly increase the response length.}}\n    \\begin{tabular}{lll}\n    \\toprule[1.2pt]\n         & ChatGLM-6B & ChatGLM-32B \\\\\n    \\midrule\n        SFT model & 255.3 & 305.6 \\\\\n       \\qquad  + DPO & 427.5 & 410.9 (+32.6\\%) \\\\\n       \\qquad  + PPO & 237.4 & 379.6 (+22.9\\%) \\\\\n    \\bottomrule[1.2pt]\n    \\end{tabular}\n    \\vspace{-2mm}\n    \\label{tab:length}\n\\end{table}\n\n\\begin{figure}[ht]\n    \\centering\n    \\begin{minipage}[t]{0.35\\textwidth}\n        \\includegraphics[width=\\textwidth]{figures/Rewards.pdf}\n    \\end{minipage}\n    \\begin{minipage}[t]{0.35\\textwidth}\n        \\includegraphics[width=\\textwidth]{figures/KL-estimated.pdf}\n    \\end{minipage}\n    \\caption{Ablation on the effects of reference reward. \\textmd{The two figures show the reward and estimated-KL during the training process of ChatGLM-32B-PPO.}}\n    \\vspace{-2mm}\n    \\label{fig:reference_reward}\n\\end{figure}\n\n\\subsection{Ablation Study}\n\n\\vpara{Response Length} In automatic evaluation, GPT-4 serves as the automated evaluator for assessing response quality. Previous study~\\cite{zheng2023judging} suggests that GPT-4 is inclined to assign higher scores to lengthier responses, although the response may not imply better quality and instead denotes redundancy. To explore this aspect, we examine the lengths of responses generated by our models and report the results in Table~\\ref{tab:length}. It is observed that all models produced responses that surpassed the SFT model in length. Notably, RLHF methods exhibit a remarkable increase in response length, i.e., 110 tokens in DPO and 75 tokens in PPO. Therefore, the improvement in automatic evaluation scores for creative writing tasks might be partly attributed to the increased response lengths. Moreover, the comparison between PPO and DPO reveals that despite the fact that PPO can generate shorter responses on average, it outperforms DPO in terms of performance. This indicates that PPO is capable of generating higher-quality responses with less redundancy.\n\n\\vpara{Effects of Reference Rewards} In our implementation of PPO, we employ a reference reward by deducting the reward of a baseline response. To evaluate the effectiveness of this approach, we compare PPO trained with (\\textit{w/}) and without (\\textit{w/o}) the reference reward, under the same hyper-parameter setting. Figure \\ref{fig:reference_reward} illustrates the progression of reward and estimated KL-divergence throughout the training process. Remarkably, both experiments show similar patterns in reward improvement. However, the starting point of the reward value for the \\textit{w/ ref} scenario is near zero and this contributes to the stabilization of the training process, which is also observed in the training loss curve. Moreover, the increase in estimated KL-divergence for the \\textit{w/o ref} scenario is much lower, thereby drifting less far from the original SFT model, as expected.\n\n\\hide{\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.45\\textwidth]{figures/case_study.pdf}\n    \\vspace{-1mm}\n    \\caption{Case study of human evaluation.}\n    \\label{fig:case_study}\n\\end{figure}\n}\n\n\\section{Conclusion}\nThis paper introduces ChatGLM-RLHF, a reinforcement learning pipeline devised to enhance the alignment of ChatGLM with human preference.\nWe provide an in-depth explanation of the three main components in ChatGLM-RLHF, data collection, reward model training, and policy model training.\nAdditionally, we discuss the challenges encountered during the practices and the corresponding solutions, such as how to mitigate reward bias and variance, implement model parallelism, and avoid catastrophic forgetting. Experimental results indicate a significant improvement of ChatGLM-RLHF over ChatGLM-SFT, marking an average of 15\\% more wins in alignment with human preference, which underscores the efficacy of the proposed pipeline.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{LongReward: Improving Long-context Large Language Models\\\\ with AI Feedback}\n\n\\begin{document}\n\n\\maketitle\n\n\\renewcommand{\\thefootnote}{\\fnsymbol{footnote}}\n    \\footnotetext[2]{Work done when JZ and ZH interned at Zhipu.AI.\n    }\n\n\\begin{abstract}\nThough significant advancements have been achieved in developing long-context large language models (LLMs), the compromised quality of LLM-synthesized data for supervised fine-tuning (SFT) often affects the long-context performance of SFT models and leads to inherent limitations. In principle, reinforcement learning (RL) with appropriate reward signals can further enhance models' capacities. However, how to obtain reliable rewards in long-context scenarios remains unexplored. To this end, we propose \\textbf{LongReward}, a novel method that utilizes an off-the-shelf LLM to provide rewards for long-context model responses from four human-valued dimensions: helpfulness, logicality, faithfulness, and completeness, each with a carefully designed assessment pipeline. By combining LongReward and offline RL algorithm DPO, we are able to effectively improve long-context SFT models. Our experiments indicate that LongReward not only significantly improves models' long-context performance but also enhances their ability to follow short instructions. We also find that long-context DPO with LongReward and conventional short-context DPO can be used together without hurting either one's performance. Our code and data are available at \\url{https://github.com/THUDM/LongReward}.\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\nIn recent years, significant advancements have been achieved in the development of long-context large language models (LLMs)~\\cite{claude-35, glm4, reid2024gemini}.  The context windows of many contemporary LLMs have been extended to over 100,000 tokens, enabling them to process extensive context as input and perform various downstream tasks such as long document understanding and summarization~\\cite{longbench}. \n\nAmong numerous methods for extending the context window, an effective and well-established approach involves continual pre-training on longer texts, followed by supervised fine-tuning (SFT) using diverse long-context question-answering (QA) data~\\cite{longllama, longalign}.  However, due to the difficulty of annotation, most long-context QA pairs are automatically synthesized by LLMs themselves~\\cite{longalign, llama-3-1, An2024}, making it challenging to guarantee the quality of data. For instance, the generated answers may not fully adhere to query requirements,  contain logical errors, include fabricated content, or be incomplete. Such compromised data quality often affects the long-context capacities of SFT models, making them suffer from inherent flaws such as hallucinations~\\cite{huang23} and inability to fully utilize the context information~\\cite{lostinmiddle, longcite}.\n\nOn the other hand, reinforcement learning (RL) with human- or AI-based rewards that penalize unpreferred behaviors has been shown as effective in reducing irrelevant, untruthful, and other undesired model outputs for short-context LLMs~\\cite{rlhf, rlaif, tian2024}. Nevertheless, obtaining such rewards in long-context scenarios is still challenging due to the unscalability of human labeling and the lack of reliable long-context reward models. \n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/intro.pdf}\n    \\caption{The compromised quality of synthesized SFT data often affects the performance of long-context SFT models, while LongReward utilizes an off-the-shelf LLM to provide reliable rewards for long-context-based model responses, enabling the employment of RL algorithms such as DPO to further enhance models' capacities.}\n    \\label{fig:intro}\n\\end{figure*}\n\nIn light of these challenges, we propose \\textbf{LongReward}, a novel method that utilizes an off-the-shelf LLM as judge to provide rewards for long-context-based model responses from four human-valued dimensions: (1) Helpfulness: whether the response is relevant and informative to the query and meets all the requirements; (2) Logicality: whether different parts of the response are logically consistent; (3) Faithfulness: whether all information in the response is consistent with the context; (4) Completeness: whether the response covers all question-relevant key points in the context, without omitting important aspects. Given a model response, LongReward will give a score ranging from 0 to 10 for each dimension, and take their average as the final reward. Specifically, for helpfulness and logicality whose assessment primarily depends on the response content and is mostly independent of the context, we employ the LLM to directly assign scores based on the query and response through few-shot learning. For the estimation of faithfulness, we require the LLM to first break the response into a list of factual statements and then judge whether each statement is supported by the retrieved context chunks. Finally, for completeness, we first let the LLM extract question-relevant information from each segment of the context, then ask it again to evaluate the response completeness according to all the extracted information.\n\nBy combining LongReward and RL algorithms such as Direct Preference Optimization (DPO)~\\cite{dpo}, we can effectively mitigate the deficiencies of long-context SFT models and further enhance their capabilities. Our experiments on Llama-3.1-8B~\\cite{llama-3-1} and GLM-4-9B~\\cite{glm4} show that the DPO models using LongReward outperform SFT models by 4.9\\% and 5.5\\% on long-context tasks, respectively, surpassing all baseline methods. Human evaluation further validates that LongReward has good alignment with human preference and helps improve long-context models from all dimensions (i.e., helpfulness, logicality, faithfulness, and completeness), bringing 46\\% more wins against the SFT baseline. Meanwhile, we find that LongReward also benefits models' short-instruction-following ability, and can be well incorporated into standard short-context DPO to jointly improve long- and short-context performance.\n\nTo summarize, our main contributions include: (1) proposing LongReward, the first method as we know to automatically provide reliable rewards for long-context-based model responses; (2) designing a long-context RL framework by combining LongReward and DPO; (3) conducting extensive experiments to validate the efficacy of LongReward in improving long-context LLMs.  \n\n\\section{Related Work}\n\\textbf{Long-context LLMs}. \nLong-context LLMs aim to break the context length limitations of existing LLMs and understand internal long-range dynamics~\\cite{longalign,ma2024megalodon}. One research direction focuses on designing efficient attention mechanisms~\\cite{longformer, bigbird, minference} or structured state space models~\\cite{poli2023hyena,gu2023mamba} to overcome the length limitations. For instance, ~\\citet{ding2023longnet} adopts sparse attention to scale the context length to billions of tokens. However, the performance of these methods usually falls short of standard Transformers~\\cite{gu2023mamba,ma2024megalodon}.\nAnother research branch focuses on extending Transformers' context window via continual pre-training and SFT on longer texts~\\cite{longllama,longalign}. Despite larger computation overhead, these methods typically demonstrate better performance on various long-context tasks. Nevertheless, their use of automatically synthesized SFT data that lacks human examination still compromises the capacities of current long-context LLMs to some extent. \n\n\\noindent\\textbf{Improving LLMs with AI feedback.} \nReinforcement learning from human feedback is crucial in aligning LLMs with human values and intentions~\\cite{rlhf, bai2022training,sun2023salmon}. However, collecting high-quality human pairwise preference data can be expensive and time-consuming~\\cite{constitutionalAI, rlaif}. \nAn alternative solution is to obtain feedback from LLMs, as modern LLMs have shown a high degree of alignment with human judgment~\\cite{ding2022gpt,gilardi2023chatgpt}. Following this direction, ~\\citet{constitutionalAI} first integrates LLM-labeled preferences data with human-labeled ones to optimize models' harmlessness.\n~\\citet{dubois2024alpacafarm} further introduces the AlpacaFarm simulator, which leverages API LLMs to choose the preferred model responses, offering lower costs compared to human labelers. \nMore recently, ~\\citet{yuan2024self} develops self-rewarding language models, wherein the LLM itself acts as a judge, providing its rewards during training. However, these diverse approaches mainly focus on short-context scenarios. In contrast, our work first incorporates AI feedback with long-context scenarios and successfully improves LLMs' long-context capability.\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.95\\linewidth]{figs/longreward.pdf}\n    \\caption{Illustration of LongReward. LongReward evaluates a long-context-based model response from four dimensions: helpfulness, logicality, faithfulness, and completeness. It assigns a score ranging from 0 to 10 for each dimension, and takes their average as the final reward.}\n    \\label{fig:longreward}\n\\end{figure*}\n\n\\section{Methodology}\n\nIn this section, we will briefly introduce reinforcement learning for LLMs as well as the DPO algorithm, and then discuss the methodology of LongReward, including multi-dimensional LLM scoring for long-context-based model responses and the combination of LongReward and DPO.\n\n\\subsection{Preliminary}\nReinforcement learning (RL) aligns LLMs with human preference by maximizing the average reward of model outputs, where a reward model $r(x,y)$ assigns a scalar reward to each input-output pair $(x, y)$ to represent its desirability~\\cite{rlhf, bai2022training, stiennon2020}. Conventional RL algorithms such as PPO~\\cite{ppo} involve online response sampling and training multiple LLMs, thereby being complex to implement, while DPO~\\cite{dpo} simplifies the RL process and proposes to directly learn from a dataset of preference pairs $\\mathcal{D}=\\{(x, y_w, y_l)\\}$, where the winning response $y_w$ is preferred over the losing response $y_l$ given the same prompt $x$. The optimization objective of DPO is to maximize the difference between likelihood of preference pairs:  \n\\begin{equation}\n\\begin{aligned}\n&\\mathcal{L}_\\text{DPO}(\\pi_\\theta; \\pi_\\text{ref})= -\\mathbb{E}_{(x, y_w, y_l)\\sim\\mathcal{D}} \\\\\n&[\\log\\sigma(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_\\text{ref}(y_w|x)}-\\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_\\text{ref}(y_l|x)})]\n\\end{aligned}\n\\end{equation}\nHere, $\\pi_\\theta$ denotes the policy model, which is the LLM being trained and usually initialized from its SFT version, $\\pi_\\text{ref}$ denotes the reference model, typically the frozen SFT model, and $\\beta$ is a coefficient that controls the penalty intensity for dispreferred responses. Though DPO eliminates the need for an explicit reward model, many works still train a reward model~\\cite{rso, rs-dpo, chatglm-rlhf} or design proxy reward methods~\\cite{tian2024} to enable automated annotations of preference pairs and efficient sampling from the SFT policy, especially when human preference labeling is costly and unscalable.\n\n\\subsection{LongReward}\nAs mentioned in Sec.~\\ref{sec:intro}, the main obstacle to employing RL for long-context LLMs is the lack of approaches to obtain reliable rewards in long-context scenarios. \nInspired by LLM-as-Judge approach in open-ended question-answering evaluation~\\cite{mt-bench, alpacaeval2, alignbench}, we propose LongReward, a novel method that utilizes an off-the-shelf LLM $M_\\text{judge}$ to provide reward signals for long-context-based model responses. As illustrated in Figure~\\ref{fig:longreward}, given a long-context prompt $x$ (typically consisting of a lengthy context $c$ and a query $q$) and a response $y$, LongReward evaluates the response based on four dimensions that are valued by humans: helpfulness, logicality, faithfulness, and completeness. For each dimension, LongReward assigns a score ranging from 0 to 10, and the average of these scores constitutes the final reward. Below, we provide a detailed description of these four dimensions and their respective assessment methods. The detailed prompts are listed in Appendix~\\ref{appendix:prompts}.\n\n\\paragraph{Helpfulness.}\nWe define a model response as ``helpful'' if it is relevant and informative to the query, fulfilling the user's intended purpose and all the requirements. Since we treat faithfulness, i.e., the truthfulness of provided information, as a separate dimension instead of an aspect of helpfulness, the assessment of helpfulness can be directly based on the query and the response without referring to the lengthy context. Specifically, we first provide $M_\\text{judge}$ with detailed scoring principles and several examples with different helpfulness scores as references, then feed the query and the response into $M_\\text{judge}$ and ask it to rate helpfulness for the response. Similar to~\\citet{mt-bench}, we introduce Chain-of-Thought (CoT)~\\cite{cot}, i.e., requiring $M_\\text{judge}$ to generate an analysis before providing the final score, to augment both the score reliability and interoperability.\n\n\\paragraph{Logicality.}\nSince LLMs generate responses in an autoregressive way that lacks a rollback mechanism, it is difficult for them to fix errors once generated, even if the subsequent output and the conclusion are correct. This typically results in logical inconsistencies within their responses. Additionally, current LLMs often make mistakes in simple calculation and reasoning tasks. The focus of logicality assessment is to detect such logical errors in model responses, which is also independent of the context. Therefore, similar to the assessment of helpfulness, we utilize few-shot learning with CoT to enable $M_\\text{judge}$ to first find possible logical errors in the response and then rate its logicality.\n\n\\paragraph{Faithfulness.} Faithfulness measures the proportion of factual information in a model response that aligns with the context. Since it is challenging to directly find out all unfaithful information given the substantial context length, we follow the break-and-check idea of FactScore~\\cite{factscore}, requiring  $M_\\text{Judge}$ to first break the model response into a list of factual statements $\\mathcal{S}=\\{s_1, \\dots, s_n\\}$, and then judge whether each statement $s_i$ is supported by the most relevant context, which are top-$k$ 128-token chunks retrieved from the context taken $s_i$ as the query. Meanwhile, we make the following changes to better adapt to long-context scenarios as well as improve efficiency: (1) Current long-context models are prone to copy sentences from the context, so we break the model response into sentence-level factual statements instead of atomic statements to improve the retrieval recall and reduce the number of fact-checking; (2) Since most responses contain several ``functional sentences'' such as ``... has the following advantages:'' and ``In summary, ...'' that do not provide factual information or are conclusions or reasoning based on the previous response content, we require $M_\\text{judge}$ to omit these sentences when decomposing responses to improve assessment accuracy. (3) For each factual statement $s_i$, we consider three supporting levels: full support, partial support, and no support, and set the corresponding score $a_i$ to be 1, 0.5, and 0, respectively. The final faithfulness score is calculated as $(10\\cdot \\sum_{i=1}^n a_i) / n$.\n\n\\paragraph{Completeness.} The focus of completeness is to ascertain whether the response covers all question-relevant key points in the context and provides sufficient information and details to meet the user's needs. Considering LLMs are likely to ignore information in the middle or tail of the context when it is extremely long~\\cite{lostinmiddle, longcite}, we adopt a divide-and-conquer strategy for completeness assessment. Specifically, we first divide the context into coarse-grained chunks with a length of 4.096 tokens and ask $M_\\text{judge}$ to extract question-relevant information from each chunk. Next, we concatenate all the information and call $M_{judge}$ again to assess whether the response encompasses all important aspects without any apparent omissions, then assign a final score for completeness. The scoring process is also implemented with few-shot learning and CoT.\n\n\\subsection{Long-context DPO with LongReward}\n\\label{sec:longreward+dpo}\nNow that we have an automated approach to calculate rewards for long-context-based model responses, we can combine it with some RL algorithms to further enhance long-context SFT models. Here, we demonstrate how to combine LongReward with DPO, one of the most popular offline RL algorithms. The key to applying DPO for a given long-context SFT model $M_\\text{SFT}$ is to construct a long-context preference dataset, which can be done using LongReward and a set of long-context prompts.  \nThese prompts can be either from the SFT dataset or newly collected. For each prompt, we first sample $m$ candidate responses from $M_\\text{SFT}$ using simple temperature sampling with temperature 1.0. For each response, we then apply LongReward to obtain its reward. Following~\\cite{chatglm-rlhf}, we ultimately choose the responses with the highest and lowest reward as the preference pair for the prompt. Finally, we can fine-tune $M_\\text{SFT}$ with DPO to enhance its capacities. To further stabilize DPO training, we add an additional cross-entropy (CE) loss on the winning sequences as a regularization term, similar to~\\cite{pan2024}: \n\\begin{equation}\n\\mathcal{L}_\\text{CE}(\\pi_\\theta)= -\\mathbb{E}_{(x, y_w)\\sim\\mathcal{D}}[\\log(\\pi_\\theta(y_w|x))],\n\\end{equation}\n\n\\begin{equation}\n    \\mathcal{L}_\\text{merge} = \\mathcal{L}_\\text{DPO} + \\lambda \\cdot\\mathcal{L}_\\text{CE}\n\\end{equation}\nwhere $\\lambda$ denotes the scaling coefficient of CE loss.\\begin{table*}[t]\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{llccccc}\n\\toprule\nDataset                    & Task Type             & \\#Data & Avg Len & Language        & Metric          & Judge Model \\\\ \\midrule\n\\multicolumn{7}{l}{\\textit{\\textbf{Long-context Benchmark}}}                                                               \\\\\nLongBench-Chat             & Multi-Task            & 50         & 35,571  & English/Chinese & Point-wise Rate & GPT-4o      \\\\ \n\\multirow{3}{*}{LongBench} & Single-Doc QA          & 750        & 8,573   & English/Chinese & Point-wise Rate & GPT-4o      \\\\ \n                           & Multi-Doc QA           & 800        & 1,0255  & English/Chinese & Point-wise Rate & GPT-4o      \\\\\n                           & Summarization         & 800        & 9,210   & English/Chinese & Point-wise Rate & GPT-4o      \\\\ \\midrule\n\\multicolumn{7}{l}{\\textit{\\textbf{Short-context Benchmark}}}                                                              \\\\\nMT-Bench                   & Instruction Following & 80         & -       & English         & Point-wise Rate & GPT-4       \\\\ \nAlpacaEval2                & Instruction Following & 805        & -       & English         & LC Win Rate     & GPT-4-turbo \\\\ \\bottomrule\n\\end{tabular}\n}\n\\caption{Detailed statistics of benchmarks we used for evaluation. \"Avg Len\" refers to the average number of words/characters in the context of English/Chinese instances. ``LC Win Rate'' denotes length-controlled Win Rate~\\cite{alpacaeval2} against GPT-4-turbo.}\n\\label{tab:datasets} \n\\end{table*}\\begin{table*}[t]\n\\centering\n\\resizebox{0.9\\linewidth}{!}{\n\\begin{tabular}{llccccc}\n\\toprule\n                               &                                                &                                  & \\multicolumn{3}{c}{LongBench}                                                           &                             \\\\ \\cline{4-6}\n\\multirow{-2}{*}{Model}        & \\multirow{-2}{*}{Method}                       & \\multirow{-2}{*}{LongBench-Chat} & S-Doc QA                    & M-Doc QA                    & Summ                        & \\multirow{-2}{*}{Avg}       \\\\ \\midrule\n                               & {\\color[HTML]{8F959E} officially post-trained} & {\\color[HTML]{8F959E} 60.2}      & {\\color[HTML]{8F959E} 59.3} & {\\color[HTML]{8F959E} 42.9} & {\\color[HTML]{8F959E} 35.3} & {\\color[HTML]{8F959E} 49.4} \\\\\n                               & SFT                                            & 69.8                             & 66.1                        & 44.5                        & 39.6                        & 55.0                        \\\\\n                               & DPO w/ SRM                                     & 67.4                             & 65.0                        & 49.6                        & 42.7                        & 56.2                        \\\\\n                               & DPO w/ Contrast                                  & 70.6                             & \\textbf{67.8}               & 46.2                        & 40.3                        & 56.2                        \\\\ \n\\multirow{-5}{*}{Llama-3.1-8B} & DPO w/ LongReward                              & \\textbf{72.6}                    & \\textbf{67.8}               & \\textbf{55.8}               & \\textbf{43.2}               & \\textbf{59.9}               \\\\ \\midrule\n                               & {\\color[HTML]{8F959E} officially post-trained} & {\\color[HTML]{8F959E} 68.6}      & {\\color[HTML]{8F959E} 67.8} & {\\color[HTML]{8F959E} 56.9} & {\\color[HTML]{8F959E} 47.9} & {\\color[HTML]{8F959E} 60.3} \\\\\n                               & SFT                                            & 64.8                             & 68.4                        & 50.9                        & 42.1                        & 56.6                        \\\\\n                               & DPO w/ SRM                                     & 66.6                             & 67.5                        & 57.4                        & 48.2                        & 59.9                        \\\\\n                               & DPO w/ Contrast                                  & 68.2                             & 67.8                        & 58.0                        & 47.8                        & 60.5                        \\\\\n\\multirow{-5}{*}{GLM-4-9B}     & DPO w/ LongReward                              & \\textbf{69.2}                    & \\textbf{71.9}               & \\textbf{58.8}               & \\textbf{48.5}               & \\textbf{62.1}      \\\\ \\bottomrule        \n\\end{tabular}\n}\n\\caption{Results of automatic evaluation on long-context benchmarks rated by GPT-4o. \"S-Doc QA\", \"M-Doc QA\", and \"Summ\" denote Single-Doc QA, Multi-Doc QA, and Summarization, respectively.}\n\\label{tab:main_long}\n\\end{table*}\\section{Experiments}\n\n\\subsection{Experimental Setup}\nWe conduct experiments on two latest open-source base models, namely Llama-3.1-8B~\\cite{llama-3-1} and GLM-4-9B~\\cite{glm4}, which have been continually pre-trained on extensive long texts and support a context window of 128k tokens. We first supervisedly fine-tune these two models, then conduct DPO experiment with LongReward, as described in Sec.~\\ref{sec:longreward+dpo}. All models are trained using Megatron-LM library~\\cite{megatron-lm} on 4 nodes with 8$\\times$H800 GPUs. \n\n\\noindent\\textbf{Supervised Fine-tuning.} We adopt the method of~\\citet{longalign} to construct the long-context SFT dataset. Specifically, we collect 10k documents from the pre-training corpus of GLM-4~\\cite{glm4}, covering 9 varied domains. These documents are mainly in English and Chinese and their lengths range from  8k to 64k tokens. For each document, we employ GLM-4 to propose a QA pair via Self-Instruct~\\cite{self-instruct}, where different task type descriptions such as summarization and information extraction are incorporated into the prompts to guarantee the diversity of generated queries. Following~\\citet{longalign}, we mixed this dataset with 76k general SFT instances from ShareGPT~\\cite{vicuna2023} and fine-tune Llama-3.1-8B and GLM-4-9B in the mixed dataset for 1,800 steps (approximately 2 epochs), with a learning rate of 1e-5 and a batch size of 8.\n\n\\noindent\\textbf{Direct Preference Optimization.} We follow the pipeline described in Sec.~\\ref{sec:longreward+dpo} to construct the preference datasets, using prompts from the long-context SFT dataset and taking GLM-4 as $M_\\text{judge}$. For each prompt, we sample 10 candidate responses. We adopt Zhipu-Embedding-2\\footnote{https://www.bigmodel.cn/dev/api/vector/embedding-2} as the retriever for the assessment of faithfulness and retrieve top-5 context chunks for each factual statement. For DPO training, we set $\\beta$ and $\\lambda$ to be 0.15 and 0.1, respectively. We use a learning rate of 1e-6 and a batch size of 16, and train from the SFT checkpoints for around 400 to 800 steps. \n\n\\subsection{Baselines}\nBesides the SFT models, we consider the following long-context preference data generation policies as baselines:\n\n\\noindent\\textbf{Short-context reward model (SRM).} This policy utilizes the short-context reward model trained by~\\citet{chatglm-rlhf} to construct preference datasets, where we ignore the context and only feed the query and response into the reward model due to its limited context window.  \n\n\\noindent\\textbf{Contrast with larger models (Contrast).} This policy uses responses generated by GLM-4 and the SFT model as the winning and losing responses, respectively, based on the observation that larger LLMs typically generate better responses.\n\nIn addition, we also report the performance of officially post-trained models, i.e., Llama-3.1-8B-Instruct and GLM-4-9B-Chat, as references.\n\n\\subsection{Evaluation} \n\\noindent\\textbf{Long-context benchmarks.} We use two bilingual benchmarks LongBench-Chat~\\cite{longalign} and LongBench~\\cite{longbench} for the evaluation of long-context capacities, where the former is a small-scale dataset that evaluates models’ long-context alignment proficiency on 50 real-ward queries, and the latter is a comprehensive benchmark that tests general long-context understanding abilities. We conduct evaluations on three types of tasks within LongBench: Single-Doc QA, Multi-Doc QA, and Summarization (each task includes 4 sub-datasets), including 2,350 instances in total. Following~\\citet{longalign} and~\\cite{longcite}, we ask GPT-4o to automatically rate the models' responses based on the query and groundtruth answers via few-shot (for LongBench-Chat) or zero-shot prompting (for LongBench). In addition, we also conduct human evaluation in Sec.~\\ref{sec:main_long} and~\\ref{sec:human_agreement} to further validate the effectiveness of LongReward and its agreement with human preference.\n\n\\noindent\\textbf{Short-context benchmarks.} We select two short-context benchmarks MT-Bench~\\cite{mt-bench} and AlpacaEval2~\\cite{alpacaeval2} to investigate whether LongReward will influence models' ability to follow short instructions. MT-Bench  covers 8 categories with\n80 questions and uses GPT-4 to rate model responses on a scale of 1-10, while AlpacaEval2 consists of 805 questions from 5 datasets and employs GPT-4-turbo to measure models' length-controlled win rate against GPT-4-turbo. More detailed statistics of the evaluation datasets are listed in Table~\\ref{tab:datasets}. \n\n\\subsection{Results on Long-Context Benchmarks}\n\\label{sec:main_long}\n\nTable~\\ref{tab:main_long} presents the automatic evaluation results on LongBench-Chat and LongBench rated by GPT-4o, where our proposed LongReward method demonstrates superior performance compared to other baselines. Specifically, the DPO version of Llama-3.1-8B and GLM-4-9B using LongReward significantly outperforms their SFT counterparts across all long-context tasks, with an average performance improvement of 4.9\\% and 5.5\\%, respectively. Moreover, the performance of the DPO models with LongReward also surpasses the officially post-trained models by 10.5\\% and 1.8\\%.\n\n\\begin{table}[t]\n\\centering\n\\resizebox{0.85\\linewidth}{!}{\n\\begin{tabular}{lcc}\n\\toprule\nMethod                         & \\#Facts                                & FactScore                              \\\\ \\midrule\n\\textit{\\textbf{Llama-3.1-8B}} & \\multicolumn{1}{l}{\\textit{\\textbf{}}} & \\multicolumn{1}{l}{\\textit{\\textbf{}}} \\\\\nSFT                            & 21.76                                  & 91.94                                  \\\\\nDPO w/ LongReward              & 32.86                                  & \\textbf{92.85}                         \\\\ \\midrule\n\\textit{\\textbf{GLM-4-9B}}     & \\multicolumn{1}{l}{\\textit{\\textbf{}}} & \\multicolumn{1}{l}{\\textit{\\textbf{}}} \\\\\nSFT                            & 18.41                                  & 91.43                                  \\\\\nDPO w/ LongReward              & 28.05                                  & \\textbf{93.62}                         \\\\ \\bottomrule\n\\end{tabular}\n}\n\\caption{FactScore of the SFT and LongReward+DPO versions of models on 260 randomly sampled questions from LongBench-Chat and LongBench, taking GPT-4o-mini as the judge. \"\\#Facts\" and \"FactScore\" denote the average number of atomic facts and the ratio of supported facts per response, respectively.}\n\\label{tab:factscore}\n\\end{table}\\begin{table}[t]\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{l|ccc|c}\n\\toprule\n& Win  & Tie  & Loss & $\\Delta$(Win-Loss) \\\\ \\midrule\nHelpfulness               & 0.14 & 0.84 & 0.02 & 0.12       \\\\\nLogicality                & 0.14 & 0.86 & 0.00 & 0.14       \\\\\nFaithfulness              & 0.32 & 0.64 & 0.04 & 0.28       \\\\\nCompleteness              & 0.26 & 0.64 & 0.10 & 0.16       \\\\ \\midrule\nOverall                   & 0.54 & 0.38 & 0.08 & 0.46       \\\\ \\bottomrule\n\\end{tabular}\n}\n\\caption{Results of human evaluation of LongReward+DPO version of Llama-3.1-8B on LongBench-Chat against the SFT baseline. We report the proportion of wins, ties, and losses of the DPO model on each dimension.}\n\\label{tab:human_eval_long}\n\\end{table}\nIn addition, we use FactScore~\\cite{factscore} and 260 questions randomly sampled from LongBench-Chat and LongBench (20 questions from each sub-dataset) to automatically evaluate models' faithfulness. Specifically, we employ GPT-4o-mini to first break the model response into atomic facts and then judge whether each fact is supported by the retrieved context chunks. The results in Table~\\ref{tab:factscore} show that the DPO models using LongReward achieve higher FactScore (i.e., the ratio of supported facts) than the SFT baseline, demonstrating the effect of LongReward in improving faithfulness as well as reducing hallucinations for long-context LLMs. Meanwhile, the responses of DPO models typically contain more atomic facts, implying that their responses are more detailed and comprehensive.\n\nBesides automatic evaluation, we also conduct a human evaluation on LongBench-Chat to further validate the effectiveness of LongReward in improving LLMs' long-context capacities. Specifically, we anonymize and randomly shuffle the responses generated by the SFT and LongReward+DPO versions of Llama-3.1-8B, and ask two of the authors who are familiar with LongBench-Chat to manually judge which response is better. The annotators are required to first rate the response from four dimensions (i.e., helpfulness, logicality, faithfulness, and completeness), following the same scoring principles as LongReward, and then give the final comparison judgment. As shown in Table~\\ref{tab:human_eval_long}, the DPO model using Longreward maintains a distinct advantage over the SFT baseline, with an overall win-rate of 54\\% compared to 8\\%. Moreover, the DPO model obtains more wins across all four dimensions, indicating that the multi-dimensional scoring strategy of LongReward effectively helps enhance the model's long-context capability from multiple aspects. Detailed cases can be found in Appendix~\\ref{appendix:cases}.\n\n\\begin{table}[t]\n\\centering\n\\resizebox{0.95\\linewidth}{!}{\n\\begin{tabular}{lcc}\n\\toprule\nMethod                                         & MT-Bench                    & AlpacaEval2                 \\\\ \\midrule\n\\multicolumn{3}{l}{\\textit{\\textbf{Llama-3.1-8B}}}                                                         \\\\\n{\\color[HTML]{8F959E} officially post-trained} & {\\color[HTML]{8F959E} 8.13} & {\\color[HTML]{8F959E} 22.9} \\\\\nSFT                                            & 7.12                        & 12.4                        \\\\\nDPO w/ SRM                                     & \\textbf{7.58}               & 13.7                        \\\\\nDPO w/ Contrast                                  & \\textbf{7.58}               & 13.8                        \\\\\nDPO w/ LongReward                              & 7.24                        & \\textbf{14.2}               \\\\ \\midrule\n\\multicolumn{3}{l}{\\textit{\\textbf{GLM-4-9B}}}                                                             \\\\\n{\\color[HTML]{8F959E} officially post-trained} & {\\color[HTML]{8F959E} 8.09} & {\\color[HTML]{8F959E} 22.4} \\\\\nSFT                                            & 7.37                        & 12.5                        \\\\\nDPO w/ SRM                                     & 7.50                        & 14.2                        \\\\\nDPO w/ Contrast                                  & 7.54                        & 14.5                        \\\\\nDPO w/ LongReward                              & \\textbf{7.58}               & \\textbf{15.4}              \\\\ \\bottomrule\n\\end{tabular}\n}\n\\caption{Performance of different models on short-context instruction-following benchmarks.}\n\\label{tab:main_short}\n\\end{table}\\subsection{Results on Short-Context Benchmarks}\nTable~\\ref{tab:main_short} lists the evaluation results on MT-Bench and AlpacaEval2. Due to the simplicity of the general SFT data (i.e., ShareGPT) we used, a performance gap exists between our trained and officially post-trained models. Nevertheless, we surprisedly find that DPO on long-context preference datasets also benefits models' ability to follow short instructions. Meanwhile, the DPO models using LongReward typically achieve better performance than other baselines, implying that the preferred values learned from LongReward can be well generalized to short-context scenarios.\n\n\\begin{table*}[t]\n\\centering\n\\resizebox{0.8\\linewidth}{!}{\n\\begin{tabular}{ll|cc|cc}\n\\toprule\n\\multirow{2}{*}{Model}        & \\multirow{2}{*}{Preference Data} & \\multicolumn{2}{c|}{Long Benchmark} & \\multicolumn{2}{c}{Short Benchmark} \\\\\n                              &                                     & LongBench-Chat        & LongBench           & MT-Bench              & AlpacaEval2           \\\\ \\midrule\n\\multirow{3}{*}{Llama-3.1-8B} & Short                               & 70.6                  & 54.5                & {\\ul 7.48}            & \\textbf{15.8}         \\\\\n                              & Long                                & {\\ul 72.6}            & {\\ul 55.6}          & 7.24                  & 14.2                  \\\\\n                              & Short + Long                        & \\textbf{73.0}         & \\textbf{57.3}       & \\textbf{7.51}         & {\\ul 14.9}            \\\\ \\midrule\n\\multirow{3}{*}{GLM-4-9B}     & Short                               & 67.0                  & 56.3                & \\textbf{7.62}         & 14.7                  \\\\\n                              & Long                                & {\\ul 69.2}            & \\textbf{59.7}       & 7.58                  & {\\ul 15.2}            \\\\\n                              & Short + Long                        & \\textbf{70.2}         & {\\ul 58.7}          & {\\ul 7.61}            & \\textbf{15.4} \\\\ \\bottomrule       \n\\end{tabular}\n}\n\\caption{Performance of DPO models using different preference datasets, where the short- and long-context preference data are constructed using short reward model trained by~\\citet{chatglm-rlhf} and LongReward, respectively.}\n\\label{tab:combine_short_dpo}\n\\end{table*}\\begin{table}[t]\n\\centering\n\\resizebox{0.65\\linewidth}{!}{\n\\begin{tabular}{lc}\n\\toprule\nMethod           & Accuracy   \\\\ \\midrule\nSRM              & 0.583 \\\\\nPaired comparison           & 0.571 \\\\\nLongReward       & \\textbf{0.662} \\\\\n\\ \\ \\ \\ w/o Helpfulness  & 0.631 \\\\\n\\ \\ \\ \\ w/o Logicality   & 0.623 \\\\\n\\ \\ \\ \\ w/o Faithfulness & 0.578 \\\\\n\\ \\ \\ \\ w/o Completeness & 0.578 \\\\ \\bottomrule\n\\end{tabular}\n}\n\\caption{Alignment of different reward methods with human preference on a set of 464 manually annotated long-context preference pairs, where the queries and responses are from LongBench-Chat and the SFT checkpoint of Llama-3.1-8B, respectively. }\n\\label{tab:reward_model}\n\\end{table}\\subsection{Combination with Short-context DPO}\nWe also explore the compatibility of long-context DPO using LongReward and normal short-context DPO using SRM. Specifically, we utilize the SRM trained by~\\citet{chatglm-rlhf} and prompts from our general SFT dataset to construct short-context preference datasets, following a similar sample-and-rate pipeline as described in Sec.~\\ref{sec:longreward+dpo}. Then we train the SFT checkpoints with DPO on the mix of long- and short-context preference data. The evaluation results in Table~\\ref{tab:combine_short_dpo} show that DPO on the mixed dataset well aggregates the advantages of individual short- and long-context DPO: it significantly improves models' long-context performance as long-context DPO and also achieves comparable short-instruction-following performance with short-context DPO, indicating that LongReward can be well incorporated into conventional DPO pipeline to simultaneously enhance long- and short-context capacities.\n\n\\subsection{Alignment with Human Preference}\n\\label{sec:human_agreement}\nWe conduct an experiment to evaluate the alignment of different reward methods with human preference in long-context scenarios. Specifically, we construct 464 preference pairs by manually rating the sampled responses from the SFT checkpoint of Llama-3.1-8B on LongBench-Chat. For each annotated pair,  we employ different reward methods to predict which response is better, and then compute their accuracy by taking human preference as the golden label.\n\nBesides the SRM trained by~\\citet{chatglm-rlhf} (which ignores the context when predicting rewards) and our proposed LongReward, we also consider paired comparison as a baseline, which asks an LLM to directly judge which response is better and is widely used in short-context RLAIF~\\cite{constitutionalAI, rlaif}. Specifically, given a preference pair, we input the two responses along with the four-dimension-based principles, query, and context into GLM-4, and require it to first give an analysis and then choose a better response. \n\nWe present the accuracy of different reward methods in Table~\\ref{tab:reward_model}. As we can observe, LongReward achieves the highest 66.2\\% accuracy, showing a better alignment with human than other baselines. This result is consistent with the observation of~\\citet{chatglm-rlhf} that a reward method can guide the training of RL with approximately 65\\% accuracy in mirroring human judgment. In addition, the ablation results in Table~\\ref{tab:reward_model} also indicate that each dimension in LongReward is important for aligning with human values. On the other hand, we find that paired comparison even performs worse than SRM, indicating that current LLMs struggle to directly discern the quality difference between similar long-context-based responses and also demonstrating the necessity of using LongReward.\n\n\\section{Conclusion}\nIn this work, we propose LongReward, a novel method that utilizes an off-the-shelf LLM to provide reliable rewards for model responses in long-context scenarios, thereby enabling the employment of RL algorithms for further enhancing the capacities of long-context LLMs. Our DPO experiments indicate that LongReward not only significantly improves models' long-context performance but also enhances their ability to follow short instructions. Meanwhile, we also find that long-context DPO using LongReward can be well combined with the standard short-context DPO without hurting either method's performance.\n\n\\section{Limitations}\nWe discuss several limitations of our work in this section: (1) LongReward relies on a well-aligned LLM such as GLM-4 to provide scores for each dimension and costs tens of API calls for each QA instance. In the future, we will try to train a smaller long-context reward model using our constructed preference datasets to enable faster and cheaper reward calculation. (2) Due to limited computational resources, we only conduct experiments on 10B level models with a maximum training length of 64k. We hope to explore long-context alignment on longer sequences and larger-scale models if there are more available resources. (3) From a data perspective, we primarily focus on user-intensive long-context scenarios like long document QA and summarization. Generalizing LongReard to other more advanced long instruction tasks such as life-long dialogues and long-history agent tasks is also a promising direction. \n\n\\section{Ethical Considerations}\nThough LongReward can effectively improve the faithfulness of long-context LLMs, it may still hallucinate, especially when the query involves common knowledge that is not presented in the context. Hence additional care and protective measures should be taken if our method is deployed in user-facing applications. \n\nWe have already desensitized the training data. All the evaluation datasets used in this work are publicly published with permissible licenses. \n\n\\newpage\n\\onecolumn\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2401.18058v1.tex",
        "arXiv-2404.00934v2.tex",
        "arXiv-2410.21252v1.tex"
    ],
    "group_id": "group_43",
    "response": "### Title: Enhancing Large Language Models (LLMs) for Long Context Handling and Alignment with Human Preferences\n\n### Introduction\n\nThe field of large language models (LLMs) has seen remarkable advancements in recent years, with models like GPT-4, Llama, and ChatGLM demonstrating unprecedented capabilities in language understanding and generation. These models are foundational in various applications, including summarization, question answering, and complex agent scenarios. However, extending their context windows to handle long contexts and aligning them with human preferences present unique challenges. The main obstacle in extending context windows is the necessity for fine-tuning on long sequences, which requires extensive datasets and efficient training methods. Additionally, aligning LLMs with human preferences involves collecting reliable human feedback, which can be costly and time-consuming. To address these challenges, researchers have proposed several methods, including supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). Despite these efforts, the performance of LLMs in long context scenarios still lags behind, and the alignment with human preferences remains an open area for improvement.\n\nThis summary focuses on three recent research papers that tackle these challenges. The first paper, LongAlign, introduces a comprehensive approach for handling long context alignment, encompassing data construction, training strategies, and evaluation benchmarks. The second paper, ChatGLM-RLHF, presents a reinforcement learning pipeline designed to enhance ChatGLM's alignment with human preferences. The third paper, LongReward, proposes a novel method to provide reliable rewards for long-context-based model responses using an off-the-shelf LLM, thereby improving the alignment of LLMs with human preferences in long context scenarios. Each paper addresses different aspects of the challenges faced in long context handling and alignment, contributing to the overall progress in the field.\n\n### LongAlign: A Recipe for Long Context Alignment Encompassing Data, Training, and Evaluation\n\n#### Overview\n\nThe paper introduces LongAlign, a comprehensive method for fine-tuning LLMs to handle long contexts effectively. It focuses on three main aspects: data construction, training strategies, and evaluation benchmarks. The authors aim to address the challenges of obtaining high-quality long instruction data, ensuring efficient training, and developing robust evaluation methods for long context tasks.\n\n#### Data Construction\n\nLongAlign constructs a diverse long instruction-following dataset using Self-Instruct. The dataset covers a broad range of tasks from various sources, including books, encyclopedias, academic papers, and codes. The authors generate 10k instruction data pieces, each ranging from 8k to 64k tokens, using Claude 2.1. To foster diversity, they incorporate task type descriptions into the prompts, such as queries for summaries, information extraction, reasoning, etc. The dataset is designed to ensure that the model can handle both long and short texts effectively.\n\n#### Training Strategies\n\nTo address the inefficiency in training due to varied length distributions, LongAlign adopts two strategies: packing and sorted batching. Packing involves concatenating sequences of varying lengths until reaching the maximum length, which helps minimize idle times within each batch. However, this strategy introduces a bias towards longer sequences and sequences containing more target tokens. To mitigate this bias, the authors propose a loss weighting method that scales the loss by the number of target tokens in each sequence. Sorted batching, on the other hand, groups sequences of similar lengths to reduce intra-batch idle time, although it introduces a bias in data distribution across batches.\n\n#### Evaluation Benchmark\n\nThe authors introduce LongBench-Chat, a benchmark for evaluating LLMs' instruction-following capabilities on queries of 10k-100k in length. This benchmark includes 50 long context real-world queries, annotated by Ph.D. students, covering diverse aspects such as reasoning, coding, summarization, and multilingual translation. GPT-4 is used to score the model's responses based on human-annotated groundtruths and few-shot scoring examples. The authors validate the reliability of GPT-4 as an evaluator through human evaluation studies, demonstrating that GPT-4's scoring aligns well with human judgments.\n\n### ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback\n\n#### Overview\n\nChatGLM-RLHF is a reinforcement learning pipeline designed to enhance ChatGLM's alignment with human preferences. The pipeline includes three main components: data collection, reward model training, and policy model optimization. The authors focus on improving the alignment of LLMs with human preferences through RLHF, addressing challenges such as reward variance reduction, model parallelism, and avoiding catastrophic forgetting.\n\n#### Data Collection\n\nThe data collection process involves setting up a system to gather human preference annotations, employing a pairwise comparison mechanism. Each annotator is asked to select a preferred response from two options generated by the SFT model, given a human instruction. The authors establish criteria and reference dimensions for annotation, including helpfulness, harmlessness, and fluency, to ensure consistency and reliability among annotators. They also implement a post-filtering process to remove undesirable patterns and biases from the preference data.\n\n#### Reward Model Training\n\nThe reward model is trained on the collected preference dataset to predict human preferences. To mitigate biases such as a preference for longer responses, the authors develop a debiasing method named `Bucket-Based Length Balancing'. This method calculates the length difference between the two responses in each preference pair and balances the number of examples where the better/worse response is longer. Additionally, they introduce a regularization loss to stabilize the training process by constraining the volatility of the score distribution.\n\n#### Policy Model Training\n\nThe policy model is optimized using reinforcement learning algorithms like Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO). PPO involves online response sampling and training multiple LLMs, while DPO simplifies the process by directly learning from preference pairs. The authors incorporate the SFT loss during RLHF training to reduce capability forgetting and maintain the model's proficiency in handling short instructions. They also implement model parallelism with fused gradient-descent to ensure efficient training of large-scale models.\n\n### LongReward: Improving Long-context Large Language Models with AI Feedback\n\n#### Overview\n\nLongReward is a method that utilizes an off-the-shelf LLM to provide reliable rewards for long-context-based model responses. The authors aim to address the issue of compromised quality in synthesized SFT data for long-context models by employing an LLM as a judge to evaluate responses from four human-valued dimensions: helpfulness, logicality, faithfulness, and completeness. By combining LongReward with DPO, they demonstrate significant improvements in long-context performance and maintain the model's ability to follow short instructions.\n\n#### Reward Calculation\n\nLongReward evaluates a model response based on four dimensions: helpfulness, logicality, faithfulness, and completeness. For helpfulness and logicality, the LLM directly assigns scores using few-shot learning. For faithfulness, the LLM breaks the response into factual statements and checks their support in the context. For completeness, the LLM extracts question-relevant information from context chunks and assesses whether the response covers all key points. The final reward is the average score across these four dimensions.\n\n#### Long-context DPO with LongReward\n\nThe authors apply LongReward to construct preference datasets for DPO training. They sample candidate responses from the SFT model and use LongReward to obtain their rewards. The winning and losing responses are chosen based on these rewards, and the policy model is fine-tuned to maximize the difference in likelihood of preference pairs. The authors also incorporate an additional cross-entropy loss on winning sequences to stabilize training and avoid catastrophic forgetting.\n\n#### Evaluation\n\nThe authors evaluate the performance of DPO models using LongReward on long-context benchmarks like LongBench-Chat and LongBench. They also conduct human evaluations to validate the effectiveness of LongReward in aligning with human preferences. The results show that LongReward significantly improves the performance of LLMs on long-context tasks, with an average improvement of 4.9\\% for Llama-3.1-8B and 5.5\\% for GLM-4-9B. Additionally, LongReward enhances the model's ability to follow short instructions without compromising performance.\n\n### Commonalities and Innovations\n\nAll three papers address the challenges of enhancing LLMs' capabilities in handling long contexts and aligning with human preferences. LongAlign focuses on constructing a diverse long instruction dataset and developing efficient training methods to minimize idle times and balance loss contributions. ChatGLM-RLHF introduces a robust RLHF pipeline for large-scale models, emphasizing the importance of reliable human preference data and efficient training strategies. LongReward proposes a novel method to provide reliable rewards for long-context-based model responses using an off-the-shelf LLM, thereby improving the alignment of LLMs with human preferences.\n\n#### Innovations\n\n- **LongAlign** introduces a loss weighting method to balance the contributions to the loss across different sequences during packing training, and a sorted batching method to group sequences of similar lengths.\n- **ChatGLM-RLHF** develops techniques to mitigate reward bias and variance, implement model parallelism, and avoid catastrophic forgetting.\n- **LongReward** utilizes an off-the-shelf LLM to provide multi-dimensional rewards for long-context responses, thereby enhancing the alignment of LLMs with human preferences.\n\n### Comparison of Results\n\nThe results of the three papers indicate significant improvements in long-context performance and alignment with human preferences. LongAlign demonstrates up to 30\\% improvement in long context tasks while maintaining proficiency in short tasks. ChatGLM-RLHF shows an average of 15\\% more wins against the SFT baseline in alignment tasks, with notable improvements in helpfulness, safety, and fluency. LongReward achieves an average improvement of 4.9\\% for Llama-3.1-8B and 5.5\\% for GLM-4-9B on long-context tasks, surpassing all baseline methods. Moreover, LongReward enhances the model's ability to follow short instructions without hurting performance.\n\n### Conclusion\n\nThe main findings of these papers highlight the importance of high-quality long instruction data, efficient training methods, and robust evaluation benchmarks in enhancing LLMs' long context handling and alignment with human preferences. LongAlign emphasizes the role of data diversity and efficient training strategies, ChatGLM-RLHF focuses on mitigating reward biases and implementing scalable training frameworks, and LongReward introduces a novel method for providing reliable rewards for long-context responses. These methods collectively contribute to the advancement of LLMs in long context scenarios and aligning them with human preferences.\n\n#### Future Research Directions\n\nFuture research could explore the integration of these methods to further enhance LLMs' capabilities in long context scenarios. Additionally, training smaller long-context reward models and generalizing LongReward to other advanced long instruction tasks, such as life-long dialogues and long-history agent tasks, could be promising directions. Improving the efficiency and reliability of reward calculation, especially for large-scale models, remains a critical area for future work."
}