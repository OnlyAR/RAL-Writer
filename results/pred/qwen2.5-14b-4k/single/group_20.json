{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{OntoProtein: Protein Pretraining With Gene Ontology Embedding}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nSelf-supervised protein language models have proved their effectiveness in learning the proteins representations. With the increasing computational power, current protein language models pre-trained with millions of diverse sequences can advance the parameter scale from million-level to billion-level and achieve remarkable improvement. However, those prevailing approaches rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better protein representations. We argue that informative biology knowledge in KGs can enhance protein representation with external knowledge. In this work, we propose \\textbf{OntoProtein}, the first general framework that makes use of structure in GO (Gene Ontology) into protein pre-training models. We construct a novel large-scale knowledge graph that consists of GO and its related proteins, and gene annotation texts or protein sequences describe all nodes in the graph. We propose novel contrastive learning with knowledge-aware negative sampling to jointly optimize the knowledge graph and protein embedding during pre-training.  Experimental results show that OntoProtein can surpass state-of-the-art methods with pre-trained protein language models in TAPE benchmark and yield better performance compared with baselines in protein-protein interaction and protein function prediction\\footnote{Code and datasets are available in \\url{https://github.com/zjunlp/OntoProtein}.}.\n\\end{abstract}\n\n\\section{Introduction}\n\nProtein science, the fundamental macromolecules governing biology and life itself, has led to remarkable advances in understanding the disease therapies and human health (\\cite{DBLP:conf/iclr/VigMVXSR21}). \nAs a sequence of amino acids, protein can be viewed precisely as a language, indicating that they may be modeled using neural networks that have been developed for natural language processing (NLP).\nRecent self-supervised pre-trained protein language models (PLMs) such as ESM (\\cite{DBLP:conf/iclr/RaoMSOR21}), ProteinBERT (\\cite{brandes2021proteinbert}), ProtTrans (\\cite{DBLP:journals/corr/abs-2007-06225}) which can learn powerful protein representations, have achieved promising results in understanding the structure and functionality of the protein. \nYet existing PLMs for protein representation learning generally cannot sufficiently capture the biology factual knowledge, which is crucial for many protein tasks but is usually sparse and has diverse and complex forms in sequence.\n\nBy contrast, knowledge graphs (KGs) from gene ontology\\footnote{\\url{http://geneontology.org/}} contain extensive biology structural facts, and knowledge embedding (KE) approaches (\\cite{DBLP:conf/nips/BordesUGWY13}, \\cite{DBLP:conf/acl/ZhengWCYZZZQMZ20}) can efficiently embed them into continuous vectors of entities and relations. \nFor example, as shown in Figure \\ref{intro}, without knowing \\emph{PEX5} has specific biological processes and cellular components, it is challenging to recognize its interaction with other proteins. \nFurthermore, since \\emph{protein's shape determines its function}, it is more convenient for models to identify protein's functions with the prior knowledge of protein functions having similar shapes.\nHence, considering rich knowledge can lead to better protein representation and benefits various biology applications, e.g., protein contact prediction, protein function prediction, and protein-protein interaction prediction. \nHowever, different from knowledge-enhanced approaches in NLP {\\color{highlight} (\\cite{DBLP:conf/acl/ZhangHLJSL19}, \\cite{DBLP:journals/tacl/WangGZZLLT21}, \\cite{DBLP:conf/acl/WangTDWHJCJZ21}) }, protein sequence and gene ontology are two different types of data. \nNote that protein sequence is composed of amino acids while gene ontology is a knowledge graph with text description; thus, severe issues of structured knowledge encoding and heterogeneous information fusion remain. \n\nIn this paper, we take the first to propose protein pre-training with gene ontology embedding (\\textbf{OntoProtein}), which is the first general framework to integrate external knowledge graphs into protein pre-training.  \nWe propose a  hybrid encoder to represent language text and protein sequence and introduce contrastive learning with knowledge-aware negative sampling to jointly optimize the knowledge graph and the protein sequence embedding during pre-training. \nFor the KE objective, we encode the node descriptions (go annotations) as their corresponding entity embeddings and then optimize them following vanilla KE approaches (\\cite{DBLP:conf/nips/BordesUGWY13}). \nWe further leverage gene ontology of molecular function, cellular component, and biological process and introduce a knowledge-aware negative sampling method for the KE objective. \nFor the MLM {\\color{highlight} (Mask Language Modeling)} objective, we follow the approach of existing protein pre-training approaches (\\cite{DBLP:conf/iclr/RaoMSOR21}). \nOntoProtein has the following strengths:\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[scale=0.36]{figs/case_first_new_change.pdf}\n\\caption{\n{\\color{highlight}\n\\textbf{Left}: A protein example with biology knowledge (molecular function, biological process and cellular component): $K^{+}$ (potassium ion) Cyclic nucleotide-gated cation channel protein. \\textbf{Right}: The corresponding sub-graph regarding $K^{+}$ carrier proteins in \\textbf{ProteinKG25}. \\textbf{Yellow} nodes are protein sequences and \\textbf{blue} nodes are GO (Gene Ontology) entities with biological descriptions.\n}\n}\n\\label{intro}\n\\end{figure*}\n\n(1) OntoProtein inherits the strong ability of protein understanding from PLMs with the MLM object.\n(2) OntoProtein can integrate biology knowledge into protein representation with the supervision from KG by the KE object.\n(3) OntoProtein constitutes a model-agnostic method and is readily pluggable into a wide range of protein tasks without additional inference overhead since we do not modify model architecture but add new training objectives. \n\nFor pre-training and evaluating OntoProtein, we need a knowledge graph with large-scale biology knowledge facts aligned with protein sequences.\nTherefore, we construct \\textbf{ProteinKG25}, which contains about 612,483 entities, 4,990,097 triples, and aligned node descriptions from GO annotations. \nTo the best of our knowledge, it is the first large-scale KG dataset to facilitate protein pre-training.\nWe deliver data splits for both the inductive and the transductive settings to promote future research. \n\nTo summarize, our contribution is three-fold:\n(1) We propose OntoProtein, the first knowledge-enhanced protein pre-training approach that brings promising improvements to a wide range of protein tasks. \n(2) By contrastive learning with knowledge-aware sampling to jointly optimize knowledge and protein embedding, OntoProtein shows its effectiveness in widespread downstream tasks, including protein function prediction, protein-protein interaction prediction, contact prediction, and so on. \n(3) We construct and release the ProteinKG25, a novel large-scale KG dataset, promoting the research on protein language pre-training. \n(4) We conduct extensive experiments in widespread protein tasks, including TAPE benchmark, protein-protein interaction prediction, and protein function prediction, which demonstrate the effectiveness of our proposed approach.  \n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[scale=0.43]{figs/model_new.pdf}\n\\caption{Overview of our proposed OntoProtein, which jointly optimize knowledge graph embedding and masked protein model (Best viewed in color.).\n}\n\\label{arc}\n\\end{figure*}\n\n\\section{Methodologies}\nWe begin to introduce our approach of protein pre-training with ontology embedding (OntoProtein), as shown in Figure \\ref{arc}.\nOntoProtein incorporates external knowledge from Gene Ontology (Go) into language representations by jointly optimizing two objectives. \nWe will first introduce the hybrid encoder, masked protein modeling, and knowledge encoder, and then we will present the details of contrastive learning with knowledge-aware negative sampling. \nFinally, we will illustrate the overall pre-training objects. \n\n\\subsection{Hybrid Encoder}\nWe first introduce the hybrid encoder to represent protein and GO knowledge. \nFor the protein encoder, we use the pre-trained ProtBert from \\cite{DBLP:journals/corr/abs-2007-06225}.\nProtBert is pre-trained using the BERT architecture with UniRef100 datasets.\nCompared to BERT \\cite{DBLP:conf/naacl/DevlinCLT19}, ProtBert encodes amino acid sequences into token level or sentence level representations, which can be used for downstream protein tasks such as contacts prediction tasks.\nThe encoder  takes a protein sequence of $N$ tokens $(x_1,...,x_N)$ as inputs, and computes contextualized amnio acid representation $H_{Protein}^{i}$ and sequence representation $H_{Protein}$ \n{\\color{highlight}via \\textit{mean pooling}. \nTo bridge the gap between text and protein, we utilize affine transformation (an extra linear layer) to project those representation to the same space.}\nWe will discuss details of learning protein representation in Section \\hyperref[sec:MLM]{Mask Protein Modeling}.\n\nFor the Go encoder, we leverage BERT (\\cite{DBLP:conf/naacl/DevlinCLT19}), a Transformer (\\cite{DBLP:conf/nips/VaswaniSPUJGKP17}) based text encoder for biological  descriptions in Gene Ontology entities.\nSpecifically, we utilize the pre-trained language model from (\\cite{pubmedbert})\\footnote{\\url{https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext}}.\nThe encoder takes a sequence of $N$ tokens $(x_1,...,x_N)$ as inputs, and computes  Go representations $H_{GO}\\in R^{N\\times d}$  by averaging all the token embeddings.\n\nSince the relations in Gene Ontology are important for representing the knowledge of biology features, thus, we utilize a relation encoder with the random initialization, and those embeddings of relations will be optimized and updated during pre-training.\n\n \n\\subsection{Knowledge Embedding}\n \\label{sec:KE}\n {\\color{highlight} We leverage the knowledge embedding (KE) objective to obtain representations in the pre-training process since Gene Ontology is actually a factual knowledge graph.}\nSimilar to \\cite{DBLP:conf/nips/BordesUGWY13}, we use distributed representations to encode entities and relations. \nThe knowledge graph here consists of lots of triples to describe relational facts.\nWe define a triplet as $(h, r, t)$, where $h$ and $t$ are head and tail entities, $r$ is the relation whose type usually is pre-defined in the schema\\footnote{The schema of the knowledge graph can be found in Appendix \\ref{app:dataset}}.\nNote that there are \\textbf{two different types of nodes} $e_{GO}$ and $e_{protein}$ in our knowledge graph. \n$e_{GO}$ is denoted as nodes that exist in the gene ontology, such as molecular function or cellular component nodes, and $e_{GO}$ can be described by annotation texts.\n$e_{protein}$ is the protein node that links to the gene ontology, and we also represent $e_{protein}$ with amnio acids sequences.\nConcretely, the triplets in this knowledge graph can be divided into two groups, $triple_{GO2GO}$ and $triple_{Protein2GO}$.\nTo integrate multi-modal descriptions into the same semantic space and address the heterogeneous information fusion issue, we utilize hybrid encoders introduced in the previous Section.\nNote that protein encoder and GO encoder represent protein sequence and GO annotations separately.\n\n\\subsection{Masked Protein Modeling}\n\\label{sec:MLM}\nWe use masked protein modeling to optimize protein representations.\nThe masked protein modeling is similar to masked language modeling (MLM).\nDuring model pre-training, we use a 15\\% probability to mask each token (amino acid) and leverage a cross-entropy loss $\\ell_{MLM}$ to estimate these masked tokens.\nWe initialize our model with the pre-trained model of ProtBert and regard $\\ell_{MLM}$ as one of the overall objectives of OntoProtein by jointly training KE (knowledge embedding) and MLM.\nOur approach is model-agnostic, and other pre-trained models can also be leveraged.\n\n\\subsection{Contrastive Learning with Knowledge-aware Negative Sampling}\n\nKnowledge embedding (KE) is to learn low-dimensional representations for entities and relations, and contrastive estimation represents a scalable and effective method for inferring connectivity patterns. \nNote that a crucial aspect of contrastive learning approaches is the choice of corruption distribution that generates hard negative samples, which force the embedding model to learn discriminative representations and find critical characteristics of observed data.\nHowever, previous approaches either employ too simple corruption distributions, i.e., uniform, yielding easy uninformative negatives, or sophisticated adversarial distributions with challenging optimization schemes.\nThus, in this paper, we propose contrastive learning with knowledge-aware negative sampling, an inexpensive negative sampling strategy that utilizes the rich GO knowledge to sample negative samples.\nFormally, the KE objective can be defined as: \n\\begin{equation}\n\\label{eq:1}\n{\\color{highlight} \n\\ell_{KE}=-\\log{\\sigma (\\gamma-d(h,t))} - \\sum_{i=1}^{n}{\\frac{1}{n}\\log{\\sigma(d(h_i', t_i')-\\gamma)} } \n}\n\\end{equation}\n\n$(h_i', t_i')$ is the negative sample, in which head or tail entities are random sampled to construct the corrupt triples.\n$n$ is the number of negative samples, $\\sigma$ is the sigmoid function, and $\\gamma$ means the margin.\n$d$ is the scoring function, and we use TransE (\\cite{DBLP:conf/nips/BordesUGWY13}) for simplicity, where \n\\begin{equation}\n{\\color{highlight} \nd_r(h, t) = \\left \\| h + r - t \\right \\| \n}\n\\end{equation}\n\nSpecifically, we define triple sets and entity sets as $T$ and $E$, all triplets are divided into two groups.\nIf the head entity is protein node and the tail entity is GO node, we denote the triple as $T_{protein-GO}$.\nSimilarly, if head and tail entities are both GO nodes, we denote them as $T_{GO-GO}$.\nAs Gene Ontology describes the knowledge of the biological domain concerning three aspects, all entities in Gene Ontology belong to MFO (Molecular Function),  CCO (Cellular Component), or BPO (Biological Process).\n\nTo avoid plain negative samples, for those $T_{GO-GO}$ triples, we sample triples by replacing entities with the same aspect (MFO, CCO, BPO)\\footnote{For $T_{protein-GO}$ triples, it is also intuitive to replace the proteins with their homologous proteins to generate hard negative triples, and we leave this for future works.}. \nFinally, we define the negative triple sets $T'$ and {\\color{highlight} positive triple as $(h, r, t)$, the negative sampling process can be described as follows}:\n{\\color{highlight}\n\\begin{equation}\n\\begin{aligned}\n&T_{GO-GO (h,r,t)}^{'}=\\{(h',r,t)\\mid h'\\in E', h\\in E' \\} \\cup \\{ (h,r,t')\\mid t'\\in E', t\\in E' \\}\n\\\\\n&T_{Protein-GO (h,r,t)}^{'} = \\{ (h,r,t')\\mid t'\\in E' \\}\n\\\\\n\\end{aligned}\n\\end{equation}\n\nwhere $ E' \\in \\{ E_{MFO}, E_{CCO}, E_{BPO} \\}$, and we only replace the tail entities for  $T_{Protein-GO}$ triples.\n}\n\n\\subsection{Pre-training Objective}\nWe adopt the mask protein modeling object and knowledge embedding objective to construct the overall object of the OntoProtein.\nWe jointly optimize the overall object as follows:\n\\begin{equation}\n\\ell = \\alpha \\ell_{KE} + \\ell_{MLM}\n\\end{equation}\nwhere $\\alpha$ is the hyper-parameter. \nOur approach can be embedded into existing fine-tuning scenarios.\n \n\\section{Experiment}\nExtensive experiments have been conducted to prove the effectiveness of our approach.\nIn the pre-training stage, we construct a new knowledge graph dataset that consists of Gene Ontology and public annotated proteins.\nOur proposed model is pre-trained with this dataset and evaluated in several downstream tasks.\nWe evaluate OntoProtein in protein function prediction, protein-protein interaction and  TAPE benchmark (\\cite{DBLP:conf/nips/RaoBTDCCAS19}).\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[scale=0.48]{figs/dataset.pdf}\n\\caption{\\textbf{Top}: Data Distribution of GO Terms. \\textbf{Bottom}: Statistics of Protein-GO Term.}\n\\label{dataset}\n\\end{figure*}\n\n\\subsection{Datasets}\n \n\\paragraph{Pre-training Dataset}\nTo incorporate Gene Ontology knowledge into language models, we build a new pre-training dataset called ProteinKG25\\footnote{\\url{https://zjunlp.github.io/project/ProteinKG25/}}, which is a large-scale KG dataset with aligned descriptions and protein sequences respectively to GO terms\\footnote{The structure of GO can be described in terms of a graph, where each GO term is a node, and the relationships between the terms are edges between the nodes.} and proteins entities. \nGene Ontology consists of a set of GO terms (or concepts) with relations that operate between them, e.g., molecular function terms describe activities that occur at the molecular level.\nA GO annotation is a statement about the function of a particular gene or gene product, e.g., the gene product “cytochrome c” can be described by the molecular function oxidoreductase activity.\nDue to the connection between Gene Ontology and Gene Annotations, we combine the two structures into a unified knowledge graph. \nFor each GO term in Gene Ontology, we align it to its corresponding name and description and concatenate them by a colon as an entire description.\nFor each protein in Gene annotation, we align it to the Swiss-Prot\\footnote{\\url{https://www.uniprot.org/}}, a protein knowledge database, and extract its corresponding sequence as its description.\nIn ProteinKG25, there exists 4,990,097 triples, including 4,879,951 $T_{protein-GO}$ and 110,146 $T_{GO-GO}$ triples. \nFigure \\ref{dataset} illustrate the statistics of our ProteinKG25.\nDetailed construction procedure and analysis of pre-train datasets can be found in Appendix \\ref{app:dataset}.\n \n\\paragraph{Downstream Task Dataset}\n\nWe use TAPE as the benchmark (\\cite{DBLP:conf/nips/RaoBTDCCAS19}) to evaluate protein representation learning.\nThere are three types of tasks in TAPE, including structure, evolutionary, and engineering for proteins.\nFollowing \\cite{DBLP:conf/icml/RaoLVMCASR21}, we select 6 representative datasets including secondary structure (SS), contact prediction to evaluate OntoProtein.\n\nProtein-protein interactions (PPI) are physical contacts of high specificity established between two or more protein molecules; we regard PPI as a sequence classification task and use three datasets with different sizes for evaluation. \nSTRING is built by \\cite{DBLP:conf/ijcai/LvHBZ21}, which contains 15,335 proteins and 593,397 PPIs.\nWe also use SHS27k and SHS148k, which are generated by \\cite{DBLP:journals/bioinformatics/ChenJZCZCZW19}.\n\nProtein function prediction aims to assign biological or biochemical roles to proteins, and we also regard this task as a sequence classification task.  \nWe build a new evaluation dataset based on our ProteinKG25 following the standard CAFA protocol (\\cite{zhou2019cafa}). \nSpecifically, we design two evaluation settings, the transductive setting and the inductive setting, which simulate two scenarios of gene annotation in reality. \nIn the transductive setting, the model can generate embeddings of unseen protein entities with entity descriptions. \nOn the contrary, for the inductive setting, those entities have occurred in the pre-training stage. \nThe detailed construction of the dataset can be found in Appendix \\ref{app:dataset}. \nAs shown in Figure \\ref{dataset}, proteins are, on average, annotated by 2 terms in CCO, 4 in MFO, and 3 in BPO, indicating that protein function prediction can be viewed as a multi-label problem.\n{\\color{highlight}\nNotably, we notice that leaf GO terms tend to have  more specific concepts than non-leaf GO terms. \nMeanwhile, there exists a challenging long-tail issue for the function prediction task. \n}\n\n \n\\subsection{Results}\n\\begin{table}[]\n\\centering\n\\begin{tabular}{lcccccc}\n\\toprule\n\\multirow{2}{*}{Method} & \\multicolumn{3}{c}{Structure} & Evolutionary & \\multicolumn{2}{c}{Engineering} \\\\ \n                        & SS-Q3   & SS-Q8   & Contact   & Homology     & Fluorescene     & Stability     \\\\ \\midrule\nLSTM            & 0.75 & 0.59 & 0.26 & 0.26 & 0.67 &  0.69 \\\\\nTAPE Transformer & 0.73 & 0.59 & 0.25 & 0.21 & {\\textbf{0.68}} &  0.73 \\\\\nResNet          & 0.75 & 0.58 & 0.25 & 0.17 &  0.21 &  0.73 \\\\\nMSA Transformer & - & \\textbf{0.73} & \\textbf{0.49} & - & - & - \\\\\nProtBert        & 0.81 & 0.67 & 0.35 & {\\color{highlight} \\textbf{0.29}} & 0.61 & {\\color{highlight} \\textbf{0.82}} \\\\\nOntoProtein     & \\textbf{0.82} & 0.68 & 0.40 & 0.24 & 0.66 & 0.75 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\nResults on TAPE Benchmark. \nSS is a secondary structure task that evaluates in CB513.\nIn contact prediction, we test medium- and long-range using P@L/2 metrics.\nIn protein engineering tasks, we test fluorescence and stability prediction using spearman's $\\rho$ metric.%\\protect\\footnotemark.\n}\n\\label{tape}\n\\end{table}\n\n\\subsection*{TAPE Benchmark}\n\\paragraph{Baselines}\nIn TAPE, we evaluate our OntoProtein compared with five baselines. \nThe first is the model with LSTM encoding of the input amino acid sequence, which provides a simple baseline. \nThe second is TAPE Transformer that provides a basic transformer baseline.\nWe further select  ResNet from \\cite{DBLP:conf/cvpr/HeZRS16} as a baseline.\nThe forth is the MSA Transformer (\\cite{DBLP:conf/icml/RaoLVMCASR21}).\nNote that MSA Transformer takes advantage of multiple sequence alignments (MSAs) and is the current state-of-the-art approach.\nFinally, we use ProtBert (\\cite{DBLP:journals/corr/abs-2007-06225}) with 30 layers of BERT encoder, which is the largest pre-trained model among baselines.\n\n\\paragraph{Results}\nWe detail the experimental result on TAPE in Table \\ref{tape}.\nConcretely, we notice that OntoProtein yields better performance in all token level tests.\nFor the second structure (SS-Q3 and SS-Q8) and contact prediction, OntoProtein outperforms TAPE Transformer and ProtBert, showing that it can benefit from those informative biology knowledge graphs in pre-training.\nMoreover, OntoProtein can achieve comparable performance with MSA transformer.\nNote that our proposed OntoProtein does not leverage the information from MSAs.\nHowever, with external gene ontology knowledge injection, OntoProtein can obtain promising performance.\nIn sequence level tasks, OntoProtein can achieve better performance than ProtBert in fluorescence prediction. \nHowever, we observe that OntoProtein does not perform well in protein engineering, homology, and stability prediction, which are all regression tasks.\nWe think this is due to the lack of sequence-level objectives in our pre-training object, and we leave this for future work.\n\n\\subsection*{Protein-Protein Interaction}\n\\paragraph{Baselines}\nWe choose four representative methods as baselines for protein-protein interaction.\nPIPR (\\cite{DBLP:journals/bioinformatics/ChenJZCZCZW19}), {\\color{highlight} DNN-PPI (\\cite{li2018deep}) and DPPI (\\cite{DBLP:journals/bioinformatics/HashemifarNKX18})} are  deep learning based methods.\nGNN-PPI (\\cite{DBLP:conf/ijcai/LvHBZ21}) is a graph neural network based method for better inter-novel-protein interaction prediction.\nTo evaluate our OntoProtein, we replace the initial protein embedding part of GNN-PPI with ProtBERT and OntoProtein as baselines.\n\n\\paragraph{Results}\n\nFrom Table \\ref{tab:ppi_result}, we observe that the performance of OntoProtein is better than PIPR, which demonstrates that external structure knowledge can be beneficial for protein-protein interaction prediction. \nWe also notice th         at our method can achieve promising improvement in smaller dataset SHS2K, even outperforming GNN-PPI and GNN-PPI (ProtBert).\nWith a larger size of datasets, OntoProtein can still obtain comparable performance to GNN-PPI and GNN-PPI (ProtBert).\n\n\\begin{table}[]\n    \\centering\n    \\begin{tabular}{l cccccc}\n    \\toprule\n    \\multicolumn{1}{l}{} &\n    \\multicolumn{2}{c}{\\textbf{SHS27k}} &\n    \\multicolumn{2}{c}{\\textbf{SHS148k}} &\n    \\multicolumn{2}{c}{\\textbf{STRING}} \\\\\n    \\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-7}\n    \\textbf{Methods} & BFS & DFS & BFS & DFS & BFS & DFS \\\\\n    \\midrule\n    {\\color{highlight} DPPI}  & {\\color{highlight} 41.43} & {\\color{highlight} 46.12} & {\\color{highlight} 52.12} & {\\color{highlight} 52.03} & {\\color{highlight} 56.68} & {\\color{highlight} 66.82} \\\\\n    {\\color{highlight} DNN-PPI} & {\\color{highlight} 48.90} & {\\color{highlight} 54.34} & {\\color{highlight} 57.40} & {\\color{highlight} 58.42} & {\\color{highlight} 53.05}  & {\\color{highlight} 64.94} \\\\\n    PIPR & 44.48 & 57.80 & 61.83 & 63.98 & 55.65 & 67.45 \\\\\n    GNN-PPI & 63.81 & 74.72 & 71.37 & 82.67 & 78.37 & 91.07 \\\\\n    GNN-PPI (ProtBert) & 70.94 & 73.36 & 70.32 & 78.86 & 67.61 & 87.44 \\\\\n    {\\color{highlight} GNN-PPI (OntoProtein)$^\\dagger$} & {\\color{highlight}\\textbf{72.26}} & {\\color{highlight}\\textbf{78.89}} & {\\color{highlight} \\textbf{75.23}} & {\\color{highlight}77.52 } &  {\\color{highlight} 76.71 } & {\\color{highlight}\\textbf{91.45}} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\n    Protein-Protein Interaction Prediction Results.\n    Breath-First Search (BFS) and Depth-First Search (DFS) are strategies that split the training and testing PPI datasets. \n    }\n    \\label{tab:ppi_result}\n\\end{table}\n\n\\begin{table}[]\n\\centering\n\\begin{tabular}{lcccccc}\n    \\toprule\n    \\multicolumn{1}{l}{} & \\multicolumn{3}{c}{\\textbf{Transductive}} & \\multicolumn{3}{c}{\\textbf{Inductive}}  \\\\\n    \\cmidrule(lr){2-4} \\cmidrule(lr){5-7}\n    \\textbf{Method} & BPO & MFO & CCO & BPO & MFO & CCO \\\\\n    \\midrule\n    ProtBert & 0.58 & 0.13 & 8.47 & 0.64 & 0.33 & 9.27 \\\\\n    OntoProtein & 0.62 & 0.13 & 8.46 & 0.66 & 0.25 & 8.37 \\\\\n    \\bottomrule\n\\end{tabular}\n\\caption{\nProtein Function Prediction Results on three sub-sets with two settings. \nBPO  refers to Biological Process, \nMFO refers to Molecular Function,\nand CCO refers to Cellular Component. \n}\n\\label{result:protein_function}\n\\end{table}\n \n\\subsection*{Protein Function Prediction}\n\\paragraph{Baselines}\nFor simplicity, we leverage Seq2Vec (\\cite{littmann2021embeddings}) as the backbone for fair comparison and initialize embeddings with ProtBert and our OntoProtein.\nNote that our approach is model-agnostic, and other backbones can also be leveraged. \n \n\\paragraph{Results}\nWe split the test sets into three subsets (BPO, MFO, and CCO) and evaluate the performance of models separately.\nFrom Table \\ref{result:protein_function}, we notice that our OntoProtein can yield a 4\\% improvement with transductive setting and 2\\% advancement with inductive setting in BPO, further demonstrating the effectiveness of our proposed approach. \nWe also observe that OntoProtein obtain comparable performance in other subsets.\nNote that there exists a severe long-tail issue in the dataset, and knowledge injecting may affect the representation learning for the head but weaken the tail representation, thus cause performance degradation.\nWe leave this for future works.\n\n\\subsection{Analysis}\nTable \\ref{Contact Prediction} illustrates a detailed experimental analysis on the contact prediction. \nTo further analyze the model’s performance, we conduct experiments to probe the performance of different sequences.\nSpecifically, protein sequence lengths from short-range ($6 \\le seq < 12$) to long-range ($24 \\le seq$) are tested with three metrics (P@L, P@L/2, P@L/5).\nWe choose several basic algorithms such as LSTM and TAPE transformer as baselines.\nFor fairness, ProtBert is also leveraged for comparison.\nIt can be seen that the performance of OntoProtein exceeds all other methods in all test settings, which is reasonable because the knowledge injected from Gene Ontology is beneficial.\n{\\color{highlight} \nFurther, we random sample a protein instance from the test dataset and analyze its attention weight of OntoProtein. \nWe conduct visualization analysis as shown in Figure \\ref{fig:attention} to compare the contacts among amino acids with the contact label matrix.\n}\n\n \n\n\\begin{table}[]\n\\begin{tabular}{p{3.0cm}p{0.6cm}p{0.8cm}p{0.8cm}p{0.6cm}p{0.8cm}p{0.8cm}p{0.6cm}p{0.8cm}p{0.8cm}}\n\\toprule\n\\multirow{2}{*}{}     & \\multicolumn{3}{c}{$6 \\le seq < 12$} & \\multicolumn{3}{c}{$12 \\le seq < 24$} & \\multicolumn{3}{c}{$24 \\le seq$} \\\\\n                      & P@L & P@L/2 & P@L/5 & P@L & P@L/2 & P@L/5 & P@L & P@L/2 & P@L/5     \\\\\n\\midrule\nTAPE Transformer & 0.28   & 0.35   & 0.46   & 0.19    & 0.25   & 0.33   & 0.17   & 0.20  & 0.24  \\\\\nLSTM       & 0.26   & 0.36   & 0.49   & 0.20    & 0.26   & 0.34   & 0.20   & 0.23  & 0.27  \\\\\nResNet     & 0.25   & 0.34   & 0.46   & 0.18    & 0.25   & 0.35   & 0.10   & 0.13  & 0.17  \\\\\nProtBert       & 0.30 & 0.40 & 0.52 & 0.27 & 0.35 & 0.47 & 0.20 & 0.26 & 0.34 \\\\\nOntoProtein       & \\textbf{0.37} & \\textbf{0.46} & \\textbf{0.57} & \\textbf{0.32} & \\textbf{0.40} & \\textbf{0.50} & \\textbf{0.24} & \\textbf{0.31} & \\textbf{0.39} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\nAblation study of contact prediction. \n$seq$ refers to the sequence length between amino acids.\n“P@K” is precision for the top $K$ contacts and $L$ is the length of the protein.}\n\\label{Contact Prediction}\n\\end{table}\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[scale=0.3]{figs/contact_label_attention.png}\n\\caption{\nWe randomly select a protein from the contact test dataset for visual analysis.\n\\textbf{Left}: We visualize the 7th head in the last attention layer in OntoProtein. \\textbf{Right}: It is the  contact label matrix.\n}\n\\label{fig:attention}\n\\end{figure*}\n\n\\subsection{Discussion}\nApplying techniques from NLP to proteins opens new opportunities to extract information from proteins in a self-supervised, data-driven way. \nHere we show for the first time that injecting external knowledge from gene ontology can help to learn protein representation better, thus, boosting the downstream protein tasks. \nHowever, the gains in our proposed OntoProtein compared to previous pre-trained models using large-scale corpus is still relatively small. \nNote that the knowledge graph ProteinKG25 can only cover a small subset of all proteins, thus, limiting the advancement.\nWe will continue to maintain the knowledge graph by adding new facts from Gene Ontology. \nBesides, previous studies (\\cite{DBLP:conf/aaai/LiuZ0WJD020,DBLP:conf/ijcai/ZhangDCCZZC21}) indicate that not all external knowledge are beneficial for downstream tasks, and it is necessary to investigate when and how to inject external knowledge into pre-trained models effectively.\nFinally, our proposed approach can be viewed as jointly pre-training human language and protein (the language of life).\nOur motivation is to crack the language of life’s code with gene knowledge injected protein pre-training. \nOur work is but a small step in this direction.  \n\n\\section{Related Work}\n\n\\subsection{Pre-trained Language Models}\nUp to now, various efforts have been devoted to exploring large-scale PTMs, either for NLP (\\cite{DBLP:conf/naacl/PetersNIGCLZ18,DBLP:conf/naacl/DevlinCLT19}), or for CV (\\cite{DBLP:conf/emnlp/TanB19}).\nFine-tuning large-scale PTMs such as ELMo (\\cite{DBLP:conf/naacl/PetersNIGCLZ18}), GPT3 (\\cite{DBLP:conf/nips/BrownMRSKDNSSAA20}), BERT (\\cite{DBLP:conf/naacl/DevlinCLT19}), XLNet (\\cite{DBLP:conf/nips/YangDYCSL19}) UniLM (\\cite{DBLP:conf/nips/00040WWLWGZH19}) for specific AI tasks instead of learning models from scratch has also become a consensus (\\cite{DBLP:journals/corr/abs-2106-07139}). \nApart from the of large scale language models for natural language processing, there has been considerable interest in developing similar models for proteins (\\cite{DBLP:journals/corr/abs-2108-07435,DBLP:journals/pnas/RivesMSGLLGOZMF21}).\n\\cite{DBLP:conf/icml/RaoLVMCASR21} is the first to study protein Transformer language models, demonstrating that information about residue-residue contacts can be recovered from the learned representations by linear projections supervised with protein structures. \n\\cite{DBLP:conf/iclr/VigMVXSR21} performs an extensive analysis of Transformer attention, identifying correspondences to biologically relevant features, and also finds that different layers of the model are responsible for learning different features. \n\\cite{DBLP:journals/corr/abs-2007-06225} proposes ProtTrans, which explores the limits of up-scaling language models trained on proteins as well as protein sequence databases and compares the effects of auto-regressive and auto-encoding pre-training upon the success of the subsequent supervised training. \nHuman-curated or domain-specific knowledge is essential for downstream tasks, {\\color{highlight} which is extensively studied such as  \\cite{himmelstein2015heterogeneous}, \\cite{smaili2018onto2vec}, \\cite{smaili2019opa2vec}, \\cite{hao2020bio},\\, \\cite{ioannidis2020drkg} }.  However these pre-training methods do not explicitly consider external knowledge like our proposed OntoProtein. \n\n\\subsection{Knowledge-enhanced Language Models}\n\nBackground knowledge has been considered as an indispensable part of language understanding (\\citep{DBLP:conf/ijcai/ZhangDCCZZC21,DBLP:conf/acl/DengZLHTCHC20,DBLP:journals/corr/abs-2109-08306,DBLP:conf/naacl/ZhangDSWCZC19,DBLP:conf/coling/YuZDYZC20,DBLP:journals/corr/abs-2109-00895,DBLP:conf/kdd/ZhangJD0YCTHWHC21,DBLP:journals/corr/abs-2104-07650,DBLP:journals/corr/abs-2201-03335,DBLP:conf/cpaior/Silvestri0M21,DBLP:journals/corr/abs-2112-01404,DBLP:journals/corr/abs-2201-05742,DBLP:journals/corr/abs-2201-05575}), which has inspired knowledge-enhanced models including ERNIE (Tsinghua) (\\cite{DBLP:conf/acl/ZhangHLJSL19}), ERNIE (Baidu) (\\cite{DBLP:journals/corr/abs-1904-09223}), KnowBERT (\\cite{DBLP:conf/emnlp/PetersNLSJSS19}), WKLM (\\cite{DBLP:conf/iclr/XiongDWS20}), LUKE (\\cite{DBLP:conf/emnlp/YamadaASTM20}), KEPLER (\\cite{DBLP:journals/tacl/WangGZZLLT21}), K-BERT (\\cite{DBLP:conf/aaai/LiuZ0WJD020}), K-Adaptor (\\cite{DBLP:conf/acl/WangTDWHJCJZ21}), and CoLAKE (\\cite{DBLP:conf/coling/SunSQGHHZ20}). \nERNIE (\\cite{DBLP:conf/acl/ZhangHLJSL19}) injects relational knowledge into the pre-trained model BERT, which aligns entities from Wikipedia to facts in WikiData.  \nKEPLER (\\cite{DBLP:journals/tacl/WangGZZLLT21}) jointly optimizes knowledge embedding and pre-trained language representation (KEPLER), which can not only better integrate factual knowledge into PLMs but also effectively learn KE through the abundant information in the text.\n \nInspired by these works, we propose OntoProtein that integrates external knowledge graphs into protein pre-training.\nTo the best of our knowledge, we are the first to inject gene ontology knowledge into protein language models. \n\n\\section{Conclusion and Future Work}\nIn this paper, we take the first step to integrating external factual knowledge from gene ontology into protein language models.\nWe present protein pretraining with gene ontology embedding (OntoProtein), which is the first general framework to integrate external knowledge graphs into protein pre-training. \nExperimental results on widespread protein tasks demonstrate that efficient knowledge injection helps understand and uncover the grammar of life.\nBesides, OntoProtein is compatible with the model parameters of lots of pre-trained protein language models, which means that users can directly adopt the available pre-trained parameters on OntoProtein without modifying the architecture. \nThese positive results point to future work in \n(1) improving OntoProtein by injecting more informative knowledge with gene ontology selection; (2) extending this approach to sequence generating tasks for protein design.\n\n\\section*{Acknowledgments}\nWe  want to express gratitude to the anonymous reviewers for their hard work and kind comments. This work is funded by NSFCU19B2027/NSFC91846204, National Key R\\&D Program of China (Funding No.SQ2018YFC000004), Zhejiang Provincial Natural Science Foundation of China (No. LGG22F030011), Ningbo Natural Science Foundation (2021J190), and Yongjiang Talent Introduction Programme (2021A-156-G). \n\n\\section*{Reproducibility Statement}\nOur code and datasets are all available in the \\url{https://github.com/zjunlp/OntoProtein} for reproducibility.\nHyper-parameters are provided in the Appendix \\ref{apendix:hypter-parameters}.\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Submission and Formatting Instructions for \\\\\n           International Conference on Machine Learning (ICML 2023)}\n\n\\begin{document}\n\n\\twocolumn[\n\\icmltitle{Submission and Formatting Instructions for \\\\\n           International Conference on Machine Learning (ICML 2023)}\n\n\\icmlsetsymbol{equal}{*}\n\n\\begin{icmlauthorlist}\n\\icmlauthor{Firstname1 Lastname1}{equal,yyy}\n\\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}\n\\icmlauthor{Firstname3 Lastname3}{comp}\n\\icmlauthor{Firstname4 Lastname4}{sch}\n\\icmlauthor{Firstname5 Lastname5}{yyy}\n\\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}\n\\icmlauthor{Firstname7 Lastname7}{comp}\n\\icmlauthor{Firstname8 Lastname8}{sch}\n\\icmlauthor{Firstname8 Lastname8}{yyy,comp}\n\\end{icmlauthorlist}\n\n\\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}\n\\icmlaffiliation{comp}{Company Name, Location, Country}\n\\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}\n\n\\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}\n\\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}\n\n\\icmlkeywords{Machine Learning, ICML}\n\n\\vskip 0.3in\n]\n\n\\printAffiliationsAndNotice{\\icmlEqualContribution} % otherwise use the standard text.\n\n\\begin{abstract}\nThis document provides a basic paper template and submission guidelines.\nAbstracts must be a single paragraph, ideally between 4--6 sentences long.\nGross violations will trigger corrections at the camera-ready phase.\n\\end{abstract}\n\n\\section{Electronic Submission}\n\\label{submission}\n\nSubmission to ICML 2023 will be entirely electronic, via a web site\n(not email). Information about the submission process and \\LaTeX\\ templates\nare available on the conference web site at:\n\\begin{center}\n\\textbf{\\texttt{http://icml.cc/}}\n\\end{center}\n\nThe guidelines below will be enforced for initial submissions and\ncamera-ready copies. Here is a brief summary:\n\\begin{itemize}\n\\item Submissions must be in PDF\\@. \n\\item \\textbf{New to this year}: If your paper has appendices, submit the appendix together with the main body and the references \\textbf{as a single file}. Reviewers will not look for appendices as a separate PDF file. So if you submit such an extra file, reviewers will very likely miss it.\n\\item Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited. For the final version of the paper, authors can add one extra page to the main body.\n\\item \\textbf{Do not include author information or acknowledgements} in your\n    initial submission.\n\\item Your paper should be in \\textbf{10 point Times font}.\n\\item Make sure your PDF file only uses Type-1 fonts.\n\\item Place figure captions \\emph{under} the figure (and omit titles from inside\n    the graphic file itself). Place table captions \\emph{over} the table.\n\\item References must include page numbers whenever possible and be as complete\n    as possible. Place multiple citations in chronological order.\n\\item Do not alter the style template; in particular, do not compress the paper\n    format by reducing the vertical spaces.\n\\item Keep your abstract brief and self-contained, one paragraph and roughly\n    4--6 sentences. Gross violations will require correction at the\n    camera-ready phase. The title should have content words capitalized.\n\\end{itemize}\n\n\\subsection{Submitting Papers}\n\n\\textbf{Paper Deadline:} The deadline for paper submission that is\nadvertised on the conference website is strict. If your full,\nanonymized, submission does not reach us on time, it will not be\nconsidered for publication. \n\n\\textbf{Anonymous Submission:} ICML uses double-blind review: no identifying\nauthor information may appear on the title page or in the paper\nitself. \\cref{author info} gives further details.\n\n\\textbf{Simultaneous Submission:} ICML will not accept any paper which,\nat the time of submission, is under review for another conference or\nhas already been published. This policy also applies to papers that\noverlap substantially in technical content with conference papers\nunder review or previously published. ICML submissions must not be\nsubmitted to other conferences and journals during ICML's review\nperiod.\nInformal publications, such as technical\nreports or papers in workshop proceedings which do not appear in\nprint, do not fall under these restrictions.\n\n\\medskip\n\nAuthors must provide their manuscripts in \\textbf{PDF} format.\nFurthermore, please make sure that files contain only embedded Type-1 fonts\n(e.g.,~using the program \\texttt{pdffonts} in linux or using\nFile/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)\nmight come from graphics files imported into the document.\n\nAuthors using \\textbf{Word} must convert their document to PDF\\@. Most\nof the latest versions of Word have the facility to do this\nautomatically. Submissions will not be accepted in Word format or any\nformat other than PDF\\@. Really. We're not joking. Don't send Word.\n\nThose who use \\textbf{\\LaTeX} should avoid including Type-3 fonts.\nThose using \\texttt{latex} and \\texttt{dvips} may need the following\ntwo commands:\n\n{\\footnotesize\n\\begin{verbatim}\ndvips -Ppdf -tletter -G0 -o paper.ps paper.dvi\nps2pdf paper.ps\n\\end{verbatim}}\nIt is a zero following the ``-G'', which tells dvips to use\nthe config.pdf file. Newer \\TeX\\ distributions don't always need this\noption.\n\nUsing \\texttt{pdflatex} rather than \\texttt{latex}, often gives better\nresults. This program avoids the Type-3 font problem, and supports more\nadvanced features in the \\texttt{microtype} package.\n\n\\textbf{Graphics files} should be a reasonable size, and included from\nan appropriate format. Use vector formats (.eps/.pdf) for plots,\nlossless bitmap formats (.png) for raster graphics with sharp lines, and\njpeg for photo-like images.\n\nThe style file uses the \\texttt{hyperref} package to make clickable\nlinks in documents. If this causes problems for you, add\n\\texttt{nohyperref} as one of the options to the \\texttt{icml2023}\nusepackage statement.\n\n\\subsection{Submitting Final Camera-Ready Copy}\n\nThe final versions of papers accepted for publication should follow the\nsame format and naming convention as initial submissions, except that\nauthor information (names and affiliations) should be given. See\n\\cref{final author} for formatting instructions.\n\nThe footnote, ``Preliminary work. Under review by the International\nConference on Machine Learning (ICML). Do not distribute.'' must be\nmodified to ``\\textit{Proceedings of the\n$\\mathit{40}^{th}$ International Conference on Machine Learning},\nHonolulu, Hawaii, USA, PMLR 202, 2023.\nCopyright 2023 by the author(s).''\n\nFor those using the \\textbf{\\LaTeX} style file, this change (and others) is\nhandled automatically by simply changing\n$\\mathtt{\\backslash usepackage\\{icml2023\\}}$ to\n$$\\mathtt{\\backslash usepackage[accepted]\\{icml2023\\}}$$\nAuthors using \\textbf{Word} must edit the\nfootnote on the first page of the document themselves.\n\nCamera-ready copies should have the title of the paper as running head\non each page except the first one. The running title consists of a\nsingle line centered above a horizontal rule which is $1$~point thick.\nThe running head should be centered, bold and in $9$~point type. The\nrule should be $10$~points above the main text. For those using the\n\\textbf{\\LaTeX} style file, the original title is automatically set as running\nhead using the \\texttt{fancyhdr} package which is included in the ICML\n2023 style file package. In case that the original title exceeds the\nsize restrictions, a shorter form can be supplied by using\n\n\\verb|\\icmltitlerunning{...}|\n\njust before $\\mathtt{\\backslash begin\\{document\\}}$.\nAuthors using \\textbf{Word} must edit the header of the document themselves.\n\n\\section{Format of the Paper}\n\nAll submissions must follow the specified format.\n\n\\subsection{Dimensions}\n\nThe text of the paper should be formatted in two columns, with an\noverall width of 6.75~inches, height of 9.0~inches, and 0.25~inches\nbetween the columns. The left margin should be 0.75~inches and the top\nmargin 1.0~inch (2.54~cm). The right and bottom margins will depend on\nwhether you print on US letter or A4 paper, but all final versions\nmust be produced for US letter size.\nDo not write anything on the margins.\n\nThe paper body should be set in 10~point type with a vertical spacing\nof 11~points. Please use Times typeface throughout the text.\n\n\\subsection{Title}\n\nThe paper title should be set in 14~point bold type and centered\nbetween two horizontal rules that are 1~point thick, with 1.0~inch\nbetween the top rule and the top edge of the page. Capitalize the\nfirst letter of content words and put the rest of the title in lower\ncase.\n\n\\subsection{Author Information for Submission}\n\\label{author info}\n\nICML uses double-blind review, so author information must not appear. If\nyou are using \\LaTeX\\/ and the \\texttt{icml2023.sty} file, use\n\\verb+\\icmlauthor{...}+ to specify authors and \\verb+\\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information\nwill not be printed unless \\texttt{accepted} is passed as an argument to the\nstyle file.\nSubmissions that include the author information will not\nbe reviewed.\n\n\\subsubsection{Self-Citations}\n\nIf you are citing published papers for which you are an author, refer\nto yourself in the third person. In particular, do not use phrases\nthat reveal your identity (e.g., ``in previous work \\cite{langley00}, we\nhave shown \\ldots'').\n\nDo not anonymize citations in the reference section. The only exception are manuscripts that are\nnot yet published (e.g., under submission). If you choose to refer to\nsuch unpublished manuscripts \\cite{anonymous}, anonymized copies have\nto be submitted\nas Supplementary Material via CMT\\@. However, keep in mind that an ICML\npaper should be self contained and should contain sufficient detail\nfor the reviewers to evaluate the work. In particular, reviewers are\nnot required to look at the Supplementary Material when writing their\nreview (they are not required to look at more than the first $8$ pages of the submitted document).\n\n\\subsubsection{Camera-Ready Author Information}\n\\label{final author}\n\nIf a paper is accepted, a final camera-ready copy must be prepared.\nFor camera-ready papers, author information should start 0.3~inches below the\nbottom rule surrounding the title. The authors' names should appear in 10~point\nbold type, in a row, separated by white space, and centered. Author names should\nnot be broken across lines. Unbolded superscripted numbers, starting 1, should\nbe used to refer to affiliations.\n\nAffiliations should be numbered in the order of appearance. A single footnote\nblock of text should be used to list all the affiliations. (Academic\naffiliations should list Department, University, City, State/Region, Country.\nSimilarly for industrial affiliations.)\n\nEach distinct affiliations should be listed once. If an author has multiple\naffiliations, multiple superscripts should be placed after the name, separated\nby thin spaces. If the authors would like to highlight equal contribution by\nmultiple first authors, those authors should have an asterisk placed after their\nname in superscript, and the term ``\\textsuperscript{*}Equal contribution\"\nshould be placed in the footnote block ahead of the list of affiliations. A\nlist of corresponding authors and their emails (in the format Full Name\n\\textless{}email@domain.com\\textgreater{}) can follow the list of affiliations.\nIdeally only one or two names should be listed.\n\nA sample file with author names is included in the ICML2023 style file\npackage. Turn on the \\texttt{[accepted]} option to the stylefile to\nsee the names rendered. All of the guidelines above are implemented\nby the \\LaTeX\\ style file.\n\n\\subsection{Abstract}\n\nThe paper abstract should begin in the left column, 0.4~inches below the final\naddress. The heading `Abstract' should be centered, bold, and in 11~point type.\nThe abstract body should use 10~point type, with a vertical spacing of\n11~points, and should be indented 0.25~inches more than normal on left-hand and\nright-hand margins. Insert 0.4~inches of blank space after the body. Keep your\nabstract brief and self-contained, limiting it to one paragraph and roughly 4--6\nsentences. Gross violations will require correction at the camera-ready phase.\n\n\\subsection{Partitioning the Text}\n\nYou should organize your paper into sections and paragraphs to help\nreaders place a structure on the material and understand its\ncontributions.\n\n\\subsubsection{Sections and Subsections}\n\nSection headings should be numbered, flush left, and set in 11~pt bold\ntype with the content words capitalized. Leave 0.25~inches of space\nbefore the heading and 0.15~inches after the heading.\n\nSimilarly, subsection headings should be numbered, flush left, and set\nin 10~pt bold type with the content words capitalized. Leave\n0.2~inches of space before the heading and 0.13~inches afterward.\n\nFinally, subsubsection headings should be numbered, flush left, and\nset in 10~pt small caps with the content words capitalized. Leave\n0.18~inches of space before the heading and 0.1~inches after the\nheading.\n\nPlease use no more than three levels of headings.\n\n\\subsubsection{Paragraphs and Footnotes}\n\nWithin each section or subsection, you should further partition the\npaper into paragraphs. Do not indent the first line of a given\nparagraph, but insert a blank line between succeeding ones.\n\nYou can use footnotes\\footnote{Footnotes\nshould be complete sentences.} to provide readers with additional\ninformation about a topic without interrupting the flow of the paper.\nIndicate footnotes with a number in the text where the point is most\nrelevant. Place the footnote in 9~point type at the bottom of the\ncolumn in which it appears. Precede the first footnote in a column\nwith a horizontal rule of 0.8~inches.\\footnote{Multiple footnotes can\nappear in each column, in the same order as they appear in the text,\nbut spread them across columns and pages if possible.}\n\n\\begin{figure}[ht]\n\\vskip 0.2in\n\\begin{center}\n\\centerline{\\includegraphics[width=\\columnwidth]{icml_numpapers}}\n\\caption{Historical locations and number of accepted papers for International\nMachine Learning Conferences (ICML 1993 -- ICML 2008) and International\nWorkshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was\nproduced, the number of accepted papers for ICML 2008 was unknown and instead\nestimated.}\n\\label{icml-historical}\n\\end{center}\n\\vskip -0.2in\n\\end{figure}\n\n\\subsection{Figures}\n\nYou may want to include figures in the paper to illustrate\nyour approach and results. Such artwork should be centered,\nlegible, and separated from the text. Lines should be dark and at\nleast 0.5~points thick for purposes of reproduction, and text should\nnot appear on a gray background.\n\nLabel all distinct components of each figure. If the figure takes the\nform of a graph, then give a name for each axis and include a legend\nthat briefly describes each curve. Do not include a title inside the\nfigure; instead, the caption should serve this function.\n\nNumber figures sequentially, placing the figure number and caption\n\\emph{after} the graphics, with at least 0.1~inches of space before\nthe caption and 0.1~inches after it, as in\n\\cref{icml-historical}. The figure caption should be set in\n9~point type and centered unless it runs two or more lines, in which\ncase it should be flush left. You may float figures to the top or\nbottom of a column, and you may set wide figures across both columns\n(use the environment \\texttt{figure*} in \\LaTeX). Always place\ntwo-column figures at the top or bottom of the page.\n\n\\subsection{Algorithms}\n\nIf you are using \\LaTeX, please use the ``algorithm'' and ``algorithmic''\nenvironments to format pseudocode. These require\nthe corresponding stylefiles, algorithm.sty and\nalgorithmic.sty, which are supplied with this package.\n\\cref{alg:example} shows an example.\n\n\\begin{algorithm}[tb]\n   \\caption{Bubble Sort}\n   \\label{alg:example}\n\\begin{algorithmic}\n   \\STATE {\\bfseries Input:} data $x_i$, size $m$\n   \\REPEAT\n   \\STATE Initialize $noChange = true$.\n   \\FOR{$i=1$ {\\bfseries to} $m-1$}\n   \\IF{$x_i > x_{i+1}$}\n   \\STATE Swap $x_i$ and $x_{i+1}$\n   \\STATE $noChange = false$\n   \\ENDIF\n   \\ENDFOR\n   \\UNTIL{$noChange$ is $true$}\n\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Tables}\n\nYou may also want to include tables that summarize material. Like\nfigures, these should be centered, legible, and numbered consecutively.\nHowever, place the title \\emph{above} the table with at least\n0.1~inches of space before the title and the same after it, as in\n\\cref{sample-table}. The table title should be set in 9~point\ntype and centered unless it runs two or more lines, in which case it\nshould be flush left.\n\n\\begin{table}[t]\n\\caption{Classification accuracies for naive Bayes and flexible\nBayes on various data sets.}\n\\label{sample-table}\n\\vskip 0.15in\n\\begin{center}\n\\begin{small}\n\\begin{sc}\n\\begin{tabular}{lcccr}\n\\toprule\nData set & Naive & Flexible & Better? \\\\\n\\midrule\nBreast    & 95.9$\\pm$ 0.2& 96.7$\\pm$ 0.2& $\\surd$ \\\\\nCleveland & 83.3$\\pm$ 0.6& 80.0$\\pm$ 0.6& $\\times$\\\\\nGlass2    & 61.9$\\pm$ 1.4& 83.8$\\pm$ 0.7& $\\surd$ \\\\\nCredit    & 74.8$\\pm$ 0.5& 78.3$\\pm$ 0.6&         \\\\\nHorse     & 73.3$\\pm$ 0.9& 69.7$\\pm$ 1.0& $\\times$\\\\\nMeta      & 67.1$\\pm$ 0.6& 76.5$\\pm$ 0.5& $\\surd$ \\\\\nPima      & 75.1$\\pm$ 0.6& 73.9$\\pm$ 0.5&         \\\\\nVehicle   & 44.9$\\pm$ 0.6& 61.5$\\pm$ 0.4& $\\surd$ \\\\\n\\bottomrule\n\\end{tabular}\n\\end{sc}\n\\end{small}\n\\end{center}\n\\vskip -0.1in\n\\end{table}\n\nTables contain textual material, whereas figures contain graphical material.\nSpecify the contents of each row and column in the table's topmost\nrow. Again, you may float tables to a column's top or bottom, and set\nwide tables across both columns. Place two-column tables at the\ntop or bottom of the page.\n\n\\subsection{Theorems and such}\nThe preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.\n\\begin{definition}\n\\label{def:inj}\nA function $f:X \\to Y$ is injective if for any $x,y\\in X$ different, $f(x)\\ne f(y)$.\n\\end{definition}\nUsing \\cref{def:inj} we immediate get the following result:\n\\begin{proposition}\nIf $f$ is injective mapping a set $X$ to another set $Y$, \nthe cardinality of $Y$ is at least as large as that of $X$\n\\end{proposition}\n\\begin{proof} \nLeft as an exercise to the reader. \n\\end{proof}\n\\cref{lem:usefullemma} stated next will prove to be useful.\n\\begin{lemma}\n\\label{lem:usefullemma}\nFor any $f:X \\to Y$ and $g:Y\\to Z$ injective functions, $f \\circ g$ is injective.\n\\end{lemma}\n\\begin{theorem}\n\\label{thm:bigtheorem}\nIf $f:X\\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.\n\\end{theorem}\nAn easy corollary of \\cref{thm:bigtheorem} is the following:\n\\begin{corollary}\nIf $f:X\\to Y$ is bijective, \nthe cardinality of $X$ is at least as large as that of $Y$.\n\\end{corollary}\n\\begin{assumption}\nThe set $X$ is finite.\n\\label{ass:xfinite}\n\\end{assumption}\n\\begin{remark}\nAccording to some, it is only the finite case (cf. \\cref{ass:xfinite}) that is interesting.\n\\end{remark}\n\n\\subsection{Citations and References}\n\nPlease use APA reference format regardless of your formatter\nor word processor. If you rely on the \\LaTeX\\/ bibliographic\nfacility, use \\texttt{natbib.sty} and \\texttt{icml2023.bst}\nincluded in the style-file package to obtain this format.\n\nCitations within the text should include the authors' last names and\nyear. If the authors' names are included in the sentence, place only\nthe year in parentheses, for example when referencing Arthur Samuel's\npioneering work \\yrcite{Samuel59}. Otherwise place the entire\nreference in parentheses with the authors and year separated by a\ncomma \\cite{Samuel59}. List multiple references separated by\nsemicolons \\cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'\nconstruct only for citations with three or more authors or after\nlisting all authors to a publication in an earlier reference \\cite{MachineLearningI}.\n\nAuthors should cite their own work in the third person\nin the initial version of their paper submitted for blind review.\nPlease refer to \\cref{author info} for detailed instructions on how to\ncite your own papers.\n\nUse an unnumbered first-level section heading for the references, and use a\nhanging indent style, with the first line of the reference flush against the\nleft margin and subsequent lines indented by 10 points. The references at the\nend of this document give examples for journal articles \\cite{Samuel59},\nconference publications \\cite{langley00}, book chapters \\cite{Newell81}, books\n\\cite{DudaHart2nd}, edited volumes \\cite{MachineLearningI}, technical reports\n\\cite{mitchell80}, and dissertations \\cite{kearns89}.\n\nAlphabetize references by the surnames of the first authors, with\nsingle author entries preceding multiple author entries. Order\nreferences for the same authors by year of publication, with the\nearliest first. Make sure that each reference includes all relevant\ninformation (e.g., page numbers).\n\nPlease put some effort into making references complete, presentable, and\nconsistent, e.g. use the actual current name of authors.\nIf using bibtex, please protect capital letters of names and\nabbreviations in titles, for example, use \\{B\\}ayesian or \\{L\\}ipschitz\nin your .bib file.\n\n\\section*{Accessibility}\nAuthors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.\nTips of how to achieve this and what to pay attention to will be provided on the conference website \\url{http://icml.cc/}.\n\n\\section*{Software and Data}\n\nIf a paper is accepted, we strongly encourage the publication of software and data with the\ncamera-ready version of the paper whenever appropriate. This can be\ndone by including a URL in the camera-ready copy. However, \\textbf{do not}\ninclude URLs that reveal your institution or identity in your\nsubmission for review. Instead, provide an anonymous URL or upload\nthe material as ``Supplementary Material'' into the CMT reviewing\nsystem. Note that reviewers are not required to look at this material\nwhen writing their review.\n\n\\section*{Acknowledgements}\n\n\\textbf{Do not} include acknowledgements in the initial version of\nthe paper submitted for blind review.\n\nIf a paper is accepted, the final camera-ready version can (and\nprobably should) include acknowledgements. In this case, please\nplace such acknowledgements in an unnumbered section at the\nend of the paper. Typically, this will include thanks to reviewers\nwho gave useful comments, to colleagues who contributed to the ideas,\nand to funding agencies and corporate sponsors that provided financial\nsupport.\n\n\\nocite{langley00}\n\n\\newpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{\\ourapproach: Function-Informed Protein Multi-Modal Learning}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nMulti-modality pre-training paradigm that aligns protein sequences and biological descriptions has learned general protein representations and achieved promising performance in various downstream applications. However, these works were still unable to replicate the extraordinary success of language-supervised visual foundation models due to the ineffective usage of aligned protein-text paired data and the lack of an effective function-informed pre-training paradigm. To address these issues, this paper curates a large-scale protein-text paired dataset called \\ourdataset with a property-driven sampling strategy, and introduces a novel function-informed protein pre-training paradigm. Specifically, the sampling strategy determines selecting probability based on the sample confidence and property coverage, balancing the data quality and data quantity in face of large-scale noisy data. Furthermore, motivated by significance of the protein specific functional mechanism, the proposed paradigm explicitly model protein static and dynamic functional segments by two segment-wise pre-training objectives, injecting fine-grained information in a function-informed manner.\nLeveraging all these innovations, we develop \\ourapproach, a multi-modality foundation model that comprehensively represents function-aware protein embeddings.\nOn 22 different protein benchmarks within 5 types, including protein functionality classification, mutation effect prediction, cross-modal transformation, semantic similarity inference and protein-protein interaction prediction, our \\ourapproach consistently achieves SOTA performance, with remarkable improvements of 75\\% on average in five cross-modal transformation benchmarks, 59.9\\% in GO-CC and 39.7\\% in GO-BP \nprotein function prediction. The experimental results verify the extraordinary potential of \\ourapproach serving as the protein multi-modality foundation model.\n\\end{abstract}\n\n\\section{Introduction}\nProteins are essential functional units of cells, responsible for performing a wide range of vital and versatile functions crucial to life.\nMirroring the language-supervised pre-training paradigm towards powerful and unified vision representations~\\cite{clip, DALLE2, ImageBind, BLIP2}, \nprevious work has explored in the pre-training of multi-modality Protein Language Models (PLMs) by aligning protein sequences with textual function descriptions to achieve function-centric protein representations~\\cite{OntoProtein, ProtST, ProteinCLIP, ProtET}.\nHowever, these works were still unable to replicate the extraordinary success of image-text foundation models, and have shown to discard fine-grained protein functional information~\\cite{ProteinCLIP}, which results in the suboptimal performance on cross-modal transformation~\\cite{BioBridge} and localization prediction~\\cite{ProtST}.\nLiterature has summarized that the success of visual foundation models primarily stems from \\textbf{the efficient utilization of large-scale data}~\\cite{clip, internVL} and \\textbf{a holistic multi-modal pre-training framework}~\\cite{KAD, PRIOR}, which points to two inherent obstacles that hinder further progress in multi-modal protein-biotext pre-training:\n\n\\noindent \\textbf{(i) Absence of large-scale datasets and ineffective data usage.}\nLarge-scale aligned dataset is an indispensable part of obtaining powerful multi-modality foundation models. \nHowever, biotexts describing protein functions are much harder to construct than image captions, as often requiring detailed annotated process including manual review by experts or computational analysis by machines. \nThis highlights the pressing need of large-scale multi-modal datasets containing protein sequences with high-quality functional annotations across multiple attribute domains.\nEven with large-scale protein-biotext pairs, it is non-trivial to effectively inject biological property information into PLMs during multi-modal pre-training. This is primarily because the machine-analyzed process leads to numerous noisy labels (\\textit{i.e.}, less accurate annotations)~\\cite{Swiss_trEMBL}. Currently, there is still a lack of efficient learning techniques to effectively utilize large-scale proteins with noisy annotations for protein-biotext pre-training.\n\n\\noindent \\textbf{(ii) Lack of a function-informed pre-training paradigm.}\nUnlike the alignment of natural image-text pairs, the understanding of proteins is strongly influenced by their specific functional mechanism, which has been largely neglected by previous research yet. Proteins perform specific biological functions depending on their corresponding functional domains in 3D structural spaces. The amino acids at these active site are contiguous or discrete in 1D protein sequences. In this paper, we introduce the static and dynamic functional segment, new concepts which directly determine the specific protein functions and should be primarily focused during the alignment with biological function descriptions. However, we find existing protein-biotext pre-training works directly take after the original CLIP methodology for coarse-grained alignment, discarding the fine-grained information of protein unique functional mechanism (\\textit{i.e.}, static or dynamic functional segments primarily determine protein specific functions and properties), which significantly prevents the better performance of protein-biotext pre-training.\n\nOur work proposes a step towards constructing a universally applicable protein multi-modality foundation model aligning biological and natural language. We present \\ourapproach, consistently alleviates the aforementioned two intrinsic problems and introduces remarkable innovations in multiple dimensions including the pre-training data, sampling strategy, and multi-modality objectives.\n\nWe first construct a high-quality protein-biotext paired dataset \\ourdataset with sparse version (\\ourdatasetS) and dense version (\\ourdatasetD), derived from the existing protein function database~\\cite{UniProt}. \n\\ourapproach employs \\ourdatasetD comprising 251.5 million aligned pairs for large-scale protein-biotext pre-training, which is the same order of magnitude as large-scale image-text pre-training. \nSince there exist some inevitable noisy annotations in \\ourdatasetD (caused by machine-annotated bias), we propose a novel property-driven sampling strategy motivated by~\\cite{MixMatch, DivideMix}.\nCompared to the vanilla uniformly sampling, the proposed sampling strategy decides the selecting probability based on the sample confidence and property coverage, simultaneously balancing the data quality and data quantity in face of large-scale noisy labels.\nFurthermore, a function-informed pre-training paradigm is constructed motivated by significance of the protein functional mechanism. \nWithin such paradigm, we utilize CLIP loss~\\cite{clip} to inject coarse-grained information, and two segment-wise objectives are designed to capture fine-grained information of the static and dynamic functional segments.\nConcretely, on the one hand, we design a cross-modality reconstruction module to recover the masked static segments based on knowledge from both modalities.\nOn the other hand, the property prototype is exploited to aggregate dynamic segments in an unsupervised way. The resulting property-grouped dynamic segments are contrasted with property prototypes within the same protein-biotext pair, mitigating the mutual interference across multiple attribute domains.\n\nEvaluated by extensive experiments, \\ourapproach sets new state-of-the-art on 22 important yet challenging protein benchmarks within five types. For protein classification engineering and mutation effect prediction, the superiority of \\ourapproach in representation learning attributes to incorporation of multi-modal information (\\textit{e.g.}, 59.9\\%/39.7\\% improvements in Go-CC/GO-BP benchmarks). For cross-modal transformation, \\ourapproach surpasses baselines by a significant margin (75\\% improvement). \nFor semantic similarity inference and protein-protein interaction prediction, \\ourapproach ranks the best, which verifies effectiveness of the proposed data-efficient and function-informed multi-modal learning.\n\n\\section{Methods}\nIn this section, we first describe the curated multi-modal dataset, \\ourdataset, and the property-driven sampling strategy to enhance data usage effectiveness. Next, we introduce the model architectures and our novel function-informed pre-training paradigm, which incorporates holistic multi-modal pre-training objectives to capture both coarse-grained and fine-grained information. Finally, we summarize the overall loss function used for protein-biotext pre-training.\n\\begin{table}[ht]\n    \\caption{Data distribution of \\ourdatasetS and \\ourdatasetD with different sample confidence. We highlight the confidence where protein entries are mostly concentrated in \\textbf{bold}.}\n    \\vspace{-1em}\n    \\label{Tab:data_confidence}\n    \\begin{center}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{c c c c c c}\n            \\toprule\n            \\textbf{Dataset} & \\textbf{Conf-L1} & \\textbf{Conf-L2} & \\textbf{Conf-L3} & \\textbf{Conf-L4} & \\textbf{Conf-L5} \\\\\n            \\midrule\n            \\ourdatasetS & 0.1982 & 0.0980 & \\textbf{0.6777} & 0.0229 & 0.0032 \\\\\n            \\midrule\n            \\ourdatasetD & 0.0013 & 0.0057 & 0.3269 & \\textbf{0.6661} & 0.0000 \\\\\n            \\bottomrule\n    \\end{tabular}}\n    \\vspace{-1em}\n    \\end{center}\n\\end{table}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.95\\linewidth]{coverage.pdf}\n    \\caption{Data distribution of \\ourdatasetS and \\ourdatasetD with different property coverage.}\n    \\vspace{-1em}\n    \\label{Fig:data_coverage}\n\\end{figure}\n\\subsection{Pre-training data}\n\\label{Sec:data}\n\\begin{figure*}[htbp]\n    \\centering\n    \\includegraphics[width=0.97\\textwidth]{results.pdf}\\vspace{-0.5em}\n    \\caption{Comparison results on 22 downstream benchmarks within five types. \\ourapproach consistently achieves the state-of-the-art performance on all these tasks. PPI: protein-protein interaction.}\n    \\label{Fig:results}\\vspace{-1em}\n\\end{figure*}\n\\subsubsection{Dataset Curation}\nTo enable pre-training of the protein multi-modality foundation model aligning biological and natural language, it is essential to build dataset containing large-scale pairs of protein sequences and textual property descriptions. Our pre-training data is sourced from SwissProt and trEMBL~\\cite{Swiss_trEMBL}, containing proteins with textual descriptions. \nWe align protein sequences with meticulously selected properties to curate \\ourdataset, which is available in sparse version (\\ourdatasetS) and dense version (\\ourdatasetD). \\ourdatasetS includes 0.5 million manually reviewed protein-biotext pairs with higher annotation quality, whereas \\ourdatasetD comprises 251.5 million mostly computationally analyzed protein-biotext pairs which are less accurate due to the machine-annotated bias. \nTo gain more insights into the dataset, we conduct extensive quantitative analyses, and display the compositional structure of \\ourdataset with varying confidence $C$ and property coverage $R$ in Table~\\ref{Tab:data_confidence} and Figure~\\ref{Fig:data_coverage}. \nDetails about selected properties for alignment and the data format are further illustrated in Appendix~\\ref{App:data_format}.\n\n\\subsubsection{Property-driven Sampling Strategy}\nFor protein-biotext pre-training, most prior works only used scarce proteins with manually reviewed annotations (equivalent to \\ourdatasetS), and the attempt to incorporate plentiful computationally analyzed proteins (equivalent to \\ourdatasetD) has been unsuccessful, declaring ``\\textit{data quality could be more important than data quantity.}''~\\cite{ProtST}. However, we question and rethink this issue, and propose the property-driven sampling strategy which integrate the merits of the multi-modality data quality and data quantity. Specifically, the main considerations for sampling probability are sample confidence $C$ and property coverage $R$ and data size $N$. Note that the smaller the confidence, the more reliable the entry is, and $C \\in \\{1,2,3,4,5\\}$, $R \\in \\{1/4, 2/4, 3/4, 4/4\\}$.\nInitially, we discard machine-annotated entries with $C=4,5$ (less accurate) and $R=1/4,2/4$ (low coverage) for comprehensive property understanding. \nNext, rather than uniform sampling, we explicitly build the sampling distribution according to the aforementioned three factors. \nThe likelihood of selecting protein entries from cluster $u$ with $\\{C_u, R_u, N_u\\}$ during multi-modality pre-training is defined as:\n\\begin{equation}\n    P = \\frac{C_u^{-3} \\cdot \\sqrt{R_u} \\cdot N_u}{\\sum_{i,j,k} C_i^{-3} \\cdot \\sqrt{R_j} \\cdot N_k}.\n\\end{equation}\nIn this paper, we perform large-scale protein-biotext pre-training exploiting \\ourdatasetD, in conjunction with the proposed property-driven sampling strategy.\n\n\\subsection{Model Architecture}\nThe overview of our framework is displayed in Figure~\\ref{Fig:framework}, which contains a protein encoder and a biotext encoder. The protein encoder is a protein language model for learning biological features from protein sequences and we use pre-trained ESM-2-650M~\\cite{esm-2} here. The biotext encoder is a text language model for learning linguistic features from biotext descriptions and we use PubMedBERT~\\cite{PubMedBERT} here. Initialization with these two pre-trained large models significantly facilitates pre-training process by providing decent representations in the early stage of training.\n\n\\begin{figure*}[htbp]\n    \\centering\n    \\includegraphics[width=0.95\\textwidth]{framework.pdf}\n    \\caption{Overview of \\ourapproach. We curate a large-scale protein-biotext dataset \\ourdataset with a property-driven sampling strategy, and proposes a function-informed pre-training paradigm containing two segment-wise objectives BSR and PDA. }\n    \\vspace{-1em}\n    \\label{Fig:framework}\n\\end{figure*}\n\n\\subsection{Function-informed Pre-training Paradigm}\nTo accomplish the holistic function-informed multi-modal pre-training, we jointly optimize four protein-biotext pre-training objectives, with two classic ones and two newly proposed segment-wise ones, customized for learning locality-aware and fine-grained information of protein specific functional mechanism. \n\n\\subsubsection{Global Contrastive Loss}\nGlobal Contrastive loss (GC) learning aligns representations of two modalities by encouraging positive pairs to have higher similarity in contrast to the negative pairs. Considering the effectiveness of $\\textit{\\textbf{L}}_{\\text{GC}}$ for multi-modal understanding in many previous works~\\cite{clip,BLIP2,MoSu} from different domains, we perform it to realize global alignment of protein-biotext. Given a batch of sequence-text pairs $\\{(S_i, T_i)\\}_{i=1}^{K}$, $\\textit{\\textbf{L}}_{\\text{GC}}$ is composed of two symmetric standard InfoNCE loss:\n\\begin{equation}\n\\begin{split}\n    \\textit{\\textbf{L}}_{\\text{GC}} = -\\frac{1}{2} \\left[ \\mathbb{E}_{p(S,T)}(log\\frac{\\exp(\\mathtt{sim}(S_i, T_i)/\\tau_1)}{\\sum_{j=1}^{K} \\exp(\\mathtt{sim}(S_i, T_j)/\\tau_1)}) \\right. \\\\\n    \\left. +~\\mathbb{E}_{p(S,T)}(log\\frac{\\exp(\\mathtt{sim}(T_i, S_i)/\\tau_1)}{\\sum_{j=1}^{K} \\exp(\\mathtt{sim}(T_i, S_j)/\\tau_1)}) \\right ],\n\\end{split}\n\\end{equation}\nwhere $\\mathtt{sim}(;)$ is the consine similarity and $\\tau_1$ denotes the temperature parameter that controls the softmax distribution.\n\n\\subsubsection{Biotext-guided Static Segment Reconstruction (BSR)}\nGiven the global contrastive objective modeling coarse-grained information, the fine-grained information of static and dynamic segments are ubiquitous, which primarily determines protein specific functions and properties. To capture such locality-aware information of static segments, we propose Biotext-guided Static segment Reconstruction (BSR) to reconstruct corrupted static segments using information from both modalities.\nSpecifically, given a sequence of protein residues $S=\\{x_1, x_2,\\dots,x_n\\}$, we sample $l$ consecutive tokens as a static segment at a time, until the total sampling length reaches 15\\% of $S$. In other words, we execute sampling iterations to prepare a random set of static segments $\\{e_1, e_2, \\dots, e_m\\}$ with $e_i \\in S$ for subsequent masking and reconstruction. \nAt each iteration, we randomly select the starting point of each segment and its length $l$ follows a discrete uniform distribution between 5 and 10.\nNote that all static segments are non-overlapping and their total length accounts for 15\\% of $S$. \n\nGiven the selected diverse static segments, we introduce a novel cross-modality reconstruction module to reconstruct masked segments according to the biotext functional descriptions, as displayed in Figure~\\ref{Fig:framework}. Specifically, the protein sequence with masked segments $e^{m}$ and biotext $T$ are fed into a cross-attention module to obtain the fused representation by attending to all tokens along the biological property description. Then a MLP with the GELU activation~\\cite{GELU} and layer normalization~\\cite{LN} serves as the reconstruction head.\nFormally, the loss function for BSR is:\n\\begin{equation}\n    \\textit{\\textbf{L}}_{\\text{BSR}} = \\mathbb{E}_{p(T, e^{m})} H(\\Phi(T, e^{m}), y_e),\n\\end{equation}\nwhere $\\Phi(T, e^{m})$ is the predicted probability of protein sequence with masked static segments $e^{m}$, and $y_e$ is the corresponding ground truth. $H(;)$ is the cross-entropy function. \n\n\\subsubsection{Property-grouped Dynamic Segment Alignment (PDA)}\nTo capture the fine-grained information of dynamic segments, we propose Property-grouped Dynamic Segment Alignment (PDA), optimizing the alignment between property-grouped dynamic segments and corresponding property descriptions.\n\nSpecifically, a prototype memory bank is constructed to approximate property descriptive sentences, without any need to accurately retain redundant information such as syntax. \n\nThen the property prototype is exploited to aggregate dynamic segments in an unsupervised way, which are more flexible than static segments in BSR.  \nProvided property description prototypes of biotext $T=\\{a_1, a_2, a_3, a_4\\}$ and the corresponding sequence of residues $S=\\{x_1, x_2, \\dots, x_n\\}$, we first compute similarity weights as:\n\\begin{equation}\n    w_{ij}=a_i \\cdot x_j,~i=1,2,3,4,~j=1,2,\\dots,n,\n\\end{equation}\nwhere $w_{ij} \\in \\mathbb{R}$ and $\\cdot$ is the inner product.\nThen min-max normalization is applied along the residue dimension to normalize $w_{ij}$ to $[0,1]$. After that, some non-functional protein residues are discarded by sparsifying the similarity weights with a threshold $\\theta$:\n\\begin{equation}\n    \\hat{w}_{ij}=\n    \\begin{cases}\n        w_{ij}, & \\text{if}~w_{ij} \\geq \\theta \\\\\n        0, & \\text{otherwise}.\n    \\end{cases}\n\\end{equation}\nEventually, we obtain the property-grouped dynamic segments by multiplying similarity weights and protein residues:\n\\begin{equation}\n    e_i = \\{\\hat{w}_{ij} x_{j}~|~j=1,2,\\dots,n\\},~i=1,2,3,4.\n\\end{equation}\nProperty-grouped dynamic segment alignment is conducted to align these dynamic segments with property descriptions within the same protein-\nbiotext pair, mitigating the mutual interference across multiple attribute domains:\n\n\\begin{equation}\n\\begin{split}\n    \\textit{\\textbf{L}}_{\\text{PDA}} = -\\frac{1}{2} \\left[ \\mathbb{E}_{p(e,a)}(log\\frac{\\exp(\\mathtt{sim}(e_i, a_i)/\\tau_2)}{\\sum_{k} \\exp(\\mathtt{sim}(e_i, a_k)/\\tau_2)}) \\right. \\\\\n    \\left. +~\\mathbb{E}_{p(e,a)}(log\\frac{\\exp(\\mathtt{sim}(e_i, a_i)/\\tau_2)}{\\sum_{k} \\exp(\\mathtt{sim}(e_i, a_k)/\\tau_2)}) \\right ],\n\\end{split}\n\\end{equation}\nwhere $\\mathtt{sim}(;)$ represents the consine similarity and $\\tau_2$ denotes the temperature parameter that controls the softmax distribution.\n\nAiming to extract the essential knowledge of protein sequences, we select the most relevant residues based on their similarities to each property description, resulting in segments of variable lengths. Owing to such variable length, dynamic segments are flexible to capture information of consecutive or non-consecutive functional residues, excluding redundant and non-functional ones. Additionally, the threshold $\\theta$ directly influences the segment length by determining different number of zero values in each row of the similarity weights, which decouples similarities of individual residues to different property descriptions. In essence, the thresholding operation allows for different properties to match different residues that are the most relevant, thereby forming dynamic segments.\n\n\\subsection{Overall Loss Function}\nThe overall loss function of \\ourapproach comprises four terms. Global contrastive loss $\\textit{\\textbf{L}}_{\\text{GC}}$ learns coarse-grained information, while biotext-guided static segment reconstruction $\\textit{\\textbf{L}}_{\\text{BSR}}$ and property-grouped dynamic segment alignment $\\textit{\\textbf{L}}_{\\text{PDA}}$ focuses on fine-grained information. And we keep the protein masked language modeling $\\textit{\\textbf{L}}_{\\text{MLM}}$ to preserve unimodal knowledge when injecting multi-modality information from biological texts.\nWe optimize these terms jointly via a weighted sum with hyper-parameters $\\lambda_1$ and $\\lambda_2$:\n\\begin{equation}\n    \\textit{\\textbf{L}} = \\textit{\\textbf{L}}_{\\text{GC}}+\\lambda_1 \\textit{\\textbf{L}}_{\\text{BSR}}+\\lambda_2 \\textit{\\textbf{L}}_{\\text{MLM}}+\\textit{\\textbf{L}}_{\\text{PDA}}.\n\\end{equation}\nDuring the training process, we observe a significant mutual interference between segment-level reconstruction $\\textit{\\textbf{L}}_{\\text{BSR}}$ and token-level reconstruction $\\textit{\\textbf{L}}_{\\text{MLM}}$, and set $\\lambda_1+\\lambda_2=1$. The investigation of their equilibrium is in Section~\\ref{Sec:ablation}.\n\n\\section{Experiments}\nIn this section, we first introduce some training setups, and then provide configurations and result discussions about five types of downstream applications (Figure~\\ref{Fig:downstream}) on totally 22 benchmarks. Eventually, the analysis of ablation experiments are presented to further validate the effectiveness of our pre-training objectives.\n\n\\subsection{Training Setups}\nWe build our codes upon the PyTorch framework and conduct experiments on 64 Tesla V100 GPUs with 10,000 GPU hours.\nAn Adam optimizer is used (learning rate: $1.0 \\times 10^{-5}$, weight decay: $0$) to train the model. The batch size is 2048 and 512 for pre-training and downstream experiments. \nWithin the function-informed pre-training paradigm, we set hyper-parameters $\\theta=0.3, \\lambda_1=0.7, \\lambda_2=0.3$.\n\n\\subsection{Protein Classification Engineering}\n\\label{Sec:exp1}\n\\subsubsection{Configurations}\nProtein classification engineering aims to classify protein locations and functions. For location classification, we consider two such problems from DeepLoc~\\cite{DeepLoc}, subcellular localization prediction (Sub) with 10 categories and binary localization prediction (Bin) with 2 categories. For function classification, we employ two benchmarks~\\cite{DeepFRI} namely Enzyme Commission (EC) number prediction and Gene Ontology (GO) term prediction. On GO benchmark, there are three branches that predict molecular function (GO-MF), biological process (GO-BP) and cellular component (GO-CC).\nThe compared baselines include three parts: (a) four traditional protein encoders CNN~\\cite{CNN}, ResNet~\\cite{TAPE}, LSTM~\\cite{TAPE}, Transformer~\\cite{TAPE}; (b) \nfour single-modal PLMs ProtBERT~\\cite{ProtTrans}, OntoProtein~\\cite{OntoProtein}, ESM-1b~\\cite{esm-1b}, ESM2~\\cite{esm-2}); (c) one multi-modal PLM ProtST-ESM2~\\cite{ProtST}.\nThe evaluation metrics are accuracy for location prediction, and AUPR and F$_\\text{max}$ for function prediction. AUPR denotes the pair-centric area under precision-recall curve. It computes average precision scores for all protein-biotext pairs, which is exactly the micro-average precision score for the classification problem. F$_\\text{max}$ demotes the protein-centric maximum F-score.\n\\begin{table}[!ht]\n    \\caption{Results on location classification (Loc class) and mutation effect prediction (Effect pred) tasks. We highlight the best results in \\textbf{bold}.}\n    \\vspace{-1em}\n    \\label{Tab:class1_reg}\n    \\begin{center}\n    \\setlength{\\tabcolsep}{0.3em}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{c|l|c c| c c c c c}\n            \\toprule\n            \\multirow{2}{*}{\\textbf{\\makecell{Modality \\\\ Amount}}}& \\multicolumn{1}{c|}{\\multirow{2}{*}{\\textbf{Method}}} & \\multicolumn{2}{c}{\\textbf{Loc class} (\\small{Acc \\%})} & \\multicolumn{5}{|c}{\\textbf{Effect pred} (\\small{Spearman's $\\rho$})} \\\\\n            \\cmidrule{3-9}\n            & & Bin & Sub & $\\beta$-lac & AAV & Thermo & Flu & Sta \\\\\n            \\midrule\n            \\multicolumn{9}{c}{Tranditional models trained from scratch}\\\\\n            \\midrule\n            \\multirow{4}{*}{Single} & CNN & 82.67 & 58.73 & 0.781 & 0.746 & 0.494 & 0.682 & 0.637 \\\\\n            & ResNet & 78.99 & 52.30 & 0.152 & 0.739 & 0.528 & 0.636 & 0.126 \\\\\n            & LSTM & 88.11 & 62.98 & 0.139 & 0.125 & 0.564 & 0.494 & 0.533 \\\\\n            & Transformer & 75.74 & 56.02 & 0.261 & 0.681 & 0.545 & 0.643 & 0.649 \\\\\n            \\midrule\n            \\multicolumn{9}{c}{PLMs under linear probing} \\\\\n            \\midrule\n            \\multirow{4}{*} {Single} & ProtBERT & 81.54 & 59.44 & 0.616 & 0.209 & 0.562 & 0.339 & 0.697 \\\\\n            & OntoProtein & 84.87 & 68.34 & 0.471 & 0.217 & 0.605 & 0.432 & 0.688 \\\\\n            & ESM-1b & 91.61 & 79.82 & 0.528 & 0.454 & 0.674 & 0.430 & 0.750 \\\\\n            & ESM2 & 91.32 & 80.84 & 0.559 & 0.374 & 0.677 & 0.456 & 0.746 \\\\\n            \\midrule\n            \\multirow{2}{*}{Multiple} & ProtST-ESM2 & 92.52 & 83.39 & 0.565 & 0.398 & 0.681 & 0.499 & 0.776 \\\\\n            & \\textbf{\\ourapproach} & \\cellcolor{gray!20}94.39 & \\cellcolor{gray!20}83.65 & \\cellcolor{gray!20}0.565 & \\cellcolor{gray!20}0.532 & \n            \\cellcolor{gray!20}0.682 & \n            \\cellcolor{gray!20}0.503 & \n            \\cellcolor{gray!20}0.795 \\\\\n            \\midrule\n            \\multicolumn{9}{c}{PLMs under full tuning}\\\\\n            \\midrule\n            \\multirow{4}{*}{Single}\n            & ProtBERT & 91.32 & 76.53 & 0.731 & 0.794 & 0.660 & 0.679 & 0.771 \\\\\n            & OntoProtein & 92.47 & 77.59 & 0.757 & 0.791 & 0.662 & 0.630 & 0.731 \\\\\n            & ESM-1b & 92.40 & 78.13 & 0.839 & 0.821 & 0.669 & 0.679 & 0.694 \\\\\n            & ESM2 & 91.72 & 78.67 & 0.867 & 0.817 & 0.672 & 0.677 & 0.718 \\\\\n            \\midrule\n            \\multirow{2}{*}{Multiple}& ProtST-ESM2 & 92.52 & 80.22 & 0.879 & 0.825 & 0.682 & 0.682 & 0.738 \\\\\n            & \\textbf{\\ourapproach} & \\cellcolor{gray!20}\\textbf{95.08} & \\cellcolor{gray!20}\\textbf{85.34} & \\cellcolor{gray!20}\\textbf{0.884} & \\cellcolor{gray!20}\\textbf{0.892} & \n            \\cellcolor{gray!20}\\textbf{0.686} & \n            \\cellcolor{gray!20}\\textbf{0.685} & \n            \\cellcolor{gray!20}\\textbf{0.819} \\\\\n            \\bottomrule\n    \\end{tabular}}\n    \\end{center}\n\\end{table}\n\\begin{table}[!ht]\n    \\caption{Results on function classification task. We highlight the best results in \\textbf{bold}.}\n    \\vspace{-1em}\n    \\label{Tab:class2}\n    \\begin{center}\n    \\setlength{\\tabcolsep}{0.3em}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{c|l|c c| c c| c c| c c}\n            \\toprule\n            \\multirow{2}{*}{\\textbf{\\makecell{Modality \\\\ Amount}}} & \\multicolumn{1}{c|}{\\multirow{2}{*}{\\textbf{Method}}} & \\multicolumn{2}{c|}{\\textbf{EC}} & \\multicolumn{2}{c|}{\\textbf{GO-BP}} & \\multicolumn{2}{c|}{\\textbf{GO-MF}} & \\multicolumn{2}{c}{\\textbf{GO-CC}}\\\\\n            \\cmidrule{3-10}\n            & & \\small{AUPR} & $F_{\\text{max}}$ & \\small{AUPR} & $F_{\\text{max}}$ & \\small{AUPR} & $F_{\\text{max}}$ & \\small{AUPR} & $F_{\\text{max}}$ \\\\\n            \\midrule\n            \\multicolumn{10}{c}{Traditional model trained from scratch}\\\\\n            \\midrule\n            \\multirow{4}{*}{Single} & CNN & 0.540 & 0.545 & 0.165 & 0.244 & 0.380 & 0.354 & 0.261 & 0.387 \\\\\n            & ResNet & 0.137 & 0.187 & 0.166 & 0.280 & 0.281 & 0.267 & 0.266 & 0.403 \\\\\n            & LSTM & 0.032 & 0.082 & 0.130 & 0.248 & 0.100 & 0.166 & 0.150 & 0.320 \\\\\n            & Transformer & 0.187 & 0.219 & 0.135 & 0.257 & 0.172 & 0.240 & 0.170 & 0.380 \\\\\n            \\midrule\n            \\multicolumn{10}{c}{PLMs under full tuning}\\\\\n            \\midrule\n            \\multirow{4}{*}{Single}\n            & ProtBERT & 0.859 & 0.838 & 0.188 & 0.279 & 0.464 & 0.456 & 0.234 & 0.408 \\\\\n            & OntoProtein & 0.854 & 0.841 & 0.284 & 0.436 & 0.603 & 0.631 & 0.300 & 0.441 \\\\\n            & ESM-1b & 0.884 & 0.869 & 0.332 & 0.452 & 0.630 & 0.659 & 0.324 & 0.477 \\\\\n            & ESM2 & 0.888 & 0.874 & 0.340 & 0.472 & 0.643 & 0.662 & 0.350 & 0.472 \\\\\n            \\midrule\n            \\multirow{2}{*}{Multiple}& ProtST-ESM2 & 0.898 & 0.878 & 0.342 & 0.482 & 0.647 & 0.668 & 0.364 & 0.487 \\\\\n            & \\textbf{\\ourapproach} & \\cellcolor{gray!20}\\textbf{0.906} & \\cellcolor{gray!20}\\textbf{0.908} & \\cellcolor{gray!20}\\textbf{0.567} & \\cellcolor{gray!20}\\textbf{0.574} & \n            \\cellcolor{gray!20}\\textbf{0.696} & \n            \\cellcolor{gray!20}\\textbf{0.691} & \n            \\cellcolor{gray!20}\\textbf{0.582} &\n            \\cellcolor{gray!20}\\textbf{0.541} \\\\\n            \\bottomrule\n    \\end{tabular}}\\vspace{-1em}\n    \\end{center}\n\\end{table}\n\\subsubsection{Results}\nTable~\\ref{Tab:class1_reg} (left) and Table~\\ref{Tab:class2} show that \\ourapproach establishes state-of-the-art results on all six classification benchmarks under both linear probing and full tuning settings.\nMoreover, \\ourapproach performs best on protein classification engineering among all five type of downstream tasks.\n\\begin{figure*}[htbp]\n    \\centering\n    \\includegraphics[width=0.93\\textwidth]{downstream.pdf}\n    \\vspace{-0.5em}\n    \\caption{An overview of downstream tasks within five types.}\n    \\label{Fig:downstream}\n\\end{figure*}\n\\begin{table*}[!ht]\n\\begin{minipage}{0.49\\textwidth}\n    \\caption{Mean reciprocal rank (MRR) results on cross-modal transformation task. Prot: protein.}\\vspace{-1em}\n    \\label{Tab:trans}\n    \\setlength{\\tabcolsep}{0.3em}\n    \\begin{center}\n    \\resizebox{1\\linewidth}{!}{\n    \\begin{tabular}{l c c c c c}\n            \\toprule\n            \\multicolumn{1}{c}{\\textbf{Method}} & \\textbf{Prot2BP} & \\textbf{Prot2MF} & \\textbf{Prot2CC} & \\textbf{Prot2Drug} & \\textbf{Disease2Prot} \\\\\n            \\midrule\n            ComplEx & 0.084 & 0.100 & 0.099 & 0.079 & 0.059 \\\\\n            DistMult & 0.054 & 0.089 & 0.095 & 0.044 & 0.033 \\\\\n            RotatE & 0.079 & 0.119 & 0.107 & 0.125 & 0.070 \\\\\n            BioBridge & 0.136 & 0.326 & 0.319 & 0.172 & 0.084 \\\\\n            \\ourapproach & \\cellcolor{gray!20}\\textbf{0.224} & \\cellcolor{gray!20}\\textbf{0.475} & \\cellcolor{gray!20}\\textbf{0.414} & \\cellcolor{gray!20}\\textbf{0.409} & \\cellcolor{gray!20}\\textbf{0.161} \\\\\n            \\bottomrule\n    \\end{tabular}}\\vspace{-1em}\n    \\end{center}\n\\end{minipage}\n\\hfill\n\\begin{minipage}{0.49\\textwidth}\n    \\caption{Results on semantic similarity inference (Sim) and protein-protein interaction prediction (PPI) tasks.}\\vspace{-1em}\n    \\label{Tab:sim_PPI}\n    \\begin{center}\n    \\resizebox{0.96\\linewidth}{!}{\n    \\begin{tabular}{l| c c c| c c c c}\n            \\toprule\n            \\multicolumn{1}{c|}{\\multirow{2}{*}{\\textbf{Method}}} & \\multicolumn{3}{c|}{\\textbf{Sim} (\\small{Spearman's $\\rho$})} & \\multicolumn{3}{c}{\\textbf{PPI} (\\small{F1 score})}  \\\\\n            \\cmidrule{2-7}\n            & MF & BP & CC & SHS27K & SHS148K & STRING \\\\\n            \\midrule\n            ESM2-3B & 0.33 & 0.42 & 0.23 & 0.732 & 0.733 & 0.834 \\\\\n            KeAP & 0.41 & 0.41 & 0.40 & 0.733 & 0.726 & 0.834 \\\\\n            BioBridge & 0.91 & 0.80 & 0.73 & 0.739 & 0.739 & 0.836 \\\\\n            \\ourapproach & \\cellcolor{gray!20}\\textbf{0.92} & \\cellcolor{gray!20}\\textbf{0.88} & \\cellcolor{gray!20}\\textbf{0.76} & \\cellcolor{gray!20}\\textbf{0.744} & \\cellcolor{gray!20}\\textbf{0.740} & \\cellcolor{gray!20}\\textbf{0.838} \\\\\n            \\bottomrule\n    \\end{tabular}}\\vspace{-1em}\n    \\end{center}\n\\end{minipage}\n\\end{table*}\n\\subsection{Mutation Effect Prediction}\n\\subsubsection{Configurations}\nMutation effect prediction is a regression task that predicts the effect of residue mutations on protein fitness. We utilize $\\beta$-lactamase ($\\beta$-lac) landscape from PEER~\\cite{PEER}, Fluorescence (Flu) and Stability (Sta) landscapes from TAPE~\\cite{TAPE}, and AAV and Thermostability (Thermo) landscapes from FLIP~\\cite{FLIP}. Following the setup of~\\cite{ProtST}, we use ``two vs many'' and ``human cell'' dataset splits for AAV and Thermo, and the split settings of the rest tasks remain default and unchanged.\nThe baselines remain the same as mentioned in Section~\\ref{Sec:exp1}. The performance is measured by Spearman’s $\\rho$. \nMoreover, \nwe evaluate \\ourapproach and PLMs under both linear probing and full tuning settings on location prediction and mutation effect prediction tasks.\n\n\\subsubsection{Results}\nTable~\\ref{Tab:class1_reg} illustrates that \\ourapproach consistently ranks the best among other baselines. We can observe that although traditional models (\\textit{e.g.}, CNN) pose strong competition in mutation effect prediction, \\ourapproach still retains the lead, especially on Stability benchmark in full tuning setting.\n\n\\subsection{Cross-modal Transformation}\n\\subsubsection{Configurations}\nCross-modal transformation matches the transformed embedding with candidates from the target modality, where embeddings from \\ourapproach are transformed by an extra transformation module. Following~\\cite{BioBridge}, we leverage the raw knowledge graph (KG) data and undertake some preprocessing steps, with the training/validation/test split of 80\\%/10\\%/10\\%. The baselines are BioBridge~\\cite{BioBridge} and three knowledge graph embedding methods \n(ComplEx~\\cite{ComplEx}, DistMult~\\cite{DistMult}, RotatE~\\cite{RotatE}).\nWe use mean reciprocal rank (MRR) as the metric.\n\n\\subsubsection{Results}\nTable~\\ref{Tab:trans} reports our remarkable enhancement over all baselines. The first three baselines are traditional KG encoders trained from scratch, which lack flexibility, while BioBridge cannot fully unleash the potential of PLMs. Instead, \\ourapproach compensates for their shortcomings and incorporates flexibility, data-efficiency and high performance.\nParticularly, \\ourapproach is 2.4 $\\times$ better than the best baseline for ``Prot2Drug'' and 2 $\\times$ better for ``Prot2BP'' and ``Disease2Prot'', which signals the superiority of \\ourapproach in multimodal understanding.\n\n\\subsection{Semantic Similarity Inference}\n\\label{Sec:exp4}\n\\subsubsection{Configurations}\nSemantic similarity inference computes the relevance between predicted and groundtruth similarity matrices~\\cite{exp_sim}. our goal is to evaluate the extent to which the encoded protein embeddings can capture biomolecular functional similarity (\\textit{i.e.}, BP, CC, MF). The predicted matrix contains pairwise Manhattan Similarities of the encoded protein embeddings, while the groundtruth stores pairwise Lin Similarities of the protein associated BP, MF, and CC. We compare \\ourapproach with three baselines (\\textit{i.e.}, ESM2-3B~\\cite{esm-2}, KeAP~\\cite{KeAP}, BioBridge~\\cite{BioBridge}). The metric is Spearman's $\\rho$.\n\\subsubsection{Results}\nIn Table~\\ref{Tab:sim_PPI} (left), \\ourapproach achieves the best performance over other baselines. In particular, \\ourapproach \nsurpasses the vanilla ESM2-3B by a large margin, demonstrating the proposed data-efficient and function-informed multi-modal learning is generally beneficial to the unimodal PLM.\n\n\\subsection{Protein-Protein Interaction Prediction}\n\\subsubsection{Configurations}\nProtein-protein interaction (PPI) prediction seeks to classify 7 interaction types of a pair of proteins. Following~\\cite{OntoProtein}, we extract the protein embeddings with \\ourapproach and baselines, which serve as the input for a graph neural network model to be trained on the PPI network. The\nbaselines remain the same as mentioned in Section~\\ref{Sec:exp4}. Additionally, F1 score is reported on SHS27K~\\cite{SHSdataset}, SHS148K~\\cite{SHSdataset} and STRING~\\cite{STRING} datasets for evaluation.\n\\subsubsection{Results}\nTable~\\ref{Tab:sim_PPI} (right) presents average results on three benchmarks. \\ourapproach performs the best and exceeds the prior state-of-the-art BioBridge owing to its pre-training on the enormous dataset \\ourdatasetD with the property-driven sampling strategy.\n\n\\subsection{Ablation Study}\n\\label{Sec:ablation}\nWe conduct extensive ablation experiments from multiple aspects. Unless otherwise specified, ESM-2-150M serves as the protein encoder and we evaluate on three downstream benchmarks from different types in ablation experiments.\n\n\\subsubsection{Ablation study on Pre-training Data}\nAs seen in Section~\\ref{Sec:data}, we curate a new dataset \\ourdataset with a property-driven sampling strategy. Table~\\ref{Tab:ablation_data} displays comparison of different pre-training data organization. \nObviously, single dataset pre-training and pretrain+finetune (first pretrained on machine-annotated data, then fine-tuned on manually-reviewed data) are inferior to the model pre-trained on \\ourdatasetD with the proposed sampling strategy. Such phenomenon demonstrates that low-quality data still holds potential value if subjected to elaborate processing and sampling, and \\ourdataset strikes a good balance between data quality and data quantity.\n\n\\subsubsection{Ablation Study on Pre-training Objectives}\nTable~\\ref{Tab:ablation_loss} reports results with full or partial pre-training objectives. We can observe that  both PDA and BSR are essential for injecting fine-grained information, and the absence of PDA leads to a more significant drop compared to the lack of BSR. Such results signal the competence of our function-informed paradigm for protein-biotext multi-modal learning.\n\\begin{table}[!ht]\n    \\caption{Analysis on pre-training data. Pretrain+fintune: first pretrained on low accurate data, then fine-tuned on high accurate data. Property-driven sampling strategy: pretrained on \\ourdatasetD with the proposed sampling strategy.}\n    \\vspace{-1em}\n    \\label{Tab:ablation_data}\n    \\begin{center}\n    \\resizebox{0.9\\linewidth}{!}{\n    \\begin{tabular}{l|c | c  c | c}\n            \\toprule\n            \\multicolumn{1}{c|}{\\multirow{2}{*}{\\textbf{\\makecell{Pre-training \\\\ data}}}} & \\textbf{Sub} & \\multicolumn{2}{c|}{\\textbf{EC}} & \\textbf{Prot2MF} \\\\\n            \\cmidrule{2-5}\n            & Acc \\% & \\small{AUPR} & $F_{\\text{max}}$ & \\small{MRR} \\\\\n            \\midrule\n            \\ourdatasetS & 72.41 & 0.216 & 0.282 & 0.246 \\\\\n            \\ourdatasetD & 73.72 & 0.282 & 0.309 & 0.256 \\\\\n            Pretrain+finetune & 74.98 & 0.312 & 0.404 & 0.283 \\\\\n            \\midrule\n            Our sampling strategy & \\textbf{75.77} & \\textbf{0.384} & \\textbf{0.441} & \\textbf{0.299} \\\\\n            \\bottomrule\n    \\end{tabular}}\n    \\vspace{-1em}\n    \\end{center}\n\\end{table}\n\\begin{table}[!ht]\n    \\caption{Ablation study on pre-training objectives.}\n    \\vspace{-1em}\n    \\label{Tab:ablation_loss}\n    \\begin{center}\n    \\resizebox{0.85\\linewidth}{!}{\n    \\begin{tabular}{l|c | c c | c}\n            \\toprule\n            \\multicolumn{1}{c|}{\\multirow{2}{*}{\\textbf{Config}}} & \\textbf{Sub} & \\multicolumn{2}{c|}{\\textbf{EC}} & \\textbf{Prot2MF} \\\\\n            \\cmidrule{2-5}\n            & Acc \\% & \\small{AUPR} & $F_{\\text{max}}$ & \\small{MRR} \\\\\n            \\midrule\n            w/o $\\textit{\\textbf{L}}_{\\text{BSR}}$ & 76.09 & 0.189 & 0.254 & 0.282 \\\\\n            w/o $\\textit{\\textbf{L}}_{\\text{PDA}}$ & 73.64 & 0.136 & 0.227 & 0.210 \\\\\n            \\midrule\n            Full loss & \\textbf{76.52} & \\textbf{0.204} & \\textbf{0.320} & \\textbf{0.312} \\\\\n            \\bottomrule\n    \\end{tabular}}\\vspace{-1em}\n    \\end{center}\n\\end{table}\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{interference.pdf}\\vspace{-1em}\n    \\caption{Mutual interference between segment-wise reconstruction $\\textbf{\\textit{L}}_\\text{BSR}$ and token-wise reconstruction $\\textbf{\\textit{L}}_\\text{MLM}$ during the pre-training process.}\n    \\vspace{-1em}\n    \\label{Fig:interference}\n\\end{figure}\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.97\\linewidth]{loss_weight.pdf}\\vspace{-1em}\n    \\caption{Ablation study on loss weights.}\n    \\vspace{-1em}\n    \\label{Fig:ablation_weights}\n\\end{figure}\n\\subsubsection{Ablation Study on Loss Weights}\nDuring the pre-training process, we observe a significant mutual\ninterference between segment-level reconstruction $\\textbf{\\textit{L}}_\\text{BSR}$ and token-level reconstruction $\\textbf{\\textit{L}}_\\text{MLM}$. As depicted in Figure~\\ref{Fig:interference}, the loss curve fluctuate violently without falling if we apply no loss weights whereas it declines smoothly if the appropriate loss weights are assigned. Therefore, it is essential to find the optimal loss wights.\nIn Figure~\\ref{Fig:ablation_weights}, different values of loss weights $\\lambda_1$ yield different ablation results on two location classification benchmarks. Due to evident advantages, the ultimate weights are $\\lambda_1=0.7$ and thus $\\lambda_2=1-\\lambda_1=0.3$.\n\n\\section{Related Work}\n\\subsection{Multi-modal Image-Text Pre-training}\nIn an effort to overcome the limitations of single-modality learning~\\cite{groupon}, multi-modal image-text pre-training has been introduced to learn and align visual and textual representations by pre-training the model on large-scale image-text pairs. \nOne of the most representative methods is CLIP~\\cite{clip} , which has achieved multi-modal alignment through contrastive learning with massive noisy data scrapped from the internet. BLIP-family~\\cite{BLIP, BLIP2} has continuously explored image-text unification and model lightweighting. Aiming to reduce the modality gap, LaVIT~\\cite{LaVIT} and FDT~\\cite{FDT} have respectively designed a unified discrete tokenizer to embed visual and textual input.\nLarge-scale image-text pre-training has become a widely used paradigm for learning general vision representations for a wide range of downstream tasks as well as for constructing multi-modality foundation models~\\cite{LLaVA, LLaVA-Med}. \nDespite their impressive performance, previous methods have only learned coarse-grained representations and ignored localized details.\nMotivated by this, SPARC~\\cite{SPARC} proposes a fine-grained sequence-wise loss., encoding detailed information in a computationally inexpensive way. \nFILIP~\\cite{filip} has constructed a cross-modal late interaction mechanism to optimize the token-wise maximal similarity between image and text. \nMGCA~\\cite{MGCA}, PRIOR~\\cite{PRIOR} and MedKLIP~\\cite{MedKLIP} learn complex semantic structures in medical reports from different levels (disease-level, sentence-level and entity-level).\nHowever, most of them are specifically tailored for image-text alignment, and cannot seamlessly be applied to multi-modal protein-biotext pre-training.\n\n\\subsection{Multi-modal Protein-Biotext Pre-training}\nRecently, models that jointly pre-train protein sequences and biotext descriptions have gradually drawing the attention of researchers. OntoProtein~\\cite{OntoProtein} first incorporates knowledge graphs to enhance protein representation with external biological descriptions. \nChroma~\\cite{chroma} conducts text-guided protein backbone editing towards desired properties and functions. Meanwhile, ProtDT~\\cite{ProtDT} is a newly proposed multi-modal framework that aligns the representations of proteins and biotexts, and leverages textual descriptions \nfor protein design. ProtST~\\cite{ProtST} has shown a tremendous performance on exploiting biomedical function annotations to enhance protein sequence understanding. Additionally, a novel multi-modal framework for the accurate prediction of protein functional descriptions in free text format is proposed by~\\cite{Prot2Text}. BioBridge~\\cite{BioBridge} introduces a bridge module to learn transformations between protein, molecule and biotext foundation models. Nevertheless, existing works of protein-biotext alignment primarily exploit the global alignment objective proposed by CLIP~\\cite{clip}, without utilizing protein specific functional mechanism to fully facilitate fine-grained understanding of protein and biotext.\n\n\\section{Conclusion}\nThis paper has accomplished data-efficient and function-informed multi-modal learning of proteins and biotexts. We build the \\ourdataset dataset with large-scale aligned protein sequences and functional descriptions. The property-driven sampling strategy is introduced to strike a balance between data quality and data quantity for pre-training, thereby facilitating the effective harnessing of large-scale noisy data. Inspired by the intricate mechanisms of protein functionality, we novelly adopt a function-informed pre-training paradigm with newly proposed segment-wise objectives to explicitly model protein static and dynamic segments. Such paradigm seamlessly integrates multi-modality information from coarse-grained to fine-grained levels, culminating in the holistic function-centric protein representation. We also identified that \\ourapproach achieves the new state-of-the-art results on 22 protein downstream benchmarks. In the future, we envision that \\ourapproach has the potential to serve as the protein multi-modality foundation model to promote controllable protein discovery and optimization in real-world scenarios. \n\n\\section{Acknowledgments}\nThis research was partially supported by National Natural Science Foundation of China under grants No. 12326612, Zhejiang Key R\\&D Program of China under grant No. 2023C03053, the Opening Foundation of the State Key Laboratory of Transvascular Implantation Devices, grant No. SKLTID2024003, and Alibaba Research Intern Program.\n\n\\clearpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2201.11147v6.tex",
        "arXiv-2301.12040v2.tex",
        "arXiv-2412.20014v1.tex"
    ],
    "group_id": "group_20",
    "response": "### Title: Advances in Protein Representation Learning through Multi-Modal and Knowledge-Enhanced Pre-Training\n\n### Introduction\n\nProtein representation learning is a burgeoning field within computational biology, aiming to understand and predict the structure, function, and interactions of proteins. This field has seen significant advancements with the advent of self-supervised learning techniques, particularly through the use of protein language models (PLMs) that leverage large-scale protein sequence data. These models have demonstrated remarkable capabilities in capturing the intricate grammatical and structural nuances of proteins, thereby enhancing their utility in downstream tasks such as protein-protein interaction prediction, protein function prediction, and contact prediction. However, despite these successes, there remain challenges in fully integrating external biological knowledge and in handling large-scale, noisy datasets effectively.\n\nThe integration of external knowledge graphs (KGs) and multi-modal data has been proposed as a means to address these challenges. Knowledge graphs, such as Gene Ontology (GO), provide structured biological knowledge that can be used to enhance protein representations. Similarly, multi-modal datasets that align protein sequences with textual descriptions of their functions offer a rich source of information for pre-training models. Yet, the effectiveness of these approaches in capturing fine-grained functional information and in mitigating the impact of noisy data remains an open question.\n\nThis summary examines three recent research papers that tackle these challenges through innovative pre-training frameworks. The first paper, \"OntoProtein: Protein Pretraining with Gene Ontology Embedding,\" introduces a method that integrates GO knowledge into protein pre-training. The second paper, \"Submission and Formatting Instructions for International Conference on Machine Learning (ICML 2023),\" provides general guidelines for electronic submissions to the conference, but is not directly relevant to the topic of protein representation learning. The third paper, \"Function-Informed Protein Multi-Modal Learning,\" proposes a new approach to multi-modal protein-biotext pre-training that addresses the limitations of existing methods by incorporating a function-informed paradigm and a property-driven sampling strategy.\n\n### Main Content of Each Paper\n\n#### OntoProtein: Protein Pretraining with Gene Ontology Embedding\n\nThe paper \"OntoProtein: Protein Pretraining with Gene Ontology Embedding\" introduces a novel approach to pre-training protein language models by integrating knowledge from Gene Ontology (GO). GO is a structured knowledge graph that provides detailed information about molecular functions, cellular components, and biological processes. The authors argue that by incorporating GO knowledge, protein models can better capture the biological context and functional mechanisms of proteins.\n\nThe OntoProtein framework consists of two main components: a hybrid encoder that represents both protein sequences and GO descriptions, and a contrastive learning approach that employs knowledge-aware negative sampling to optimize the embeddings of both proteins and GO entities. The hybrid encoder uses a pre-trained ProtBert model for protein sequences and a pre-trained BERT model for GO descriptions. The contrastive learning objective is designed to ensure that the embeddings of proteins and GO entities are aligned in a way that reflects their biological relationships.\n\nThe authors construct a large-scale knowledge graph called ProteinKG25, which contains approximately 612,483 entities and 4,990,097 triples. This dataset is used for pre-training the OntoProtein model. They evaluate the effectiveness of their approach on various downstream tasks, including protein function prediction, protein-protein interaction prediction, and contact prediction. The results show that OntoProtein outperforms existing models in most tasks, demonstrating the benefits of integrating structured biological knowledge into protein pre-training.\n\n#### Function-Informed Protein Multi-Modal Learning\n\nThe paper \"Function-Informed Protein Multi-Modal Learning\" addresses the limitations of existing multi-modal protein-biotext pre-training approaches by proposing a new framework that explicitly models the functional segments of proteins. This framework, called \\ourapproach, is designed to capture both coarse-grained and fine-grained information from protein sequences and their corresponding functional descriptions.\n\nThe authors first curate a large-scale dataset called \\ourdataset, which contains 251.5 million aligned pairs of protein sequences and functional descriptions. This dataset is derived from the existing protein function database and is organized into a sparse version (\\ourdatasetS) and a dense version (\\ourdatasetD). The dense version (\\ourdatasetD) is used for large-scale pre-training and includes a property-driven sampling strategy to balance data quality and quantity.\n\nThe \\ourapproach framework consists of a protein encoder and a biotext encoder, both initialized with pre-trained models (ESM-2-650M and PubMedBERT, respectively). The authors introduce two segment-wise objectives: Biotext-guided Static Segment Reconstruction (BSR) and Property-grouped Dynamic Segment Alignment (PDA). BSR focuses on reconstructing static segments of protein sequences using information from both modalities, while PDA aligns dynamic segments with property descriptions in an unsupervised manner. These objectives are designed to capture the localized functional details of proteins, which are often neglected in existing coarse-grained alignment methods.\n\nThe overall loss function of \\ourapproach includes a global contrastive loss (GC) for coarse-grained alignment, a BSR loss for static segment reconstruction, a PDA loss for dynamic segment alignment, and a protein masked language modeling (MLM) loss to preserve unimodal knowledge. The authors conduct extensive experiments on 22 protein benchmarks across five types of tasks, including protein classification engineering, mutation effect prediction, cross-modal transformation, semantic similarity inference, and protein-protein interaction prediction. The results show that \\ourapproach consistently achieves state-of-the-art performance across all tasks, highlighting the effectiveness of their function-informed pre-training paradigm.\n\n### Commonalities and Innovations\n\nBoth \"OntoProtein: Protein Pretraining with Gene Ontology Embedding\" and \"Function-Informed Protein Multi-Modal Learning\" aim to enhance protein representation learning by incorporating external knowledge and multi-modal data. However, they differ in their approaches and the specific challenges they address.\n\n- **Commonalities:**\n  - Both papers leverage large-scale datasets to pre-train their models.\n  - They both use contrastive learning objectives to align protein sequences with external knowledge or descriptions.\n  - Both models are designed to be compatible with existing protein language models, allowing for easy integration into downstream tasks.\n\n- **Innovations:**\n  - **OntoProtein:**\n    - Integrates structured biological knowledge from Gene Ontology (GO) into protein pre-training.\n    - Employs a contrastive learning approach with knowledge-aware negative sampling to optimize embeddings.\n    - Constructs a large-scale knowledge graph (ProteinKG25) for pre-training.\n  - **Function-Informed Protein Multi-Modal Learning:**\n    - Proposes a property-driven sampling strategy to balance data quality and quantity.\n    - Introduces a function-informed pre-training paradigm that explicitly models static and dynamic functional segments.\n    - Uses a combination of global contrastive loss, BSR, PDA, and MLM to capture both coarse-grained and fine-grained information.\n\n### Comparison of Results\n\nBoth papers demonstrate significant improvements over existing baselines in various downstream tasks. However, there are differences in the specific tasks and benchmarks they evaluate on.\n\n- **OntoProtein:**\n  - Achieves better performance than existing models in protein function prediction, protein-protein interaction prediction, and contact prediction.\n  - The improvements are modest in sequence-level tasks such as homology and stability prediction, which are regression tasks.\n  - The authors note that the gains are still relatively small due to the limited coverage of ProteinKG25.\n\n- **Function-Informed Protein Multi-Modal Learning:**\n  - Consistently achieves state-of-the-art results across 22 protein benchmarks.\n  - Shows significant improvements in cross-modal transformation tasks, achieving up to 75% better performance than existing methods.\n  - Demonstrates superior performance in protein function prediction, with improvements of 59.9% in GO-CC and 39.7% in GO-BP.\n  - Outperforms baselines in semantic similarity inference and protein-protein interaction prediction, indicating the effectiveness of the function-informed paradigm.\n\n### Conclusion\n\nThe papers \"OntoProtein: Protein Pretraining with Gene Ontology Embedding\" and \"Function-Informed Protein Multi-Modal Learning\" represent significant advancements in protein representation learning by integrating external knowledge and multi-modal data. OntoProtein leverages structured biological knowledge from Gene Ontology to enhance protein embeddings, while \\ourapproach introduces a function-informed paradigm to capture localized functional details of proteins. Both approaches demonstrate the potential of incorporating external knowledge and multi-modal data into protein pre-training, leading to improved performance in downstream tasks.\n\nThe main findings of these papers include:\n- The effectiveness of integrating structured biological knowledge into protein pre-training.\n- The importance of balancing data quality and quantity in large-scale pre-training.\n- The benefits of explicitly modeling static and dynamic functional segments in protein sequences.\n\nFuture research directions could include:\n- Further refining the sampling strategies to better handle noisy data.\n- Exploring the integration of additional types of biological knowledge beyond GO.\n- Extending the function-informed paradigm to other types of multi-modal data, such as integrating protein sequences with other types of biological descriptions or data.\n- Investigating the potential of these models in real-world applications, such as drug discovery and protein design.\n\nBy addressing these challenges, the field of protein representation learning can continue to advance, leading to more accurate and comprehensive models that can better understand and predict the behavior of proteins in biological systems."
}