{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate}\n\n\\begin{document}\n\n\\title{EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate}\n\n\\author{\n\\centering\n\\IEEEauthorblockN{Xiaonan Nie, Xupeng Miao, Zhi Yang, Bin Cui}\n\\IEEEauthorblockA{\nDepartment of Computer Science, Peking University, Beijing, China \\\\\n\\{xiaonan.nie, xupeng.miao, yangzhi, bin.cui\\}@pku.edu.cn}\n}\n\\author{\\IEEEauthorblockN{\nXiaonan Nie$^{\\dag}$~~~~~Xupeng Miao$^{\\dag\\$}$~~~~~Shijie Cao$^\\ddagger$~~~~~Lingxiao Ma$^\\ddagger$~~~~~Qibin  Liu$^\\dag$~~~~~Jilong Xue$^\\ddagger$ \\\\ Youshan Miao$^\\ddagger$~~~~~~Yi Liu$^\\#$~~~~~~Zhi Yang$^{\\dag}$~~~~~Bin Cui$^{\\dag\\S}$}\n\\IEEEauthorblockA{\\textit{$^{\\dag}$School of Computer Science \\& Key Laboratory of High Confidence Software Technologies (MOE), Peking University} \\\\\n\\textit{$^\\$$Carnegie Mellon University}\\quad \n\\textit{$^\\ddagger$Microsoft Research}\\quad \\textit{$^\\#$Tencent Inc}\\\\\n\\textit{$^\\S$Institute of Computational Social Science, Peking University (Qingdao), China}\\\\\n\\textit{\\{xiaonan.nie, xupeng.miao, 1700012767, yangzhi, bin.cui\\}@pku.edu.cn}\\\\\n\\textit{\\{lingxiao.ma, shijiecao, jxue, yomia\\}@microsoft.com}~~~~~~\\textit{callbackliu@tencent.com}}\n}\n\n\\maketitle\n\n\\begin{abstract}\nMixture-of-experts (MoE) is becoming popular due to its success in improving the model quality, especially in Transformers.\nBy routing tokens with a sparse gate to a few experts (i.e., a small pieces of the full model), MoE can easily increase the model parameters to a very large scale while keeping the computation cost in a constant level.\nMost existing works just initialize some random experts, set a fixed gating strategy (e.g., Top-$k$), and train the model from scratch in an ad-hoc way. We identify that these MoE models are suffering from the immature experts and unstable sparse gate, which are harmful to the convergence performance.\n\nIn this paper, we propose an efficient end-to-end MoE training framework called \\evomoe. \\evomoe starts from training one single expert and gradually evolves into a large and sparse MoE structure.\n\\evomoe mainly contains two phases: the \\textit{expert-diversify} phase to train the base expert for a while and spawn multiple diverse experts from it,\nand the \\textit{gate-sparsify} phase to learn an adaptive sparse gate and activate a dynamic number of experts.\n\\evomoe{} naturally decouples the joint learning of both the experts and the sparse gate and focuses on learning the basic knowledge with a single expert at the early training stage. Then it diversifies the experts and continues to train the MoE with a novel Dense-to-Sparse gate (DTS-Gate). Specifically, instead of using a permanent sparse gate, DTS-Gate begins as a dense gate that routes tokens to all experts, then gradually and adaptively becomes sparser while routes to fewer experts.\nEvaluations are conducted on three popular models and tasks, including RoBERTa for masked language modeling task, GPT for language modeling task and Transformer for machine translation task. \nThe results show that \\evomoe{} outperforms existing baselines, including Switch, BASE Layer, Hash Layer and StableMoE. Specifically, \\ourmethods{} outperforms other MoE methods on GLUE benchmark up to 0.562 and 0.403 on average. \nOur code is available \\footnote{https://github.com/codecaution/EvoMoE}.\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nDeep Learning, Transformer, Mixtures of Experts, Dense to Sparse.\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:intro}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{Figures/overview.pdf}\n    \\caption{Illustration on the workflow of \\ourmethods{}, which contains two phases: (1) expert-diversify phase and (2) gate-sparsify phase. In the first stage, we train one shared-expert instead of $N$ individual experts and then adopts $diversify$ functions (i.e., random masking) to spawn multiple diverse experts from the shared expert. In the second stage, we propose the Dense-to-Sparse gate, which starts as a dense gate that routes tokens to most experts and then gradually becomes sparser. Different from previous TopK-based gates, we propose the content-based gating mechanism, which activates experts whose weight is beyond the threshold.} \n    \\label{fig:overview}\n\\end{figure*}\n\nThe Transformer model architecture is becoming increasingly important in data mining and has achieved impressive results in a wide range of applications, such as natural language processing~\\cite{DBLP:conf/nips/gpt3}, computer vision~\\cite{DBLP:ViT}, graph learning~\\cite{graphormer}, and recommendation systems~\\cite{chen2019behavior}. Recently, there is a trend of improving the capability of Transformer models through enlarging data and model scales \\citep{DBLP:conf/nips/gpt3}. Specifically, \\cite{DBLP:scaling_laws} explored the scaling law of transformer models which shows that the model performance scales as a power-law with data sizes, model sizes and the computation. However, with the rapid increasing of the model sizes, it is hard to further scale the model to extremely large sizes due to the limited computation power of available hardware devices.\nTo address these challenges, sparsely-gated Mixture-of-Experts (MoE), a popular form of conditional computation, has been proposed to increase the model size while without increasing the computational cost (e.g., FLOPs) proportionally~\\citep{DBLP:journals/corr/BengioLC13, DBLP:lstm_moe, DBLP:switch, DBLP:gshard, lewis2021base, roller2021hash}. \nSpecifically, the input tokens are routed by a sparse gate to a few experts, leading to lower computational costs compared to a dense model with the same model size.\n\nThe success of MoE model relies on both the large model capacity introduced by plenty of experts and the sophisticated sparse routing connections learned by the gate network. \nMany existing works~\\citep{DBLP:switch, DBLP:gshard, lewis2021base, roller2021hash} are exploring novel gating networks to improve the model quality or the training efficiency.\nThey typically adopt a pre-defined sparse gate architecture (e.g., Top-1 or Top-2 with a fixed number of activated experts), and then train the model parameters of both the gate and experts jointly from scratch.\nHowever, such \\textit{joint-training over pre-defined sparse architecture} could severely limit the model quality, and even the training efficiency.\nParticularly, at the beginning of training a MoE model, both the gate and the experts are randomly initialized. The gate does not have evidence to decide which expert to process an input token, and the experts also do not have experiences to process a randomly-assigned input token. Training all experts from a random state with random routed samples requires a long and duplicated warming-up process.\nFurthermore, these pre-defined gates limit the MoE to explore only 1 or 2 experts at a time. But in the early stage with a immature gate, such small opportunities could be easily  influenced by the random routing noises, and the improper routing could even be reinforced for a long time.\nOur observation shows that such random routing in the initial stage and long-distance reinforce-based model updating in existing approaches could affect both the training time and final model quality.\n\nIn this paper, to overcome the limitations in existing approaches, we revisit the learning process of MoE models and advocate a simple but effective end-to-end training paradigm, named \\evomoe{}. \nInstead of directly training from a pre-defined sparse architecture, \\evomoe{} gradually evolves into a diverse and sparse MoE architecture from an initial model in two phases: \\textbf{expert-diversify} and \\textbf{gate-sparsify}.\nSpecifically, we find that both the gate and the experts are underachieving in MoE training thus resulting in unstable routing performance. Motivated by the successes of weight-sharing models, in the first stage, we introduce the expert-diversify mechanism to learn the commonly shared knowledge across different experts.\nOur proposed mechanism only trains one common experts with all input tokens at first, which could be seen as sharing the model parameters across all the experts.\nTo involve the diversity of these experts, we then randomly perturb each expert with different masks as the initial model state of the following training steps.\nIn the gate-sparsify phase, the weight-sharing constraint is released and the training of MoE turns to the sparsely activated manner over these diverse experts.\nUnlike the pre-defined sparse gate in the previous works, we introduce DTS (Dense-to-Sparse) gate to decide the sparse gate gradually for MoE models. We proposed DTS gate to adaptively learn a better gating network from a dense one and gradually route tokens to fewer experts, making the training structure sparser and continuously reducing the computation cost, while keeping the model quality improving as usual.\nIn particular, to implement the DTS gate, our idea is to carefully control the temperature of a softmax-based routing function, so that to adjust the weights distribution among experts and control the sparsity of the MoE layer during training.\n\nIn short, \\evomoe advances in two aspects. First, compared to the \\textit{joint-training of gate and experts} from scratch, \\evomoe splits joint-training process and provides an opportunity to train experts during a warm start. Such mechanism of \\textit{training gate after experts} can reduce a lot of random error-trails at the beginning. Second, compared to the reinforce-based model updating, starting with a dense gate allows us to get training feedback from all diverse experts and adjust the routing weights directly to the right direction, which not only speeds up the convergence of the gate, but also benefits for the expert specialization.\n\nWe evalute \\evomoe{} on three popular models and tasks, including RoBERTa~\\cite{liu2019roberta} (Encoder-Only) for masked language modeling (MLM) task, GPT~\\cite{gpt2} (Decoder-Only) for language modeling (LM) task and Transformer~\\cite{DBLP:conf/nips/VaswaniSPUJGKP17} (Encoder-Decoder) for machine translation (MT) task. \nThe results show that \\evomoe{} outperforms existing baselines, including Switch~\\cite{DBLP:switch}, BASE Layer~\\cite{DBLP:conf/icml/baselayer}, Hash Layer~\\cite{roller2021hash} and StableMoE~\\cite{DBLP:conf/acl/stablemoe}. Specifically, on MLM task, \\ourmethods{} outperforms other MoE methods up to 0.562 GLUE score and 0.403 in average for the GLUE benchmark~\\cite{DBLP:conf/emnlp/glue}; \non LM task,  \\ourmethods{} outperforms other MoE methods up to 0.88 ppl and 0.545 ppl on average;\non translation task, \\evomoe{} can averagely achieve 1.0 BLEU score improvement as well as 1.33x speed-up that Switch Transformer.\nExperiments also verify the ability of \\evomoe for scaling models with more experts or more MoE layers.\n\nThe rest of the paper is organized as follows. We first introduce the background of Transformers and MoEs in Section~\\ref{sec:preliminary}. And we identify two key defects in existing MoE training process including the conformity and instability in Section~\\ref{sec:motivation}. Motivated by these properties, we present our \\evomoe design in Section~\\ref{sec:methods} and introduce the expert-diversity stage and gate-sparsify stage respectively. Section~\\ref{sec:impl} describes some implementation details.\nWe provide the evaluation methodology and conduct substantial experiments under various settings in section~\\ref{sec:experiment} to support our claims. More relevant approaches are introduced in Section~\\ref{sec:related_work}. Finally, we provide some concluding remarks in section~\\ref{sec:conclusion}.\n\n\\begin{figure}[t]\n    \\centering\n    \\subfigure[Transformer Layer]{\n            \\includegraphics[width=0.13\\textwidth]{Figures/preliminary/transformer_layer.pdf}\n            \\label{fig:transformer}\n    }\n    \\subfigure[Transformer Layer with Mixture-of-Expert]{\n            \\includegraphics[width=0.32\\textwidth]{Figures/preliminary/moe_transformer_layer.pdf}\n            \\label{fig:moe_transformer}\n    }\n    \\caption{A brief architecture of Transformer Encoder Layer and Transformer with Mixture-of-Expert Layer. The Transformer encoder layer contains two main components: a Multi-Head Self-Attention Layer and a Position-wise Feed-Forward Layer. Based on Transformer layer, the transformer with MoE replaces the FFN with a series of FFNs and introduce a gate network.}\n    \\label{fig:preliminary}\n\\end{figure}\n\n\\section{Preliminary}\n\\label{sec:preliminary}\n\\subsection{Transformer}\n\nThe model architecture of Transformer~\\cite{vaswani2017attention} has demonstrated its superior performance in many sequence-to-sequence natural language processing (NLP) tasks, which contains several encoder layers and decoder layers. Each encoder layer is stacked by a multi-head self-attention network and a position-wise feed-forward network (FFN), which is illustrated in Figure~\\ref{fig:transformer}. \nIt employs a residual connection on each of these two sub-layers, followed by a normalization layer~\\cite{DBLP:journals/corr/layernorm}. Formally, each sub-layer, e.g., attention and FFN, produces its output as $\\text{LayerNorm}(x + \\text{Sublayer}(x))$.\nThe decoder is similarly constructed, except for an additional cross-attention mechanism between attention and FFN to introduce the output of the encoder. For a sequence of input tokens $(x_{1},...,x_{s})$ $\\in$ $\\mathbb{R}^{D}$, we formulate the function of each sub-layer in following:\n\n\\begin{table}[t]\n\\centering\n\\renewcommand{\\multirowsetup}{\\centering}\n\\caption{Notations}\n\\label{tab:notation}\n\\vspace{-2mm}\n\\begin{tabular}{cc}\n\\toprule \nSymbols & Definitions \\\\\n\\midrule\n$Q$ & Queries in self-attention module \\\\\n\\midrule\n$K$ & Keys in self-attention module \\\\\n\\midrule\n$V$ & Values in self-attention module\\\\\n\\midrule\n$d_k$ & Feature dimension of each Query/Key \\\\\n\\midrule\n$S$ & A set of input tokens \\\\\n\\midrule\n$E$ & An series of experts\\\\\n\\midrule\n$D$ & Feature dimension of each token \\\\\n\\midrule\n$N$ & Number of experts in each MoE layer \\\\\n\\midrule\n$e_{i}(x_s)$ & The output of $i$-$th$ Expert by taking input $x_s$ \\\\\n\\midrule\n$g(x_s)_i$ & The routing score for $x_s$ on $i$-$th$ Expert\\\\\n\\midrule\n$c$ & Threshold for expert selection \\\\\n\n\\midrule\n$G_{S, E}$ & The routing score for $S$ on $E$ \\\\\n\\midrule\n$Id_{S}$ & The set of selected expert id on $S$\\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\textbf{Attention:} The attention module~\\cite{DBLP:conf/nips/VaswaniSPUJGKP17} could capture the dependencies between tokens in the sequence, and is effective in sequence modeling. It performs as a ternary function, which maps the input queries (Q), keys (K) and values (V) to the output (O). Equation~\\ref{equ:attention} represents the Scaled Dot-Product Attention~\\cite{DBLP:conf/nips/VaswaniSPUJGKP17}, which performs dot products of each query with all keys, divides each by $\\sqrt{d_k}$ and then adopts the softmax function to get the weight of each value. In addition, $d_k$ is the dimension of queries and keys.\n\\begin{equation}\n    \\texttt{Attention}(Q, K, V) = \\texttt{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n\\label{equ:attention}\n\\end{equation}\n\n\\textbf{Multi-Head Attention:} ~\\citet{DBLP:conf/nips/VaswaniSPUJGKP17} proposed the multi-head attention mechanism to jointly learn from different representation subspaces at different positions and thus improved the model performance. The multi-head attention linearly projected the queries, keys and values $h$ times with learned linear projections to $d_k$, $d_k$ and $d_v$, dimensions, respectively. \n\n\\begin{align}\n    \\texttt{MultiHead}&(Q, K, V) = \\texttt{Concat}(head_1, ..., head_h){W^O} \\\\\n    \\text{where}&\\ head_i = \\texttt{Attention}(QW_{i}^Q, KW_{i}^K, VW_{i}^V) \\nonumber\n\\label{equ:multi_head}  \n\\end{align}\n\n\\noindent The projections are the trainable parameter matrices, where $W_{i}^Q \\in \\mathbb{R}^{d_{model} \\times d_k}$, $W_{i}^K \\in \\mathbb{R}^{d_{model} \\times d_k}$, $W_{i}^V \\in \\mathbb{R}^{d_{model} \\times d_v}$. Meanwhile, $h$ is the number of heads, and $d_k = d_v = d_{model}/h$. Because the dimension of each head is reduced from $d_{model}$ to $d_{model}/h$, the time cost of multi-head attention is similar to that of the original attention. In addition, the decoder employs a masked self-attention, which only sees the tokens on the left of the sequence.\n\n\\textbf{Position-wise Feed-Forward Networks:} Each transformer layer also includes a fully connected feed-forward network (Equation~\\ref{equ:ffn}), which consists of two fully connected networks and a ReLU activation function.\n\n\\begin{equation}\n    \\texttt{FFN}(x_{s}) = W_{2} \\cdot \\texttt{ReLU}(W_{1} \\cdot x_{s} + b_{1}) + b_{2}\n\\label{equ:ffn}\n\\end{equation}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.415\\textwidth]{Figures/preliminary/moe_layer.pdf}\n    \\caption{Illustration on the workflow of Mixture-of-Expert (MoE). The input token $x_s$ is first processed by the gate network to calculate similarities between $x_s$ and each expert. Then, it performs Top-K operation on these similarities to determine the target activated experts. Finally, $y_s$ is produced by the linearly weighted combination of each expert's output on the token by the gate's output. }\n    \\label{fig:moe_layer}\n\\end{figure}\n\n\\begin{figure*}[t]\n    \\centering\n    \\subfigure[Expert Loads Distribution .]{\n            \\includegraphics[width=0.22\\textwidth]{Figures/expert_loads_conformity.pdf}\n            \\label{fig:duration_distribution}\n    }\n    \\subfigure[Unstable Routing Pattern for Token ``the'']{\n            \\includegraphics[width=0.68\\textwidth]{Figures/expert_selection.pdf}\n            \\label{fig:weights_distribution}\n    }\n    \\caption{The observations of GPT-MoE with 16 experts and the Top-1 gate among 12 MoE-layers (totally 24 layer).\n    Figure~\\ref{fig:duration_distribution} shows the expert load distribution (deeper color represents heavier workload, i.e., more tokens to be processed) and Figure~\\ref{fig:weights_distribution} shows the expert selection for a specific token ``the''.}\n    \\label{fig:dense_moe}\n\\end{figure*}\n\n\\subsection{Mixture of Experts}\nBecause larger pretrained models always achieve better model quality~\\cite{DBLP:scaling_laws}, the size of state-of-the-art NLP models has been increasing $10\\times$ per year, e.g., BERT~\\cite{DBLP:conf/naacl/BERT}, GPT~\\cite{gpt}, T5~\\cite{DBLP:googleT5}, GPT-2~\\cite{gpt2}, GPT-3~\\cite{DBLP:conf/nips/gpt3}, which require increasing compute budgets. To improve the model capacity without increasing computation budgets, researchers sparsely scale transformers recently as Figure~\\ref{fig:moe_transformer} by replacing the feed-forward network with the mixture of experts (MoE) architecture and activating only a subset of these experts for each input sample~\\cite{DBLP:lstm_moe, DBLP:switch, DBLP:gshard}.\nThe main components of the MoE architecture include an expert network $E$ for scaling model capacity and a sparse gate network $G$ for introducing model sparsity. \n\n\\textbf{Expert Network:} \nThe expert network $E$ includes a series of experts $\\{e_{1},...,e_{N}\\}$ to increase the model capacity, where each expert $e_{i}$ represents a single neural network, e.g., \\text{FFN}, and contains its own parameters. In Figure~\\ref{fig:moe_layer}, the MoE layer consists of three FFN networks. \nFor each expert $e_{i}$ ($e_{i}: \\mathbb{R}^{D} \\rightarrow \\mathbb{R}^{D}$), it takes the token $x_{s}$ as an input to produce its own output $e_{i}(x_{s})$. The final output of the expert network $y_{s}$ is the linearly weighted combination of each expert’s output on the token by the gate's output, formulated as Equation \\ref{equ:moe_weighted_sum}. \n\\begin{equation}\n    y_{s} = \\sum_{i=1}^{N}g(x_{s})_{i} \\cdot e_{i}(x_{s})\n\\label{equ:moe_weighted_sum}\n\\end{equation}\n\\noindent In Figure~\\ref{fig:moe_layer}, the expert network takes the input token $x_{s}: [-0.2, 0.4, 1.5]$ and produces the output of each individual expert on $x_s$, e.g., $e_{0}(x_s)$, $e_{1}(x_s)$ and $e_{2}(x_s)$. By combining the gate's output, i.e., $[0.35, 0.65, 0]$, the output of this MoE layer is $y_s = 0.35*e_{0}(x_s) + 0.65*e_{1}(x_s)$.\n\n\\textbf{Sparse Gate Network:}\nThe sparse gate network $G$ is the key component to introduce model sparsity, which takes a batch of tokens $\\{x_{1},...,x_{s}\\}$ as input and produces the probability of them with respective to all experts $\\{e_{1},...,e_{N}\\}$.\n\\citet{DBLP:lstm_moe} proposes the Top-K gating as Equation~\\ref{equ:topk_gate}, which keeps only the top k values before the softmax function. In addition, $W_{g}$ is a trainable variable ($W_{g} \\in \\mathbb{R}^{D \\times N}$) and determine the targeted experts for each token.\n\n\\begin{equation}\n    g(x_s) = softmax(TopK(x_{s} \\cdot W_{g}) )\n\\label{equ:topk_gate}\n\\end{equation}\n\\noindent We illustrate the workflow of a MoE layer in Figure~\\ref{fig:moe_layer}, where $k=2$ and $W_g$ is a 3 $\\times$ 3 (i.e., feature dimension $\\times$ number of experts) matrix to represents the parameter of gate network. We first perform a dot-product on $x_s$ and $W_g$ to calculate similarity between the input token and the experts. The result, $[2.01, 2.64, 1.8]$, indicates that the input prefers $e_1 > e_0 > e_2$ and we only activate $e_0$ and $e_1$ as $k = 2$. Finally, we conduct a softmax function to get the weight score of each expert and perform a weighted sum to get the final output $y_s$. \n\nPrevious work mainly focuses on how to improve the quality and efficiency of training such sparse gate network. ~\\citet{DBLP:lstm_moe} proposed the noisy Top-K gating on Long Short-TerM memory (LSTM) kayers~\\cite{DBLP:journals/neco/lstm} and ~\\citet{DBLP:gshard} introduced MoE with Top-2 gate into Transformer.\n~\\citet{lewis2021base} adopted the numerous solution for balanced  token-to-expert routing and \\citet{roller2021hash} utilized the hash-based routing strategy.\n\n\\textbf{Distributed Training of MoE Models:}\nExpert parallel training is a specific method of parallelism for MoE models, which is first proposed by GShard~\\cite{DBLP:gshard}. Experts are placed on different workers and each worker takes a different batch of training samples. For non-MoE layers, expert parallelism behaves the same as data parallelism. In MoE layers, tokens in the sequence are sent to workers where their desired experts reside. Similar to model parallelism, the outputs of each MoE layer are exchanged again to be organized back into original sequences for the computation of the next layer. As MoE models often have numerous experts, expert parallelism can scale up with model size better than model parallelism.\n\n\\subsection{Observation and Motivation}\n\\label{sec:motivation}\nIn this section, we revisit the learning process of MoE models and introduce our two key findings in the following, which motivates us to design our \\evomoe framework.\n\n\\textbf{Conformity in Mixture of Experts:} One interesting finding is conformity. During the early training stage, existing join-training methods of sparse MoE make the routing decision to comply with most tokens.\nHere we train a GPT model including 24 transformer layers, with every FFN layer replaced by 16-expert MoE layer using the Top-1 gate. \n\nFigure~\\ref{fig:duration_distribution} shows that most tokens keep concentrating on the 8-th expert at first, since it has been greedily reinforced.\nAfter around hundreds of training steps (i.e., 1 duration equals 40 training steps), the other experts gradually catch up and the workload becomes balanced. Such phenomenon motivates us to focus on training a common expert and utilize the computational resources to accelerate the early stage.\n\n\\textbf{Instability in Mixture of Experts:} Another important finding is the instability. We take a single token ``the'' as an example and Figure~\\ref{fig:weights_distribution} shows its expert selection results for a longer training process. As we can see, the selection is highly unstable later since both the gate network and the experts are not knowledgeable enough to obtain a stable routing pattern, especially at the early stage of the training process. This indicates that a pre-defined gates (e.g., Top-K) in existing works, which assumes a fixed number of activated experts, could limit the exploration of potential valuable experts. Aggressively increasing the number of activated experts could improve the model capacity but inherently violates the original design intention of sparse MoE. Such a dilemma motivates us to design an adaptive solution to balance the trade-off between the convergence performance and computation costs.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.25\\textwidth]{Figures/shared_to_diverse.pdf}\n    \\caption{\n    To spawn multiple diverse experts from the shared-expert, \\ourmethods{} adopt the random masking technique. Specifically, \n    part of the shared expert's weight are masked as 0.}\n    \\label{fig:shared_to_diverse}\n\\end{figure}\n\n\\section{Methods}\n\\label{sec:methods}\nThe observations in Section~\\ref{sec:motivation} motivates EvoMoE, a two-phase framework that gradually and adaptively training MoE-based models, which is different from existing methods that jointly train the gate network and the expert network over a pre-defined sparse (e.g., Top-1 or Top-2) gate and a series of randomly initialized experts.\nAs shown in Figure~\\ref{fig:overview}, EvoMoE contains two phases: an expert-diversify phase and a gate-sparsify phase.\nIn the expert-diversify phase, \\ourmethods{} shares the weights among experts in one MoE layer for several training steps and then makes experts diverse by randomly masking.  \nIn the gate-sparsify phase, \\ourmethods{} introduces the dense-to-sparse (i.e., DTS) gate, which begins routing as a dense gate that routes tokens to all experts and then adaptively learns the weights of routing to each expert and gradually anneals to standard Top-1 gating.\n\n\\subsection{Problem Formulation}\nGiven an input token $x_s$, a series of experts $\\{e_1,..., e_N\\}$ and a learn-able gate with parameter $W_g$, \\texttt{Func} is adopted by the gate network to determine the targeted experts for it, i.e., the token-to-expert assignment, formulated in Equation~\\ref{equ:gate_problem}. $g(x_s)$ is a 1$\\times$N vector, which represents the scores of $x_s$ with respect to experts.\nMeanwhile, each expert will process the input token separately as $e_i(x_s)$ and combine their output as Equation~\\ref{equ:moe_weighted_sum_problem}.\n\n\\begin{equation}\n    g(x_s) = \\texttt{Func}(x_{s} \\cdot W_{g})\n\\label{equ:gate_problem}\n\\end{equation}\n\n\\begin{equation}\n    y_{s} = \\sum_{i=1}^{N}g(x_{s})_{i} \\cdot e_{i}(x_{s})\n\\label{equ:moe_weighted_sum_problem}\n\\end{equation}\n\nExisting work adopts a pre-defined Top-K as $Func$, such as Top-2 for GShard ~\\cite{DBLP:gshard} and Top-1 for Switch-Transformer~\\cite{DBLP:switch}. However, due to the non-derivability of Top-K, only the selected experts would back-propagate their gradients to the gate network and update their corresponding columns in $W_g$.\nFor example, only 1 expert is selected and 1 column of the gate would be updated in Switch-Transformer. So it is hard for Top-K gate to optimize this \\textbf{expert-selection} problem.\nMoreover, as observed in Figure~\\ref{fig:duration_distribution}, the loads of experts are extremely imbalanced at the early stage of training and thus most GPUs suffer from low utilization due to stragglers in expert parallelism. \n\n\\setlength{\\textfloatsep}{0.1cm}\n\\begin{algorithm}[t]\n\t\\SetAlgoLined\n\t\\SetKwProg{Fn}{Function}{}{end}\n\t\\KwData{$x_{S}$: a group of tokens of size $S$, \\\\ \n\t\\quad \\quad $E$: expert network \\\\\n\t\\quad \\quad $T_{S}$: number of iterations for shared-expert, \\\\\n\t\\quad \\quad $T_{D}$: number of iterations for dense-gate, \\\\\n\t\\quad \\quad $T$: number of training iterations.}\n\t\\SetInd{0.61em}{0.61em}\n\t\\For{$i \\leftarrow 1\\ to\\ T_{S}$}{\n        $y_{S} \\leftarrow e(x_{S})$ \\;\n    }\n    // Diversify experts from the shared \\;\n\t\\For{$e_{i} \\in E$}{\n\t    $e_{i} \\leftarrow diversify(e, i)$ \\;\n\t}        \n\t\\For{$i \\leftarrow T_{S}\\ to\\ T$}{\n\t$\\tau \\leftarrow \\text{\\textit{temperature scheduler}}(i)$ \\;\n\t// Get selected expert ids and weights for each token \\;\n\t$G_{S,\\ E}, Id_{S} \\leftarrow DTS\\_Gate(x_{S},\\ \\tau, T_{D})$\\;\n\t\\For{$s \\leftarrow 1\\ to\\ S$}{\n\t    $y_{s} \\leftarrow 0$\n    \t\\For{$id \\in id_{s}$}{\n    \t    $y_{s} \\leftarrow y_{s} + G_{s, \\ id} * e_{id}(x_{s})$ \\;\n    \t}\n\t}    \t\n\t}\n\\caption{Training MoE in the \\evomoe{} Framework}\n\\label{alg:std_algo}\n\\end{algorithm}\n\n\\subsection{Stage 1: Expert-Diversify}\nAs the gate network and expert network are both randomly initialized, it requires a vast amount of computation budget for trial and errors, which is inefficient for models' training. Based on the observation in Section~\\ref{sec:motivation} that most tokens are processed by the same expert and other experts waste their computation budget, we train one shared-expert instead of $N$ individual experts in the early stage (illustrated as the left of Figure~\\ref{fig:overview}). Because all experts within the same MoE layer share weights, the model is equal to its corresponding non-MoE model as a small dense model.\n\nAlgorithm~\\ref{alg:std_algo} illustrates the MoE training process in our \\ourmethods{} framework. First, input tokens are processed by the shared expert $e_{0}$ (line 1-2). Then \\ourmethods{} switches the training into standard MoE models' training, by adding a gate network at each MoE layer and diversifying all experts from the shared expert (line 4-5). After this expert-diversify phase, \\ourmethods{} steps into the gate-sparsify phase, where it schedules the gate temperature coefficients and then obtains the token-to-expert routing relation from DTS-gate (line 7-9). Tokens will be dispatched to corresponding experts and aggregated together by weighted sum operating (line 10-12).\n\nMultiple $diversify$ techniques can be adopted to spawn multiple diverse experts from one expert, such as noise, NAS, random mask.\n\\ourmethods{} adopts the random mask, which masks part of the shared expert's weights as 0 (shown as Figure~\\ref{fig:shared_to_diverse}). For example, expert-1 is initialized by masking the central value. The proposed expert-diversify stage avoids joint training from scratch and the well-trained diverse experts could be treated as a better initialization to benefit the following model convergence.\n \n\n\\subsection{Stage 2: Gate-Sparsify}\nAlthough sparse gating has demonstrated its superior model efficiency in both training and inference, prior work tends to convergence to a sub-optimal model under the fixed computation budget or the dataset size due to the jointly training of the randomly initialized gate network and expert network.   \nIn this paper, we propose a new mechanism for training the gate network, named \\texttt{Dense-to-Sparse} gate (DTS-Gate, as illustrated in Algorithm~\\ref{alg:dts_algo} ), which starts as a dense gate that routes tokens to most experts and then gradually becomes sparser. \nDTS-Gate benefits from the sufficient training of experts in the early stage and then make the experts selection becomes sparser on the basis of specialized experts.\nThis dense-to-sparse process only occupies a small fraction compared with the total training time, which usually takes days to weeks.\n\n\\textbf{Gate with Temperature:}\nIn order to control the sparsity during training, we adopt the softmax temperature to adjust the weights distribution among experts. Formulated as Equation~\\ref{equ:gumbel_softmax}, $W_{g}$ is the parameter of gate, $\\zeta$ is the extra noise and sampled from \\texttt{Gumbel}$(0, 1)$ distribution~\\citep{DBLP:gumbel_softmax}, and $\\tau$ is the softmax temperature which controls the distribution. \nWhen the $\\tau$ increases, the distribution of $g'(x_{s})$ becomes more uniform, which evolves more experts into the computation of each token.\nAs the $\\tau$ approaching 0, the distribution becomes one-hot, which is more confident for the gate network.\n\n\\begin{equation}\ng'(x_{s})  =  \\frac {e^{(x_{s} \\cdot W_{g}+\\zeta)/\\tau }}{\\sum _ {s'=1}^ {N}e^{(x_{s'} \\cdot W_{g}+\\zeta)/\\tau }}\n\\label{equ:gumbel_softmax}\n\\end{equation}\n\n\\textbf{Content-based Sparsity:}\nDifferent from existing static Top-K based gate~\\cite{DBLP:switch}~\\cite{DBLP:gshard}, \\ourmethods{} adopts the \\textit{content-based sparsity} method to determine the number of activated experts, which keeps the value beyond an threshold $c$. As formulated by Equation~\\ref{equ:adaptive_sparsity}, we drop the experts whose weights fall below the threshold $c$ and no extra communication or computation will be wasted. \nIt's worth noting that the sum of selected experts' score can not be equal to 1 because we don't normalize them after dropping. It is useful to remain the original score, especially only one expert is selected, which was verified in Switch. To meet the demand of above two designs, we enable each expert with this content-based gate to make them well specialized. transformer~\\cite{DBLP:switch}.\n\n\\begin{equation}\ng(x_{s})_i = \\begin{cases}\n  g'(x_{s})_i, &\\text{if}\\quad {g(x_{s})_i} > c \\\\\n  0, &\\text{else}\n\\end{cases}\n\\label{equ:adaptive_sparsity}\n\\end{equation}\n\n\\textbf{Sparsity Scheduler:} \nWith temperature $\\tau$ increasing, the distribution tends to be uniform and more experts will be selected. So the sparsity decreases and the training cost of the neural network would increases.\nOn the opposite, less experts are involved into computation and thus the sparsity increases.\nBy scheduling the temperature of Equation~\\ref{equ:gumbel_softmax}, we can control the sparsity of the MoE layer over different training stages. There is a trade-off between model quality and training cost for the selection of temperature. \nFor example, when the distribution of experts is nearly one-hot, it would lead to large variance of gradients between experts and thus make the learning of MoE learning difficult. To optimize this problem, our DTS-Gate starts at a large temperature that routes tokens to most experts and then anneals to a small temperature that gradually sparsifies the MoE layer. \n\n\\textbf{Balance Loss:} Similar to Switch transformer~\\cite{DBLP:switch}, we utilize the balance loss $\\mathcal{L}_{balance}$ to avoid imbalanced assignments for different experts which would cause the straggler problem and thus lead to low training efficiency.\n\\begin{equation}\n\\mathcal{L}_{balance} = \\alpha N \\sum_{i=1}^N(\\frac{\\sum_{x_s \\in \\mathcal{B}}{\\mathbb{I}\\{g(x_s)_i} > 0\\}}{|\\mathcal{B}|^2} \\sum_{x_s \\in \\mathcal{B}}{g'(x_s)_i})\n\\label{equ:balance_loss}\n\\end{equation}\n\n\\noindent As formulated in Equation~\\ref{equ:balance_loss}, $\\alpha$ is a hyper-parameter and $\\mathcal{B}$ represents current batch of tokens. $\\sum_{x_s \\in \\mathcal{B}}{\\mathbb{I}\\{g(x_s)_i} > 0\\}$ represents the number of tokens dispatch to expert $i$ and $\\sum_{x_s \\in \\mathcal{B}}{g'(x_s)_i}$ represents the gate probability allocated for expert $i$. Intuitively, the balance loss will reduce the amount of data for overloaded-experts and move towards to balanced loads  at the batch data level.\n\n\\textbf{Training Objective:}\nIn the first stage, the experts of each MoE layer share same weights and thus the loads can be divided to them equally. The training objective is to optimize the model quality (i.e., $\\mathcal{L}_{task}$). In the second stage, both the model quality and training efficiency (i.e., balanced workloads between experts) need to be considered.\n\\begin{equation}\n\\mathcal{L} = \\begin{cases}\n  \\mathcal{L}_{task}, &\\text{if}\\quad $stage = 1$ \\\\\n  \\mathcal{L}_{task} + \\mathcal{L}_{balance}, &\\text{else}\n\\end{cases}\n\\label{equ:train_obj}\n\\end{equation}\n\n\\setlength{\\textfloatsep}{0.1cm}\n\\begin{algorithm}[t]\n\t\\SetAlgoLined\n\t\\SetKwProg{Fn}{Function}{}{end}\n\t\\KwData{$x_{S}$: a group of tokens of size $S$, $\\tau$: temperature,\\\\ \\quad\\quad $T_{D}$: number of iterations of dense-gate.}\n\t\\KwResult{$G_{S,\\ E}$: group combine weights, $Id_{S}$: Index of selected experts}\n\t\\SetInd{0.61em}{0.61em}\n    \\Fn{DTS\\_Gate($x_{S},\\ \\tau$, $T_{D}$):}{\n        $g_{S,\\ E} \\leftarrow gumbel\\_softmax(x_{S} \\cdot W_{g},\\ \\tau)$ \\;\n    \t\\If{current\\_iteration $<$ $T_{D}$}{\n    \t//select experts for token, N $\\geq$ len($id_{s}$) $\\geq$1 \\;\n    \t$Id_{S} \\leftarrow select\\ g_{S,\\ E} \\textgreater threshold$ \\;\n    \t}\n    \t\\Else{\n    \t//select Top-1 expert for token, len($id_{s}$) = 1 \\;\n    \t$Id_{S} \\leftarrow select\\ Top1(g_{S,\\ E})$\\;\n    \t}\n    \t\\For{$s \\leftarrow 1\\ to\\ S$}{\n        \t\\For{$id \\in ids$}{\n                $G_{s,\\ id} \\leftarrow g_{s,\\ id}$ \\;      \t    \n        \t}\n    \t}\n    \tReturn $Id_{S}, \\ G_{S,\\ E}$ \\;\n    }\n\\caption{Dense-to-Sparse Gate Mechanism}\n\\label{alg:dts_algo}\n\\end{algorithm}\n\n\\section{Implementation}\n\\label{sec:impl}\nEvoMoE is implemented by adding support for MoE models on FairSeq\\footnote{https://github.com/facebookresearch/fairseq}~\\cite{FairScale2021}. Meanwhile, EvoMoE proposes several system optimizations, including: \n\n\\textbf{Topology-Aware Hierarchical All-To-All Communication:}\nIn AllToAll operation, each GPU sends its data to all GPUs (one-for-all) and receives data sent by all GPUs (all-for-one), where each data will be divided equally into n parts.\nCurrent AllToAll operations implemented in NCCL and MPI may suffer from low utilization of network bandwidth because of the small message size.  We propose \\texttt{Topology-Aware Hierarchical AllToAll}, which combines hierarchical networks (intra-node and inter-node) and aggregates messages, to optimize the communication between multi-nodes equipped with one NIC. It first gathers the data of eight GPUs inside the same node into one GPU, and then performs a data layout transformation to orignize the placement of tokens. Afterwards, it launches All-To-All communication between nodes, and then performs the data layout transformation and scatters tokens to its corresponding expert. In this way, the size of data transferred between nodes is ${\\#\\text{GPU}}^2$ times larger than before. \nMeanwhile, this two-level decoupled AllToAll also fully utilizes the intra-node (NvLink or PCIe) and inter-node bandwidth (Infiniband or Ethernet).\n\n\\textbf{MoE-Aware Recomputation:} Recomputation is a mainstream techniques to reduce the memory footprint of training models, which evicts feature map tensors in the forward pass for memory saving and then regenerates them for calculating gradients by executing corresponding computation. Existing systems, e.g., DeepSpeed~\\cite{DBLP:conf/kdd/deepspeed} and FairSeq~\\cite{DBLP:conf/naacl/fairseq}, \nadopt recomputation as the recommended configuration for training large models, which only saves the input tensor of each Transformer layer and re-executes the whole Transformer in the backward. As the MoE layer involves two All-To-All communication in the forward, re-executing them may lead to large time cost. To keep the memory efficiency while improve training efficiency, we propose the MoE-aware recomputation, which save three tensors of each Transformer layer, including the input tensor of Transformer layer, and the input tensor of two All-To-All operations.\n\n\\section{Experiments}\n\\label{sec:experiment}\n\\begin{table*}[t]\n    \\caption{Evaluating the pre-trained models on the \\textbf{GLUE Benchmark}. We scale the Standard Transformer(TRM) into the MoE model by replacing every the other FFN with a MoE layer.}\n        \\begin{center}\n            \\begin{tabular}{l|ccc|cccccccc|c}\n            \\toprule \n            \\textbf{Models} & \\textbf{\\#Shared Params.} & \\textbf{\\#Expert Params.} &\\textbf{FLOPs}& \\textbf{MNLI} & \\textbf{QNLI} & \\textbf{QQP} & \\textbf{RTE} & \\textbf{SST-2} & \\textbf{MRPC} & \\textbf{CoLA} & \\textbf{STS} & \\textbf{Avg} \\\\\n            \\midrule\n            Standard TRM& 355M & -& 207B & 88.2& 93.2& 92.1 & 85.1 & 95.8 & 88.6 & 84.5 & 90.5 & 89.750\\\\\n            Larger TRM& 370M &  -&  220B& 88.4& 93.5& 92.2 & 85.3 & 95.8 & 88.9 & 84.7 & 90.6 & 89.925\\\\\n            \\midrule\n            Switch TRM& 259M & 1536M & 220B&89.2& 93.7 & 92.2 & 86.4& 95.8 & 89.1& 85.3 & 90.8 & 90.313\\\\\n            BASE Layer & 259M &1536M & 220B&89.5& \\textbf{93.9}& \\textbf{92.4}& 87.3 & 96.0 & 89.4 & 85.5 & 91.2 & 90.650\\\\\n            Hash Layer&259M &1536M & 220B&89.4& \\textbf{93.9}& 92.2 & 87.1 & 95.9 & 89.2 & 85.5 & 90.9 & 90.513\\\\\n            StableMoE &259M &1536M & 220B&89.3& 93.8 & 92.1 & 86.7& 95.8& 89.2 & 85.4 & 91.0 & 90.413\\\\\n            EvoMoE &259M &1536M & 220B& \\textbf{89.9} & \\textbf{93.9} & 92.3 & \\textbf{88.1} & \\textbf{96.1} & \\textbf{89.6} & \\textbf{85.6} & \\textbf{91.5} & \\textbf{90.875}\\\\\n            \\bottomrule\n            \\end{tabular}\n        \\end{center}\n\\label{tab:roberta_glue}\n\\end{table*}\n\n\\subsection{Experimental Setup}\n\n\\subsubsection{Machine Environment}\nWe conduct experiments on DGX-A100, where each server is equipped 2 AMD CPUs and 8 NVIDIA Ampere A100 (40GB) GPUs, with Ubuntu 20.04, CUDA 11.3, CuDNN 8.2.0 and NCCL 2.12.7. GPUs inside a server are connected via NVLink 3.0\nwith and servers are connected with 8 InfiniBand NICs via 8*200 Gbps bandwidth totally. The RDMA is used by default and the PyTorch version is 1.11. \\\\ \\\\\n\n\\subsubsection{Baselines}\nTo verify the effectiveness of our method, we compare it with the several representative baselines, including Switch-Transformer~\\cite{DBLP:switch}, BASELayer~\\cite{DBLP:conf/icml/baselayer}, HashLayer~\\cite{roller2021hash}, DSelectK~\\cite{DBLP:conf/nips/dselectk}\nand StableMoE~\\cite{DBLP:conf/acl/stablemoe}.\nSwitch-Transformer~\\cite{DBLP:switch} proposed to adopt Top-1 gate for the training of large-scale models.\nBASELayer~\\cite{lewis2021base} formulates the token-expert routing as a linear assignment problem and guarantees balanced compute loads by employing numerous algorithms. \nHashLayer~\\cite{roller2021hash} replaces the gating network with a hash-based routing strategy (e.g., random hash, clustered hash). DSelectK~\\cite{DBLP:conf/nips/dselectk} involves sparse gates (Top-K) in the multi-gate MoE (i.e., MMoE) architecture for better parameter sharing among different tasks and trains gates from dense to sparse for smoothness. StableMoE~\\cite{DBLP:conf/acl/stablemoe} also proposed two training stages, which learn the gate as well as distill it into a lightweight one in the first stage, and freezes the parameter of gate for stable routing in the second stage.\nOur \\evomoe mainly contains two phases: an \\texttt{expert-diversify}\nphase to spawn multiple diverse experts from one single well-trained base expert,\nand a \\texttt{gate-sparsify} phase that gradually and adaptively learns a increasingly sparse gate from a dense gate.\n\n\\subsubsection{Benchmark and Datasets}\nWe evaluate \\evomoe{} on three popular tasks, including the machine translation (MT) task for domain-specific models, the Masked Language Modeling (MLM) task and the language modeling (LM) task for pre-trained models.\n\nWe adopt standard Transformer architecture~\\cite{vaswani2017attention} (Encoder-Decoder) \nfor the MT task and train models on four popular translation datasets, WMT17 (English to German/German to English)\\footnote{https://www.statmt.org/wmt17/}, and WMT14 (English to French/French to English)\\footnote{https://www.statmt.org/wmt14/}. BLEU scores of the test sets are reported for comparison.\n\nWe adopt RoBERTa architecture architecture~\\cite{liu2019roberta} (Encoder-only) \nfor the MLM task and train models on the combination of datasets, including Wikipedia\\footnote{https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-abstract.xml.gz}, BooksCorpus\\footnote{https://battle.shawwn.com/sdb/books1/books1.tar.gz}, OpenWebText\\footnote{https://zenodo.org/record/3834942/files/openwebtext.tar.xz} and CC-100\\footnote{https://data.statmt.org/cc-100/}. Moreover, these datasets are tokenized by byte-pair encoding with a vocabulary size of 50257. Models are validated on the famous General Language Understanding Evaluation(GLUE) benchmark~\\cite{DBLP:conf/emnlp/glue} for comparison.\n\nWe adopt GPT architecture architecture~\\cite{gpt2} (Decoder-only) \nfor the LM task and train models on OpenWebText as ~\\citet{gpt2}. We report train/valid/test perplexity (PPL) for comparison.\n\nWe also report the inference FLOPs of each model, which represents the speed of deploying this model at industry. All the training data are downloaded and pre-processed by following the example scripts from Fairseq\\footnote{https://github.com/facebookresearch/fairseq/tree/main/examples}.\n\n\\subsubsection{Hyper-Parameter Detail}\nWe sparsely scale these models by replacing every other the \\textit{feed-forward} layer (FFN) with MoE-FFN Layer, which contains a series of FFN experts.\nAll models use the GeLU activation functions~\\cite{gelu_activation}, polynomial learning rate scheduler and Adam optimizer~\\cite{DBLP:journals/corr/adam_optim}, where $\\beta_1 = 0.9$ and $\\beta_2 = 0.98$ . \nWe set clip norm as 0.0, weight decay as 0.1 and dropout rate as 0.1.\nWe use CrossEntropy as the criterion and utilize the label smoothed technique with coefficient of 0.1 for the MT task.  \nThe coefficient of balance loss is set as 0.1 in Switch-Transformer~\\cite{DBLP:switch}, StableMoE~\\cite{DBLP:conf/acl/stablemoe} and our EvoMoE.\nWe set the threshold $c$ of our dense-to-sparse gate as 0.001 over training steps, which determines how large the expert's weight is important and is a trade-off between training cost and model quality from our point of view. \n\n\\begin{table*}[t]\n    \\caption{Perplexity results of language modeling task.}\n        \\begin{center}\n            \\begin{tabular}{l|cccc|c}\n            \\toprule \n            \\textbf{Models}  & \\textbf{\\#Shared Params.} &  \\textbf{\\#Expert}& \\textbf{\\#Expert Params.} & \\textbf{FLOPs} & \n            \\textbf{Perplexity($\\downarrow$)}  \\\\\n            \\midrule\n            Standard TRM&  345M& -  & -  & 207B& 15.14\\\\\n            Larger TRM (wider)& 360M & - & - & 220B& 14.92\\\\\n            \\midrule\n            Switch TRM & 249M& 192& 1536M& 220B& 13.12\\\\\n            BASE Layer & 249M& 192& 1536M& 220B& 12.45\\\\\n            Hash Layer& 249M& 192& 1536M& 220B& 12.87\\\\\n            StableMoE & 249M& 192& 1536M&220B & 12.91\\\\\n            EvoMoE & 249M& 192& 1536M& 220B& \\textbf{12.24}\\\\\n            \\bottomrule\n            \\end{tabular}\n        \\end{center}\n\\label{tab:gpt_lm}\n\\end{table*}\n\n\\begin{table}[t]\n    \\caption{BLEU score on each machine translation datasets}\n        \\begin{center}\n            \\begin{tabular}{c|cccc}\n            \\toprule \n            \\textbf{Models}\n            & \\textbf{En-De} & \\textbf{De-En} & \\textbf{En-Fr} & \\textbf{Fr-En}\\\\ \n            \\midrule\n            {TRM-Base}  \n            & {28.1} &{34.8}\n            & {39.2} &{38.1}\\\\\n            {Switch-TRM}\n            & {28.4} &{34.6}  \n            & {39.1} &{38.2} \\\\\n            \\midrule\n            {\\evomoe{}} \n            & {\\textbf{29.6}} &{\\textbf{36.7}}\n            & {\\textbf{40.3}} &{39.2} \\\\\n            {\\quad\\quad\\quad\\quad w/o \\ stage\\ 1}  \n            & {\\textbf{29.6}} &{36.5}  \n            & {40.2} &{\\textbf{39.3}} \\\\\n            {\\quad\\quad\\quad\\quad w/o \\ stage\\ 2}  \n            & {28.7} &{35.2}  \n            & {39.4} &{38.3} \\\\\n            \\bottomrule\n            \\end{tabular}\n        \\end{center}\n\\label{tab:trans_tasks}\n\\end{table}\n\n\\subsection{GLUE Results}\n\\textbf{Model Architecture:} We pretrain the representative RoBERTa model for the masked language modeling task, where we set standard Transformer(TRM) with 24 encoder layers, hidden dimension as 1024 and number of attention heads as 16. We replace every other FFN layer in standard Transformer with the MoE layer (16 experts per layers) to construct the MoE models.  The standard Transformer is a dense model and contains 355M parameters totally, whose inference FLOPs is 207B. Meanwhile, the sparse MoE model contains 1759M parameters totally, including 259M parameters for shared backbone and 1536M parameters for the expert network. In our setting that only 1 expert is active at a time, each input token will activate 335M parameters of the sparse MoE models, which is the same as standard Transformer model except for the gate network. To exactly match the inference speed (FLOPs) of MoE models, we slightly increase the FFN hidden size of standard TRM to construct the larger TRM.\n\n\\textbf{Model Performance:}\nWe pretrained each model for 100k steps totally, 5k of which was the warm-up phase. For our \\ourmethods{}, we scheduled the first 5k steps as the expert-diversify stage and the following 5k steps for annealing temperature from 2.0 to 0.3. After the pre-training stage, we finetune the pre-trained models on each GLUE task and summarized the results in Table~\\ref{tab:roberta_glue}. As for RTE, we finetune it starting from the MNLI model rather than the pretrained model as ~\\citet{liu2019roberta}. \n\nCompared with other baselines, \\ourmethods{} achieves state-of-the-art results on 7 out of 8 tasks and the best averaged score. The MoE model is constructed by adding the gate network and replacing the original FFN layer of standard Transformer, which increases its model size and thus enlarges its capacity. Thus all MoE models outperform their backbone model (standard TRM), e.g., 89.750 for standard TRM and 90.313 (+ 0.563) for Switch TRM with respect to the avg score. Larger TRM slightly outperforms standard TRM because of its large model size. As verified by ~\\citet{DBLP:scaling_laws}, larger models tend to be more sample-efficient, which represents better model quality with fixed training data/steps.\n\nCompared with other MoE methods, \\ourmethods{} benefits from the sufficient training of experts in the early stage and then make the experts selection becomes sparser on the basis of specialized experts. \nSpecifically, \\ourmethods{} outperforms other MoE methods on GLUE benchmark up to 0.562 and 0.403 on average. \nSwitch TRM~\\cite{DBLP:switch} jointly trains the randomly initialized experts and gates, which aims to learn better parameter as well as balanced routing. It is hard to optimize them simultaneously and thus performs bad among MoE models. To alleviate this problem, StableMoE~\\cite{DBLP:conf/acl/stablemoe} freezes the parameter of gate network after the early training stage and improves over Switch-TRM. Hash Layer~\\cite{roller2021hash} utilizes the fixed hash strategy to route tokens, which is based on the input embedding. Because both the hash strategy and input embedding is fixed, Hash Layers only need to learn the parameter of experts. However, it may lead to sub-optimal because the hash strategy is selected based on human knowledge and may be inappropriate. BASE Layer~\\cite{DBLP:conf/icml/baselayer} enforces a balanced token-to-expert assignment through a linear assignment problem, which simplify the training in another way. All these work find the problem of jointly training and targeting at alleviate it.\n\n\\subsection{Language Modeling Results}\n\\textbf{Model Architecture:} We pretrain the representative GPT model for the language modeling task, where we set standard Transformer(TRM) with 24 decoder layers, hidden dimension as 1024 and number of attention heads as 16. \nEvery other FFN layer is replaced by in standard Transformer with the MoE layer (16 experts per layers) to construct the MoE models. There totally exists 12 MoE layers and thus 192 experts (i.e., $12 \\times 16$).  \nMeanwhile, larger TRM is scaled by increasing its FFN hidden size.\n\n\\textbf{Model Performance:}\nWe pretrained each model on the OpenWebText dataset for 200k steps totally, 10k of which was the warm-up phase. For our \\ourmethods{}, we scheduled the first 10k steps as the expert-diversify stage and the following 5k steps for annealing temperature from 2.0 to 0.3. We report the perplexity on the test set. Results are summarized in Table~\\ref{tab:gpt_lm}.\n\nCompared with other baselines, our \\ourmethods{} achieves the best result among all baselines. Specifically, the perplexity of \\ourmethods{} is 12.24, which achieves a 2.90 improvement compared with 15.14 of standard TRM. Meanwhile, all MoE models outperform their backbone model (standard TRM) because of their large model capacity. Larger TRM slightly outperforms standard TRM because of its large model size, which demonstrates the sample-efficient of large models.\n\nCompared with other MoE methods, \\ourmethods{} benefits from the sufficient training of experts in the early stage and then make the experts selection becomes sparser on the basis of specialized experts. Specifically, \\ourmethods{} outperforms other MoE methods up to 0.88 ppl and 0.545 ppl on average. The analysis between different methods is same as that in GLUE results.\n\n\\begin{table}[t]\n    \\caption{BLEU performance of MoE models with different expert number. }\n        \\begin{center}\n            \\begin{tabular}{c|ccc}\n            \\toprule \n            {} & \\multicolumn{3}{c}{\\textbf{Number\\ of\\ Experts }} \\\\\n            &{\\textbf{4}} & {\\textbf{8}} &{\\textbf{16}}\\\\ \n            \\midrule\n            {Switch}\n            &{28.4} \n            &{28.6} &{28.7}\\\\\n            {\\ourmethods{}} \n            &\\textbf{29.6} &\\textbf{29.9}\n            &{30.0} \\\\\n            {\\quad\\quad\\quad\\quad w/o \\ stage\\ 1}  \n            &\\textbf{29.6} &\\textbf{29.9}  \n            &\\textbf{30.1}  \\\\\n            {\\quad\\quad\\quad\\quad w/o \\ stage\\ 2}  \n            &{28.7} &{28.9}  \n            &{28.9} \\\\\n            \\bottomrule\n            \\end{tabular}\n        \\end{center}\n\\label{tab:expert_number}\n\\end{table}\n\n\\begin{table}[t]\n    \\caption{Efficiency of MoE models with different expert number, and the results are normalized over Switch.}\n        \\begin{center}\n            \\begin{tabular}{c|ccc}\n            \\toprule \n            {} & \\multicolumn{3}{c}{\\textbf{Number\\ of\\ Experts }} \\\\\n            &{\\textbf{4}} & {\\textbf{8}} &{\\textbf{16}}\\\\ \n            \\midrule\n            {Switch}\n            &{1} \n            &{1} &{1}\\\\\n            {\\ourmethods{}} \n            &\\textbf{0.82}\n            &\\textbf{0.78} &\\textbf{0.75} \\\\\n            {\\quad\\quad\\quad\\quad w/o \\ stage\\ 1}  \n            &{0.86}  \n            &{0.82} &{0.81} \\\\\n            {\\quad\\quad\\quad\\quad w/o \\ stage\\ 2}  \n            &{0.95}  \n            &{0.93} &{0.92} \\\\\n            \\bottomrule\n            \\end{tabular}\n        \\end{center}\n\\label{tab:efficiency}\n\\end{table}\n\n\\begin{figure*}[t]\n    \\begin{center}\n    \\subfigure[Validation PPL over steps]{\n            \\includegraphics[width=0.35\\textwidth]{Figures/e2e_iteration.pdf}\n            \\label{fig:iters_ppl}        }    \n    \\subfigure[Validation PPL over FLOPs]{\n            \\includegraphics[width=0.35\\textwidth]{Figures/e2e_flops.pdf}\n            \\label{fig:flops_ppl}\n    }\n    \\caption{End-to-end performance comparison between GPT-ori, GPT-Switch and GPT-DTS. Figure~\\ref{fig:iters_ppl} and Figure~\\ref{fig:flops_ppl} represent the curve of PPL over iterations and FLOPs, where GPT-DTS can obtain $2.0$x speed-up to reach the same validation perplexity, as well as higher FLOPs-efficiency of a $1.42$x speed-up.\n}\n    \\label{fig:end2end}\n    \\end{center}\n\\end{figure*}\n\n\\subsection{Machine Translation Results}\n\n\\textbf{Model Architecture:} We pretrain the representative T5 model for the machine translation task, where we set standard Transformer(TRM) with 12 encoder-decoder layers, hidden dimension as 768 and number of attention heads as 12. \nEvery other FFN layer is replaced by in standard Transformer with the MoE layer (4 experts per layers) to construct the MoE models. \n\n\\textbf{Model Performance:}\nWe compare \\evomoe{} with Transformer and Switch-Transformer on four language-pair datasets, including English to German, German to English, English to French and French to English. Results are shown in Table~\\ref{tab:trans_tasks},~\\ref{tab:expert_number},~\\ref{tab:efficiency}.\nWe remark that these models all have the same inference speed even if MoE models enlarge the parameter size.\nWe show the BLEU score on the test set of each dataset in Table~\\ref{tab:trans_tasks}.\n\\ourmethods{} outperforms other methods by 1 BLEU score on average.\nAlthough Switch-Transformer scale the model size, it still achieves a similar performance with Transformer-Base, which is parameter-efficient.\nTable~\\ref{tab:expert_number} shows the BLEU performance of different expert number on the English-German datasets. \\ourmethods{} can still outperform the Switch-Transformer about 1.3 BLEU with the increasing number of experts. Because of the datasets' quality, the effect of increasing expert number is limited.\n\n\\textbf{Model-Efficency:} Table~\\ref{tab:efficiency} show the model efficiency of \\ourmethods{} and Switch Gate on the English-German datasets. \\ourmethods{} is efficient at the speed of converge. For example, \\ourmethods{} need only $75\\%$ compute budget of Switch-Transformer to reach the same PPL. It is worth noting that the speedup over Switch-Transformer improves as the expert number increases. \n\n\\textbf{Ablation Study:}\nWe present a ablation study on \\ourmethods{} to show the influence of two stages by removing the expert-diversify phase and the gate-sparsify phase respectively. Results are summarized in Table~\\ref{tab:trans_tasks}~\\ref{tab:expert_number}~\\ref{tab:efficiency}.\nAs for the model performance metric, it will lead to performance degradation when \\ourmethods{} removes the gate-sparsify stage, such as 38.3/39.2 in Fr-En of Table~\\ref{tab:trans_tasks}. \nMeanwhile, it is worth noting that influence is little as for w/ and w/o the expert-diversify stage, which encourages us to involve this stage for saving computation budget.\nAs for the FLOPs-efficiency metric, the gate-sparsify phase can improve the FLOPs-efficiency by $17\\%$. By introducing the expert-diversify stage, \\ourmethods{} can obtain an extra $4\\%$ improvement.\n\nIn summary, the gate-sparsify phase can both improve the model performance and FLOPs-efficiency significantly and the expert-diversify phase can introduce extra FLOPs-efficiency without performance degradation. In the following sections, we will detail analyze the gate-sparsify phase and evaluate it at large scale.\n\n\\subsection{Breakdown on the Gate-Sparsify Phase}\n\n\\textbf{Model Architecture:} We pretrain the representative GPT model for the language modeling task, where we set GPT-ori with 24 decoder layers, hidden dimension as 1024 and number of attention heads as 16. \nEvery other FFN layer is replaced by in standard Transformer with the MoE layer (16 experts per layers) to construct the GPT-MoE model. \nGPT-Switch represents training MoE models with Switch gate, which keeps Top-1 selection.\nGPT-DTS represents training MoE models with the \\texttt{dense-to-sparse} gate, which starts as a dense gate that routes tokens to most experts and then gradually becomes sparser. \n\nWe compare the required FLOPs to train models to show the FLOPs-efficiency of different methods.\nThe FLOPs-efficiency is defined as the best model performance (PPL) can be achieved given the fixed number of floating-point operations (computation budget).\nBecause the actual training time could be affected by the system engineering efforts on the implementation details, which are not our focus in this approach. Instead, in our experiments, we prefer to choose the computation complexity for fair comparisons. \n\n\\begin{figure*}[t]\n    \\centering\n    \\subfigure[More Experts]{\n            \\includegraphics[width=0.33\\textwidth]{Figures/expert_scalability_flops.pdf}\n            \\label{fig:expert_scalability}\n    }\n    \\subfigure[More MoE Layers]{\n            \\includegraphics[width=0.33\\textwidth]{Figures/moe_layer_scalability_flops.pdf}\n            \\label{fig:moe_layer_scalability}\n        }\n    \\caption{Scalability for DTS gate. It shows that more experts or more MoE-layers (larger models with constant FLOPs), will lead to better FLOPs-efficiency.\n    }\n    \\label{fig:scalability}\n\\end{figure*}\n\n\\textbf{Model Performance:}\nWe pretrained each model on the OpenWebText dataset for 300k steps totally, 10k of which was the warm-up phase. For our \\ourmethods{}, we scheduled the first 10k steps for annealing temperature from 2.0 to 0.3. We report the perplexity on the validation set. Results are shown in Figure~\\ref{fig:end2end}.\nTo improve the computation efficiency, only part parameters are used for each token in sparse models with the cost of model performance. \nDTS-Gate aims to shift the model training from dense to sparse, and keep the inference cost same as sparse models.\nExperiments show that compared with the state-of-the-art Switch-Transformer in GPT-MoE model with OpenWebText dataset, GPT-DTS can obtain\n2.0x speed-up to reach the same validation perplexity (Figure~\\ref{fig:iters_ppl}), as well as higher FLOPs-efficiency of a 1.42x speed-up (Figure~\\ref{fig:flops_ppl}).\nExperiments also verify the ability of \\texttt{dense-to-sparse} gate for scaling models with more experts or more MoE layers.\n\nComparison with Sparse Models.\nMoE-Switch pre-defines its static Top-1 gating network and jointly training the gate and experts networks. Different from GPT-Switch, GPT-DTS utilizes temperature to adjust the distribution of the token-to-experts (one-hot or uniform) and threshold to remove computation of experts with low weights. \\ourmethods{} performs better than GPT-Switch in sample-efficiency because of more experts involved in training and updates at the beginning, shown as Figure~\\ref{fig:iters_ppl}. As for FLOPs-efficiency, DTS-Gate first involves more experts into warm-up training, which is poor FLOPs-efficency. But with the training going on, GPT-DTS can obtain greater than $25\\%$ improvements in FLOPs-efficiency compared with the state-of-the-art Switch-Transformer in GPT-MoE model with OpenWebText dataset.\n\n\\subsection{Scalability}\n\nIn this subsection, we investigate different experiment settings to validate the scalability of our DTS-Gate. \n\n\\textbf{Model Architecture:} We choose the GPT-small as the backbone model for the language modeling task, where we set the model with 12 decoder layers, hidden dimension as 768 and number of attention heads as 12. \n\n\\textbf{Increase the Expert Number:} Based on GPT-Small model with 117M parameters, we replace the 7-th FFN layer by one MoE layer and vary its experts number within $\\{1, 4, 8, 16\\}$. \nAs shown by Figure~\\ref{fig:expert_scalability}, with increasing expert numbers, \\ourmethods{} keeps consistent improvements (i.e., lower PPL) during training.\n\n\\textbf{Increase the MoE layer number:} Similarly, we also vary the number of MoE layers to validate the performance of DTS gate. \nWe insert $k$ MoE layers in GPT-Small, where $k \\in \\{0, 1, 4, 6\\}$ and each MoE layer contains 8 experts. \nFigure~\\ref{fig:moe_layer_scalability} shows that by increasing MoE layers, \\ourmethods{} can achieve better model performance with same FLOPs. \n\n\\subsection{Effect of Sparsity Scheduler}\n\nIt is worth noting that several hyper-parameters are introduced in Dense-To-Sparse gate, such as max/min temperature and decay iterations.\nIn this section, we analyze the effect of different hyper-parameter setting by conducting  experiments of various settings. The training model is GPT-MoE, 24-layer decoder with 12 MoE-layer (16 experts per layer) and the dataset is OpenWebText.\n\nWe decay temperature $\\tau$ from $max\\_value$ to $min\\_value$ in the first 15000 iterations and switch to Top1 then. Experiments with different $max\\_value$ to $min\\_value$ are evaluated, and the results are shown in Figure~\\ref{fig:scheduler}.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.35\\textwidth]{Figures/temperature_scheduler_flops.pdf}\n    \\caption{Effect of different temperature scheduler.}\n    \\label{fig:scheduler}\n\\end{figure}\n\n\\textbf{Max/Min Temperature} Under small temperatures, the weight distribution of experts is close to one-hot, which leads to the one-token-one-expert distribution and low training cost, \nbut the variance of gradients is large.\nIn contrast, large temperatures result in nearly uniform distribution gate weights, \nwhich evolves more experts into training but the variance of gradients is small. As shown in Figure~\\ref{fig:scheduler}, we find the these two hyper-parameters have low influence on the model quality under the same training budget, except for the extrame value, e.g., 1.0 for $max\\_value$ and 0.1 for $min\\_value$.\n\n\\begin{table}[t]\n    \\caption{The most frequent tokens assigned to each expert in the validation set, which shows that some experts assignment decisions are made based on local contexts. For many other experts, the assignment decision depends on longer context, and is harder to visualize.}\n        \\begin{center}\n            \\begin{tabular}{c|ccccc|c}\n            \\toprule \n            \\textbf{Expert}  & \\multicolumn{5}{c|}{\\textbf{Top5 Proceeding Tokens}} & \\textbf{Descriptions} \\\\  \n            \\midrule\n            1 & is & was & be & are & have & auxiliary verbs \\\\\n            3 & .& \\textbackslash n & /& (&;  & punctuations \\\\\n            4 & in& , & of& and& from & prepositions \\\\\n            6 & and& I & it& that& they & possessive cases\\\\\n            12 & out& up & go& back& down & directional prepositions \\\\\n            \\bottomrule\n            \\end{tabular}\n        \\end{center}\n\\label{tab:visual}\n\\end{table}\n\n\\subsection{Visualization of Expert Specialization}\n\nWe visualize the routing strategy of the pre-trained GPT model by \\ourmethods{} in Table~\\ref{tab:visual} through its corresponding input embedding, where each MoE layer contains 16 experts. For each expert, we present the Top5 proceeding tokens assigned and give descriptions for explanations from our points of view. For example, Expert 1 captures the auxiliary verbs and Expert 6 captures possessive cases. These experts can capture local contexts of each embedding well. For other experts, it is diff cult to visualize because of the long contexts' influence.\n\\section{Related Work}\n\\label{sec:related_work}\n\\paragraph{Static Sparse Neural Networks}\nExploiting the sparsity in deep neural networks can reduce both storage and computation requirements for model training and inference.\nOne of the most widely used sparsification methods is weight pruning \\citep{lecun1990optimal,han2015deep}.\nPrevious studies proposed to prune away redundant or less useful weights based on various pruning criteria (e.g., the importance of individual weights \\citep{han2015deep} or groups of weights \\citep{wen2016learning,luo2017thinet,he2017channel}) and then fine-tune remaining weights to regain the lost accuracy.\nAfter pruning and fine-tuning, parts of weights are permanently removed, inducing a static sparsity pattern in DNNs.\nThe sparsity pattern/structure is a trade-off between model effectiveness and hardware efficiency \\citep{mao2017exploring}. %affects all or is a trade-off between \nEarly work attempts to increase the sparsity ratio or model accuracy by employing unstructured sparsification methods, while recent work focuses more on structured sparsity for practical speedup on hardware.\nInterestingly, \\cite{frankle2018lottery} points out training a sparse network from scratch is superior or comparable to pruning-based methods.\nOur \\evomoe{} adopts a dense-to-sparse gate, which is analogous to pruning-based methods that train all experts first and then learning the sparse gate routing.\n\n\\paragraph{Conditional Computation with \\moe{}}\nDifferent from previous static sparse neural networks that permanently remove partial weights, conditional computation~\\cite{DBLP:journals/corr/BengioLC13} only activates the relevant parameter of the model on a per-sample basis, which can be regarded as a dynamic sparsity structure that remains all model weights but brings sparsity into the computation. %or operation\nThe mixture-of-expert (\\moe{}) architecture~\\cite{DBLP:lstm_moe}, as a specific form of conditional computation, contains a series of experts and a trainable gating network which routes each input sample to its corresponding experts.\nConditional computation is capable of reducing inference cost (without reducing model capacity) or increasing model capacity (without increasing inference cost) from a model acceleration or scaling perspective.\nOn the other hand, the sparsely activated parts (i.e., \\moe{} in models) can be regarded as structured sparse blocks, which does not introduce additional computational overhead.\nHowever, conditional computation models are often difficult to train, since they require learning discrete routing decisions from individual examples to experts and the gating network tends to converge to a state that only selects the same few experts~\\cite{eigen2013learning}.\nLSTM-MoE~\\cite{DBLP:lstm_moe}, GShard~\\cite{DBLP:gshard} and Switch-Transformer~\\cite{DBLP:switch} utilize auxiliary load-balancing losses to mitigate this self-reinforcement phenomenon and thus improve training efficiency.\nIn such \\moe{} models, the gating network and experts, as two critical components,\nare jointly trained which may interfere with each other. \nIn \\evomoe, we consider decoupling the training of experts and the gating network by involving all experts starting with a high temperature in Gumbel-Softmax and then training the gating network to be sparser and select the best expert through decaying this temperature. \nBASELayer~\\cite{lewis2021base} formulates the token-expert routing as a linear assignment problem and guarantees balanced compute loads by employing numerous algorithms. \nHashLayer~\\cite{roller2021hash} replaces the gating network with a hash-based routing strategy (e.g., random hash, clustered hash dispersed hash).\nMoEfication~\\cite{zhang2021moefication} utilizes the sparse activation in feed-forward networks (FFNs) of a Transformer model and divides each large dense FFN into several experts to accelerate model inference, while EvoMoE evolves a small dense model into a large and sparse MoE model.\n\n\\paragraph{Multi-Task Learning with \\moe{}} \nThe multi-task learning (MTL) adopts a shared architecture to learn multiple tasks, which exploits relationships among tasks and achieve better generalization performance~\\cite{DBLP:journals/corr/mtl_overview}. However, sharing parameters between unrelated tasks can potentially degrade performance.\nThe multi-gate MoE (i.e., MMoE) architecture is introduced as an effective way to exploit both the commonalities and differences among tasks, where each task has its own gate that adaptively controls the extent of parameter sharing~\\cite{ DBLP:conf/kdd/mmoe}. \nDSelect-K~\\cite{DBLP:conf/nips/dselectk} involves sparse gates (Top-K) for better parameter sharing and trains gates from dense to sparse for smoothness,\nRegarding DSelect-K and the DTS gate in our \\evomoe{} both propose the dense-to-sparse mechanism, they are trying to address two totally different problems, though both two pieces of work show SparseMoE is better than DenseMoE by coincidence.\nFor our DTS work, because DenseMoE performs well but cost for large model pretraining, we tried to find a more efficient solution (SparseMoE). While for the multi-task work, because DenseMoE performs badly for multi-task learning, they tried to find a better solution to deal with various tasks, i.e., DSelectK. Therefore, this two pieces of work have clearly different motivations.\n\n\\section{Conclusion and Future Work}\n\\label{sec:conclusion}\nMoE models suffer from the training efficiency challenge due to the difficulty of training many experts and the gate network jointly. \nIn this work, we presented an MoE training framework \\evomoe that decouples the training of experts and the gate network by first spawning multiple diverse experts from one single well-trained base expert and then learning a increasingly sparse gate from a dense gate.\nOur evaluations show that \\evomoe can not only achieve better model quality in Transformers with given computation budget but also achieve better FLOPs-efficiency when comparing with previous works in MoE training. \nOn the other hand, \\evomoe opens challenges for system execution due to the computation in the early stage and the adaptive capacity of experts. \nIn the future, we would like to design and implement system-level optimizations to achieve efficient training in both model quality and system execution.\n\n\\balance\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nMixture-of-Experts (MoE) language models can reduce computational costs by 2-4$\\times$ compared to dense models without sacrificing performance, making them more efficient in computation-bounded scenarios. However, MoE models generally require 2-4$\\times$ times more parameters to achieve comparable performance to a dense model, which incurs larger GPU memory requirements and makes MoE models less efficient in I/O-bounded scenarios like autoregressive generation. In this work, we propose a hybrid dense training and sparse inference framework for MoE models (DS-MoE) which achieves strong computation and parameter efficiency by employing dense computation across all experts during training and sparse computation during inference. Our experiments on training LLMs demonstrate that our DS-MoE models are more parameter-efficient than standard sparse MoEs and are on par with dense models in terms of total parameter size and performance while being computationally cheaper (activating 30-40\\% of the model's parameters). Performance tests using vLLM show that our DS-MoE-6B model runs up to $1.86\\times$ faster than similar dense models like Mistral-7B, and between $1.50\\times$ and $1.71\\times$ faster than comparable MoEs, such as DeepSeekMoE-16B and Qwen1.5-MoE-A2.7B.\n\\end{abstract}\n\n\\section{Introduction}\nWhile scaling up Large Language Models (LLMs) has proven to be an effective way to improve performance on a huge range of tasks, increased scale leads to increased computational costs. The Mixture-of-Experts (MoE) approach \\cite{shazeer2017outrageously, fedus2022switch, shen2023moduleformer, jiang2024mixtral} presents one possible solution by selectively utilizing a subset of parameters for improved computational efficiency while maintaining or even enhancing performance. This efficiency is particularly beneficial in computation-bound scenarios where many tokens need to be processed simultaneously, like preprocessing a large batch of prompts. Despite these benefits, MoE models often require 2-4$\\times$ more parameters than dense models \\cite{dai2024deepseekmoe, shen2023moduleformer} to achieve comparable performance. The large number of parameters makes MoE models consume more memory and less efficient in I/O-bounded scenarios such as recurrent token generation. We hypothesize that the relative parameter inefficiency of MoE models is primarily due to the sparse training approaches typically used to train MoE models, wherein only a subset of experts is activated and optimized for each token. In addition, sparse training can lead to inefficient GPU utilization when expert parallelism is used and expert usage is unbalanced \\cite{gale2023megablocks}.\n\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=\\linewidth]{figures/teaser_1B.pdf}\n\\caption{Subfigure (a) showcases the sizes and computational profiles of the Dense-3B, SMoE-5B, and DS-MoE-3B models, each achieving a comparable averaged task performance in Table \\ref{tab:base_performance}. The computational cost is quantified by counting the number of active parameters engaged during inference. Subfigure (b) displays the performance of our DS-MoE-6B model in sparse inference, set against that of the traditional dense models and SMoE models. The radius of the icon circle reflects the total number of the model parameters.}\n\\label{fig:teaser}\n\\end{figure}\nIn our study, we introduce dense training and sparse inference as a method to improve the parameter efficiency of MoE models. As illustrated in Figure \\ref{fig:teaser}, our DS-MoE matches the performance of the same-size dense model while notably decreasing the number of active computing parameters during inference. In comparison with a performance-matched sparse MoE \\cite{fedus2022switch, gale2023megablocks}, our DS-MoE significantly diminishes the total parameter count of the model while activating a similar number of parameters. The critical distinction between our DS-MoE and traditional sparse training lies in the involvement of \\textbf{all experts} in each layer throughout the training phase. Additionally, we incorporate a Mutual Information (MI) loss \\cite{shen2023moduleformer} that focuses on load balancing and expert concentration. This MI loss ensures the router produces an even distribution across experts and a sparse distribution for individual tokens, thereby ensuring expert use is balanced while allowing for sparse inference post-training. We then employ sparse inference by selecting the top \\(K\\) experts in each layer, based on their router scores. The value of \\(K\\) is either a fixed number or dynamically determined based on a predefined router score threshold \\(\\epsilon\\). As a result, our DS-MoE model achieves performance comparable to that of dense models with same model size while only activating 30\\% to 40\\% of the parameters during inference.\n\nOur experimental results demonstrate that: (1) Our DS-MoE significantly improves the parameter efficiency of MoE models and outperforms conventional sparse training methods for MoE; (2) when compared to  parameter-matched dense models, our DS-MoE model not only maintains comparable performance but also substantially reduces computation by activating 30-40\\% of parameters during inference;  (3) we observe that larger models exhibit greater tolerance to sparsity, effectively maintaining dense-inference performance levels by engaging fewer experts, and (4) our DS-MoE has the best throughput performance in both computation-bounded and I/O-bounded scenarios.\\vspace{-1mm}\n\\section{Related Work}\n\\vspace{-1mm}\n\\paragraph{Spa60rsely Gated Mixture-of-Experts Models} Shazeer et al. \\cite{shazeer2017outrageously} introduced the sparsely gated Mixture of Experts (MoE) layer, which are sparsely activated subnetworks whose routing is determined by softmax gates, to enhance the scalability of LSTMs \\cite{Hochreiter1997LongSM}. With the advent of transformers \\cite{Vaswani2017AttentionIA, Radford2018ImprovingLU, Devlin2019BERTPO}, which have shown significant improvements with scaling \\cite{kaplan2020scaling}, MoE has been recognized as a promising avenue for advancing model performance. The integration of MoE into the Transformer framework has been furthered by innovations such as Gshard \\cite{Lepikhin2020GShardSG}, Switch transformers \\cite{fedus2022switch}, and GLAM \\cite{Du2021GLaMES}, which each introduce parallelization techniques for efficient training of MoE models across multiple devices. Nonetheless, the complexity of training MoE models is augmented by the routing problem \\cite{Rosenbaum2019RoutingNA, Mittal2022IsAM}, prompting numerous studies to enhance routing strategies either by redesigning the router \\cite{Roller2021HashLF, Lewis2021BASELS, Chi2022OnTR, Zhou2022MixtureofExpertsWE} or by altering the training methodology \\cite{Zoph2022STMoEDS, Dai2022StableMoESR, Shen2023ModuleFormerLM}. Additional research has explored converting dense models into MoE models \\cite{Komatsuzaki2022SparseUT, Zhang2021MoEficationTF} or utilizing a complete transformer model as an expert \\cite{Li2022BranchTrainMergeEP}. Beyond merely scaling, MoE offers benefits in managing diverse tasks, with notable achievements in machine translation \\cite{Kudugunta2021BeyondDT}, multitask learning \\cite{Hazimeh2021DSelectkDS, Gupta2022SparselyAM}, and instruction tuning \\cite{Shen2023MixtureofExpertsMI}. The landscape of MoE models is rapidly expanding, with recent introductions of more powerful frameworks \\cite{jiang2024mixtral, dai2024deepseekmoe}, and studies by \\cite{shen2023mixture} and \\cite{qiu2023emergent} have shown that instruction finetuning significantly enhances MoE models, bolstering their prevalence.\n\n\\paragraph{Sparsity in Dense Model.}Sparsity is a common trait in large language models. \\cite{liu2023deja} has demonstrated that similar performance levels can be achieved by activating only 10-20\\% of the neurons in these models. The concept of MoEfication, as introduced by another study \\cite{zhang2021moefication}, involves organizing neurons into distinct expert groups within a dense model and then converting it into a sparse MoE model. This transformation is accomplished by training a router to manage these expert groups, thereby preserving performance. Model pruning techniques \\cite{voita2019analyzing,michel2019sixteen} leverage the inherent sparsity within dense models to eliminate superfluous neurons, enhancing efficiency. Similarly, dynamic inference strategies \\cite{wang2018skipnet,wu2018blockdrop,pan2021va} aim to selectively engage only the necessary parts of the model during computation, optimizing resource use. Furthermore, studies on model-preserving compression \\cite{chee2022model} have revealed that neurons in dense models are often redundant. It has been shown that the model's parameter size can be significantly reduced by generating portions of the neuron through cost-effective operations on existing neurons, further affirming the potential for optimization in model design.\n\n\\paragraph{Efficient Inference for LLM.} The high computational costs of large-scale LLMs have led to a great deal of work that aims to make inference of LLMs more efficient. Structured pruning techniques \\cite{xia2023sheared,xia2022structured,cai2019once,wen2016learning,liu2017learning,luo2017thinet} aim to trim a pre-trained large model in a systematic manner, resulting in a smaller model that can be further enhanced through continuous learning. Quantization methods \\cite{xiao2023smoothquant,nagel2019data,nagel2020up,wang2019haq,lin2023awq,frantar2022gptq} significantly reduce the model size and notably accelerate inference speed. Additionally, speculative decoding strategies \\cite{stern2018blockwise,chen2023accelerating,leviathan2023fast} expedite LLM inference by employing a compact draft model to decode tokens concurrently, showcasing innovative approaches to improve computational efficiency and model performance.\n\\vspace{-2mm}\n\\section{Methods}\n\\vspace{-2mm}\n\\begin{figure*}\n\\centering\n\\includegraphics[width=\\linewidth]{figures/teaser_arxiv.pdf}\n\\caption{Illustration of Dense Training of MoE models: Subfigure (a) illustrates the conventional sparse training method in MoE models, characterized by sparse gradient propagation in both the router and the experts. In subfigure (b), we detail the dense training strategy in our DS-MoE, which involves dense propagation of gradients for both routers and experts.}\n\\label{fig:method}\n\\end{figure*}\nIn this section, we first provide an overview of the MoE language model. Subsequently, we present our DS-MoE framework, detailing the process of densely training our MoE model prior to sparse inference, as well as two pivotal elements for DS-MoE framework: the mutual information (MI) loss and the Mixture of Attention Head (MoA) blocks.\n\n\\vspace{-2mm}\n\\subsection{Preliminary: Mixture-of-Experts Language Models} \\label{sec:preliminary}\nWe take the feed-forward network (FFN) in the transformer language model for example to illustrate the MoE architecture. An MoE FFN comprises $N$ experts and a router $\\mathbf{h}$, where each expert $\\mathbf{e}$ is an MLP module and the router predicts a score for each expert. Given an input token $\\mathbf{X}\\in \\mathbb{R}^{d_h}$, the MoE computes the output $\\mathbf{O}\\in \\mathbb{R}^{d_h}$ through:\n\\begin{align}\n     \\mathbf{S} = \\mathrm{softmax}\\big(\\mathbf{h(X)}\\big), \\quad\n     \\mathbf{O} = \\sum_{i=1}^{K}S_{\\mathbf{A}_i}\\cdot\\mathbf{e}_{\\mathbf{A}_i}\\mathbf{(X)},\n\\end{align}\nwhere $\\mathbf{S}\\in \\mathbf{R}^N$ is the score vector for the experts and $\\mathbf{A}_i$ is the index for $i^{th}$ of the $K$ experts with the highest scores. During training, gradients only backpropagate through the selected experts $\\mathbf{e}_{topK_i}$ and the corresponding scores $S_{topK_i}$. \n\n\\subsection{DS-MoE Framework}\nThe traditional MoE language models, despite being able to match the performance of dense models with approximately 40-50\\% of the computation, necessitate significantly more parameters, typically 2-3 times more. This increased requirement for parameters arises from the process of backpropagation in MoE models, which is sparsely optimized as we discussed in Section \\ref{sec:preliminary}, thus not as efficient as in dense models. Our approach trains the MoE model in a dense manner with an additional MI loss, and performs inference sparsely. This strategy aims to retain the runtime efficiency of traditional MoE models while achieving the parameter efficiency of dense models.\n\n\\paragraph{Dense Training.} \nThe fundamental concept of dense training revolves around optimizing the router using complete gradients. Unlike traditional MoEs, where the gradient of \\(\\mathbf{S}\\) is expressed as:\n\\begin{align}\n    \\mathbf{\\nabla S} = \\left[\\mathbf{e}_1(\\mathbf{X}),..., \\mathbf{e}_N(\\mathbf{X})\\right]^\\intercal \\mathbf{\\nabla O} \\odot \\mathbf{M},\n\\end{align}\nin this context, \\(\\mathbf{M}\\in \\{0,1\\}^{N}\\) serves as a binary mask identifying the activated experts. Specifically, \\(\\mathbf{M}_{i}=1\\) indicates that expert \\(\\mathbf{e}_i\\) is active in the forward pass, and \\(\\mathbf{M}_{i}=0\\) otherwise. To preserve all gradients of \\(\\mathbf{S}\\), we ensure that the output of every expert is computed during the forward pass, and this output is retained for use during backpropagation. The gradients of \\(\\mathbf{S}\\) and \\(\\mathbf{e}_i(\\mathbf{X})\\) in our approach are articulated as follows:\n\\begin{align}\n    & \\mathbf{\\nabla S} = \\left[\\mathbf{e}_1(\\mathbf{X}),..., \\mathbf{e}_N(\\mathbf{X})\\right]^\\intercal \\mathbf{\\nabla O}, \\quad\n     \\mathbf{\\nabla e}_j\\mathbf{(X)} = S_j\\nabla \\mathbf{O},\n\\end{align}\nwhere $j$ represents the expert index. As shown in Figure \\ref{fig:method}, our approach to training the MoE model densely involves activating all experts. \n\n\\paragraph{Sparse Inference.}\nDuring inference, only the top \\(K\\) experts, determined by their scores, are used. The selection of \\(K\\) is based either on a predetermined value or adaptively, depending on how many experts have scores above a specified threshold \\(\\epsilon\\). We adopt the SimpleMoE~\\cite{tan2024scattered} implementation for the sparse inference. More details can be found in Section \\ref{sec:inference}.\n\n\\paragraph{Mutual Information Loss.} \nTo achieve load balance among all experts and prevent the underutilization of model capacity, we integrate a Mutual Information (MI) loss into the router. This method, following \\cite{shen2023moduleformer}, aims to maximize the entropy of the expert distribution to ensure even distribution of workload:\n\\begin{align}\n    H(\\mathbf{e}) = - \\sum_{i=1}^{N}p(\\mathbf{e})\\log p(\\mathbf{e}),\n\\end{align}\nthereby promoting load balance across experts. In addition, to avoid the router adopting overly simplistic solutions and to ensure expert concentration, we minimize the conditional entropy of the expert distribution, \\( H(\\mathbf{e}|\\mathbf{X}) \\). The overall loss function is therefore defined as:\n\\begin{align}\n    \\mathcal{L}_{\\mathrm{MI}} & = -  H(\\mathbf{e}) + \\frac{1}{|\\mathcal{X}|} \\sum_{\\mathbf{X}\\in \\mathcal{X}} H(\\mathbf{e}|\\mathbf{X}),\n\\end{align}\nwhere \\( \\mathcal{X} \\) denotes the tokens in a minibatch. This approach not only ensures an equitable load balance among the experts but also maintains a high level of concentration on the appropriate solutions, optimizing the router's performance. The total loss is then calculates as\n\\begin{align}\n    \\mathcal{L} & = \\mathcal{L}_{\\mathrm{LM}} + \\alpha \\mathcal{L}_{\\mathrm{MI}}\n    \\label{eq:total-loss}\n\\end{align}\nwhere $\\mathcal{L}_{\\mathrm{LM}}$ is the standard autoregressive language modeling loss and $\\alpha$ is the weight for mutual information loss.\n\n\\paragraph{Mixture of Attention Head.} \nUnlike the majority of current MoE language models that retain a dense layer for self-attention, we have substituted our self-attention layer with a Mixture of Attention (MoA) heads layer \\cite{zhang2022mixture}. Our MoA heads are constructed following the usage of group-query attention (GQA) \\cite{ainslie2023gqa}, where key and value pairs are shared among a group of query vectors. In our implementation, each expert in the MoA heads is responsible for computing $N_{\\mathrm{head}}$ query vectors $\\mathbf{Q}\\in \\mathbb{R}^{d_\\mathrm{{head}}}$. For a given input token $\\mathbf{X}\\in \\mathbb{R}^{d_\\mathrm{h}}$, the output from an expert $e_i$ is derived as follows:\n\\begin{align}\n\\mathbf{Q}_{i} = \\mathbf{W_q}\\mathbf{X}, \\quad\n    \\mathbf{O}_{ij} = \\mathrm{softmax}(\\mathbf{Q}_{ij}\\mathbf{K}_j^\\intercal)\\mathbf{V}_j\\mathbf{W_o}_j,\n\\end{align}\nhere, $\\mathbf{W_q}\\in \\mathbb{R}^{N_{\\mathrm{head}}\\times d_\\mathrm{{head}}\\times d_\\mathrm{{h}}}$ represents the query projection weight for expert $\\mathbf{e}_i$. It is important to note that the key and value cache, represented by $\\mathbf{K}, \\mathbf{V}\\in \\mathbb{R}^{N\\mathrm{head} \\times L\\times d\\mathrm{head}}$, where $L$ is the length of the cache, is shared among all the experts.\nThe output projection for the expert is indicated by $\\mathbf{W_o}\\in \\mathbb{R}^{N_{\\mathrm{head}}\\times d_\\mathrm{head}\\times d_\\mathrm{h}}$. The final output of the layer is calculated as:\n\\begin{align}\n    \\mathbf{O} = \\sum_{k=1}^K S_{\\mathbf{A}_k} \\sum_{j=1}^{N_\\mathrm{head}}\\mathbf{O}_{\\mathbf{A}_kj},\n\\end{align}\nwhere $\\mathbf{A}$ is the index set for the activated experts.\n\\section{Empirical Study}\nIn this section, we comprehensively evaluate our DS-MoE, focusing on its performance in downstream tasks, sparsity, and GPU inference speed. The primary objective of our study is to investigate the advantages of DS-MoE compared to both dense models and SMoE models. We test our model and baselines in moderate-scale language modeling.\n\n\\subsection{Experimental Setup}\n\n\\paragraph{Dataset and Tokenization}\nWe pretrain our models using a subset of the Pile dataset \\cite{gao2020pile}, and apply tokenization using the CodeGen tokenizer \\cite{nijkamp2022codegen}. This dataset encompasses 300B tokens. Specifically, we utilize a 30B token subset for training our 1B-scale models and a 100B token subset for the training of models at the 3B and 6B scales.\n\n\\begin{table*}[h]\n    \\centering\n    \\small\n    \\caption{Model Architecture Hyperparameters. Here, $N_\\mathrm{att}$ and $N_\\mathrm{ffd}$ denote the number of experts in the self-attention layer and the MLP layer, respectively. In the case of the SMoE models, the top-2 experts are activated both during training and inference phases.}\n    \\begin{tabular}{c|ccccccccc}\n        \\toprule\n        \\multirow{2}{*}{Model} & \\multirow{2}{*}{$D_{\\text{emb}}$} & \\multirow{2}{*}{$N_\\mathrm{layer}$} & \\multirow{2}{*}{$N_\\mathrm{att}$} & \\multirow{2}{*}{$N_\\mathrm{head}$} & \\multirow{2}{*}{$D_\\mathrm{att}$} & \\multirow{2}{*}{$N_\\mathrm{ffd}$} & \\multirow{2}{*}{$D_\\mathrm{ffd}$} & total \\\\\n         & & & & & & & & params \\\\\n        \\midrule\n        Dense-1B & 2048 & 24 & 1 & 32 & 64 & 1 & 8192 & 1017M  \\\\\n        Dense-3B & 3072 & 28 & 1 & 32 & 96 & 1 & 12288 & 2705M \\\\\n        Dense-6B & 4096 & 36 & 1 & 32 & 128 & 1 & 16384 & 6186M \\\\\n        \\midrule\n        SMoE-1B & 2048 & 24 & 1 & 32 & 64 & 8 & 1024 & 1042M \\\\\n        SMoE-1.5B & 2048 & 24 & 1 & 32 & 64 & 12 & 1024 & 1445M  \\\\\n        SMoE-5B & 3072 & 28 & 1 & 32 & 96 & 16 & 1536 & 4911M \\\\\n        \\midrule\n        DS-MoE-1B & 2048 & 24 & 16 & 2 & 64 & 32 & 256 & 1067M \\\\\n        DS-MoE-3B & 3072 & 28 & 8 & 4 & 96 & 32 & 384 & 2846M \\\\\n        DS-MoE-6B & 4096 & 36 & 8 & 4 & 128 & 32 & 512 & 6343M  \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\label{tab:model_arch}\n\\end{table*}\n\\paragraph{Model Hyperparameters.} \nWe list the hyperparameter settings of different model architectures in Table~\\ref{tab:model_arch}. Here, $N_\\mathrm{att}$ and $N_\\mathrm{ffd}$ represent the number of experts in each attention layer and each feed-forward layer respectively. In our models, we use the GeLU \\cite{hendrycks2016gaussian} activation function. We use Grouped-Query Attention (GQA) \\cite{ainslie2023gqa} in our attention blocks. We use 2 shared key-value heads for the 1B models and 4 for the 3B and 6B models. \n\n\\begin{wraptable}{r}{0.45\\textwidth}\n    \\centering\n    \\caption{Value of $\\alpha$ in Our Models.}\n    \\begin{tabular}{c|cc}\n        \\toprule\n        Model  & $\\alpha$ in MoA & $\\alpha$ in MoE \\\\\n        \\midrule\n        DS-MoE-1B & $3.5e-4$ & $6.3e-4$ \\\\\n        DS-MoE-3B & $2e-4$ & $4e-4$ \\\\\n        DS-MoE-6B & $2e-4$ & $2e-4$ \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\label{tab:alpha}\n\\end{wraptable}\\paragraph{Training Details.} We train our models using the AdamW optimizer \\cite{loshchilov2017decoupled} with a learning rate of $3 \\times 10^{-4}$. The training includes a cosine learning rate schedule with a warmup of 1 billion tokens for 1B models and 2 billion for 3B and 6B models. We use a constant weight decay of 0.01 and clip gradients at 1.0 throughout the training. Batch sizes are 0.5 million tokens for 1B models and 2 million for 3B and 6B models, with a sequence length of 2048 tokens. To optimize training, we use fully sharded data parallelism \\cite{zhao2023pytorch, rajbhandari2020zero} and activation checkpointing \\cite{korthikanti2023reducing}. Training times are 24 hours for 1B models on 8 H100 80GB GPUs, 64 hours for 3B models and 124 hours for 6B models on 32 H100 GPUs. The mutual information loss weights ($\\alpha$ in Equation \\ref{eq:total-loss}) are listed in Table \\ref{tab:alpha}.  \n\n\\subsubsection{Evaluation Settings}\n\\paragraph{Baselines.} We compare our method against two baselines. \\textit{A. Dense model.} For each instance of our DS-MoE model, we train an analogous dense model. This model is designed to have a parameter size similar to that of our DS-MoE model, as detailed in Table \\ref{tab:model_arch}. The only parameter difference between the dense model and our DS-MoE model arises from the router function. \\textit{B. Sparse MoE.} We train the MoE model with traditional sparse gradient propagation to match the performance of Dense-1B and Dense-3B models. The Top-K is set to be 2 with the use of switch loss \\cite{fedus2022switch} for the load balance of the routers. For the implementation of the sparse MoE block, we employ dMoE \\cite{gale2023megablocks}. The model hyperparameters for our SMoE baselines are outlined in Table \\ref{tab:model_arch}. These baseline models aim to highlight the parameter inefficiencies found in traditional sparse MoE training approaches.\n\n\\paragraph{Downstream tasks.} We assess our models across a diverse array of downstream tasks, encompassing both common-sense reasoning and question-answering. These include evaluations on PiQA \\cite{bisk2020piqa}, HellaSwag \\cite{zellers2019hellaswag}, WinoGrande \\cite{sakaguchi2021winogrande}, SciQ \\cite{welbl2017crowdsourcing}, and Arc~\\cite{allenai:arc}. Additionally, we measure and report the model's perplexity on the Wikitext dataset \\cite{wikitext}. For all these evaluations, we utilize the LM evaluation harness \\cite{eval-harness} to ensure consistency and reliability in our testing methodology.\n\n\\begin{table*}\n    \\centering\n    \\caption{Evaluation of Base Models in Zero-shot and Language Modeling Tasks. The number of active parameter and the percentage of the active hidden are calculated across all the downstream tasks and the wikitext dataset. Acronyms: HS (HellaSwag), WG (WinoGrande).\n    }\n    \\begin{adjustbox}{width=\\textwidth,center}\n        \\begin{tabular}{c|Hcccccc|cc|cc}\n        \\toprule\n        \\multirow{2}{*}{Model} & \\multirow{2}{*}{Gradients} & \\multirow{2}{*}{HS} & \\multirow{2}{*}{PIQA} & \\multirow{2}{*}{WG} & \\multirow{2}{*}{SciQ} & \\multirow{2}{*}{Arc-e} & \\multirow{2}{*}{Arc-c} & Avg. & Wikitext & Active & Active\\\\\n         & & & & & & & & Perf.$\\uparrow$ & PPL$\\downarrow$ & Params & Hidden \\\\\n        \\midrule\n        Dense-1B  & Dense & 33.1 & 66.6 & 51.1 & 80.0 & 50.8 & 21.5 & 50.5 & 20.48 & 1017M & 100\\% \\\\\n        SMoE-1B & Sparse & 32.8 & 66.4 & 52.4 & 79.7 & 50.7 & 21.7 & 50.5 & 21.09 & 419M & 40\\% \\\\\n        SMoE-1.5B & Sparse & 33.1 & 67.7 & 52.5 & 79.7 & 50.5 & 22.8 & 51.0 & 20.32 & 419M & 29\\% \\\\\n        DS-MoE-1B & Dense & 33.7 & 68.1 & 50.8 & 81.1 & 52.4 & 22.2  & 51.4 & 20.37 & 439M & 41\\%\\\\\n        \n        \\midrule\n        Dense-3B & Dense & 40.4 & 71.4 & 58.7 & 86.0 & 59.6 & 26.1 & 57.0 & 14.77 & 2705M & 100\\%\\\\\n        SMoE-5B & Sparse & 40.1 & 70.7 & 56.5 & 85.6 & 58.4 & 24.8 & 56.0 & 14.93 & 1212M & 25\\% \\\\\n        DS-MoE-3B & Dense & 39.3 & 71.6 & 57.9 & 85.6 & 57.7 & 24.9 & 56.2 & 15.48 & 934M & 34\\% \\\\\n        \\midrule\n        Dense-6B  & Dense & 44.3 & 72.2 & 59.9 & 88.0 & 62.9 & 27.9 & 59.2 & 12.98 & 6186M & 100\\% \\\\\n        DS-MoE-6B & Dense &  43.5\t& 73.0 & 57.9 & 86.9 & 61.9 & 27.9 & 58.5 & 13.89 & 1813M & 29\\% \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\label{tab:base_performance}\n    \\end{adjustbox}\n\\end{table*}\n\n\\subsection{Results}\\label{sec:exp-results}\nIn Table~\\ref{tab:base_performance}, we count the mean active parameters across a range of zero-shot tasks as well as the Wikitext \\cite{wikitext} language modeling task. Additionally, we evaluate the proportion of active parameters within the hidden layers. For DS-MoE-1B model, we evaluate the performance of the dense model baseline and the sparse training baseline. For all the DS-MoE models, experts are activated based on a criterion where their normalized probability\n\\footnote{The normalized probability is calculated by multiplying the router's output probability by the total number of experts.} \nexceeds a threshold $\\epsilon$. The threshold $\\epsilon$ can be flexibly adjusted to balance the performance and sparsity.\n\n\\begin{figure*}\n\\centering\n\\includegraphics[width=0.95\\linewidth]{figures/performance_vs_h_usage.pdf}\n\\caption{We assess the sparsity in our DS-MoEs by gradually deactivating experts to attain increasingly sparse configurations, monitoring until a significant performance drop occurs.}\n\\label{fig:sparsity}\n\\end{figure*}We list the evaluation results of the baselines and our models Table~\\ref{tab:base_performance}, from which we derive three key insights.\n\\textbf{Firstly}, it is evident that the DS-MoE model demonstrates superior parameter efficiency compared to its sparsely trained counterpart.  For example, the table reveals that our DS-MoE-3B model not only aligns with the SMoE-5B model in terms of performance and computational expenses but does so with half the number of expert parameters in the MLP layers compared to the SMoE-5B model. This parameter efficiency improves the inference throughput when I/O is bounded as we demonstrated in Section \\ref{sec:inference}.\n\\textbf{Secondly}, applying dense optimization to experts can achieve comparable parameter efficiency to that of traditional dense models. The table demonstrates that, across all three varying sizes, our DS-MoE models either closely match or even surpass their dense model counterparts in downstream task performance and language modeling capabilities. This model performance is achieved with a way lesser count of active parameters, thereby significantly reducing computational costs. \\textbf{Thirdly}, the sparsity observed in DS-MoE models intensifies as the model size expands. This increase in sparsity is evident from the rising ratio of activated parameters within the hidden layers, indicating a clear trend across our DS-MoE models from 1B to 6B in size. Also as illustrated in Figure \\ref{fig:sparsity}, there is a strategic reduction in the number of sampled experts to a point where further reduction noticeably degrades performance. This is visually represented by the turning point moving towards the left (indicating increased sparsity) as the model size grows.   This pattern suggests an even higher level of sparsity in models of a larger magnitude, including those with over 70B parameters.\n\n\\subsection{Ablation Study and Analysis}\\label{sec:abl}\n\n\\begin{wraptable}{r}{0.6\\textwidth}\n    \\centering\n    \\caption{Effect of Different $\\alpha$ on our DS-MoE-6B model.}\n    \\begin{tabular}{cc|c|cc}\n        \\toprule\n         Active & Active & \\multirow{2}{*}{$\\alpha_{\\text{mlp}}$} & \\multirow{2}{*}{Avg.} & Wikitext \\\\\n         Params & Hidden & &  & PPL \\\\\n        \\midrule\n        1826M & 29\\% & $4e-4$ & 57.8 & \\textbf{13.9} \\\\\n        1813M & 29\\% & $2e-4$ & \\textbf{58.5} & \\textbf{13.9} \\\\\n        \\midrule\n        1496M & 24\\% & $4e-4$ & \\textbf{57.8} & \\textbf{14.0} \\\\\n        1497M & 24\\% & $2e-4$ & 56.9 & 16.1 \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\label{tab:6b-abl}\n\\end{wraptable}\\paragraph{Effect of $\\alpha$.} To explore the impact of the mutual information loss weight on model sparsity and performance, we conduct an ablation study using our DS-MoE-6B models. In this study, we fix the $\\alpha$ value at $2e-4$ in the self-attention layer, while in the MLP layer, we vary the weight from $2e-4$ to $4e-4$. This adjustment is made during the model training phase. For evaluation purposes, we modulate the $\\alpha$ value to ensure that both models operated at identical sparsity levels. We assess the models' performance on zero-shot tasks and their Wikitext perplexity at two sparsity levels: 24\\% and 29\\%. According to the results presented in Table \\ref{tab:6b-abl}, the model trained with $\\alpha=4e-4$ demonstrates resilience at higher sparsity levels, maintaining its performance across both tested sparsity thresholds. Conversely, the model trained with $\\alpha=2e-4$ exhibits diminished performance at the 24\\% sparsity level, though it has the best performance at the 29\\% sparsity level. Hence, we deduce that the $\\alpha$ parameter plays a pivotal role in balancing the model's tolerance to high sparsity against its overall performance.\n\n\\begin{wrapfigure}{r}{0.52\\textwidth}\n  \\centering\n  \\vspace{-3mm}\n  \\includegraphics[width=0.5\\textwidth]{figures/expert_sample_fig.pdf}\n\\caption{Expert Sampling Strategy Evaluation. We assess the impact of different expert sampling strategies on the Wikitext perplexity (PPL) using our DS-MoE-3B model.}\n\\label{fig:exp_sample}\n\\end{wrapfigure}\n\\paragraph{Expert Sampling Strategy.} \nWe explore various strategies for expert selection in our DS-MoE models. Employing a threshold on normalized expert probability yields significant reductions in active parameters but introduces challenges for real-world deployment, especially during batch inference where different tokens in the same batch may engage varying numbers of experts. To address this, we investigate two alternative sampling methods: TopK and Threshold-TopK. The TopK approach selects a set number of experts, $K$, in each MLP layer, activating all experts in the self-attention layers due to lower sparsity in self-attention. \nMeanwhile, the Threshold-TopK strategy sets a threshold for normalized expert probability, then determines the total and average number of experts activated per token in a batch, using this average as the $K$ value. These methods are demonstrated through the Wikitext perplexity and the active parameter count in our DS-MoE-3B model, as shown in Figure \\ref{fig:exp_sample}. By adjusting the sparsity — either by increasing the threshold or decreasing $K$ — we find that all three expert sampling strategies strike an effective balance between computational efficiency and Wikitext perplexity. The Threshold strategy achieves the best trade-off, whereas the TopK and Threshold-TopK methods are more adaptable for real-world applications.\n\n\\begin{figure*}\n\\centering\n\\includegraphics[width=\\linewidth]{figures/layer_utilization.pdf}\n\\caption{Layer Utilization Assessment. We determine the average proportion of activated experts within both the self-attention and MLP layers. This analysis is conducted using the Wikitext dataset with our DS-MoE-3B model.}\n\\label{fig:layer_util}\n\\end{figure*}\\paragraph{Layer Utilization.} In Figure \\ref{fig:layer_util}, we showcase the average percentage of active experts in each layer for a threshold value of \\(\\epsilon=0.48\\), utilizing data from experiments conducted with the DS-MoE-3B model. The figure is augmented with error bars that depict the range of activated experts per layer, highlighting the maximum and minimum counts observed. Our findings highlight two key observations: (1) The MLP layer exhibits significantly greater sparsity compared to the self-attention layer, a trend that persists in our 6B model even when the weighting of the MI loss is identical across both the self-attention and MLP layers. (2) Within a single layer, the activated number of experts for processing different tokens exhibits substantial variance. Although sparsely trained MoEs traditionally employ a fixed number of experts, denoted as \\(K\\), for each layer and token, our results suggest that adhering strictly to this fixed assumption may lead to computational inefficiencies.\n\n\\subsection{GPU Inference Analysis}\\label{sec:inference}\n\\begin{table*}[h]\n    \\centering\n    \\small\n    \\caption{Inference Speed of Dense Models and DS-MoE Models. Top-K represents the number of active experts in the MLP layer. We evaluate the model inference speed by measuring the latency (second) and the input token throughput (token per second). The models are deployed on HuggingFace's transformers \\cite{wolf2020transformers}.}\n    \\begin{adjustbox}{width=\\textwidth,center}\n    \\begin{tabular}{c|cccc|cc|cc}\n        \\toprule\n        \\multirow{2}{*}{Model} & Total & Active & \\multirow{2}{*}{Top-K} & Wikitext & \\multirow{2}{*}{Latency} & \\multirow{2}{*}{Speedup} & \\multirow{2}{*}{TPS} & \\multirow{2}{*}{Speedup} \\\\\n         & Params & Params & & PPL & & & \\\\\n        \\midrule\n        Dense-3B & 2705M & 2705M & N/A & 14.77 & 4.28 & \\multirow{2}{*}{1.16$\\times$} &  40854.5& \\multirow{2}{*}{1.51$\\times$} \\\\\n        DS-MoE-3B & 2793M & 1039M & 6 & 15.63 & 3.68 & & 61515.9 \\\\\n        \\midrule\n        Dense-6B & 6186M & 6186M & N/A & 12.98 & 8.58 & \\multirow{2}{*}{1.49$\\times$} & 18354.2 & \\multirow{2}{*}{1.91$\\times$} \\\\\n        DS-MoE-6B & 6338M & 2043M & 4 & 13.92 & 5.75 &  & 35046.7 \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\label{tab:latency}\n    \\end{adjustbox}\n\\end{table*}\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=0.95\\textwidth]{figures/speedup_fig.pdf}\n\\caption{Input and Output Token Throughput. We measure the input and output token throughput of each model on an A100-80GB GPU. The X-axis represent the model size of the dense model. In this comparison, we contrast each dense model with its corresponding performance-matched DS-MoE and SMoE models. The details of the model hyperparameters can be found in Table~\\ref{tab:scaled}.}\n\\label{fig:speedup}\n\\end{figure}In this section, we evaluate the inference performance of the dense model and our DS-MoE model on GPUs. We assess inference speed using three key metrics: (1) \\textbf{Latency}, which measures the total time taken by the model to process the text input and generate a complete response. This evaluation involves processing a batch of 64 sentences, each comprising 2,000 tokens, with the model producing 20 tokens in response. (2) \\textbf{Input token throughput}, which is the speed at which the model processes and encodes input tokens. For this metric, we set the input token sequence length at 256 and adjust the batch size to its maximum to optimize the utilization of GPU memory. (3) \\textbf{Output token throughput}, which is measured as the model's ability to generate tokens per second from the given input tokens. This is evaluated under conditions where the model decodes 512 tokens utilizing a key-value cache mechanism. These benchmarks for assessing performance are carried out on an A100-80GB GPU. We utilize the \\texttt{ParallelLinear} operation \\cite{tan2024scattered} for sparse inference in the MLP layer and employ \\texttt{torch.nn} \\cite{paszke2019pytorch} to perform dense inference in the self-attention layer. Figure~\\ref{fig:layer_util} reveals that the layer utilization of the self-attention layer consistently exceeds 60\\%. We find that at this level of sparsity, sparse inference can become even slower than dense inference, primarily due to operation overheads of dynamic inference such as the duplication of intermediate tokens and the aggregation of outputs from various experts.\n\nWe first compare our DS-MoE model with the dense models regarding latency and input throughput in Table \\ref{tab:latency}. We employ the TopK inference strategy, as elaborated in Section \\ref{sec:abl}. As we can see in Table \\ref{tab:latency}, the DS-MoE model consistently achieves a speedup across both metrics. Notably, the speedup effect amplifies on the DS-MoE-6B model with an increase in model size. This correlation is attributed to the models becoming sparser with larger parameter sizes, a phenomenon detailed in Section \\ref{sec:exp-results}. Additionally, with the augmentation in model size, the models lean more towards being computation-bounded, making the operation overheads of dynamic inference increasingly insignificant. \n\n\\begin{table}\n    \\centering\n    \\caption{Hyperparameters of the Scaled Models for the Speed Test. We maintain the total number of heads $N_\\mathrm{att}\\times N_\\mathrm{head}$ as 32 and increase the number of layers $N_\\mathrm{layer}$ to 36.}\n    \\begin{tabular}{c|cHcccc|cc}\n        \\toprule\n    \\multirow{2}{*}{Model} & \\multirow{2}{*}{$D_{\\text{emb}}$} & \\multirow{2}{*}{$N_\\mathrm{layer}$} & \\multirow{2}{*}{$D_\\mathrm{att}$} & \\multirow{2}{*}{$N_\\mathrm{ffd}$} & \\multirow{2}{*}{$D_\\mathrm{ffd}$} & \\multirow{2}{*}{Top-K} & total & active\\\\\n & & & & & & & params & params\\\\\n        \\midrule\n        Dense-10B &  \\multirow{3}{*}{5120} & \\multirow{3}{*}{36} & \\multirow{3}{*}{160} & 1 & 20480 & 1 & 9667M & 9667M \\\\\n        SMoE-17B &  &  & & 16 & 2560 & 2 & 17413M & 4201M \\\\\n        DS-MoE-10B &  &  & & 32 & 640 & 4 & 9857M & 3257M \\\\\n        \\midrule\n        Dense-14B & \\multirow{3}{*}{6144} & \\multirow{3}{*}{36} & \\multirow{3}{*}{192} & 1 & 24576 & 1 & 13923M & 13923M \\\\\n        SMoE-25B &  &  &  & 16 & 3072 & 2 & 25029M & 6004M \\\\\n        DS-MoE-14B &  &  &  & 32 & 768 & 4 & 14149M & 4645M \\\\\n        \\midrule\n        Dense-19B & \\multirow{3}{*}{7168} & \\multirow{3}{*}{36} & \\multirow{3}{*}{224} & 1 & 28672 & 1 & 18951M & 18951M \\\\\n        SMoE-34B &  &  &  & 16 & 3584 & 2 & 34023M & 8127M\\\\\n        DS-MoE-19B &  &  &  & 32 & 896 & 4 & 19216M & 6278M\\\\\n        \\bottomrule\n    \\end{tabular}\n    \\label{tab:scaled}\n\\end{table}\n\nThen, to thoroughly examine the inference advantages of our DS-MoE model, we conduct a comparison with both dense model and SMoE models at larger model scales. To ensure the models we compare have matched performance, we assume the SMoE to have $2\\times$ the parameters in the MLP layer as both the dense and DS-MoE models, referencing Table \\ref{tab:base_performance} and relevant literature \\cite{dai2024deepseekmoe, shen2023moduleformer}. We enlarge the models by increasing both the embedding dimension ($D_{\\text{emb}}$) and the number of hidden layers ($N_{\\text{layer}}$), while keeping the number of experts and attention heads constant. The hyperparameters for these scaled models are detailed in Table \\ref{tab:scaled}. We evaluate both the input and output throughput of the models, corresponding to computation-bounded and I/O-bounded scenarios, respectively. As illustrated in Figure \\ref{fig:speedup}(a), in computation-bounded scenarios, our DS-MoE model demonstrates a significantly higher input throughput, particularly when compared to the dense model, showcasing its computational efficiency. In contrast, Figure \\ref{fig:speedup}(b) reveals that while our DS-MoE model achieves comparable throughput to the dense model, it significantly outperforms the SMoE model in terms of throughput, highlighting its parameter efficiency.\n\n\\paragraph{Comparison with other MoEs.} We further deploy our DS-MoE models with vLLM \\cite{kwon2023efficient} to benchmark our inference speed against other models at the 7B performance tier. For comparison, we select the Mistral-7B \\cite{jiang2023mistral}, which stands out as one of the leading open-source 7B models. According to Table \\ref{tab:speed-comparison}, our DS-MoE-6B model demonstrates a speed increase of $1.86\\times$ and $1.64\\times$ over the Mistral-7B on A100-80GB GPU and H100-80GB GPU, respectively. For MoEs, we choose DeepSeekMoE-16B \\cite{dai2024deepseekmoe} and Qwen1.5-MoE-A2.7B \\cite{qwen}. Both of them are sparsely trained and comparable to the performance of 7B dense models. Table \\ref{tab:speed-comparison} illustrates that DeepSeekMoE-16B and Qwen1.5-MoE-A2.7B possess active parameters similar to those of DS-MoE-6B, but their total weights nevertheless occupy more than $2\\times$ the GPU memory compared to DS-MoE-6B. This affects both the maximum batch size that a GPU can handle and the I/O latency, subsequently impacting throughput. As shown in Table \\ref{tab:speed-comparison}, our DS-MoE-6B model is $1.50\\times$ and $1.27\\times$ faster than Qwen1.5-MoE-A2.7B on A100-80GB GPU and H100-80GB GPU, respectively.\n\\begin{table*}[h]\n    \\centering\n    \\small\n    \\caption{Speed Comparison with other MoEs. We deploy our Dense-6B and DS-MoE-6B models with vLLM \\cite{kwon2023efficient} and test the performance under the experimental setup where the number of input tokens are 1000 and output tokens are 1000. We measure the performance with two metrics: (1) \\textbf{Throughput}: requests processed per second; (2) \\textbf{TPS}: tokens processed per second. The GPU memory utilization is set to be 0.9.}\n    \\begin{tabular}{c|ccc|cc|cc}\n        \\toprule\n        \\multirow{3}{*}{Model} & \\multirow{2}{*}{Total} & \\multirow{2}{*}{Active} & \\multirow{2}{*}{Model} & \\multicolumn{2}{c|}{\\multirow{2}{*}{\\textbf{A100-80GB}}} & \\multicolumn{2}{c}{\\multirow{2}{*}{\\textbf{H100-80GB}}} \\\\\n         & \\multirow{2}{*}{Params} & \\multirow{2}{*}{Params} & \\multirow{2}{*}{Memory} & \\multirow{2}{*}{Throughput} & \\multirow{2}{*}{TPS} & \\multirow{2}{*}{Throughput} & \\multirow{2}{*}{TPS}\\\\\n         & & & & & & & \\\\\n        \\midrule\n        Dense-6B & 6.4B & 6.4B & 12.3 GiB & 1.04 & 2079.8 & 1.40 & 2808.7 \\\\\n        Mistral-7B & 7.2B & 7.2B & 13.5 GiB & 1.07 & 2140.8 & 1.52 & 3047.4 \\\\\n        DeepSeekMoE & 17.3B & 2.8B & 30.5 GiB & 1.17 & 2330.1 & 1.57 & 3144.1 \\\\\n        Qwen1.5-MoE & 16.4B & 2.7B & 26.7 GiB & 1.33 & 2665.7 & 1.81 & 3616.9 \\\\\n        DS-MoE-6B & 6.5B & 2.2B & 12.6 GiB & 2.00 & 3992.8 & 2.30 & 4603.9 \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\label{tab:speed-comparison}\n\\end{table*}\n\n\\section{Conclusion}\nIn this study, we investigate the application of dense pre-training, sparse inference for MoE (DS-MoE) models. Our analysis reveals that traditional sparse training methods do not optimize MoE models efficiently during the training phase, leading to suboptimal parameter effectiveness in the resulting MoE models. In contrast, we adopt a strategy where gradients are densely propagated to both the router and experts. This approach ensures our MoE models maintain the parameter efficiency comparable to dense models, while the computation efficiency similar with MoE models. Moreover, we demonstrate that our DS-MoE model achieves superior inference throughput in both computation-bounded and I/O-bounded scenarios. Upon comparing our DS-MoE against other MoE models, it is evident that our DS-MoE model achieves the highest throughput.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Mixture of LoRA Experts}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nLow-Rank Adaptation (LoRA)~\\citep{lora} has emerged as a pivotal technique for fine-tuning large pre-trained models, renowned for its efficacy across a wide array of tasks. The modular architecture of LoRA has catalyzed further research into the synergistic composition of multiple trained LoRAs, aiming to amplify performance across various tasks. However, the effective composition of these trained LoRAs presents a formidable challenge:\n(1) Linear arithmetic composition can lead to the diminution of the generative capabilities inherent in the original pre-trained models or the distinctive attributes of the individually trained LoRAs, potentially resulting in suboptimal outcomes.\n(2) Reference tuning-based composition exhibits limitations in adaptability and incurs significant computational costs due to the requirements to retrain a large model.\nIn response to these challenges, we propose \\textbf{M}ixture \\textbf{o}f \\textbf{L}oRA \\textbf{E}xperts (\\textbf{\\our{}}). \n\\our{} treats each layer of trained LoRAs as a distinct expert and implements hierarchical weight control by integrating a learnable gating function within each layer to learn optimal composition weights tailored specifically to the objectives of a given domain. \n\\our{} not only demonstrates enhanced performance in LoRA composition but also preserves the essential flexibility necessary for effective composition of trained LoRAs with minimal computational overhead. Extensive experiments conducted in both Natural Language Processing (NLP) and Vision \\& Language (V\\&L) domains validate the effects of \\our{}. Our code are available at~\\href{https://github.com/yushuiwx/MoLE.git}{\\texttt{https://github.com/yushuiwx/MoLE.git}}.\n\\end{abstract}\n\n\\section{Introduction}\n\\begin{wrapfigure}{R}{0.45\\textwidth}\n\\vspace{-8mm}\n\\centering\n\\includegraphics[width=\\linewidth]{worflow.pdf}\n\\vspace{-6mm}\n\\caption{\\textbf{Workflow of \\our{}}. In the training phase, \\our{} predicts weights for multiple LoRAs. In the inference phase, \\our{} can allocate weights to multiple LoRAs, or, without altering the gating weights, achieve a more flexible LoRA composition by masking out undesired LoRAs and recalculating and distributing weights proportionally.}\n\\label{fig:workflow}\n\\vspace{-5mm}\n\\end{wrapfigure}\nRecent advances in deep learning have been driven by large-scale pre-trained models such as OPT~\\citep{opt}, LLaMA~\\citep{llama} in the Natural Language Processing~(NLP) domain and CLIP~\\citep{clip}, DALL·E 2~\\citep{dell2} in the Vision \\& Language~(V\\&L) domain.\nThese models show outstanding performance across various tasks when fine-tuned on down-stream datasets, but their increasing size entails significant computational costs for full fine-tuning.\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=\\linewidth]{LoRA_fusion.pdf}\n\\\\\n\\makebox[0.32\\linewidth]{(a)}\n\\makebox[0.32\\linewidth]{(b)}\n\\makebox[0.32\\linewidth]{(c)}\n\\vspace{-2mm}\n\\caption{Overview of LoRA composition methods:~(a) Linear arithmetic composition (Eq.\\ref{Eq.Normalize-Combination}), which commonly applies the same composition weight $\\boldsymbol{W}_i$ to all layers of the $i^{th}$ LoRA. (b) Reference tuning-based composition involves retraining a large model by integrating outputs from multiple LoRAs using manually-crafted mask information. (c) Our \\our, which learns a distribution $\\Upsilon^{j}$ for the $j^{th}$ layer of LoRAs to determine the composition weight $\\boldsymbol{W}^{j}_i$.}\n\\label{fig:main_motivation}\n\\vspace{-3mm}\n\\end{figure*}\nTo mitigate this, LoRA~\\citep{lora} is introduced.\nBy freezing the pretrained model weights and injecting trainable rank decomposition matrices, LoRA is proven to be an effective fine-tuning methodology in scenarios with constrained computational resources~\\citep{lester2021power,an2022input}.\n\nWhile LoRA serves as plug-and-play plugins for pre-trained models, recent initiatives explores the composition of separate trained LoRAs to achieve joint generation of learned characteristics~\\citep{huang2023lorahub,zhang2023composing,ruiz2023dreambooth}. However, these efforts may encounter several challenges.\nAs shown in Figure~\\ref{fig:main_motivation} (a), linear arithmetic composition~\\citep{zhang2023composing,huang2023lorahub,han2023svdiff} composes trained LoRAs directly. However, composing multiple LoRAs (typically $\\geq$ 3) can impair the generative performance of pre-trained models. To mitigate this, weight normalization is applied prior to the composition, but may erase the unique characteristics of individual trained LoRAs as the composing weight of each LoRA is reduced (refer to Observation 1 in \\S~\\ref{Sec:Observation}). \nAnother approach, as depicted in Figure~\\ref{fig:main_motivation} (b), known as reference tuning-based composition~\\citep{gu2023mix}, is tailored for the V\\&L domain and achieves superior performance. However, it is limited in terms of LoRA flexibility due to the utilization of manually-designed masks and involves substantial training costs, necessitating a full model retraining.\nIn light of the above situation, an important question arises:\n\n\\begin{tcolorbox}[colback=gray!20, colframe=gray!50, sharp corners, center title]\n\\centering\n\\textit{\\small How can multiple trained LoRAs be composed dynamically and efficiently, while preserving \\\\ all their individual characteristics?}\n\\end{tcolorbox}\n\nTo address that issues, we introduce \\textbf{M}ixture \\textbf{o}f \\textbf{L}oRA \\textbf{E}xperts~(\\textbf{\\our{}}). Recognizing that individual layers of a trained LoRA exhibit distinct characteristics, which collectively define the overall characteristic of the trained LoRA (refer to Observation 2 in \\S~\\ref{Sec:Observation}), \\our{} involves modulating the weights of different trained LoRAs within each layer, which we refer to as ``hierarchical weight contro''. As shown in Figure~\\ref{fig:main_motivation}~(c), \\our{} views each layer of trained LoRAs as a individual expert and incorporates a gating function within each layer to learn the optimal composition weights based on a specified domain objective. This dynamically enhances desirable characteristics while mitigating less favorable ones, ultimately achieving a more effective composition of LoRAs and prevents the loss of desirable LoRA characteristics that may occur in linear arithmetic composition.\n\nAdditionally, unlike reference tuning-based composition~\\citep{gu2023mix}, our \\our{} maintains flexibility in composing multiple trained LoRAs with reduced computational costs.\nAs the workflow of \\our{} shown in Figure~\\ref{fig:workflow}, during training, \\our{} learns the gating function for multiple trained LoRAs and keep all other parameters frozen, resulting in minimal computational costs. During inference, \\our{} has two inference modes:\nIn the first mode, \\our{} utilizes all trained LoRAs with the learned gating function, preserving their individual characteristics with allocated weights.\nDuring the second mode, \\our{} allows manual masking of unwanted LoRAs and recalculates and distributes weights proportionally without the need for retraining.\nThese two modes enable \\our{} to adapt to different scenarios, providing a versatile and flexible approach for effective LoRA composition.\n\nWe validate the effects of \\our{} in both NLP and V\\&L domains. Our findings, encompassing both qualitative and quantitative results, demonstrate that \\our{} outperforms existing LoRA composition approaches.\nThe contributions of our paper are the following:\n\\begin{itemize}\n\\vspace{-1mm}\n    \\item We introduce a significant and intricate problem: how to dynamically and efficiently compose multiple trained LoRAs while preserving all their individual characteristics, to further investigate the applicability of LoRA in real-world scenarios.\n    \\item We introduce Mixture of LoRA Experts (\\our{}), a method that achieves a more efficient and flexible composition of multiple trained LoRAs by employing hierarchical weight control through learnable gating functions within each layer of trained LoRAs.\n    \\item Extensive experiments on both V\\&L and NLP domain demonstrate that \\our{} can enhance LoRA composition performance and mitigates issues associated with existing composition methods.\n\\end{itemize}\n\n\\section{Background}\n\n\\subsection{LoRAs Composition}\n\\label{Sec:LoRA Merging}\nLoRA~\\citep{lora} is a parameter-efficient fine-tuning method to adapt large models to novel tasks and shows superior performance~\\citep{lora,huang2023lorahub,zhang2023composing,sung2022vl}.\nIn practical applications, a individual LoRA often fall short of meeting user expectations. A common solution is to compose multiple trained LoRAs, each specialized in specific aspects (e.g., clothing or facial features), with the aim of creating a comprehensive character representation.\nResearch on LoRA composition is limited and primarily concentrates on two distinct methodologies as follows:\n\n\\noindent\\textbf{Linear arithmetic composition}. \nAs shown in Figure~\\ref{fig:main_motivation}~(a), the most commonly employed composition method is directly composing multiple LoRAs,~i.e., \n\\begin{equation}\n\\vspace{-1mm}\n\\hat{\\boldsymbol{W}} = \\boldsymbol{W} + \\sum_{i=1}^N \\Delta \\boldsymbol{W}_i, \n\\vspace{-1mm}\n\\label{Eq.Direcly-Combination}\n\\end{equation}\nwhere $\\boldsymbol{W}$ indicates the original parameter of pre-trained model and $\\Delta\\boldsymbol{W}_i$ denotes the $i^{th}$ trained LoRA.\nHowever, this manner may affect the original weight $\\boldsymbol{W}$ when $N$ increasing, thereby diminishing the model's generative capabilities.\nSo, it is common practice to normalize the composition weights, termed as normalized linear arithmetic composition, ~i.e., \n\\begin{equation}\n\\vspace{-1mm}\n\\hat{\\boldsymbol{W}} = \\boldsymbol{W} + \\sum_{i=1}^N w_i \\cdot \\Delta \\boldsymbol{W}_i,\n\\vspace{-1mm}\n\\label{Eq.Normalize-Combination}\n\\end{equation}\nwhere $\\sum_{i=1}^N w_i = 1$. This manner prevents any adverse impact on the embedding of the original model, but leading to the loss of individual LoRA characteristics, as the composing weight $w_i$ for each trained LoRA is reduced~\\citep{gu2023mix}. \n\nIn NLP domain, PEMs~\\citep{zhang2023composing} first  define arithmetic operators for LoRA, and explore the effectiveness of composing multiple LoRAs in several scenarios. LoRAhub~\\citep{huang2023lorahub} utilizes a gradient-free manner to estimate the composition weights of trained LoRAs and achieves adaptable performance on unseen tasks. In V\\&L domain, SVDiff~\\citep{han2023svdiff} introduces a arithmetic-based manner to compose multiple visual concepts into a single image.\n\n\\noindent\\textbf{Reference tuning-based composition}. As shown in Figure~\\ref{fig:main_motivation}~(b), reference tuning-based composition~\\citep{gu2023mix} tackles the limitations of linear arithmetic composition by introducing gradient fusion and controllable sampling. However, it suffers from compositional inflexibility due to manually designed masks, which necessitates retraining when incorporating different LoRAs or creating new masks. Moreover, this approach entails retraining large models, resulting in substantial computational costs.\n\nIt is important to note that reference tuning-based composition relies on position masks, which distinguishes it from our model. Consequently, direct comparisons may not be appropriate due to the fundamentally different underlying principles. Therefore, our primary focus in this paper is to compare \\our{} with linear arithmetic composition.\n\n\\subsection{Mixture-of-Experts}\nMixture-of-Experts~(MoE)~\\citep{moec} is a promising approach to scale up the number of parameters within the same computational bounds. \nDifferent from standard transformer models, each MoE layer consists of $N$ independent feed-forward networks $\\{\\boldsymbol{E}_i\\}^{N}_{i=0}$ as the experts, along with a gating function $\\alpha \\left(\\cdot\\right)$ to model a probability distribution indicating the weights over these experts' outputs.\nFor the hidden representation $\\boldsymbol{h} \\in \\mathbb{R}^{d}$ of input token, the gate value of routing $\\boldsymbol{h}$ to expert $\\boldsymbol{E}_i$ is denoted as:\n\\begin{equation}\n\\vspace{-2mm}\n\\alpha \\left(\\boldsymbol{E}_i\\right) = \\exp\\left(\\boldsymbol{h}\\cdot\\boldsymbol{e}_i\\right) / \\sum_{j=0}^{N}\\exp\\left(\\boldsymbol{h}\\cdot\\boldsymbol{e}_j\\right),\n\\end{equation}\nwhere $\\boldsymbol{e}_i$ denotes the trainable embedding of $\\boldsymbol{E}_i$. Then, the corresponding $k$ experts, according to the top-$k$ gated values, are activated and the output $\\boldsymbol{O}$ of the MoE layer is\n\\begin{equation}\n\\vspace{-2mm}\n\\boldsymbol{O} = \\boldsymbol{h} + \\sum_{i=0}^{N}\\alpha \\left(\\boldsymbol{E}_i\\right) \\cdot \\boldsymbol{E}_i\\left(\\boldsymbol{h}\\right).\n\\vspace{-2mm}\n\\end{equation}\n\n\\begin{figure*}[!tb]\n\\centering\n\\includegraphics[width=\\linewidth]{main_motivations.pdf}\n\\vspace{-2mm}\n\\caption{\\textbf{Left:} Results of (a) linear arithmetic composition~(Eq.~\\ref{Eq.Direcly-Combination}) and~(b) normalized linear arithmetic composition~(Eq.~\\ref{Eq.Normalize-Combination}) based on Dreambooth~\\citep{ruiz2023dreambooth}. \\textbf{Right:} Visualization of the effects for different layers in LoRA by selectively activating specific parameters from the network, moving from the beginning to the end. \n}\n\\label{fig:Observation}\n\\vspace{-4mm}\n\\end{figure*}\n\n\\section{Method}\nIn this section, we first introduce some motivating observations in \\S~\\ref{Sec:Observation}. Then, we introduce the structure details and training objectives of \\our{} in \\S~\\ref{Sec:MOLE} and \\S~\\ref{Sec:training}, respectively.\n\\subsection{Motivating Observation}\n\\label{Sec:Observation}\n\n\\begin{tcolorbox}[colback=gray!20, colframe=gray!50, sharp corners, center title]\n\\textit{\\small \\textbf{Observation 1}: \\textit{Directly composing multiple trained LoRAs~(Eq.~\\ref{Eq.Direcly-Combination}) impacts the model's generative ability, whereas applying weight normalization~(Eq.~\\ref{Eq.Normalize-Combination}) preserves this capacity but may sacrifice LoRA characteristics.}}\n\\end{tcolorbox}\n\nSpecifically, in V\\&L domain, as depicted in left of Figure~\\ref{fig:Observation}, we observe that directly composing multiple trained LoRAs into the original embedding led to significant parameter variations, resulting in meaningless output. Furthermore, when normalization was applied, some of the original characteristics of these trained LoRAs are indeed compromised. These observations align with those elaborated upon in~\\citep{gu2023mix}.\n\nIn NLP domain, when composing four or more LoRAs within the FLAN-T5~\\citep{flant5} model, we observed that the model's output became disordered.\nFurthermore, implementing weight normalization for LoRAs trained across five datasets, as presented in Table~\\ref{tb: NLP_motivation1}, led to a decreased performance of the composition model. This suggests that while weight normalization preserves generative capacity, it adversely affects the intrinsic qualities of these trained LoRAs.\n\n\\begin{tcolorbox}[colback=gray!20, colframe=gray!50, sharp corners, center title]\n\\textit{\\small \\textbf{Observation 2}: \\textit{Individual layers of a trained LoRA exhibit unique traits, which cumulatively define the LoRA's overall attributes.}}\n\\end{tcolorbox}\n\nInspired by the findings of~\\citep{voynov2023p+}, which revealed that different layers in text-to-image models govern various attributes, such as style and color, we investigate the features learned by different layers within LoRA.\nIn V\\&L domain, as illustrated in the right of Figure~\\ref{fig:Observation}, we observed that different layers of LoRA encode distinct features, such as dog coat color and facial features.\nIn NLP domain, we trained a single LoRA on a combined dataset comprising ANLI-R1~\\citep{ANLI}, ANLI-R2~\\citep{ANLI}, and QNLI~\\citep{QNLI} datasets, as depicted in Table~\\ref{tb: NLP_motivation2}. Notably, when evaluated on these sub-datasets, we observed significant variations in performance across different layers of this LoRA. Specifically, the layers ranging from 0\\% to 20\\% performed best on QNLI, the layers spanning from 40\\% to 60\\% excelled on ANLI-R2, and the layers covering 80\\% to 100\\% outperformed others on ANLI-R1.\n\\begin{wrapfigure}{R}{0.45\\textwidth}\n\\vspace{-10mm}\n\\centering\n\\includegraphics[width=\\linewidth]{main_MoLE.pdf}\n\\vspace{-6mm}\n\\caption{\\textbf{Illustration of proposed \\our{}}. \\our{} employs a learnable gating function that utilizes the outputs of multiple LoRAs at each layer to determine composition weights.}\n\\label{fig:Mole_structure}\n\\vspace{-13mm}\n\\end{wrapfigure}\nThis observation inspires that we can dynamically optimizes the layer-specific weights according to a defined domain objective, enhancing desirable characteristics while suppressing less favorable ones, thereby achieving a more effective composition of trained LoRAs.\n\n\\subsection{Mixture of Lora Experts}\n\\label{Sec:MOLE}\nDrawing inspiration from above observations, we introduce the Mixture of LoRA Experts.\n\nReferring to Figure~\\ref{fig:Mole_structure}, consider a transformer block within the pre-trained model, parameterized by $\\theta$ (encompassing both the multi-head attention layer and the feed-forward neural network), and a set of corresponding trained LoRAs $\\Omega = \\{\\Delta\\theta_i\\}^{N}_{i=0}$ where $N$ represents the number of trained LoRA candidates, when given a input $\\boldsymbol{x} \\in \\mathbb{R}^{L\\times d}$, the output of the pre-trained model block $\\theta$ is presented as $\\boldsymbol{F}_{\\theta} \\in \\mathbb{R}^{L\\times d}$:\n\\begin{align}\n\\boldsymbol{x}^{'}_{\\theta} &= \\boldsymbol{x} + f_{\\text{Attn}}\\Big(\\text{LN}\\big(\\boldsymbol{x}\\big)\\big|\\theta\\Big), \\\\\n\\boldsymbol{F}_{\\theta}\\big(\\boldsymbol{x}\\big) &= \\boldsymbol{x}^{'}_{\\theta} + f_{\\text{FFN}}\\Big(\\text{LN}\\big(\\boldsymbol{x}^{'}_{\\theta}\\big)\\big|\\theta\\Big),\n\\end{align}\nwhere $L$ and $d$ indicate the sequence length and the dimension of $\\boldsymbol{x}$, respectively. $f_{\\text{Attn}}\\left(\\cdot\\right)$ and $f_{\\text{FFN}}\\left(\\cdot\\right)$ denotes the multi-head attention layer and feed-forward neural network, respectively. LN refers to layer normalization.\nThe output of each LoRA is presented as $\\boldsymbol{E}_{\\Delta\\theta_i}\\left(\\boldsymbol{x}\\right) \\in \\mathbb{R}^{L\\times d}$,\n\\begin{align}\n\\boldsymbol{x}^{'}_{\\Delta\\theta_i} &= \\boldsymbol{x} + f_{\\text{Attn}}\\Big(\\text{LN}\\big(\\boldsymbol{x}\\big)\\big|\\Delta\\theta_i\\Big), \\\\\n\\boldsymbol{E}_{\\Delta\\theta_i}\\big(\\boldsymbol{x}\\big) &= \\boldsymbol{x}^{'}_{\\Delta\\theta_i} + f_{\\text{FFN}}\\Big(\\text{LN}\\big(\\boldsymbol{x}^{'}_{\\Delta\\theta_i}\\big)\\big|\\Delta\\theta_i\\Big).\n\\end{align}\n\nAfter that, \\our{} applies a learnable gating function $\\mathcal{G}\\left(\\cdot\\right)$ to model the optimal distribution of composition weights for outputs of these trained LoRAs.\nSpecifically, by taking $\\{\\boldsymbol{E}_{\\Delta\\theta_i}\\left(\\boldsymbol{x}\\right)\\}_{i=0}^{N}$ as input, $\\mathcal{G}\\left(\\cdot\\right)$ first apply concatenation (denoted as $\\oplus$) and normalization (for training stability),~i.e.\n\\begin{equation}\n    \\boldsymbol{E}_{\\Omega}\\left(\\boldsymbol{x}\\right) =  \\text{Normalization}\\Big(\\boldsymbol{E}_{\\Delta\\theta_0}\\left(\\boldsymbol{x}\\right)\\, \\oplus \\,\\ldots\\, \\oplus \\,\\boldsymbol{E}_{\\Delta\\theta_{N-1}}\\left(\\boldsymbol{x}\\right)\\Big),\n\\end{equation}\nwhere $\\boldsymbol{E}_{\\Omega}\\left(\\boldsymbol{x}\\right) \\in \\mathbb{R}^{\\xi}$ and $\\xi = N\\times L\\times d$. $\\oplus$ indicates the concatenation operation.\nThen we flatten and reduce the $\\boldsymbol{E}_{\\Omega}\\left(\\boldsymbol{x}\\right)$ to $N$-dimensions by a dot-product operation with the learnable parameter $\\boldsymbol{e} \\in \\mathbb{R}^{\\xi \\times N}$ in the gating function $\\mathcal{G}\\left(\\cdot\\right)$, \n\\begin{equation}\n\\varepsilon = \\text{Flatten}\\Big(\\boldsymbol{E}_{\\Omega}\\left(\\boldsymbol{x}\\right)\\Big)^{\\top} \\cdot \\boldsymbol{e}, \\quad \\varepsilon \\in \\mathbb{R}^{N},\n\\end{equation}\nThe gate value for each LoRA is computed as\n\\begin{equation}\n\\label{EQ. temp}\n\\mathcal{G}\\big(\\varepsilon_i\\big) = \\frac{\\exp\\big(\\varepsilon_i / \\tau\\big)}{\\sum_{j=1}^{N} \\exp\\big(\\varepsilon_j / \\tau\\big)},\n\\end{equation}\nthe temperature scalar $\\tau$ is learnable.\nThe final output $\\Tilde{\\boldsymbol{E}}_{\\Omega}(\\boldsymbol{x})$ of the gating function $\\mathcal{G}\\left(\\cdot\\right)$ is obtained by multiplying the output of each LoRA expert with the corresponding gating values, presented as\n\\begin{equation}\n\\Tilde{\\boldsymbol{E}}_{\\Omega}(\\boldsymbol{x}) = \\sum_{i=0}^{N}\\mathcal{G}_i\\left(\\varepsilon_i\\right)\\cdot\\boldsymbol{E}_{\\Delta\\theta_i}\\left(\\boldsymbol{x}\\right),\n\\end{equation}\nin which $\\Tilde{\\boldsymbol{E}}_{\\Omega}(\\boldsymbol{x}) \\in \\mathbb{R}^{L\\times d}$ and $\\mathcal{G}_i\\left(\\cdot\\right)$ represents the weight of the $i^{th}$ trained LoRA. So, the final output of this block is computed by adding the output of the gating function to the output of the pre-trained network:\n\\begin{equation}\n\\boldsymbol{O}\\left(\\boldsymbol{x}\\right) = \\boldsymbol{F}_{\\theta}\\left(\\boldsymbol{x}\\right) + \\Tilde{\\boldsymbol{E}}_{\\Omega}\\left(\\boldsymbol{x}\\right).\n\\end{equation}\nBesides, we conducted an exploration of \\our's performance when employing gating functions at different hierarchical levels~(layer-wise and matrix-wise, etc). Please refer to Section~\\ref{Sec:detailed_analysis}.\n\n\\subsection{Training Objective}\n\\label{Sec:training}\n\\noindent\\textbf{Gating Balancing Loss}.\nAs shown in Figure~\\ref{fig:gating_imbalance_compare}~(a), we observed that the average entropy of the distribution probabilities from the gating functions gradually decreases as the number of training steps increases,~i.e., the gating function tends to converge to a state where it always produces large weights for a early-stage well-performing LoRA~(e.g., shown in Figure.~\\ref{fig:gating_imbalance_compare}~(b), 68\\% gating probability for LoRA $\\beta$ among three LoRAs), leading to only a handful of LoRAs having a significant impact in the end and a loss of the characteristics of other LoRAs.\n\\begin{wrapfigure}{R}{0.45\\textwidth}\n\\vspace{-10mm}\n\\centering\n\\includegraphics[width=0.6\\linewidth, height=0.6\\linewidth]{gating_imbalance_compare.png}\n\\hspace{-5mm}\n\\includegraphics[width=0.39\\linewidth, height=0.6\\linewidth]{gating_imbalance_bar_compare.png}\n\\\\\n\\vspace{-1mm}\n\\makebox[0.6\\linewidth]{\\small (a)}\n\\hspace{-5mm}\n\\makebox[0.39\\linewidth]{\\footnotesize (b)}\n\\vspace{-2mm}\n\\caption{(a) The average gating entropy of all gating functions varies with the training steps. (b) The average weight distribution (\\%) of three LoRAs w and w/o $\\mathcal{L}_{\\text{balance}}$.}\n\\label{fig:gating_imbalance_compare}\n\\vspace{-14mm}\n\\end{wrapfigure}\nTo alleviate this, we propose a gating balancing loss $\\mathcal{L}_{\\text{balance}}$ as\n\\begin{equation}\n \\mathcal{L}_{\\text{balance}} = - \\log\\left(\\prod_{i=0}^{N} \\textbf{q}^{(i)}\\right),\n\\end{equation}\nwhere \n\\begin{equation}\n\\textbf{q}^{(i)} = \\frac{1}{M}\\sum_{k=1}^M \\frac{\\exp\\left(\\varepsilon_i^k / \\tau\\right)}{\\sum_{j=1}^{N} \\exp\\left(\\varepsilon_j^k / \\tau\\right)},\n\\end{equation}\nand $M$ represents the number of blocks where gating functions are placed and $N$ denotes the number of LoRAs. This balanced loss encourages balanced gating because it is minimized when the dispatching is ideally balanced.\n\n\\noindent\\textbf{Domain-specific Loss}. \nAdditionally, for adaptation to different domains, we employ distinct domain-specific training objectives denoted as $\\mathcal{L}_{\\text{D}}$.\nIn V\\&L domain. we employ unsupervised training with both local and global guidance from CLIP~\\citep{radford2021learning} to optimize \\our{}.\nIn NLP domain, we follow the loss function in FLAN-T5~\\citep{flant5}.\n\\begin{table*}[t]\n\\centering\n\\setlength{\\tabcolsep}{6pt}\n\\caption{Text-alignment and image-alignment results for multiple LoRAs composition in CLIP feature space. NLA denotes normalized linear arithmetic composition (Eq.~\\ref{Eq.Normalize-Combination}). The best performance is in bold.}\n\\vspace{-3mm}\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{cccc|ccc|ccc|ccc}\n\\toprule[1pt]\n\\# Visual Concepts & \\multicolumn{3}{c}{Text-alignment} & \\multicolumn{3}{c}{\\shortstack[c]{Image-alignment, \\\\ (Concept 1)}} & \\multicolumn{3}{c}{\\shortstack[c]{Image-alignment, \\\\ (Concept 2)}} & \\multicolumn{3}{c}{\\shortstack[c]{Image-alignment, \\\\ (Concept 3)}} \\\\\n \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n & NLA & SVDiff & \\our & NLA & SVDiff & \\our & NLA & SVDiff & \\our & NLA & SVDiff & \\our \\\\\n\\midrule\n\\small Fancy boot + Monster + Clock & 0.754 & 0.742 & 0.832%\n& 0.781 & 0.758 & 0.784 %\n& 0.791 & 0.749 & 0.801 %\n& 0.763 & 0.812 & 0.809 \\\\\n\n\\small Emoji + Car + Cartoon & 0.610 & 0.607 & 0.696 %\n& 0.619 & 0.734 & 0.839 %\n& 0.711 & 0.702 & 0.709 %\n& 0.652 & 0.686 & 0.679 \\\\\n\n\\small Vase + Wolf plushie + Teapot & 0.752 & 0.812 & 0.863 %\n& 0.687 & 0.807 & 0.835 %\n& 0.705 & 0.782 & 0.746 %\n& 0.653 & 0.694 & 0.721 \\\\\n\n\\small White Cat + Wolf plushie + Can & 0.704 & 0.772 & 0.780 %\n& 0.801 & 0.804 & 0.802 %\n& 0.678 & 0.763 & 0.825 %\n& 0.650 & 0.729 & 0.714 \\\\\n\n\\small Shiny sneaker + Wolf plushie + Teapot & 0.778 & 0.789 & 0.791 %\n& 0.812 & 0.783 & 0.690 %\n& 0.723 & 0.751 & 0.790 %\n& 0.688 & 0.676 & 0.721 \\\\\n\n\\small Car + Wolf plushie + Teapot & 0.635 & 0.681 & 0.684 %\n& 0.652 & 0.763 & 0.713 & %\n0.601 & 0.664 & 0.745 %\n& 0.685 & 0.612 & 0.707 \\\\\n\n\\small Can + Wolf plushie + backpack & 0.601 & 0.782 & 0.754 %\n& 0.653 & 0.705 & 0.767 %\n& 0.602 & 0.755 & 0.782 %\n& 0.681 & 0.738 & 0.723 \\\\\n\n\\small Golden Retriever + Wolf plushie + Teapot & 0.670 & 0.716 & 0.784 \n& 0.713 & 0.784 & 0.790 %\n& 0.601 & 0.802 & 0.809 %\n& 0.678 & 0.761 &  0.748 \\\\\n\n\\small Golden Retriever + Boot + Monster & 0.614 & 0.762 & 0.755%\n& 0.665 & 0.662 & 0.620 %\n& 0.748 & 0.832 & 0.862 %\n& 0.723 & 0.719 & 0.735 \\\\\n\n\\small Backpack dog + Bowl + Teapot & 0.607 & 0.712 & 0.703\n& 0.653 & 0.672 & 0.756 %\n& 0.734 & 0.720 & 0.755 %\n& 0.692 & 0.688 & 0.701 \\\\\n\n\\small Backpack dog + White Cat + Emoji & 0.648 & 0.703 & 0.717\n& 0.674 & 0.692 & 0.812 %\n& 0.719 & 0.741 & 0.701 %\n& 0.742 & 0.720 & 0.796 \\\\\n\n\\small Dog + Wolf + Backpack & 0.717 & 0.738 & 0.722 \n& 0.547 & 0.565 & 0.552 %\n& 0.679 & 0.681 & 0.707 %\n& 0.766 & 0.795 & 0.831\\\\\n\n\\small Cat + Sunglasses + Boot & 0.770 & 0.791 & 0.837 & 0.845 & 0.793 & 0.815 & 0.845 & 0.793 & 0.815 & 0.845 & 0.793 & 0.815 \\\\\n\n\\small Table + Can + Teapot & 0.836 & 0.827 & 0.810 & 0.753 & 0.770 & 0.741 & 0.751 & 0.799 & 0.806 & 0.818 & 0.771 & 0.829 \\\\\n\n\\small Robot + Dog + Clock & 0.663 & 0.638 & 0.693 & 0.689 & 0.764 & 0.797 & 0.645 & 0.674 & 0.710 & 0.661 & 0.715 & 0.717\\\\\n\n\\hc Average & 0.678 & 0.728 & \\textbf{0.759}& 0.715 & 0.746 & \\textbf{0.783} & 0.682 & 0.731 & \\textbf{0.756} & 0.686 & 0.708 & \\textbf{0.732} \\\\\n\\bottomrule[1pt]\n\\end{tabular}\n}\n\\label{tb: VL_main_tab2}\n\\vspace{-3mm}\n\\end{table*}\nThe overall training objective $\\mathcal{L}$ is the weighted sum of the above-mentioned two losses, represented as:\n\\begin{equation}\n   \\mathcal{L} = \\mathcal{L}_{\\text{D}} + \\alpha\\mathcal{L}_{\\text{balance}},\n\\end{equation}\nwhere $\\alpha$ is a coefficient for weight balancing.\n\n\\noindent\\textbf{Optimization Gating Function Only.}\nWe freeze all trained LoRAs and pre-trained model parameters, optimizing only the gating function's parameters. This helps preserve characteristics of trained LoRAs, particularly when training data is limited.\n\n\\section{Experiments}\n\\label{sec:experiment}\n\\subsection{\\our{} on V\\&L domain}\n\\noindent\\textbf{Experimental Setup.} For V\\&L domain, we apply \\our{} to multi-subjects text-to-image generation task and choose DreamBooth~\\citep{ruiz2023dreambooth}~(built on Stable Diffusion V2.1) as the base generator. \nFollowing the common setting~\\citep{han2023svdiff, TI}, where 2 to 3 concepts are typically composed into a new multi-concept image, we conduct experiments by composing three separate trained LoRAs.\nDuring training \\our, we process the image resolution to 512$\\times$512 and set learning rate as 1e-5. We use DDPM sampler~\\citep{DDPM} with 50 steps in each case and train 400 iterations for each required composition with batch size 2 and $\\alpha$ as 0.5.\n\n\\noindent\\textbf{Metrics and Compared Baselines.} Following~\\citep{ruiz2023dreambooth, han2023svdiff}, we evaluate our method on (1) Image alignment. The visual similarity of generated images with the individual composed concepts, using similarity in CLIP~\\citep{clip} image feature space, (2) Text-alignment of the generated images with given text prompts, using text-image similarity in CLIP feature space~\\citep{clip}. For each composition, we calculated the average scores among 200 generated images per prompt using 5 text prompts.\nWe compared our \\our{} with normalized linear arithmetic composition (Eq.~\\ref{Eq.Normalize-Combination}) and SVDiff~\\citep{han2023svdiff}. Additionally, to further validate the effectiveness of \\our, we also compare \\our{} with state-of-the-art multi-subjects generation methods (full-parameters training based), which can be found in Section~\\ref{Sec:detailed_analysis}.\n\\begin{table*}[t]\n\\centering\n\\setlength{\\tabcolsep}{4pt}\n\\caption{Text-alignment and image-alignment results for multiple LoRA experts composition in CLIP feature space. The best performance is in bold and \\underline{the second-best value} is indicated with an underline. NLA denotes normalized linear arithmetic composition (Eq.~\\ref{Eq.Normalize-Combination}). \\emph{SOTA full-parameter training methods are highlighted by} \\textcolor{white}{\\colorbox{purple!10}{\\quad}}.}\n\\vspace{-3mm}\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{ccccccccccc}\n\\toprule\n\\multirow{2}{*}{\\shortstack{\\# Number of Concepts}} & \\multicolumn{5}{c}{Text-alignment} & \\multicolumn{5}{c}{Average Image-alignment} \\\\\n\\cmidrule(lr){2-6} \\cmidrule(lr){7-11}\n\\tiny & NLA & \\cellcolor{purple!10} Custom & \\cellcolor{purple!10} Textual Inversion & SVDiff & \\our & NLA & \\cellcolor{purple!10} Custom & \\cellcolor{purple!10} Textual Inversion & SVDiff & \\our \\\\\n\\midrule\n3 & 0.678 & \\underline{0.751} & 0.709 & 0.728 & \\textbf{0.759} %\n& 0.694 & \\textbf{0.761} & 0.720 & 0.719 & \\underline{0.757} \\\\\n\n4 & 0.681 & \\textbf{0.735} & 0.721 & 0.717 & \\underline{0.725} %\n& 0.712 & \\textbf{0.760} & 0.736 & 0.721 & \\underline{0.742} \\\\\n\n5 & 0.652 & \\underline{0.731} & 0.704 & 0.723 & \\textbf{0.762} %\n& 0.682 & \\textbf{0.798} & 0.710 & 0.708 & \\underline{0.737} \\\\\n\n6 & 0.678 & 0.722 & \\textbf{0.735} & 0.709 & \\underline{0.727} %\n& 0.698 & 0.721 & \\textbf{0.747} & 0.712 & \\underline{0.736} \\\\\n\n\\hc Average & 0.672 & \\underline{0.734} & 0.717 & 0.719 & \\textbf{0.752} & 0.692 & \\textbf{0.760} & 0.728 & 0.715 & \\underline{0.743} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\label{tb: VL_compare_IT_custom}\n\\vspace{-5mm}\n\\end{table*}\n\n\\noindent\\textbf{Main Results.} As shown in Table~\\ref{tb: VL_main_tab2}, this study involves 15 different compositions of three visual subjects.\nThe overall results show that our method significantly outperforms other comparative methods in terms of Text-alignment score, with a 0.031 average improvement compared to SVDiff, as well as the Image-alignment score associated with three visual concepts~(e.g., 0.037 average improvement compared to SVDiff in concept 1), providing evidence of of our \\our's superior capability in accurately capturing and depicting the subject information of user-provided images, as well as displaying multiple entities concurrently within a single image.\nSignificantly, prior research~\\citep{kumari2023multi,gal2022image} indicates a trade-off between Text-alignment and Image-alignment scores in multi-subjects generation. Excelling in both scores is challenging, highlighting the strength of our \\our{}.\nAdditionally, as shown in Figure~\\ref{fig:VL_main_pic},~\\ref{fig:VL_main_pic2} and~\\ref{fig:VL_main_pic3}, our approach outperforms two other methods in preserving subject fidelity in generated images. The comparative methods often omit a subject, as seen in the NLA composition's failure to include elements like ``cat'' in Figure~\\ref{fig:VL_main_pic} (line 2) and ``barn'' in Figure~\\ref{fig:VL_main_pic2}, and SVDiff's inability to precisely represent ``dog'' and ``cat'' in Figure~\\ref{fig:VL_main_pic2}. Furthermore, while these methods can generate images with three subjects, there's a noticeable leakage and mixing of appearance features, resulting in lower subject fidelity compared to user-provided images. In contrast, our method effectively retains the subjects specified by the user, with each accurately depicted.\n\n\\begin{wrapfigure}{R}{0.55\\textwidth}\n\\vspace{-4mm}\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\setlength{\\tabcolsep}{3pt}\n\\begin{tabular}{lcccc}\n\\toprule\n\\# Task & Metric & LoRAHub & PEMs & \\our\\\\\n\\midrule\n\\underline{\\textbf{Translation}} \\\\\nWMT '14 En$\\rightarrow$Fr & BLEU & \\underline{27.4} & 25.6 & \\textbf{29.1} \\\\\nWMT '14 Fr$\\rightarrow$En & BLEU & \\underline{29.4} & 27.1 & \\textbf{31.3} \\\\\nWMT '16 En$\\rightarrow$De & BLEU & 24.6 & \\underline{24.9} & \\textbf{27.7} \\\\\nWMT '16 De$\\rightarrow$En & BLEU & \\textbf{29.9} & 28.0 & \\underline{29.1} \\\\\nWMT '16 En$\\rightarrow$Ro & BLEU & \\underline{17.7} & 15.2 & \\textbf{18.9} \\\\\nWMT '16 Ro$\\rightarrow$En & BLEU & \\underline{23.5} & 21.7 & \\textbf{25.1} \\\\\n\\hc Average &  & \\underline{25.4} & 24.2 & \\textbf{26.9}\\\\\n\\midrule \n\\underline{\\textbf{Struct to Text}} \\\\\nCommonGen & Rouge-1 & \\underline{53.7} & 48.8 & \\textbf{55.1} \\\\\n& Rouge-2 & \\textbf{23.1} & 22.4 & \\underline{23.1} \\\\\n& Rouge-L & \\underline{49.7} & 47.2 & \\textbf{53.9} \\\\\nDART & Rouge-1 & 45.3 & \\underline{46.2} & \\textbf{48.8} \\\\\n& Rouge-2 & \\underline{22.6} & 18.9 & \\textbf{23.5} \\\\\n& Rouge-L & 35.1 & \\textbf{37.6} & \\underline{36.0} \\\\\nE2ENLG & Rouge-1 & \\underline{41.1} & 40.7 & \\textbf{42.0} \\\\\n& Rouge-2 & \\underline{26.3} & 24.2 & \\textbf{29.0} \\\\\n& Rouge-L & 38.8 & \\textbf{42.1} & \\underline{41.8} \\\\\nWebNLG & Rouge-1 & \\underline{52.1} & 52.0 & \\textbf{54.5} \\\\\n& Rouge-2 & 23.9 & \\underline{24.6} & \\textbf{26.8} \\\\\n& Rouge-L & 45.2 & \\underline{47.8} & \\textbf{49.3} \\\\\n\\hc Average & & \\underline{38.1} & 37.7 & \\textbf{40.3} \\\\\n\\midrule\n\\underline{\\textbf{Closed-Book QA}} \\\\\nARC-c & EM & \\underline{51.7} & 50.4 & \\textbf{52.9} \\\\\nARC-e & EM& \\underline{69.7} & 65.7 & \\textbf{70.3} \\\\\nNQ & EM& \\underline{17.3} & 16.1 & \\textbf{23.5} \\\\\nTQA & EM & \\textbf{54.5} & 53.9 & \\underline{54.0} \\\\\n\\hc Average & & \\underline{48.3} & 46.5 & \\textbf{50.2} \\\\\n\\midrule \n\\underline{\\textbf{Big-Bench Hard~(BBH)}} \\\\\nBoolean Expressions & EM & \\underline{55.1} & 53.0 & \\textbf{57.3} \\\\\nCausal Judgement & EM & \\underline{57.6} & 51.1 & \\textbf{57.9} \\\\\nDate Understanding & EM & \\textbf{31.0} & 29.3 & \\underline{30.7} \\\\\nDisambiguation & EM & 46.6 & \\underline{47.2} & \\textbf{49.3} \\\\\nPenguins in a Table & EM & \\underline{41.4} & 39.8 & \\textbf{45.0} \\\\\nReasoning Objects & EM & \\underline{35.2} & \\textbf{37.5} & 33.7 \\\\\nRuin Names & EM & \\underline{19.9} & 19.3 & \\textbf{21.2} \\\\\n\\hc Average &  & \\underline{38.4} & 33.2 & \\textbf{42.2} \\\\\n\\midrule \n\\underline{\\textbf{Natural Language Inference~(NLI)}} \\\\\nANLI-R1 & EM & \\underline{81.0} &  80.3 & \\textbf{82.7} \\\\\nANLI-R2 & EM & \\underline{80.9} & 80.2 & \\textbf{82.4} \\\\\nANLI-R3 & EM & \\underline{77.4} & 76.6 & \\textbf{78.9} \\\\\nQNLI & EM & 77.6 & \\underline{78.0} & \\textbf{78.1}\\\\\n\\hc Average &  & \\underline{79.2} & 78.8 & \\textbf{80.5}\\\\\n\\bottomrule\n\\end{tabular}\n}\n\\vspace{-2mm}\n\\captionof{table}{Evaluation results on Translation, Struct to Text, Closed-Book QA, NLI and BBH. The \\textbf{best value} is in bold and \\underline{the second-best value} is underlined.}\n\\label{tab:NLP_Main}\n\\vspace{-15mm}\n\\end{wrapfigure}\n\\subsection{\\our{} on NLP domain}\n\\noindent\\textbf{Experimental Setup.} For NLP domain,~following~\\citep{huang2023lorahub}, we employ Flan-T5~\\citep{flant5} as our chosen LLM and created several LoRAs based on FLAN datasets. \nWe conducted extensive experiments across various tasks, including Translation, Natural Language Inference~(NLI), Struct to Text, Closed-Book QA, and multiple subtasks within the Big-Bench Hard (BBH)~\\citep{ghazal2013bigbench} dataset.\nWe train 800 iterations for each required composition of LoRAs with an initial learning rate of 1e-5, batch size 12 and $\\alpha$ as 0.5.\n\n\\noindent\\textbf{Compared Baselines.} We compared our \\our{} with recently released state-of-the-art LoRA composition methods: LoRAhub and PEMs.\n\n\\noindent\\textbf{Main Results.} The corresponding experimental results are encapsulated in the Table~\\ref{tab:NLP_Main}. In summary, our \\our{} surpasses state-of-the-art LoRA composition methods on five distinct datasets. Notably, on the BBH dataset, our \\our{} achieves an average performance improvement of 3.8 over LoRAHub and outperforms PEMs by a notable margin of 9.0.\nFurthermore, in the realm of generation tasks, specifically in Translation and Struct to Text categories, \\our{} consistently outshines its counterparts. In the Translation task set, it surpasses LoRAHub by an average margin of 1.5 and PEMs by 2.7. Correspondingly, within the Struct to Text task set, our model boasts an average performance superiority of 2.1 over LoRAHub and 2.6 over PEMs. These findings underscore the efficacy and versatility of our \\our{} in handling language generation tasks.\n\n\\section{Analysis}\n\\label{Sec:detailed_analysis}\n\\vspace{-1mm}\n\\noindent\\textbf{The effectiveness of gating balancing loss}. Figure~\\ref{fig:gating_imbalance_compare} (a) and (b) illustrate how our $\\mathcal{L}_{\\text{balance}}$ function mitigates the reduction in entropy rates within gating functions, leading to a more uniform composition weight distribution.\nThe performance comparison between \\our{} and \\our{} $_{w/o~\\mathcal{L}_{\\text{balance}}}$ in Table~\\ref{tb: NLP_tempare} underscores the performance enhancement achieved with the inclusion of $\\mathcal{L}_{\\text{balance}}$.\nAdditionally, we conducted an experiment wherein we solely increased the temperature $\\tau$ in Eq.~\\ref{EQ. temp}, as an alternative to adding $\\mathcal{L}_{\\text{balance}}$. Results in Table~\\ref{tb: NLP_tempare} shows declining performance in \\our{} variants \\our$^{\\tau_1}$, \\our$^{\\tau_2}$, \\our$^{\\tau_3}$ ($\\tau_1 \\prec \\tau_2 \\prec \\tau_3$) with increasing temperature. While temperature rise addresses gating imbalance, it restricts dynamic LoRA exploration in \\our, leading to inferior outcomes.\n\n\\noindent\\textbf{Further comparison with SOTA multi-concept generation methods}. In the absence of comparable LoRA composition methods in the V\\&L domain, we incorporated two leading multi-concept generation algorithms that do not utilize LoRA: Custom~\\citep{kumari2023multi} and Textual Inversion~\\citep{TI}, both of which emphasize full-parameter training for enhanced results.\nAs presented in Table~\\ref{tb: VL_compare_IT_custom}, \\our{} outperforms Textual Inversion in both image and text alignment and excels over Custom in text alignment. Furthermore, it's worth noting that our MoLE is more lightweight compared to these full-parameter training methods. These comparisons underscore the superior effectiveness of our MoLE relative to methods that involve extensive parameter tuning.\n\n\\noindent\\textbf{Scale to a larger number of LoRAs}. We explore the performance as the number of LoRAs increases. \nIn the NLP domain, experiments were conducted with varying numbers of LoRA (8, 24, 48, 128), as detailed in Table~\\ref{tb: NLP_large_number_lora}. Our \\our{} demonstrated optimal performance across these configurations, notably excelling with larger LoRA counts of 48 and 128, surpassing LoRAHub by \\textbf{2.5} and \\textbf{3.0}, respectively. Analysis revealed that LoRAHub's optimization algorithm often zeroes out many LoRA weights in larger arrays, thus underutilizing the potential of all LoRA. Conversely, \\our{} effectively overcomes this limitation. However, all methods, including \\our, showed performance declines with an extremely large number of LoRA (128), highlighting a need for further research in this area.\nIn the V\\&L domain, Table~\\ref{tb: VL_large_lora} shows experiments with increased composed LoRAs. While typical composition involve 3-4 visual concepts, our range was 3-6 to avoid ambiguity in outputs. Results indicate that \\our{} consistently outperforms other LoRA composition models in text and image alignment as the number of LoRAs increases, underscoring its robustness and superior composition capabilities.\n\n\\noindent\\textbf{Coarse-to-fine gating analysis}. To examine the impact of different granularity levels in gating functions, we delineated four levels in \\our: matrix-wise (\\our, gating at the parameter matrix level), layer-wise (\\our), block-wise (\\our), and network-wise (\\our), abbreviated as m-\\our, l-\\our, b-\\our, and n-\\our{} respectively.\nTable~\\ref{tb: c-to-f MOLE} reveals that intermediate granularities, b-\\our{} and l-\\our, achieved the highest performance. In contrast, the coarsest level, n-\\our, which involves minimal optimizable parameters (a single gating for the entire network), showed suboptimal outcomes.\nAdditionally, the finest granularity, m-\\our, underperformed, potentially due to its excessive control interfering with inherent relationships in LoRA parameters.\n\n\\noindent\\textbf{Generalization to new datasets}. To further validate the effectiveness of our \\our, we conducted generalization experiments. Specifically, all LoRA candidates and LoRA composition variants, including \\our, PEMs and LoRAHub, were trained on NLI tasks~(ANLI-R1, ANLI-R2, ANLI-R3, QNLI, and WNLI, among others). Subsequently, we evaluated these methods on the BBH dataset.\nAs illustrated in Table~\\ref{tab:NPL_generalization}, our \\our{} achieves an average performance advantage of 2.4 over LoRAHub and 3.7 over PEMs, underscoring its superior generalization ability.\n\n\\noindent\\textbf{Flexibility of \\our}. As discussed in Section~\\ref{Sec:LoRA Merging}, a well-designed LoRA composition method should not only achieve effective LoRA composition but also retain the characteristics of individual LoRA. It should be versatile enough to function as a standalone LoRA generator, ensuring its practical applications are flexible and widespread.\nFigure~\\ref{fig:retain_ability} displays a comparison of the qualitative results for the retaining ability of several composition methods, we find that our \\our{} can generate images that closely resemble the original features of the LoRA experts (e.g., dog ears, the color of the backpack), while other composition methods tend to produce confusion and loss of LoRA characteristics.\nBesides, as shown in Figure~\\ref{fig:workflow}, we can also degrade \\our{} by masking out the LoRA experts we do not wish to use, transforming it into a \\our{} that merges fewer LoRAs without affecting the composition effect of the remaining LoRAs. As shown in Figure~\\ref{fig:retain_ability2}, our \\our{} can achieve the same flexible LoRA composition as linear arithmetic composition method without altering the weights of \\our{}, while reference tuning-based composition~\\citep{gu2023mix} can not accomplish.\n\n\\noindent\\textbf{Hierarchical control analysis}. \\our{} aims to achieve improved LoRA composition effects through finer-grained hierarchical control. As illustrated in the Figure~\\ref{fig:gating_div_vis}, we visualize the weight distributions assigned by the gating functions learned by \\our{} at different levels in both NLP and V\\&L domains. \nWe observe that \\our{} adaptively assigns weights to different LoRA experts at various layers. Consequently, finer-grained weight combination methods lead to superior results.\n\n\\vspace{-1mm}\n\n\\section{Conclusion and Limitations}\n\\vspace{-2mm}\nIn this study, we introduce the Mixture of LoRA Experts (\\our) as a versatile and dynamic approach for composing multiple trained LoRAs. The key innovation of \\our{} lies in its learnable gating functions, which utilize the outputs of multiple LoRAs at each layer to determine composition weights. Our comprehensive evaluation in both the both NLP and V\\&L domains establishes that \\our{} outperforms existing LoRA composition methods.\n\n\\noindent\\textbf{Limitations}. As described in Section~\\ref{Sec:detailed_analysis}, when the number of LoRAs increases to a very large value (e.g., 128), despite our \\our~exhibiting superior performance, the performance of all LoRA composition methods, including our \\our, tends to decrease. This suggests that our \\our~still faces challenges when performing large-scale LoRA composition. It also highlights the significance of researching better approaches for handling large-scale LoRA composition effectively.\n\n\\clearpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2112.14397v2.tex",
        "arXiv-2404.05567v1.tex",
        "arXiv-2404.13628v1.tex"
    ],
    "group_id": "group_60",
    "response": "### Title: Advances in Training and Inference Strategies for Mixture-of-Experts (MoE) Models\n\n### Introduction\nThe field of deep learning has seen remarkable advancements, particularly in the development of large-scale pre-trained models such as Transformers, which have demonstrated superior performance across a multitude of tasks. However, the increasing size of these models poses significant challenges in terms of computational efficiency and resource utilization. To address these issues, researchers have introduced the Mixture-of-Experts (MoE) architecture, which selectively activates a subset of parameters to reduce computational costs while maintaining or enhancing performance. MoE models are characterized by their ability to scale model capacity without proportionally increasing computational costs, making them particularly useful in scenarios where computational resources are limited. Despite these benefits, MoE models often require a larger number of parameters compared to dense models, leading to increased memory demands and inefficiencies in Input/Output (I/O)-bound scenarios, such as autoregressive generation.\n\nThis summary focuses on three recent research papers that explore innovative training and inference strategies for MoE models. The first paper, \"EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate,\" proposes a two-phase framework to improve the training efficiency and model quality of MoE models. The second paper, \"Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models,\" introduces a hybrid dense training and sparse inference framework (DS-MoE) to enhance the parameter efficiency and computational performance of MoE models. The third paper, \"Mixture of LoRA Experts,\" presents a novel approach to dynamically and efficiently compose multiple trained Low-Rank Adaptation (LoRA) modules, which are used for fine-tuning large pre-trained models, into a single MoE structure.\n\n### Main Content of Each Paper\n\n#### EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate\nThe authors of \"EvoMoE\" identify two key issues in the training of MoE models: conformity and instability. Conformity refers to the tendency of the gating mechanism to route tokens to the same expert initially, leading to imbalanced workload distribution among experts. Instability pertains to the erratic routing behavior of tokens to experts, especially during the early stages of training, due to the random initialization of both the gate and the experts. To overcome these challenges, they propose an efficient end-to-end training framework called EvoMoE, which consists of two main phases: expert-diversify and gate-sparsify. In the expert-diversify phase, the model starts by training a single shared expert and then diversifies the experts through random masking techniques. This phase ensures that the experts have a foundational knowledge base before being activated individually. The gate-sparsify phase introduces a Dense-to-Sparse (DTS) gate mechanism, which begins as a dense gate that routes tokens to all experts and gradually anneals to a sparse gate. This approach allows for a more adaptive and stable routing mechanism, improving both the training efficiency and the final model quality.\n\nThe EvoMoE framework is evaluated on three popular tasks: masked language modeling (MLM) using RoBERTa, language modeling (LM) using GPT, and machine translation (MT) using Transformer. The results show that EvoMoE outperforms existing MoE methods, including Switch, BASE Layer, Hash Layer, and StableMoE, across these tasks. Specifically, EvoMoE achieves a 0.562 improvement in GLUE benchmark scores and a 0.403 improvement in average for the MLM task. For the LM task, EvoMoE reduces perplexity by 0.88 and achieves a 0.545 improvement in average perplexity. In the MT task, EvoMoE improves BLEU scores by 1.0 and achieves a 1.33x speed-up compared to Switch Transformer. Additionally, EvoMoE demonstrates scalability by improving performance and efficiency with more experts or MoE layers.\n\n#### Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models\nThe second paper, \"Dense Training, Sparse Inference,\" addresses the parameter inefficiency of MoE models during training. It introduces a hybrid framework called DS-MoE, which employs dense training and sparse inference. During training, DS-MoE activates all experts to ensure comprehensive optimization of the gating function and expert parameters. This dense training phase helps in achieving better convergence and performance. In contrast, during inference, DS-MoE activates only a subset of experts, determined by their scores, to reduce computational costs. This framework is particularly effective in scenarios where computational resources are limited but memory and I/O constraints are significant.\n\nThe DS-MoE framework includes a Mutual Information (MI) loss to balance the workload among experts and ensure that the gating function does not converge to a state where only a few experts are used. This loss helps in maintaining a balanced distribution of tokens across experts during training. Additionally, DS-MoE incorporates a Mixture of Attention Head (MoA) block, which replaces the dense self-attention layer with a sparse version. This modification ensures that the model remains efficient during inference while preserving the generative capabilities of the pre-trained model.\n\nThe authors evaluate DS-MoE on moderate-scale language modeling tasks and compare it against dense and sparse MoE models. DS-MoE demonstrates superior parameter efficiency, achieving comparable performance to dense models while activating only 30-40% of the model's parameters during inference. Furthermore, DS-MoE shows better throughput performance in both computation-bounded and I/O-bounded scenarios, highlighting its versatility and efficiency.\n\n#### Mixture of LoRA Experts\nThe third paper, \"Mixture of LoRA Experts,\" focuses on the dynamic composition of multiple trained Low-Rank Adaptation (LoRA) modules into a single MoE structure. LoRA is a parameter-efficient fine-tuning method that involves injecting trainable rank decomposition matrices into a pre-trained model. The authors observe that directly composing multiple LoRAs can diminish the generative capabilities of the model, while applying weight normalization can preserve these capabilities but may sacrifice the unique characteristics of individual LoRAs. To address these issues, they propose a hierarchical weight control mechanism using learnable gating functions within each layer of the trained LoRAs. This approach allows for the dynamic optimization of composition weights based on the specific domain objective, enhancing desirable characteristics while suppressing less favorable ones.\n\nThe Mixture of LoRA Experts (MoLE) framework is evaluated in both the Natural Language Processing (NLP) and Vision & Language (V&L) domains. In the V&L domain, MoLE is applied to multi-subject text-to-image generation tasks, and it outperforms existing LoRA composition methods in terms of text and image alignment scores. In the NLP domain, MoLE is tested on various tasks, including translation, natural language inference, and closed-book QA, and it consistently achieves better performance compared to other LoRA composition methods. The authors also conduct an ablation study to analyze the impact of different gating function granularities, finding that intermediate levels of granularity (block-wise and layer-wise) yield the best performance.\n\n### Commonalities and Innovations\nAll three papers aim to improve the efficiency and effectiveness of MoE models. They address the challenges of training MoE models, which typically involve random initialization and joint training of the gate and experts, leading to inefficiencies and suboptimal performance. The common theme among these papers is the introduction of novel training strategies to enhance the performance and efficiency of MoE models.\n\n- **EvoMoE** introduces a two-phase training framework that decouples the training of experts and the gate network. It starts with a dense training phase to ensure that all experts have a foundational knowledge base before transitioning to a sparse training phase. This approach helps in achieving better convergence and performance by avoiding the instability and conformity issues observed in traditional MoE training methods.\n- **DS-MoE** proposes a hybrid dense training and sparse inference framework. It ensures that all experts are activated during training to optimize the gating function and expert parameters comprehensively. During inference, only a subset of experts is activated based on their scores, reducing computational costs. The framework includes a Mutual Information (MI) loss to maintain a balanced distribution of tokens across experts and a Mixture of Attention Head (MoA) block to enhance computational efficiency during inference.\n- **MoLE** focuses on the dynamic composition of multiple trained LoRAs into a single MoE structure. It employs a hierarchical weight control mechanism using learnable gating functions within each layer of the trained LoRAs. This approach allows for the adaptive optimization of composition weights, preserving the unique characteristics of individual LoRAs while enhancing the overall performance of the model.\n\n### Comparison of Results and Discussion\nThe results from the three papers highlight the effectiveness of their respective approaches in improving the performance and efficiency of MoE models. EvoMoE demonstrates significant improvements in model quality and efficiency across various tasks, including masked language modeling, language modeling, and machine translation. DS-MoE achieves comparable performance to dense models while activating only 30-40% of the model's parameters during inference, showcasing its computational efficiency. MoLE outperforms existing LoRA composition methods in both NLP and V&L domains, achieving better performance in tasks such as translation, natural language inference, and closed-book QA.\n\nHowever, there are differences in the specific challenges addressed and the methodologies employed. EvoMoE primarily focuses on the early training phase and the instability of the gating mechanism, proposing a two-phase training framework to improve convergence and performance. DS-MoE addresses the parameter inefficiency during training by employing dense training and sparse inference, ensuring comprehensive optimization of the gating function and expert parameters while reducing computational costs during inference. MoLE tackles the issue of composing multiple trained LoRAs into a single MoE structure, introducing a hierarchical weight control mechanism to preserve the unique characteristics of individual LoRAs and enhance the overall performance of the model.\n\n### Conclusion\nThe main findings of these papers underscore the importance of innovative training and inference strategies in enhancing the performance and efficiency of MoE models. EvoMoE demonstrates the benefits of a two-phase training framework that decouples the training of experts and the gate network, leading to improved convergence and performance. DS-MoE highlights the advantages of dense training and sparse inference, achieving comparable performance to dense models while reducing computational costs. MoLE introduces a hierarchical weight control mechanism for the dynamic composition of multiple trained LoRAs, preserving their unique characteristics and enhancing the model's performance.\n\nFuture research directions could include further optimizations in system-level execution to achieve efficient training in both model quality and system performance. Additionally, exploring more sophisticated gating mechanisms and weight control strategies could lead to even better performance and efficiency in MoE models. The development of more scalable and flexible composition methods for LoRAs could also enhance the applicability of MoE models in real-world scenarios."
}