{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Automatic Chain of Thought Prompting \\\\ in Large Language Models}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n\nLarge language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like ``Let’s think step by step'' to facilitate step-by-step thinking before answering a question. \nThe other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. \nThe superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. \nWe show that such manual efforts may be eliminated by leveraging LLMs \nwith the ``Let’s think step by step'' prompt\nto generate reasoning chains for demonstrations one by one,\ni.e., \\emph{let's think not just step by step, but also one by one}. \nHowever, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at \\url{https://github.com/amazon-research/auto-cot}\n\n\\end{abstract}\n\n\\section{Introduction}\n\nLarge language models (LLMs) \\citep{brown2020language, lamda, gopher, palm} have performed impressively on complex reasoning tasks by decomposing the multi-step problems into intermediate steps before producing the answer. This reasoning process is elicited by a very recent technique: chain-of-thought (CoT) prompting  \\citep{cot_wei}. \n\n\\begin{figure}[htb]\n\\vspace{-3mm}\n  \\begin{center}\n   \\includegraphics[width=1\\columnwidth]{fig_example.pdf}\n  \\end{center}\n  \\caption{Zero-Shot-CoT \\citep{kojima2022large} (using the ``Let’s think step by step'' prompt) and Manual-CoT \\citep{cot_wei} (using manually designed demonstrations one by one) with example inputs and outputs of an LLM.}\n  \\label{fig_examples}\n\\end{figure}\n\nCoT prompting can be categorized into two major paradigms. One adds a single prompt like ``Let's think step by step'' after the test question to facilitate the reasoning chains in LLMs \\citep{kojima2022large}. Since this prompting paradigm is task-agnostic and does not need input-output demonstrations, it is called \\textbf{Zero-Shot-CoT} (left of Figure~\\ref{fig_examples}). With Zero-Shot-CoT, LLMs have shown to be decent zero-shot reasoners. The other paradigm is few-shot prompting with  manual reasoning demonstrations one by one \\citep{cot_wei}. Each \\emph{demonstration} has a question and a reasoning chain. A \\emph{reasoning chain} is composed of a \\emph{rationale} (a series of intermediate reasoning steps) and an expected answer.\nWith all the demonstrations being manually designed, this paradigm is referred to as \\textbf{Manual-CoT} (right of Figure~\\ref{fig_examples}).\n\nIn practice, Manual-CoT has obtained stronger performance than Zero-Shot-CoT \\citep{cot_wei,kojima2022large}. However, this superior performance hinges on the hand-drafting of effective demonstrations. Specifically, the hand-drafting involves nontrivial efforts in designs of both questions and their reasoning chains for demonstrations. Moreover, human efforts for designing task-specific demonstrations are even more: different tasks, such as arithmetic \\citep{multiarith} and commonsense reasoning \\citep{commonsenseqa}, require different ways of demonstrations. \n\nTo eliminate such manual designs, we advocate another \\textbf{Auto-CoT} paradigm to automatically construct demonstrations with questions and reasoning chains.\nSpecifically, Auto-CoT\nleverages LLMs \nwith the ``Let’s think step by step'' prompt\nto generate reasoning chains for demonstrations one by one,\ni.e., \\emph{let's think not just step by step, but also one by one}.\nHowever, we find that this challenge cannot be effectively addressed by simple solutions. For example, given a test question of a dataset, retrieving semantically similar questions and invoking Zero-Shot-CoT to generate reasoning chains will fail. Although LLMs are decent zero-shot reasoners, they are not perfect: Zero-Shot-CoT can still make mistakes in reasoning chains.\n\nTo mitigate the effect of reasoning chain mistakes from Zero-Shot-CoT, our analysis shows that\ndiversity of demonstration questions is the key. Based on this insight, we propose an Auto-CoT method to automatically construct demonstrations. Auto-CoT consists of two main steps. First, partition questions of a given dataset into a few clusters. Second, select a representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics.\n\nWe evaluate Auto-CoT on ten benchmark reasoning tasks including: (i) arithmetic reasoning (MultiArith \\citep{multiarith}, GSM8K \\citep{gsm8k}, AQUA-RAT \\citep{aqua}, SVAMP \\citep{svamp}); (ii) commonsense reasoning (CSQA \\citep{commonsenseqa}, StrategyQA \\citep{strategyqa}); (iii) symbolic reasoning (Last Letter Concatenation, Coin Flip) \\citep{cot_wei}. Experimental results show that with GPT-3, Auto-CoT consistently matches or exceeds the performance of Manual-CoT that requires manual designs. This indicates that LLMs can perform CoT reasoning by automatically constructing demonstrations. \n\n\\section{Related Work}  \nThis section reviews two lines of research that form the basis of this work: chain-of-thought (CoT) prompting for multi-step reasoning and in-context learning for inducing LLMs to learn from demonstrations.\n\n\\subsection{Chain-of-thought Prompting}\nCoT prompting is a gradient-free technique of inducing LLMs to produce intermediate reasoning steps that lead to the final answer. \\citet{cot_wei} formally studied the topic of CoT prompting in language models. This technique elicits LLMs to generate a coherent series of intermediate reasoning steps that lead to the final answer to a question. Studies have shown that LLMs can perform CoT reasoning with zero-shot prompting (Zero-Shot-CoT) \\citep{kojima2022large} or manually written few-shot demonstrations (Manual-CoT) \\citep{cot_wei}.\n\n\\paragraph{Zero-Shot-CoT.} \\citet{kojima2022large} showed that LLMs are decent zero-shot reasoners whose generated rationales have already reflected the CoT reasoning. This finding inspires our work to leverage the self-generated rationales for demonstrations. Generating rationales by LLMs was shown to be practical in a recent work \\citep{zelikman2022star}. In their work, an LLM is prompted to generate rationales and those rationales that lead to the correct answer are selected. The selection requires a training dataset of questions with annotated answers. In contrast, our work considers a more challenging scenario where only a set of test questions are given (without a training dataset), following CoT prompting studies by \\citet{cot_wei} and \\citet{kojima2022large}.\n\n\\paragraph{Manual-CoT.} Manual-CoT achieves stronger performance by eliciting the CoT reasoning ability with effective manual demonstrations. The demonstrations for the reasoning process are manually designed. However, the human efforts in designs of both\nquestions and their reasoning chains are nontrivial. Instead of addressing this limitation, recent studies mainly focus on hand-crafting more complex demonstrations or leveraging ensemble-like methods. One trend is problem decomposition. In least-to-most prompting \\citep{zhou2022least}, complex problems are reduced to sub-problems, and then the sub-problems are solved sequentially. The other trend is to vote over multiple reasoning paths for a test question. \\citet{cot_wei_sc} introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers. \\citet{wang2022rationale} and \\citet{li2022advance} introduced randomness in the input space to produce more diverse outputs for voting. They used manually-designed demonstrations as the seed set and generated additional rationales: leave one question from the seed set and use the remaining demonstrations to generate rationales for this question by the LLM.\nUnlike the aforementioned research lines that rely on manually-designed demonstrations, our work intends to eliminate manual designs with competitive performance.\n\n\\subsection{In-Context Learning}\nCoT prompting is closely related to in-context learning (ICL) \\citep{Radford2019LanguageMA,brown2020language}. ICL enables LLMs to perform a target task by feeding a few prompted examples as part of the input. Without gradient update, ICL allows a single model to perform various tasks universally. There are various research lines to improve the performance of ICL: (i) retrieving related demonstrations to the test instance where the popular practice is dynamically retrieving related training examples for a given test input \\citep{rubin2021learning,Su2022SelectiveAM}; (ii) augmenting with fine-grained information, such as incorporating task instruction \\citep{mishra2022cross,wei2022finetuned,sanh2022multitask}; (iii) manipulating  output probabilities of LLMs instead of directly computing the likelihood\nof target labels \\citep{holtzman2021surface,zhao2021calibrate,min2022noisy}.\n\nDespite the success of ICL, studies \\citep{liu2022makes,lu2022fantastically} have shown that the strength of ICL may vary widely depending on the choice of in-context demonstrations \\citep{liu2022few}. In detail, the formatting of the prompt, such as wording or order of demonstrations, may lead to performance fluctuations \\citep{webson2021prompt,zhao2021calibrate}. A recent work \\citep{min2022rethinking} even questioned the necessity of ground-truth input-output mapping: using incorrect\nlabels in the examples only marginally lowers the performance. However, the existing analysis of ICL is mainly based on standard classification and multi-choice datasets that only have simple <input$\\rightarrow$output> mappings. We discover that those findings may not be applicable to the CoT prompting scenario with more complex <input$\\rightarrow$rationale$\\rightarrow$output> mappings. \nFor example, mistakes in either the <input$\\rightarrow$rationale> mapping or the <rationale$\\rightarrow$output> mapping will lead to a dramatic performance drop (Appendix \\ref{appendix:demo_impact}).\n\n\\section{Challenge of Auto-CoT}\\label{sec:prelim}\n\nAs just discussed, the performance of ICL hinges on hand-crafted demonstrations. \nAs reported in Manual-CoT \\citep{cot_wei}, using demonstrations written by different annotators brings up to 28.2\\% accuracy disparity in a symbolic reasoning task, while changing the order of demonstrations results in less than 2\\% changes in most tasks.\nThis suggests that the key challenge of Auto-CoT lies in automatically constructing demonstrations with \\emph{good} questions and their reasoning chains.\n\nRecall that Manual-CoT hand-crafts a few (e.g., 8) questions in demonstrations. With similarity-based retrieval methods being widely adopted for prompting LLMs \\citep{rubin2021learning,Su2022SelectiveAM}, a promising candidate solution is to sample demonstration questions using similarity-based retrieval. We follow the more challenging assumption in CoT studies \\citep{cot_wei,kojima2022large} that only a set of test questions are given (without a training dataset). Following \\citet{liu2022makes}, we use Sentence-BERT \\citep{reimers-2019-sentence-bert} to encode questions. \nFor each question $q^\\text{test}$ in a test dataset, we sample demonstration questions $q^\\text{demo}_i$ ($i = 1, \\ldots, k$) from the rest of the questions. We design a \\textbf{Retrieval-Q-CoT} method to retrieve the top-$k$ (e.g., $k=8$) similar questions based on cosine similarity. To compare with this similarity-based method, we also test a relatively more diversity-based method: \\textbf{Random-Q-CoT}, which randomly samples $k$ other test questions for each test question. \n\nBoth Retrieval-Q-CoT and Random-Q-CoT invoke Zero-Shot-CoT \\citep{kojima2022large} to generate the reasoning chain $c^\\text{demo}_i$ (rationale and answer) for each sampled question $q^\\text{demo}_i$, as LLMs are decent zero-shot reasoners \\citep{kojima2022large}. \nWe use GPT-3 \\citep{brown2020language} with 175B parameters (text-davinci-002) for the LLM unless otherwise stated. \nOn a high level, both Retrieval-Q-CoT and Random-Q-CoT take the concatenation of $q^\\text{demo}_i, c^\\text{demo}_i$ pairs ($i = 1, \\ldots, k$) and $q^\\text{test}$ as input to predict the reasoning chain for $q^\\text{test}$, which contains the answer in the end (like right of Figure \\ref{fig_examples}).\n\n\\begin{wraptable}{r}{0.4\\textwidth}\n    \\centering\n        \\caption{Accuracy (\\%) of different sampling methods. Symbol $\\dagger$  indicates using training sets with annotated reasoning chains. \\label{tab:preliminary}}\n    \\setlength{\\tabcolsep}{2pt}\n\\begin{tabular}{l|c|cc}\\toprule\n {Method} & {MultiArith} & {GSM8K} & {AQuA}  \\\\\\midrule\n Zero-Shot-CoT & 78.7 & 40.7 & 33.5  \\\\\n Manual-CoT & \\textbf{91.7} & 46.9 & 35.8$\\dagger$ \\\\\n\\midrule\nRandom-Q-CoT & 86.2 & 47.6$\\dagger$ & 36.2$\\dagger$ \\\\\nRetrieval-Q-CoT & 82.8 & \\textbf{48.0}$\\dagger$ & \\textbf{39.7}$\\dagger$ \\\\\n\\bottomrule\n\\end{tabular}\n\\end{wraptable}\n\nTo our surprise, Retrieval-Q-CoT underperforms Random-Q-CoT on the arithmetic dataset MultiArith \\citep{multiarith} (Table \\ref{tab:preliminary}).\nNote that the retrieval methods were originally proposed in tasks with \\emph{annotated} labels \\citep{rubin2021learning,Su2022SelectiveAM}, however, invoking Zero-Shot-CoT does not guarantee entirely correct reasoning chains. \nThus, we hypothesize that the inferior performance of Retrieval-Q-CoT is caused by incorrect reasoning chains by Zero-Shot-CoT.\nTo test this hypothesis, we experiment with Retrieval-Q-CoT on two other datasets GSM8K \\citep{gsm8k} and AQuA \\citep{aqua} that have training sets with \\emph{annotated} reasoning chains. The results are shown with $\\dagger$ in Table \\ref{tab:preliminary}. Under the setting with annotated reasoning chains, Retrieval-Q-CoT even outperforms Manual-CoT. The result indicates that Retrieval-Q-CoT is effective when human annotations are available. \n\nAlthough human annotations are useful, such manual efforts are nontrivial.\nHowever, automatically generating reasoning chains via Zero-Shot-CoT underperforms Manual-CoT, especially when the challenge of question sampling is not addressed.\nTo design more effective Auto-CoT, we need to understand its challenge better.\n\n\\subsection{Retrieval-Q-CoT Fails due to Misleading by Similarity} \n\\label{subsec:Retrieval-Q-CoT Fails}\n\nSince Retrieval-Q-CoT uses a few prompting demonstrations like in Manual-CoT, Retrieval-Q-CoT is expected to perform competitively as well. However, reasoning chains (both rationales and answers) in Retrieval-Q-CoT are generated by Zero-Shot-CoT: they may have mistakes that lead to wrong answers.\nLet us simply call demonstrations with wrong answers as \\textit{wrong demonstrations}.\nIntuitively, after \\emph{similar} questions to a test question are retrieved, wrong demonstrations caused by Zero-Shot-CoT may mislead the same LLM to reason similarly with a wrong answer (e.g., replicating mistakes) for the test question.\nWe refer to this phenomenon as \\textit{misleading by similarity}.\nWe will investigate whether misleading by similarity contributes to the inferior performance of Retrieval-Q-CoT. \n\n\\begin{wrapfigure}{r}{0.4\\textwidth}\n    \\centering\n    \\pgfplotsset{width=6.5cm, height=4.5cm}\n    \\centering\n\\vspace{-5mm}\n  \\begin{tikzpicture}  \n        \\begin{axis}  \n        [  \n            ybar,\n            ymin=20, ymax=55,\n            ytick={20,30,40,50},\n            major x tick style = transparent,\n            bar width=16pt,\n            legend columns=1 row=2,\n            enlarge x limits=0.4,\n            ylabel={{Rate (\\%)}},\n            symbolic x coords={1,2,3}, \n            xticklabels={{Retrieval-Q-CoT}, {Random-Q-CoT}},\n            xtick=data,  ylabel style={align=right},\n            legend cell align=left,\n            legend style={\n                        at={(1.0,0.49)},\n                        anchor=south east,\n                        column sep=1ex,\n                },\n            ]  \n        \\addplot[ybar, fill=bananayellow,  postaction={pattern=north east lines}] coordinates {\n            (1,46.875)(2,25.781)\n        };  \n        \\end{axis}  \n    \\end{tikzpicture}  \n    \\caption{{Unresolving Rate}.\\label{fig:pre-unsolved}\n    }\n    \\vspace{-3mm}\n\\end{wrapfigure}\n\nTo begin with, we invoke Zero-Shot-CoT on all the 600 questions from the MultiArith dataset. Among them, we collect those 128 questions (denoted as $\\mathcal{Q}$) where Zero-Shot-CoT generates  wrong answers (error rate: $21.3\\% = 128/600$). \nAs we mentioned, with extra demonstrations, Retrieval-Q-CoT and Random-Q-CoT are expected to perform more competitively than Zero-Shot-CoT.\nAmong $\\mathcal{Q}$ where Zero-Shot-CoT fails, we call those where Retrieval-Q-CoT or Random-Q-CoT still fail as their \\textit{unresolved questions}. \nWe divide the number of unresolved questions by 128 (number of questions in $\\mathcal{Q}$) to calculate the \\textit{unresolving rate}. A higher unresolving rate means that a method more likely still makes mistakes like Zero-Shot-CoT.\nFigure \\ref{fig:pre-unsolved} shows that the unresolving rate of Retrieval-Q-CoT (46.9\\%) is much higher than Random-Q-CoT (25.8\\%). \nIt indicates that \nwith similar questions being sampled for test questions,\nRetrieval-Q-CoT is negatively affected by misleading by similarity.\n\nTo show that\nunresolved questions of Retrieval-Q-CoT\ntend to be similar,\nwe present a case study in Table~\\ref{tab:pre_failure}. In the left part, the retrieved demonstration questions are similar to the test question and ask  ``\\textit{how long will it take him to cook the rest?}'' The reasoning chains generated by Zero-Shot-CoT produce answers regarding ``\\textit{the total of}'' instead of ``\\textit{the rest}''. Following the demonstrations, Retrieval-Q-CoT also fails by misunderstanding the meaning of ``\\textit{the rest}''. In contrast, Random-Q-CoT correctly understands ``\\textit{the rest}'' better without making similar mistakes in the demonstrations, thanks to relatively more diverse (random) demonstrations. \n\n\\subsection{Errors Frequently Fall into the Same Cluster}\n\nMotivated by the observations in Table~\\ref{tab:pre_failure}, we use $k$-means to partition all the 600 test questions into $k=8$ clusters, where each cluster contains similar questions.\\footnote{We use Sentence-BERT \\citep{reimers-2019-sentence-bert} to encode questions and apply $k$-means for clustering.} With these clusters\nand reasoning chains generated by Zero-Shot-CoT (in Section \\ref{subsec:Retrieval-Q-CoT Fails}), now we are curious if certain clusters contain \nquestions where Zero-Shot-CoT frequently fails.\nThus, we calculate the error rate (questions with wrong Zero-Shot-CoT answers / total questions) for each cluster.\n\n\\begin{wrapfigure}{r}{0.4\\textwidth}\n    \\centering\n    {\n\\pgfplotsset{width=6.7cm, height=4.5cm}\n    \\begin{tikzpicture} % Tikz environment\n            \\begin{axis}  \n        [xticklabels={1,2,3,4,5,6,7,8},\n        major x tick style = transparent,xtick={0,1,2,3,4,5,6,7}, bar width=8pt,enlarge x limits=0.1,ymin=0, ymax=60,\n        ytick={0,20,40,60},\n  ylabel={Error Rate (\\%)},  \n  ylabel style={align=left},xlabel={}]\n        \\addplot[ybar, fill=cinnamon,  postaction={pattern=north west lines}] coordinates {\n            (0,9)(1,52)(2,16)(3,13)(4,16)(5,22)(6,29)(7,22)\n        };  \n        \\end{axis}  \n    \\end{tikzpicture}\n    \\vspace{-2mm}\n    \\caption{Clusters of similar questions.\\label{pre-cluster}}\n}\n\\end{wrapfigure}\n\nAs shown in Figure \\ref{pre-cluster}, there exists a cluster (Cluster 2) with frequent Zero-Shot-CoT errors (52.3\\%).\nThe phenomenon could be generic as Zero-Shot-CoT may lack some skills to solve some common problems in target tasks.\\footnote{We observe  similar phenomena when changing the cluster number or using other datasets (Appendix \\ref{appendix:hard_cluster}).}\nFor convenience of descriptions, let us call the cluster with the highest error rate as the \\emph{frequent-error cluster} (e.g., Cluster 2 in Figure \\ref{pre-cluster}). Therefore, the imperfect nature of generated reasoning chains in a zero-shot fashion\nposes risks of retrieving multiple similar questions inside a frequent-error cluster by using similarity-based methods. \nFor the test question in the frequent-error cluster, Retrieval-Q-CoT more easily constructs demonstrations with multiple similar mistakes. As a result, Retrieval-Q-CoT often makes similar mistakes like Zero-Shot-CoT, reiterated by its higher unresolving rate in Figure \\ref{fig:pre-unsolved}.\n\n\\begin{table}[t]\\centering\n\\footnotesize\n\\caption{Examples of Retrieval-Q-CoT and Random-Q-CoT for constructing demonstrations and prediction. The upper part contains the automatically constructed demonstrations, and the lower part contains test questions along with predicted reasoning chains.}\n\\vspace{2.8mm}\n\\begin{tabular}{p{0.52\\textwidth}|p{0.425\\textwidth}}\n\\toprule\n\n\\textbf{Retrieval-Q-CoT} &\n\\textbf{Random-Q-CoT}\n\\\\\n\n\\midrule\n\n\\bblack{Q:} A chef needs to cook 9 potatoes. He has already cooked 7. If each potato takes 3 minutes to cook, how long will it take him to cook \\underline{\\textbf{the rest}}?\n&\n\\bblack{Q:} Wendy uploaded 79 pictures to Facebook. She put 44 pics into one album and put {the rest} into 5 different albums. How many pictures were in each album?\n\\\\\n\\bblack{A:} Let's think step by step. The chef has already cooked 7 potatoes. That means it has taken him 7 * 3 minutes to cook those 7 potatoes. That means it will take him 3 more minutes to cook each of the remaining 2 potatoes. \\textcolor{red}{That means it will take him a total of 7 * 3 + 2 * 3 = 21 minutes to cook all 9 potatoes.} The answer is 21. \\ngmark\n&\n\\bblack{A:} Let's think step by step. Wendy uploaded 79 pictures to Facebook. She put 44 pictures into one album. \\textcolor{blue}{That means she put the rest of the pictures into 5 different albums.} That means each album would have (79-44)/5 = 15 pictures in each album. The answer is 15.\\okmark\n\\\\\n\\bblack{Q:} A chef needs to cook 13 potatoes. He has already cooked 5. If each potato takes 6 minutes to cook, how long will it take him to cook \\underline{\\textbf{the rest}}?\n&\n\\bblack{Q:} At a restaurant each adult meal costs \\$5 and kids eat free. If a group of 15 people came in and 8 were kids, how much would it cost for the group to eat?\n\\\\\n\\bblack{A:} Let's think step by step. The chef has already cooked 5 potatoes. That means it has taken him 30 minutes so far. There are 13 potatoes in total. 5 have been cooked so far. That means there are 8 left to cook. Each potato takes 6 minutes to cook. That means it will take the chef 48 minutes to cook the 8 remaining potatoes. \\textcolor{red}{In total, it will take the chef 78 minutes to cook all 13 potatoes.} The answer is 78.  \\ngmark\n&\n\\bblack{A:} Let's think step by step. There are 8 kids, so that means there are 7 adults. Each adult meal costs \\$5, so that means that the 7 adults will cost \\$5*7=\\$35. Now we just need to add the two numbers together. \\$35+8=43\\$. So it would cost \\$43 for the group to eat. The answer is 43.  \\ngmark\n\\\\\n\\midrule\n\\bblack{Q:} A chef needs to cook 15 potatoes. He has already cooked 8. If each potato takes 9 minutes to cook, how long will it take him to cook \\underline{\\textbf{the rest}}?\n&\n\\bblack{Q:} A chef needs to cook 15 potatoes. He has already cooked 8. If each potato takes 9 minutes to cook, how long will it take him to cook \\underline{\\textbf{the rest}}?\n\\\\\n\\bblack{A:} Let's think step by step. The chef has already cooked 8 potatoes. That means it has taken him 8 * 9 minutes to cook those 8 potatoes. That means it will take him 9 minutes to cook each of the remaining 7 potatoes. \\textcolor{red}{That means it will take him a total of 8 * 9 + 7 * 9 = 135 minutes to cook all 15 potatoes.} The answer is 135. \\ngmark\n&\n\\bblack{A:} Let's think step by step. The chef needs to cook 15 potatoes. He has already cooked 8. That means he has to cook 15-8=7 more potatoes. Each potato takes 9 minutes to cook. \\textcolor{blue}{That means it will take him 9*7=63 minutes to cook \\underline{\\textbf{the rest}} of the potatoes.} The answer is 63. \\okmark\n\\\\\n\\bottomrule\n\\end{tabular}\n\\vspace{-3.6mm}\n\\label{tab:pre_failure}\n\\end{table}\n\n\\subsection{Diversity May Mitigate Misleading by Similarity}\n\\label{subsec:diversity may mitigate}\n\nThe analysis so far compellingly shows that LLMs are still not \\emph{perfect} zero-shot reasoners; thus, we aim to mitigate the effect of their Zero-Shot-CoT errors, especially to mitigate misleading by similarity in the design of Auto-CoT.\n\nAs we will show later (Section \\ref{analysis:incorrect}), presenting a small portion of mistakes (e.g., 1 or 2 wrong demonstrations out of 8) would not harm the overall reasoning performance for test questions. \nSuppose that questions of all the wrong demonstrations fall into the same frequent-error cluster; then sampling one question from every different cluster will lead to a higher than $7/8=87.5\\%$ chance to construct all the 8 correct demonstrations.\nSince different clusters reflect diverse semantics of the questions, this clustering-based sampling method can be considered as diversity-based, which is in sharp contrast to similarity-based Retrieval-Q-CoT.\nOn one hand, sampling questions with diversity may mitigate the effect of misleading by similarity (Section \\ref{subsec:Retrieval-Q-CoT Fails}). \nOn the other hand, if we took each demonstration as a kind of skill, diverse demonstrations seem to cover more alternative skills for solving target questions: even though there still exists a small portion (e.g., $1/8$) of mistakes in the demonstrations, the performance will not be negatively affected (to be shown in Figure \\ref{fig_wr}).\n\nNevertheless, the clustering-based sampling method may still construct a small portion of wrong demonstrations, such as from questions in the frequent-error cluster.\nAs we will show later, some of these wrong demonstrations may be eliminated with heuristics. \nFor example, wrong demonstrations often come with long questions and long rationales. \nUsing simple and generic heuristics, such as only considering shorter questions with shorter rationales, further helps mitigate the effect of imperfect Zero-Shot-CoT capabilities (Appendix \\ref{appendix:rule}). \n\n\\section{Auto-CoT: Automatic Chain-of-Thought Prompting}\n\\label{sec:proposal}\n\nBased on the observations and considerations in Section \\ref{sec:prelim}, we propose an \\textbf{Auto-CoT} method to construct demonstrations with questions and reasoning chains automatically. Auto-CoT consists of two main stages: (i) question clustering: partition questions of a given dataset into a few clusters; (ii) demonstration sampling: select a representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. The overall procedure is illustrated in Figure~\\ref{fig_overview}. \n\n\\begin{figure}[htb]\n  \\begin{center}\n   \\includegraphics[width=1.0\\columnwidth]{fig_model.pdf}\n  \\end{center}\n  \\caption{Overview of the Auto-CoT method. Different from Manual-CoT in Figure \\ref{fig_examples}, demonstrations (on the right) are automatically constructed one by one (total: $k$) using an LLM with the ``Let's think step by step'' prompt.}\n  \\label{fig_overview}\n\\end{figure}\n\n\\subsection{Question Clustering} \n\nSince diversity-based clustering may mitigate misleading by similarity\n(Section \\ref{subsec:diversity may mitigate}), we perform cluster analysis for a given set of questions $\\mathcal{Q}$. \nWe first compute a vector representation for each question in $\\mathcal{Q}$ by Sentence-BERT \\citep{reimers-2019-sentence-bert}. \nThe contextualized vectors are averaged to form a fix-sized question representation. Then, the question representations are processed by the $k$-means clustering algorithm\nto produce $k$ clusters of questions. For questions in each cluster $i$, sort them into a list $\\mathbf{q}^{(i)} = [q_1^{(i)}, q_2^{(i)}, \\ldots]$ in the ascending order of the distance to the center of cluster $i$.\nThis question clustering stage is summarized in Algorithm \\ref{alg:cluster}.\n\n\\subsection{Demonstration Sampling} \nIn the second stage, we need to generate reasoning chains for those sampled questions and then sample demonstrations that satisfy our selection criteria.\n\nMore concretely, we construct a demonstration $d^{(i)}$ (concatenation of a question, a rationale, and an answer) for each cluster $i$ ($i=1,\\ldots, k$).\nFor cluster $i$, we iterate over questions in the sorted list $\\mathbf{q}^{(i)} = [q_1^{(i)}, q_2^{(i)}, \\ldots]$ (obtained by Algorithm \\ref{alg:cluster}) until satisfying our selection criteria.\nIn other words, a question that is closer to the center of cluster $i$ is considered earlier. Say that the $j$-th closest question $q_j^{(i)}$ is being considered.\nA prompted input is formulated as: [Q: \\texttt{$q_j^{(i)}$}. A: \\texttt{[P]}], where \\texttt{[P]} is a single prompt ``Let's think step to step''.\nThis formed input is fed into an LLM using Zero-Shot-CoT \\citep{kojima2022large} to output the reasoning chain consisting of the rationale $r_j^{(i)}$ and the extracted answer $a_j^{(i)}$. \nThen, a candidate demonstration $d_j^{(i)}$ for the $i$-th cluster is constructed by concatenating the question, rationale, and answer: $[\\text{Q: } q_j^{(i)}, \\text{A: } r_j^{(i)} \\circ a_j^{(i)}]$.\n\nSimilar to the criteria of the hand-crafting demonstrations in \\citet{cot_wei}, our selection criteria follow simple heuristics to encourage sampling simpler questions and rationales: set the selected demonstration $d^{(i)}$ as $d_j^{(i)}$ if it has a question $q_j^{(i)}$ with no more than $60$ tokens and a rationale $r_j^{(i)}$ with no more than $5$ reasoning steps.\\footnote{Because Zero-Shot-CoT often uses ``$\\backslash$n'' for separating the reasoning steps, the rule can be easily implemented by counting the ``$\\backslash$n'' tokens in the generated rationales.} \n\n\\begin{minipage}{0.46\\textwidth}\n  \\begin{algorithm}[H]\\small\n    \\caption{Cluster}\\label{alg:cluster}\n\\begin{algorithmic}[1]\n\\Require A set of questions $\\mathcal{Q}$ and the number of demonstrations $k$\n\\Ensure Sorted questions $\\mathbf{q}^{(i)} = [q_1^{(i)}, q_2^{(i)}, \\ldots]$ for each cluster $i$ ($i = 1, \\ldots, k$)\n\\Procedure{Cluster}{$\\mathcal{Q}$, $k$}\n\\For{each question $q$ in $\\mathcal{Q}$}\n\\State Encode $q$ by Sentence-BERT\n\\EndFor\n\\State Cluster all the encoded question representations into $k$ clusters\n\\For{each cluster $i = 1, \\ldots, k$}\n\\State Sort questions $\\mathbf{q}^{(i)} = [q_1^{(i)}, q_2^{(i)}, \\ldots]$ in the ascending order of the distance to the cluster center\n\\EndFor\n\\State \\textbf{return} $\\mathbf{q}^{(i)}$ ($i = 1, \\ldots, k$)\n\\EndProcedure%\n\\end{algorithmic}\n  \\end{algorithm}\n\\end{minipage}\n  \\hfill\n\\begin{minipage}{0.52\\textwidth}\n  \\begin{algorithm}[H]\\small\n  \\caption{Construct}\\label{alg:construct}\n\\begin{algorithmic}[1]\n\\Require Sorted questions $\\mathbf{q}^{(i)} = [q_1^{(i)}, q_2^{(i)}, \\ldots]$ for each cluster $i$ ($i = 1, \\ldots, k$), empty demonstration list $\\mathbf{d}$\n\\Ensure Demonstration list $\\mathbf{d} = [d^{(1)}, \\ldots, d^{(k)}]$ \n\\Procedure{Construct}{$\\mathbf{q}^{(i)}, \\ldots, \\mathbf{q}^{(k)}$}\n\\For{each cluster $i = 1, \\ldots, k$}\n\\For{each question $q_j^{(i)}$ in $\\mathbf{q}^{(i)}$}\n\\State Generate rationale $r_j^{(i)}$ and answer $a_j^{(i)}$ for $q_j^{(i)}$ using Zero-Shot-CoT\n\\If{$q_j^{(i)}, r_j^{(i)}$ satisfy selection criteria}\n    \\State Add $d^{(i)} = [\\text{Q: } q_j^{(i)}, \\text{A: } r_j^{(i)} \\circ a_j^{(i)}]$ to  $\\mathbf{d}$\n    \\State \\textbf{break}\n\\EndIf\n\\EndFor\n\\EndFor\n\\State \\textbf{return} $\\mathbf{d}$ \n\\EndProcedure%\n\\end{algorithmic}\n  \\end{algorithm}\n\\end{minipage}\n\nAs summarized in Algorithm \\ref{alg:construct}, after demonstration sampling for all the $k$ clusters, there will be $k$ constructed demonstrations $[d^{(1)}, \\ldots, d^{(k)}]$.\nThe constructed demonstrations are used to augment a test question $q^{\\text{test}}$ for in-context learning. Specifically, the input is the concatenation of all the demonstrations $[d^{(1)}, \\ldots, d^{(k)}]$ followed by [Q: $q^{\\text{test}}$. A: \\texttt{[P]}].\nThis input is fed to LLMs to obtain the reasoning chain with the answer in the end for $q^{\\text{test}}$ (right of Figure \\ref{fig_overview}).\n\n\\section{Experiments}\n\nWe briefly describe the experimental setup and present main experimental results.\nMore experimental details and results can be found in the appendices.\n\n\\subsection{Experimental setup}\n\\paragraph{Tasks and Datasets.} Our method is evaluated on ten benchmark datasets from three categories of reasoning tasks: (i) arithmetic reasoning (MultiArith \\citep{multiarith}, GSM8K \\citep{gsm8k}, AddSub \\citep{addsub}, AQUA-RAT \\citep{aqua}, SingleEq \\citep{koncel2015parsing}, SVAMP \\citep{svamp}); (ii) commonsense reasoning (CSQA \\citep{commonsenseqa}, StrategyQA \\citep{strategyqa}); (iii) symbolic reasoning (Last Letter Concatenation, Coin Flip) \\citep{cot_wei}.\n\n\\paragraph{Implementation.} We use the public  GPT-3 \\citep{brown2020language} of the text-davinci-002 version\nwith 175B parameters for the LLM  \\citep{instructgpt} unless otherwise stated. We select this LLM because it has the strongest CoT reasoning performance among public LLMs, as reported in \\citet{kojima2022large} and \\citet{cot_wei}.\nWe also evaluate the Codex model \\citep{chen2021evaluating} (code-davinci-002) as the LLM.\nFollowing \\citet{cot_wei}, the number of demonstrations $k$ is 8  except for AQuA and Letter (4), CSQA (7), and StrategyQA (6).\n\n\\paragraph{Baselines.} We compare our methods with four baseline methods: Zero-Shot \\citep{kojima2022large}, Zero-Shot-CoT \\citep{kojima2022large}, Few-Shot \\citep{cot_wei}, and Manual-CoT \\citep{cot_wei}. Zero-Shot-CoT and Manual-CoT are illustrated in Figure~\\ref{fig_examples}. The Zero-Shot baseline concatenates a test question with the prompt ``The answer is'' as the LLM input. The Few-Shot baseline has the same LLM input as Manual-CoT except for removed rationales from all the demonstrations.\n\n\\begin{table}[t]\\centering\n\n\\setlength{\\tabcolsep}{3.6pt}\n\\caption{Accuracy on ten datasets from three categories of reasoning tasks. \n}\n\\vspace{2.8mm}\n\\begin{tabular}{lcccccccccc}\\toprule\nModel &\\multicolumn{6}{c}{\\textit{Arithmetic}} &\\multicolumn{2}{c}{\\textit{Commonsense}} &\\multicolumn{2}{c}{\\textit{Symbolic}}\\\\\n\\cmidrule(r){2-7}\n\\cmidrule(r){8-9}%\n\\cmidrule(r){10-11}%\n&MultiArith   &GSM8K &AddSub &AQuA &SingleEq &SVAMP &CSQA &Strategy &Letter &Coin  \\\\\\midrule\nZero-Shot  &  22.7 & 12.5  & {77.0}& 22.4 & {78.7}& 58.8 & {72.6} & {54.3} & 0.2 & 53.8\\\\\nZero-Shot-CoT  & {78.7}  & {40.7}& 74.7& {33.5} & {78.7} & {63.7} & 64.6 & 54.8 & 57.6 & 91.4\\\\\n\\midrule\nFew-Shot & 33.8  & 15.6 & 83.3 & 24.8 & 82.7 & 65.7 & \\textbf{79.5} & \\textbf{65.9} & 0.2 & 57.2\\\\\nManual-CoT   & 91.7 & 46.9& 81.3 & 35.8 & {86.6} & {68.9} & 73.5 & 65.4 & 59.0 & 97.2 \\\\\n\\midrule\nAuto-CoT & \\textbf{92.0}   & \\textbf{47.9}   & \\textbf{84.8}   & \\textbf{36.5}  & \\textbf{87.0}  & \\textbf{69.5}  &  74.4  &  {65.4} & \\textbf{59.7} & \\textbf{99.9}  \\\\\n\n\\bottomrule\n\\end{tabular}\n\\label{tab:main_results}\n\\end{table}\n\n\\begin{wraptable}{r}{0.48\\textwidth}\n    \\centering\n    \\caption{Accuracy using the Codex LLM.}\\label{exp-codex} \n    \\setlength{\\tabcolsep}{3pt}\n            \\begin{tabular}{lccc}\n    \\toprule\n     {Method}& {MultiArith} & {GSM8K}& {AddSub} \\\\\n    \\midrule\n    Zero-Shot-CoT & 64.8 & 31.8 & 65.6  \\\\\n    Manual-CoT &\\textbf{96.8} & 59.4 & 84.6 \\\\\n     \\midrule\n    Auto-CoT  & {93.2} & \\textbf{62.8} & \\textbf{91.9} \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{wraptable}\n\n\\subsection{Competitive Performance of Auto-CoT on Ten Datasets}\n\nTable \\ref{tab:main_results} compares accuracy on ten datasets from three categories of reasoning tasks.\nThe Zero-Shot and Zero-Shot-CoT results are taken from \\citet{kojima2022large}, the Few-Shot and Manual-CoT results are taken from \\citet{cot_wei}, and the Auto-CoT results are averaged over three random runs.\nOverall, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. \nDue to the cost of manual designs, Manual-CoT may design the same demonstrations for multiple datasets (e.g., $5/6$ of the arithmetic datasets). In contrast, Auto-CoT is more flexible and task-adaptive: every single dataset gets its own demonstrations that are automatically constructed.\n\n\\subsection{Visualization of Question Clustering}\\label{appendix:vis}\n\nFigure~\\ref{fig:vis} visualizes question clustering (with PCA projection) in ten datasets. The illustration indicates that there exist generic patterns, where different patterns may be characterized by questions from different clusters. We present the constructed demonstrations of Auto-CoT in Appendix \\ref{sec:appendix-full-prompts}. \n\n\\begin{figure}[htb]\n  \\begin{center}\n  \\includegraphics[width=1.0\\columnwidth]{fig_vis.pdf}\n  \\end{center}\n  \\caption{Question clustering on ten datasets of reasoning tasks. Stars denote cluster centers.}\n  \\label{fig:vis}\n\\end{figure}\n\n\\subsection{General Effectiveness Using the Codex LLM}\n\nTo evaluate the general effectiveness of Auto-CoT using different LLMs, here we change the LLM to the Codex model \\citep{chen2021evaluating}.\nAs in Table \\ref{exp-codex}, the Codex LLM leads to performance improvement for Manual-CoT when compared with \nTable \\ref{tab:main_results} that uses the GPT-3 (text-davinci-002) LLM. \nNonetheless, using the Codex LLM, the overall performance of Auto-CoT is still competitive compared to Manual-CoT, providing additional empirical evidence for the effectiveness of Auto-CoT.\n\n\\subsection{Effect of Wrong Demonstrations}\\label{analysis:incorrect}\n\nRecall our discussions in Section \\ref{subsec:diversity may mitigate} that there can be wrong demonstrations (whose answers are wrong).\nTo see if diversity mitigates this effect, we design an In-Cluster Sampling baseline\nthat constructs demonstrations by randomly sampling questions from the same cluster that contains a test question.\nFigure~\\ref{fig_wr} compares accuracy with varying amounts of wrong demonstrations on MultiArith. \nCompared with In-Cluster Sampling, Auto-CoT (using diversity-based clustering) is less affected by wrong demonstrations: its performance still does not degrade significantly even when presented with 50\\% wrong demonstrations. \n \n\\makeatletter\\def\\@captype{figure}\\makeatother\n\\begin{minipage}{.42\\textwidth}\\centering\n{\n\\pgfplotsset{compat=1.13,\n    /pgfplots/ybar legend/.style={\n    /pgfplots/legend image code/.code={%\n       \\draw[##1,/tikz/.cd,yshift=-0.25em]\n        (0cm,0cm) rectangle (7pt,0.8em);},\n   },\n}\n\\pgfplotsset{width=6.92cm, height=4.5cm}\n    \\centering\n    \\vspace{2.8mm}\n    \\begin{tikzpicture}  \n        \\begin{axis}  \n        [  \n            ybar,\n            ymin=75, ymax=100,\n            ytick={80,85,90,95,100},\n            major x tick style = transparent,\n            bar width=6.8pt,\n            enlarge x limits=0.2,\n            ylabel={Accuracy (\\%)},\n            symbolic x coords={0,1,2,3},  \n            xtick=data,  \n            xticklabels={12.5\\%, 25.0\\%, 37.5\\%, 50.0\\%},\n            xlabel={Percentage of wrong demonstrations},\n        legend cell align=left,\n         legend columns=2 row=1,\n                legend style={\n                        at={(0.5,1.05)},\n                        anchor=south,\n                        column sep=1ex,\n                        font=\\small,\n                }\n            ]  \n        \\addplot[ybar, fill=bananayellow,  postaction={pattern=north east lines}] coordinates {\n            (0,91.2)(1,88.0)(2,83.5)(3,80.5)\n        };  \n        \\addplot[ybar, fill=babyblue,  postaction={pattern=north west lines}] coordinates {\n            (0, 93.7)(1, 93.3)(2, 89.8)(3, 90.2)\n        };\n        \\legend{In-Cluster Sampling,Auto-CoT} \n        \\end{axis}  \n    \\end{tikzpicture}\n    \\caption{\n   Effect of wrong demonstrations.\\label{fig_wr}\n    }\n}\n\\end{minipage}\n\\quad\n\\makeatletter\\def\\@captype{figure}\\makeatother\n\\begin{minipage}{.55\\textwidth}\n\t\\centering\n{\n\\pgfplotsset{compat=1.13,\n    /pgfplots/ybar legend/.style={\n    /pgfplots/legend image code/.code={%\n       \\draw[##1,/tikz/.cd,yshift=-0.25em]\n        (0cm,0cm) rectangle (7pt,0.8em);},\n   },\n}\n\\pgfplotsset{width=7.72cm, height=4.5cm}\n    \\centering\n    \\vspace{2.8mm}\n    \\begin{tikzpicture}  \n        \\begin{axis}  \n        [  \n            ybar,\n            ymin=60, ymax=100,\n            ytick={60,70,80,90,100},\n            major x tick style = transparent,\n            bar width=2.8pt,\n            legend columns=3 row=1,\n            ylabel={Accuracy (\\%)},xlabel={Batch},\n            symbolic x coords={1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20},  \n            xtick=data,  \n        legend cell align=left,\nlegend style={\n                        at={(1,1.05)},\n                        anchor=south east,\n                        column sep=1ex,\n                        font=\\small,\n                },\n            ]  \n        \\addplot[ybar, fill=cinnamon,  postaction={}] coordinates {\n            (1, 80.0) (2, 73.33)(3, 83.33)(4, 73.33)(5, 70.0)(6, 76.66)(7, 83.33)(8, 90.0)(9, 66.66)(10, 83.33)\n        };\n        \\addplot[ybar, fill=blanchedalmond,  postaction={pattern=north east lines}] coordinates {\n            (1, 96.66) (2, 96.66)(3, 96.66)(4, 96.66)(5, 93.33)(6, 83.33)(7, 86.66)(8, 93.33)(9, 90.0)(10, 96.66)\n        };  \n        \\addplot[ybar, fill=babyblue,  postaction={pattern=north west lines}] coordinates {\n          (1, 80.0) (2, 93.33)(3, 96.66)(4, 93.33)(5, 96.66)(6, 93.33)(7, 96.66)(8, 90.0)(9, 80.0)(10, 96.66)\n        };  \n        \\legend{\\scriptsize{Zero-Shot-CoT}, \\scriptsize{Manual-CoT}, \\scriptsize{Auto-CoT*}}\n        \\end{axis}  \n    \\end{tikzpicture}  \n    \\caption{Bootstraping for the streaming setting.\\label{fig:boostrap}}\n}\n\\end{minipage}\n\n\\subsection{More Challenging Streaming Setting}\\label{sec:bootstraping}\n\nCoT studies commonly assume that a full dataset with test questions is given \\citep{cot_wei,kojima2022large}. \nBased on the given dataset, Auto-CoT samples questions to construct the demonstrations.\nNonetheless, now we consider a more challenging \\emph{streaming setting} where a small batch of test questions (say $m$ questions) arrive at a time like in data streams.\n\nTo address this challenge, we extend Auto-CoT to a bootstrapping  version Auto-CoT*:\n(i) Initialize an empty set $\\mathcal{M}_0$;\n(ii) When batch $1$ of questions $q_1^{(1)}, \\ldots, q_m^{(1)}$ arrive, invoke Zero-Shot-CoT (no clustering due to small $m$) for each $q_i^{(1)}$ to obtain its reasoning chain $c_i^{(1)}$.\nAdd question-chain pairs $(q_1^{(1)}, c_1^{(1)}), \\ldots, (q_m^{(1)}, c_m^{(1)})$ to $\\mathcal{M}_0$ and call the new set $\\mathcal{M}_1$;\n(iii) When batch $b$ ($b>1$) of questions $q_1^{(b)}, \\ldots, q_m^{(b)}$ arrive, construct demonstrations with existing questions and reasoning chains in $\\mathcal{M}_{b-1}$ (like Auto-CoT) and use the demonstrations for in-context reasoning for each $q_i^{(b)}$.\nAdd question-chain pairs $(q_1^{(b)}, c_1^{(b)}), \\ldots, (q_m^{(b)}, c_m^{(b)})$ to $\\mathcal{M}_{b-1}$ and call the new set $\\mathcal{M}_b$.\n\nFigure~\\ref{fig:boostrap} \ncompares the accuracy on MultiArith at each batch ($m=30$) in this streaming setting \n(extended version: Figure~\\ref{appendix-fig:boostrap} in the Appendix). \nAs expected, for batch $1$, Auto-CoT* and Zero-Shot-CoT obtain equal accuracy. \nFrom batch $2$, Auto-CoT* performs comparably with Manual-CoT.\nThis result indicates that our method is still effective in the more challenging streaming setting.\n\n\\section{Conclusion}\n\nLLMs have shown reasoning capabilities with CoT prompting. The superior performance of Manual-CoT hinges on the hand-crafting of demonstrations. To eliminate such manual designs, we proposed Auto-CoT to automatically construct demonstrations. It samples questions with diversity and generates reasoning chains to construct demonstrations. Experimental results on ten public benchmark reasoning datasets showed that with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. \n\n\\newpage\n\n\\newpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{ReFT: Reasoning with Reinforced Fine-Tuning}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nOne way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. \nThis approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. \nIn math problem-solving, for example, \nthere is usually only one annotated reasoning path for each question in the training data. \nIntuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question.\nTo address this issue,  we propose a simple yet effective approach called \\textit{Reinforced Fine-Tuning} (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example.  \nReFT first warmups the model with SFT, \nand then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, \nwhere an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. \nExtensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT,\nand the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. \nNote that ReFT obtains the improvement by learning from the same training questions as SFT, \nwithout relying on extra or augmented training questions.\nThis indicates a superior generalization ability for ReFT\n\\footnote{Code: \\url{https://github.com/lqtrung1998/mwp_ReFT}}.\n\\end{abstract}\n\n\\section{Introduction}\nThe state-of-the-art approaches to solving math problems~\\cite{luo2023wizardmath,wang2023mathcoder} employ Supervised Fine-Tuning (SFT) to train the models using Chain-of-Thought (CoT) annotations~\\cite{wei2022chain}. As shown in Figure \\ref{fig:sft_vs_rft}, a CoT annotation outlines the intermediate reasoning steps toward solving a math problem. \n\n\\begin{figure}\n    \\centering\n    \\adjustbox{max width=1.0\\linewidth}{\n        \\includegraphics{sft_vs_rft_with_question_v3.pdf}\n    }\n    \\vspace*{-5mm}\n    \\caption{An example of question ($x$), CoT ($e$), and answer ($y$) in GSM8K~\\cite{cobbe2021training}. \n    The SFT process iterates several epochs on the training data.\n    The proposed ReFT warm-up from SFT and performs RL training on the same data.\n    }\n    \\label{fig:sft_vs_rft}\n\\end{figure}\n\nUsually there is one CoT annotation for each question in the training data, i.e., one correct reasoning path, which is utilized in SFT. We observe that this may result in relatively weak generalization abilities of the SFT models. It is often the case that multiple valid CoT annotations exist for the same question~\\cite{cobbe2021training,zhang2023interpretable}, underscoring the need for a more powerful fine-tuning approach. To address this problem, we propose a simple yet effective approach called \\textit{Reinforced Fine-Tuning} (ReFT) (Figure \\ref{fig:sft_vs_rft} bottom).\n\nReFT commences with a warm-up stage involving Supervised Fine-Tuning (SFT) in one or two epochs (Figure \\ref{fig:sft_vs_rft}, shaded box). This initial stage equips the model with the ability to generate correct responses to mathematical problems to some extent, as demonstrated in prior work~\\cite{cobbe2021training}. \nNext, ReFT proceeds to further refine the model through the utilization of an online Reinforcement Learning (RL) algorithm~\\cite{sutton2018reinforcement}, specifically Proximal Policy Optimization (PPO)~\\cite{schulman2017proximal} in this paper. \nIn this way,\nReFT is able to sample multiple correct reasoning paths or CoT annotations and learn from them (Figure \\ref{fig:sft_vs_ref_path}, right). \n\nSince the training data include ground-truth answers, the golden rewards can be naturally derived from them when training PPO.\nConsequently, there is no requirement for a separately trained reward model.\nIn contrast, RLHF~\\cite{ouyang2022training} has to utilize a reward model that is learned from human-labeled data.\n\n\\begin{figure}\n    \\centering\n    \\adjustbox{max width=1.0\\linewidth}{\n        \\includegraphics{sft_vs_reft_path_v3.pdf}\n    }\n    \\vspace*{-5mm}\n    \\caption{Comparison between SFT and ReFT on the presence of CoT alternatives.}\n    \\label{fig:sft_vs_ref_path}\n\\end{figure}\n\nDuring the warm-up stage, ReFT acquires a certain level of accuracy by supervised learning. In the RL stage, ReFT further enhances its ability by reinforcement learning through sampling various CoT reasoning paths.\nIn this way, ReFT gets much richer supervision signals than SFT.\nThis approach enables ReFT to greatly improve generalization in math problem-solving~\\cite{gao2018reinforcement,brown2020better}. %as illustrated in Figure \\ref{fig:sft_vs_ref_path}. \nNote that ReFT outperforms SFT by using the same training questions,\nwithout relying on extra or augmented training questions.\nIn fact, ReFT does not conflict with such data engineering and can be seamlessly combined with it.\n\nOur contributions are as follows:\n\\squishlist\n\n\\item We introduce a novel fine-tuning approach, reinforced fine-tuning (ReFT), which utilizes reinforcement learning to solve math problems. ReFT exhibits enhanced generalization capabilities compared to conventional supervised fine-tuning when trained on the same dataset.\n\n\\item We conduct extensive experiments using two foundational models, CodeLLAMA~\\cite{roziere2023code} and Galactica~\\cite{taylor2022galactica}, on three standard datasets: GSM8K~\\cite{cobbe2021training}, MathQA~\\cite{amini2019mathqa}, and SVAMP~\\cite{patel2021nlp}. Our experiments cover both natural language and program-based CoTs, demonstrating the significantly improved performance and generalization ability of ReFT.\n\n\\item Additionally, we demonstrate that ReFT benefits from both majority voting~\\cite{wang2022self} and reward model reranking~\\cite{uesato2022solving} at inference-time, further improving its performance.\n\\squishend\n\n\\section{Related Work}\n\n\\paragraph{Math Problem Solving}\nRecent research efforts focus on CoT prompt design and data engineering. \nMost of them attempted to make CoT comprehensive and fine-grained to present the step-by-step reasoning solutions~\\cite{nye2021show,fu2022complexity,zhou2022least,khot2022decomposed,zelikman2022star,imani2023mathprompter,miao2023selfcheck}.\n\\citet{gao2023pal} further proposed to use the Python program as CoT prompt, demonstrating more accurate reasoning steps and significant improvements over the natural language CoT~\\cite{wei2022chain}.\n\\citet{zhou2023solving} introduced a prompting method that generates code to verify the intermediate reasoning step with GPT-4~\\cite{openai2023gpt4}, thus achieving state-of-the-art performance on GSM8K~\\cite{cobbe2021training} and MATH~\\cite{hendrycks2021measuring}.  \nAnother line of work focuses on improving the quality of CoT~\\cite{wang2023mathcoder,liu2023tinygsm,yu2023metamath} and increasing the amount of CoT data~\\cite{luo2023wizardmath,yue2023mammoth} from OpenAI's ChatGPT (\\texttt{gpt-3.5-turbo}) or GPT-4\\footnote{\\url{https://chat.openai.com/}}. \n\n\\paragraph{Reinforcement Learning}\nOur work is mostly related to the recent work that applies PPO~\\cite{schulman2017proximal} to natural language process for aligning human preferences~~\\cite{ouyang2022training}. \nSince then, several training algorithms have been proposed to efficiently improve the alignment,\nincluding direct preference optimization (DPO)~\\cite{rafailov2023direct}, identity preference optimization (IPO)~\\cite{azar2023general}, and Kahneman-Tversky optimization (KTO)~\\cite{ethayarajh2023halos}. \nOther than the purpose of alignment, we aim to adopt reinforcement learning as a fine-tuning paradigm to improve performance over conventional supervised fine-tuning.\n\nSpecifically for solving math problems, \\citet{uesato2022solving} and \\citet{lightman2023lets}  trained an outcome-based or process-based reward model to perform reranking~\\cite{cobbe2021training} to achieve much better performance over SFT and majority voting~\\cite{wang2022self}. \nWhile our approach aims to improve the performance of the policy itself, these reward model reranking approaches can be easily integrated into the resulting policy model.\n\n\\begin{algorithm*}[t!]\n\\newcommand{\\mycommfont}[1]{\\normalfont\\textcolor{black}{#1}}\n\\SetCommentSty{mycommfont}\n\\DontPrintSemicolon\n\\KwIn{$\\mathcal{D}_{train} = \\{(\\boldsymbol{x}, \\boldsymbol{e}, \\boldsymbol{y})\\}$: Tuples of (\\textit{question}, \\textit{CoT}, \\textit{answer}), $W$: number of warm-up steps, $T$: number of RL steps, $U$: number of updates per RL step, $\\boldsymbol{\\pi}_\\theta^{(0)}$: Initial policy.}\n\n\\KwOut{$\\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}$: Final policy}\n\n$\\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}=\\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}^{(0)}$ \\;\n//~\\textcolor{mintleaf}{\\em Warm-up stage}\\;\n\\For{$i\\gets 1$ \\KwTo $W$}{\n    $\\boldsymbol{x}, \\boldsymbol{e}, \\boldsymbol{y} \\sim  \\mathcal{D}_{train}$ \\tcp*[r]{\\textcolor{mintleaf}{Sample mini-batch from $\\mathcal{D}_{train}$}}\n    $\\boldsymbol{\\theta} = \\textsc{Optimization\\_Step}(\\mathcal{L}_{SFT}(\\boldsymbol{\\theta}))$ \\tcp*[r]{\\textcolor{mintleaf}{\\text{Equation 1}}}\n}\n//~\\textcolor{mintleaf}{\\em Reinforcement learning stage}\\;\n\\For{$i\\gets 1$ \\KwTo \\em $T$}{\n\t $\\boldsymbol{x},\\_ , \\boldsymbol{y} \\sim  \\mathcal{D}_{train}$ \\tcp*[r]{\\textcolor{mintleaf}{Sample mini-batch without CoT}}\n\t $\\hat{\\boldsymbol{e}} \\sim \\boldsymbol{\\pi}_{\\boldsymbol{\\theta}} (\\boldsymbol{x})$ \\tcp*[r]{\\textcolor{mintleaf}{On-policy CoT sampling}}\n    $ \\hat{\\boldsymbol{y}} \\gets \\textsc{Extract}(\\hat{\\boldsymbol{e}})$\\tcp*[r]{\\textcolor{mintleaf}{Extract the answer from CoT}}\n    $\\boldsymbol{\\pi_{\\theta_{\\text{old}}}} \\gets \\boldsymbol{\\pi_{\\theta}}, V_{\\boldsymbol{\\phi_{\\text{old}}}} \\gets V_{\\boldsymbol{\\phi}}$ \\;\n    \\text{Compute $\\delta_t, \\hat{A_t}, \\hat{R_t}$ using $\\pi_{\\boldsymbol{\\theta_{\\text{old}}}}, V_{\\boldsymbol{\\phi_{\\text{old}}}}, \\boldsymbol{x}, \\hat{\\boldsymbol{e}}, \\hat{\\boldsymbol{y}}$ and $\\boldsymbol{y}$}\\;% \\tcp*[r]{\\scriptsize \\textcolor{mintleaf}{\\S3.1 \\textbf{Reinforcement Learning}}}\n    \\For{$j\\gets 1$ \\KwTo \\em $U$}{\n        $\\boldsymbol{\\theta}, \\boldsymbol{\\phi} = \\textsc{Optimization\\_Step}(\\mathcal{L}_{RL}(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}))$ \\tcp*[r]{\\textcolor{mintleaf}{Equation 2}}\n    }\n}\n\\Return{$\\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}$}\n\n\\caption{Reinforced Fine-Tuning}\n\\label{algo:reft}\n\\end{algorithm*}\n\n\\section{Method}\n\nIn this work, we focus on \\textit{natural language CoT} (\\textbf{N-CoT})~\\cite{wei2022chain} (Figure \\ref{fig:sft_vs_rft}) and \\textit{program-based CoT}~\\cite{gao2023pal} (\\textbf{P-CoT}) using Python. \n\\citet{gao2023pal} proposed the program-based CoT for math problem solving. \nWe can simply execute the program to obtain the answer. \nTo ensure clarity and avoid ambiguity, we use the terms N-CoT and P-CoT to represent natural language and program-based CoTs, respectively.\n\n\\subsection{Reinforced Fine-Tuning}\n\\label{sec:reft}\nThe proposed Reinforced Fine-Tuning (ReFT) process consists of two stages: the warm-up stage and the reinforcement learning stage. \nThe overall algorithm is shown in Algorithm \\ref{algo:reft}.\n\n    \n    \n\n\\paragraph{Warm-up}\nIn this stage, the policy is fine-tuned for a few epochs on a dataset comprising of the ``(\\textit{question}, \\textit{CoT})'' tuples: $(\\boldsymbol{x}, \\boldsymbol{e})$.\nIt enables the model to have basic problem-solving skills to generate a proper response\\footnote{The underlying concept is similar to the verifier training~\\cite{cobbe2021training} to generate multiple solutions.}. \nFormally, the CoT generation process can be decomposed into a sequence of next token prediction actions. \nThe last action token, \\texttt{<eos>}, signals the generation process to terminate. \nThe CoT $\\boldsymbol{e}$ is written as:\n$$\n    \\boldsymbol{e} = [a_1, a_2, ..., a_{L-1}, a_L\\texttt{=<eos>}]\n$$\nwhere $L$ represents the maximum length. \nAt timestep $t$, the action $a_t$ is sampled from a policy $\\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}(\\cdot|s_t)$ where $a_t$ can be any token in the vocabulary and the state $s_t$ comprises of all tokens in the question and all tokens generated so far. \nAfter each action, the resulting state $s_{t+1}$ is the concatenation of the current state $s_t$ and the action $a_t$:\n$$\n    s_{t+1} = \n        \\begin{cases}\n            \\boldsymbol{x}, & t = 0 \\\\\n            [s_t, a_t],     & 1 \\leq t \\leq L \\\\\n        \\end{cases}.\n$$\nAs the produced action is the \\texttt{<eos>} token, \nthe resulting state $s_{L+1}$ is the terminal state and the generation process is finished.\nWith this notation, the loss function for a sample can be written as:\n\\begin{equation}\n    \\mathcal{L}_{SFT}(\\boldsymbol{\\theta})=-\\mathbb{E}_{\\boldsymbol{e} \\sim \\mathcal{D}}\\left[\\sum_{t=1}^{L}\\log\\left(\\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}(a_t\\vert s_t)\\right)\\right]\n    \\label{eq:sft-loss}\n\\end{equation}\n\n\\paragraph{Reinforcement Learning}\nIn this stage, the policy improves its performance via a form of online self-learning using a dataset comprising of (\\textit{question}, \\textit{answer}) tuples: $(\\boldsymbol{x}, \\boldsymbol{y})$. \nSpecifically, the policy model learns by repeatedly sampling responses (Figure \\ref{fig:sft_vs_ref_path}), evaluating the response's answer correctness, and updating its parameters in an online fashion (line 7-14 in Algorithm \\ref{algo:reft}).\nWe employ PPO~\\cite{schulman2017proximal} with a clipped objective algorithm for training. \nFollowing \\citet{ziegler2019fine}, the value model $V_{\\phi}$ is constructed by appending a linear value head on top of the last hidden states of the policy model $\\pi_{\\theta}$, which is the model after the warm-up stage.\nThe reward of 0 is given for all action resulting in non-terminal state. \nAt the terminal state, we use a reward function that directly compares the answer extracted from the state's CoT and the ground-truth answer $\\boldsymbol{y}$ . \nHere, the reward function returns 1 if the answer is deemed correct, otherwise 0 is returned. \nOn dataset whose answers are all numeric, \\textit{partial reward}~\\cite{zhong2017seq2sql,le2022coderl} of 0.1 can be applied when the answer can be extracted and it is of numeric type. \nFor $1 \\leq t \\leq L$, we write\n\\begin{equation}\n    r(s_t, a_t, s_{t+1})\\! =\\! \n        \\begin{cases} \n            1, &  \\!\\!\\!\\!\\! \\texttt{EXTRACT}(s_{t+1}) = \\boldsymbol{y} \\\\\n            0.1, &\\!\\!\\!\\!\\! \\texttt{EXTRACT}(s_{t+1}) \\neq \\texttt{null}, \\neq \\boldsymbol{y} \\\\\n            0, & \\!\\!\\!\\!\\! \\texttt{EXTRACT}(s_{t+1}) = \\texttt{null}\n    \\end{cases}\n    \\nonumber\n    \\label{equ:reward_function}\n\\end{equation}\nSuch a partial reward can help reduce the effect of learning from sparse reward~\\cite{riedmiller2018learning,trott2019keeping}.\nIn addition, following \\citet{zheng2023secrets}, our total reward is the sum of the reward function score and the Kullback-Leibler (KL) divergence~\\cite{kullback1951information} between the learned RL policy and initial policy scaled by a coefficient factor $\\beta$. \n\\begin{equation*}\n    \\begin{split}\n        r_{total}(s_{t}, & a_{t},s_{t+1})=r(s_{t},a_{t},s_{t+1}) \\\\ & - \\beta \\textit{KL}\\left(\\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}(\\cdot|s_t), \\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}^{(0)}(\\cdot|s_t)\\right)\n    \\end{split}\n\\end{equation*}\nThe generalized advantage estimate~\\cite{schulman2018highdimensional} is used for advantage calculation:\n\\begin{equation*}\\label{eq:Advantage}\n    \\hat{A_t} = \\sum_{l=0}^{L-t} (\\gamma\\lambda)^{l}\\delta_{t+l},\n\\end{equation*}\nwhere the Temporal Difference (TD) is defined as\n\\begin{equation*}\n    \\delta_{t'} = -V_{\\phi}(s_{t'}) + r_{total}(s_{t'},a_{t'},s_{t'+1}) + \\gamma V_{\\phi}(s_{t'+1})\n\\end{equation*}\nwith the terminal state value $V_{\\phi}(s_{L+1}) := 0$,\n$\\lambda \\in (0,1]$ is the discount factor for rewards,\nand $\\gamma \\in [0,1]$ is the discount factor for TD.\nFor the estimate of return, \nwe leverages the $\\lambda$-return $\\hat{R_t}$, \nwhich can be written as the sum of the generalized advantage estimate and the value estimate:\n\\begin{equation*}\n    \\hat{R_t} = \\hat{A_t} + V_{\\boldsymbol{\\phi}}(s_t)\n\\end{equation*}\nLastly, the policy and value objectives can be written as in two equations below\n\\begin{equation*}\n\\begin{split}\n\\mathcal{L}_{policy}(&\\boldsymbol{\\theta})  = -\\mathbb{E_{\\boldsymbol{e} \\sim \\boldsymbol{\\pi}_{\\theta_{\\text{old}}}}}\\Bigg[ \\min \\Bigg( \\frac{\\boldsymbol{\\pi}_{\\boldsymbol{\\theta}} (a_t|s_t)}{\\boldsymbol{\\pi}_{\\theta_{\\text{old}}} (a_t|s_t)} \\hat{A}_t, \\\\\n&\\text{clip}\\left( \\frac{\\boldsymbol{\\pi}_{\\boldsymbol{\\theta}} (a_t|s_t)}{\\boldsymbol{\\pi}_{\\theta_{\\text{old}}} (a_t|s_t)}, 1-\\epsilon, 1+\\epsilon \\right) \\hat{A}_t \\Bigg) \\Bigg]\n\\end{split}\n\\label{eq:loss_policy}\n\\end{equation*}\n\\begin{equation*}\n\\begin{split}\n\\mathcal{L}_{value}(&\\boldsymbol{\\phi})  = \\frac{1}{2}~ \\mathbb{E_{\\boldsymbol{e} \\sim \\boldsymbol{\\pi}_{\\theta_{\\text{old}}}}}\\Bigg[\n\\!\\! \\max \\Bigg(\\!\\! \\norm{V_{\\boldsymbol{\\phi}}(s_t) - \\hat{R_t}}^2, \\\\ \n&\\norm{\\text{clip}\\left(\\hat{R_t}-V_{\\phi}(s_t), \\hat{A_t} -\\epsilon, \\hat{A_t} + \\epsilon \\right)}^2 \\Bigg) \\Bigg]\n\\end{split}\n\\label{eq:loss_value}\n\\end{equation*}\nwhere $\\boldsymbol{\\pi}_{\\theta_{\\text{old}}}$, $V_{\\phi_{\\text{old}}}$ are used for sampling CoT and computing $\\hat{A_t}$, $\\hat{R_t}$.\nThe unified loss function is the weighted sum of the above objectives.\n\\begin{equation}\\label{eq:loss_rl}\n    \\mathcal{L}_{RL}(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}) = \\mathcal{L}_{policy} + \\alpha \\mathcal{L}_{value}\n\\end{equation}\nwhere $\\alpha$ is the coefficient for the value objective.\n\n\\section{Experiments}\n\n\\subsection{Datasets}\nWe conduct experiments on three math problem datasets: GSM8K~\\cite{cobbe2021training}, SVAMP~\\cite{patel2021nlp} and MathQA~\\cite{amini2019mathqa}.\nFor both GSM8K and SVAMP, the format of answers is a numeric value.\nIn MathQA, the format is instead a list of multiple choices (i.e., \\texttt{ABCD}).\nTable \\ref{tab:data_statistics} presents the statistics of all datasets.\nWe perform few-shot prompting~\\cite{wei2022chain,gao2023pal} using \\texttt{GPT-3.5-turbo} to obtain both the N-CoT and P-CoT annotations\\footnote{Examples of N-CoT and P-CoT representations can be found in Appendix \\ref{sec:appendix_representation}.}.\nThe N-CoT and P-CoT annotations are obtained following \\citet{jie2023design}.\nWe also conducted an additional experiment on a numeric version of MathQA~\\cite{jie2023leveraging} where the format is also a numeric value.\nSuch experiments are used to demonstrate our assumptions of potential reward hacking phenomenon~\\cite{skalse2022defining} on MathQA (\\S\\ref{sec:results}).\n\n\\begin{table}[t!]\n    \\centering\n    \\adjustbox{max width=1\\linewidth}{\n    \\begin{tabular}{ccccc}\n    \\toprule\n        \\textbf{} & \\textbf{GSM8k} & \\textbf{SVAMP} & \\textbf{MathQA$_\\text{MCQ}$} & \\textbf{MathQA$_\\text{numeric}$} \\\\ \\midrule\n    \\textbf{Train N-CoT}  & 7,465  & 3,076  & 14,862  & 8,955           \\\\ \n    \\textbf{Train P-CoT}  & 7,356  & 3,043  & 15,250  & 7,672           \\\\ \n    \\textbf{Test} & 1,319  & 1,000  & \\textcolor{white}{0}1,605   & 1,605          \\\\ \n    \\bottomrule\n    \\end{tabular}\n    }\n\\caption{Statistics of the train and test datasets.}\n\\label{tab:data_statistics}\n\\end{table}\n\n\\subsection{Baseline}\n\\label{sec:baseline}\nWe compare ReFT with SFT and self-training~\\cite{xie2020self,amini2022self} baselines.\nSFT simply fine-tunes the language model on the training data.\nExperiments with self-training methods ensure a relatively fair comparison because these methods share the mechanism that the samples generated from the model are used for training.\n\nWe implemented Offline Self-Training (\\textbf{Offline-ST})~\\cite{he2020revisiting}, and Online~\\cite{hoi2021online} Self-Training (\\textbf{Online-ST}).\nThe Offline-ST method is similar to expert iteration~\\cite{anthony2017thinking,uesato2022solving,zelikman2022star}.\nWe first use the SFT checkpoint from the early checkpoint to sample the CoTs and verify them against the ground truth. \nWe only retain those expert samples that have a correct answer.\nWe perform SFT on the combination of original training data and the expert samples.\n\nThe Online-ST method is made to be closely comparable to ReFT.\nFollowing ReFT, Online-ST has the same warm-up process.\nAfter that, we perform continual training with the samples generated on the fly.\nAt each training step, the model first samples CoTs for a batch and only retains those with correct answers. \nThe resulting batch consists of both sampled and ground-truth CoTs.\nWe then update the model parameters on this batch with the supervised fine-tuning objective $\\mathcal{L}_{SFT}$.\nCompared with ReFT, Online-ST neither makes use of negative responses (with an incorrect answer) nor has a dedicated mechanism to prevent the model from significantly diverging from the initial model, which can manifest as task-specific overfitting and training instability.\n\n\\begin{table*}[t!]\n    \\centering\n    \\adjustbox{max width=1.0\\linewidth}{\n    \\begin{tabular}{lccccccccc}\n        \\toprule\n        \\multirow{2}{*}{ \\bf Method } & \\multirow{2}{*}{ \\bf Size } & \\multicolumn{2}{c}{\\bf GSM8K} & \\multicolumn{2}{c}{\\bf SVAMP} & \\multicolumn{2}{c}{\\textbf{MathQA}$_\\text{MCQ}$} & \\multicolumn{2}{c}{\\bf Average}\\\\\n        &  & \\textbf{N-CoT} & \\textbf{P-CoT} & \\textbf{N-CoT} & \\textbf{P-CoT} & \\textbf{N-CoT} & \\textbf{P-CoT} & \\textbf{N-CoT} & \\textbf{P-CoT}  \\\\\n        \\midrule\nGalactica + SFT & 6.7B                                      & $42.68$ & $58.83$ & $54.50$ & $70.09$ & $58.07$ & $64.61$ \t& $51.75$ & $64.51$ \\\\\nGalactica + Offline Self-Training & 6.7B                    & $42.60$ & $60.72$ & $57.90$ & $72.30$ & $\\mathbf{60.75}$ & $67.04$ \t& $53.75$ & $66.69$ \\\\\nGalactica + Online Self-Training  &6.7B                     & $47.84$ & $62.93$ & $59.40$ & $\\mathbf{74.59}$ & $59.38$ & $61.24$ \t& $55.54$ & $66.25$ \\\\\nGalactica + ReFT &  6.7B                                    & $\\mathbf{48.14}$ & $\\mathbf{68.91}$ & $\\mathbf{61.40}$ & $74.09$ & $58.13$ & $\\mathbf{70.47}$ \t& $\\mathbf{55.89}$ & $\\mathbf{71.16}$ \\\\\n        \\midrule\n        \\midrule\nCodeLLAMA + SFT & \\textcolor{white}{0.}7B                   & $43.59$ & $63.68$ & $58.09$ & $75.40$ & $56.01$ & $64.79$ \t& $52.56$ & $67.96$ \\\\\nCodeLLAMA + Offline Self-Training & \\textcolor{white}{0.}7B & $45.10$ & $68.00$ & $60.20$ & $77.69$ & $59.81$ & $68.53$ \t& $55.04$ & $71.41$ \\\\\nCodeLLAMA + Online Self-Training &\\textcolor{white}{0.}7B   & $44.66$ & $67.85$ & $58.60$ & $77.40$ & $56.95$ & $68.85$ \t& $53.40$ & $71.37$ \\\\\nCodeLLAMA + ReFT & \\textcolor{white}{0.}7B                  & $\\mathbf{53.30}$ & $\\mathbf{75.28}$ & $\\mathbf{64.50}$ & $\\mathbf{79.19}$ & $\\mathbf{60.13}$ & $\\mathbf{71.83}$ \t& $\\mathbf{59.31}$ & $\\mathbf{75.43}$ \\\\\n        \\bottomrule\n    \\end{tabular}\n    }\n    \\caption{Value accuracy of ReFT and the baselines fine-tuned with two foundation models on all datasets.\n    }\n    \\label{tab:sft_ppo_result}\n\\end{table*}\n\n\\subsection{Experimental Setup}\n\\label{sec:setup}\n\nWe conduct experiments with two foundation models: Galactica-6.7B\\footnote{\\rurl{huggingface.co/facebook/galactica-6.7b}}~\\cite{taylor2022galactica} and CodeLLAMA-7B\\footnote{\\rurl{huggingface.co/codellama/CodeLlama-7b-hf}}\\footnote{Additional preliminary experiments were conducted using Gemma~\\cite{gemmateam2024gemma}. However, these results are not included in the current version of this paper due to unresolved implementation issues that align with known challenges reported within the open-source community (\\url{https://huggingface.co/google/gemma-7b/discussions}).}~\\cite{roziere2023code}.\nBoth models are reported to have strong performance in math solving and are commonly adopted in recent literature on reasoning tasks~\\cite{yue2023mammoth,luo2023wizardmath}.\n\nIn addition to the comparison with baselines, we also apply common techniques, majority voting~\\cite{wang2022self} and reward model reranking~\\cite{lightman2023lets} on GSM8K. \n\n\\paragraph{Hyper-parameters}\nIn all experiments, the training is done with 8 A100-80GB GPUs using DeepSpeed~\\cite{rajbhandari2020zero,rasley2020deepspeed} Zero stage 2 and HuggingFace Accelerate~\\cite{accelerate}.\nDuring the warm-up stage of ReFT, we use AdamW~\\cite{loshchilov2017decoupled} optimizer with 10\\% warm-up ratio. \nThe batch size is 48 and learning rate is $1e$-$5$.\nThe maximum length is set to $1024$. \nThe number of epochs in the warm-up stage is 2 in all settings except on MathQA$_\\text{MCQ}$ and MathQA$_\\text{numeric}$ where we use up to 5 and 10 respectively. \nThe model is trained for $300$ epochs with a learning rate of $3e$-$7$.\nFollowing \\citet{ziegler2019fine}, the $\\lambda$, $\\gamma$, $\\alpha$, $\\epsilon$ and $U$ in PPO are set to $1$, $0.95$, $5$, $0.2$, and $2$, respectively.\nThe KL coefficient $\\beta$ is set to $0.01$ for P-CoT and is set to $0.05$ for N-CoT experiments. \nFurther hyperprameter settings about ReFT can be found in Appendix \\ref{sec:appendix_hyperparameter}.\n\nFor SFT baseline, we train the model for 40 epochs and choose the checkpoint with best performance. \nThis number of epochs has been chosen to be sufficiently large to ensure SFT converges.\nFor Offline-ST baseline, we sample the CoTs by using the checkpoint from the ReFT warm-up stage.\nUsing the generation temperature of 1.0 and max length of 1024, we sample 100 CoTs for each question and only keep those with a correct answer. \nFollowing \\citet{singh2023human}, we then sub-sample the CoTs to 10 random unique CoTs per question to balance difficulties of questions.\nThe number of fine-tune epoch is set to 20, which is sufficiently large to ensure the training to converge.\nAs mentioned in \\S\\ref{sec:baseline}, the Online-ST baseline tries to mimic the same setting as in ReFT. \nWe have the same warm-up process and the hyperparameter setting is roughly the same as ReFT.\n\n\\paragraph{Reward Model Reranking}\n\\label{sec:reward_model_reranking}\nFollowing \\cite{cobbe2021training,uesato2022solving}, we train a reward model (RM) to determine the correctness of the CoT. \nTo construct the RM training data, we use the model from the warm-up stage and perform sampling to obtain 100 CoTs for each question in the training set.\nThe CoTs are deduplicated and the binary labels can be obtained by comparing the extracted answer against the ground truth.\n\nAs a common practice, the reward model is a language model that is initialized from the best SFT checkpoint~\\cite{cobbe2021training,ouyang2022training}. \nSimilar to the outcome-based reward model (ORM) \\cite{uesato2022solving}, the reward model is trained to predict a binary label that indicates the ``\\textit{correct}'' or ``\\textit{incorrect}'' solution. Once the input passes through the reward model, classification is conducted with a linear classifier on the hidden state of the last token. \nFinally, the solution with the highest ``correct'' score among the candidates is selected as the final answer. \nWe train the RM model for 3 epochs using a batch size of 24, the maximum length of 700 and a linear learning rate schedule with $10$\\% warm-up period and the max learning rate of $1$e$-6$.\n\n\\paragraph{Evaluation}\nWe report value accuracy for both N-CoT and P-CoT on all datasets. \nFor majority voting and reranking (Table \\ref{tab:voting_reranking}), we sample 100 CoTs for evaluation. \nIn voting, the valid answer with majority counts is chosen as the final answer for computing accuracy.\nIn reranking, we choose the CoT with the highest score and extract the answer. \n\n\\subsection{Results}\n\\label{sec:results}\n\n\\paragraph{ReFT Outperforms SFT}\nTable \\ref{tab:sft_ppo_result} compares the performance among the baselines and proposed ReFT on GSM8K, SVAMP, and MathQA datasets. \nWe can observe that ReFT consistently achieves much better performance over the SFT \nexcept on MathQA$_\\text{MCQ}$ N-CoT.\nSpecifically, we have closed to $10$-point and $12$-point improvement over SFT with CodeLLAMA on GSM8K N-CoT and P-CoT, respectively. \nOn average, we achieve $6.7$-point and $7.4$-point improvements with CodeLLAMA on all datasets in N-CoT and P-CoT, respectively. \nNotably, no additional annotations or reward models are used in ReFT. \nSuch strong results demonstrate robust generalization of ReFT (see Analysis \\S\\ref{sec:analysis}) and huge potential for further exploring the training data with reinforcement learning~\\cite{lu2023reinforcement}. \n\nOffline self-training includes the sampling data from the initial policy for fine-tuning. \nWe can see this simple baseline can improve the performance compared with SFT~\\cite{he2020revisiting,gulcehre2023reinforced} but the improvements are far behind the one made by ReFT. \nSuch comparisons indicate that ``\\textit{exploring}'' is essential in ReFT to have good performance. \nThough online self-training achieves some more improvements with Galactica, \nit is still far behind ReFT on average.\nThis result indicates that incorrect instances are also very essential to guide the model for better exploration. \nComparisons with self-training also suggest the proposed approach with on-policy sampling and reinforcement learning is better than standard data augmentation approaches. \n\n\\begin{figure}[t!]\n    \\centering\n    \\adjustbox{max width=1.0\\linewidth}{\n\t\t\\begin{tikzpicture}[node distance=2.0mm and 2.0mm, >=Stealth, \n\t\t\twordnode/.style={draw=none, minimum height=5mm, inner sep=0pt},\n\t\t\tchainLine/.style={line width=0.8pt,-, color=mygray},\n\t\t\tentbox/.style={draw=black, rounded corners, fill=red!20, dashed},\n\t\t\tmathop/.style={draw=none, circle, minimum height=2mm, inner sep=1pt, line width=0.8pt, fill=pinkgla},\n\t\t\tstepsty/.style={draw=pinkgla, circle, minimum height=2mm, inner sep=1pt, line width=0.8pt},\n\t\t\tquant/.style={draw=none, minimum height=2mm, inner sep=1pt, line width=1pt},\n\t\t\texpr/.style={draw=electronblue, rectangle, minimum width=20mm, minimum height=4.5mm, inner sep=2pt, line width=0.8pt, rounded corners},\n\t\t\tbox/.style={draw=electronblue, rectangle, rounded corners, line width=0.9pt, dashed, minimum width=26mm, minimum height=20mm},\n\t\t\tinvis/.style={draw=none},\n            highlight/.style={fill=yellow!50, rounded corners}\n\t\t\t]\n\n            \\node[wordnode, align=left, text width=12cm, font=\\fontfamily{phv}\\selectfont] (question) [] {\\textbf{Question}: The diagonals of a rhombus are 18 cm and 22 cm. Find its area?};\n\n            \\node[wordnode, below=of question, align=left, font=\\fontfamily{phv}\\selectfont, text width=12cm] (options) [] {A) 277,  B) 266, \\textcolor{white}{{C) 198}}, D) 288, E) 212};\n\t\t\t\n            \\node[wordnode, align=left, font=\\fontfamily{phv}\\selectfont] (correctOption) at (options.south west) [xshift=3.36cm, yshift=0.22cm] {\\underline{\\bf C) 198}};\n\n            \\node[wordnode, align=left, text width=12cm, font=\\fontfamily{phv}\\selectfont, below=of options, draw=electronblue, inner sep=3pt, line width=1.5pt] (cot) [] {\\textbf{Generated CoT}: To find the area of a rhombus, we need to multiply the lengths of its diagonals and divide the result by 2. \\\\\n            Area of rhombus = (Product of diagonals) / 2 \\\\\n            Area of rhombus = (18 cm x 22 cm) / 2 \\\\\n            Area of rhombus = 344 cm$^2$ / 2 \\\\\n            Area of rhombus = \\textcolor{red}{172 cm$^2$} \\\\\n            Therefore, the answer is: C };\n                        \n            \\begin{pgfonlayer}{background}\n\t\t\t\t\\node[highlight, fit=(correctOption)] {};\n\t\t\t\\end{pgfonlayer}\n            \n\t\t\t\n\t\t\\end{tikzpicture}\n\t}\n\t\\vspace*{-5mm}\n    \\caption{Example prediction of MathQA$_\\text{MCQ}$ reveals reward hacking.}\n    \\label{fig:reward_hacking}\n\\end{figure}\n\n\\begin{table}[t!]\n    \\centering\n    \\adjustbox{max width=1.0\\linewidth}{\n        \\begin{tabular}{llc}\n        \\toprule\n        \\textbf{N-CoT} & \\textbf{Galactica} &  \\textbf{CodeLLAMA}   \n        \\\\ \\midrule\n        SFT   & $40.08$   &  $37.32$ \\\\ \n        Offline Seft-Training & $44.23$   &  $41.24$ \\\\ \n        Online Seft-Training  & $43.78$   &  $38.06$ \\\\ \n        ReFT  & $\\mathbf{45.23}$ &  $\\mathbf{42.24}$  \\\\ \n        \\bottomrule\n        \\end{tabular}\n    }\n\\caption{Value accuracy of ReFT and the baselines with two foundation models on MathQA$_\\text{numeric}$ benchmark}\n\\label{tab:mathqa_numeric}\n\\end{table}\n\n\\paragraph{Reward Hacking for MathQA}\nOur investigation of the negative results on MathQA$_\\text{MCQ}$ indicates that ReFT suffers from the reward hacking~\\cite{skalse2022defining} on the multi-choice question during training. \nFigure \\ref{fig:reward_hacking} shows how the sampled solutions produce ``\\textit{inaccurate rewards}'', which makes the RL training suffer. \nAs we can see, the sampled CoT obtains an incorrect answer ``\\textit{172}'' which is not half of the product of ``\\textit{18}'' and ``\\textit{22}''.\nHowever, the final reasoning step still predicts the option ``\\textit{C}'' as the final answer as the model would always predict one of the options from $\\{\\texttt{A, B, C, D, E}\\}$ regardless of the correctness of intermediate CoT\\footnote{We found that program-based CoTs are less likely to suffer as it is more rigorous than natural language.}. \nThus, such a misleading CoT will receive a positive reward ``$1$'' and misguide the model to treat this as a correct CoT. \nThe underlying reward hacking phenomenon severely tampers the model training~\\cite{everitt2021reward}. \nThis is also the reason that we chose the checkpoint with longer warm-up steps for MathQA N-CoT to reduce the reward hacking effect.\n\n\\begin{table}[t!]\n    \\centering\n    \\adjustbox{max width=1.0\\linewidth}{\n    \\begin{tabular}{lcccc}\n    \\toprule\n         \\multirow{2}{*}{\\bf Method}& \\multirow{2}{*}{\\bf Size} &  \\multicolumn{2}{c}{\\bf GSM8K} & {\\bf Extra SFT} \\\\\n         & & \\bf N-CoT &\\bf  P-CoT & {\\bf Data}\\\\\n         \\midrule\n         Galactica + SFT + Voting & 6.7B& $52.8$ & $62.9$  & \\includegraphics[width=0.15in]{green_cross.png}  \\\\\n         Galactica + ReFT + Voting & 6.7B& $58.5$ & $71.8$ & \\includegraphics[width=0.15in]{green_cross.png} \\\\\n         \\hdashline\n         Galactica + SFT + Reranking& 6.7B &  $57.5$ & $73.4$ & \\includegraphics[width=0.15in]{green_cross.png} \\\\\n         Galactica + ReFT + Reranking & 6.7B&  $\\mathbf{59.2}$ & $\\mathbf{76.4}$ & \\includegraphics[width=0.15in]{green_cross.png} \\\\\n         \\midrule\n         \\midrule\n         CodeLLAMA + SFT  + Voting & \\textcolor{white}{0.}7B& $53.5$ & $68.0$ & \\includegraphics[width=0.15in]{green_cross.png}\\\\\n         CodeLLAMA + ReFT  + Voting & \\textcolor{white}{0.}7B& $63.2$ & $78.0$ & \\includegraphics[width=0.15in]{green_cross.png}\\\\\n         \\hdashline\n         CodeLLAMA + SFT  + Reranking& \\textcolor{white}{0.}7B &  $62.9$ & $77.0$& \\includegraphics[width=0.15in]{green_cross.png}\\\\\n         CodeLLAMA + ReFT  + Reranking & \\textcolor{white}{0.}7B&  $\\mathbf{66.0}$ & $\\mathbf{81.2}$ & \\includegraphics[width=0.15in]{green_cross.png} \\\\\n         \\midrule\n         \\midrule\n         \\multicolumn{3}{l}{\\textcolor{mygray}{Other Foundation Models}~$\\dagger$} &\\\\\n         \\textcolor{black}{~WizardMath}~\\cite{luo2023wizardmath} & \\textcolor{white}{0}7B & $54.9$ & - & \\includegraphics[width=0.15in]{check.png} ($\\textcolor{white}{0}96$k) \\\\\n         \\textcolor{black}{~WizardMath}~\\cite{luo2023wizardmath} & 13B & $63.9$ & - & \\includegraphics[width=0.15in]{check.png} ($\\textcolor{white}{0}96$k)\\\\\n         \\textcolor{black}{~MathCoder}~\\cite{wang2023mathcoder} & \\textcolor{white}{0}7B & $67.8$ & - & \\includegraphics[width=0.15in]{check.png} ($\\textcolor{white}{0}80$k)\\\\ \n         \\textcolor{black}{~MAmmoTH-Coder}~\\cite{yue2023mammoth} & \\textcolor{white}{0}7B & $22.2$ & $58.8$  & \\includegraphics[width=0.15in]{check.png} ($260$k)\\\\\n         \\textcolor{black}{~MAmmoTH-Coder}~\\cite{yue2023mammoth} & 70B & $72.4$ & $76.7$  & \\includegraphics[width=0.15in]{check.png} ($260$k)\\\\\n         \\textcolor{black}{~DeepSeekMath}~\\cite{shao2024deepseekmath} & \\textcolor{white}{0}7B & $88.2$ & $86.7$ & \\includegraphics[width=0.15in]{check.png} ($776$k)\\\\\n         \\midrule\n         \\midrule\n         \\textcolor{black}{~GPT-3.5-turbo~\\cite{jie2023design}} & N.A. & $75.3$ & $78.0$& N.A.  \\\\\n         \\textcolor{black}{~GPT-4~\\cite{openai2023gpt4, zhou2023solving}} & N.A. & $93.0$ & $97.0$  & N.A.\\\\\n         \\bottomrule\n    \\end{tabular}\n    }\n    \\caption{Solving accuracy of majority voting and reward model reranking for SFT and ReFT on GSM8K. We also include existing approaches for comparison.}\n    \\label{tab:voting_reranking}\n\\end{table}\n\nTo further demonstrate the negative effect of MCQ questions, we experiment on the MathQA variant by \\citet{jie2023leveraging}, MathQA$_\\text{numeric}$ (Table \\ref{tab:data_statistics}), which removes the options in the question, and directly predicts the numeric answer. \nTable \\ref{tab:mathqa_numeric} presents the comparison against the baselines. \nWe can observe that ReFT consistently outperforms the baselines using both Galactica and CodeLLAMA. \nIdeally, we could reduce the reward hacking effect on MathQA$_\\text{MCQ}$ if we can obtain a more fine-grained reward (e.g., process-based reward~\\cite{lightman2023lets}) for the intermediate reasoning steps. \nHowever, the development of a reliable process-based reward model is expensive, and requires extensive manual annotations of reasoning steps. \nRecognizing these challenges, we consider controlling reward hacking and its analysis as an important problem to be addressed in future work.\n\n\\paragraph{Majority Voting and Reranking Benefit ReFT}\nFollowing \\citet{wang2022self,uesato2022solving,lightman2023lets}, we also perform majority voting and reward model reranking to show that ReFT can benefit from these common techniques. \nSpecifically, we perform sampling from both SFT and ReFT policies. \nWe sample $100$ CoT solutions for each question and employ the reward model described in \\S\\ref{sec:setup} to perform reranking. \nResults in Table \\ref{tab:voting_reranking} demonstrate that ReFT consistently achieves the best performance on GSM8K by reward model reranking. \nReFT + Voting significantly outperforms SFT + Voting by $8.6$ points on average across all settings.\nReFT with reranking outperforms SFT with reranking by more than $3$ points.\n\nCompared with existing open-source approaches~\\cite{luo2023wizardmath,wang2023mathcoder,yue2023mammoth} (Table \\ref{tab:voting_reranking} bottom\\footnote{Numbers are taken from original papers. The N-CoT and P-CoT results for MAmmoTH-Coder are reported in their appendix.}), our best P-CoT variant achieves the best performance with accuracy $81.2$ on GSM8K. \nIn addition, these approaches mainly include extra data generated from ChatGPT and perform distillation during fine-tuning. \nIn contrast, we improve the policy itself by exploiting the potential of existing training data and pushing the limit of the policy performance. \nOur best result reported in Table \\ref{tab:voting_reranking}, \ni.e., the CodeLLAMA + ReFT  + Reranking with P-CoT setting, \neven surpasses \\texttt{GPT-3.5-turbo}.\nHowever, we obtain the result with a model that is only in the size of 7B. \n\n\\begin{table}[t!]\n    \\centering\n     \\adjustbox{max width=1.0\\linewidth}{\n    \\begin{tabular}{lccc}\n        \\toprule\n         \\textbf{Method} &  \\bf GSM8K & \\bf SVAMP &\\bf  MathQA$_\\text{MCQ}$ \\\\\n         \\midrule\n         Galactica-125M + SFT  & $23.7$ & $35.6$ & $58.4$ \\\\\n         Galactica-125M + ReFT & $29.8$ & $39.4$ & $60.7$ \\\\\n         \\midrule\n         Codeparrot-small + SFT  & $13.8$ & $25.7$ & $55.3$ \\\\\n         Codeparrot-small + ReFT & $16.8$ & $27.4$ & $58.3$ \\\\\n         \\midrule\n         Codegen-350M + SFT  & $20.4$ & $34.4$ & $56.4$ \\\\\n         Codegen-350M + ReFT & $28.4$ & $39.3$ & $59.1$ \\\\\n         \\bottomrule\n    \\end{tabular}\n    }\n    \\caption{Experiments on P-CoT with Galactica-125M, Codeparrot-small and Codegen-350M.}\n    \\label{tab:small_model}\n\\end{table}\n\n\\begin{figure*}[t!]\n    \\centering\n    \\begin{subfigure}[b]{0.32\\linewidth}\n\t\t\\centering\n\t\t\\adjustbox{max width=1\\textwidth}{\n\t\t\t\\begin{tikzpicture}\n        \t\\tikzstyle{every node}=[font=\\bfseries]\n        \t\\begin{axis}[\n        \t\txlabel style={font=\\bfseries},\n        \t\tylabel style={font=\\bfseries},\n        \t\twidth=10cm,\n        \t\theight=6cm,\n        \t\tgrid=major,\n        \t\tgrid style={dashed,gray!30},\n        \t\tenlarge x limits=false,\n        \t\txmin=0, xmax=60, % Set the range for the x-axis\n        \t\ttick label style={font=\\large, /pgf/number format/fixed},\n        \t\taxis line style = thick,\n        \t\tlabel style={font=\\large},\n        \t\tlegend style={font=\\small, at={(1,0.3)}, anchor=north east, line width=0.4mm},\n        \t\tlegend cell align={left},\n        \t\tcycle list name=exotic,\n        \t\tline width = 0.2mm,\n        \t\tno markers,\n        \t\tsmooth\n        \t\t]\n        \t\t\n        \t\t\\addplot table[x expr=\\thisrowno{0}/460, y expr=\\thisrowno{1}] {actual_reward.txt};\n        \t\t\\addlegendentry{Actual Data}\n        \t\t\\addplot+[draw=puffin] table[x expr=\\thisrowno{0}, y expr=\\thisrowno{1}] {reward_sma.txt};\n        \t\t\\addlegendentry{10-point Smooth Moving Average}\n        \t\t\n        \t\\end{axis}\n        \\end{tikzpicture}\n\t\t}\n    \\vspace*{-6mm}\n    \\caption{Mean Training reward}\n\t\\end{subfigure}\n     \\begin{subfigure}[b]{0.32\\linewidth}\n     \\centering\n     \\adjustbox{max width=1\\textwidth}{\n        \\begin{tikzpicture}\n        \t\\tikzstyle{every node}=[font=\\bfseries]\n        \t\\begin{axis}[\n        \t\txlabel style={font=\\bfseries},\n        \t\tylabel style={font=\\bfseries},\n        \t\twidth=10cm,\n        \t\theight=6cm,\n        \t\tgrid=major,\n        \t\tgrid style={dashed,gray!30},\n        \t\tenlarge x limits=false,\n        \t\txmin=0, xmax=300, % Set the range for the x-axis\n        \t\ttick label style={font=\\large, /pgf/number format/fixed},\n        \t\taxis line style = thick,\n        \t\tlabel style={font=\\large},\n        \t\tlegend style={font=\\small, at={(1,0.2)}, anchor=north east, line width=0.4mm},\n        \t\tlegend cell align={left},\n        \t\tcycle list name=exotic,\n        \t\tline width = 1mm,\n        \t\tno markers,\n        \t\tsmooth\n        \t\t]\n        \t\t\n        \t\t\\addplot+[line width=2pt, draw=deeppurple] table[x expr=\\thisrowno{0} / 460, y expr=\\thisrowno{1}] {eval_accuracy_new.txt};\n        \t\t\n        \t\t\n        \t\\end{axis}\n        \\end{tikzpicture}\n\n         }\n        \\vspace*{-6mm}\n    \\caption{Evaluation accuracy}\n     \\end{subfigure}\n     \\begin{subfigure}[b]{0.32\\linewidth}\n     \\centering\n     \\adjustbox{max width=1\\textwidth}{\n        \\begin{tikzpicture}\n        \t\\tikzstyle{every node}=[font=\\bfseries]\n        \t\\begin{axis}[\n        \t\txlabel style={font=\\bfseries},\n        \t\tylabel style={font=\\bfseries},\n        \t\twidth=10cm,\n        \t\theight=6cm,\n        \t\tgrid=major,\n        \t\tgrid style={dashed,gray!30},\n        \t\tenlarge x limits=false,\n        \t\txmin=0, xmax=60, % Set the range for the x-axis\n        \t\ttick label style={font=\\large, /pgf/number format/fixed},\n        \t\taxis line style = thick,\n        \t\tlabel style={font=\\large},\n        \t\tlegend style={font=\\small, at={(1,0.7)}, anchor=north east, line width=0.4mm},\n        \t\tlegend cell align={left},\n        \t\tcycle list name=exotic,\n        \t\tline width = 0.2mm,\n        \t\tno markers,\n        \t\tsmooth\n        \t\t]\n        \t\t\n        \t\t\\addplot+[draw=darkgrass] table[x expr=\\thisrowno{0}/460, y expr=\\thisrowno{1}] {kl.txt};\n        \t\t\n        \t\\end{axis}\n        \\end{tikzpicture}\n         }\n         \\vspace*{-6mm}\n    \\caption{Mean Sequence KL}\n     \\end{subfigure}\n     \\vspace*{-2mm}\n    \\caption{Training reward of ReFT, evaluation accuracy, KL against training epoch on GSM8K P-CoT. }\n    \\label{fig:training_reward}\n\\end{figure*}\n\n\\paragraph{Experiments with Small Model}\nIntuitively, exploration could lead to imperfect demonstration with a small language model. \nWe conduct an experiment on P-CoT data using Galactica-125M\\footnote{\\rurl{huggingface.co/facebook/galactica-125m}}, Codeparrot-small\\footnote{\\rurl{huggingface.co/codeparrot/codeparrot-small}} and Codegen-350M\\footnote{\\rurl{huggingface.co/Salesforce/codegen-350M-mono}}.\nTable \\ref{tab:small_model} shows the performance comparison between SFT and ReFT. \nSurprisingly, ReFT still outperforms SFT on three datasets.\nSuch improvements demonstrate the robustness of ReFT during the exploration of reasonable programs. \n\n\\begin{table}[t!]\n    \\centering\n     \\adjustbox{max width=1.0\\linewidth}{\n    \\begin{tabular}{lc}\n    \\toprule\n         \\textbf{Model Setting} &  \\textbf{Accuracy}\\\\\n         \\midrule\n        CodeLLAMA + ReFT & $75.28$ \\\\\n        ~~~~~~~  -- remove partial reward & $74.40$ \\\\\n        ~~~~~~~  -- KL coefficient $\\beta = 0$ & \\textit{collapse} \\\\\n        ~~~~~~~  -- non-shared value model & $75.15$ \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\caption{Ablation study on GSM8K P-CoT.}\n    \\label{tab:ablation}\n\\end{table}\n\n\\paragraph{Ablation Study}\nWe perform the ablation study using CodeLLAMA on GSM8K P-CoT (Table \\ref{tab:ablation}). \nWithout the partial reward, ReFT obtains a lower accuracy $74.4$ but it is still much better than SFT. \nAs mentioned in \\S\\ref{sec:reft}, such a partial reward can help reduce the effect of sparse reward~\\cite{trott2019keeping} during training.\nIn addition, the policy distribution will easily collapse to produce unexpected results (i.e., $0$ accuracy) if we set the KL coefficient $\\beta$ to $0$. \nIt is certainly critical to impose constraints on the space that the policy explores~\\cite{ouyang2022training}. \nThe initial warm-up step essentially makes such constraints and allows the policy to further explore within the range that is governed by $\\beta$. \nWe also experiment with a separate value model~\\cite{andrychowicz2021matters,cobbe2021phasic}, \nwhere the torso parameters are initialized the same as the policy model.\nWe found that such a setting allows the policy to converge faster in early RL training, but eventually reaches an on par performance.\nCompared to the original setting of a shared value model, \nit is, however, twice the computation overhead due to one extra forward-pass, \nas well as twice the memory cost due to the storage of the separate value net.\nFinally, in Appendix~\\ref{sec:case_study} we give a case study to show how the generated P-CoT evolve for SFT and ReFT.\n\n\\section{Analysis}\n\\label{sec:analysis}\n\n\\paragraph{Generalization}\nFigure \\ref{fig:training_reward} shows the mean reward, evaluation accuracy, and the KL divergence during training of ReFT\\footnote{For illustration purpose, we only shows the mean reward and KL for $60$ epochs.} on GSM8K P-CoT using CodeLLAMA as foundation model. \nSFT converges and becomes overfiting when approaching 40$^{th}$ epoch. \nHowever, we can see the mean reward is around 80\\% to 90\\% for the ReFT policy at 40$^{th}$ epoch, and the value accuracy is also increasing. \nIn addition, we can see that the KL divergence (Figure \\ref{fig:training_reward} (c)) is very large in the beginning and then maintains a reasonable value between $0$ and $10$.\nThe stable KL divergence indicates our policy performs exploration within a space that contains appropriate programs. \nThe underlying reinforcement learning mechanism greatly improves the generalization ability of ReFT~\\cite{brown2020better}. \n\n\\paragraph{Qualitative Evaluation}\n\nWe perform a human evaluation to qualitatively assess the output from the SFT model, Warmup checkpoint, and ReFT model. \nThe evaluation uses 50 questions and samples the solutions in GSM8K test set that can be solved correctly by all three models. \nWe ask four different annotators to score the reasoning path according to the following criteria, each scored on a scale from $0$ to $1$.\n\\squishlist\n\\item \\textit{Logic}: evaluates if the logic leading to the answer is correct.\n\\item \\textit{Naming}: evaluates if the variable conveys appropriate and reasonable semantics \n\\item \\textit{Compactness}: evaluates if the reasoning paths contain redundant information. \n\\squishend\nA perfect score of 3 indicates good performance across these three dimensions.\nTo ensure the evaluation is impartial and faithful, we strictly follow the setting:\n(1) The origin of each reasoning path (from SFT, Warmup, or ReFT) is anonymized to prevent annotator bias.\n(2) Four different annotators are responsible for different portions of the samples.\n\n\\begin{table}[t!]\n    \\centering\n    \\adjustbox{max width=1.0\\linewidth}{\n        \\begin{tabular}{lcccc}\n        \\toprule\n        {\\textbf{Method}} &  \\textbf{Logic} &  \\textbf{Naming} &  \\textbf{Compactness} &  \\textbf{Overall Score} \\\\ \\midrule\n        {\\textbf{SFT}}    &  0.986          &  0.988           &  0.994                &  2.967                  \\\\ \n        {\\textbf{Warmup}} &  0.949          &  0.982           &  0.990                &  2.920                  \\\\ \n        {\\textbf{ReFT}}   &  0.992          &  0.990           &  0.996                &  \\textbf{2.982}         \\\\ \\bottomrule\n        \\end{tabular}\n    }\n    \\caption{Qualitative scores of models from three methods trained on GSM8k P-CoT dataset.}\n    \\label{tab:qualitative_table}\n\\end{table}\n\nAs seen in table \\ref{tab:qualitative_table}, though the overall scores are quite close, ReFT performs slightly better than SFT, and outperforms the Warmup variant. Note that SFT is inherently trained to learn from the ground truth, thus, it is likely to have a high score.\nThis comparative analysis underscores the robustness of ReFT in generating accurate and semantically coherent reasoning paths. \n\n\\paragraph{When ReFT surpasses SFT?}\nTo further investigate the relationship between ReFT and SFT, we perform ReFT training with different number of warm-up steps from SFT. \nFigure \\ref{fig:reft_epoch} shows the value accuracy of different ReFT variants against SFT\\footnote{We only show 60 epochs for illustration purposes. The performance for the later epoch is shown in Figure \\ref{fig:training_reward} (b).}. \nSpecifically, if the warmup step is $3$, that means the policy initialize from the $3^{rd}$-epoch SFT checkpoint. \nWe can see that the performance of all ReFT policies decreases right after the warm-up in the beginning, until the training epoch reaches around $8$.\nBecause the linear layer in the shared value model is randomly initialized, and it could take a few epochs to adjust the distribution. \nStarting from the $30^{th}$ epoch, SFT converges and all ReFT variants are still improving. \nWe can also see that all variants outperform SFT by a significant margin and there is no obvious advantage of any specific ReFT variant. \n\n\\begin{figure}[t!]\n    \\centering\n    \\adjustbox{max width=0.95\\linewidth}{\n    \\begin{tikzpicture}\n\t\\tikzstyle{every node}=[font=\\bfseries]\n\t\\begin{axis}[\n\t\txlabel={Training Epoch},\n\t\txlabel style={font=\\bfseries},\n\t\tylabel style={font=\\bfseries},\n\t\tgrid=major,\n\t\tgrid style={dashed,gray!30},\n\t\tenlarge x limits=false,\n\t\txmin=0, xmax=60, % Set the range for the x-axis\n\t\ttick label style={font=\\large, /pgf/number format/fixed},\n\t\taxis line style = thick,\n\t\tlabel style={font=\\large},\n\t\tlegend style={font=\\small, at={(1,0.5)}, anchor=north east, line width=0.4mm},\n\t\tlegend cell align={left},\n\t\tcycle list name=exotic,\n\t\tline width = 1mm,\n\t\tno markers,\n\t\tsmooth\n\t\t]\n\t\t\n\t\t\\addplot+[line width=2pt,restrict x to domain=0:60] table[x expr=\\thisrowno{0}, y expr=\\thisrowno{3}] {reft_ep.txt};\n\t\t\\addplot+[line width=2pt,restrict x to domain=0:60] table[x expr=\\thisrowno{0}, y expr=\\thisrowno{4}] {reft_ep.txt};\n\t\t\\addplot+[line width=2pt,restrict x to domain=0:60] table[x expr=\\thisrowno{0}, y expr=\\thisrowno{5}] {reft_ep.txt};\n\t\t\\addplot+[line width=2pt,restrict x to domain=0:60] table[x expr=\\thisrowno{0}, y expr=\\thisrowno{6}] {reft_ep.txt};\n\t\t\\addplot+[line width=2pt,restrict x to domain=0:40] table[x expr=\\thisrowno{0}, y expr=\\thisrowno{1}] {reft_ep.txt};\n        \n\t\t\\addlegendentry{ReFT$_\\text{warm-up\\_ep.=1}$}\n\t\t\\addlegendentry{ReFT$_\\text{warm-up\\_ep.=2}$}\n\t\t\\addlegendentry{ReFT$_\\text{warm-up\\_ep.=3}$}\n\t\t\\addlegendentry{ReFT$_\\text{warm-up\\_ep.=4}$}\n\t\t\\addlegendentry{SFT}\n\t\t\n\t\\end{axis}\n\\end{tikzpicture}\n    }\n    \\vspace*{-3mm}\n    \\caption{Accuracy comparison between SFT and ReFT with different number of warm-up epoch.}\n    \\label{fig:reft_epoch}\n\\end{figure}\n\n\\section{Conclusion}\nWe have introduced reinforced fine-tuning (ReFT) as a new method for fine-tuning models to solve math problems. In contrast to SFT, ReFT optimizes a non-differentiable objective by exploring multiple CoT annotations in the search for the correct answer, rather than relying on a single annotation.\n\nThrough extensive experimentation on three datasets using two foundation models, we have demonstrated that ReFT outperforms SFT in terms of performance and generalization ability. Moreover, we have showcased the compatibility of models trained with ReFT with techniques such as majority voting~\\cite{wang2022self} and reward model reranking~\\cite{cobbe2021training,uesato2022solving}.\n\nFurthermore, ReFT has exhibited superior performance compared to several publicly available open-source models of comparable sizes in math problem-solving. This demonstrates the effectiveness and practical value of the ReFT approach.\n\n\\section{Future Work}\nWe have made the first attempt of applying reinforcement learning, specifically the PPO algorithm~\\cite{schulman2017proximal}, to fine-tune of LLMs for math problem-solving. Our future work includes utilization of offline reinforcement learning techniques~\\cite{levine2020offline,gulcehre2023reinforced}, development of a \\textit{warm-up free} method to enhance training efficiency and performance, thereby reducing the gap with the reranking method. Additionally, \\citet{lightman2023lets} suggests that a well-trained process-based reward model (PRM) can significantly enhance performance. Hence, it would be worthwhile to explore the implementation of process-based rewards in reinforcement learning training. \nLastly, as ReFT is a versatile approach, we intend to apply it to more general reasoning tasks where the inference can be formalized with CoT.\n\n\\section*{Limitations}\n\\paragraph{Training Efficiency}\nAs depicted in Figure \\ref{fig:training_reward} (b), it is evident that ReFT necessitates a greater number of epochs to reach convergence compared to SFT. This is primarily due to the fact that ReFT optimizes a non-differentiable objective and requires exploration of the generation space to attain correct answers. While a larger learning rate may expedite convergence, it also makes the policy more susceptible to instability and potential collapse. Alternatively, using a larger batch size is a viable option; however, it comes at the expense of increased computational costs.\n\n\\paragraph{Reward Hacking}\nOur reward function relies solely on the final answer to determine the reward. However, as demonstrated in the experiments conducted on the MathQA$_\\text{MCQ}$ N-CoT dataset, the policy can be easily manipulated if the possible space of final answers is limited, such as ${\\texttt{A,B,C,D}}$. To mitigate the issue of reward hacking, it may be necessary to employ a more detailed or process-based reward function that takes into account a broader range of factors.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{MLLM-as-a-Judge:\\\\ Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark}\n\n\\begin{document}\n\n\\twocolumn[\n\\icmltitle{MLLM-as-a-Judge:\\\\ Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark\n}\n\n\\icmlsetsymbol{equal}{*}\n\n\\begin{icmlauthorlist}\n\\icmlauthor{Dongping Chen}{equal,hust}\n\\icmlauthor{Ruoxi Chen}{equal,zjut}\n\\icmlauthor{Shilin Zhang}{equal,hust}\n\\icmlauthor{Yaochen Wang}{equal,hust}\n\\icmlauthor{Yinuo Liu}{equal,hust}\n\\icmlauthor{Huichi Zhou}{equal,hust}\n\\icmlauthor{Qihui Zhang}{equal,hust}\n\\icmlauthor{Yao Wan}{hust}\n\\icmlauthor{Pan Zhou}{hust}\n\\icmlauthor{Lichao Sun}{lehigh}\n\\end{icmlauthorlist}\n\n\\icmlaffiliation{hust}{Huazhong University of Science and Technology}%School of Computer Science and Technology, \n\\icmlaffiliation{zjut}{Zhejiang University of Technology}\n\\icmlaffiliation{lehigh}{LAIR Lab, Lehigh University}\n\n\\icmlcorrespondingauthor{Yao Wan}{wanyao@hust.edu.cn}\n\\icmlcorrespondingauthor{Pan Zhou}{panzhou@hust.edu.cn}\n\n\\vskip 0.3in\n]\n\n\\printAffiliationsAndNotice{\\icmlEqualContribution} % otherwise use the standard text.\n\n\\begin{abstract}\nMultimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence.\nHowever, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence of multimodal benchmarks that align with human preferences.\nDrawing inspiration from the concept of LLM-as-a-Judge within LLMs, this paper introduces a novel benchmark, termed \\text{MLLM-as-a-Judge}, to assess the ability of MLLMs in assisting judges across diverse modalities, encompassing three distinct tasks: \\textit{Scoring Evaluation}, \\textit{Pair Comparison}, and \\textit{Batch Ranking}.\nOur study reveals that, while MLLMs demonstrate remarkable human-like discernment in \\textit{Pair Comparison}, there is a significant divergence from human preferences in \\textit{Scoring Evaluation} and \\textit{Batch Ranking}.\nFurthermore, a closer examination reveals persistent challenges in the judgment capacities of LLMs, including diverse biases, hallucinatory responses, and inconsistencies in judgment, even in advanced models such as GPT-4V.\nThese findings emphasize the pressing need for enhancements and further research efforts to be undertaken before regarding MLLMs as fully reliable evaluators. \nIn light of this, we advocate for additional efforts dedicated to supporting the continuous development within the domain of MLLM functioning as judges. The code and dataset are publicly available at our project homepage: \\url{https://mllm-judge.github.io/}.\n\n\\end{abstract}\n\\section{Introduction}\nThe advent of Large Language Models (LLMs), such as GPT-3~\\cite{openai2023gpt4} and Llama~\\cite{touvron2023llama}, has achieved substantial progress in content generation, \nincluding text generation~\\citep{openai2023gpt4}, code generation~\\cite{roziere2023code}, and video synthesis~\\citep{wu2023next}. The emergent abilities of LLMs, as demonstrated by the Chain-of-Thought (CoT) framework~\\cite{wei2022chain}, present a promising avenue for their utilization as evaluators, also referred to as the LLM-as-a-Judge~\\citep{zheng2023judging}. Initial explorations indicate a better alignment with human preferences, emphasizing the considerable potential inherent in this approach.\n\nRecently, building upon LLMs, Multimodal Large Language Models (MLLMs) like GPT-4V~\\cite{openai2023gpt4v} and LLaVA~\\cite{liu2023llava} exhibit exceptional proficiency by incorporating multiple modalities (e.g., text, charts, images, and videos) and showcasing remarkable performance in multimodal applications, including text-to-video~\\cite{wu2023next} and visual dialog~\\cite{cai2023low}.\nDespite this, \nassessing the effectiveness of MLLMs remains challenging due to the limitations of traditional metrics, which hinge on text-based exact matches or embedding distances. These metrics fall short in adhering to the granular evaluation criteria of interest and fail to capture the rich context within the generated outputs.\nDrawing inspiration from the concept of LLM-as-a-Judge within LLMs, a pertinent research question arises: \\textit{``Can MLLMs effectively serve as judges in the multimodal domain, and how closely do their evaluations align with human preferences?''}\n\n\\begin{figure*}[t]\n\\vspace{-5pt}\n    \\centering\n    \\includegraphics[width=.88\\linewidth]{figure/Radar.pdf}\n      \\vspace{-13pt}\n    \\caption{Comparative performance of different MLLMs across three judging settings in 10 datasets, each is the average of three iterations. As the CogVLM is unable to perform the batch ranking task, we show the other six MLLMs only.}\n    \\label{fig: Radar Figure}\n      \\vspace{-12pt}\n\\end{figure*}\n\nTo answer this question, this paper undertakes an extensive study, introducing a groundbreaking benchmark, MLLM-as-a-Judge, specifically crafted to evaluate the efficacy of MLLMs in assisting judges across diverse modalities.\nTo achieve this goal, we first thoughtfully curate a selection of 14 datasets across various tasks, including image captioning, math reasoning, text reading, and infographics understanding, culminating in acquiring a dataset comprising 4,414 image-instruction pairs.\nSubsequently, we utilize six main-stream MLLMs from a model pool which includes GPT-4V \\citep{openai2023gpt4v}, Gemini \\citep{geminiteam2023gemini}\\footnote{For conciseness, we refer to  GPT-4V(ision) as GPT-4V, and Gemini-Pro-Vision as Gemini throughout this paper.}, LLaVA-1.5-13b, LLaVA-1.6-34b \\citep{liu2023llava}, CogVLM \\citep{wang2023cogvlm}, Qwen-VL-Max \\citep{Qwen-VL}, to generate responses to each instruction across three distinct evaluation settings.\nThe produced responses are subsequently gathered and undergo additional annotation by human evaluators, who apply stringent criteria to ensure an impartial and thorough assessment of the judgments made by the MLLMs.\n\nFurthermore, we assess the ability of MLLMs as judges in multimodal tasks by calculating the similarity between human and MLLMs judgment and measuring human agreement on the analysis and judgment made by those MLLMs.\nIn particular, we target eleven widely-used MLLMs, i.e., GPT-4V and Gemini-Pro-1.0/1.5, CogVLM, LLaVA-1.5/1.6 family, and Qwen-VL family, across two settings (with, or without vision input), over three distinct tasks (i.e., \\textit{Scoring Evaluation}, \\textit{Pair Comparison}, and \\textit{Batch Ranking}). Figure~\\ref{fig: Radar Figure} compares the performance of various MLLMs across different datasets and settings, illustrating that GPT-4V exhibits significantly superior capabilities as a judge compared to other MLLMs.\n\nAs a benchmark, we also release two curated datasets to facilitate further studies: \\textsc{MLLM-as-a-Judge-HQ}, which showcases responses with a high level of concordance with human judgments, and \\textsc{MLLM-as-a-Judge-Hard}, which includes responses marked by inconsistency with human preferences and instances of hallucination. Additionally, we address the limitations of MLLMs in judgment, such as egocentric bias, position bias, length bias, and hallucination. We demonstrate that integrating CoT \\citep{wei2022chain} and a vision expert system can effectively mitigate some of these biases.\n\n\\paragraph{Take-Aways.}\nWe evaluate the judgment performance of 11 MLLMs across 14 datasets under three settings: score evaluation, pair comparison, and batch ranking. Our findings reveal several key insights. First, while MLLMs demonstrate proficiency in aligning with human preferences in pair comparison tasks, they require further improvement in score evaluation and batch ranking, particularly in reasoning tasks. Secondly, GPT-4V consistently outperforms other models across all tasks and settings.\n\nFinally, the presence of hallucinations, biases, and inconsistent judgments in MLLMs highlights significant challenges that must be addressed for these models to become a viable alternative to traditional human evaluations.\n\nTo summarize, our work provides three key contributions: \n\\vspace{-5pt}\n\\begin{itemize}[nolistsep, leftmargin=*]\n    \\item \\textbf{A Benchmark.}\n    We are the first to develop a comprehensive benchmark MLLM-as-a-Judge in multimodal domains, with human annotations to assess the judging capability of MLLMs in tasks of \\textit{Scoring Evaluation}, \\textit{Pair Comparison} and \\textit{Batch Ranking}. \n    \\item \\textbf{Two Datasets.}\n    We curate two human preference datasets: \\textsc{MLLM-as-a-Judge-HQ}, which contains high-quality questions, and \\textsc{MLLM-as-a-Judge-HARD}, which includes instances of hallucination. These datasets can serve as rigorous testing grounds to facilitate the development of MLLMs in aligning human preferences.\n\n    \\item \\textbf{Findings and Implications.}\n    Our evaluation of mainstream MLLMs reveals that while MLLMs exhibit alignment with human judgments in \\textit{Pair Comparison}, notable discrepancies can be found in \\textit{Scoring Evaluation} and \\textit{Batch Ranking}. Furthermore, our findings reveal that MLLMs exhibit a range of biases and hallucinations, along with inconsistent judgments during the evaluation process, representing significant hurdles in establishing MLLMs as reliable judges.\n\n\\end{itemize}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=0.95\\linewidth]{figure/fig1-v2.pdf}\n    \\vspace{-8pt}\n    \\caption{An overview of {MLLM-as-a-Judge}.}\n      \\vspace{-8pt}\n    \\label{fig: pipeline}\n\\end{figure*}\n\n\\section{{MLLM-as-a-Judge}: A Benchmark to Assess Vision-Language Judging Ability}\n\\label{Section 3: MLLM-as-a-Judge}\n\n\\begin{table}[t]\n\\centering\n\\caption{The statistics of responses in different steps for MLLM judging. In \\textit{Step 3}, under the w.o. vision input settings, we sample 10\\% from the original data and mainly proceed with \\text{GPT-4V} and \\text{Gemini}. We only list the amount of judgments generated by four models here. M-I: Image-Instruction.}\n\\renewcommand\\arraystretch{1.1}\n\\label{table: comparison}\n\\resizebox{0.98\\linewidth}{!}{\n\\begin{tabular}{ccclclc}\n\\toprule[1.5pt]\n\\textbf{Step}           & \\multicolumn{2}{c}{\\textbf{Setting}}                                   & \\textbf{Input}                                                                     & \\textbf{Num.}          & \\textbf{Output}                                                                    & \\textbf{Num.}          \\\\ \\midrule\n\\multirow{2}{*}{1}  & \\multicolumn{2}{c}{\\multirow{2}{*}{/}}                                 & Image                                                                              & 4,144                  & \\multirow{2}{*}{\\begin{tabular}[c]{@{}l@{}}M-I Pairs\\end{tabular}} & \\multirow{2}{*}{4,400} \\\\\n                        & \\multicolumn{2}{c}{}                                                   & Instruction                                                                        & 4,414                  &                                                                                    &                       \\\\ \\midrule\n2  & \\multicolumn{2}{c}{/}                                 & \\begin{tabular}[c]{@{}l@{}} M-I Pairs\\end{tabular} & 3,300 & MLLMs & 17,096\\\\ \\midrule\n\\multirow{25}{*}{3} & \\multicolumn{2}{c}{\\multirow{12}{*}{\\rotatebox{90}{w. Vision Input} }}                   & \\multirow{4}{*}{Batch}                                                             & \\multirow{4}{*}{1,470} & \\text{Gemini}                                                                             & 1,340                   \\\\\n                        & \\multicolumn{2}{c}{}                                                   &                                                                                    &                       & \\text{GPT-4V}                                                                                & 1,454                \\\\\n                        & \\multicolumn{2}{c}{}                                                   &                                                                                    &                       & \\text{Qwen-VL-Max}                                                                             & 1,458                  \\\\\n                         & \\multicolumn{2}{c}{}                                                   &                                                                                    &                       & \\text{LLaVA}                                                                             & 1,468                  \\\\\\cline{4-7} \n                        & \\multicolumn{2}{c}{}                                                   & \\multirow{4}{*}{Pair}                                                              & \\multirow{4}{*}{8,256} & \\text{Gemini}                                                                             & 7,751                  \\\\\n                        & \\multicolumn{2}{c}{}                                                   &                                                                                    &                       & \\text{GPT-4V}                                                                               & 8,117                  \\\\\n                        & \\multicolumn{2}{c}{}                                                   &                                                                                    &                       & \\text{Qwen-VL-Max}                                                                             & 8,012                  \\\\ \n                        & \\multicolumn{2}{c}{}                                                   &                                                                                    &                       & \\text{LLaVA}                                                                             & 8,253                  \\\\ \n                        \\cline{4-7} \n                        & \\multicolumn{2}{c}{}                                                   & \\multirow{4}{*}{Score}                                                             & \\multirow{4}{*}{5,883} & \\text{Gemini}                                                                             & 5,337                  \\\\\n                        & \\multicolumn{2}{c}{}                                                   &                                                                                    &                       & \\text{GPT-4V}                                                                                & 5,708                  \\\\\n                        & \\multicolumn{2}{c}{}                                                   &                                                                                    &                       & \\text{Qwen-VL-Max}                                                                             & 5,701                   \\\\\n                        \n                        & \\multicolumn{2}{c}{}                                                   &                                                                                    &                       & \\text{LLaVA}                                                                             &  5,729              \\\\\n                        \\cline{2-7} \n                        & \\multirow{12}{*}{\\rotatebox{90}{w.o. Vision Input} } & \\multirow{6}{*}{\\rotatebox{90}{No Vision} } & \\multirow{2}{*}{Batch}                                                             & \\multirow{2}{*}{110}  & \\text{Gemini}                                                                             & 107                   \\\\\n                        &                                    &                                   &                                                                                    &                       & \\text{GPT-4V}                                                                                & 110                   \\\\ \\cline{4-7} \n                        &                                    &                                   & \\multirow{2}{*}{Pair}                                                              & \\multirow{2}{*}{425}  & \\text{Gemini}                                                                             & 385                   \\\\\n                        &                                    &                                   &                                                                                    &                       & \\text{GPT-4V}                                                                                & 355                   \\\\ \\cline{4-7} \n                        &                                    &                                   & \\multirow{2}{*}{Score}                                                             & \\multirow{2}{*}{612}  & \\text{Gemini}                                                                             & 582                   \\\\\n                        &                                    &                                   &                                                                                    &                       & \\text{GPT-4V}                                                                                & 584                   \\\\ \\cline{3-7} \n                        &                                    & \\multirow{6}{*}{\\rotatebox{90}{Vision Experts}}   & \\multirow{2}{*}{Batch}                                                             & \\multirow{2}{*}{110}  & \\text{Gemini}                                                                             & 107                   \\\\\n                        &                                    &                                   &                                                                                    &                       & \\text{GPT-4V}                                                                                & 110                   \\\\ \\cline{4-7} \n                        &                                    &                                   & \\multirow{2}{*}{Pair}                                                              & \\multirow{2}{*}{425}  & \\text{Gemini}                                                                             & 396                   \\\\\n                        &                                    &                                   &                                                                                    &                       & \\text{GPT-4V}                                                                                & 425                   \\\\ \\cline{4-7} \n                        &                                    &                                   & \\multirow{2}{*}{Score}                                                             & \\multirow{2}{*}{612}  & \\text{Gemini}                                                                             & 576                   \\\\\n                        &                                    &                                   &                                                                                    &                       & \\text{GPT-4V}                                                                                & 612                   \\\\ \\bottomrule[1.5pt]\n\\end{tabular}}\n  \\vspace{-15pt}\n\\end{table}\n\nFigure \\ref{fig: pipeline} shows an overview of our proposed MLLM-as-a-Judge, consisting of three steps: 1) image-instruction pair collection, 2) MLLM response collection, and 3) comparison with human annotation.\nInitially, we collect a dataset $\\mathcal{P} = \\{(M_1, I_1), \\ldots, (M_n, I_n)\\}$, containing pairs of images $(M)$ and their corresponding instructions $(I)$ sourced from 10 diverse domains (e.g., math, chart, diffusion), ensuring comprehensive coverage for a wide array of downstream tasks.\nSubsequently, each pair $(M_i, I_i)$ is processed through several MLLMs, \ngenerating a set of responses $\\mathcal{R}_i = \\{r_1, r_2, \\ldots, r_n\\}$ for each pair. This process contributes to the formation of the dataset of image-instruction-responses pairs, denoted as $\\mathcal{D} = \\{(M_i, I_i, \\mathcal{R}_i) | (M_i, I_i) \\in \\mathcal{P}\\}$.\nFinally, the dataset $\\mathcal{D}$ is partitioned into three distinct subsets to facilitate diverse task evaluations: $\\mathcal{D}_{\\text{score}}$ for \\textit{Scoring Evaluation}, $\\mathcal{D}_{\\text{pair}}$ for \\textit{Pair Comparison}, and $\\mathcal{D}_{\\text{batch}}$ for \\textit{Batch Ranking}.\nEach subset will be employed for specific judging tasks, with each of them being configured as follows.\n\\begin{itemize}[nolistsep, leftmargin=*]\n\\item \\textbf{Scoring Evaluation}: Each individual response is evaluated on a scale from 1 to 5, with the specific criteria for this rating system detailed in Appendix~\\ref{Prompt templates}.\n\\item \\textbf{Pair Comparison}: It involves a direct comparison between two responses, \nculminating in the identification of the superior one. Following the principles outlined by \\citep{deutsch2023ties}, a tie option is incorporated to ensure a more equitable assessment.\n\\item \\textbf{Batch Ranking}: \nThe responses are systematically arranged in descending order of quality based on a given instruction, without any tie option.\n\\end{itemize} \n\n\\subsection{Step 1: Image-Instruction Pair Collection}\nWe meticulously curate a dataset consisting of 4,414 image-text pairs, gathered from a variety of downstream task datasets, as detailed in Table~\\ref{Step1: Detailed Dataset} in Appendix~\\ref{Detailed Benchmark Construction}. These pairs are carefully tailored into image-instruction pairs to suit a free-form response format. To illustrate, within the domain of diffusion tasks, our dataset incorporated pairs challenging models to adeptly recognize and articulate connections between provided images and user-specified keywords.\n\n\\subsection{Step 2: MLLM Response Collection}\n\nWe employ six widely-used MLLMs – \\text{GPT-4V} \\citep{openai2023gpt4v}, \\text{Gemini} \\citep{geminiteam2023gemini}, \\text{LLaVA} \\citep{liu2023llava}, Qwen-VL-Max \\citep{Qwen-VL}, LLaVA-1.6-34b \\citep{liu2023llava}, and \\text{CogVLM} \\citep{wang2023cogvlm} – to generate responses based on the image-instruction pairs, obtaining approximately 17,000 responses. \nResponses that are either too brief or non-compliant with security regulations (e.g., \\textit{``I'm sorry, but I cannot assist with this request''}) from \\text{GPT-4V} and \\text{Gemini} are excluded. \nThe number of responses and the length distributions for different MLLMs are shown in Table~\\ref{table: comparison} and Figure~\\ref{fig:length_distribution}, respectively. \nWe show specific hyper-parameter settings in Appendix~\\ref{Detailed Benchmark Construction: Step 2}. Besides, we segment these responses into three non-overlapping groups, to prevent response overlap. \n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{figure/step2_length.pdf}\n    \\vspace{-15pt}\n    \\caption{Length distribution in responses for different MLLMs. Horizontal axis: length; Vertical axis: density.}\n    \\label{fig:length_distribution}\n    \\vspace{-10pt}\n\\end{figure}\n\n\\subsection{Step 3: Comparison with Human Annotations}\nThe annotation is conducted by 6 authors of this paper independently. These annotators are proficient in this domain, with different genders, ages, and educational backgrounds to ensure diversity~\\citep{sun2020evolution}. They are required to give objective judgments without considering answer lengths, and certain names or positions of the response to minimize human bias. More details are referred to Appendix~\\ref{Human Labeling and Agreement Collection}.\n\n\\begin{table*}[htbp]\n\\centering\n\\large\n\\renewcommand\\arraystretch{1.2}\n\\label{tab: experiment result}\n\\caption{The overall performance of different MLLMs in judging, compared with human annotations on different datasets. We sample all the data three times and took the average to mitigate the casualty. \\textit{w.} and \\textit{w.o.} tie represents tie and non-tie situations respectively. We omit Gemini's results on the diffusion task for its challenges in processing AI-generated images. \nAll presented data of Pearson similarity exhibit a $p$-value below 0.05, indicating a statistically significant level of confidence. Please refer to the Appendix~\\ref{full} for more results.\n}\n\\resizebox{1\\linewidth}{!}{\n\\begin{tabular}{ll|ccccccccccccccc}\\toprule[1.5pt]\n\\textbf{Settings} & \\textbf{MLLM}  & COCO  & C.C. & Diff. & Graphics & Math & Text & WIT & Chart & VisIT & CC-3M & M2W & SciQA & Aes & MM-Vet & Ave. \\\\ \\midrule\n\\multirow{5}{*}{\\textbf{Score ($\\uparrow$)}} & LLaVA-1.5-13b & 0.247 & 0.227 & 0.060 & 0.242 & 0.093 & 0.245 & 0.109 & 0.237 & 0.177 & 0.071 & \\textbf{0.424} & 0.279 & \\textbf{0.414} & 0.322 & 0.225 \\\\\n & LLaVA-1.6-34b & 0.285 & 0.251 & -0.012 & 0.262 & 0.238 & 0.258 & 0.151 & 0.318 & 0.198 & 0.109 & 0.022 & 0.206 & 0.025 & 0.265 & 0.184 \\\\\n & Gemini & 0.262 & 0.408 & - & 0.400 & 0.228 & 0.222 & 0.418 & 0.343 & 0.336 & 0.374 & 0.324 & 0.073 & 0.360 & 0.207 & 0.304 \\\\\n & GPT-4V & \\textbf{0.454} & \\textbf{0.507} & \\textbf{0.458} & \\textbf{0.645} & \\textbf{0.606} & \\textbf{0.624} & \\textbf{0.579} & \\textbf{0.645} & \\textbf{0.620} & \\textbf{0.431} & 0.185 & \\textbf{0.383} & 0.401 & \\textbf{0.326} & \\textbf{0.490} \\\\\n & Qwen-vl-max & 0.311 & 0.117 & 0.072 & 0.218 & 0.175 & 0.196 & 0.028 & 0.312 & 0.151 & 0.045 & 0.244 & 0.115 & 0.177 & 0.216 & 0.170 \\\\ \\midrule\n\\multirow{5}{*}{\\textbf{Pair w. Tie ($\\uparrow$)}} & LLaVA-1.5-13b & 0.273 & 0.478 & 0.286 & 0.273 & \\textbf{0.657} & 0.510 & 0.369 & 0.383 & 0.456 & 0.484 & 0.347 & 0.223 & 0.389 & 0.254 & 0.384 \\\\\n & LLaVA-1.6-34b & 0.493 & 0.600 & 0.570 & 0.300 & 0.374 & 0.551 & 0.543 & 0.254 & 0.398 & 0.392 & 0.513 & \\textbf{0.434} & 0.524 & 0.499 & 0.460 \\\\\n & Gemini & 0.616 & 0.787 & - & \\textbf{0.650} & 0.436 & 0.664 & 0.605 & 0.500 & \\textbf{0.660} & 0.560 & 0.370 & 0.262 & 0.190 & 0.312 & 0.509 \\\\\n & GPT-4V & \\textbf{0.696} & \\textbf{0.824} & \\textbf{0.847} & 0.639 & 0.564 & \\textbf{0.673} & \\textbf{0.679} & \\textbf{0.657} & 0.640 & \\textbf{0.612} & \\textbf{0.521} & 0.415 & \\textbf{0.606} & \\textbf{0.529} & \\textbf{0.636} \\\\\n & Qwen-vl-max & 0.403 & 0.464 & 0.372 & 0.494 & 0.438 & 0.500 & 0.533 & 0.479 & 0.421 & 0.421 & 0.411 & 0.392 & 0.325 & 0.474 & 0.438 \\\\ \\midrule\n\\multirow{5}{*}{\\textbf{Pair w.o. Tie ($\\uparrow$)}} & LLaVA-1.5-13b & 0.327 & 0.537 & 0.302 & 0.300 & 0.726 & 0.684 & 0.600 & 0.610 & 0.648 & 0.583 & 0.449 & 0.443 & 0.498 & 0.344 & 0.504 \\\\\n & LLaVA-1.6-34b & 0.607 & 0.824 & 0.855 & 0.402 & 0.587 & 0.750 & \\textbf{0.758} & 0.381 & 0.503 & 0.564 & \\textbf{0.712} & \\textbf{0.679} & 0.694 & \\textbf{0.762} & 0.648 \\\\\n & Gemini & 0.717 & 0.840 & - & 0.770 & 0.678 & 0.793 & 0.688 & 0.658 & 0.711 & 0.652 & 0.471 & 0.358 & 0.265 & 0.400 & 0.615 \\\\\n & GPT-4V & \\textbf{0.804} & \\textbf{0.870} & \\textbf{0.922} & \\textbf{0.807} & \\textbf{0.801} & \\textbf{0.805} & 0.734 & \\textbf{0.849} & \\textbf{0.761} & \\textbf{0.703} & 0.699 & 0.647 & \\textbf{0.755} & 0.659 & \\textbf{0.773} \\\\\n & Qwen-vl-max & 0.657 & 0.674 & 0.556 & 0.667 & 0.635 & 0.732 & 0.647 & 0.638 & 0.560 & 0.586 & 0.608 & 0.646 & 0.741 & 0.662 & 0.644 \\\\ \\midrule\n\\multirow{5}{*}{\\textbf{Batch ($\\downarrow$)}} & LLaVA-1.5-13b & 0.577 & 0.492 & 0.562 & 0.535 & 0.598 & 0.650 & 0.616 & 0.644 & 0.620 & 0.563 & 0.639 & 0.563 & 0.650 & 0.652 & 0.597 \\\\\n & LLaVA-1.6-34b & 0.449 & 0.411 & 0.500 & 0.561 & 0.575 & 0.544 & 0.483 & 0.552 & 0.542 & 0.479 & \\textbf{0.529} & 0.437 & 0.500 & 0.450 & 0.501 \\\\\n & Gemini & \\textbf{0.287} & \\textbf{0.299} & - & 0.473 & 0.462 & 0.430 & 0.344 & 0.520 & 0.426 & 0.357 & 0.613 & \\textbf{0.412} & 0.467 & 0.529 & 0.432 \\\\\n & GPT-4V & 0.318 & 0.353 & \\textbf{0.070} & \\textbf{0.385} & \\textbf{0.348} & \\textbf{0.319} & \\textbf{0.290} & \\textbf{0.347} & \\textbf{0.300} & \\textbf{0.402} & 0.597 & 0.462 & 0.453 & \\textbf{0.411} & \\textbf{0.361} \\\\\n & Qwen-vl-max & 0.477 & 0.407 & 0.500 & 0.480 & 0.507 & 0.515 & 0.493 & 0.539 & 0.468 & 0.407 & 0.563 & 0.503 & \\textbf{0.444} & 0.500 & 0.486 \\\\ \\bottomrule[1.5pt]\n\\end{tabular}}\n  \\vspace{-10pt}\n\\end{table*}\n\n\\section{Experiment Settings}\n\\subsection{Settings of MLLM-as-a-Judge}\nWe evaluate the judging performance of eleven leading MLLMs – GPT-4V~\\citep{openai2023gpt4v}, Gemini-Pro-Vision-1.0 \\citep{geminiteam2023gemini}, LLaVA-1.5-13b, LLaVA-1.6-7b/13b/34b \\citep{liu2023llava},  Qwen-VL-Plus/Max \\citep{Qwen-VL} and \\text{CogVLM} \\citep{wang2023cogvlm} – across three distinct evaluation settings. Adapting the ``Analyze-then-Judge'' paradigm from \\citet{chiang2023closer}, which is a one-step CoT approach \\citep{wei2022chain}, we first ask MLLMs to analyze responses and then provide a judgment based on their analysis. However, due to capability limitations to perform the ``Analyze-then-Judge'' setting for \\text{LLaVA} and \\text{CogVLM}, we prompt them to directly output their judgment. We also evaluate whether multi-step CoT will enhance the performance of MLLM serving as a judge.\n\nFurthermore, to extensively explore MLLMs judging capabilities, we conduct experiments on various settings, including scenarios without vision input, replacing vision input with a detailed description generated by \\text{GPT-4V} as a vision expert, and employing multi-step CoT. \nConsidering that the first two settings do not involve image inputs, we also include tests on the latest \\text{GPT-4} \\citep{openai2023gpt4} \\text{Gemini} \\citep{geminiteam2023gemini}, \\text{LLaMA-2-70b} \\citep{touvron2023llama}, and \\text{Mixtral-8x7b} \\citep{jiang2024mixtral} to assess whether LLMs can effectively perform judging tasks without vision perception. Comprehensive details of these experimental setups are available in Appendix \\ref{Detailed Experiment Settings}, and the prompts can be found in Appendix \\ref{Prompt templates}.\n\n\\subsection{Judging Metrics}\nAfter collecting responses from MLLM judgments, we quantify their alignment with human annotations across three settings, employing distinct metrics as follows:\n\n    $\\triangleright$ \\textbf{Scoring Evaluation:} Following \\text{LLM-as-a-Judge} \\citep{zheng2023judging}, we compute the Pearson similarity \\citep{lee1988thirteen} between the MLLMs' judgments and human ratings across different sub-datasets.\n    \n    $\\triangleright$ \\textbf{Pair Comparison:} We assess the similarity between the MLLM judgments and human decisions using accuracy, F1-score \\citep{goutte2005probabilistic}, and recall \\citep{goutte2005probabilistic} to assess the judging abilities of models.\n    \n    $\\triangleright$ \\textbf{Batch Evaluation:} We consolidate the ranking results into a singular sequence and employ the Normalized Levenshtein distance \\citep{levenshtein1966binary} to evaluate the similarity between judgments from MLLMs and human annotation.\n\n \\subsection{Human Agreement in MLLM Judgment}\nApart from traditional metrics for similarity assessment between judgments from MLLMs and humans, we further evaluate the judgments provided by MLLMs to uncover latent bias and hallucination in 10 datasets. We also invite human annotators for further validation, focusing on the following aspects:\n\n    $\\triangleright$ \\textbf{Human Agreement:} This involves a simple `yes' or `no' response to assess agreement with the MLLM judgments. While some judgments might appear reasonable, they may still be considered incorrect due to unique human perspectives. Hence, we conduct experiments on human agreement to address situations that traditional metrics may not adequately capture.\n    \n    $\\triangleright$ \\textbf{Analysis Grading:} Each MLLM analysis is assigned a score from 1 to 5, considering relevance, accuracy, creativity, and response granularity, detailed in Appendix~\\ref{Prompt templates}.\n    \n    $\\triangleright$ \\textbf{Hallucination Detection:} Given the propensity for hallucination issues in the complex reasoning chains and long-term vision-language contexts of MLLMs, we task human annotators with identifying any hallucinations in the analyses of MLLM judgments, adhering to established definitions of vision and language hallucination \\citep{sun2024trustllm}.\n    \n\\section{Empirical Results and Analysis}\n\n\\begin{table*}[ht]\n\\centering\n\n\\caption{Human agreement percentage on MLLM-as-a-Judge in 10 datasets. Each judgment is independently reviewed three times by different annotators and consensus results are recorded. \\text{Gemini} failed in diffusion tasks and its results are omitted.}\n\\setlength{\\tabcolsep}{6pt} \n\\renewcommand\\arraystretch{1.2}\n\\label{tab: human agreement}\n\\resizebox{0.98\\linewidth}{!}{\n\\begin{tabular}{l l|c c c c c c c c c c c}  \n\\toprule[1.5pt]\n\\textbf{Settings} & \\textbf{MLLM}  & COCO  & C.C. & Diffusion & Graphics & Math & Text & WIT & Chart & VisIT & CC-3M & Average\\\\\n\\midrule\n\\multirow{2}{*}{\\textbf{{Score ($\\uparrow$)}}}\n                       & \\text{Gemini} & 0.783 & \\textbf{0.739} & - & 0.618 & 0.536 & 0.621 & \\textbf{0.749} & 0.630 & 0.712 & 0.702 & 0.677\\\\\n                        & \\text{GPT-4V}& \\textbf{0.799} & 0.725 & \\textbf{0.506} & \\textbf{0.688} & \\textbf{0.638} & \\textbf{0.706} & 0.714 & \\textbf{0.676} & \\textbf{0.779} & \\textbf{0.754} & \\textbf{0.699}\\\\ \\midrule\n\\multirow{2}{*}{\\textbf{{Pair ($\\uparrow$)}}} \n                      & \\text{Gemini} &0.705 & 0.833 & - & 0.733 & 0.520 & 0.717 & \\textbf{0.827} & 0.620 & \\textbf{0.853} & 0.703 & 0.724\\\\\n                      & \\text{GPT-4V} & \\textbf{0.821} & \\textbf{0.926} & \\textbf{0.873} & \\textbf{0.794} & \\textbf{0.618} & \\textbf{0.752} & 0.790 & \\textbf{0.796} & 0.797 & \\textbf{0.766} & \\textbf{0.793}\\\\\n\\midrule\n\\multirow{2}{*}{\\textbf{{Batch ($\\downarrow$)}}} \n                       & \\text{Gemini} & 0.642 & \\textbf{0.639} & - & 0.333 & 0.330 & 0.473 & 0.511 & 0.315 & 0.422 & \\textbf{0.554} & 0.469 \\\\\n                       & \\text{GPT-4V} & \\textbf{0.663} & \\textbf{0.639} & \\textbf{0.912} & \\textbf{0.536} & \\textbf{0.475} & \\textbf{0.615} & \\textbf{0.641} & \\textbf{0.640} & \\textbf{0.622} & 0.467 & \\textbf{0.621} \\\\\n\\bottomrule[1.5pt]\n\\end{tabular}}\n\\vspace{-10pt}\n\\end{table*}\n\n\\begin{figure*}[h]\n    \\centering\n    \\vspace{-2pt}\n    \\includegraphics[width=\\linewidth]{figure/combined.pdf}\n     \\vspace{-17pt}\n    \\caption{\\textit{Pair Comparison} density (Left) and \\textit{Scoring Evaluation} density (Right) of different MLLMs judgments and human annotations.}\n    \\label{fig: score and pair result}\n     \\vspace{-15pt}\n\\end{figure*}\n\n\\subsection{MLLM Judgment \\textit{vs} Human Annotation}\nAs shown in Figure~\\ref{fig: Radar Figure} and Table \\ref{tab: human agreement}, judgments made by \\text{GPT-4V} are closer to human annotations among all settings, while \\text{Gemini} is far different, with \\text{LLaVA}, CogVLM and Qwen-VL-Max are even worse. Overall, MLLM judgments perform better on \\textit{Pair Comparison}, while falling short in \\textit{Scoring Evaluation} and \\textit{Batch Ranking}, showing a huge gap between the model and human preferences. Under the ``Analyze-then-Judge'' setting, \\text{GPT-4V} prefers to give a longer judge in all settings, convincing its ability to reason on long-term text.\n\n    $\\triangleright$ \\textbf{Scoring Evaluation:} \\text{GPT-4V} demonstrates the highest similarity to human scoring with a similarity score of 0.490. In contrast, \\text{Gemini} achieves only 0.304, with \\text{LLaVA} and \\text{CogVLM} scoring even lower. This discrepancy is mainly due to \\text{Gemini}'s tendency to assign scores around 4 points as depicted in Figure \\ref{fig: score and pair result}, seldom giving 1 or 2 points. \\text{LLaVA} and \\text{CogVLM} show a pattern similar to \\text{Gemini}, predominantly assigning scores around 4 points. We attribute this to a `High-Score' Bias, akin to the `Yes/No' bias identified by \\citet{liu2023hallusionbench}, which may result from an imbalance in positive and negative judging instructions in their training data \\citep{liu2023aligning}, severely limits their ability to provide just and varied scores in scoring settings. In comparison, \\text{GPT-4V}'s scores are more evenly distributed and align closely with human preferences.  \n    \n    $\\triangleright$ \\textbf{Pair Comparison:} As illustrated in Figure~\\ref{fig: score and pair result}, \\text{GPT-4V} outshines other MLLMs in pair comparison tasks, achieving 0.636 in tie settings and 0.773 in non-tie settings, surpassing 0.8 in many datasets, which indicate a strong alignment with human preferences. \\text{Gemini}, \\text{LLaVA}, and \\text{CogVLM} show a marked preference for declaring a clear winner, possibly due to a lack of tie situations in their training, leading to biased judgments. It's also interesting that the frequency of ties given by \\text{GPT-4V} closely mirrors that of human judges, suggesting similar thresholds for tie decisions.\n    \n    $\\triangleright$ \\textbf{Batch Ranking:} \\text{GPT-4V} aligns more closely with human ranking results, indicating a significant lead with a mean Levenshtein Distance of 0.361. However, there is still substantial room for improvement in this task for all MLLMs. Notably, \\text{CogVLM} is unable to provide a full ranking in this context, offering only the top choice; so it was excluded from this comparison; \\text{LLaVA} also exhibits position bias influenced by prompt structure, often replicating judgments seen in example prompts, which complicates its ability to produce fair judgments.\n\n\\begin{table}[t]\n\\centering\n\\large\n\\caption{Consistency comparisons of \\text{GPT-4V} and \\text{Gemini} in 10 datasets. Average means weighted average for consistency times, ``MCC'' stands for ``Majority Consistency Criterion'', which deems responses consistent if over half of them are identical across our 6 repetitions of experiments.}\n\\renewcommand\\arraystretch{1.2}\n\\resizebox{1\\linewidth}{!}{\n\\begin{tabular}{l|c c|c c|c c}\n\\toprule[1.5pt]\n\\multirow{2}{*}{MLLM}& \\multicolumn{2}{c|}{\\textbf{Score}}  & \\multicolumn{2}{c|}{\\textbf{Pair}} & \\multicolumn{2}{c}{\\textbf{Batch}} \\\\\n                           & Average & MCC & Average & MCC & Average & MCC \\\\\n\\midrule\n\\text{Gemini}            & 0.531 & 0.054 & 0.781 & 0.547 & 0.629 & 0.338 \\\\\n\\text{GPT-4V}           & \\textbf{0.796} & \\textbf{0.611} & \\textbf{0.836} & \\textbf{0.675} & \\textbf{0.679} & \\textbf{0.418}  \\\\\n\\bottomrule[1.5pt]\n\\end{tabular}}\n  \\vspace{-15pt}\n\\label{tab: consistency}\n\n\\end{table}\n\n\\subsection{MLLM Judging Consistency}\nTo be a reliable judge, consistent decision-making across repeated evaluations of the same query is crucial. For this purpose, we conduct six repeated tests with MLLM judgments and calculated the weighted average consistency scores and Majority Consistency Criterion ratios for \\text{GPT-4V} and \\text{Gemini}, as shown in Table~\\ref{tab: consistency} and Figure~\\ref{fig: consitency_bar}. Despite a higher temperature setting, \\text{GPT-4V} substantially outperforms \\text{Gemini} across all tasks. Particularly in \\textit{Pair Comparison}, \\text{GPT-4V} achieves a higher consistency score of 0.675, but it encounters difficulties in maintaining similar levels of consistency in \\textit{Scoring} and \\textit{Batch Ranking} tasks, with scores dropping to 0.611 and 0.418, indicating the challenge of producing qualified and convincing judgments. \n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figure/consistency.pdf}\n    \\vspace{-20pt}\n    \\caption{\n    Consistency checking on 6 repetitions of experiments on \\text{GPT-4V} (Left) and \\text{Gemini} (Right). \\text{GPT-4V} outperforms \\text{Gemini} with a relatively higher ratio for high consistency.\n    }\n     \\vspace{-15pt}\n    \\label{fig: consitency_bar}\n\\end{figure}\n\n\\begin{table*}[ht]\n\\centering\n\n\\large\n\\renewcommand\\arraystretch{1.05}\n\\centering\n\\caption{Results of GPT-4V and Gemini-Pro acting as a judge with a 3-step CoT approach in a selected subset. \n}\n\\resizebox{0.98\\linewidth}{!}{\n\\begin{tabular}{ll|ccccccccccc}\\toprule[1.5pt]\n\\textbf{Settings} & \\textbf{MLLM} & COCO & C.C. & Diffusion & Graphics & Math & Text & WIT & Chart & VisIT & CC-3M & Ave. \\\\ \\midrule\n\\multirow{4}{*}{\\textbf{Score ($\\uparrow$)}} & GPT-4V & \\textbf{0.454} & \\textbf{0.507} & \\textbf{0.458} & \\textbf{0.645} & \\textbf{0.606} & \\textbf{0.624} & \\textbf{0.579} & \\textbf{0.645} & \\textbf{0.620} & \\textbf{0.431} & \\textbf{0.557} \\\\\n & GPT-4V (+CoT) & 0.246 & 0.165 & 0.192 & 0.385 & 0.397 & 0.400 & 0.298 & 0.443 & 0.423 & 0.038 & 0.299 \\\\\n & Gemini & 0.262 & 0.408 & - & 0.400 & 0.228 & 0.222 & 0.418 & 0.343 & 0.336 & 0.374 & 0.299 \\\\\n & Gemini (+CoT) & 0.127 & 0.068 & 0.117 & 0.220 & 0.132 & 0.182 & 0.105 & 0.140 & 0.222 & 0.128 & 0.144 \\\\ \\midrule\n\\multirow{4}{*}{\\textbf{Pair w. Tie ($\\uparrow$)}} & GPT-4V & \\textbf{0.696} & \\textbf{0.824} & \\textbf{0.847} & \\textbf{0.639} & \\textbf{0.564} & \\textbf{0.673} & \\textbf{0.679} & \\textbf{0.657} & 0.640 & \\textbf{0.612} & \\textbf{0.683} \\\\\n & GPT-4V (+CoT) & 0.507 & 0.657 & 0.561 & 0.601 & 0.515 & 0.580 & 0.489 & 0.521 & \\textbf{0.646} & 0.553 & 0.563 \\\\\n & Gemini & 0.616 & 0.787 & - & 0.650 & 0.436 & 0.664 & 0.605 & 0.500 & 0.660 & 0.560 & 0.609 \\\\\n & Gemini (+CoT) & 0.233 & 0.239 & 0.420 & 0.207 & 0.284 & 0.329 & 0.352 & 0.357 & 0.247 & 0.239 & 0.291 \\\\ \\midrule\n\\multirow{4}{*}{\\textbf{Pair w.o. Tie ($\\uparrow$)}} & GPT-4V & \\textbf{0.804} & \\textbf{0.870} & \\textbf{0.922} & \\textbf{0.807} & \\textbf{0.801} & \\textbf{0.805} & \\textbf{0.734} & \\textbf{0.849} & \\textbf{0.761} & \\textbf{0.703} & \\textbf{0.806} \\\\\n & GPT-4V (+CoT) & 0.673 & 0.821 & 0.845 & 0.707 & 0.738 & 0.787 & 0.548 & 0.756 & 0.753 & 0.654 & 0.728 \\\\\n & Gemini & 0.717 & 0.840 & - & 0.770 & 0.678 & 0.793 & 0.688 & 0.658 & 0.711 & 0.652 & 0.723 \\\\\n & Gemini (+CoT) & 0.267 & 0.275 & 0.573 & 0.264 & 0.414 & 0.424 & 0.427 & 0.511 & 0.299 & 0.319 & 0.377 \\\\ \\midrule\n\\multirow{4}{*}{\\textbf{Batch ($\\downarrow$)}} & GPT-4V & 0.323 & 0.344 & \\textbf{0.092} & \\textbf{0.401} & \\textbf{0.367} & \\textbf{0.341} & \\textbf{0.302} & \\textbf{0.364} & \\textbf{0.313} & 0.407 & \\textbf{0.325} \\\\\n & GPT-4V (+CoT) & 0.428 & 0.416 & - & 0.427 & 0.434 & 0.401 & 0.366 & 0.406 & 0.422 & 0.472 & 0.419 \\\\\n & Gemini & \\textbf{0.287} & \\textbf{0.299} & - & 0.473 & 0.462 & 0.430 & 0.344 & 0.520 & 0.426 & \\textbf{0.357} & 0.400 \\\\\n & Gemini (+CoT) & 0.441 & 0.481 & 0.542 & 0.595 & 0.494 & 0.533 & 0.483 & 0.569 & 0.486 & 0.463 & 0.509 \\\\\n \\bottomrule[1.5pt]\n\\end{tabular}}\n  \\vspace{-10pt}\n  \\label{tab: COT result}\n\\end{table*}\n\n\\subsection{Human Agreement}\nOur manual evaluation of MLLMs on agreement and scoring, revealed notable findings. Table \\ref{tab: human agreement} shows that GPT-4V achieved around 70\\% human agreement across all settings, excelling in the \\textit{Pair Comparison} task with 79.3\\% agreement. Specifically, GPT-4V reached 78\\% in human agreement for \\textit{Pair Comparison}, with Gemini close at 72\\%, indicating strong performance in most sample pairs and supporting the idea that large models excel in pairwise distinctions \\citep{zheng2023judging}, though improvements are needed in other judging settings.\n\nIn \\textit{Scoring Evaluation}, GPT-4V achieves a 70\\% human agreement rate, peaking at 79.9\\% in MS-COCO, while Gemini averaged 67.7\\%. To assess the consistency of MLLM judging quality across multiple responses to a single image-instruction pair, we use Mean Absolute Deviation (MAD) metric to measure the average absolute variance between individual scores and the mean. \nFigure~\\ref{fig:MAD} shows that GPT-4V exhibits lower variation in quality assessments, indicating more consistent and reliable judgment compared to Gemini. However, in \\textit{Batch Ranking}, both models exhibited decreased alignment with human judgments, especially in Maths and graphic information processing, suggesting that models may lack the capabilities to fully comprehend user instructions, leading to less reliable judgments.\n\n\\subsection{Multi-steps CoT Do Not Enhance Performance}\nWe have conducted additional tests using GPT-4V and Gemini with a 3-step CoT approach for judging, as detailed in Table~\\ref{tab: COT result}. Our analysis reveals that while employing CoT with additional steps markedly reduces hallucinations in judgments, it does not align more closely with human preferences. On numerous datasets, this approach even diminishes judging performance. Specifically, Gemini's effectiveness drops more drastically. With 3-step CoT, there is an increased likelihood that the judgment will be disturbed by its understanding of the figure and its own responses to the instruction, thereby undermining its final judgment if hallucinations exist in the previous chain.\n\n\\subsection{Vision Perception Benefits MLLM Judging}\nWe explore the feasibility of using LLMs for judging text-based responses without directly analyzing the original images. This involves two approaches: omitting vision information entirely and providing a detailed description of the picture. We choose LLaMA-70b, Mixtral8x7b-v0.1 and GPT-3.5 to provide descriptions. \nSurprisingly, as illustrated in Table~\\ref{tab: ablation study on llm}, we find that LLMs' performance in multimodal judging tasks significantly improve with picture descriptions, achieving a Pearson similarity of 0.435 in \\textit{Scoring Evaluation} tasks, markedly outperformed judgments made without any vision perception. Notably, in no-tie \\textit{Pair Comparison}, MLLMs with detailed vision descriptions even exceed the standard performance of MLLMs in judging. This suggests that MLLMs may lack certain human-like judging capabilities, while LLMs can be potential judges for multimodal tasks when provided with comprehensive task-related descriptions.\n\n\\begin{table}[t]\n\\vspace{-0.5em}\n\\renewcommand\\arraystretch{1.1}\n\\caption{How vision perception significantly enhances multimodal judging performance in traditional LLM-as-a-Judge setting, slightly outperforming MLLMs in judging. Vision Exp. stands for judging with a detailed image description. }\n\\label{tab: ablation study on llm}\n\\resizebox{1\\linewidth}{!}{\n\\begin{tabular}{llc|cc|c}\n\\toprule[1.5pt]\n\\multirow{2}{*}{\\textbf{MLLM}} & \\multirow{2}{*}{\\textbf{Settings}} & \\textbf{Score ($\\uparrow$)} & \\multicolumn{2}{c|}{\\textbf{Pair ($\\uparrow$)}} & \\textbf{Batch ($\\downarrow$)} \\\\\n &  & Pearson & w. Tie & w.o. Tie & Edit Dis. \\\\ \\midrule\n\\multirow{2}{*}{\\textbf{LLaMA2-70b}} & Vision Exp & 0.060 & 0.404 & 0.550 & 0.643 \\\\\n & No Vision & 0.126 & 0.374 & 0.537 & 0.583 \\\\ \\midrule\n\\multirow{2}{*}{\\textbf{Mixtral-8x7b}} & Vision Exp & 0.054 & 0.374 & 0.543 & 0.603 \\\\\n & No Vision & 0.151 & 0.478 & 0.731 & 0.546 \\\\ \\midrule\n\\multirow{2}{*}{\\textbf{GPT-3.5}} & Vision Exp & 0.154 & 0.453 & 0.591 & 0.473 \\\\\n & No Vision & 0.223 & 0.459 & 0.644 & 0.504 \\\\ \\midrule\n\\multirow{2}{*}{\\textbf{GPT-4V}} & Vision Exp & \\textbf{0.435} & \\textbf{0.544} & \\textbf{0.878} & 0.400 \\\\\n & No Vision & 0.299 & 0.491 & 0.868 & \\textbf{0.394} \\\\ \\midrule\n\\multirow{2}{*}{\\textbf{Gemini}} & Vision Exp & 0.120 & 0.438 & 0.785 & 0.472 \\\\\n & No Vision & 0.108 & 0.433 & 0.758 & 0.470 \\\\ \\bottomrule[2pt]\n\\end{tabular}}\n  \\vspace{-20pt}\n\\end{table}\n\n\\subsection{Bias and Hallucination}\n\\begin{figure*}[h]\n    \\centering\n    \\vspace{-3pt}\n    \\includegraphics[width=\\linewidth]{figure/length_bias_score_pair.pdf}\n    \\vspace{-10pt}\n    \\caption{Length bias in 10 datasets. The horizontal axis represents length, and the vertical axis represents density.}\n    \\vspace{-10pt}\n    \\label{fig: length_bias}\n\\end{figure*}\n\\begin{figure*}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figure/Verbosity_Bias_of_Judge_LMMs.pdf}\n      \\vspace{-15pt}\n    \\caption{Length Bias in Different MLLM judgments.}\n      \\vspace{-10pt}\n    \\label{fig: Verbosity Bias}\n\\end{figure*}\n    \\paragraph{Egocentric Bias.}\n    Models tend to assign higher scores to their own responses while scoring others lower \\citep{zheng2023judging, li2024leveraging}. In Figures~\\ref{fig: Ego Bias} and \\ref{fig: Ego Bias on pair}, \\text{GPT-4V} exhibits a slight degree of Egocentricity. Conversely, Gemini maintains a uniform scoring distribution across different sources, demonstrating a more equitable approach to judgment. In contrast, GPT-4V shows self-preference, aligning its judgments with its predefined ethical guidelines. For example, GPT-4V consistently emphasizes privacy preservation, leading to higher scores for privacy-related questions based on its own metrics. Despite efforts in prompt engineering to ensure neutrality, these models still rely on judgment criteria set during post-alignment training \\citep{ouyang2022training}. This bias can result in judgments that deviate from human preferences, highlighting the complexity of aligning MLLM judgments with humans'.\n    \n    \\paragraph{Position Bias.}\n    Model consistently favor answers in specific positions, often influenced by training data that typically places correct responses at the beginning or end of prompts \\citep{liu2023lost}. Figure \\ref{fig: score and pair result} illustrates bias in LLaVA and CogVLM during Pair Comparison tasks, where they consistently prefer answers in a specific position. This bias likely arises from their limited ability to follow complex instructions, leading them to be influenced by prompt structure. For example, if a \\textit{Batch Ranking} prompt includes a sequence like `ABCD’, LLaVA replicates this sequence in 88.2\\% of responses, significantly more than other sequences. However, this bias can be reduced by introducing multiple examples, suggesting that prompts with more examples can better direct these models to follow instructions accurately.\n    \n    \\paragraph{Length Bias.}\n    Models tend to prefer longer answers over concise but correct ones \\citep{li2024leveraging}, also known as verbosity bias \\citep{zheng2023judging}. Figure~\\ref{fig: length_bias} shows that both GPT-4V and Gemini assign higher scores to longer content. We conducted an expanded scoring experiment using GPT-4 \\citep{openai2023gpt4} without vision, increasing the semantic length of answers without changing their original intent. In Figure~\\ref{fig: Verbosity Bias}, we observe noticeable score increases, with GPT-4V and Gemini showing average gains of 0.6 and 0.75 points, respectively. These results suggest that MLLMs may favor longer text for higher scores.\n\n\\noindent\\textbf{Hallucination Detection and Mitigation.}\nWe observe a higher frequency of hallucinations in \\textit{Batch Ranking}, compared to \\textit{Pair Comparison} and \\textit{Scoring Evaluation}. These hallucinations involved significant misinterpretations and retrieval errors, impacting judgment accuracy and reliability. To address this, we employed a multi-step CoT approach on \\textsc{MLLM-as-a-Judge-Hard}, adding reasoning steps before the conventional ``Analyze-then-Judge'' process. This enhanced procedure included: 1) image-instruction, 2) image, and 3) instruction. In Table~\\ref{tab: COT score}, this strategy effectively reduced hallucinations across all formats, with significant improvements in tasks involving image-related information. In the \\textit{Batch Ranking} task, which requires handling longer text sequences, the detailed reasoning steps were particularly effective in reducing hallucinations.\n\n\\subsection{Scaling Law for MLLM-as-a-Judge}\nWe conduct two sets of experiments with models of different sizes, the LLaVA-1.6 series models and the Qwen series models in four newly added datasets, illustrated in Figure \\ref{fig: rebuttal figure1} and \\ref{fig: rebuttal figure2}. In \\textit{Score evaluation}, LLaVA-1.6-34b and Qwen-VL-Max slightly outperform others in Math, Chart, and Text tasks, showing a relatively strong scaling law. \n\n\\begin{table}[]\n\\vspace{-0.5em}\n\\centering\n\\renewcommand\\arraystretch{1.1}\n\\caption{\nReduction of hallucinations in \\textsc{MLLM-as-a-Judge-Hard} through additional CoT steps compared to normal setting.\n}\n\\label{tab: COT score}\n\\resizebox{0.9\\linewidth}{!}{\n\\begin{tabular}{cccc}\n\\toprule[1.5pt]\n\\textbf{Setting} & \\begin{tabular}[c]{@{}c@{}}Figure-\\\\ instruction\\end{tabular} & Figure & Instruction \\\\ \\midrule\n\\textbf{Score} & 46.15\\%                                                                 & \\textbf{48.72\\%}  & 33.33\\%        \\\\\n\\textbf{Pair}  & 28.21\\%                                                                 & \\textbf{35.90\\%}   & 33.33\\%        \\\\\n\\textbf{Batch} & \\textbf{43.59\\%}                                                                 & 35.90\\%   & 35.90\\%        \\\\ \\bottomrule[1.5pt]\n\\end{tabular}}\n\\vspace{-16pt}\n\\end{table}\n\n\\section{Related Work}\n\\paragraph{LLM as a Judge.}\nThe evolution of LLMs has made them increasingly effective evaluators in Natural Language Processing (NLP) tasks. \\citet{zhu2023judgelm} introduced JudgeLM for LLM evaluation, followed by AUTO-J \\citep{li2023generative}, aligning closely with human judgment \\citep{bai2023touchstone, li2023alpacaeval, kim2023prometheus}. Advancements in CoT reasoning \\citep{wei2022chain, chu2023survey} and training-free instruction following \\citep{brown2020language, wei2021finetuned} further extend LLMs' judging capability in diverse tasks like translation quality assessment \\citep{kocmi2023large} and story generation \\citep{chiang2023can}.\n\n\\paragraph{Hallucination and Bias in Judgments.}\nMLLMs suffer from vision and language hallucinations \\citep{ji2023survey, huang2023survey, cui2023holistic, wang2023evaluation}, often due to vision-language misalignments in training phase~\\citep{sun2024trustllm, huang2023trustgpt}. Recent research focuses on hallucination evaluation \\citep{liu2023hallusionbench}, detection \\citep{li2023evaluating, wang2023evaluation}, and mitigation \\citep{yin2023woodpecker, gunjal2023detecting, zhou2023analyzing}, noting that even GPT-4V suffer from these issues \\citep{shi2023exploring, liu2023hallusionbench, cui2023holistic}. Besides, biases in MLLM-as-a-Judge, similar to those in human decision-making \\citep{blunch1984position, raghubir2006center} and other ML domains \\citep{wang2018position, liu2023lost}, such as position \\citep{zheng2023large}, egocentric \\citep{li2024leveraging}, and verbosity biases \\citep{saito2023verbosity}, are compounded by the integration of visual perception, necessitating further investigation.\n\n\\section{Future Directions}\n\\paragraph{Multimodal RLHF/DPO.}\nOur work is highly connected with multimodal RLHF/DPO \\citep{sun2023aligning, li2023silkie, yu2023rlhf}. Our dataset includes extensive human annotations, such as manually assigned scores and preference on pairs, which could serve as invaluable training material for RLHF reward models and supply paired data essential for DPO \\citep{rafailov2024direct, zhang2024direct}, paving the way for enhancing the training of MLLMs.\n\n\\paragraph{Exploring the upper bound of MLLM-as-a-Judge.}\nBeyond expanding the steps in the Chain of Thought prompting \\citep{wei2022chain}, we see significant potential in more sophisticated reasoning frameworks, such as multi-agent debating \\citep{chan2023chateval} when MLLM acts as a Judge, which could enhance the judging accuracy through improved reasoning capabilities. Additionally, addressing inherent biases in the model during the judgment process is crucial. For instance, position bias in \\textit{Pair Comparison} and \\textit{Batch Ranking} \\citep{zheng2023large, wang2024my}, and the tendency to assign higher scores, as discussed in \\citep{lee2024prometheus}, are critical areas for improvement.\n\nIncorporating a human-in-the-loop approach \\citep{wang2023large} offers a promising solution to enhance judgment consistency and reliability. For example, if judgment results vary in more than half of several repeated judgments, it may need human intervention for consistency checking. When it's challenging to discern the MLLM's judgment due to non-compliance with the suggested output format or lack of a clear outcome, human intervention may be required to refine this process by manually verifying judgments. \n\n\\section{Conclusion}\nIn this paper, we have presented a new benchmark, termed {MLLM-as-a-Judge}, to assess the judging capabilities of MLLMs across three critical evaluation settings in the multimodal domain: \\textit{Scoring Evaluation}, \\textit{Pair Comparison}, and \\textit{Batch Ranking}. We further evaluate their agreement with humans. Our results reveal that advanced MLLMs can win significant human recognition in \\textit{Pair Comparisons}, but perform poorly in \\textit{Scoring Evaluation} and \\textit{Batch Ranking} tasks. Our work highlights potential areas for future refinement and improvement of MLLMs.\nWe advocate for additional efforts dedicated to supporting the continuous development of MLLMs as judges.\n\n\\section*{Impact Statement}\nIn this paper, we introduce a novel benchmark, termed \\text{MLLM-as-a-Judge}, designed to propel the evolution of MLLMs toward achieving judgments that align more closely with human perspectives. This benchmark establishes a heightened criterion for assessing MLLMs, emphasizing their proficiency in comprehending and processing information in a manner reflective of human cognitive processes.  One limitation of our work lies in the bias in human annotation and MLLMs. We leave the exploration of more objectives, ethically principled, and socially beneficial MLLM-as-a-Judge systems as our future work.\n\n\\newpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2210.03493v1.tex",
        "arXiv-2401.08967v3.tex",
        "arXiv-2402.04788v3.tex"
    ],
    "group_id": "group_26",
    "response": "### Summary of Research Papers on Large Language Models and Multimodal Judging\n\n#### Title: Enhancing Large Language Models and Multimodal Judging Capabilities\n\n#### Introduction\nThe field of large language models (LLMs) has seen significant advancements in recent years, with models like GPT-3 and its successors demonstrating impressive reasoning and generation capabilities. These models can perform complex reasoning tasks, generate coherent chains of thought (CoT), and even solve intricate problems such as math questions by decomposing them into intermediate steps. However, despite their remarkable performance, LLMs still face challenges in generalization and consistency, particularly when dealing with tasks that require nuanced understanding and evaluation across multiple modalities. The introduction of multimodal LLMs (MLLMs) has further expanded the scope of these models, enabling them to process and generate content in various forms, including text, images, and videos. This paper aims to summarize three recent studies that explore different aspects of enhancing LLMs and MLLMs. The first paper, \"Automatic Chain of Thought Prompting in Large Language Models,\" focuses on automating the process of constructing demonstrations for reasoning tasks. The second paper, \"ReFT: Reasoning with Reinforced Fine-Tuning,\" introduces a method to enhance the reasoning capabilities of LLMs through reinforced fine-tuning. The third paper, \"MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark,\" evaluates the judgment capabilities of MLLMs across various tasks and highlights the challenges and potential improvements in their performance.\n\nThe history of LLMs dates back to the early 2010s, with models like BERT and GPT-1 laying the groundwork for more sophisticated language understanding and generation. Since then, the field has rapidly evolved, with models like GPT-3 and LLaMA showcasing unprecedented capabilities in handling complex reasoning tasks. However, these models often require manual demonstrations or fine-tuning to achieve optimal performance, which can be labor-intensive and limit their generalization. The recent advent of MLLMs, such as GPT-4V and LLaVA, has further expanded their capabilities, but assessing their performance in multimodal tasks remains a challenge due to the lack of comprehensive benchmarks.\n\nCurrent progress in LLMs and MLLMs includes the development of chain-of-thought (CoT) prompting techniques, which enable models to generate intermediate reasoning steps before arriving at a final answer. These techniques have been shown to improve the reasoning capabilities of LLMs, but they still require manual demonstrations, which can be time-consuming and limit the model's adaptability to new tasks. Additionally, the evaluation of MLLMs in multimodal tasks is still in its nascent stages, with few benchmarks available to assess their performance comprehensively. This paper highlights recent advancements in automating CoT prompting, enhancing reasoning through reinforced fine-tuning, and evaluating MLLMs in multimodal judging tasks.\n\nThe challenges faced by LLMs and MLLMs include the need for extensive manual effort to construct demonstrations for reasoning tasks, the susceptibility to reward hacking during fine-tuning, and the presence of biases and hallucinations in their judgments. These challenges necessitate innovative solutions to improve the models' performance and reliability, making the research in this area highly relevant and impactful.\n\n#### Main Content of Each Paper\n\n**Paper 1: Automatic Chain of Thought Prompting in Large Language Models**\n\nThe first paper introduces an automated method for constructing chain-of-thought (CoT) demonstrations in large language models (LLMs), termed Auto-CoT. The authors argue that the superior performance of CoT prompting, especially in the few-shot paradigm, relies heavily on the hand-crafted design of demonstrations. This process is both time-consuming and task-specific, making it difficult to apply to new datasets without significant human effort. Auto-CoT aims to eliminate this manual design by leveraging LLMs to generate reasoning chains for demonstrations automatically.\n\nThe method consists of two main stages: question clustering and demonstration sampling. In the clustering stage, questions from a given dataset are partitioned into clusters based on their semantic similarity. Each cluster is then sampled to select representative questions, which are used to generate reasoning chains using the \"Let's think step by step\" prompt. The generated chains are evaluated for correctness and diversity, with the goal of mitigating the impact of mistakes in the reasoning process. The authors use GPT-3 (text-davinci-002) for their experiments and evaluate Auto-CoT on ten benchmark datasets, including arithmetic, commonsense, and symbolic reasoning tasks.\n\nThe results show that Auto-CoT consistently matches or exceeds the performance of the manual CoT prompting method, indicating that LLMs can perform CoT reasoning effectively by automatically constructing demonstrations. The authors also explore the effectiveness of Auto-CoT in a streaming setting, where a small batch of test questions arrives at a time, and find that it performs comparably with manual CoT prompting, demonstrating its robustness and adaptability.\n\n**Paper 2: ReFT: Reasoning with Reinforced Fine-Tuning**\n\nThe second paper presents a novel approach called Reinforced Fine-Tuning (ReFT) to enhance the reasoning capabilities of LLMs, particularly in solving math problems. ReFT builds upon the concept of Supervised Fine-Tuning (SFT) by incorporating on-line reinforcement learning, specifically the Proximal Policy Optimization (PPO) algorithm, to further refine the model's performance. The authors argue that SFT, which relies on a single CoT annotation for each question, may not provide sufficient generalization ability. ReFT addresses this by sampling multiple CoT reasoning paths during training and using the ground-truth answers to derive rewards.\n\nThe method involves two stages: a warm-up stage where the model is fine-tuned using SFT, and a reinforcement learning stage where the model learns from multiple CoT paths generated on-the-fly. The authors conduct extensive experiments on three math problem-solving datasets (GSM8K, MathQA, and SVAMP) using two foundation models (Galactica-6.7B and CodeLLAMA-7B). They find that ReFT significantly outperforms SFT in terms of accuracy and generalization, without requiring additional training data. The authors also explore the impact of majority voting and reward model reranking on ReFT, demonstrating that these techniques can further enhance its performance.\n\n**Paper 3: MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark**\n\nThe third paper introduces a benchmark called MLLM-as-a-Judge to assess the judgment capabilities of MLLMs across three critical tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. The authors curate a dataset of 4,414 image-instruction pairs and use six mainstream MLLMs (GPT-4V, Gemini, LLaVA, CogVLM, Qwen-VL-Max) to generate responses. These responses are then evaluated for their alignment with human judgments using metrics such as Pearson similarity, accuracy, F1-score, and Levenshtein distance.\n\nThe authors find that while MLLMs demonstrate strong alignment with human preferences in Pair Comparison tasks, they struggle in Scoring Evaluation and Batch Ranking tasks, showing significant gaps in their ability to provide nuanced judgments. They also identify various biases and hallucinations in MLLM judgments, including egocentric bias, position bias, and verbosity bias. The paper concludes that MLLMs need further refinement and training to become reliable judges, and advocates for the development of more sophisticated reasoning frameworks and human-in-the-loop approaches to enhance their judgment capabilities.\n\n#### Commonalities and Innovations\n\nAll three papers focus on enhancing the reasoning and judgment capabilities of large language models (LLMs) and multimodal large language models (MLLMs). They explore different methods to improve the models' performance and reliability, addressing the challenges of manual effort, reward hacking, and biases in judgments.\n\n**Commonalities:**\n- **Chain-of-Thought (CoT) Prompting:** Each paper leverages CoT prompting to enhance the reasoning capabilities of LLMs. CoT prompting involves generating intermediate reasoning steps to arrive at a final answer, which is a key technique for improving the models' performance in complex reasoning tasks.\n- **Evaluation Metrics:** The papers use various metrics to evaluate the performance of the models, including accuracy, F1-score, Levenshtein distance, and human agreement scores. These metrics provide a comprehensive assessment of the models' reasoning and judgment abilities.\n- **Dataset and Model Usage:** The authors employ a variety of datasets and models for their experiments, including GSM8K, MathQA, SVAMP, Galactica, and CodeLLAMA. This diverse usage of datasets and models ensures that the findings are generalizable and applicable across different domains and tasks.\n\n**Innovations:**\n- **Auto-CoT:** The first paper introduces Auto-CoT, an automated method for constructing CoT demonstrations. This method addresses the challenge of manual demonstration design by leveraging clustering and diversity-based sampling to mitigate the impact of mistakes in the reasoning process.\n- **ReFT:** The second paper proposes ReFT, a method that combines SFT with on-line reinforcement learning to enhance the generalization capabilities of LLMs. ReFT demonstrates significant improvements over SFT without requiring additional training data, highlighting the potential of reinforcement learning in fine-tuning LLMs.\n- **MLLM-as-a-Judge:** The third paper develops a benchmark to assess the judgment capabilities of MLLMs across various tasks. This benchmark provides a comprehensive evaluation of MLLMs' performance, identifying biases and hallucinations that affect their judgment reliability. The authors also explore the impact of detailed image descriptions on LLMs' performance, suggesting that LLMs can be potential judges for multimodal tasks when provided with comprehensive task-related descriptions.\n\n#### Comparison of Results and Discussion\n\nThe results from the three papers highlight the effectiveness of their respective methods in enhancing the reasoning and judgment capabilities of LLMs and MLLMs. However, there are some differences in the approaches and outcomes that warrant further discussion.\n\n**Auto-CoT vs. Manual-CoT:**\n- **Performance:** Auto-CoT consistently matches or exceeds the performance of Manual-CoT across various reasoning tasks, demonstrating its effectiveness in automating the demonstration construction process.\n- **Flexibility:** Auto-CoT is more flexible and task-adaptive, as it constructs demonstrations specific to each dataset, whereas Manual-CoT often uses the same demonstrations across multiple datasets.\n- **Streaming Setting:** Auto-CoT performs comparably with Manual-CoT in a streaming setting, indicating its robustness and adaptability to different scenarios.\n\n**ReFT vs. SFT:**\n- **Improvement:** ReFT significantly outperforms SFT in terms of accuracy and generalization, particularly in math problem-solving tasks. The improvements are consistent across different models and datasets.\n- **Generalization:** ReFT demonstrates superior generalization ability by learning from multiple CoT paths, whereas SFT relies on a single CoT annotation for each question.\n- **Training Efficiency:** ReFT requires more training epochs compared to SFT, but the improvements in performance justify the additional training time.\n\n**MLLM-as-a-Judge vs. Traditional LLM Evaluation:**\n- **Multimodal Tasks:** MLLM-as-a-Judge evaluates MLLMs in multimodal tasks, providing a more comprehensive assessment of their judgment capabilities.\n- **Bias and Hallucination:** The benchmark identifies various biases and hallucinations in MLLM judgments, highlighting the need for further refinement and training to enhance their reliability.\n- **Human Agreement:** MLLMs exhibit strong agreement with human judgments in Pair Comparison tasks but struggle in Scoring Evaluation and Batch Ranking tasks, indicating the need for more nuanced training data and evaluation metrics.\n\n#### Conclusion and Future Research Directions\n\nThe three papers collectively contribute to the field of large language models and multimodal judging by introducing innovative methods to enhance their reasoning and judgment capabilities. Auto-CoT automates the construction of CoT demonstrations, making the process more efficient and adaptable. ReFT leverages reinforcement learning to improve the generalization of LLMs, demonstrating significant performance gains without requiring additional training data. MLLM-as-a-Judge provides a comprehensive benchmark to assess the judgment capabilities of MLLMs, identifying biases and hallucinations that affect their reliability.\n\n**Main Findings:**\n- **Auto-CoT:** LLMs can perform CoT reasoning effectively by automatically constructing demonstrations, demonstrating the potential of diversity-based sampling to mitigate the impact of mistakes.\n- **ReFT:** Reinforced fine-tuning significantly enhances the reasoning capabilities of LLMs, particularly in math problem-solving tasks, without requiring additional training data.\n- **MLLM-as-a-Judge:** MLLMs exhibit strong alignment with human preferences in Pair Comparison tasks but struggle in Scoring Evaluation and Batch Ranking tasks, highlighting the need for more nuanced training and evaluation.\n\n**Future Research Directions:**\n- **Bias and Hallucination Mitigation:** Further research is needed to develop methods for mitigating biases and hallucinations in MLLMs, ensuring their judgments align more closely with human preferences.\n- **Multimodal RLHF/DPO:** The integration of multimodal reinforcement learning and direct preference optimization techniques could enhance the training of MLLMs, making them more reliable judges in multimodal tasks.\n- **Human-in-the-Loop Approaches:** Incorporating human-in-the-loop methods could improve the consistency and reliability of MLLMs' judgments, particularly in tasks that require nuanced understanding and evaluation.\n\nOverall, these studies underscore the importance of developing innovative methods to enhance the reasoning and judgment capabilities of LLMs and MLLMs, paving the way for more reliable and generalizable models in the future."
}