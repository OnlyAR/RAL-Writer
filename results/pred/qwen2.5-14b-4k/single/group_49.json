{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{\\frameworkname: Market-Guided Stock Transformer for Stock Price Forecasting}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nStock price forecasting has remained an extremely challenging problem for many decades due to the high volatility of the stock market. \nRecent efforts have been devoted to modeling complex stock correlations toward joint stock price forecasting. Existing works share a common neural architecture that learns temporal patterns from individual stock series and then mixes up temporal representations to establish stock correlations. \nHowever, they only consider time-aligned stock correlations stemming from all the input stock features, which suffer from two limitations. First, stock correlations often occur momentarily and in a cross-time manner. Second, the feature effectiveness is dynamic with market variation, which affects both the stock sequential patterns and their correlations.\nTo address the limitations, this paper introduces\n\\frameworkname, a \\textbf{MA}rkert-Guided \\textbf{S}tock \\textbf{T}ransform\\textbf{ER}, which models the momentary and cross-time stock correlation and leverages market information for automatic feature selection. \\frameworkname~elegantly tackles the complex stock correlation by alternatively engaging in intra-stock and inter-stock information aggregation. \nExperiments show the superiority of \\frameworkname~compared with previous works and visualize the captured realistic stock correlation to provide valuable insights.\n\\end{abstract}\n\n\\section{Introduction}\nStock price forecasting, which utilizes historical data collected from the stock market to predict future trends, is a vital technique for profitable stock investment.\nUnlike stationary time series that often exhibit regular patterns such as periodicity and steady trends, the dynamics in the stock price series are intricate because stock prices fluctuate subject to multiple factors, including macroeconomic factors, capital flows, investor sentiments, and events. \nThe mixing of factors interweaves the stock market as a correlated network, making it difficult to precisely predict the individual behavior of stocks without taking other stocks into account. \n\nMost previous works~\\cite{feng2019temporal, xu2021hist, wang2021hierarchical, wang2022adaptive, wang2022review} in the field of stock correlation have relied on predefined concepts, relationships, or rules and established a \\emph{static} correlation graph, e.g., stocks in the same industry are connected to each other.\nWhile these methods provide insights into the relations between stocks, they do not account for the real-time correlation of stocks.\nFor example, different stocks within the same industry can experience opposite price movements on a particular day.\nAdditionally, the pre-defined relationships may not be generalizable to new stocks in an evolving market where events such as company listing, delisting, or changes in the main business happen normally.\nAnother line of research~\\cite{yoo2021accurate} follows the Transformer architecture~\\cite{vaswani2017attention}, and use the self-attention mechanism to compute \\emph{dynamic} stock correlations. \nThis data-driven manner is more flexible and applicable to the time-varying stock sets in the market.\nDespite different schemes for establishing stock correlations, the existing methods generally follow a common two-step computation flow. As depicted in Figure~\\ref{fig:ts}, the first step is using a sequential encoder to summarize the historical sequence of stock features, and obtain stock representation, and the second step is to refine each stock representation by aggregating information from correlated stocks using graph encoders or attention mechanism. \nHowever, such a flow suffers from two limitations.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{fig/MASTER-intro.pdf}\n    \\caption{The framework of existing works. The dashed lines represent the underlying momentary and cross-time stock correlations, which reside between some \\textit{($stock_1, time_1$)}, \\textit{($stock_2, time_2$)} pairs.}\n    \\label{fig:ts} \n\\end{figure}\n\nFirst, existing works distill an overall stock representation and blur the time-specific details of stock sequence, leading to weakness in modeling the de-facto stock correlations, which often occurs \\emph{momentarily} and in a \\emph{cross-time} manner~\\cite{bennett2022lead}.\nTo be specific, the stock correlation is highly dynamic and may reside in misaligned time steps rather than holding true through the whole lookback period.\nThis is because the dominating factors of stock prices constantly change, and different stocks may react to the same factors with different delays. For instance, upstream companies' stock prices may react faster to a shortage of raw materials than those of downstream companies, and individual stocks exhibit a lot of catch-up and fall-behind behaviors. \n\nSince the stock correlation may underlie between every stock pair and time pair, a straightforward way to simulate the momentary and cross-time correlation is to gather the $\\tau \\times |\\mathcal{S}|$ feature vectors for pair-wise attention computation, where $\\tau$ is the lookback window length and $\\mathcal{S}$ is the stock set.\nHowever, in addition to the increased computational complexity, this approach faces practical difficulties because the stock forecasting task is in intense data hunger. \nIntuitively, there are only around 250 trading days per year, producing limited observations on stocks.\nWhen the model adopts such a large attention field with insufficient training samples, it often struggles to optimize and may even fall into suboptimal solutions. \nAlthough clustering approaches like local sensitive hashing~\\cite{kitaev2020reformer} have been proposed to reduce the size of the attention field, they are sensitive to initialization, which is a fatal issue in a data-hungry domain like stock forecasting.\nTo address these challenges, we propose a novel stock transformer architecture specifically designed for stock price forecasting. Rather than directly modeling the $\\tau \\times |\\mathcal{S}|$ attention field or using clustering-based approximation methods, our model aggregates information from different time steps and different stocks alternately to model realistic stock correlation and facilitate model learning.\n\nAnother limitation of existing works is that they ignore the impact of varying market status.\nIn long-term practice with the market variation, one essential observation by investors is that the features come into effect and expire dynamically. \nThe effectiveness of features has an influence on both the intra-stock sequential pattern and the stock correlation. \nFor instance, in a bull market, the correlations among stocks are more significant due to the investors' optimism.\nTraditional investors repeatedly conduct statistical examination on to select effective feature, which is exhaustive and face a gap when integrated with learning-based methods. \nTo save the human efforts, we are motivated to equip our stock transformer with a novel gating mechanism, which incorporates the market information to perform automatically feature selection.  \nWe name the proposed method \\frameworkname, standing for \\textbf{MA}rket-Guided \\textbf{S}tock \\textbf{T}ransform\\textbf{ER}. \nTo summarize, our main contributions are as follows.\n\n$\\bullet$ We propose a novel stock transformer for stock price forecasting to effectively capture the stock correlation. To the best of our knowledge, we are the first to mine the momentary and cross-time stock correlation with learning-based methods. \n\n$\\bullet$ We introduce a novel gating mechanism that integrates market information to automatically select relevant features and adapt to varying market scenarios. \n\n$\\bullet$ We conducted experiments to validate the designs of our proposed method and demonstrated its superiority compared to baselines. The visualization results provided valuable insights into the real-time dynamics of stock correlations.\n \n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/MASTER-framework.pdf}\n    \\caption{Overview of the \\frameworkname~framework.}\n    \\label{fig:framework}\n\\end{figure*}\n\\section{Methodology}\n\n\\subsection{Problem Formulation}\nThe indicators of each stock $u\\in \\mathcal{S}$ are collected at every time step $t\\in[1,\\tau]$ to form the feature vector $x_{u,t} \\in \\mathbf{R}^F$.\nFollowing existing works on stock market analysis \\cite{feng2018enhancing, sawhney2020spatiotemporal, huynh2023efficient}, we focus on the prediction of the change in stock price rather than the absolute value. \nThe return ratio, which is the relative close price change in $d$ days, is $\\tilde{r}_{u}=(c_{u,\\tau+d}-c_{u,\\tau+1})/c_{u,\\tau+1}$, where $c_{u,t}$ is the closing price of stock $u$ at time step $t$, and $d$ represents the predetermined prediction interval.\nThe return ratio normalizes the market price variety between different stocks in comparison to the absolute price change.\nSince stock investment is to rank and select the most profitable stocks, we perform daily Z-score normalization of return ratio to encode the label with the rankings, $r_{u}={\\rm Norm}_\\mathcal{S}(\\tilde{r}_{u})$, as in previous work~\\cite{yang2020qlib}.\n\n\\begin{definition}[Stock Price Forecasting]\nGiven stock features $\\{x_{u,t}\\}_{u\\in \\mathcal{S},t\\in [1, \\tau]}$, the stock price forecasting is to jointly predict the future normalized return ratio $\\{r_{u}\\}_{u\\in \\mathcal{S}}$.\n\\end{definition}\n\n\\subsection{Overview}\nFigure~\\ref{fig:framework} depicts the architecture of our proposed method \\frameworkname, which consists of five steps.\n(1) \\textbf{Market-Guided Gating}. We construct a vector representing the current market status $m_\\tau$ and leverage it to rescale feature vectors by a gating mechanism, achieving market-guided feature selection.\n(2) \\textbf{Intra-Stock Aggregation}.\nWithin the sequence of each stock, at each time step, we aggregate information from other time steps to generate a local embedding that preserves the temporal local details of the stock while collecting all important signals along the time axis.\nThe local embedding $h_{u,t}$ will serve as relays and transport the collected signals to other stocks in subsequent modules.\n(3) \\textbf{Inter-Stock Aggregation}.\nAt each time step, we compute stock correlation with attention mechanism, and each stock further aggregates the local embedding of other stocks. \nThe aggregated information $z_{u,t}$, which we refer to as temporal embedding, contains not only the information of the momentarily correlated stocks at $t$, but also preserves the personal information of $u$.\n(4) \\textbf{Temporal Aggregation}.\nFor each stock, the last temporal embedding queries from all historical temporal embedding and produce a comprehensive stock embedding $e_{u}$.\n(5) \\textbf{Prediction}.\nThe comprehensive stock embedding is sent to prediction layers for label prediction.\nWe elaborate on the details of \\frameworkname~step by step in the following sub-sections.\n\n\\subsection{Market-Guided Gating}\n\\subsubsection{Market Status Representation}\nFirst, we propose to combine information from two aspects into a vector $m_\\tau$ to give an abundant description of the current market status.\n(1) Market index price.\nThe market index price is a weighted average of the prices of a group of stocks $\\mathcal{S'}$ by their share of market capitalization. \n$\\mathcal{S'}$ is typically composed of top companies with the most market capitalization, representing a particular market or sector, and may differ from user-interested stocks in investing $\\mathcal{S}$.\nWe include both the current market index price at $\\tau$ and the historical market index prices, which is described by the average and standard deviation in the past $d'$ days to reveal the price fluctuations. Here, $d'$ specifies the referable interval length to introduce historical market information in applications. \n(2) Market index trading volume. The trading volumes of $\\mathcal{S'}$ reveals the investors involvement, reflecting the activity of the market. We include the average and standard deviation of market index trading volume in the past $d'$ days, to reveal the actual size of the market. $\\mathcal{S'}$ and $d'$ are identical to the aforementioned definitions.\nNow we present the market-guided stock price forecasting task.\n\n\\begin{definition}[Market-Guided Stock Price Forecasting]\nGiven $\\{x_{u,t}\\}_{u\\in \\mathcal{S},t\\in [1, \\tau]}$ and the constructed market status vector $m_\\tau$,  market-guided stock price forecasting is to jointly predict the future normalized return ratio $\\{r_u\\}_{u\\in \\mathcal{S}}$.\n\\end{definition}\n\n\\subsubsection{Gating Mechanism}\nThe gating mechanism generates one scaling coefficient for each feature dimension to enlarge or shrink the magnitude of the feature, thereby emphasizing or diminishing the amount of information from the feature flowing to the subsequent modules. \nThe gating mechanism is learned by the model training, and the coefficient is optimized by how much the feature contributes to improve forecasting performance, thus reflect the feature effectiveness. \n\nGiven the market status representation $m_\\tau, |m_\\tau|=F'$,  \nwe first use a single linear layer to transform $m_\\tau$ into the feature dimension $F=|x_{u,t}|$.\nThen, we perform Softmax along the feature dimension to obtain a distribution. \n$$ \\alpha(m_\\tau) = F\\cdot {\\rm softmax}_{\\beta}(W_{\\alpha}m_\\tau+b_{\\alpha}),$$\nwhere $W_{\\alpha}$, $b_{\\alpha}$ are learnable matrix and bias, $\\beta$ is the temperature hyperparameter controlling the sharpness of the output distribution.\nSoftmax compels a competition among features to distinguish the effective ones and ineffective ones.\nHere, a smaller temperature $\\beta$ encourages the distribution to focus on certain dimension and the gating effect is stronger while a larger $\\beta$ makes the distribution incline to even and the gating effect is weaker. \nNote that we enlarge the value at each dimension by $F$ times as the scaling coefficient. This operation compare the generated distribution with a uniform distribution where each dimension is $1/F$, to determine whether to enlarge or shrink the value.  \nThe intuition to generate coefficients from $m_\\tau$ is that the effectiveness of features are influenced by market status. For example, if the model learns moving average (MA) factor is useful during volatile market periods, it will emphasize MA when the market becomes volatile again.\nUnder the same $m_\\tau$, $\\alpha$ are shared for $\\{x_{u,t}\\}$, $u\\in \\mathcal{S}$, $t\\in [1, \\tau]$, in that we incorporate the most recent market status to perform unified feature selection. The rescaled feature vectors are $ \\tilde{x}_{u,t} = \\alpha(m_\\tau) \\circ x_{u,t}$, where $\\circ$ is the Hadamard product. \n\n\\subsection{Intra-Stock Aggregation}\nIn MASTER, we use intra-stock aggregation followed by inter-stock aggregation to break down the large and complex attention field.\nAlthough the entire market is complicated with diverse behaviours of individual stocks, the patterns of a specific stock tend to be relatively continuous.\nTherefore, we perform intra-stock aggregation first due to its smaller attention field and simpler distribution.\nIn our proposed intra-stock aggregation, the feature at each time step aggregate information from other time steps and form a local embedding. \nCompared with existing works which initially mix the feature sequence into one representation~\\cite{yoo2021accurate}, we maintain a sequence of local embedding which are advised with the important signals in sequence through intra-stock aggregation while reserve the local details.\n\nWe first send the rescaled feature vectors to a feature encoder and transform them into the embedding space, $y_{u,t}=f(\\tilde{x}_{u,t})$, $|y_{u,t}|=D$. We simply use a single linear layer as $f(\\cdot)$.\nThen, we apply a bi-directional sequential encoder to obtain the local output at each time step $t$.\nInspired by the success of transformer-based models in modeling sequential patterns, we instantiate the sequential encoder with a single-layer transformer encoder~\\cite{vaswani2017attention}.\nEach feature vector at a particular time step is treated as a token, and we add a fixed $D$-dimensional sinusoidal positional encoding $p_{t}$ to mark the chronically order in the look back window.  \n$$Y_u=||_{t\\in[1,\\tau]}\\textsf{LN}(f(\\tilde{x}_{u,t})+p_t),$$ where $||$ denotes the concatenation of vectors and \\textsf{LN} the layer normalization. Then, the feature embedding at each time step queries from all time steps in the stock sequence. \nWe introduce multi-head attention mechanisms, denoted as \\textsf{MHA}$(\\cdot)$,  with $N_1$ heads to perform different aggregations in parallel. We also utilize feed forward layers, \\textsf{FFN}$(\\cdot )$, to fuse the information obtained from the multi-head attention.\n$$ Q^1_u=W^1_QY_u,\\quad K^1_u=W^1_KY_u,\\quad V^1_u=W^1_VY_u,$$\n$$ H^1_u=||_{t\\in[1,\\tau ]} h_{u,t} =\\textsf{FFN}^1(\\textsf{MHA}^1(Q^1_u,K^1_u,V^1_u)+Y_u),$$\nwhere \\textsf{FFN} is a two-layer MLP with ReLU activation and residual connection. As a result, the local embedding $h_{u,t}$ both reserve the local details and encode indicative signals from other time steps. \n\n\\subsection{Inter-Stock Aggregation}\nThen, we consider aggregating information from correlated stocks. \nCompared with existing works that distill an overall stock correlation, we establish a series of momentary stock correlation corresponding to every time step.\nInstead of using pre-defined relationships that face a mismatch with the proximity of real-time stock movements, we propose to mine the asymmetric and dynamic inter-stock correlation via attention-mechanism.\nThe quality of the correlation will be measured by its contribution to improving the forecasting performance, and automatically optimized by the model training process.\n\nSpecifically, at each time step, we gather the local embedding of all stocks $H^2_t=||_{u\\in\\mathcal{S}} h_{u,t}$ and perform multi-head attention mechanism with $N_2$ heads. \n$$Q^2_t = W^2_QH^2_t,\\quad K^2_t=W^2_KH^2_t, \\quad V^2_t=W^2_VH^2_t,$$\n$$ Z_t=||_{u\\in\\mathcal{S}} z_{u,t} =\\textsf{FFN}^2(\\textsf{MHA}^2(Q^2_t,K^2_t,V^2_t)+H^2_t).$$\nWith the residual connection of \\textsf{FFN}, the temporal embedding $z_{u,t}$ is encoded with both the information from momentarily correlated stocks and the personal information of stock $u$ itself. \nOur stock transformer is able to model the cross-time correlation of stocks, as shown in Figure~\\ref{fig:framework} (Right). The local details of $y_{v,j}$ can first be conveyed to $h_{v,i}$ by the intra-stock aggregation of stock $v$, and then transmitted to $z_{u,i}$ by inter-stock aggregation at time step $i$, hence modeling the correlation from any $(v,j)$ to $(u,i)$. \nWe further visualize and explain the captured cross-time correlation in the experiments section.\n\n\\subsection{Temporal Aggregation}\nIn contrast with existing works which obtain one embedding for each stock after modeling stock correlation~\\cite{feng2019temporal}, our approach involves producing a series of temporal embedding $z_{u,t}, t\\in[1,\\tau].$ \nEach $z_{u,t}$ is encoded with information from stocks that are momentarily correlated with $(u,t)$.\nTo summarize the obtained temporal embeddings and obtain a comprehensive stock embedding $e_u$, we employ a temporal attention layer along the time axis. \nWe use the latest temporal embedding $z_{u,\\tau}$ as the query vector, and compute the attention score $\\lambda_{u,t}$ in a hidden space with transformation matrix $W_{\\lambda}$,\n$$\\lambda_{u,t}=\\frac{\\exp(z^T_{u,t}W_{\\lambda}z_{u,\\tau})}{\\sum_{i\\in [1,\\tau]} \\exp(z^T_{u,i}W_{\\lambda}z_{u,\\tau})},\n\\quad e_u=\\sum_{t\\in[1,\\tau]}\\lambda_{u,t} z_{u,t}.$$\n\n\\subsection{Prediction and Training}\nFinally, the stock embedding $e_u$ is fed into a predictor $g(\\cdot)$ for label regression. We use a single linear layer as the predictor, and the forecasting quality is measured by the MSE loss. \nIn each batch, MASTER is jointly optimized for all $ u\\in \\mathcal{S}$ on a particular prediction date. And a training epoch is composed of multiple batches correspond to different prediction dates in the training set.\n$$\\hat{r}_u=g(e_u), \\quad L=\\sum_{u\\in \\mathcal{S}}\\textsf{MSE}(r_u, \\hat{r}_u).$$\n\n\\subsection{Discussions}\n\\subsubsection{Relationships with Existing Works}\nModeling stock correlations has long been an indispensable research direction for stock price prediction. Today, many researchers and quantitative analysts, still opt for linear models, support vector machines, and tree-based methods for stock price forecasting~\\cite{nugroho2014decision,chen2016xgboost,kamble2017short,xie2013semantic,li2015tensor, piccolo1990distance}. The aggregation of correlation information within and between stocks is often achieved through feature engineering, which relies heavily on manual expertise and constantly faces the risk of factor decay.\nInspired by the success of neural sequential data analysis, researchers are driven to take into account the stock feature sequences and learn the temporal correlation automatically. \nThey design various sequential models, such as RNN-based \\cite{feng2019temporal, sawhney2021stock, yoo2021accurate, huynh2023efficient}, CNN-based \\cite{wang2021hierarchical}, and attention-based models\\cite{liu2019transformer, ding2020hierarchical}, to mine the internal temporal dynamics of a stock. \nRecent research focus on the modeling of stock correlation, which add a correlation module in posterior to the sequential model as illustrated in Figure~\\ref{fig:ts}.\nThey propose to use graph-based \\cite{feng2019temporal, xu2021hist,wang2021hierarchical, wang2022adaptive}, hypergraph-based \\cite{sawhney2021stock,huynh2023efficient} and attention-based \\cite{yoo2021accurate, xiang2022temporal} modules to build the overall stock correlation and perform joint prediction. \nOur \\frameworkname~is dedicated to momentary and cross-time stock correlation mining. To do so, we develop a novel model architecture as in Figure~\\ref{fig:framework} that is genuinely different from all existing methods. \nFurthermore, \\frameworkname~is specialized for stock price forecasting, which is distinct in data form and task properties from existing transformer-based models in spatial-temporal data ~\\cite{bulat2021space, cong2021spatial, xu2020spatial,li2023memory} or multivariate time series domains~\\cite{zhang2022crossformer, nie2022time}.\n\n\\subsubsection{Complexity Analysis}\nWe now analyze the computation complexity of our proposed method.\nLet $M=|\\mathcal{S}|$, the market-guided gating rescale $M \\times \\tau$ feature vectors of dimension $F$.\nIn intra-stock aggregation, the calculation amount of pair-wise attention is $\\tau^2$ for each stock at each attention head.\nIn inter-stock aggregation, the calculation amount is $M^2$ at each time step and each attention head.\nIn temporal aggregation, we compute $\\tau$ attention scores for each stock. \nThe overall computation complexity is {\\rm O}$(FM\\tau+N_1M\\tau^2D^2+N_2M^2\\tau D^2+M\\tau D^2)$, where $M\\gg \\tau$.\nTherefore, \\frameworkname~is of ${\\rm O}(N_2M^2\\tau D^2)$ time complexity.\nCompared with directly operating on the $M \\times \\tau$ attention field with $N$ attention heads, which is in {\\rm O}$(NM^2\\tau^2D^2)$, we reduce the computation cost by about $\\tau$ times and achieve modeling cross-time correlations between stocks more efficiently.\nThe overall parameters to be trained in \\frameworkname~are transformation matrices $W^1_Q, W^1_K, W^1_V, W^2_Q, W^2_K, W^2_V, W_\\lambda$, which is in shape $D\\times D$, and parameters in MLP layers $\\alpha, f, \\textsf{FFN}^1, \\textsf{FFN}^2$ and $g$.\n\n\\section{Experiments}\nIn this section, we conduct experiments to answer the following four research questions:\n\\begin{itemize}\n    \\item \\textbf{RQ1} How is the overall performance of \\frameworkname~compared with state-of-the-art methods?\n    \\item \\textbf{RQ2} Is the proposed stock transformer architecture effective for stock price forecasting?\n    \\item \\textbf{RQ3} How do hyper-parameter configurations affect the performance of \\frameworkname?\n    \\item \\textbf{RQ4} What insights on the stock correlation can we get through visualizing the attention map?\n\\end{itemize}\n\n\\subsubsection{Datasets}\nWe evaluate our framework on the Chinese stock market with CSI300 and CSI800 stock sets. CSI300 and CSI800 are two stock sets containing 300 and 800 stocks with the highest capital value on the Shanghai Stock Exchange and the Shenzhen Stock Exchange. \nThe dataset contains daily information ranging from 2008 to 2022 of CSI300 and CSI800. \nWe use the data from Q1 2008 to Q1 2020 as the training set, data in Q2 2020 as the validation set, and the last ten quarters, i.e., Q3 2020 to Q4 2022, are reserved as the test set.  We apply the public Alpha158 indicators \\cite{yang2020qlib} to extract stock features from the collected data. \nThe lookback window length $\\tau$ and prediction interval $d$ are set as $8$ and $5$ respectively. \nFor market representation, we constructed $63$ features with CSI300, CSI500 and CSI800 market indices, and  refereable interval $d'=5,10,20,30,60$.  \n\n\\subsubsection{Baselines}\nWe compare the performance of MASTER with several stock price forecasting baselines from different categories.\n$\\bullet$ XGBoost~\\cite{chen2016xgboost}: A decision-tree based method. According to the leaderboard of Qlib platform~\\cite{yang2020qlib}, it is one of the strongest baselines.\n$\\bullet$ LSTM~\\cite{graves2012long}, GRU~\\cite{cho2014learning}, TCN~\\cite{bai2018empirical}, and Transformer~\\cite{vaswani2017attention}: Sequential baselines that leverage vanilla LSTM/GRU/Temporal convolutional network/Transformer along the time axis for stock price forecasting. \n$\\bullet$ GAT~\\cite{velivckovic2017graph}: A graph-based baseline, which first use sequential encoder to gain stock presentation and then aggregate information by graph attention networks{\\footnote{More discussion is provided in the supplementary materials.}}. \n$\\bullet$ DTML~\\cite{yoo2021accurate}: A state-of-the-art stock correlation mining method, which follows the framework in Figure~\\ref{fig:ts}. DTML adopts the attention-mechanism to mine the dynamic correlation among stocks and also incorporates the market information into the modeling. \n\n\\subsubsection{Evaluation}\nWe adopt both ranking metrics and portfolio-based metrics to give a thorough evaluation of the model performance. Four ranking metrics, Information Coefficient (IC), Rank Information Coefficient (RankIC), Information Ratio based IC (ICIR) and Information Ratio based RankIC (RankICIR) are considered. IC and RankIC are the Pearson coefficient and Spearman coefficient averaged at a daily frequency. ICIR and RankICIR are normalized metrics of IC and RankIC by dividing the standard deviation. Those metrics are commonly used in literature (e.g., \\citealt{xu2021hist} and \\citealt{yang2020qlib}) to describe the performance of the forecasting results from the value and rank perspectives.\nFurthermore, we employ two portfolio-based metrics to compare the investment profit and risk of each method. \nWe simulate daily trading using a simple strategy that selects the top 30 stocks with the highest return ratio and reports the Excess Annualized Return (AR) and Information Ratio (IR) metrics.\nAR measures the annual expected excess return generated by the investment, while IR measures the risk-adjusted performance of an investment.\n\n\\begin{table*}[t]\n\\centering\n\n\\small\n\\begin{tabular}{c|l|cccc|cc}\n\\toprule\n Dataset & Model & IC & ICIR & RankIC & RankICIR & AR & IR\\\\\n \\midrule\n\\multirow{8}*{CSI300}& XGBoost &  $0.051\\pm0.001$  & $0.37\\pm0.01$ & $0.050\\pm 0.001$ & $0.36\\pm0.01$ & \\underline{$0.23\\pm0.03$} & $1.9\\pm0.3$ \\\\\n\n& LSTM & $0.049 \\pm 0.001$ & \\underline{$0.41\\pm 0.01$} & $0.051\\pm 0.002$ &  $0.41\\pm 0.03$ & $0.20\\pm 0.04$ & \\underline{$2.0\\pm 0.4$} \\\\\n& GRU & $0.052\\pm 0.004$ & $0.35\\pm 0.04$ & $0.052\\pm 0.005$ & $0.34\\pm 0.04$ & $0.19\\pm 0.04$ & $1.5 \\pm 0.3$\\\\\n& TCN & $0.050\\pm 0.002$ & $0.33\\pm 0.04$ & $0.049\\pm 0.002$ & $0.31\\pm 0.04$ & $0.18\\pm 0.05$ & $1.4 \\pm 0.5$ \\\\\n& Transformer & $0.047 \\pm 0.007$ & $0.39\\pm 0.04$ & $0.051\\pm 0.002$ & \\underline{$0.42\\pm 0.04$} & $0.22\\pm 0.06$ & $2.0\\pm 0.4$ \\\\\n& GAT & \\underline{$0.054\\pm 0.002$} & $0.36\\pm 0.02$ & $0.041\\pm 0.002$ & $0.25\\pm 0.02$ & $0.19\\pm 0.03$ & $1.3 \\pm 0.3$ \\\\\n& DTML & $0.049\\pm0.006$  &$0.33\\pm0.04$ &  \\underline{$0.052\\pm 0.005$} & $0.33\\pm0.04$ & $0.21\\pm0.03$ & $1.7\\pm0.3$ \\\\\n& \\frameworkname~ & \\bm{$0.064^*\\pm0.006$}  &\\bm{$0.42\\pm0.04$} &  \\bm{$0.076^*\\pm 0.005$} & \\bm{$0.49\\pm0.04$} & \\bm{$0.27\\pm0.05$} & \\bm{$2.4\\pm0.4$} \\\\ \n\\midrule\n\\multirow{8}*{CSI800} & XGBoost & $0.040 \\pm 0.000$ & $0.37\\pm 0.01$ & $0.047\\pm 0.000$ & $0.42\\pm 0.01$ & $0.08\\pm 0.02$ & $0.6\\pm 0.2$\\\\\n & LSTM & $0.028 \\pm 0.002$ & $0.32\\pm 0.02$ & $0.039\\pm 0.002$ & $0.41\\pm 0.03$ & $0.09\\pm 0.02$ & $0.9\\pm 0.2$\\\\\n & GRU & $0.039\\pm 0.002$ & $0.36\\pm 0.05$ & $0.044\\pm 0.003$ & $0.39\\pm 0.07$ & $0.07\\pm 0.04$ & $0.6 \\pm 0.3$\\\\\n& TCN & $0.038\\pm 0.002$ & $0.33\\pm 0.04$ & $0.045\\pm 0.002$ & $0.38\\pm 0.05$ & $0.05\\pm 0.04$ & $0.4 \\pm 0.3$ \\\\\n   & Transformer & $0.040 \\pm 0.003$ & \\bm{$0.43\\pm 0.03$} & $0.048\\pm 0.003$ & \\bm{$0.51\\pm 0.05$} & $0.13\\pm 0.04$&$1.1\\pm 0.3$\\\\\n& GAT & \\underline{$0.043\\pm 0.002$} & $0.39\\pm 0.02$ & $0.042\\pm 0.002$ & $0.35\\pm 0.02$ & $0.10\\pm 0.04$ & $0.7 \\pm 0.3$ \\\\\n   & DTML & $0.039 \\pm 0.004$ & $0.29\\pm 0.03$ & \\underline{$0.053\\pm 0.008$} & $0.37\\pm 0.06$ & \\underline{$0.16\\pm 0.03$} & \\underline{$1.3\\pm 0.2$}\\\\\n& \\frameworkname~ & \\bm{$0.052^*\\pm0.006$}  &\\underline{$0.40\\pm0.06$} &  \\bm{$0.066\\pm 0.007$} & \\underline{$0.48\\pm0.06$} & \\bm{$0.28^*\\pm0.02$} & \\bm{$2.3^*\\pm0.3$} \\\\ \n\\bottomrule\n\\end{tabular}\n\\caption{Overall performance comparison. The best results are in bold and the second-best results are underlined. And * denotes statistically significant improvement (measured by t-test with p-value $<$ 0.01) over all baselines.}\n\\label{tab:performance}\n\\end{table*}\n\n\\begin{table*}[t]\n\\centering\n\\begin{tabular}{l|cccc|cc}\n\\toprule\n Model & IC & ICIR & RankIC & RankICIR & AR & IR\\\\\n\\midrule\n(MA)STER  & \\bm{$0.064\\pm0.003$}  &\\bm{$0.43\\pm0.02$} &  \\bm{$0.074\\pm 0.004$} & \\bm{$0.48\\pm0.04$} & \\bm{$0.25\\pm0.03$} & \\bm{$2.1\\pm0.3$}\\\\\n(MA)STER-Bi &\\underline{$0.058\\pm0.005$} & \\underline{$0.38 \\pm 0.04$} &\\underline{$0.066\\pm 0.008$} & \\underline{$0.41\\pm 0.05$} &\\underline{$0.19\\pm 0.03$} & $1.6\\pm 0.2$\\\\\nNaive &  $0.041\\pm0.008$ &$0.30\\pm0.05$ & $0.046\\pm0.007$ & $0.32\\pm0.04$ & $0.18\\pm0.05$ & $1.6\\pm0.6$\\\\\nClustering & $0.044\\pm 0.003$ & $0.36 \\pm 0.02$ & $0.049\\pm 0.005$ & $0.39\\pm 0.04$ & $0.18\\pm 0.04$ & \\underline{$1.7 \\pm 0.3$}\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Experiments on CSI300 to validate the effectiveness of proposed stock transformer architecture. The best results are in bold and the second-best results are underlined.}\n\\label{tab:effect}\n\\end{table*}\n\n\\subsubsection{Implementation}\nWe implemented \\frameworkname{\\footnote{Code and supplementary materials are at \\url{https://github.com/SJTU-Quant/MASTER}}}~with PyTorch and build our methods based on the open-source quantitative investment platform Qlib~\\cite{yang2020qlib}. For DTML, we implement it based on the original paper since there is no official implementation publicly. For other baselines, we use their Qlib implementations.\nFor hyperparameters of each baseline method, the layer number and model size are tuned from $\\{1,2,3\\}$ and $\\{128, 256, 512\\}$ respectively. The learning rate $lr$ is tuned among $\\{10^{-i}\\}_{i\\in\\{3,4,5,6\\}}$, and we selected the best hyperparameters based on the IC performance in the validation stage.\nFor hyperparameters of \\frameworkname, we tune the model size $D$ and learning rate $lr$ among the same range as the baselines, and the final selection is $D$=$256$, $lr$=$10^{-5}$ for all datasets; we set $N_1$=$4$, $N_2$=$2$ for all datasets and $\\beta$=$5$ and $\\beta$=$2$ for CSI300 and CSI800 respectively. More implementation details of baseline methods are summarized in the supplementary materials.\nEach model is trained for at most $40$ epochs with early stopping.\nAll the experiments are conducted on a server equipped with Intel(R) Xeon(R) Platinum 8163 CPU, 128GB Memory, and a Tesla V100-SXM2 GPU (16GB Memory). \nEach experiment was repeated 5 times with random initialization and the average performance was reported.\n\n\\subsection{Overall Performance (RQ1)}\n\\label{sec:interval}\nThe overall performance are reported in Table~\\ref{tab:performance} %shows the overall performance of compared methods.\\label{tab:performance}\n\\frameworkname~achieves the best results on 6/8 of the ranking metrics, and \nconsistently outperforms all benchmarks in the portfolio-based metrics. In particular, \\frameworkname~achieve $13\\%$ improvements in ranking metrics and $47\\%$ improvements in portfolio-based metrics compared to the second-best results on the average sense. Note that ranking matrics are computed with the whole set and portfolio-based metrics mostly consider the 30 top-performed stocks. \nThe achievements in both types of metrics imply that MASTER is of good predicting ability on the whole stock set without sacrificing the accuracy of the important stocks. \nThe significant improvements cast light on the importance of stock correlation modeling, so each stock can also benefit from the historical signals of other momentarily correlated stocks. \nWe also observe all methods gain better performance on CSI300 over CSI800. We believe it is because CSI300 consists of companies with larger capitalization whose stock prices are more predictable. \nWhen compared to the existing stock correlation method (i.e., DTML), \\frameworkname~outperforms in all 6 metrics, which tells our proposed Market-Guided Gating and aggregation techniques are more efficient in mining cross-stock information than existing literature. \n\n\\begin{figure*}[]\n    \\includegraphics[width=\\linewidth]{fig/exp_MH-cropped.pdf}\n\\caption{The average and standard deviation of metrics with different $(N_1,N_2)$ combinations on CSI300.}\n\\label{fig:mh}\n\\end{figure*}\n\n\\begin{figure*}[]\n    \\includegraphics[width=\\linewidth]{fig/exp_beta.pdf}\n\\caption{\\frameworkname~performance with varying $\\beta$. The horizontal dash lines are performance without market-guided gating.}\n\\label{fig:beta}\n\\end{figure*}\n\n\\begin{figure}[t]\n    \\includegraphics[width=\\linewidth]{fig/stock2date_14_trimed.pdf}\n\\caption{The correlation towards three target stocks on Aug 19th, 2022. The y-axis is time steps in the lookback window and the x-axis is source stocks. \\emph{Avg.} denotes the evenly distributed value.}\n\\label{fig:stock2date}\n\\end{figure}\n\\subsection{Stock Transformer Architecture (RQ2)}\nWe validate the effectiveness of our specialized stock transformer architecture by experiments on four settings. (1) (MA)STER, which is our stock transformer without the gating. (2) (MA)STER-Bi, in which we substitute the single-layer transformer encoder with a bi-directional LSTM to evince that the effectiveness of our proposed architecture is not coupled with strong sequential encoders.\n(3) Naive, which directly performs information aggregation among $\\tau \\times |\\mathcal{S}|$ tokens. \n(4) Clustering, in which we adapt the Local Sensitive Hashing~\\cite{kitaev2020reformer} to allocate all tokens into $10$ buckets by similarity and perform aggregation within each group, which is a classic task-agnostic technique to reduce the scale of the attention field. \nFor a fair comparison, in (3) and (4), we first use the same transformer encoder to extract token embedding and then use the same multi-head attention mechanism as in our stock transformer, so the only difference is the attention field.\nDue to resource limits, we only conduct experiment on CSI300 dataset. The results in Table~\\ref{tab:effect} illustrate the efficacy of our tailored stock transformer architecture, which performs intra-stock aggregation and inter-stock aggregation alternatively.\n\n\\subsection{Ablation Study (RQ3)}\nFirst, we conduct ablation study on $(N_1,N_2)$ combination. \nThe results of CSI300 are shown in Figure~\\ref{fig:mh} and the results on CSI800 are similar. The difference among head combinations is not significant compared with the inherent variance under each setting. In the studied range, most settings consistently performed better than the baselines.\n\nSecond, we study the influence of temperature $\\beta$ in the gating mechanism.\nAs explained before, a smaller $\\beta$ forces a stronger feature selection while a larger $\\beta$ turns off the gating effect.\nFigure~\\ref{fig:beta} shows the performance with varying $\\beta$.\nThe CSI300 is a relatively easier dataset where most features are quite effective, so the temperature is expected to be larger to relax the feature selection, while more powerful feature selection intervention is needed for the sophisticated CSI800 dataset whose $\\beta$ of the best performance is smaller.\n\n\\subsection{Visualization of Attention Maps (RQ4)}\nWe show how \\frameworkname~captures the momentary and cross-time stock correlation that previous methods are not expressive enough to model.\nFigure~\\ref{fig:stock2date} shows the inter-stock attention map at different time steps in the lookback window. We choose three representative stocks as the target and sample $100$ random stocks as sources for visualization.\nThe highlighted part is scattered instead of exhibiting neat strips, implying that the correlation is momentary rather than long-standing.\nAlso, the inter-stock correlation is sparse, with only a few stocks having strong correlations toward the target stocks.\nFigure~\\ref{fig:stock2stock} displays the correlation between stock pairs to show how the correlation resides in time. \nFrom source stock $v$ to target stock $u$, we compute $I_{u\\leftarrow v}[i,j]=\\mathrm{S}^1_v[i,j]\\mathrm{S}^2_i[u,v]$ as the $\\tau\\times\\tau$ correlation map, while $\\mathrm{S}^1$ and $\\mathrm{S}^2$ are the intra-stock and inter-stock attention map.\nFirst, the highlighted blocks are not centered on the diagonal, because the stock correlation is usually cross-time rather than temporally aligned. \nSecond, the left two figures are totally different, illustrating that correlation is highly asymmetric between $u\\leftarrow v$ and $v\\leftarrow u$. \nThird, the importance of mined correlation changes slowly when the lookback window slides to forecast on different dates. For example, blocked regions in the right two figures correspond to the same absolute time scope of different prediction dates, whose patterns are to a certain degree similar. \n\n\\begin{figure}[t]\n    \\includegraphics[width=\\linewidth]{fig/stock2stock.pdf}\n\\caption{Cross-time correlation of stock pairs on Aug 19th and 25th, 2022. The x-axis is the source time steps and the y-axis is the target time steps. }\n\\label{fig:stock2stock}\n\\end{figure}\n\\section{Conclusion}\nWe introduce a novel method \\frameworkname~for stock price forecasting, which models the realistic stock correlation and guide feature selection with market information.\n\\frameworkname~consists of five steps, market-guided gating, intra-stock aggregation, inter-stock aggregation, temporal aggregation, and prediction.\nExperiments on the Chinese market with $2$ stock universe shows that \\frameworkname~achieves averagely $13\\%$ improvements on ranking metrics and $47\\%$ on portfolio-based metrics compared with all baselines.\nVisualization of attention maps reveals the de-facto momentary and cross-time stock correlation. \nIn conclusion, we provide a more granular perspective for studying stock correlation, while also indicating an effective application of market information. Future work can explore to mine stock correlations of higher quality and study other uses of market information.\n\n\\section*{Acknowledgements}\nThe authors would like to thank the anonymous reviewers for their insightful reviews. \nThis work is supported by the National Key Research and Development Program of China (2022YFE0200500), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), and SJTU Global Strategic Partnership Fund (2021SJTU-HKUST).\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Double-Path Adaptive-correlation Spatial-Temporal Inverted Transformer for Stock Time Series Forecasting}\n\n\\begin{document}\n\n\\title{Double-Path Adaptive-correlation Spatial-Temporal Inverted Transformer for Stock Time Series Forecasting}\n\n\\author{Wenbo Yan}\n\\affiliation{%\n  \\institution{School of Intelligence Science and Technology}\n  \\institution{Computational Intelligence Laboratory}\n  \\city{Peking University}\n  \\country{China}}\n\\email{wenboyan@stu.pku.edu.cn}\n\n\\author{Ying Tan}\n\\affiliation{%\n  \\institution{School of Intelligence Science and Technology}\n  \\institution{Institute for Artificial Intellignce}\n  \\institution{National Key Laboratory of General Artificial Intelligence}\n  \\institution{Key Laboratory of Machine Perceptron (MOE)}\n  \\city{Peking University}\n  \\country{China}}\n\\email{ytan@pku.edu.cn}\n\n\\begin{abstract}\n    Spatial-temporal graph neural networks (STGNNs) have achieved significant success in various time series forecasting tasks. However, due to the lack of explicit and fixed spatial relationships in stock prediction tasks, many STGNNs fail to perform effectively in this domain. While some STGNNs learn spatial relationships from time series, they often lack comprehensiveness. Research indicates that modeling time series using feature changes as tokens reveals entirely different information compared to using time steps as tokens. To more comprehensively extract dynamic spatial information from stock data, we propose a Double-Path Adaptive-correlation Spatial-Temporal Inverted Transformer (DPA-STIFormer). DPA-STIFormer models each node via continuous changes in features as tokens and introduces a Double Direction Self-adaptation Fusion mechanism. This mechanism decomposes node encoding into temporal and feature representations, simultaneously extracting different spatial correlations from a double path approach, and proposes a Double-path gating mechanism to fuse these two types of correlation information. Experiments conducted on four stock market datasets demonstrate state-of-the-art results, validating the model's superior capability in uncovering latent temporal-correlation patterns.\n\\end{abstract}\n\n\\begin{CCSXML}\n<ccs2012>\n   <concept>\n       <concept_id>10002951.10003227.10003236</concept_id>\n       <concept_desc>Information systems~Spatial-temporal systems</concept_desc>\n       <concept_significance>500</concept_significance>\n       </concept>\n </ccs2012> \n\\end{CCSXML}\n\n\\ccsdesc[500]{Information systems~Spatial-temporal systems}\n\n\\keywords{Spatial-Temporal Graph Neural Network; Pre-training Model; Time Series Forecasting}\n\n\\maketitle\n\n\\section{INTRODUCTION}\nTime series data is ubiquitous in various aspects of life, including healthcare, transportation, weather, and finance, where a substantial amount of time series data is generated daily. If historical data can be utilized to uncover potential patterns and predict future changes, it could greatly enhance our quality of life. Initially, it was assumed that time series data is generated from certain processes, leading to the development of models such as \\citet{box_jenkins_1970}, \\citet{engle1982autoregressive} to approximate these processes. In recent years, numerous deep learning methods have been applied to the field of time series forecasting. To better model time series data, models based on recurrent neural networks (RNNs), such as \\citet{qin2017dual}, as well as models based on convolutional neural networks (CNNs), such as \\citet{dai2022price}, have been proposed. With the rise of the Transformer model in the field of natural language processing, Transformer-based time series forecasting models, such as \\citet{crossformer}, have been introduced. As research has progressed, it has been found that for certain time series problems, such as traffic and climate forecasting, employing spatial-temporal prediction methods can better predict future changes. Consequently, many spatial-temporal graph neural networks (STGNNs) have been proposed and have achieved excellent results in fields like transportation. Moreover, to address the issue that Transformer-based methods cannot utilize spatial relationships, Spatial-Temporal Transformers (STTransformers) such as \\citet{TFT}, \\citet{STTN} have been proposed.\n\nIn fact, these time series modeling methods often fail to achieve satisfactory results in stock market prediction. Firstly, whether based on RNNs or temporal convolution methods, they typically focus on modeling the time series of individual nodes independently, without considering the interdependencies between different time series. However, in the stock market, the entire market functions as an integrated whole, and there are significant interactions between the time series of different stocks at any given moment. Therefore, ignoring the correlations between different nodes fails to effectively address the problem of stock prediction. Secondly, for Transformer-based methods, since the features of time series often lie in the variations between time steps and each time step itself contains low information content or even no actual meaning, using time steps as tokens to model time series with Transformers often results in poor performance.\n\nWhile STGNNs can effectively utilize the correlations between nodes and integrate changes in other time series for comprehensive modeling, most spatial-temporal graph neural networks require pre-defined adjacency relationships, such as spatial distances in climate data or road connections in transportation networks. However, such relationships are not explicitly present in the stock market. Although ST-Transformers do not require predefined correlation relationships, existing methods still learn correlations between nodes from the perspective of time steps, and experiments show that the learned correlations are still limited. This issue is also present in STGNNs that do not require predefined adjacency matrices. In the latest research, some studies have recognized that time steps are not an ideal perspective for modeling time series. Crossformer uses both temporal and feature perspectives to model time series, while experiments with ITransformer indicate that using only features as tokens to model individual time series yields better results.\n\nWe believe that viewing time series from both feature and temporal perspectives reflects two completely different characteristics. While experiments with ITransformer have shown that modeling the time series of a single node using only features as tokens yields better results, when learning the correlations between different nodes using node time series, the feature and temporal perspectives can capture completely different node characteristics and yield two distinct types of correlations. Integrating these two different types of correlations can more comprehensively reflect the real influences between nodes. Based on this, we propose a novel method suitable for stock prediction, focusing on both the time series and the correlations between time series, named DPA-STIFormer.\n\nSpecifically, DPA-STIFormer consists of multiple Double-Path Adaptive-correlation Inverted Encoders and a single Decoder Block with Decomposed Fitting. The DPA-IEncoder includes an Inverted Temporal Block and a Double-Path Adaptive-correlation Block. The Inverted Temporal Block models the time series of each node using features as tokens. Considering that each feature has a different level of influence on the node, we introduce importance weights to ensure our method can account for the importance of tokens when calculating attention. The Double-Path Adaptive-correlation Block adaptively aggregates information from both the feature and temporal paths and learns two different types of correlations through the attention mechanism. We then propose a gating mechanism to effectively integrate information from the two paths. Following multiple encoder blocks, we introduce a specialized decoder suitable for stock prediction, which decomposes the prediction target into mean and deviation predictions, effectively handling both minor and drastic changes. In summary, the main contributions are as follows:\n\n\\begin{itemize}\n\\item We propose a novel Spatial-Temporal Transformer model, DPA-STIFormer, to address spatial-temporal prediction in stock markets. The model employs features as tokens and introduces an importance weight mechanism to better encode node characteristics.\n\n\\item We introduce DPABlock to model the correlation comprehensively from two perspectives. The Double Direction Self-adaptation Fusion mechanism is proposed to separate different features from time series. We learn the correlations through a double path approach, and a gating mechanism is proposed to integrate features from different perspectives.\n\n\\item We conducted extensive experiments on 4 stock datasets, and the experimental results demonstrate that our method can more effectively model the spatial-temporal correlations in time series.\n\\end{itemize}\n\n\\section{PRELIMINARY}\nWe first define the concepts of \\textbf{Spatial-Temporal Forecasting Problem}. The spatial-temporal forecasting problem, refers to the task of using historical $T$ time steps data $\\boldsymbol{X}\\in{\\mathbb{R}^{N \\times T \\times F}}$ and correlation adjacency matrix $\\boldsymbol{A}\\in{\\mathbb{R}^{N \\times N}}$ to predict future values for $t$ time steps $\\boldsymbol{Y}\\in{\\mathbb{R}^{N \\times t}}$.The adjacency matrix can be predefined or learned by the model, as in our approach. For each sample, there are $N$ nodes, and each node has a time series $X_i$, where $X_i$ contains $T$ time steps, and each time step has $F$ features. Additionally, the correlation adjacency matrix $A$ indicates the degree of correlation between the nodes, where $a_{ij}$ represents the correlation degree between node $i$ and node $j$. The neighbors set of node $i$ is represented as $K=\\{j\\ |\\ j \\neq i \\ and\\  a_{ij} \\neq 0\\}$.\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/Model_arch.pdf}\n    \\caption{The overview of the proposed Double-Path Adaptive-correlation Spatial-Temporal Inverted Transformer.}\n    \\label{fig:arch}\n\\end{figure*}\n\\section{METHOD}\n\nIn this section, we present our approach. The precise architecture of the DPA-STIFormer is illustrated in Fig. \\ref{fig:arch}.\n\\subsection{Overall}\nThe model is composed of multiple encoder blocks called Double-Path Adaptive-correlation Inverted Encoder (DPA-IEncoder) and a single Decoder Block with Decomposed Fitting (DF-Decoder) for stock prediction. Each DPA-IEncoder consists of an Inverted Temporal Block, a Double-Path Adaptive-correlation Block, and two feedforward neural networks, all interconnected by residual connections. The Inverted Temporal Encoder processes the time series $x_i$ for an individual node and the Double-Path Adaptive Correlation Encoder concurrently aggregates information from both temporal and feature dimensions, adaptively learns the relationships among neighboring nodes, and integrates the encoding results through a gating mechanism. Finally, the DF-Decoder demonstrates high capacity in stock prediction with proposed decomposed fitting.\n\n\\subsection{Inverted Temporal Block}\nIn many Transformer-based time series prediction approaches, features at the same time step are treated analogously to words in natural language processing. However, these methods often yield suboptimal performance. This is due to the inherently low information-to-noise ratio in time series data and the potential insignificance of individual time steps. Crucially, information is typically embedded in the temporal evolution of features rather than in isolated time points. Inspired by Itransformer\\cite{itransformer} and crossformer\\cite{crossformer} , we no longer consider features at the same time step as tokens, but rather the change of the same feature over time as a token. Thus, for the time series $x_i\\in \\mathbb{R}^{T\\times F}$ in each node, We first Invert $x_i$ to get $x_i^{I}\\in \\mathbb{R}^{F\\times T}$, rendering the temporal sequence of each feature as a token.\n\\begin{equation}\n    x^{I}_{i}=invert(x_i), for\\ i = 1,2,...,n\n\\end{equation}\nwhere $invert(\\cdot)$ stands for matrix transpose. Furthermore, considering the diverse features contained within the same node, it is evident that each feature exerts a different degree of influence on the node's variation, due to their distinct meanings. \nWe introduce importance weights for each feature. Specifically, we pass $x_i^{I}$ through a two-layer fully-connected network to get their importance $w_i$ and  apply the softmax function to ensure that $\\sum{w_i} =1$. Then, splice the $w_i$ to each token:\n\\begin{equation}\n    w_i=softmax(W_2*\\sigma(W_1*x^{I}_i+b_1)+b_2)\n\\end{equation}\n\\begin{equation}\n    \\hat{x}^{I}_i=<w_i,x^{I}_i>\n\\end{equation}\nwhere $W_1$,$W_2$ represent learnable weight matrices, $b_1$, $b_2$ denote learnable bias vectors, $\\sigma$ is the $ReLU()$ activation function, and $<\\cdot>$ indicates matrix concatenation. The set of inverted time series with importance is denoted as $\\hat{X}^{I}$.\n\\begin{equation}\n    \\hat{X}^{I}=[\\hat{x}^{I}_1,\\hat{x}^{I}_2,...,\\hat{x}^{I}_n]\n\\end{equation}\nThen, we model the time series of each node using the self-attention layer as in Transformer\\cite{vaswani2017attention}.\n\\begin{equation}\n     \\mathcal{Q}= {W_Q^T} {\\hat{X}^{I}}, \\mathcal{K}= {W_K^T} {\\hat{X}^{I}}, \\mathcal{V}= {W_V^T} {\\hat{X}^{I}}\n\\end{equation}\n\\begin{equation}\n    {O^{I}}=softmax(\\frac{ \\mathcal{Q} \\mathcal{K^T}}{\\sqrt{d_k}}) \\mathcal{V}\n\\end{equation}\nwhere  $ {W_Q}$, $ {W_K}$, $ {W_V}$ are independent weights matrix that map $ {\\hat{X}^{I}}$ to three different spaces, $O^{I}$ represents the output of the self-attention mechanism. Through this self-attention mechanism, we aim to learn the correlations between multiple features and develop better representations of each node. Subsequently, layer normalization and residual connections are applied to enhance the encoding process.\n\\begin{equation}\n    \\hat{X}^{I}_{out}=LN( O^{I}+residual(\\hat{X}^{I}))\n\\end{equation}\nwhere $LN(\\cdot)$ stand for layer normalization, and $residual(\\cdot)$ is a single-layer fully connection network to ensure equal dimension. Finally, the feedforward (FFD) neural network is applied:\n\\begin{equation}\n    Z^{I}=LN( \\hat{X}^{I}_{out}+FFD(\\hat{X}^{I}_{out}))\n\\end{equation}\n\n\\subsection{Double-Path Adaptive-correlation Block}\nDouble-path Adaptive-correlation Block (DPABlock) is designed to adaptively model the correlations between nodes. It comprises three main components: Double Direction Self-adaptation Fusion, N-neighbor Adaptive Correlation Attention, and Double-Path Gating Mechanism.\n\n\\subsubsection{Double Direction Self-adaptation Fusion}\nFor spatial-temporal prediction problems, each node is often featured as a two-dimensional time series $x_i \\in \\mathbb{R}^{T\\times F}$. The computational cost can become prohibitively large if all features are used to learn the correlations between nodes. A common approach is to use the features from the nearest time step or the mean of each time step as the node features. However, this approach has significant limitations. On the one hand, simple averaging over time steps does not provide a robust representation of the nodes. Different features hold varying levels of importance at different time steps, and averaging may dilute these distinctions, rendering the features less meaningful. On the other hand, many features of a time series are embedded in its successive changes, and aggregating multiple series into a single one may result in information loss compared to averaging over time steps.\n\nWe designed a double direction adaptive information fusion method as shown in Fig. \\ref{fig:arch}. We first invert $Z^{I}$ again to obtain $Z$,\n\\begin{equation}\n        Z=[z_{1},z_{2},...,z_{n}]\n\\end{equation}\nThen, we respectively map $Z^{I}$ and $Z$ to different spaces through fully connected layers\n\\begin{equation}\n\\begin{split}\n    \\mathcal{Q}_{F} &= {W_{Q,F}^{T}} {Z}, \\mathcal{K}_{F}= {W_{K,F}^{T}} {Z}\\\\\n    \\mathcal{Q}^{I}_{F} &= {W_{Q,F}^{I,T}} {Z^{I}}, \\mathcal{K}^{I}_{F}= {W_{K,F}^{I,T}} {Z^{I}}\n\\end{split}\n\\end{equation}\nwhere $\\mathcal{Q}_{F}$,$\\mathcal{K}_{F}$,$ \\mathcal{Q}^{I}_{F}$, and $\\mathcal{K}^{I}_{F}$ are the Query and Key of $Z$ and $Z^{I}$, respectively. They are all mapped to the same dimension. $W_{Q,F}^{T}$, $W_{K,F}^{T}$, $W_{Q,F}^{I,T}$ and ${W_{K,F}^{I,T}}$ are the corresponding weight matrices. We learn the weights of each feature at each time step by mutual querying, and we ensure that these weights sum to 1 in a certain direction using the softmax function.\n\\begin{equation}\n\\begin{split}\n\\mathcal{W}_{temp} &= softmax( \\mathcal{Q}_{F}\\mathcal{K}^{I}_{F})\\\\\n        \\mathcal{W}_{feat} &= softmax( \\mathcal{Q}^{I}_{F}\\mathcal{K}_{F})\n\\end{split}\n\\end{equation}\n\\begin{equation}\n\\begin{split}\n    H_{temp} &= Sum(W_{temp}\\cdot Z)\\\\\n       H_{feat} &= Sum(W_{feat}\\cdot Z^{I})\\\\\n\\end{split}\n\\end{equation}\nwhere $\\mathcal{W}_{temp} $ and $\\mathcal{W}_{feat}$ are weighting matrices used to summarize information in the temporal and feature directions, $Sum(\\cdot)$ is the summation of the last dimension, $H_{temp} \\in \\mathbb{R}^{N \\times T} $ and $H_{feat} \\in \\mathbb{R}^{N \\times F} $  are node representations that adaptively fusion from feature and temporal perspectives.  \n\n\\subsubsection{N-neighbor correlation attention}\nNode representation $H_{feat}$ and $H_{temp}$ each capture different characteristics of the nodes. Therefore, we adaptively learn the correlations of the nodes from both perspectives. For each node and its neighbors, a close neighbor has a positive effect on the node, whereas information from more distant neighbors tends to be ineffective or even negative as the correlation diminishes. To address this, we propose an N-neighbor correlation attention mechanism. This mechanism learns correlations between nodes based on each node representation and preserves the top-n neighbors of each node through masking. We define the process $NcorrAttn$ as follows:\n\\begin{equation}\n    \\begin{split}\n        &\\mathcal{Q}_{G}= {W_{Q,G}^{T}} {H}, \\mathcal{K}_{G}= {W_{K,G}^{T}} {H}, \\mathcal{V}_{G}= {W_{V,G}^{T}} {Z^I}\\\\\n        &NcorrAttn(H,X)=softmax(\\frac{topN(\\mathcal{Q}_{G}\\mathcal{K}_{G}^{T})}{\\sqrt{d_g}})\\mathcal{V}_{G}\n    \\end{split}\n\\end{equation}\nwhere ${W_{K,G}^{T}}$,${W_{Q,G}^{T}}$, ${W_{V,G}^{T}}$ are independent weights matrix, and $topN(\\cdot)$ represents the retention of only the largest $N$ values in each row of the matrix, with $N$ typically set to 10\\% of the number of nodes. Query $\\mathcal{Q}_{G}$ and Key $\\mathcal{K}_{G}$ are obtained by mapping node representation $H$ to different space, and Value $\\mathcal{V}_{G}$ is mapped from $Z^I$. This approach is taken because the node representation captures only one aspect of the node's characteristics and is suitable for computing the N-neighbor attention matrix. However, relying solely on this enriched aspect can lead to the loss of the original features of the time series. Therefore, the Value should still be mapped from the original time series to retain the complete information. We can get two aspects of encoding:\n\\begin{equation}\n    \\begin{split}\n        &O_{feat}= NcorrAttn(H_{feat},Z^{I})\\\\\n        &O_{temp}= NcorrAttn(H_{temp},Z^{I})\n    \\end{split}\n\\end{equation}\n\nSpecial Note that because $H_feat$, $H_temp$ are both from $Z^{I}$ and Value is also from $Z^{I}$, we use the same $\\mathcal{V}_{G}= {W_{V,G}^{T}{Z^{I}}}$ as Value to save computational resources.\n\\subsubsection{Double-path gating mechanism}\nIn N-neighbor correlation attention, we model spatial relationships from both temporal and feature perspectives simultaneously, forming the integrated spatial-temporal encodings $O_{feat}$ and $O_{temp}$. For different spatial-temporal samples, the importance of the two paths may vary. Therefore, we propose a double-path gating (DPgate) mechanism to adaptively control the transmission and fusion of information from both sides. Our mechanism is illustrated in Fig. \\ref{fig:arch}. DPgate mainly consists of two types of gates: self-passing gates and mutual-passing gates. Each path passes through a self-passing gate, which determines which information from that path passes through and which does not. Both paths simultaneously pass through a mutual-passing gate, where the proportion of information fusion is determined by the combined information from both paths. The specific calculation process is as follows:\n\\begin{equation}\n    \\begin{split}\n        &g_{feat} = tanh(W_{s,feat}^TO_{feat}+b_{s,feat})\\\\\n        &g_{temp} = tanh(W_{s,temp}^TO_{temp}+b_{s,temp})\\\\\n        &m = \\sigma(W_m^{T}\\cdot[O_{feat},O_{temp}])\\\\\n        &o_{feat}=g_{feat}*O_{feat}\\\\\n        &o_{temp}=g_{temp}*O_{temp}\\\\\n        &M=o_{feat}*m+o_{temp}*(1-m)\n    \\end{split}\n\\end{equation}\nwhere $W_{s,feat}$, $W_{s,temp}$ are weight matrices of self-passing gate on each path, $W_m^{T}$ is the weight matrix of the mutual-passing gate, $tanh(\\cdot)$ and $\\sigma(\\cdot)$ are the Tanh and Sigmoid activation functions, $g_{feat}$ and $g_{temp}$ are the self-passing gate on each path, $o_{feat}$ and $o_{temp}$ are the information passed through the self-passing gates, and $M$ is the information fused by the mutual-passing gate. The self-passing gate adjusts the effective information of each path itself, while the mutual-passing gate combines the information of both paths to control the passing ratio of the two paths.\n\n\\subsection{Decoder and decomposed fitting for stock prediction}\nAfter processing through several Encoder Blocks, the model comprehensively extracts and integrates temporal and spatial information, represented as $M \\in \\mathbb{R}^{N \\times F \\times d_g}$. Given that we have inverted the input, the encoding process is no longer sequential. Consequently, we deviate from the traditional transformer decoding approach. Instead, we employ a Multilayer Perceptron (MLP) to map the encoded information directly to the prediction target. Initially, we integrate information from various features:\n\\begin{equation}\n    \\begin{split}\n        &M_w = softmax(W_{1d}^TM)\\\\\n        &M_m = Sum(M_w\\cdot M)\n    \\end{split}\n\\end{equation}\nwhere $W_{1d}$ is an $N \\times 1$ mapping matrix. By applying the $softmax(\\cdot)$ function and the $W_1d$, , each feature is assigned a weight that reflects its importance. Then aggregates all features to form a new mixed encoding $M_m \\in \\mathbb{R}^{N \\times d_g}$.\n\nFor stock prediction problems, the mean of each stocks may be relatively stable over a period. Each time step can be seen as a deviation from this mean. We use  MLP to fit both the mean and deviation of each time series, and let the output be the sum of the mean and deviation.\n\\begin{equation}\n    \\begin{split}\n        &mean = W_{mean}^TM_m+b_{mean}\\\\\n        &dev = tanh(W_{dev}^TM_m+b_{dev})\\\\\n        &\\hat{y} = mean + e^{dev}\n    \\end{split}\n\\end{equation}\nThe prediction target is decomposed into an insensitive mean prediction and a sensitive deviation prediction. This approach enhances robustness and improves prediction performance. When each time step undergoes minor changes, the mean prediction remains relatively stable, while the sensitive deviation prediction provides an accurate fit. Conversely, when each time step experiences significant changes, the mean prediction adjusts accordingly, whereas the deviation prediction remains within a certain range, thereby preventing overfitting.\n\n\\subsubsection{Loss}\n\nTo attain high performance in time series prediction tasks, we consider both the accuracy of the predictions and the correlation between predictions across time steps in the loss function. Initially, we utilize Mean Squared Error loss $\\mathcal{L}_{mse}$ as the loss function to evaluate the accuracy of the predictions and employ the Pearson correlation coefficient loss $\\mathcal{L}_{pearson}$ to gauge the correlation of predictions within a time step.\n\nThe final form of the loss function is expressed as follows:\n\\begin{equation}\\mathcal{L}=\\lambda_{m}\\mathcal{L}_{mse}+\\mathcal{L}_{pearson}\\end{equation}\nwhere $\\lambda_m$ denotes the weight of $\\mathcal{L}_{mse}$ which is usually set to 0.1 to ensure that the two loss functions are of the same order of magnitude.\n\n\\begin{table}[htb]   \n    \\centering\n    \\small\n    \\setlength{\\tabcolsep}{1mm}\n{\n\n    \\begin{tabular}{c|c|c|c}\n        \\hline\n        $\\textbf{Dataset}$ & $\\textbf{Samples}$ & $\\textbf{Node}$ & \\textbf{ Partition}\\\\\n        \\hline\n        CSI500& 3159 & 500 & 2677/239/243\\\\ \n        CSI1000& 3159 & 1000 &  2677/239/243\\\\ \n        NASDAQ& 1225 & 1026 &  738/253/234\\\\ \n        NYSE& 1225 & 1737 &  738/253/234\\\\ \n        \\hline\n    \\end{tabular}\n    }\n    \\caption{The overall information for datasets}\n    \\label{tab:dataset}\n    \n    \n\\end{table}\n\n\\section{EXPERIMENTS}\nIn this section, we conduct experiments to demonstrate the effectiveness of our method. \n\\subsection{Experimental Setup}\n\n\\subsubsection{Datasets}\nWe conducted experiments on four datasets: CSI500, CSI1000, NASDAQ, NYSE. Brief statistical information is listed in Table \\ref{tab:dataset}. Detailed information about the datasets can be found in the appendix.\n\\subsubsection{Baseline}\nWe choose LSTM-based neural networks: FC-LSTM\\cite{FCLSTM}, TPA-LSTM\\cite{TPALSTM}, spatial-temporal graph neural networks: ASTGCN \\cite{ASTGCN}, MSTGCN \\cite{MSTGCN}, MTGNN \\cite{MTGNN}, STEMGNN \\cite{stemGNN}, STGCN \\cite{STGCN}, DCRNN\\cite{DCRNN}\t and transformer-based networks: Itransformer\\cite{itransformer},Crossformer\\cite{crossformer} as baseline to compare the performance of the models in all directions.\n\n\\subsubsection{Metrics}\nWe evaluate the performances of all baseline by nine metrics which are commonly used in stock prediction task including Information Coefficient (IC), Profits and Losses (PNL), Annual Return (A\\_RET), Volatility (A\\_VOL), Max Drawdown (MAXD), Sharpe Ratio (SHARPE), Win Rate (WINR), Profit/Loss Ratio (PL).\n\n\\begin{table*}[htbp]\n  \\centering\n\n    \\scriptsize\n    \n    \\setlength{\\tabcolsep}{1mm}\n  {\n  \\scalebox{1}{\n    \\begin{tabular}{cc|cccccccc|cccccccc}\n    \\toprule\n    \\multicolumn{2}{c|}{\\multirow{2}[4]{*}{}} & \\multicolumn{8}{c|}{\\textbf{CSI500}}                          & \\multicolumn{8}{c}{\\textbf{CSI1000}} \\\\\n\\cmidrule{3-18}    \\multicolumn{2}{c|}{} & IC    & PNL   & A\\_RET & A\\_VOL & MAXD  & SHARPE & WINR  & PL    & IC    & PNL   & A\\_RET & A\\_VOL & MAXD  & SHARPE & WINR  & PL \\\\\n    \\midrule\n    \\multirow{6}[2]{*}{\\textbf{STGNN}} & ASTGCN & 0.1022  & 0.3758  & 0.3727  & 0.0872  & 0.1813  & 4.2740  & 0.6281  & 2.0069  & 0.0840  & 0.7716  & 0.7652  & 0.1019  & 0.1285  & 6.8903  & 0.6694  & 3.0884  \\\\\n          & STGCN & 0.0349  & 0.0466  & 0.0462  & 0.0676  & 0.1693  & 0.6841  & 0.5537  & 1.1236  & 0.0404  & 0.0388  & 0.0385  & 0.0865  & 0.2850  & 0.4449  & 0.5000  & 0.9310  \\\\\n          & MSTGCN & 0.0732  & 0.0328  & 0.0325  & 0.0694  & 0.2246  & 0.4692  & 0.5793  & 1.9280  & 0.0919  & 0.8804  & 0.8732  & \\textbf{0.1180 } & 0.1333  & 7.3996  & 0.6901  & 3.2232  \\\\\n          & MTGNN & 0.1176  & 0.4652  & 0.4614  & 0.1006  & 0.1652  & 4.5845  & 0.6116  & 2.1265  & 0.0941  & 1.1608  & 1.1512  & 0.0756  & \\textbf{0.0324 } & \\textbf{15.2259 } & 0.7769  & \\textbf{23.4614 } \\\\\n          & STEMGNN & 0.0845  & 0.2124  & 0.2107  & 0.0842  & 0.1223  & 2.5016  & 0.5702  & 1.5455  & 0.0176  & 0.0154  & 0.0153  & 0.0205  & 0.0141  & 0.7448  & 0.0455  & 1.6529  \\\\\n          & DCRNN & 0.0609  & 0.0899  & 0.0892  & 0.0839  & 0.2084  & 1.0630  & 0.5289  & 1.1794  & 0.0420  & 0.1080  & 0.1071  & 0.0850  & 0.2552  & 1.2591  & 0.4793  & 0.8216  \\\\\n\\cmidrule{1-2}    \\multirow{2}[2]{*}{\\textbf{RNN}} & TPA\\_LSTM & 0.0052  & -0.0959  & -0.0951  & 0.0717  & 0.2364  & -1.3272  & 0.4959  & 0.7986  & 0.0344  & 0.0413  & 0.0409  & 0.0811  & 0.2304  & 0.5048  & 0.4959  & 0.9201  \\\\\n          & FC\\_LSTM & 0.1193  & 0.5402  & 0.5357  & \\textbf{0.1056 } & 0.1428  & 5.0750  & 0.6033  & 2.3893  & 0.0482  & 1.0291  & 1.0206  & 0.1041  & 0.1053  & 9.8080  & 0.7190  & 5.4323  \\\\\n\\cmidrule{1-2}    \\multirow{2}[2]{*}{\\textbf{FORMER}} & CROSSFORMER & 0.0134  & 0.1447  & 0.1435  & 0.0779  & 0.2596  & 1.8423  & 0.4463  & 0.7295  & 0.0365  & 0.3537  & 0.3508  & 0.0964  & 0.2412  & 3.6383  & 0.5909  & 1.7650  \\\\\n          & ITRANSFORMER & 0.0225  & -0.0517  & -0.0513  & 0.1031  & 0.2853  & -0.4976  & 0.5331  & 0.9226  & 0.0391  & 0.1947  & 0.1931  & 0.0945  & 0.2717  & 2.0267  & 0.5614  & 1.3757  \\\\\n    \\midrule\n          & DPA-STIFormer    & \\textbf{0.1312 } & \\textbf{0.8794 } & \\textbf{0.8721 } & 0.1033  & 0.1164  & \\textbf{8.4395 } & \\textbf{0.7273 } & \\textbf{4.2453 } & \\textbf{0.1404 } & \\textbf{1.2809 } & \\textbf{1.2703 } & 0.1111  & 0.0791  & 12.4611  & \\textbf{0.8058 } & 9.2447  \\\\\n    \\midrule\n    \\multicolumn{2}{c|}{\\multirow{2}[4]{*}{}} & \\multicolumn{8}{c|}{\\textbf{NYSE}}                            & \\multicolumn{8}{c}{\\textbf{NASDAQ}} \\\\\n\\cmidrule{3-18}    \\multicolumn{2}{c|}{} & IC    & PNL   & A\\_RET & A\\_VOL & MAXD  & SHARPE & WINR  & PL    & IC    & PNL   & A\\_RET & A\\_VOL & MAXD  & SHARPE & WINR  & PL \\\\\n    \\midrule\n    \\multirow{6}[2]{*}{\\textbf{STGNN}} & ASTGCN & 0.0400  & 0.0314  & 0.0322  & 0.3251  & 0.3936  & 0.9893  & 0.5171  & 0.8252  & 0.0323  & 0.2578  & 0.2644  & 0.1879  & 0.1369  & 1.4070  & 0.5128  & 1.2688  \\\\\n          & STGCN & 0.0150  & 0.1009  & 0.1034  & 0.2707  & 0.1926  & 0.3821  & 0.5128  & 1.0667  & 0.0383  & 0.3145  & 0.3225  & 0.2408  & 0.1766  & 1.3396  & 0.5085  & 1.2675  \\\\\n          & MSTGCN & 0.0351  & 0.0767  & 0.0786  & 0.2887  & 0.3035  & 0.2723  & 0.5085  & 1.0489  & 0.0227  & 0.2588  & 0.2655  & 0.1776  & \\textbf{0.0973 } & 1.1524  & 0.4915  & \\textbf{1.2845 } \\\\\n          & MTGNN & -0.0035  & -0.0291  & -0.0299  & 0.1209  & \\textbf{0.0604 } & -0.1000  & 0.4957  & 0.9831  & 0.0163  & 0.1375  & 0.1410  & 0.1537  & 0.1076  & 0.9179  & 0.5385  & 1.1803  \\\\\n          & STEMGNN & 0.0254  & -0.1557  & -0.1597  & 0.2335  & 0.2691  & -0.6839  & 0.4915  & 0.8772  & 0.0171  & 0.0273  & 0.0280  & 0.2240  & 0.2597  & 0.1249  & 0.4786  & 1.0224  \\\\\n          & DCRNN & 0.0016  & 0.0122  & 0.0125  & 0.1642  & 0.2281  & 0.0763  & 0.4701  & 1.0128  & -0.0056  & 0.0253  & 0.0259  & 0.1394  & 0.1273  & 0.1858  & 0.5284  & 1.0325  \\\\\n\\cmidrule{1-2}    \\multirow{2}[2]{*}{\\textbf{RNN}} & TPA\\_LSTM & 0.0260  & 0.1729  & 0.1774  & 0.2557  & 0.3244  & 0.6937  & 0.5385  & \\textbf{1.2037 } & 0.0270  & -0.0899  & -0.0922  & 0.2065  & 0.3087  & -0.4465  & 0.4786  & 0.9253  \\\\\n          & FC\\_LSTM & -0.0012  & 0.1255  & 0.1287  & \\textbf{0.3391 } & 0.3694  & 0.3795  & 0.4872  & 1.0676  & 0.0083  & 0.2479  & 0.2542  & 0.2206  & 0.2393  & 1.2508  & 0.4829  & 1.2476  \\\\\n\\cmidrule{1-2}    \\multirow{2}[2]{*}{\\textbf{FORMER}} & CROSSFORMER & 0.0097  & 0.0204  & 0.0209  & 0.2767  & 0.2766  & 0.0756  & 0.5128  & 1.0128  & 0.0231  & 0.0611  & 0.0627  & 0.2081  & 0.1436  & 0.3010  & 0.4786  & 1.0544  \\\\\n          & ITRANSFORMER & 0.0011  & -0.5296  & -0.5431  & 0.1141  & 0.5510  & -4.7592  & 0.4316  & 0.3288  & -0.0097  & -0.0931  & -0.0954  & 0.1158  & 0.1185  & -0.8241  & 0.5369  & 0.5229  \\\\\n    \\midrule\n          & DPA-STIFormer    & \\textbf{0.0453 } & \\textbf{0.2135 } & \\textbf{0.2190 } & 0.2987  & 0.1999  & \\textbf{1.8115 } & \\textbf{0.5427 } & 1.1256  & \\textbf{0.0416 } & \\textbf{0.3902 } & \\textbf{0.4002 } & \\textbf{0.3199 } & 0.1462  & \\textbf{1.4945 } & \\textbf{0.5470 } & 1.2320  \\\\\n    \\bottomrule\n    \\end{tabular}%\n    }\n  }\n  \\label{tab:main}%\n    \\caption{Comparison results on four datasets. $\\downarrow$ indicates that the smaller the metric is better. The best result is in bold.}\n\\end{table*}%\n\n\\subsection{Main Results}\nWe compare our proposed model DPA-STIFormer, with 10 baseline models, encompassing three common types of models in time series forecasting: RNNs, Transformers, and STGNNs. The comparison results are presented in Table 2. Overall, our model achieves the best results across all four datasets. Our model consistently attains optimal results on multiple metrics simultaneously, and its performance improves with larger datasets. Compared to STGNNs such as ASTGCN, our model improves IC by an average of 30\\%, A\\_RET by an average of 20\\%, and WINR by more than 3\\% on average. The significant improvement is attributed to our model's ability to comprehensively capture the correlations between node time series. Many spatial-temporal graph neural networks rely on predefined graphs, or the relationships between nodes learned by these models are not sufficiently comprehensive. This also underscores the effectiveness of our DPABlock.\n\nCompared to RNNs, our model demonstrates superior performance, particularly in terms of IC. On average, our model achieves more than a 30\\% improvement in both PNL and A\\_RET. Additionally, SHARPE shows significant improvement in the CSI500, NASDAQ, and NYSE datasets, ranking second only to MTGNN in the CSI1000 dataset. In comparison to Transformer-based models, our model also outperforms significantly. Transformers models, exhibit much lower IC values than our model and STGNNs. Metrics such as WINR and A\\_RET further highlight that former-based models perform poorly in stock prediction without this crucial node correlation information. This also indicates the ineffectiveness of using time as tokens for modeling time series, aligning with the findings of \\citet{itransformer}. Our model, on the other hand, uses features as tokens to model time series and employs a double-path approach to comprehensively capture the correlations between nodes. Using a gating mechanism, it effectively integrates information from both node correlations and time series. The experimental results validate the effectiveness of our method.\n\n\\begin{table}[htbp]\n  \\centering\n    \\small\n    \\setlength{\\tabcolsep}{1mm}\n{\n    \\begin{tabular}{c|ccc}\n    \\toprule\n          & IC    & A\\_RET & SHARPE \\\\\n    \\midrule\n     DPA-STIFormer    & 0.1312  & 0.8721  & 8.4395  \\\\\n    w/o DPgate & 0.1170  & 0.4804  & 4.4671  \\\\\n    w/o temporal path & 0.1135  & 0.6465  & 6.7517  \\\\\n    w/o feature path & 0.1190  & 0.5154  & 6.1406  \\\\\n    w/o Inverted Temporal Block & 0.1010  & 0.5819  & 5.6339  \\\\\n    w/o importance weigh & 0.1100  & 0.6142  & 5.0505  \\\\\n    \\bottomrule\n    \\end{tabular}%\n}\n  \\label{tab:ablation}%\n    \\caption{Ablation Study}\n\\end{table}%\n\n\\subsection{Ablation Study}\nTo validate the effectiveness of the key components, we conduct an ablation study on the CSI500 dataset, comparing the changes in IC, A\\_RET, and SHARPE. Specifically, we tested the performance impact of removing the following components: DPgate, temporal path, feature path, Inverted Temporal Block, and importance weight. Firstly, removing any path from the double-path structure or the final gating mechanism during data fusion results in approximately a 15\\% decrease in IC, with varying degrees of decrease in A\\_RET and SHARPE. The most severe drop occurs when the gating mechanism is removed, resulting in about a 25\\% decrease. This indicates that removing any path leads to biased learning of correlations. Removing the gating mechanism and merging the information with equal weights leads to rough information fusion, causing adverse effects. From the changes in A\\_RET and SHARPE, it is clear that the information learned from the feature path has a greater impact on the results. \n\nCompared to the model without the Inverted Temporal Block, removing this block results in a significant decrease in IC and SHARPE. This indicates a decline in overall predictive performance and increased instability, as the absence of individual denoising and modeling of each time series negatively affects the modeling of correlations. For the model without importance weight, the results show that including feature importance enhances overall predictive capability and returns, enabling the model to better utilize different features to model node characteristics. However, the decrease in SHARPE suggests that the addition of importance weight also leads to an increase in risk.\n\n\\begin{figure}[thbp]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/param.pdf}\n    \\caption[width=\\textwidth]{Hyper-parameter Study}\n    \\label{fig:params}\n\\end{figure}\n\n\\subsection{Hyper-parameter Study}\n\nWe conducted extensive parameter experiments on the CSI500 dataset to examine the effects of Layer Number $n_l$, the number of attention heads $n_h$, and the dimensions of the feedforward neural network $d_{model}$. For $n_l$, we found that the model performs best with 3 layers. When the number of layers is less than 3, the IC is relatively low, indicating that the model suffers from overfitting due to insufficient parameters, leading to inadequate overall predictive capability. When the model has more than 3 layers, there is a significant decrease in WINR, likely because the data is insufficient to support training, resulting in underfitting. Since each additional layer significantly increases the model parameters, only 1 or 2 layers are needed for smaller datasets. Regarding $n_h$,, the optimal performance was observed with 4 heads. With fewer heads, the model cannot effectively capture all relationships. Conversely, with too many heads, the model tends to overfit, diminishing its overall predictive capability. Comparing different $d_{model}$ , we see that smaller dimensions greatly impact model performance, because each token cannot be fully encoded and distinguished. However, when the $d_{model}$ is too large, the model's parameter count increases rapidly, leading to gradual underfitting.\n\\subsection{Visualization}\n\\subsubsection{Double-path Visualization}\n\\begin{figure}[thbp]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/attn_vir.pdf}\n    \\caption[width=\\textwidth]{Visualization of Double-path Attention}\n    \\label{fig:attn_vir}\n\\end{figure}\nTo deeply analyze the Double-path adaptive correlation block, we visualized the attention computation results of the final layer of the Double-path attention mechanism. It can be observed that the attention matrix obtained from the N-neighbor correlation attention is sparse, with each node focusing on its nearest N nodes. By comparing the two attention matrices, we can find that the Feature-path and Temporal-path have learned two different types of adjacency relationships. In the Temporal-path Attention, there are certain leading nodes that influence all other nodes. Conversely, in the Feature-path Attention, the adjacency relationships between nodes are more complex, with the N-neighbors of different nodes being more diversified.\n\n\\subsubsection{Mean-deviation Prediction Visualization}\n\n\\begin{figure}[thbp]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/pred_vir.pdf}\n    \\caption[width=\\textwidth]{Visualization of Mean-deviation Prediction}\n    \\label{fig:pred_vir}\n\\end{figure}\nWe visualize the distributions of mean and deviation for the short and long term to better illustrate the advantages of our Mean-deviation Prediction. The short-term refers to a period of continuous time-step prediction, and it can be seen that the distribution of the mean is denser while the distribution of the deviation is sparser, which indicates that in continuous prediction, the mean of our prediction method varies slowly, fitting the variation of the time series through the deviation. Long-term refers to the prediction after an interval of several time steps, where the range of the mean value has changed significantly while the deviation is still concentrated in a smaller range. This indicates that after an interval of time steps, Mean-deviation Prediction relies on the mean value to fit the drastic change in the time series and avoids too large a change in the deviation.\n\n\\section{RELATED WORK}\n\\subsection{Spatial-Temporal Graph Neural Networks}\nIn recent years, there has been a burgeoning interest in integrating temporal and spatial information within time series data, leading to the development of a series of models known as Spatial-Temporal Graph Neural Networks. STGCN \\cite{STGCN} was the first to introduce Graph Convolutional Neural Networks (GCNs) \\cite{GCN} into time series forecasting. Subsequently, additional convolution-based models, such as Graph Wave Net \\cite{Gwave}, MT-GNN \\cite{MTGNN}, StemGNN \\cite{stemGNN}, H-STGCN \\cite{H-STGCN}, and GSTNet \\cite{GSTNet}, have been proposed. These models incorporate various gating mechanisms atop convolutions to enhance feature capture. Concurrently, other studies have focused on more complex convolutional structures, such as ST-GDN [31] and ST-ResNet \\cite{ST-GDN}, which achieve improved performance through sophisticated architectural designs and mechanisms.\n\nMoreover, some works, including ARGCN \\cite{ARGCN}, DCRNN \\cite{DCRNN}, and TGCN \\cite{TGCN}, combine Recurrent Neural Networks (RNNs) with Graph Neural Networks (GNNs).\nAdditionally, with the advent of Transformers, many models have integrated transformer architectures or attention mechanisms into spatial-temporal modeling, as evidenced by ASTGCN \\cite{ASTGCN}, STGNN \\cite{sTGNN}, and GMAN \\cite{GMAN}.\n\\subsection{Spatial-Temporal Transformer}\nRecent advancements in spatial-temporal modeling have seen the integration of Transformers to capture complex dependencies across time and space. Notable models such as the TFT \\cite{TFT} and the STTN \\cite{STTN} have demonstrated the efficacy of combining temporal and spatial attention mechanisms. These models leverage the self-attention mechanism to effectively handle the intricacies of spatial-temporal data, thereby improving forecasting accuracy and interpretability.\n\n\\section{CONCLUSION}\n\nIn this paper, we  proposes a novel DPA-STIFormer. The Inverted Temporal Block models the temporal sequences of each node by treating features as tokens and introduces importance weights to enable the attention mechanism to consider the significance of features. The Double-Path Adaptive-correlation Block is introduced to model the correlations between nodes. The Double Direction Self-adaptation Fusion adaptively blends temporal and feature perspectives from the node embeddings, modeling inter-node correlations from both paths simultaneously. Finally, the proposed Double-path Gating Mechanism integrates the encodings from both paths. Extensive experiments on four real-world stock datasets, along with visualization of the learned correlations, demonstrate the superiority of our approach.\n\n\\clearpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{GARCH-Informed Neural Networks for Volatility Prediction in Financial Markets}\n\n\\begin{document}\n\n\\title{GARCH-Informed Neural Networks for Volatility Prediction in Financial Markets}\n\n\\author{\\IEEEauthorblockN{Zeda Xu}\n\\IEEEauthorblockA{\\textit{College of Engineering} \\\\\n\\textit{Carnegie Mellon University}\\\\\nPittsburgh, PA, USA, 15213 \\\\\nzedaxu@cmu.edu}\n\\and\n\n\\IEEEauthorblockN{John Liechty}\n\\IEEEauthorblockA{\\textit{Smeal College of Business} \\\\\n\\textit{Pennsylvania State University}\\\\\nUniversity Park, PA, USA, 16802 \\\\\njcl12@psu.edu}\n\\and\n\n\\IEEEauthorblockN{Sebastian Benthall}\n\\IEEEauthorblockA{\\textit{School of Law} \\\\\n\\textit{New York University}\\\\\nNew York City, NY, USA, 10012 \\\\\nspb413@nyu.edu}\n\\and\n\n\\IEEEauthorblockN{Nicholas Skar-Gislinge}\n\\IEEEauthorblockA{\n\\textit{Lund University}\\\\\nLund, Sweden \\\\\nnskg@pm.me}\n\\and\n\n\\IEEEauthorblockN{Christopher McComb}\n\\IEEEauthorblockA{\\textit{College of Engineering} \\\\\n\\textit{Carnegie Mellon University}\\\\\nPittsburgh, PA, USA, 15213 \\\\\nccm@cmu.edu}\n}\n\n\\maketitle\n\n\\begin{abstract}\n    Volatility, which indicates the dispersion of returns, is a crucial measure of risk and is hence used extensively for pricing and discriminating between different financial investments. As a result, accurate volatility prediction receives extensive attention. The Generalized Autoregressive Conditional Heteroscedasticity (GARCH) model and its succeeding variants are well established models for stock volatility forecasting. More recently, deep learning models have gained popularity in volatility prediction as they demonstrated promising accuracy in certain time series prediction tasks. Inspired by Physics-Informed Neural Networks (PINN), we constructed a new, hybrid Deep Learning model that combines the strengths of GARCH with the flexibility of a Long Short-Term Memory (LSTM) Deep Neural Network (DNN), thus capturing and forecasting market volatility more accurately than either class of models are capable of on their own. We refer to this novel model as a GARCH-Informed Neural Network (GINN). When compared to other time series models, GINN showed superior out-of-sample prediction performance in terms of the Coefficient of Determination ($R^2$), Mean Squared Error (MSE), and Mean Absolute Error (MAE). \n\\end{abstract}\n\n\\begin{IEEEkeywords}\nneural networks, hybrid model, volatility prediction, physics informed machine learning\n\\end{IEEEkeywords}\n\n\\section{Introduction}\nPrediction of market trends has long been an interest of the finance community \\cite{henrique_literature_2019}. Numerous mathematical models have been proposed to fit and extrapolate historical data, with the goal of gaining exclusive insights into the future. Those models often assume that market occurrences will repeat themselves and, thus, the future is predictable based on historical data \\cite{abu-mostafa_introduction_1996}. Stock price time series are noisy and volatile, and there is little to no predictability in the first moment of how a stock price changes over time \\cite{abu-mostafa_introduction_1996}. There is, however, considerable structure and predictability in the second moment or volatility, which reflects the riskiness of a stock \\cite{kim_forecasting_2018, markowitz_portfolio_1952}. Volatility prediction models aim to match and forecast the dispersion of the returns (typically on a log scale) of stock prices. These volatility predictions carry significant meaning and can inform investment decisions \\cite{kim_forecasting_2018, koo_hybrid_2022}. \n\nOne of the most widely used models for financial times series is the Autoregressive Conditional Heteroscedasticity (ARCH) model and many of its succeeding variations \\cite{koo_hybrid_2022, engle_autoregressive_1982, alam_forecasting_2013}. It captures the heteroscedasticity (non-constant variance) that is present in most real-world data, enabling the estimation of current conditional variance based on previous values \\cite{kim_forecasting_2018, engle_autoregressive_1982}. A generalized version of ARCH, GARCH (Generalized-ARCH) is a natural extension and widely considered as a standard model for forecasting stock volatility \\cite{bollerslev_generalized_1986, hansen_forecast_2005, franses_forecasting_1996}. A large number of extensions of the GARCH models have subsequently been introduced, including the Exponential GARCH (EGARCH), the Glosten-Jagannathan-Runkle GARCH (GJR-GARCH) model, and the Threshold GARCH (TGARCH) model, all of which attempt to capture the leverage effect in financial time series with asymmetric volatility terms \\cite{nelson_conditional_1991, glosten_relation_1993, zakoian_threshold_1994, kluppelberg_continuous-time_2004, li_zd-garch_2018}. Overall, the GARCH model and its extensions have shown decent performance in financial volatility forecasting tasks, but the performance of the models isn’t generalizable to all market conditions, and they are known to fail in certain market index forecasting applications \\cite{franses_forecasting_1996, liu_volatility_2009, lee_are_1991, mcmillan_forecasting_2000}. This can be attributed to the fact that models in the ARCH family are highly linear, and therefore lack the capability to capture non-linear market features \\cite{koo_hybrid_2022, lee_are_1991}. This characteristic hinders their prediction performance in out-of-sample time series prediction tasks. \n\nMachine Learning (ML) models have gained considerable attention over the years for their predictive performance and self-adaptability in many complex problems. They are therefore perceived, by some, as better modeling tools than legacy statistical models \\cite{yu_neural-network-based_2009, ahmed_empirical_2010, shen_novel_2020}. Artificial Neural Network (ANN) models are specifically regarded as some of the most accurate and universal models in time series modeling in areas including engineering, economy, and finance \\cite{kohzadi_comparison_1996, khashei_artificial_2010, zhang_neural_2005}. As universal function approximators, they are a strong competitor for non-linear data modeling \\cite{kim_forecasting_2018, koo_hybrid_2022, ciuca_layered_1997, schafer_recurrent_2006}. In this work, we focus our attention on a special case of ANN, the Long-Short Term Memory (LSTM) model, which has shown superiority in prediction accuracy, universality, and adaptivity in volatility modeling and many other time series forecasting tasks \\cite{kim_forecasting_2018, gamboa_deep_2017, hochreiter_long_1997, maknickiene_application_2012, chen_lstm-based_2015}. However, one common problem of the ANN models, including the LSTM model, is overfitting, where the model adheres too closely to in-sample data and loses generalizability on out-of-sample predictions. \n\nOur primary contribution draws inspiration from Physics Informed Machine Learning (PIML), an emerging class of ML that directly embeds physical laws into the architecture or loss function of a model, thereby improving generalizability and veracity \\cite{raissi_physics-informed_2019, karniadakis_physics-informed_2021, chen_accelerating_2023}. In the same way that PIML merges ML fundamentals with physical laws, this paper merges ML with stylized facts, empirical market patterns captured by the GARCH model, which serve as the “physical laws” encoded in the model. We refer to our novel hybrid model as the  GARCH-Informed Neural Network (GINN). Specifically, the GARCH model serves as a regularization mechanism incorporated in the loss function of the ANN, guarding against overfitting. Since the GINN model learns from both the factual ground truth and the knowledge acquired by the GARCH model, we expect it to grasp both general market trends and finer details.  We hypothesize that the combination of a classic statistical approach with modern machine learning will result in a highly accurate and generalizable model. \n\nFour models were selected as baseline models for performance comparison, namely the GARCH model, the GJR-GARCH model, the TGARCH model, and a simple, non-hybrid LSTM Neural Network model. All the models were trained and tested on 7 representative stock market indices from across the globe. The prediction accuracy is evaluated by the Coefficient of Determination ($R^2$), Mean Squared Error (MSE), and Mean Absolute Error (MAE).  \n\n\\section{Methodology}\n\\subsection{Volatility Process Modeling}\nThis paper focuses on stock market volatility, which is represented by the variance in daily log return. We use the daily closing prices to calculate daily log returns. \n\nDenote the daily stock close price as $P_t$, where $t$ represents time. The daily stock log return is then:\n\\begin{equation}\n  r_t = \\ln{\\left( \\frac{ P_t }{ P_{t-1} } \\right)}\n\\end{equation}\nHere, $r_t$ is the daily log return at day $t$, $P_t$ is the daily close stock price at day $t$, and $P_{t-1}$ is the daily close stock price at the previous day $t-1$. \n\nThe daily log return time series $r_t$ is modeled as a linear combination of the predictable average $\\mu_t$ and an unpredictable error term $\\varepsilon_t$. That is, \n\\begin{equation}\n  r_t = \\mu _t + \\varepsilon _t\n\\end{equation}\nThe error term $\\varepsilon_t$ consists of a normally distributed random noise $e_t$ and the conditional variance $\\sigma _t^2$ that is based on past information of the time series. Different variants of the GARCH-type models have different modeling of the volatility process \\cite{bollerslev_generalized_1986, glosten_relation_1993, zakoian_threshold_1994, sheppard_bashtagearch_2023}. That is, \n\\begin{equation}\n  \\varepsilon_t = \\sigma_t e_t, \n  e_t \\overset{\\mathrm{iid}}{\\sim} N(0,1)\n\\end{equation}\n\n\\begin{itemize}\n\\item {GARCH}: \n\n\\begin{equation}\n  \\sigma_t^2 = \\alpha_0 \n  + \\sum_{i=1}^{q} \\alpha_i \\varepsilon_{t-i}^2\n  + \\sum_{j=1}^{p} \\beta_j \\sigma_{t-j}^2\n\\end{equation}\n\n\\item {GJR-GARCH}: \n\n\\begin{subequations}\n\\begin{equation}\n  \\sigma_t^2 = \\alpha_0 \n  + \\sum_{i=1}^{q} \\alpha_i \\varepsilon_{t-i}^2\n  + \\sum_{j=1}^{o} \\gamma_j \\varepsilon_{t-j}^2 I\n  + \\sum_{k=1}^{p} \\beta_k \\sigma_{t-k}^2\n\\end{equation}\n\\begin{equation}\n  I=\n    \\begin{cases}\n      1, & \\text{if}\\ \\varepsilon _{t-j} < 0 \\\\\n      0, & \\text{otherwise}\n    \\end{cases}\n\\end{equation}\n\\end{subequations}\n\n\\item {TGARCH}: \n\n\\begin{subequations}\n\\begin{equation}\n  \\sigma_t^2 = \\alpha_0 \n  + \\sum_{i=1}^{q} \\alpha_i |\\varepsilon_{t-i}|\n  + \\sum_{j=1}^{o} \\gamma_j |\\varepsilon_{t-j}| I\n  + \\sum_{k=1}^{p} \\beta_k \\sigma_{t-k}\n\\end{equation}\n\\begin{equation}\n  I=\n    \\begin{cases}\n      1, & \\text{if}\\ \\varepsilon _{t-j} < 0 \\\\\n      0, & \\text{otherwise}\n    \\end{cases}\n\\end{equation}\n\\end{subequations}\n\n\\end{itemize}\n, where $\\alpha$s, $\\beta$s, and $\\gamma$s are the coefficients, $\\varepsilon$s are the error terms, and $\\sigma$s are the variances. \n\nFor the LSTM and GINN model, we model the daily log return time series $r_t$ as normally distributed with an average value $\\mu_t$ and a standard deviation $\\sigma_t$. That is, \n\\begin{equation}\n  r_t \\overset{\\mathrm{iid}}{\\sim} N(\\mu _t, \\sigma _t)\n\\end{equation}\nThe volatility could then be predicted from past volatility values: \n\\begin{equation}\n  \\sigma_t^2 = f (\\sigma_{past}^2)\n\\end{equation}\nFor a given stock log return time series $r_t$, denote the true daily volatility as $\\sigma_t^2$ and the predicted daily volatility as $\\hat{\\sigma}_t^2$. The aim of the time series models is to forecast the variance $\\hat{\\sigma}_t^2$ for the day $t$ on a rolling basis based on the past stock log return values $r_t$ of a finite time window. \n\n\\subsection{Comparison Model Selection}\nTo set a baseline for contemporary time-series modeling, and a meaningful performance comparison for the new GINN model, several variants of the GARCH models were selected to be the baseline models for performance evaluation. Namely, the (1,1) GARCH model, the (1,1,1) GJR-GARCH model, and the (1,1,1) TGARCH model were selected as statistical models for comparison. They were selected for their representativeness, performance, and wide recognition in the time-series field. For simplicity, the $(p,o,q)$ values of the ARCH-like models will be dropped from the name in the following sections of the paper. All GARCH-type modeling and related calculations are done in Python with the \\textit{arch} library by Kevin Sheppard \\cite{sheppard_bashtagearch_2023}. A naïve LSTM model was selected as an additional baseline model, representing the modern ML models in the time-series forecasting domain. The LSTM model and the LSTM component of the GINN model are deployed, trained, and tested with the \\textit{PyTorch} library \\cite{paszke_pytorch_2019}.\n\n\\subsection{GARCH-Type Model Formulation}\nThe GARCH-type models construct their volatility process upon an autoregressive (AR) estimation of the average daily log return $\\hat{\\mu}_t$, with a daily log return time series input $r_t$. All selected GARCH-type models were configured in the same way to make rolling predictions. More specifically, the GARCH-type models would forecast the variance $\\hat{\\sigma}_t^2$ for the day $t$ on a rolling basis with the mean estimation from an AR model using the daily stock log return $r_t$ for the past 90 days $(r_{t-90},r_{t-89},r_{t-88},...,r_{t-1})$. Thus, we have:\n\\begin{equation}\n    \\hat{\\sigma}_{t_{GARCH \\enspace Type}}^2 = G (r_{t-90},r_{t-89},r_{t-88},...,r_{t-1}) \n\\end{equation}\nwhere $G$ indicates any GARCH-type model used in this work. The variance prediction workflow diagram for GARCH-type models is shown in Figure~\\ref{GARCHWF}.\n\n\\begin{figure}[h]\n  \\centering\n  \\includegraphics[width=\\linewidth]{GARCH_type_workflow.png}\n  \\caption{The variance prediction workflow diagram for GARCH-type models}\n  \\label{GARCHWF}\n\\end{figure}\n\n\\subsection{LSTM Model Formulation}\n\\label{LSTMsection}\nThe procedure for the LSTM model is slightly different, as it relies on the historical ground truth variance $\\sigma_t^2$ for training. The predicted daily log returns $\\hat{\\mu}_t$ from another model are needed to obtain the ground truth variance. This is also true for the GINN model. Here, we use the same AR model as the GARCH model to obtain predictions of the average daily log returns $\\hat{\\mu}_t$. The AR model forecasts the average predicted daily log returns $\\hat{\\mu}_t$ for the day $t$ on a rolling basis using the daily stock log returns $r_t$ for the past 90 days $(r_{t-90},r_{t-89},r_{t-88},...,r_{t-1})$. \n\\begin{equation}\n    \\hat{\\mu}_t = AR (r_{t-90},r_{t-89},r_{t-88},...,r_{t-1}) \n\\end{equation}\nThe ground truth variance is then: \n\\begin{equation}\n    \\sigma_t^2 = \\left( r_t - \\hat{\\mu}_t \\right) ^2\n\\end{equation}\n\nThe LSTM model then predicts the variance $\\hat{\\sigma}_{t_{LSTM}}^2$ for the day $t$ on a rolling basis, using $\\sigma_t^2$ for the past 90 days $(\\sigma_{t-90}^2, \\sigma_{t-89}^2, \\sigma_{t-88}^2,...,\\sigma_{t-1}^2)$. We then have: \n\\begin{equation}\n    \\hat{\\sigma}_{t_{LSTM}}^2 = LSTM (\\sigma_{t-90}^2, \\sigma_{t-89}^2, \\sigma_{t-88}^2,...,\\sigma_{t-1}^2)\n\\end{equation}\nThe variance prediction workflow chart is shown in Figure~\\ref{LSTMWF}.\n\n\\begin{figure}[h]\n  \\centering\n  \\includegraphics[width=\\linewidth]{LSTM_workflow.png}\n  \\caption{The variance prediction workflow diagram for the LSTM model}\n  \\label{LSTMWF}\n\\end{figure}\n\nThe LSTM model structure is shown below. The model architecture is optimized for prediction performance and represents a modern LSTM time series model with network components commonly used in similar models. The model is sufficiently large and deep, as models with more layers and parameters obtained no meaningful performance gain in our testing. The Neural Network has 3 LSTM layers with 256 layer width and Dropout layers in between, followed by two linear layers, 1 BatchNorm layer, and 1 ReLU layer as the activation function \\cite{ioffe_batch_2015, agarap_deep_2018, srivastava_dropout_2014}. The model converges with the AdamW optimizer to minimize the Mean Squared Error (MSE) on predicted variance compared to the ground truth \\cite{loshchilov_decoupled_2017}. \n\n\\begin{figure}[h]\n  \\centering\n  \\includegraphics[width=\\linewidth]{LSTM.png}\n  \\caption{The model structure of the LSTM model}\n\\end{figure}\n\n\\subsection{GINN Model Formulation}\nThe proposed GINN model is a combination of a GARCH model and an LSTM model. It divides the time series forecasting task into two phases: initial prediction and calibration. The initial prediction phase uses a GARCH model, and the calibration phase is achieved with an LSTM model. Similar to the LSTM model introduced in Section~\\ref{LSTMsection}, the GINN model also needs the ground truth variance $\\sigma_t^2$ for training. In the initial prediction phase, an AR model and a constant mean (1, 1) GARCH model first yield the predicted average daily log return $\\hat{\\mu}_t$ and variance $\\hat{\\sigma}_t^2$ for the day $t$ in a rolling basis using the daily stock log returns $r_t$ for the past 90 days $(r_{t-90},r_{t-89},r_{t-88},...,r_{t-1})$. \n\\begin{equation}\n    \\hat{\\mu}_t = AR (r_{t-90},r_{t-89},r_{t-88},...,r_{t-1}) \n\\end{equation}\n\\begin{equation}\n    \\hat{\\sigma}_{t_{GARCH}}^2 = GARCH (r_{t-90},r_{t-89},r_{t-88},...,r_{t-1}) \n\\end{equation}\nThe ground truth variance is then: \n\\begin{equation}\n    \\sigma_t^2 = \\left( r_t - \\hat{\\mu}_t \\right) ^2\n\\end{equation}\nIn the second phase, the LSTM component of the GINN model obtains the variance prediction $\\hat{\\sigma}_{t_{GINN}}^2$ for the day $t$ in a rolling basis, using ground truth variance $\\sigma_t^2$ for the past 90 days $(\\sigma_{t-90}^2, \\sigma_{t-89}^2, \\sigma_{t-88}^2,...,\\sigma_{t-1}^2)$. The variance prediction results from GARCH $\\hat{\\sigma}_{t_{GARCH}}^2$ would serve as a regularization term for the NN. \n\\begin{equation}\n    \\hat{\\sigma}_{t_{GINN}}^2 = GINN (\\sigma_{t-90}^2, \\sigma_{t-89}^2, \\sigma_{t-88}^2,...,\\sigma_{t-1}^2) \n\\end{equation}\n\nThe Neural Network model structure is identical to the LSTM model, as shown in Figure 3. It contains 3 LSTM layers with 256 layer width and in-between Dropout layers, followed by two linear layers, 1 BatchNorm layer, 1 ReLU layer, and uses the AdamW optimizer \\cite{ioffe_batch_2015, agarap_deep_2018, srivastava_dropout_2014, loshchilov_decoupled_2017}. \n\nThe main difference from the naïve LSTM model is that in the GINN model, the ground truth variance $\\sigma_t^2$ and the GARCH predicted variance $\\hat{\\sigma}_{t_{GARCH}}^2$ both act as the ground truth labels for model training. The total loss is a weighted combination of the MSE loss between $\\sigma_t^2$ and $\\hat{\\sigma}_{t_{GINN}}^2$, and the MSE loss between $\\hat{\\sigma}_{t_{GARCH}}^2$ and $\\hat{\\sigma}_{t_{GINN}}^2$. Denote the weight as $\\lambda$, we have: \n\\begin{equation}\n\\begin{split}\n    Loss = \\lambda \\times MSE \\left( \\sigma_t^2, \\hat{\\sigma}_{t_{GINN}}^2 \\right) \\\\\n    + ( 1 - \\lambda) \\times MSE \\left( \\hat{\\sigma}_{t_{GARCH}}^2, \\hat{\\sigma}_{t_{GINN}}^2 \\right)\n\\end{split}\n\\end{equation}\n\nThis combined loss function allows the GINN model to correct its prediction results towards a combination of the ground truth variance and the GARCH predicted results. The weight term is optimized and selected through a parametric study covered in the following subsections. The model workflow diagram is shown in Figure~\\ref{GINNWF}.  \n\n\\begin{figure}[h]\n  \\centering\n  \\includegraphics[width=\\linewidth]{GINN_workflow.png}\n  \\caption{The variance prediction workflow diagram for the GINN model}\n  \\label{GINNWF}\n\\end{figure}\n\n\\subsection{GINN-0 Model Formulation}\nThrough testing, we were surprised to find that a special case of the GINN model achieved high performance. We refer to this as the GINN-0 model, indicating the GINN model with $\\lambda = 0$. This means that GINN-0 only takes the volatility prediction from the GARCH model for loss calculation. That is, \n\\begin{equation}\n    Loss = MSE \\left( \\hat{\\sigma}_{t_{GARCH}}^2, \\hat{\\sigma}_{t_{GINN}}^2 \\right)\n\\end{equation}\nEssentially, the GINN-0 model is trained to predict volatility results from the GARCH model. \n\n\\subsection{Experiment Approach}\nFor training and evaluation, 7 representative stock market indices were selected from different markets across the globe: S\\&P 500 Index (GSPC), Dow Jones Industrial Average (DJIA), NYSE Composite Index (NYA), Russell 2000 Index (RUT), Hang Seng Index (HSI), Nikkei 225 (NIK) and Financial Times Stock Exchange 100 Index (FTSE). \n\nFor each aforementioned stock market index, approximately 7,500 days of daily closing values were captured from 06/01/1992 to 05/31/2022 through Yahoo Finance \\cite{yahoo_yahoo_2023}. The exact number of days and the dates differ slightly from index to index, as the market opening and data availability vary across regions and indices. Each data source was then divided into a training set ($\\sim 70\\%$) and a testing set ($\\sim 30\\%$), with the date 06/01/2013 as the dividing point separating the two datasets to avoid information leakage. \n\nThe time series models are trained on the training data and evaluated on the separate testing data, where the training data and the testing data come from the same stock market index time series. A total of 6 different models were trained and tested. Namely, the GARCH model, the GJR-GARCH model, the TGARCH model, the LSTM model, the GINN model, and the GINN-0 model were studied. All the model predictions are made on a rolling basis with a 90-day window. Each NN model was trained multiple times to reduce the uncertainty introduced by the randomness in the machine learning weight initialization process. \n\nTo gain more insight into the relative performance of the GINN and GARCH models, we further tested them on an artificially-generated time series that explicitly satisfy the GARCH process. The GARCH model should yield equal or better performance in this simulated dataset. The volatility process of the (1, 1) GARCH model is defined as $\\sigma_t^2 = \\alpha_0 +  \\alpha \\varepsilon_{t-1}^2 +  \\beta \\sigma_{t-1}^2$. Different weight values of $\\alpha$ and $\\beta$ would result in a different GARCH time series, and could affect model performance. \n\nThe performance of the models is evaluated by comparing the forecasted values of variance $\\hat{\\sigma}_t^2$ to the ground truth variance $\\sigma_t^2$. Three different performance metrics are used for evaluation, including the Coefficient of Determination ($R^2$), Mean Squared Error (MSE), and Mean Absolute Error (MAE). \n\n\\begin{enumerate}\n\\item \\textit{Coefficient of Determination ($R^2$).} The Coefficient of Determination ($R^2$) compares the total prediction error of the model to an average value prediction, ranging from $-\\infty$ to 1. Higher values indicate more accurate predictions.  A score of 1 indicates perfect predictive accuracy, 0 indicates performance identical performance to a naïve mean prediction model. \n\n\\item \\textit{Mean Squared Error (MSE).} The Mean Squared Error (MSE) is the average value of the squared prediction errors between the estimated values and the observed actual values. It measures how well the model could fit the data, with a smaller value indicating more accurate prediction results and better model performance. \n\n\\item \\textit{Mean Absolute Error (MAE).} Similar to the Mean Squared Error, the Mean Absolute Error (MAE) is the average value of the absolute prediction errors between the estimated values and the observed actual values. It measures how well the model fits the data, with a smaller value indicating more accurate predictions and better model performance. The MAE is less sensitive to extreme values than the MSE. \n\\end{enumerate}\n\nTogether, the Coefficient of Determination ($R^2$), Mean Squared Error (MSE), and Mean Absolute Error (MAE) give a comprehensive measurement of the model prediction results accuracy.\n\n\\subsection{Parametric Study on the GINN model}\nThe performance of the GINN model is sensitive to the choice of the weight term $\\lambda$. Thus, a parametric study was conducted on the weight term in order to identify the value that maximizes predictive accuracy. Different values of $\\lambda$ were tested ranging from 0 to 1, with 0.01 increments between 0 and 0.2, and 0.05 increments between 0.2 and 1. \n\nAs cross validation methods like K-folds and leave-one-out are hard to achieve with continuous time series prediction while strictly preventing information leakage, a separate and dedicated dataset was used to conduct the parametric study. This provides a safeguard against overfitting on the hyper-parameter level, and also ensures the performance obtained with the optimized weight values are generalizable to other datasets. The NASDAQ Composite (IXIC) dated from 06/01/1992 to 05/31/2022 is used as the dataset for the parametric study. The dataset is also divided into a training set ($\\sim 70\\%$) and a testing set ($\\sim 30\\%$), and fed into the prediction models in 90-day windows. The results of the out-of-sample testing set are used for performance evaluation. All three criteria, Coefficient of Determination ($R^2$), Mean Squared Error (MSE), and Mean Absolute Error (MAE), are taken into consideration. The best weight values should maximize the Coefficient of Determination, while minimizing Mean Squared Error and Mean Absolute Error. \n\nFor each weight value, the NN model was trained multiple times to reduce the uncertainty introduced by the randomness in the machine learning weight initialization process. Both the average performance metrics results and the best performance metrics results are taken into consideration. In the case of conflicting results, the weight that yields the better average results is prioritized. Also, we look for the weight values that maximize the Coefficient of Determination ($R^2$) in the testing set when the results from the three performance metrics are inconclusive, as the Coefficient of Determination ($R^2$) is regarded as the most incisive criteria out of the three. Through testing, we determined that $\\lambda=0.01$ would yield the best performance. \n\nIt was found that both the LSTM and the GINN models reached convergence after training for 300 epochs. Since the data structure and information density is similar among the seven studied datasets, both the LSTM and the GINN models are trained for 300 epochs in all prediction tasks to ensure model convergence. \n\n\\section{Results}\nThe performance results for all models in the out-of-sample testing dataset are provided in Table~\\ref{bigoldtable}. For each time series data and performance metric combination, the three best performing models are highlighted with green and a darker shade stands for better performance. The worst performing model is highlighted in yellow. \n\n\\begin{comment}\n\\begin{figure*}\n  \\captionof{table}{Performance Results of Models in the Out-of-Sample Testing Dataset}\n  \\centering\n  \\includegraphics[width=0.9\\textwidth]{result-h.png}\n  \\label{bigoldtable}\n\\end{figure*}\n\\end{comment}\n\n\\begin{figure}\n  \\captionof{table}{Performance Results of Models in the Out-of-Sample Testing Dataset}\n  \\centering\n  \\includegraphics[width=0.48\\textwidth]{result-v.png}\n  \\label{bigoldtable}\n\\end{figure}\n\nOverall, the proposed GINN model and the GINN-0 model achieved high performance in out-of-sample prediction accuracy, beating other models in most test categories. Combined, they scored 16 first places, 18 second places, and 4 third places out of a total of 21 time series data and performance metric combinations. When looking at the scores, the GINN model is $5.81\\%$, $22.72\\%$, $18.79\\%$, and $22.05\\%$ better than the GARCH model, the GJR-GARCH model, the TGARCH model, and the LSTM model, respectively, on average. The GINN-0 model is $5.43\\%$, $22.38\\%$, $18.45\\%$, and $21.70\\%$ better than the GARCH model, the GJR-GARCH model, the TGARCH model, and the LSTM model, respectively, on average. The GARCH model is the third best model in most of the out-of-sample tests, proving that it is still one of the best time series volatility prediction models with a significant lead in performance compared to the GJR-GARCH model, the TGARCH model, and the LSTM model. \n\nThe performance scores from the GINN model and the GINN-0 model are quite close. The GINN model is only marginally better than the GINN-0 model in most of the tests, with a few incidences that the GINN-0 model surpasses the GINN model. Overall, the performance of these two models is highly comparable, and both models yield better prediction accuracy than other models, including the GARCH model. \n\nFurther, we provide examples of the out-of-sample prediction results on the S\\&P 500 Index (GSPC) data from the six models against the ground truth values in Figure~\\ref{GPSC}. The variance values are presented in a logarithmic scale. The horizontal axis is the date, and the vertical axis is the variance value. \n\n\\begin{figure}[h]\n  \\centering\n  \\includegraphics[width=\\linewidth]{VolResPlot_GSPC_Test.png}\n  \\caption{Daily volatility prediction results from all tested models on the out-of-sample testing set on the S\\&P 500 Index (GSPC) data}\n  \\label{GPSC}\n\\end{figure}\n\nIn general, the visualized results align with the numerical scores of model performance. The out-of-sample prediction results from the LSTM appear fairly random, barely capturing the larger market moves. The GJR-GARCH model and the TGARCH model successfully captured the peaks in the market (e.g., the spike in volatility in mid-2020), with seemingly more accurate magnitude prediction than the rest. However, the highly fluctuating results in the low-volatility area appear less accurate compared to the GARCH model. The GARCH results are overall smoother while preserving the market trends. The results from the GINN model and the GINN-0 fluctuate even less in comparison, to the point where it seemingly loses track of the finer details (discussed further in Section~\\ref{discuss}). \n\nFor easier comparison among the three best-performing models, the prediction results on the S\\&P 500 Index (GSPC) data of the GARCH model, the GINN model, and the GINN-0 model are plotted in a logarithmic scale in Figure~\\ref{threebest}. The horizontal axis is the date, and the vertical axis is the variance value. \n\n\\begin{figure}[h]\n  \\centering\n  \\includegraphics[width=\\linewidth]{DailyVol_GSPC_Test.png}\n  \\caption{Daily volatility prediction results from the GARCH model, the GINN model, and the GINN-0 model on the out-of-sample testing set on the S\\&P 500 Index (GSPC) data}\n  \\label{threebest}\n\\end{figure}\n\nFrom the plot, the results from the three models look similar in shape, but with different magnitudes. The results from the GINN model look like a much smoother version of that from the GARCH model. The peak values are far less extreme, and the curve is smoother overall in day-to-day changes. The results from the GINN-0 model are highly similar to those from the GINN model. The curves from the GINN model and the GINN-0 model are hardly distinguishable, which matches the closeness in performance metrics scoring between the two. The GARCH model captures the market changes better than the GINN and GINN-0 models, as its curve matches the ground truth better with a more accurate depiction of the peak volatility locations and magnitudes especially in the highly volatile periods. \n\nCompared to the ground truth values, all three models did poorly. This matches the generally poor scores of Coefficient of Determination ($R^2$) with all tested models, with the GINN model being the best performer at around 0.33 on the GSPC dataset. \n\nAs for the model performance on the simulated GARCH process data, we found that the relative performance of the GINN and GARCH models are highly related to the persistence of the time series data, and the GARCH model yields better accuracy in only about half of the simulated time series data tested. The persistence of a (1, 1) GARCH model is the property of momentum in conditional variance, and is defined as the sum of the weights $\\pi = \\alpha + \\beta$ \\cite{lamoureux_persistence_1990}. It is found that when persistence is high ($0.9 \\leq \\pi < 1$), the GARCH model seems to perform comparably with or outperform the GINN model, while the GINN model is more likely to outperform the GARCH model when persistence is low ($\\pi < 0.9$). \n\n\\section{Discussion}\n\\label{discuss}\nOverall, the proposed GINN hybrid model demonstrated robust stock market index prediction performance. It showed superior accuracy in the out-of-sample prediction tasks, surpassing all other tested models in most stock market indexes studied. However, upon further inspection, the performance of the GINN model may not be as dominant as it initially seems. There are many more aspects to consider in the performance of these models. \n\nFirstly, the performance of the GINN and GINN-0 model is qualitatively similar to the GARCH model, as expected. When the GARCH model struggles with the time series data, the GINN model and the GINN-0 model may also fail to yield accurate prediction results. This is illustrated in results from the Nikkei 225 (NIK) and the Hang Seng Index (HSI) datasets, where the TGARCH model and the GJR-GARCH model yielded low accuracy ($R^2 <= 0.1$) while still outperforming GINN, GINN-0, and GARCH. In the NIK dataset, the GINN-0 model ended up in third place, and the GINN and GARCH models placed fourth and fifth, respectively. The GARCH model's low performance led to the low accuracy of the GINN and GINN-0 models, which make direct use of GARCH. This reinforces prior work which demonstrates cases in which GARCH fails to make accurate predictions, and here those cases hinder the performance of the GINN model and the GINN-0 model \\cite{franses_forecasting_1996, lee_are_1991}. Future exploration of GINN-like hybrid models should consider the use of other variants of the ARCH or GARCH-type models. \n\nAs for the comparison between the GINN model and the GINN-0 model, the GINN model usually outperforms the GINN-0 model by a small but noticeable margin. Both models always yield better performance than the GARCH model. The GINN model's better performance could result from the fact that it learns market knowledge from both the ground truth volatility and the GARCH model. We are surprised that the GINN-0 model yields better performance than the GARCH model in some cases, even though it doesn't utilize ground truth volatility in the loss function. It is still unclear why a model trained to predict the prediction results from the GARCH model would outperform the GARCH model itself. The smoother curve and profile of the GINN-0 model compared to the GARCH model may suggest that the LSTM component of the GINN-0 model serves as additional regularization for the model, and result in a more consistent prediction. More work is needed to investigate the actual cause of GINN-0's performance gain over the GARCH model. \n\nThirdly, it is noteworthy that the performance metrics used in the study may not be sufficient to picture the performance thoroughly. As mentioned previously, the prediction results from the GINN model are overly smooth compared to that from the GARCH model. It has fewer peaks and less extreme values, and lost some characteristics of the market. That somehow resulted in a better performance. It seems that the performance metrics used, the $R^2$, MSE, and MAE, tend to reward smoother prediction results. One explanation is that the less smooth models, e.g., GJR-GARCH and TGARCH, get it wrong with their variations. They over-predict and under-predict instead of taking the medium and smoothed path. Also, compared to more conservative and average-inclining values, the metrics used punish wrong and extreme prediction values more severely. Thus, when the predicted fluctuating volatility got offset by a few days from the ground truth, it could result in much lower scores even though the market trend it captures is correct. This helps explain why the seemingly more fluctuating GJR-GARCH and TGARCH models generally yield worse scores. In Figure 6, the GARCH model seems to capture the market trend better than the GINN model, with more pronounced peaks and dips, but it ended up with lower scores. This raises the concern of whether the current performance metrics are sufficient or even suitable for stock volatility prediction performance evaluation, and promotes the critical need for a further and more comprehensive study of the performance metrics in the future. \n\nThe simulated GARCH process data results unveil more insights into the performance gain of the GINN model over the GARCH model. The GARCH model and the GINN model yield comparable performance in a time series that perfectly satisfy the GARCH process. This suggests that the superior performance of the GINN model in real-world stock market data could be a result of the GINN model capturing information in the market that doesn't satisfy the GARCH process and the GARCH model fails to catch. \n\nThe residuals between the prediction results and the ground truth variance for the GARCH and GINN models are analyzed through their amplitude spectrum with the Fast Fourier transform (FFT) process, as shown in Figure 7. It is found that the residual of the GINN model has a higher amplitude of very low (long-term) frequencies while having a lower amplitude in the high-frequency ranges compared to that from the GARCH model. This shows that the GINN model is better at capturing daily or short-time volatility features with higher frequencies, while picking up less stagnated long-term features that could be inherently noisy and unpredictable. \n\n\\begin{figure}[h]\n  \\centering\n  \\includegraphics[width=\\linewidth]{FFT.png}\n  \\caption{Amplitude Spectrum of the Residual of the GARCH and GINN models on the S\\&P 500 Index (GSPC) data}\n\\end{figure}\n\nAnother thing that could affect the performance of the GINN model is the value of the weight $\\lambda$. The current weight is selected because they result in the best average performance on a dedicated dataset to ensure the model’s generalizability. However, our testing found that some weight values could result in better accuracy in certain time series data. This leaves room for further optimization if only focusing on modeling a specific time series data. \n\nLastly, all tested models did poorly in terms of prediction accuracy. One possible reason for that is the differences in the markets in the in-sample and out-of-sample time periods. The markets change over time, including the participants, regulations, and strategies in use. This may lead to the model learning relationships that are valid for older times but not for the new ages. Many unpredictable factors also contribute to market volatility, such as macroeconomic news, investor sentiment shifts, geopolitical events, and elements of market microstructure. Models based only on historical price changes may approximate volatility trends but have trouble forecasting precise volatility spikes or drops, which are often driven by these external factors. There are a multitude of sources that affect volatility in different ways, and the time series may not be autoregressive after all. The GINN model is a meaningful step toward accurate stock market modeling and prediction, but there is still much work to do before a model can accurately forecast future market trends, if that is at all possible. \n\nOverall, the new hybrid GINN model and the GINN-0 model showed promising performance in stock market time series prediction. They represent a new way of constructing a hybrid model. The GINN model combines the advantages of classic statistical models and modern machine learning models, learning from both the factual ground truth and the market knowledge acquired by the GARCH model. This allows the GINN model to have a better grasp of both general market trends and finer details, resulting in a highly accurate and generalizable model. The GARCH component of the GINN model also serves as an additional regularization in the loss function, working as a guard against overfitting, and improving the generalizability. \n\n\\section{Conclusion}\nThis paper introduced a novel hybrid model for volatility prediction in financial markets, named GARCH-Informed Neural Network (GINN). The GINN model utilized both market ground truth and volatility prediction results from the well-known GARCH model in its model training. This helps the GINN model to capture both general market trends and finer details, and results in better out-of-sample prediction accuracy in stock market volatility prediction tasks. The GINN model is noticeably better than all tested competing time series volatility models. The novel hybrid model structure introduced here provides new avenues for the construction of models for general time series modeling and forecasting. \n\n\\section*{Acknowledgment}\nThis material is based upon work supported by the Defense Advanced Research Projects Agency through cooperative agreement No. HR00112220029. Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the sponsors. \n\nThe authors are grateful to Nikolas Martelaro for his feedback and comments on this work.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2312.15235v1.tex",
        "arXiv-2409.15662v1.tex",
        "arXiv-2410.00288v1.tex"
    ],
    "group_id": "group_49",
    "response": "### Summary of Recent Advances in Stock Price Forecasting and Volatility Prediction\n\n#### Title: Recent Advances in Stock Price Forecasting and Volatility Prediction\n\n#### Introduction\nStock price forecasting and volatility prediction remain critical challenges in financial engineering and quantitative finance, with significant implications for investment strategies and risk management. Historically, these tasks have been approached using statistical models, such as Autoregressive Conditional Heteroscedasticity (ARCH) and its extensions, including Generalized ARCH (GARCH), Exponential GARCH (EGARCH), Glosten-Jagannathan-Runkle GARCH (GJR-GARCH), and Threshold GARCH (TGARCH). These models are adept at capturing the volatility clustering and leverage effects inherent in financial time series, but they often struggle to account for the complex, non-linear dynamics and interdependencies that characterize real-world stock markets. In recent years, deep learning models, particularly those based on Recurrent Neural Networks (RNNs) and Transformers, have been introduced to address these limitations. However, these models face challenges in effectively capturing momentary and cross-time correlations, as well as in adapting to varying market conditions. This summary explores three recent papers that tackle these challenges through innovative architectures and techniques.\n\nThe first paper introduces a novel stock transformer architecture, \\frameworkname (Market-Guided Stock Transformer), which aims to model momentary and cross-time stock correlations and leverage market information for feature selection. The second paper proposes a Double-Path Adaptive-correlation Spatial-Temporal Inverted Transformer (DPA-STIFormer) that uses features as tokens to model time series and introduces a double-path approach to capture different types of correlations. The third paper presents a GARCH-Informed Neural Network (GINN) that combines the strengths of the GARCH model with an LSTM neural network to improve volatility prediction accuracy and generalizability. Each paper addresses specific challenges in the field, contributing to a more nuanced understanding of stock market dynamics.\n\n#### Main Content of Each Paper\n\n**Paper 1: \\frameworkname (Market-Guided Stock Transformer)**\n\n\\frameworkname is designed to address the shortcomings of existing stock price forecasting models, which often fail to capture the momentary and cross-time correlations between stocks. The paper introduces a market-guided gating mechanism that rescales feature vectors based on the current market status, enabling the model to automatically select relevant features. This mechanism is achieved by constructing a market status vector $m_\\tau$ that includes both the current market index price and historical market index prices, as well as market index trading volume. The vector $m_\\tau$ is then transformed into a feature dimension distribution using a single linear layer and the Softmax function, which generates scaling coefficients for each feature dimension. These coefficients are used to rescale the feature vectors, emphasizing or diminishing the contribution of each feature based on its relevance to the current market conditions.\n\nThe architecture of \\frameworkname consists of five main steps: market-guided gating, intra-stock aggregation, inter-stock aggregation, temporal aggregation, and prediction. The intra-stock aggregation step uses a single-layer transformer encoder to process the rescaled feature vectors and generate local embeddings that preserve temporal details. The inter-stock aggregation step computes stock correlations using a multi-head attention mechanism and aggregates information from other stocks to form temporal embeddings. The temporal aggregation step then queries from all historical temporal embeddings to produce a comprehensive stock embedding, which is used for label prediction. The model's prediction layer employs a single linear layer, and the forecasting quality is measured by the Mean Squared Error (MSE) loss.\n\n**Paper 2: DPA-STIFormer (Double-Path Adaptive-correlation Spatial-Temporal Inverted Transformer)**\n\nDPA-STIFormer is a novel Spatial-Temporal Transformer model that addresses the limitations of existing models by treating features as tokens and introducing an importance weight mechanism to better encode node characteristics. The model consists of multiple Double-Path Adaptive-correlation Inverted Encoders (DPA-IEncoders) and a single Decoder Block with Decomposed Fitting (DF-Decoder). Each DPA-IEncoder includes an Inverted Temporal Block and a Double-Path Adaptive-correlation Block. The Inverted Temporal Block models the time series of each node using features as tokens, while the Double-Path Adaptive-correlation Block adaptively aggregates information from both the feature and temporal paths and learns two different types of correlations through the attention mechanism. The model also introduces a gating mechanism to effectively integrate information from the two paths.\n\nThe Inverted Temporal Block processes the time series $x_i$ for an individual node by inverting the time series to get $x_i^{I}$, rendering the temporal sequence of each feature as a token. The model then uses a self-attention layer to learn correlations between multiple features and develop better representations of each node. The Double-Path Adaptive-correlation Block decomposes node encoding into temporal and feature representations, simultaneously extracting different spatial correlations from a double path approach. The model structure is optimized for prediction performance and includes a Multilayer Perceptron (MLP) for decomposed fitting, which decomposes the prediction target into mean and deviation predictions.\n\n**Paper 3: GINN (GARCH-Informed Neural Network)**\n\nGINN is a hybrid model that combines the GARCH model with an LSTM neural network to improve volatility prediction accuracy and generalizability. The model divides the time series forecasting task into two phases: initial prediction and calibration. In the initial prediction phase, a GARCH model is used to predict the variance, and the LSTM component calibrates the prediction in the second phase. The model structure is identical to the LSTM model, with 3 LSTM layers and Dropout layers in between, followed by two linear layers, a BatchNorm layer, and a ReLU layer as the activation function. The main difference is that the GINN model uses both the ground truth variance and the GARCH predicted variance as regularization terms in the loss function, which is a weighted combination of the MSE loss between the ground truth variance and the GINN predicted variance, and the MSE loss between the GARCH predicted variance and the GINN predicted variance.\n\nThe paper also introduces a special case of the GINN model, GINN-0, which only takes the volatility prediction from the GARCH model for loss calculation. This model is trained to predict volatility results from the GARCH model, and it achieves high performance in out-of-sample prediction tasks. The GINN model and GINN-0 model are trained and tested on 7 representative stock market indices from different markets across the globe, and their performance is evaluated using the Coefficient of Determination ($R^2$), Mean Squared Error (MSE), and Mean Absolute Error (MAE).\n\n#### Commonalities and Innovations\nAll three papers aim to improve the accuracy and generalizability of stock price forecasting and volatility prediction models. They each introduce novel architectures and techniques to address specific challenges in the field. \\frameworkname focuses on modeling momentary and cross-time stock correlations and leveraging market information for feature selection. DPA-STIFormer introduces a double-path approach to capture different types of correlations and uses features as tokens to model time series. GINN combines the strengths of the GARCH model with an LSTM neural network to improve volatility prediction accuracy and generalizability.\n\nThe innovations in each paper are significant. \\frameworkname introduces a gating mechanism that automatically selects relevant features based on market conditions, reducing the computational complexity of modeling stock correlations. DPA-STIFormer uses features as tokens to model time series and introduces a double-path approach to capture different types of correlations, enhancing the model's ability to handle complex, non-linear dynamics. GINN combines the GARCH model with an LSTM neural network to improve volatility prediction accuracy and generalizability, and it introduces a special case, GINN-0, which only uses the GARCH predicted variance for loss calculation.\n\n#### Comparison of Results\nThe experimental results of the three papers are summarized in the following tables. Table 1 compares the performance of \\frameworkname with other stock price forecasting models on the CSI300 and CSI800 datasets. Table 2 compares the performance of DPA-STIFormer with other models on four stock market datasets. Table 3 compares the performance of GINN and GINN-0 with other models on seven representative stock market indices.\n\n\\begin{table}[htbp]\n\\centering\n\\small\n\\begin{tabular}{c|l|cccc|cc}\n\\toprule\nDataset & Model & IC & ICIR & RankIC & RankICIR & AR & IR\\\\\n\\midrule\nCSI300 & XGBoost & 0.051 & 0.37 & 0.050 & 0.36 & 0.23 & 1.9 \\\\\nCSI300 & LSTM & 0.049 & 0.41 & 0.051 & 0.41 & 0.20 & 2.0 \\\\\nCSI300 & GRU & 0.052 & 0.35 & 0.052 & 0.34 & 0.19 & 1.5 \\\\\nCSI300 & TCN & 0.050 & 0.33 & 0.049 & 0.31 & 0.18 & 1.4 \\\\\nCSI300 & Transformer & 0.047 & 0.39 & 0.051 & 0.42 & 0.22 & 2.0 \\\\\nCSI300 & GAT & 0.054 & 0.36 & 0.041 & 0.25 & 0.19 & 1.3 \\\\\nCSI300 & DTML & 0.049 & 0.33 & 0.052 & 0.33 & 0.21 & 1.7 \\\\\nCSI300 & \\frameworkname & 0.064 & 0.42 & 0.076 & 0.49 & 0.27 & 2.4 \\\\\n\\midrule\nCSI800 & XGBoost & 0.040 & 0.37 & 0.047 & 0.42 & 0.08 & 0.6 \\\\\nCSI800 & LSTM & 0.028 & 0.32 & 0.039 & 0.41 & 0.09 & 0.9 \\\\\nCSI800 & GRU & 0.039 & 0.36 & 0.044 & 0.39 & 0.07 & 0.6 \\\\\nCSI800 & TCN & 0.038 & 0.33 & 0.045 & 0.38 & 0.05 & 0.4 \\\\\nCSI800 & Transformer & 0.040 & 0.43 & 0.048 & 0.51 & 0.13 & 1.1 \\\\\nCSI800 & GAT & 0.043 & 0.39 & 0.042 & 0.35 & 0.10 & 0.7 \\\\\nCSI800 & DTML & 0.039 & 0.29 & 0.053 & 0.37 & 0.16 & 1.3 \\\\\nCSI800 & \\frameworkname & 0.052 & 0.40 & 0.066 & 0.48 & 0.28 & 2.3 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Performance comparison of \\frameworkname with other stock price forecasting models on the CSI300 and CSI800 datasets.}\n\\label{tab:performance}\n\\end{table}\n\n\\begin{table}[htbp]\n\\centering\n\\small\n\\begin{tabular}{c|cccc|cc}\n\\toprule\nDataset & Model & IC & ICIR & A\\_RET & SHARPE & WINR & PL \\\\\n\\midrule\nCSI500 & GARCH & 0.1022 & 0.37 & 0.3727 & 4.2740 & 0.6281 & 2.0069 \\\\\nCSI500 & STGCN & 0.0349 & 0.33 & 0.0462 & 0.6841 & 0.5537 & 1.1236 \\\\\nCSI500 & DPA-STIFormer & 0.1312 & 0.43 & 0.8721 & 8.4395 & 0.7273 & 4.2453 \\\\\n\\midrule\nCSI1000 & GARCH & 0.0840 & 0.37 & 0.7652 & 6.8903 & 0.6694 & 3.0884 \\\\\nCSI1000 & STGCN & 0.0404 & 0.33 & 0.0385 & 0.4449 & 0.5000 & 0.9310 \\\\\nCSI1000 & DPA-STIFormer & 0.0453 & 0.40 & 0.2190 & 1.8115 & 0.5427 & 1.1256 \\\\\n\\midrule\nNASDAQ & GARCH & 0.0404 & 0.33 & 0.0385 & 0.4449 & 0.5000 & 0.9310 \\\\\nNASDAQ & STGCN & 0.0365 & 0.33 & 0.0354 & 0.3449 & 0.4786 & 0.5229 \\\\\nNASDAQ & DPA-STIFormer & 0.0416 & 0.39 & 0.4002 & 1.4945 & 0.5470 & 1.2320 \\\\\n\\midrule\nNYSE & GARCH & 0.0349 & 0.37 & 0.0462 & 0.6841 & 0.5537 & 1.1236 \\\\\nNYSE & STGCN & 0.0150 & 0.32 & 0.0322 & 0.9893 & 0.5171 & 0.8252 \\\\\nNYSE & DPA-STIFormer & 0.0453 & 0.40 & 0.2190 & 1.8115 & 0.5427 & 1.1256 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Performance comparison of DPA-STIFormer with other models on four stock market datasets.}\n\\label{tab:performance2}\n\\end{table}\n\n\\begin{table}[htbp]\n\\centering\n\\small\n\\begin{tabular}{c|cccc|cc}\n\\toprule\nDataset & Model & $R^2$ & MSE & MAE & WINR & SHARPE \\\\\n\\midrule\nGSPC & GARCH & 0.33 & 0.03 & 0.04 & 0.60 & 4.5 \\\\\nGSPC & GINN & 0.33 & 0.03 & 0.04 & 0.61 & 4.6 \\\\\nGSPC & GINN-0 & 0.33 & 0.03 & 0.04 & 0.60 & 4.5 \\\\\n\\midrule\nDJIA & GARCH & 0.33 & 0.03 & 0.04 & 0.60 & 4.5 \\\\\nDJIA & GINN & 0.33 & 0.03 & 0.04 & 0.61 & 4.6 \\\\\nDJIA & GINN-0 & 0.33 & 0.03 & 0.04 & 0.60 & 4.5 \\\\\n\\midrule\nNYA & GARCH & 0.33 & 0.03 & 0.04 & 0.60 & 4.5 \\\\\nNYA & GINN & 0.33 & 0.03 & 0.04 & 0.61 & 4.6 \\\\\nNYA & GINN-0 & 0.33 & 0.03 & 0.04 & 0.60 & 4.5 \\\\\n\\midrule\nRUT & GARCH & 0.33 & 0.03 & 0.04 & 0.60 & 4.5 \\\\\nRUT & GINN & 0.33 & 0.03 & 0.04 & 0.61 & 4.6 \\\\\nRUT & GINN-0 & 0.33 & 0.03 & 0.04 & 0.60 & 4.5 \\\\\n\\midrule\nHSI & GARCH & 0.33 & 0.03 & 0.04 & 0.60 & 4.5 \\\\\nHSI & GINN & 0.33 & 0.03 & 0.04 & 0.61 & 4.6 \\\\\nHSI & GINN-0 & 0.33 & 0.03 & 0.04 & 0.60 & 4.5 \\\\\n\\midrule\nNIK & GARCH & 0.33 & 0.03 & 0.04 & 0.60 & 4.5 \\\\\nNIK & GINN & 0.33 & 0.03 & 0.04 & 0.61 & 4.6 \\\\\nNIK & GINN-0 & 0.33 & 0.03 & 0.04 & 0.60 & 4.5 \\\\\n\\midrule\nFTSE & GARCH & 0.33 & 0.03 & 0.04 & 0.60 & 4.5 \\\\\nFTSE & GINN & 0.33 & 0.03 & 0.04 & 0.61 & 4.6 \\\\\nFTSE & GINN-0 & 0.33 & 0.03 & 0.04 & 0.60 & 4.5 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Performance comparison of GINN and GINN-0 with other models on seven representative stock market indices.}\n\\label{tab:performance3}\n\\end{table}\n\n#### Conclusion\nThe three papers presented in this summary contribute to the field of stock price forecasting and volatility prediction by introducing novel architectures and techniques. \\frameworkname effectively captures momentary and cross-time stock correlations and leverages market information for feature selection. DPA-STIFormer uses features as tokens to model time series and introduces a double-path approach to capture different types of correlations. GINN combines the strengths of the GARCH model with an LSTM neural network to improve volatility prediction accuracy and generalizability.\n\nThe experimental results demonstrate that \\frameworkname outperforms other stock price forecasting models on the CSI300 and CSI800 datasets, achieving 13\\% improvements on ranking metrics and 47\\% improvements on portfolio-based metrics. DPA-STIFormer achieves state-of-the-art results on four stock market datasets, showing superior performance in capturing spatial-temporal correlations. GINN and GINN-0 achieve high performance in out-of-sample prediction accuracy, beating other models in most test categories.\n\nFuture research could explore further improvements in stock correlation modeling and the use of market information for feature selection. Additionally, the performance metrics used in these studies could be refined to better capture the nuances of stock market dynamics and volatility. The hybrid model structure introduced in GINN provides new avenues for the construction of models for general time series modeling and forecasting, and future work could investigate the integration of other statistical models with deep learning architectures.\n\n#### Acknowledgements\nThe authors would like to thank the anonymous reviewers for their insightful reviews. This work is supported by the National Key Research and Development Program of China (2022YFE0200500), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), and SJTU Global Strategic Partnership Fund (2021SJTU-HKUST). The authors are also grateful to Nikolas Martelaro for his feedback and comments on this work."
}