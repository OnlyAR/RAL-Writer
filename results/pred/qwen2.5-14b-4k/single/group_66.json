{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{LiveQA: A Question Answering Dataset over Sports Live}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nIn this paper, we introduce LiveQA, a new question answering dataset constructed from play-by-play live broadcast. It contains 117k multiple-choice questions written by human commentators for over 1,670 NBA games, which are collected from the Chinese Hupu\\footnote{https://nba.hupu.com/games} website. Derived from the characteristics of sports games, %LiveQA requires a system to perform reasoning across timeline-based live broadcast, \nLiveQA can potentially  test the  reasoning ability across timeline-based live broadcasts, \nwhich is challenging compared to the existing datasets.\nIn LiveQA, the questions require understanding the timeline, tracking events or doing mathematical computations. \nOur preliminary experiments show that the dataset introduces a challenging problem for question answering models, and  a strong baseline model only achieves the accuracy of 53.1\\%  and cannot beat the dominant option rule. \nWe release the code and data of this paper for future research.\\footnote{code: https://github.com/PKU-TANGENT/GAReader-LiveQA}\\footnote{data: https://github.com/PKU-TANGENT/LiveQA}\n\\end{abstract}\n\n\\section{Introduction}\n\nThe research of question answering (QA), where a system needs to understand a piece of reading material and answer corresponding questions, has drawn considerable attention in recent years. While various QA datasets have been constructed to study how a QA system can understand a specific passage, the common sense knowledge and so on~\\cite{rajpurkar2016squad,lai2017race,dunn2017searchqa,rajpurkar2018know}, %most answers of these datasets could be extracted from a few relevant sentences\nmost questions in these datasets could be given their answers by extracting from a few relevant sentences so that the model only needs to find a small set of supporting evidences,% to infer the answers, where the temporal order of these information does not effect the final answer. \nwhose temporal ordering  does not effect the final answer. \nIn other words, these questions are raised only considering a fixed document. However, in the real-life question answering, a question could have its \\textbf{timelines}. To  infer the answer, a good model needs to understand series of timeline information. For example, the question ``how many points did Lebron James have?'' would have different answers based on the time when the question was asked during a basketball game, and the answer would continuously change during the game. The other question ``Which team would first earn 10 points?'' would require a system to track down information of scoring points along the timeline until one team achieves 10 points.\n\nAccording to the analysis above, we consider the timeline-based question answering problem as a gap  which has not been covered by existing datasets.\nThus, in this work we hope to construct a dataset where passages and questions both have timelines and question respondents are required to judge what information should be gathered for the questions involved in a timeline.\nSuch a timeline inference-involved QA dataset introduces a new research line of reading comprehension, that evaluates the ability of understanding temporal information of a QA model.\n\\begin{figure}[!t]                                                     %开始插图\n\\centering                                                                %图片居中\n\\includegraphics[scale=0.45]{figure1.jpg}                                    %确定图片大小/位置\n\\caption{Question Examples from the LiveQA dataset.}                                    %加名称和标签\n\\end{figure}\n\nAdditionally, the real-world questions are often involved in some math calculation, such as addition, subtraction and counting.\nTo answer the questions correctly, one not only needs to  locate  some specific sentences, but also do calculation or comparison on the extracted evidence. For example, ``How many points did the winner team win?'' needs one system to perform subtraction on the final score to get the correct answer.\n\nTo these ends, we construct a QA dataset \\textit{LiveQA} based on a Hupu-live-broadcasting-dataset, which is a set of Chinese live-broadcasting passages of NBA. Hupu is a sports news website that has live-broadcasting for basketball games.\nIn the Hupu-live-broadcasting, the host of one sport game describes the details of the game vividly with emotion and different sentence structures, and presents many game-related quizzes during the game.\nWe collect the description texts and their quizzes into  LiveQA.\nAnswering the quizzes requires one model to correctly understand the timeline information of the context: some quizzes ask about information of one-whole quarter of the game or which player reaches a certain score earlier. Thus, the model needs to fully understand the temporal information of the live-broadcasting and then performs inference based on the temporal information. \nFigure 1 shows four question examples in the LiveQA dataset. Answering the first two questions requires an addition math operation, and the 3$^{rd}$ and 4$^{th}$ questions need comparison operation. Meanwhile, we can see that all these questions are time-dependent and require temporal inference.\n\nIn summarize, the main characteristics of our LiveQA dataset include the following two aspects.\nFirstly,  the questions are time-awared. The model needs temporal inference to obtain the final answer.\nSecondly, in our dataset, reading comprehension is not limited to extracting a few specific text spans from the document,\nbut is involved with math calculation.\nThese characteristics make LiveQA challenging for previous QA systems to answer its questions. \nIn this paper, we present an analysis of the resulting dataset to show how these characteristics appear in the data. We also show how questions are involved with temporal inference, and these questions also require mathematical inference. To demonstrate how these characteristics affect the performance of the QA model, we design a pipeline method, which first tries to find supporting sentences and then uses a strong baseline multi-hop inference model named Gated-Attention Reader, to judge the baseline performance on LiveQA. Our experimental results show that \nsuch strong baseline model only slightly exceeds random choice, which achieve 53.1\\% and cannot beat the dominant option rule. \nThe analysis and experimental results show how this dataset can effectively examine how a QA system can perform multi-hop temporal and mathematical inference, which is not covered by previous studies.\n\nThe following of this paper is organized as follows: In section 2, we give a brief introduction of current QA research lines and research on live text processing. In section 3, we describe how we constructed the dataset. In section 4, we give statistics of the dataset and analyse the timelineness and mathematical inference in the data. In section 5, we give evaluation results of baseline models and error analysis.\n\n\\section{Related Works}\nIn this section, we mainly introduce the various QA datasets which can be categorized as datasets with extractive answers, datasets with descriptive answers and datasets with multiple-choice questions.\n\n\\subsection{Datasets with Extractive Answers}\nA number of QA datasets consist of numerous documents or passages which have considerable length. Each passage is equipped with several questions, answers of which are segments of the passage. The goal of a reading comprehension model is to find the correct text span. In other words, it may offer a begin position and an end position in the passage instead of generating the words itself. Such corpora are regarded as datasets with extractive answers.\n\nThe most famous dataset of this kind is Stanford Qustion Answering Dataset (SQuAD) ~\\cite{rajpurkar2016squad}. SQuAD v1.0 consists of 107,785 question-answer pairs compiled by crowdworkers from 536 Wikipedia articles, and is much larger than previous manually labeled datasets. Over 50,000 unanswerable questions are added in SQuAD v2.0 ~\\cite{rajpurkar2018know}. It is more challenging for existing models because they have to make more unreliable guesses. As performances on SQuAD have become a common way to evaluate models, some experts regard SQuAD as the ImageNet~\\cite{Deng2009ImageNet} dataset in the NLP field. \n\n\\begin{figure}[!t]                                                     %开始插图\n\\centering                                                                %图片居中\n\\includegraphics[scale=0.18]{squad2.jpg}                                    %确定图片大小/位置\n\\caption{Examples of question-answer pairs in SQuAD}                                    %加名称和标签\n\\end{figure}\n\n\\begin{comment}\n\\begin{figure}[!t]                                                     %开始插图\n\\centering                                                                %图片居中\n\\includegraphics[scale=0.38]{squad1.jpg}                                    %确定图片大小/位置\n\\caption{SQuAD compared to other datasets}                                    %加名称和标签\n\\end{figure}\n\\end{comment}\n\nAnother frequently used dataset with extractive answers is CNN/Daily Mail dataset~\\cite{Hermann2015Teaching}, which was released by Google DeepMind and University of Oxford in 2015. One shining point of it is that each entity is anonymised by using an abstract entity marker to prevent models from using word-level information or n-gram models to find the answer rather than comprehending the passage.\n\n\\begin{figure}[!t]                                                     %开始插图\n\\centering                                                                %图片居中\n\\includegraphics[scale=0.45]{cnn.jpg}                                    %确定图片大小/位置\n\\caption{An example of anonymised entity in CNN/Daily Mail}                                    %加名称和标签\n\\end{figure}\n\n\\begin{CJK}{UTF8}{gbsn}\n\\begin{figure}\n        \\centering\n       \\begin{tabular}{p{120pt}p{120pt}p{50pt}p{50pt}}\n       \\hline\n       \\textbf{Text} & \\textbf{Question} & \\textbf{Choices}& \\textbf{Answer}\\\\\n         \\hline\n         \n          ……\\\\\n          \\small{哈登弧顶控球！！面对克莱-汤普森紧逼！}\\\\\n          \\small{左侧横移！！！！}\\\\\n          \\small{哨响！克莱-汤普森逼得太紧了！吃到一次犯规！}\\\\\n         \\small{勇士要个长暂停！！！！！}\\\\\n         &\\small{停回来，勇士第一轮进攻能否得分？（罚球也算，直到球权转换）}&\\small{能/不能} & \\small{不能}\\\\\n         \\small{稍等！！！}\\\\\n         \\small{第一节还有7分29秒！}\\\\\n         &\\small{本节勇士队最后一分是否由伊戈达拉获得？}&\\small{是/不是} & \\small{不是}\\\\\n         \\small{好的！！！比赛继续！！！}\\\\\n         \\small{哈登走上罚球线！！！}\\\\\n         \\small{两罚都有！！14-9}\\\\\n         \\small{利文斯顿弧顶控球！！！}\\\\\n         ……\\\\\n\n        \\hline\n        \\hline\n       \\end{tabular}\n       \\caption{A partial example of LiveQA timeline.}\n       \\label{liveex}\n\\end{figure}\n\\end{CJK}\n\nCBT~\\cite{Hill2015The}, NewsQA~\\cite{Trischler2016NewsQA}, TriviaQA~\\cite{Joshi2017TriviaQA} and many other datasets can also be categorized into this class. They  constitute a high proportion of MRC datasets, and can test the abilities of extractive models in various ways. The most closest work to ours is DROP~\\cite{dua2019drop}, which is a QA dataset that requires discrete reasoning over the content of paragraphs. It requires the system to extract various pieces of numerical evidence and perform calculation on top of the numbers. Thus, we aim to construct a novel dataset, on which extractive models are likely to make mistakes in looking for the location of an answer, that the dataset can open a new research line for question answering by testifying the ability of models to understand timelineness.\n\n\\subsection{Datasets with Descriptive Answers}\nInstead of selecting a span from the passage, datasets with descriptive answers require a reading comprehension model to generate whole and stand-alone sentences. These corpora are more closer to reality, because most questions in the real world cannot be solved simply by presenting a span or an entity. This kind of dataset is getting popular nowadays, and  may be the trend of the development of MRC datasets.\\\\ \nMS MARCO (Microsoft MAchine Reading Comprehension) ~\\cite{Nguyen2016MS} is a dataset released by Microsoft in 2016. This dataset aims to address the questions and documents in the real world, as its questions are sampled from Bing's search query logs and its passages are extracted from web documents retrieved\nby Bing. The questions in MS MARCO are about ten times as many as SQuAD, and each question is equipped with a human generated answer. The dataset also includes unanswaerable questions. All of the above characteristics make MS MARCO worthy of trying.\n\nNarrariveQA~\\cite{Ko2017The} is another dataset with descriptive answers released by DeepMind and University of Oxford in 2017. The dataset consists of stories, %which are \nbooks and movie scripts, with human written questions and answers based solely on human-generated abstractive summaries. Answering such questions requires readers to integrate information which may distribute across several statements throughout the document, and generate a cogent answer on the basis of this integrated information. In other words, they test that the reader comprehends language, not just that it can pattern match. We judge it a referential advantage of a dataset, so LiveQA requires the ability of tracking events as well as we show in Figure \\ref{liveex}, which will be detailedly introduced in following sections.\n\n\\subsection{Datasets with Multiple-choice Questions}\nDatasets with descriptive answers have various advantages, but they are relatively diﬃcult to evaluate the system performance precisely and objectively. Thus, corpuses with more gradable QA-pairs are also needed, which leads to the development of datasets with multiple-choice questions. Through diversified types of questions, these datasets can examine almost every ability of a reading comprehension model mentioned above and are easier to get a conclusive score.\nMany datasets of this kind have been released in recent years, and they have covered multiple domains. For example, RACE~\\cite{lai2017race} and CLOTH~\\cite{xie2017large} are collected from English exams, MCTest~\\cite{richardson2013mctest} is sampled from friction stories, and ARC~\\cite{clark2018think} is extracted from science-related sentences. However, there is still not a reliable dataset which is built on sports events for MRC. Thus, our LiveQA dataset has the potential for filling several gaps in the field of MRC. \n\n \\subsection{Live Text Processing}\n \n Previously, various studies have been conducted on automatically generate sports news from live text commentary scripts, which has been seen as a summarization task. Zhang et al.~\\shortcite{zhang2016towards} proposed an investigation on summarization of sports news from live text commentary scripts, where they treat this task as a special kind of document summarization based on sentence extraction in a supervised learning to rank framework. Yao et al.~\\shortcite{yao2017content} further verify the feasibility of a more challenging setting to generate news report on the fly by treating live text input as a stream for sentence selection. Wan et el.~\\shortcite{10.1007/978-3-319-50496-4_80} studied dealing with the summarization task in Chinese. All these studies focuses on using the live text commentary scripts as the input of summarization and selecting sentences to form the summary. So far, we are the first to point out the importance of timelineness and mathematical reasoning in understanding live text commentary scripts.\n\n\\section{LiveQA: Dataset Construction}\nIn this section we introduce how to construct LiveQA from the raw Hupu text and present the corpus statistics.\nThe whole process of building LiveQA mainly includes  crawling the raw data and acquiring the game texts with corresponding quizzes.\n\\subsection{Data Crawling}\nIn Hupu, each game has a unique ID which is connected with its url. We collected the IDs from the Hupu's live schedule pages. Their formats are {\\em https://nba.hupu.com/games/year-month-date}. There are links to all the NBA games so that their IDs can be saved. After we saved the IDs into a file, we used the web debugging tool Fiddler to get a sample of the url of a game, and then changed the IDs in the url to make access to all the games. We are authorized by the legal department of hupu website to construct the dataset for only academical purpose.\n\n\\subsection{Data Processing}\n Most previous datasets usually do not care for the storing positions of the passages and their questions.\n But in our dataset, the quizzes and the contexts shouldn't be separated because the time (the position) one quiz occurs is quite important for the final answer. If we separate the quizzes and their contexts, \n most quizzes may have different answers and cannot be answered even by human.\n Here we use some rules to clean the dataset. \n The lines starting with '@' are always interactions between the host and some active readers, which are irrelevant to the game. During the half-time break, the host will give out some \"gift\" questions to please the readers waiting for the second-half. \n Some of the questions appear like normal quizzes, but they need information outside the game to answer them, thus we exclude them from the data (i.e.. which team won more matches in the history?). Usually they have a prefix -- \\begin{CJK}{UTF8}{gbsn}\"中场福利”\\end{CJK}  in common. Besides, we exclude the descriptions of pictures from our data. \n \n \\subsection{Data Structure}\n \n Here we give an explanation for the structure for each independent data sample.\n \n For each live-stream of one match, the timeline data is sorted in time order, where the questions are inserted into the corresponding timeline position so that the timeline features of the questions could be inferred. As we show in Figure \\ref{liveex}, the plain content text and the question text share the same timeline, but question records have choices and answers along with the text. For each record in the timeline, it either contains a piece of live-stream text or a question bonded with the corresponding choices and the correct answer.\n Each question has two answer choices.\n \n\n\\section{Dataset Statistics}\n\n\\begin{table}[h]\n\\begin{center}\n\\begin{tabular}{ll}\n\\hline\n\\bf Element                              & \\bf Count        \\\\\n\\hline\nDocument                          & 1,670 \\\\\nSentences in Total & 1786616\\\\\nSentences in Average & 1069.83\\\\\nQuizzes in Total& 117050\\\\\nQuizzes in Average                         & 70.09 \\\\\n\n\\hline\n\\end{tabular}\n\\end{center}\n\\caption{\\label{stat} The details of statistics of the dataset.}\n\\end{table}\n\nWe show the statistics of the dataset in Table \\ref{stat}. The LiveQA dataset contains 1,670 documents, each of which has 70.09 quizzes and 1069.83 sentences on average. \nNext we analyze the questions from two different views.\nFirst, we simply classify the questions according to the positions of their answers. \nIn general, some of the questions can be solved by extracting information from neighboring sentences, which involves a time period of the origin game. Such questions occupy\n68.6\\% of all the questions. Some questions can be replied only by summarizing all the information after the game ends and occupy about 30.6\\%. Still, there exists a small percentage (0.8\\%) of questions which are impossible to be answered from the passage.\nTable \\ref{loc} lists some examples for each type of questions. \n\n\\begin{CJK}{UTF8}{gbsn}\n\\begin{table*}[t!]\n\\centering\n\\begin{tabular}{|p{2cm}|p{1.5cm}|p{4cm}|p{4.5cm}|}\n\\hline\nQuestion type&Proportion&Example&Translation\\\\\n\\hline\n\\centering\n\\multirow{2}[6]{2cm}{Answered after the game ends}&\n\\centering\n\\multirow{2}[6]{1.5cm}{30.6\\%}&\n本场森林狼能否赢快船4分或更多？&Will the Timberwolves beat the Clippers by more than 4 points?\\\\\n\\cline{3-4}\n& &本场比赛谁会赢？&Which team will win?\\\\\n\\cline{3-4}\n\\hline\n\\multirow{2}[8]{2cm}{Answered through the context}&\n\\centering\n\\multirow{2}[8]{1.5cm}{68.6\\%}&\n第二节谁先命中三分球？&Which team will make a three-pointer first in the second quarter?\\\\\n\\cline{3-4}\n& &首节最后一分会不会由罚球获得？&Will the last point in the first quarter scored through a free-throw?\\\\\n\\cline{3-4}\n\\hline\n\\multirow{2}[9]{2cm}{Impossible to answer}&\n\\centering\n\\multirow{2}[9]{1.5cm}{0.8\\%}&\n第二节比赛开始1分30秒时间内会不会有三分球命中？&Will a three-pointer be made in the first 90s of the second quarter?\\\\\n\\cline{3-4}\n& &本场比赛会不会在北京时间10时58分之前结束？&Will the game end before 10:58 a.m.?\\\\\n\\cline{3-4}\n\\hline\n\\end{tabular}\n\\caption{Questions statistics and examples sorted by the location of their corresponding evidence.}\n\\label{loc}\n\\end{table*}\n\\end{CJK}\n\n\\begin{CJK}{UTF8}{gbsn}\n\\begin{table*}[t!]\n\\centering\n\\begin{tabular}{|p{2cm}|p{1.5cm}|p{4cm}|p{4.5cm}|}\n\\hline\nQuestion type&Proportion&Example&Translation\\\\\n\\hline\n\\centering\n\\multirow{2}[6]{2cm}{Comparison}&\n\\centering\n\\multirow{2}[6]{1.5cm}{16.6\\%}&\n勒布朗-詹姆斯本场能否得到26分或更多？&Will Lebron James get 26 points or more in this game?\\\\\n\\cline{3-4}\n& &本场谁的得分会更高？&Who will get higher score in this game?\\\\\n\\cline{3-4}\n\\hline\n\\multirow{2}[9]{2cm}{Calculation}&\n\\centering\n\\multirow{2}[9]{1.5cm}{25.4\\%}&\n本场凯尔特人能否赢猛龙3分或更多？&Will the Celtics beat the Raptors by more than 3 points?\\\\\n\\cline{3-4}\n& &本场两队总得分能否达到207分或更多？&Will the total score of the two teams reach 207 points or more?\\\\\n\\cline{3-4}\n\\hline\n\\multirow{2}[10]{2cm}{Inference}&\n\\centering\n\\multirow{2}[10]{1.5cm}{28.5\\%}&\n暂停回来，雷霆队首次进攻能否得分？&After the timeout, will the Thunder score in their first round of attack?\\\\\n\\cline{3-4}\n& &第二节比赛雷霆队最后一分会不会由威斯布鲁克得到？&Will the last point of the Thunder in the second quarter be got by Westbrook?\\\\\n\\cline{3-4}\n\\hline\n\\multirow{2}[8]{2cm}{Tracking}&\n\\centering\n\\multirow{2}[8]{1.5cm}{29.5\\%}&\n太阳队能否在本场命中8个或更多三分球？&Will the Suns make 8 three-pointers or more in this game?\\\\\n\\cline{3-4}\n& &凯文-乐福首节犯规数会不会达到2次？&Will Kevin Love commit 2 fouls or more in the first quarter?\\\\\n\\cline{3-4}\n\\hline\n\\end{tabular}\n\\caption{Questions statistics and examples sorted by how the inference process is done.}\n\\label{inf}\n\\end{table*}\n\\end{CJK}\n\nBecause most of the questions are associated with some numerical data in the game, we also classify the questions according to how the numerical data is performed. Four types of operations are commonly used including: \\textit{Comparison}, \\textit{Calculation}, \\textit{Inference} and \\textit{Tracking}. Then the questions are correspondingly classified are introduced in the following subsections. We also give some examples in Table \\ref{inf}.\n\n\\subsection{Comparison}\nTo answer the comparison questions, we usually need to find the comparative figures for the corresponding objects.\nFor example, the commentator asks which of the two players will score more or which team will win.\nThe second row in Table \\ref{inf} belong to the \\textit{Comparison} questions. The easiest way to solve this kind of questions is to find the two figures appearing in the text and comparing them. \nIt is likely to acquire such figures after the game ends, and the specific figures usually appear together in a summary of the game in the end. Thus, matching techniques are still necessary to the final answer.\n\n\\subsection{Calculation}\nThe \\textit{Calculation} questions require extracting two or three figures and calculating their sum or difference. They differ from the Comparison questions in two ways -- the figures are more scattered and a calculation step is needed. This means that a respondent has to look for more information efficiently. After the figures are obtained, if a respondent misjudges the type or the direction of the calculation, he will still probably get a wrong answer. \nSimilar to the \\textit{Comparison} questions, the \\textit{Calculation} questions are mainly dependent on the correct sentences where the figures are located. These two kinds of questions are relatively easy compared to those ones which are not based on certain sentences. The second row in Figure \\ref{inf} give two example questions.\n\\subsection{Inference}\nThe third and fourth type of questions require the ability of summarizing and tracking information. A question of the third type needs a respondent to infer some figures through the text. For example, a question may be \"After this timeout, will the Cavaliers score in the first round of attack?\". The commentator  obviously will not say that \"The Cavaliers scored 2 points.\" or \"The Cavaliers didn't score.\" A respondent may get the answer as \"JR Smith makes a 2-point shot.\" Another example is \"Will the last point of this quarter be scored through a free throw?\" The information comes from the text of \"Anthony Davis makes his second free throw ... The match ends!\". It is impossible to get a reasonable answer by matching. \n\n\\subsection{Tracking}\nThe Tracking questions require more scattered information. A respondent should collect and accumulate specific information from a part of the passage, as the question is based on events happening repeatedly in a quarter or half of the game. For example, some questions ask about how many free-throws a player \\textit{A} will make in a quarter. As this figure does not appear in the passage, a respondent needs to count how many times the event '\\textit{A} makes a free-throw' occurs. In other words, it is necessary to track events relevant to the player '\\textit{A}' and 'free-throw'. When the player(\\textit{A}) is replaced with one team name, the new question is even \nmore difficult because the information about each player belonging to the team should be tracked. Therefore, information tracking leads this kind of questions to be the most challenging ones in the dataset.\n\n\\section{Baseline Models and Results}\n\\subsection{Models}\nTo evaluate the QA performance on the LiveQA dataset, we implement 3 baseline models. The first is based on  random selection, where the system randomly chooses a choice as the answer. The second is to choose the dominant option of each question. More concretely, 80.0\\% of questions are in format of 'yes' and 'no', where 57.8\\% has the answer 'no'. For the other multiple choice questions, 50.6\\% of them take the second option as the right answer.\nThus, for 'yes/no' questions, we choose 'no', otherwise we choose the second option.\n\nWe also build a neural-network style baseline for our dataset to evaluate how state-of-the-art QA systems perform on the LiveQA dataset. Due to the uniqueness of our dataset, most of existing machine comprehension models are not suitable to it. For example, the QANet \\cite{yu2018qanet} model, which used to be a state-of-art model of SQuAD \\cite{rajpurkar2016squad}, is unavailable because it predicts the probability distribution of an answer's starting position and ending position in the context. But in LiveQA, a number of right answers do not directly appear in the context (e.g. an answer in format of 'can' or 'cannot'). \nUp to now, none of machine reading comprehension models has been designed for a dataset with consideration of timeline and mathematical computations. \nThat means that the existing ones will not be likely to perform well on our dataset. The closest work to ours is multi-hop question answering, and thus we use a novel model Gated-Attention Reader \\cite{dhingra2016gated} to experiment on LiveQA. \n\nGated-Attention Reader (GA) is an attention mechanism which uses multiplicative interactions between the query embedding and intermediate states of a recurrent neural network reader. GA enables a model to scan one document and the questions iteratively for multiple passes, and thus the multi-hop structure can target on most relevant parts of the document. It used to be the state-of-art model of several datasets, such as CNN/Daily Mail dataset \\cite{Hermann2015Teaching} and CBT dataset \\cite{hill2015goldilocks}. \n\nThe full context, which is usually composed of more than 1,000 sentences on average, is too heavy for GA as input. To apply GA to our dataset, we propose a pipeline method to first extract %the answer from\na set of candidate evidence sentences from the full content, and then apply the GA model on this set of sentences to predict the final answer. We employ TF-IDF style matching score to extract 50 most relevant sentences as the supporting evidence. %a question's document.\nTo improve the accuracy of selecting the evidence candidates, if the question clearly requires some information after the game ends, we use the ending part of the content as the input.\n\nSpecifically, taken the embedding representation of a token, the Bi-directional Gated Recurrent Units (BiGRU) process the sequence in both forward and backward directions to produce two sequences of token-level representations, which are concatenated at the output as the final representation of the token. \nTo perform multi-hop inference, the GA model reads the document and the query over $k$ horizontal layers, where layer $k$ receives the contextual embeddings $X_{(k-1)}$ of the document from the previous layer. \nAt each layer, the document representation $D^{(k)}$ is computed by taking the full output of a document BiGRU where the previous layer embedding $X_{(k-1)}$ is the input. At the same time, a layer-specific query representation $Q^{(k)}$ is computed as the full output of a separate query BiGRU taking the query embedding $Y$ as the input. The Gated-Attention is applied to $D^{(k)}$ and $Q^{(k)}$ to compute the contextual embedding $X^{(k)}$.\n\n\\begin{equation}\n    X^{(k)} = GAttn(BiGRU(X^{(k-1)}), BiGRU(Y))\n\\end{equation}\n\nAfter obtaining the query-awared document representation, we perform answer prediction by matching the similarity of answer and content. We use bidirectional Gated Recurrent Units to encode the candidate answers into vectors $A^(i)$, and then we compute matching score between summarized document and candidates using a bilinear attention.\nFinally we calculate the probability distribution of the options with softmax. The operations are similar to those in RACE \\cite{lai2017race}.\n\n\\begin{equation}\n    s = softmax([Blin(A^i,D^{(k)});]^{i=1}_{n})\n\\end{equation}\n\n\\subsection{Model Evaluation}\n\n\\begin{table}[h]\n\\begin{center}\n\\begin{tabular}{ll}\n\\hline\n\\bf Model                              & \\bf Acc        \\\\\n\\hline\nRandom                          & 50.0\\% \\\\\nDominant & \\textbf{56.4\\%}\\\\\nGA & 53.1\\%\\\\\n\n\\hline\n\\end{tabular}\n\\end{center}\n\\caption{\\label{rel} The results of different baseline models on the test set. Random denotes randomly selecting an answer. Dominate denotes selecting the dominate option. GA denotes the gated-attention reader.}\n\\end{table}\n\nFor the three baseline models, performance is reported with the accuracy on the test set in Table \\ref{rel}. The random selection method (Random) scores 50.0\\%, while the dominant option method (Dominate) reaches a score of 56.4\\%, which shows that our dataset does not have a certain pattern for the answers. Meanwhile, GA, which is a strong baseline for previous question answering problems, failed to perform better than the dominant option method and only achieves a score of 53.1\\%. Such results show that our dataset is challenging and needs further investigation for model design. In future work, how to incorporate temporal information and mathematical calculation into a QA model is the focus.\n\n\\subsection{Case Study}\n\n\\begin{CJK}{UTF8}{gbsn}\n\\begin{table*}[ht]\n\\centering\n\\begin{tabular}{|p{3.5cm}|p{3.5cm}|p{2.5cm}|p{2.5cm}|}\n\\hline\nQuestion&Translation&Correct answer&Answer given by the model\\\\\n\\hline\n跳球之争！本场比赛哪支球队获得第一轮进攻球权？&Jump ball fight! Which team will win the chance of the first round of offence?&勇士(The Warriors)&勇士(The Warriors)\\\\\n\\hline\n湖人全场总得分是奇数还是偶数？&Will the total score of the Lakers at the end of the game be odd or even?&奇数(odd)&奇数(odd)\\\\\n\\hline\n尼克杨第二节能否命中3分球？&Can Nick Young make a three pointer in the second quarter?&能(Yes)&不能(No)\\\\\n\\hline\n第三节结束，76人能否领先湖人4分或更多？&At the end of the third quarter, Will the 76ers lead the Lakers by 4 points of more?&不能(No)&能(Yes)\\\\\n\\hline\n谁先获得30分？&Who will score his 30th point earlier?&24分的哈登(James Harden who has got 24 points)&25分的托马斯(Isaiah Thomas who has got 25 points)\\\\\n\\hline\n\\end{tabular}\n\\caption{Cases in the experimental results}\n\\label{case}\n\\end{table*}\n\\end{CJK}\n\nIn this subsection, we further analyze the prediction ability of the GA model.\nTable \\ref{case} shows some prediction cases in experimental results. \nFrom the first two questions, we can see that the model gives the correct answers when judging the result of a specific event. But for the other three questions which involve multiple events, the model fails to answer them correctly. A possible explanation is that, although GA is designed for multi-hop inference, it lacks ability in both  information tracking and math calculation, which makes it difficult for the model to track down some complicated events. \n\nWe can see, for reading comprehension models that extract answers based on the similarity between the answer and the content, they would fail on LiveQA due to the fact that they cannot track down temporal information nor perform mathematical calculation.\nTo outperform existing models on LiveQA, the system should consider focusing on tracking information of a certain event through the timeline. It should also have the ability to perform mathematical inference between different contents. \n\n\\section{Conclusion}\nIn this paper, we present LiveQA, a question answering dataset constructed from play-by-play live broadcast. LiveQA can evaluate a machine reading comprehension model in its ability to understand the timeline, track events and do mathematical calculation. It  consists of 117k questions, which are time-dependent and need math inference. Due to the novel characteristics, it is hard for existing QA models to perform well on LiveQA. We expect our dataset will stimulate the development of more advanced machine comprehension models.\n\n\\section*{Acknowledgement}\nWe thank the anonymous reviewers for their helpful comments on this paper. This work was partially supported by  National Natural Science Foundation Project of China (61876009), National Key Research and Development Project\n(2019YFB1704002), and National Social Science Foundation Project of China (18ZDA295). The corresponding author of this paper is Sujian Li.\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{\\textsc{SportsSum2.0}: Generating High-Quality Sports News from Live Text Commentary}\n\n\\begin{document}\n\n\\fancyhead{}\n\\title{\\textsc{SportsSum2.0}: Generating High-Quality Sports News from Live Text Commentary}\n\n\\author{\nJiaan Wang$^{\\dagger*}$, Zhixu Li$^{\\ddagger*(\\textrm{\\Letter})}$, Qiang Yang$^{\\clubsuit}$, Jianfeng Qu$^{\\dagger}$, Zhigang Chen$^{\\spadesuit\\heartsuit}$ \\\\ Qingsheng Liu$^{\\diamondsuit}$, and Guoping Hu$^{\\spadesuit}$\n}\n\\makeatletter\n\\def\\authornotetext#1{\n \\g@addto@macro\\@authornotes{%\n \\stepcounter{footnote}\\footnotetext{#1}}%\n}\n\\makeatother\n\n\\authornotetext{The first two authors made equal contributions to this work.}\n\n\\affiliation{%\n  \\institution{$^{\\dagger}$ School of Computer Science and Technology, Soochow University, Suzhou, China}\n  \\country{}\n}\n\\affiliation{%\n  \\institution{$^{\\ddagger}$ Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University}\n  \\country{}\n}\n\\affiliation{%\n  \\institution{$^{\\clubsuit}$ King Abdullah University of Science and Technology \\quad $^{\\heartsuit}$ iFLYTEK Research, Suzhou}\n  \\country{}\n}\n\n\\affiliation{%\n  \\institution{$^{\\spadesuit}$ State Key Laboratory of Cognitive Intelligence, iFLYTEK Research \\quad $^{\\diamondsuit}$ Anhui Toycloud Technology}\n  \\country{}\n}\n\n\\email{jawang1@stu.suda.edu.cn, zhixuli@fudan.edu.cn}\n\n\\begin{abstract}\nSports game summarization aims to generate news articles from live text commentaries.\nA recent state-of-the-art work, \\textsc{SportsSum}, not only constructs a large benchmark dataset, but also proposes a two-step framework. Despite its great contributions, the work has three main drawbacks: 1) the noise existed in \\textsc{SportsSum} dataset degrades the summarization performance; 2) the neglect of lexical overlap between news and commentaries results in low-quality pseudo-labeling algorithm; 3) the usage of directly concatenating rewritten sentences to form news limits its practicability.\nIn this paper, we publish a new benchmark dataset \\textsc{SportsSum2.0}, together with a modified summarization framework.\nIn particular, to obtain a clean dataset, we employ crowd workers to manually clean the original dataset.\nMoreover, the degree of lexical overlap is incorporated into the generation of pseudo labels.\nFurther, we introduce a reranker-enhanced summarizer to take into account the fluency and expressiveness of the summarized news.\nExtensive experiments show that our model outperforms the state-of-the-art baseline.\n\\end{abstract}\n\n\\begin{CCSXML}\n<ccs2012>\n<concept>\n<concept_id>10002951.10003317.10003347.10003357</concept_id>\n<concept_desc>Information systems~Summarization</concept_desc>\n<concept_significance>500</concept_significance>\n</concept>\n</ccs2012>\n\\end{CCSXML}\n\\ccsdesc[500]{Information systems~Summarization}\n\n\\keywords{datasets, sports game summarization, text summarization}\n\n\\maketitle\n\n{%\\par\n  \\medskip\\small\\noindent{\\bfseries ACM Reference Format:}\\par\\nobreak\n  \\noindent\\bgroup\\def\\\\{\\unskip{}, \\ignorespaces}{Jiaan Wang, Qiang Yang, Jianfeng Qu, Zhixu Li, Zhigang Chen, Qingsheng Liu, Guoping Hu}\\egroup. 2021. \\textsc{SportsSum2.0}: Generating High-Quality Sports News from Live Text Commentary. In \\textit{Proceedings of the 30th ACM Int'l Conf. on Information and Knowledge Management (CIKM '21), November 1--5, 2021, Virtual Event, Australia}\\textit{.} ACM, New York, NY, USA, \\ref{TotPages}~pages. https://doi.org/10.1145/3459637.3482188\n  }\n\n\\section{Introduction}\nText Summarization aims at compressing the original document into a shorter text while preserving the main ideas~\\cite{rush-etal-2015-neural,chopra-etal-2016-abstractive,Nallapati2016AbstractiveTS,See2017GetTT,chen-bansal-2018-fast}.\nA special text summarization task in sports domain is Sports Game Summarization, as the example shown in Fig.~\\ref{example}, which focuses on generating news articles from live commentaries~\\cite{zhang-etal-2016-towards}. Obviously, this task is more challenging than conventional text summarization for two reasons: First, the length of live commentary document often reaches thousands of tokens, which is far beyond the capacity of mainstream PLMs (e.g., BERT, RoBERTa, etc.); Second, there are different text styles between commentaries and news. Specifically, commentaries are more colloquial than news. \n\n\\begin{figure}[t]\n\\centerline{\\includegraphics[width=0.40\\textwidth]{3.png}}\n\\caption{An example of Sports Game Summarization.}\n\\label{example}\n\\end{figure} \n\nSports game summarization has gradually attracted attention from researchers due to its practical significance.\nZhang et al.~\\cite{zhang-etal-2016-towards} pioneer this task and construct the first dataset with only 150 samples. Another dataset created for the shared task at NLPCC 2016 contains 900 samples~\\cite{Wan2016OverviewOT}. Although these datasets promote the research to some extent, they cannot further fit more sophisticated models due to their limited scale.\nAdditionally, early literature~\\cite{zhang-etal-2016-towards,Zhu2016ResearchOS,Yao2017ContentSF,Liu2016SportsNG} on these datasets mainly explores different strategies to select important commentary sentences to form news directly, but ignores the different text styles between commentaries and news.\nIn recent, Huang et al.~\\cite{Huang2020GeneratingSN} present \\textsc{SportsSum}, the first large-scale sports game summarization dataset with 5,428 samples. This work also proposes a two-step model which first \\textbf{selects} important commentary sentences, and then \\textbf{rewrites} each selected sentence to a news sentence so as to form news.\nIn order to provide training data for both \\textbf{selector} and \\textbf{rewriter} of the two-step model, a pseudo-labeling algorithm is introduced to find for each news sentence a corresponding commentary sentence according to their timeline information as well as semantic similarities.\n\nGiven all the existing efforts, this task is still not fully exploited in the following aspects:\n(1) The existing datasets are limited in either scale or quality. According to our observations on \\textsc{SportsSum}, more than 15\\% of samples have noisy sentences in the news articles due to its simple rule-based data cleaning process;\n(2) The pseudo-labeling algorithm used in existing two-step models only considers the semantic similarities between news sentences and commentary sentences but neglects the lexical overlap between them, which actually is an useful clue for generating the pseudo label;\n(3) The existing approaches rely on direct stitching to constitute news with the (rewritten) selected sentences, resulting in low fluency and high redundancy problems due to each (rewritten) selected sentence is transparent to other sentences. \n\nTherefore, in this paper, we first denoise the \\textsc{SportSum} dataset to obtain a higher-quality \\textsc{SportsSum2.0} dataset.\nSecondly, lexical overlaps between news sentences and commentary sentences are taken into account by our advanced pseudo-labeling algorithm.\nLastly, we extend the two-step framework~\\cite{Huang2020GeneratingSN} to a novel reranker-enhanced summarizer, where the last step reranks the rewritten sentences w.r.t importance, fluency and redundancy.\nWe evaluate our model on the \\dataset and \\sports~\\cite{Huang2020GeneratingSN} datasets. The experimental results show that our model achieves state-of-the-art performance in terms of ROUGE Scores. \n\n\\begin{figure}[t]\n\\centerline{\\includegraphics[width=0.40\\textwidth]{case.png}}\n\\caption{Noise existed in \\sports Dataset. The first example contains the descriptions of other games while the second one has the history-related descriptions. The last case includes an advertisement and irrelevant hyperlink text.}\n\\label{noisy_samples}\n\\end{figure} \n\n\\section{Data Construction}\nIn this section, we first analyze the existing noises in \\textsc{SportsSum}, and then introduce the details of manual cleaning process. Lastly, we show the statistics of \\textsc{SportsSum2.0}.\n\n\\vspace{1ex}\n\\noindent\\textbf{Noise Analysis.}\n\\textsc{SportsSum}~\\cite{Huang2020GeneratingSN} is the only public large-scale sports game summarization dataset with 5,428 samples crawled from Chinese sports website. During the observation of \\textsc{SportsSum}, we find more than 15\\% of \\emph{news articles} have noisy sentences. Specifically, we divide these noises into three classes and show an example for each class in Fig.~\\ref{noisy_samples}:\n\\begin{itemize}[leftmargin=*,topsep=0pt]\n\\item Descriptions of other games: a news website may contain news articles of multiple games, which is neglected by \\textsc{SportsSum} and resulted in 2.2\\% (119/5428) of news articles include descriptions of other games. \n\\item Descriptions of history: There are many sports news describe the matching history at the beginning, which cannot be inferred from the commentaries. To alleviate this issue, \\sports adopts a rule-based method to identify the starting keywords (e.g., ``at the beginning of the game'') and remove the descriptions before the keywords. However, about 4.6\\% (252/5428) of news articles cannot be correctly disposed by this method.\n\\item Advertisements and irrelevant hyperlink text: We find that about 9.8\\% (531/5428) of news articles in \\sports have such noise.\n\\end{itemize}\n\n\\vspace{1ex}\n\\noindent\\textbf{Manual Cleaning Process.}\nIn order to reduce the noise of \\emph{news articles}, we design a manual cleaning process. For a given news article, we first remove the descriptions related to other games, and then delete advertisements and irrelevant hyperlink text. Finally, we discard the descriptions of history.\n\n\\vspace{1ex}\n\\noindent\\textbf{Annotation Process.}\nWe recruit 7 master students to perform the manual cleaning process. In order to help the annotators fully understand our manual cleaning process, every annotator is trained in a pre-annotation process. All the cleaning results are checked by another two data experts. The manual cleaning process costs about 200 human hours in total. After manual cleaning, we discard 26 bad cases which do not contain the descriptions of current games and finally obtain 5402 human-cleaned samples. \n\n\\vspace{1ex}\n\\noindent\\textbf{Statistics.}\nTable ~\\ref{table:statistic} shows statistics of \\textsc{SportsSum2.0} and \\textsc{SportsSum}. The average length of commentary documents in \\textsc{SportsSum2.0} is slightly different from the counterpart in \\textsc{SportsSum} due to the 26 removed bad cases.\n\n\\begin{table}[t]\n\\setlength{\\belowcaptionskip}{5pt}\n\\centering\n\\resizebox{0.40\\textwidth}{!}{\n  \\begin{tabular}{|c|cc|cc|}\n  \\hline\n  \\multirow{2}{*}{Source} & \\multicolumn{2}{c|}{\\textsc{SportsSum2.0}} & \\multicolumn{2}{c|}{\\textsc{SportsSum}} \\\\\n                                  & Commentary       & News        & Commentary        & News          \\\\ \\hline\n  Avg. \\#chars           & 3464.31          & 771.93      & 3459.97           & 801.11        \\\\\n  Avg. \\#words            & 1828.56          & 406.81      & 1825.63           & 427.98        \\\\\n  Avg. \\#sent.            & 194.10           & 22.05       & 193.77            & 23.80         \\\\ \\hline\n  \\end{tabular}\n}\n\\caption{The statistics of \\textsc{SportsSum2.0} and \\textsc{SportsSum}.}\n\\label{table:statistic}\n\\vspace{-7pt}\n\\end{table}\n\n\\section{Methodology}\nAs shown in Fig.~\\ref{example}, the goal of sports game summarization is to generate sports news $R=\\{r_{1},r_{2},...,r_{n}\\} $ from a given live commentary document $C=\\{(t_{1},s_{1},c_{1}),...,(t_{m},s_{m},c_{m})\\}$ $(m \\geq n)$. $r_{i}$ represents $i$-th news sentence and $(t_{j},s_{j},c_{j})$ is $j$-th commentary, where $t_{j}$ is the timeline information, $s_{j}$ denotes the current scores and $c_{j}$ is the commentary sentence.\n\nFig.~\\ref{fig:overview} shows the overview of our reranker-enhanced summarizer which first learns a \\textbf{selector} to extract important commentary sentences, then uses a \\textbf{rewriter} to convert each selected sentence to a news sentence. Finally,  a \\textbf{reranker} is introduced to generate a news article based on the rewritten sentences. \n\n\\subsection{Pseudo-Labeling Algorithm}\nTo train the selector and rewriter, we need labels to indicate the importance of commentary sentences and their corresponding news sentences. Following \\textsc{SportsSum}\\cite{Huang2020GeneratingSN}, to obtain the labels, we \\emph{map each news sentence to its commentary sentence} through a pseudo-labeling algorithm that considers both timeline information and similarity metrics.\nThough there is no explicit timeline information of news sentences, but most of sentences start with ``in the n-th minutes'' which indicates its timeline information.\n\nFor each news sentence $r_{i}$, we first extract time information $h_{i}$ if possible, and then construct a candidate commentaries set $C^{(i)}$, where the timeline information of each commentary belongs to $[h_{i},h_{i}+3]$. Lastly, we select one commentary sentence with the highest similarity with $r_{i}$ from $C^{(i)}$, and form a pair of a mapped commentary sentence and a news sentence.\n\n\\begin{figure}[t]\n  \\centerline{\\includegraphics[width=0.45\\textwidth]{2.png}}\n  \\caption{The overview of the reranker-enhanced summarizer.}\n  \\label{fig:overview}\n  \\end{figure}\n\nThe similarity function used by Huang et al.~\\cite{Huang2020GeneratingSN} is BERTScore~\\cite{Zhang2020BERTScoreET}, which only considers the semantic similarity between news and commentaries but neglects the lexical overlap between them. However, the lexical overlap actually is a useful clue, e.g., the matching probability of a news sentence and a commentary sentence will be greatly improved if the same entity mentions appear both in these two sentences. Therefore, we decide to consider both semantics and lexical overlap in similarity function:\n\\begin{equation}\n\\label{similarity_function}\nS(r_{i},c_{j}) = \\lambda B(r_{i},c_{j}) + (1-\\lambda) R(r_{i},c_{j})\n\\end{equation}\n\nThe similarity function, i.e., $S(\\cdot, \\cdot)$ is calculated by linearly combining BERTScore function, i.e., $B(\\cdot, \\cdot)$ and ROUGE score function, i.e., $R(\\cdot, \\cdot)$. The coefficient $\\lambda$ is a hyper parameter.\n\nWith the above process, we finally obtain a large number of pairs of a mapped commentary sentence and a news sentence, which can be used for training selector and rewriter.\n\n\\begin{table*}[t]\n\\setlength{\\belowcaptionskip}{5pt}\n  \\centering\n  \\resizebox{0.80\\textwidth}{!}\n  {\n    \\centering\n    \\begin{tabular}{cclcccccc}\n      \\cline{1-9}\n      \\multirow{2}{*}{Method}   & \\multirow{2}{*}{\\#}                                                     & \\multirow{2}{*}{Model} & \\multicolumn{3}{c}{\\dataset}                                   & \\multicolumn{3}{c}{\\sports}                                      \\\\\n                                                                                     &  &                      & ROUGE-1              & ROUGE-2              & ROUGE-L              & ROUGE-1              & ROUGE-2              & ROUGE-L              \\\\ \\cline{1-9}\n      \\multirow{2}{*}{Extractive Models}    & 1                                         & TextRank               & 20.53                & 6.14                 & 19.64                & 18.37                & 5.69                 & 17.23                \\\\\n                                                         & 2                            & PacSum                 & 23.13                & 7.18                 & 22.04                & 21.84                & 6.56                 & 20.19                \\\\ \\cline{1-9}\n      \\multirow{2}{*}{Abstractive Models}                & 3                            & Abs-LSTM                   & 31.14                & 11.22                & 30.36                & 29.22                & 10.94                & 28.09                \\\\\n                                                          & 4                           & Abs-PGNet                  & 35.98                & 13.07                & 35.09                & 33.21                & 11.76                & 32.37                \\\\ \\cline{1-9}\n      \\multirow{5}{*}{\\makecell[c]{Two Step Framework \\\\ (Selector + Rewriter)}}       & 5               & SportsSUM$^{\\dagger}$              & 44.73                & 18.90                & 44.03                & 43.17                & 18.66                & 42.27                \\\\\n                                                                               & 6      & PGNet$^{*}$                  & 45.13                & 19.13                & 44.12                & 44.12                & 18.89                & 43.23                \\\\\n                                                                                & 7     & mBART$^{*}$                  & 47.23                & 19.27                & 46.54                & 46.43                & 19.54                & 46.21                \\\\\n                                                                                & 8     & Bert2bert$\\dagger$              & 46.85                & 19.24                & 46.12                & 46.54                & 19.32                & 45.93                \\\\\n                                                                                & 9     & Bert2bert$^{*}$              & 47.54                & 19.87                & 46.99                & 47.08                & 19.63                & 46.87                \\\\ \\cline{1-9}\n      \\multirow{4}{*}{\\makecell[c]{Reranker-Enhanced Summarizer \\\\ (Selector + Rewriter + Reranker)}} & 10 & PGNet$^{*}$                  & 46.23                & 19.41                & 45.37                & 45.54                & 19.02                & 45.31                \\\\\n                                                                            & 11         & mBART$^{*}$                  & 47.62                & 19.73                & 47.19                & 46.89                & \\textbf{19.72}       & 46.53                \\\\\n                                                                         & 12            & Bert2bert$^{\\dagger}$             & 47.32       & 19.33       & 47.01       & 46.14      & 19.32                & 45.53       \\\\                                                                                    & 13         & Bert2bert$^{*}$             & \\textbf{48.13}       & \\textbf{20.09}       & \\textbf{47.78}       & \\textbf{47.61}       & 19.65                & \\textbf{47.49}       \\\\ \\cline{1-9}                                               \n      \\end{tabular}\n  }\n  \\caption{Experimental results on \\textsc{SportsSum2.0} and \\textsc{SportsSum}. The models with $^{*}$ denotes they use our advanced pseudo-labeling algorithm while $^{\\dagger}$ indicates the models utilize the original pseudo-labeling algorithm~\\cite{Huang2020GeneratingSN}.}\n  \\label{table:result}\n  \\vspace{-2em}\n\\end{table*}\n\n\\subsection{Reranker-Enhanced Summarizer}\nOur reranker-enhanced summarizer extends the two-step model~\\cite{Huang2020GeneratingSN} to consider the importance, fluency and redundancy of the rewritten news sentences, as shown in Fig.~\\ref{fig:overview}.\nParticularly, we first select important commentary sentences, and then rewrite each selected sentence to a news sentence by a seq2seq model. However, the readability and fluency of these news sentences vary a lot. Some new sentences can be directly used in the final news while others are inappropriate due to the low fluency, which is one common problem in natural language generation, representing by repetition~\\cite{See2017GetTT} and incoherence~\\cite{bellec2017deep}.\nSo we use the reranker to filter out low fluent sentences while retaining high informative sentences and controlling the redundancy between the rewritten sentences.\n\n\\vspace{1ex}\n\\noindent\\textbf{Selector.} Different from existing two-step model~\\cite{Huang2020GeneratingSN}, which purely use TextCNN~\\cite{Kim2014ConvolutionalNN} as selector and ignores the contexts of a commentary sentence, we design a context-aware selector which can capture the semantics well with a sliding window.\nIn detail, we train a binary classifier to choose important commentary sentences. When training, for each commentary $c_{i}$ in $C$, we assign a positive label if $c_{i}$ can be mapped with a news sentence by the pseudo-labeling algorithm. Otherwise, we give a negative label.\nHere, a RoBERTa~\\cite{Liu2019RoBERTaAR} is employed to extract the contextual representation of commentary sentences.\nwe first tokenize the commentary sentences. Then we concatenate the target commentary sentence and its partial context with special tokens as \\texttt{[CLS] commentary1 [SEP] commentary2 [SEP] ... [SEP] commentaryN [SEP]}. the target commentary sentence in the middle of the whole sequence, and the sequence is limited to 512 tokens.\nFor prediction, the sentence embedding is obtained by averaging the token embedding of the target commentary sentence, and then is followed by a sigmoid classifier.\nThe cross-entropy loss is used as the training objective for selector.\n\n\\vspace{1ex}\n\\noindent\\textbf{Rewriter.} Each selected commentary sentence first concatenates with its timeline information, and then rewrites to a news sentence through the seq2seq model. To be more specific, we choose the following three seq2seq models:\n\n\\begin{itemize}[leftmargin=*,topsep=0pt]\n\\item \\emph{Pointer-Generator Network}~\\cite{See2017GetTT} is a popular abstractive text summarization model with copy mechanism and coverage loss.\n\\item \\emph{Bert2bert}\\footnote{\\url{https://huggingface.co/transformers/model_doc/encoderdecoder.html}} is a seq2seq model, in which both encoder and decoder are initialized by BERT~\\cite{Devlin2019BERTPO}.\n\\item \\emph{BART}~\\cite{Lewis2020BARTDS} is a denoising autoencoder seq2seq model, which achieves state-of-the-art results on many language generation tasks. Since there is no Chinese version of BART for public use, we choose its multilingual version, i.e., mBART~\\cite{Liu2020MultilingualDP}.\n\\end{itemize}\n\n\\vspace{1ex}\n\\noindent\\textbf{Reranker.} Although the rewritten sentences can convey the semantic to some extent, there still exist low fluent sentences, which leads to poor readability.\nIn view of the ability of Maximal Marginal Relevance (MMR)~\\cite{carbonell1998use} on extracting the high informative and low redundant sentences, we decide to adopt this approach as reranker. Unfortunately, vanilla MMR cannot take fluency into account. So, we propose a variant MMR algorithm by incorporating the fluency of each rewritten sentence. \n\\begin{equation}\n\\begin{split}\nMMR(D, R)={\\underset {d_{i}\\in D-R}{argmax}}[\\lambda_{1}info(d_{i})+\\lambda_{2}flu(d_{i})\\\\\n-\\lambda_{3}\\max_{d_{j}\\in R} [sim(d_{i},d_{j})]]\\ \\  (\\lambda_{1} + \\lambda_{2} + \\lambda_{3} = 1)\n\\end{split}\n\\end{equation}\n\\begin{equation}\n  \\label{equ:flu}\n    flu(d_{i}) = 1 - perplexity(d_{i})/\\eta\n\\end{equation}\nwhere $D$ represents the whole news sentence set, $R$ denotes the selected news sentence set.\nThe $info(d_{i})$ is employed to calculate the importance of the news sentence. Note that, each commentary sentence has corresponding importance predicted by selector. So we directly use the importance of corresponding commentary sentence as $info(d_{i})$.\nThe $flu$ function in our MMR algorithm is utilized to indicate the fluency of a news sentence. We consider computing perplexity of the news sentence by GPT-2~\\cite{Radford2019LanguageMA}. Equal~\\ref{equ:flu} shows the details of $flu$ function.\nThe $sim(d_{i},d_{j})$ is BERTScore function.\nWe greedily select news sentences with the highest MMR score until the total length exceeds a pre-defined budget\\footnote{We set the average length of the news articles as the budget.}. \n\n\\section{Experiments}\n\\subsection{Implementation Details}\nWe split SportsSum2.0\\footnote{The dataset is available at \\url{https://github.com/krystalan/SportsSum2.0}} into three sets: training (4803 games), validation (300 games), and testing (299 games) sets$\\footnote{We keep the original splitting except for removed 26 ``bad cases\".}$. We trained all models on one 32GB Tesla V100 GPU. Our reranker-enhanced summarizer is implemented based on RoBERTa-large (24 layers with 1024 hidden size), mBART (12 layers with 1024 hidden size) and GPT-2 of huggingface Transformers~\\cite{wolf-etal-2020-transformers} with default settings. The learning rates of selector, PGNet, Bert2bert rewriter and mBART rewriter are 3e-5, 0.15, 2e-5 and 2e-5, respectively. For all models, we set the batch size to 32 and use Adamw optimizer. The coefficient $\\lambda$ in our pseudo-labeling algorithm is 0.70. We set the $\\lambda_{1}$, $\\lambda_{2}$ and $\\lambda_{3}$ to 0.6, 0.2 and 0.2 in our variant MMR.\n\n\\begin{figure}[t]\n\\centering\n\\subfigure[Info]{\n  \\includegraphics[width=0.20\\linewidth]{informativity.png}\n}\n\\subfigure[Redundancy]{\n  \\includegraphics[width=0.20\\linewidth]{redundancy.png}\n}\n\\subfigure[Fluency]{\n  \\includegraphics[width=0.20\\linewidth]{fluency.png}\n}\n\\subfigure[Overall]{\n  \\includegraphics[width=0.20\\linewidth]{overall.png}\n}\n\\caption{Results on human evaluation.}\n\\label{fig:huamn_evaluation}\n\\end{figure}\n\n\\subsection{Experimental Results}\nWe compare our three-step models with several conventional models, including extractive summarization models, abstractive summarization models and two-step models in terms of ROUGE scores.\n\nAs shown in Table~\\ref{table:result}, our reranker-enhanced models achieve significant improvement on both datasets.\nTextRank~\\cite{Mihalcea2004TextRankBO} and PacSum~\\cite{zheng-lapata-2019-sentence} are extractive summarization models, which are limited by the different text styles between commentaries and news. Abs-PGNet~\\cite{See2017GetTT} and Abs-LSTM (abstractive summarization models) achieve better performance since they can alleviate different styles issue, but still have the drawback of dealing with long texts.\nSportsSUM~\\cite{Huang2020GeneratingSN} is a two-step state-of-the-art model that performs better than the above models where the different text styles and long text issues are solved. \nOther two-step models enhance SportsSUM through improved pseudo-labeling algorithm, selector and rewriter. However, they still suffer from low fluency problem. Our best reranker-enhanced model outperforms SportsSUM by more than 2.8 and 3.5 points in the average of ROUGE scores on \\textsc{SportsSum2.0} and \\textsc{SportsSum}, respectively.\nAdditionally, the effectiveness of our advanced pseudo-labeling algorithm is proved by comparing row 8 to 9 or 12 to 13.\n\n\\subsection{Necessity of Manual Cleaning}\nTo further demonstrate the necessity of manual cleaning, we conduct a human evaluation on sports news generated by two reranker-enhanced summarizers trained on \\textsc{SportsSum2.0} and \\textsc{SportsSum}, respectively. We denote these two models as SUM-Clean and SUM-Noisy. Five postgraduate students are recruited and each one evaluates 100 samples for each summarizer. The evaluator scores generated sports news in terms of informativeness, redundancy, fluency and overall quality with a 3-point scale.\n\nFig.~\\ref{fig:huamn_evaluation} shows the human evaluation results. SUM-Clean performs better than the SUM-Noisy on all four aspects, especially in fluency. This indicates that the summarizer trained on noisy data degrades the performance of game summarization, and it is necessary to remove the noise existed in the dataset.\n\n  \n  \n\\section{Conclusion}\nIn this paper, we study the sports game summarization on the basis of SportsSum. A high-quality dataset SportsSum2.0 is constructed by removing or rectifying noises. We also propose a novel pseudo-labeling algorithm based on both semantics and lexicons. Furthermore, an improved framework is designed to improve the fluency of rewritten news. Experimental results show the effectiveness of our model in the SportsSum2.0.\n\n\\begin{acks}\nThis research is supported by National Key R\\&D Program of China (No. 2018-AAA0101900), the Priority Academic Program Development of Jiangsu Higher Education Institutions, National Natural Science Foundation of China (Grant No. 62072323, 61632016), Natural Science Foundation of Jiangsu Province (No. BK20191420), Suda-Toycloud Data Intelligence Joint Laboratory, and the Collaborative Innovation Center of Novel Software Technology and Industrialization.\n\\end{acks}\n\n\\balance\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Knowledge Enhanced Sports Game Summarization}\n\n\\begin{document}\n\n\\fancyhead{}\n\\title{Knowledge Enhanced Sports Game Summarization}\n\n\\author{\nJiaan Wang$^{1*}$, Zhixu Li$^{2*(\\textrm{\\Letter})}$, Tingyi Zhang$^{1}$, Duo Zheng$^{3}$, Jianfeng Qu$^{1(\\textrm{\\Letter})}$ \\\\ An Liu$^{1}$, Lei Zhao$^{1}$, and Zhigang Chen$^{4}$\n}\n\\makeatletter\n\\def\\authornotetext#1{\n \\g@addto@macro\\@authornotes{%\n \\stepcounter{footnote}\\footnotetext{#1}}%\n}\n\\makeatother\n\n\\authornotetext{The first two authors made equal contributions to this work.}\n\n\\affiliation{%\n  \\institution{$^{1}$ School of Computer Science and Technology, Soochow University, Suzhou, China}\n  \\country{}\n}\n\\affiliation{%\n  \\institution{$^{2}$ Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University}\n  \\country{}\n}\n\\affiliation{%\n  \\institution{$^{3}$ Beijing University of Posts and Telecommunications \\quad $^{4}$ iFLYTEK Research, Suzhou}\n  \\country{}\n}\n\n\\email{{jawang1, tyzhang1}@stu.suda.edu.cn,zhixuli@fudan.edu.cn, zd@bupt.edu.cn}\n\\email{{jfqu, anliu, zhaol}@suda.edu.cn, zgchen@iflytek.com}\n\n\\begin{abstract}\nSports game summarization aims at generating sports news from live commentaries.\nHowever, existing datasets are all constructed through automated collection and cleaning processes, resulting in a lot of noise. Besides, current works neglect the knowledge gap between live commentaries and sports news, which limits the performance of sports game summarization.\nIn this paper, we introduce \\textsc{K-SportsSum}, a new dataset with two characteristics:\n(1) \\textsc{K-SportsSum} collects a large amount of data from massive games. It has 7,854 commentary-news pairs. To improve the quality, \\textsc{K-SportsSum} employs a manual cleaning process;\n(2) Different from existing datasets, to narrow the knowledge gap, \\textsc{K-SportsSum} further provides a large-scale knowledge corpus that contains the information of 523 sports teams and 14,724 sports players.\nAdditionally, we also introduce a knowledge-enhanced summarizer that utilizes both live commentaries and the knowledge to generate sports news.\nExtensive experiments on \\textsc{K-SportsSum} and \\textsc{SportsSum} datasets show that our model achieves new state-of-the-art performances.\nQualitative analysis and human study further verify that our model generates more informative sports news.\n\\end{abstract}\n\n\\begin{CCSXML}\n<ccs2012>\n<concept>\n<concept_id>10002951.10003317.10003347.10003357</concept_id>\n<concept_desc>Information systems~Summarization</concept_desc>\n<concept_significance>500</concept_significance>\n</concept>\n</ccs2012>\n\\end{CCSXML}\n\n\\ccsdesc[500]{Information systems~Summarization}\n\n\\keywords{datasets, sports game summarization, text summarization}\n\n\\maketitle\n\n{%\\par\n  \\medskip\\small\\noindent{\\bfseries ACM Reference Format:}\\par\\nobreak\n  \\noindent\\bgroup\\def\\\\{\\unskip{}, \\ignorespaces}{Jiaan Wang, Zhixu Li, Tingyi Zhang, Duo Zheng, Jianfeng Qu, An Liu, Lei Zhao, and Zhigang Chen}\\egroup. 2022. Knowledge Enhanced Sports Game Summarization. In \\textit{Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM '22), February 21--25, 2022, Tempe, AZ, USA}\\textit{.} ACM, New York, NY, USA, \\ref{TotPages}~pages. https://doi.org/10.1145/3488560.3498405\n  }\n\n\\section{Introduction}\n\n\\begin{figure}[t]\n\\setlength{\\belowcaptionskip}{-12pt}\n\\centerline{\\includegraphics[width=0.45\\textwidth]{example.png}}\n\\caption{An example of Sports Game Summarization.}\n\\label{example}\n\\end{figure} \n\nIn recent years, a large number of sports games are carried out every day, and it is demanding to report corresponding news articles after games. Meanwhile, manually writing sports news is labor-intensive for professional editors.\nTherefore, how to automatically generate sports news has gradually attracted attention from both the research communities and industries.\nAs the example shown in Fig.~\\ref{example}, Sports Game Summarization aims at generating sports news articles based on live commentaries~\\cite{zhang-etal-2016-towards}. Ideally, the generated sports news should record the core events of a game that could help people efficiently catch up to games.\nCompared to the most prior work on traditional text summarization, the challenges of sports game summarization lie in three aspects:\n(1) The live commentaries record the whole events of a game, usually reaching thousands of tokens that is far beyond the typical 512 token limits of BERT-style pre-trained models;\n(2) The live commentaries have a different text style from the sports news. Specifically, commentaries are more informal and colloquial;\n(3) There is a knowledge gap between commentaries and news. Sports news usually contains additional knowledge of sports teams or players, which cannot be obtained from corresponding commentaries (we will discuss more in Sec.~\\ref{sec:knowledge_corpus}).\n\nMost existing works and datasets on sports game summarization treat the problem as a single document summarization task.\nZhang et al.~\\cite{zhang-etal-2016-towards} discuss this task for the first time and build the first dataset which has 150 commentary-news pairs. Wan et al.~\\cite{Wan2016OverviewOT} also contribute a dataset with 900 samples for NLPCC 2016 shared task. Early methods design diverse strategies to select key commentary sentences, and then either form the sports news directly~\\cite{zhang-etal-2016-towards,Zhu2016ResearchOS,Yao2017ContentSF} or relies on human-constructed templates to generate final news~\\cite{Liu2016SportsNG,lv2020generate}.\nRecently, Huang et al.~\\cite{Huang2020GeneratingSN} present \\textsc{SportsSum}, the first large-scale sports game summarization dataset with 5,428 samples. They crawl live commentaries and sports news from sports reports websites, and further adopt a rule-based cleaning process to clean the data.\nSpecifically, they first remove all HTML tags, and then for each news article, remove the descriptions before starting keywords which indicate the start of a game (e.g., ``at the beginning of the game'').\nThis is because there usually exist descriptions (e.g., matching history) at the beginning of news articles, which cannot be directly inferred from the corresponding commentaries.\nHuang et al.~\\cite{Huang2020GeneratingSN} also extend early methods~\\cite{zhang-etal-2016-towards,Zhu2016ResearchOS,Yao2017ContentSF,Liu2016SportsNG,lv2020generate} to a state-of-the-art two-step summarization framework, where each selected key commentary sentence is further rewritten to a news sentence through seq2seq models.\n\nDespite above progress, there are two shortcomings in the previous works.\nFirst, the previous datasets are limited in either scale or quality\\footnote{We note that another concurrent work \\textsc{SportsSum2.0}~\\cite{Wang2021SportsSum20GH} also employs manual cleaning process on large-scale sports game summarization dataset. However, they only clean the original \\textsc{SportsSum}~\\cite{Huang2020GeneratingSN} dataset. There are three major differences between our dataset and \\textsc{SportsSum2.0}: (1) the scale of our dataset is 1.45 times theirs; (2) our manual cleaning process also remove the history-related descriptions in the middle of news articles which is neglected by \\textsc{SportsSum2.0}; (3) our dataset also provide a large-scale knowledge corpus to alleviate the knowledge gap issue.}. The scale of early datasets~\\cite{zhang-etal-2016-towards,Wan2016OverviewOT} is less than 1,000 samples, which cannot be utilized to explore sophisticated supervised models.  \\textsc{SportsSum}~\\cite{Huang2020GeneratingSN} is many times larger than early datasets, but as shown in Fig.~\\ref{noisy_samples}, we find more than 15\\% of news articles in \\textsc{SportsSum} have noisy sentences due to its simple rule-based cleaning process (detailed in Sec.~\\ref{section:human_cleaning}).\nSecond, the knowledge gap between live commentaries and sports news is neglected by previous works. Though \\textsc{SportsSum} removes a part of descriptions at the beginning of news to alleviate the knowledge gap to some extent, there are still many other descriptions leading to the knowledge gap. Moreover, their two-step framework neglects the gap.\n\n\\begin{figure}[t]\n\\centerline{\\includegraphics[width=0.48\\textwidth]{case.png}}\n\\setlength{\\belowcaptionskip}{-10pt}\n\\caption{Noisy samples existed in \\textsc{SportsSum} dataset. The first example contains the irrelevant descriptions to current game while the second one includes an advertisement. The last case has irrelevant hyperlink text.}\n\\label{noisy_samples}\n\\end{figure}\n\nIn this paper, we introduce \\textsc{K-SportsSum}, a large-scale human-cleaned sports game summarization dataset which is constructed with the following features:\n(1) In order to improve both the scale and the quality of dataset, \\textsc{K-SportsSum} collects a large amount of data from massive games and further employs a strict manual cleaning process to denoise news articles.\nIt is a large scale sports game summarization dataset, which consists of 7,854 high quality commentary-news pairs;\n(2) To narrow the knowledge gap between live commentaries and sports news, \\textsc{K-SportsSum} also provides an abundant knowledge corpus including the information of 523 sports teams and 14,724 sports players.\nAdditionally, in the aspect of model design, we propose a knowledge-enhanced summarizer that first selects key commentary sentences, and then considers the information of the knowledge corpus during rewriting each selected sentence to a news sentence so as to form final news.\nThe experimental results on \\textsc{K-SportsSum} and \\textsc{SportsSum}~\\cite{Huang2020GeneratingSN} datasets show that our model achieves new state-of-the-art performances. We further conduct qualitative analysis and human study to verify that our model generates more informative sports news.\n\nWe highlight our contributions as follows:\n\\begin{itemize}[leftmargin=*,topsep=0pt]\n\\item We introduce a new sports game summarization dataset, i.e., \\textsc{K-SportsSum}, which contains 7,854 human-cleaned samples. To the best of our knowledge, \\textsc{K-SportsSum} is currently the highest quality and largest sports game summarization dataset$\\footnote{We release the data at \\url{https://github.com/krystalan/K-SportsSum}}$.\n\\item In order to narrow the knowledge gap between commentaries and news, we also provide an abundant knowledge corpus containing the information of 523 sports teams and 14,724 sports players.\n\\item A knowledge-enhanced summarizer is proposed to take the information of knowledge corpus into account when generating sports news. It is the first sports game summarization model which considers the knowledge gap issue.\n\\item The experimental results show our model achieves a new state-of-the-art performance on both \\textsc{K-SportsSum} and \\textsc{SportsSum} datasets. Qualitative analysis and human study further verify that our model generates better sports news.\n\\end{itemize}\n\n  \n\\section{Data Construction}\nIn this section, we first show how we collect live commentary documents and news articles from Sina Sports Live (\\S~\\ref{sec:data_collection}).\nSecondly, we analyze the noise existed in collected text and introduce the manual cleaning process to denoise the news articles (\\S~\\ref{section:human_cleaning}).\nThirdly, we discuss the collection process of knowledge corpus (\\S~\\ref{sec:knowledge_corpus}).\nFinally, we give the details of benchmark settings (\\S~\\ref{sec:benchmark_settings}).\n\n\\subsection{Data Collection}\n\\label{sec:data_collection}\nFollowing previous works~\\cite{zhang-etal-2016-towards,Wan2016OverviewOT,Huang2020GeneratingSN}, we crawl the records of football games from Sina Sports Live\\footnote{\\url{http://match.sports.sina.com.cn/index.html}}, the most influential football live services in China.\nNote that the existing research of sports game summarization is oriented to the football games which are the easiest to collect, but the methods and discussions can trivially generalize to other types of sports games.\nAfter crawling all football games data from 2012 to 2020, we remove HTML tags and obtain 8,640 live commentary documents together with corresponding news articles.\n\n\\begin{table*}[t]\n  \\centering\n  \\setlength{\\belowcaptionskip}{-10pt}\n  \\resizebox{0.80\\textwidth}{!}\n  {\n    \\begin{tabular}{c|c|l}\n      \\hline\n      \\textbf{Additional Knowledge}                     & \\%                    & \\multicolumn{1}{c}{\\textbf{Examples}}                                                                            \\\\ \\hline\n      \\multirow{2}{*}{\\textbf{Home or visiting team}} & \\multirow{2}{*}{14.7} & \\textbf{The home team} broke the deadlock in the 58th minute, Brian took a free kick and Lucchini scored     \\\\\n                                                   &                       & the goal. In the 77th minute, Nica...                                                                       \\\\ \\hline\n      \\multirow{4}{*}{\\textbf{Player Information}}          & \\multirow{4}{*}{6.3}  & Paris exceeded it again in the 26th minute! Motta took a corner kick from the right. Verratti, \\textbf{who is}   \\\\\n                                                   &                       & \\textbf{1.65 meters tall}, pushed into the near corner at a small angle 6 meters away from the goal, 2-1. ...    \\\\ \\cline{3-3}\n                                                   &                       & After another 3 minutes, Keita broke through Campagnaro and Brugman and passed back. Immobile   \\\\\n                                                   &                       & shot to the top of the goal. He faced the \\textbf{previous teams} without celebrating the goal. ...                      \\\\ \\hline\n      \\multirow{3}{*}{\\textbf{Team Information}}            & \\multirow{3}{*}{4.7}  & \\textbf{Purple Lily} equalized in the 29th minute, Tomovic broke through Ljajic and Duoduo from the right.   \\\\ \\cline{3-3} \n                                                   &                       & \\textbf{The last Champions League runner-up} completely controlled the game after the opening. Reus   \\\\\n                                                   &                       & shot higher once with two feet and was rescued by Mandanda once. ...                                                \\\\ \\hline\n      \\end{tabular}\n  }\n  \\setlength{\\belowcaptionskip}{3pt}\n  \\caption{Additional knowledge required for sports game summarization.}\n  \\label{table:addition_knowledge}\n\\end{table*}\n\n\\subsection{Data Cleaning}\n\\label{section:human_cleaning}\n\n\\subsubsection{Noise Analysis}\nWe find that the \\emph{live commentaries} are of high-quality due to the \\emph{structured form}. Nevertheless, the \\emph{news articles} are \\emph{unstructured text} usually containing noises. Specifically, we divide all noises into four types:\n\n\\begin{itemize}[leftmargin=*,topsep=0pt]\n\\item Description of other games: We find about 12\\% of the news pages contain multiple news articles belonging to different games, which has been neglected by \\textsc{SportsSum}~\\cite{Huang2020GeneratingSN}, resulting in 2.2\\% (119/5428) of news articles include descriptions of other games.\n\n\\item Advertisements: There are advertisements often appear in news articles. We find about 9.3\\% (505/5428) of news articles in \\textsc{SportsS\\\\um} have such noise.\n\n\\item Irrelevant hyperlink texts: Many new sentences contain hyperlink texts. Some of them can be regarded as part of the news, but others are irrelevant to current news, which we called irrelevant hyperlink texts. About 0.6\\% (31/5428) samples of \\textsc{SportsSum} have this kind of noise.\n\n\\item History-related descriptions: There are amount of news articles containing history-related descriptions which cannot be inferred from the corresponding live commentary document.\n\\textsc{SportsSum} adopts a rule-based approach to remove all news sentences before starting keywords to alleviate part of this noise.\nHowever, this approach cannot correctly dispose about 4.6\\% (252/5428) of the news articles, because the rule-based approach cannot cover all situations.\nIn addition, history-related descriptions may not only appear at the beginning of the news. Many news articles often introduce the matching history in the middle, e.g., if a player scores, the news may introduce the recent outstanding performance of the player in the previous rounds, or count his (her) total goals in the current season.\nWe also explain why we consider the history-related descriptions as noise rather than the knowledge gap between commentaries and news in Sec.~\\ref{sec:knowledge_corpus}.\n\\end{itemize}\n\n\\begin{figure}[t]\n\\centering\n\\setlength{\\belowcaptionskip}{-10pt}\n\\centerline{\\includegraphics[width=0.45\\textwidth]{interface.png}}\n\\caption{(a) Flow chart of manual cleaning process. (b) Screenshot of manual annotation interface.}\n\\label{fig:human}\n\\end{figure}\n\n\\subsubsection{Manual Cleaning Process}\nTo reduce the noise of news articles, we design a manual cleaning process with special consideration for sports game summarization.\nFig.~\\ref{fig:human}(a) shows the manual cleaning process. We first remove the description of other games. Secondly, we delete advertisements and irrelevant hyperlink text. Finally, the history-related descriptions of the game are identified and removed.\n\n\\subsubsection{Annotation Process}\nThe interface of manual cleaning is shown in Fig.~\\ref{fig:human}(b).\nWe recruit 9 master students, who are native Chinese speakers to perform the manual cleaning process. Firstly, we randomly select 50 news articles as test samples and ask all students to clean them at the same time. Based on the results, we decide 7 annotators and 2 senior annotators. After that, each news article will be randomly assigned to 2 annotators. If the cleaning results are inconsistent, it will be determined by a third senior annotator. Finally, all the cleaning results are checked by another two data experts. If the data experts think that the result does not meet the requirements, the news article will be assigned again.\n\n\\subsubsection{Post-processing}\nAfter the manual cleaning process, we discover some news articles do not contain the description of the current game, or contain little information, e.g., only include the results of the game.\nWe discard these news articles and retain 7,854 high-quality manually cleaned news articles which together with the corresponding live commentary documents constitute the \\textsc{K-SportsSum} dataset. \n\n\\subsection{Knowledge Corpus}\n\\label{sec:knowledge_corpus}\nAs we mentioned in Sec.~\\ref{section:human_cleaning}, the history-related descriptions in original news articles are regarded as noise and have been removed during the manual cleaning process due to the following reasons: (1) we follow the settings of \\textsc{SportsSum}~\\cite{Huang2020GeneratingSN} which adopt a rule-based approach to remove all sentences before the starting keywords. Most of the removed sentences are history-related descriptions; (2) the goal of sports game summarization is to generate news articles that can \\emph{record the key events of the \\textbf{current} sports games}. The history-related descriptions make little contribution to the goal.\n\nTo investigate if there are still other descriptions in the news articles, which also cannot be inferred from the corresponding live commentary document, we randomly select 300 samples from \\textsc{K-SportsSum} and manually analyze whether the news articles contain additional knowledge that cannot be obtained from commentary documents. Tab.~\\ref{table:addition_knowledge} shows statistics of required additional knowledge: (1) Some news articles replace the name of football teams with ``home team'' or ``visiting team'', which cannot be explicitly obtain from commentary documents; (2) Some news includes the personal information of players, e.g., height, birthday, previous teams; (3) A number of news articles contain prior knowledge of football teams, such as nickname (``Purple Lily'' is the nickname of ACF Fiorentina, which is shown in the penultimate line of Tab.~\\ref{table:addition_knowledge}) and past honors.\n\nTo narrow the knowledge gap between commentaries and news, we construct a knowledge corpus whose collection process contains the following three steps:\n\n\\begin{figure}[t]\n\\centering\n\\setlength{\\belowcaptionskip}{-10pt}\n\\centerline{\\includegraphics[width=0.45\\textwidth]{pages.png}}\n\\caption{The relations among metadata pages, team pages, player pages and Wikipedia articles. Each game has a metadata page that can link to related player or team pages. We further align each team page to the corresponding Wikipedia article.}\n\\label{fig:pages}\n\\end{figure}\n\n\\vspace{1ex}\n\\noindent\\textbf{Step 1: Metadata Collection.}\nFor each game, as shown in Fig.~\\ref{fig:pages}, Sina Sports Live also provides a \\emph{metadata page} that can link to related \\emph{player pages} and \\emph{team pages}. Note that each team (or player) has a unique corresponding team (or player) page.\nAfter crawling 8,640 metadata pages of all games in \\textsc{K-SportsSum}, we obtain 559 URLs of team pages and 15591 URLs of player pages.\n\n\\vspace{1ex}\n\\noindent\\textbf{Step 2: Player Knowledge Collection.}\nThe player pages provided by Sina Sports Live contain structured knowledge cards describing players in ten aspects (i.e., name, birthday, age, etc.).\nWe crawl these pages and obtain the 14,724 players' structured knowledge cards.\nThen we convert each knowledge card to a passage through several rule-based sentence templates (e.g., an item of knowledge card $\\langle$``Ronaldo'', ``birthday'', ``September 18, 1976''$\\rangle$ can be converted to a sentence ``Ronaldo's birthday is September 18, 1976''). In this way, we obtain 14,724 player passages.\n\n\\vspace{1ex}\n\\noindent\\textbf{Step 3: Team Knowledge Collection.}\nThough Sina Sports Live offers the team pages, we find that most of them are less informative than player pages. To construct an informative knowledge corpus, we decide to manually align these 559 team pages to Wikipedia articles\\footnote{https://zh.wikipedia.org/} in which we further extract plain text\\footnote{We crawl the Wikipedia articles and then use wikiextractor tool (\\url{https://github.com/attardi/wikiextractor}) to extract plain text.} to form our knowledge corpus.\nThree master students and two data experts are recruited to perform the alignment task. Each team page is assigned to three students, if their results are inconsistent, the final result is decided by a group meeting with these five persons. Eventually, we assign 523 out of 559 team pages to corresponding Wikipedia articles.\n\nFinally, our knowledge corpus contains 523 team articles and 14,724 player passages. The corpus also provides the link and alignment relations among metadata pages, player pages, team pages and Wikipedia articles. Thus, for a given game, we can accurately retrieve the related articles and passages.\n\n\\begin{figure}[t]\n\\setlength{\\belowcaptionskip}{-10pt}\n  \\centerline{\\includegraphics[width=0.45\\textwidth]{deep_statistics.png}}\n  \\caption{Statistics of tokens in news article and commentary document. Stop words were excluded from word clouds.}\n  \\label{fig:deep_statistics}\n\\end{figure} \n\n\\begin{table*}[t]\n  \\centering\n  \\setlength{\\belowcaptionskip}{-10pt}\n  \\resizebox{0.95\\textwidth}{!}\n  {\n    \\begin{tabular}{l|l|cccccc|cccccc}\n      \\hline\n      \\multicolumn{1}{c|}{\\multirow{3}{*}{Datasets}} & \\multicolumn{1}{c|}{\\multirow{3}{*}{\\# Examples}} & \\multicolumn{6}{c|}{News article}                                                             & \\multicolumn{6}{c}{Live commentary document}                                                \\\\\n      \\multicolumn{1}{c|}{}                         & \\multicolumn{1}{c|}{}                             & \\multicolumn{2}{c}{\\# Tokens} & \\multicolumn{2}{c}{\\# Words} & \\multicolumn{2}{c|}{\\# Sent.} & \\multicolumn{2}{c}{\\# Tokens} & \\multicolumn{2}{c}{\\# Words} & \\multicolumn{2}{c}{\\# Sent.} \\\\\n      \\multicolumn{1}{c|}{}                         & \\multicolumn{1}{c|}{}                             & Avg.         & 95th pctl.      & Avg.        & 95th pctl.     & Avg.        & 95th pctl.      & Avg.         & 95th pctl.      & Avg.        & 95th pctl.     & Avg.        & 95th pctl.     \\\\ \\hline\n      Zhang et al.~\\cite{zhang-etal-2016-towards}                                  & 150                                               & -            & -               & -           & -              & -           & -               & -            & -               & -           & -              & -           & -              \\\\ \\hline\n      NLPCC 2016 shared task~\\cite{Wan2016OverviewOT}                        & 900                                               & -            & -               & -           & -              & -           & -               & -            & -               & -           & -              & -           & -              \\\\ \\hline\n      \\textsc{SportsSum}\\cite{Huang2020GeneratingSN}                                     & 5428                                              & 801.11       & 1558               & 427.98      & 924              & 23.80        & 39               & 3459.97      & 5354               & 1825.63     & 3133              & 193.77      & 379              \\\\ \\hline\n      \\textsc{K-SportsSum}                        & \\textbf{7854}                                              & 606.80        & 1430            & 351.30       & 845            & 19.22       & 40              & 2251.62      & 3581            & 1200.31     & 1915           & 187.69      & 388            \\\\\n      \\ \\ \\ \\textsc{K-SportsSum} (train)             & 6854                                              & 604.41       & 1427             & 349.88       & 843            & 19.31       & 40              & 2250.04      & 3581            & 1199.69     & 1915           & 187.81      & 390            \\\\\n      \\ \\ \\ \\textsc{K-SportsSum} (dev.)                & 500                                               & 615.04      & 1426            & 356.51      & 824            & 19.64       & 42              & 2280.04      & 3608            & 1216.76      & 1920           & 187.08      & 370            \\\\\n      \\ \\ \\ \\textsc{K-SportsSum} (test)               & 500                                               & 631.28      & 1463            & 365.54      & 859            & 20.10       & 41              & 2244.78      & 3563            & 1192.32      & 1917           & 186.73      & 376            \\\\ \\hline\n      \\end{tabular}\n  }\n  \\setlength{\\belowcaptionskip}{3pt}\n  \\caption{Statistics of \\textsc{K-SportsSum} and previous datasets (Sent.: sentence, Avg.: average, 95th pctl.: 95th percentile).}\n  \\label{table:statistic}\n\\end{table*}\n\n\\begin{figure*}[t]\n\\centerline{\\includegraphics[width=0.85\\textwidth]{overview.png}}\n\\caption{An overview of knowledge-enhanced summarizer.}\n\\label{fig:overview}\n\\end{figure*}\n\n\\subsection{Benchmark Settings}\n\\label{sec:benchmark_settings}\n\\vspace{0.5ex}\nWe randomly select 500 samples and other 500 samples from \\textsc{K-SportsSum} to form development set and testing set.\nThe remaining 6,854 samples constitute training set.\n\n\\section{Data Analysis}\nIn this section, we analyze various aspects of \\textsc{K-SportsSum} to provide a deeper understanding of the dataset and task of sports game summarization.\n\n\\vspace{1ex}\n\\noindent\\textbf{Data Size.} Tab.~\\ref{table:statistic} shows the statistics of \\textsc{K-SportsSum} and previous datasets. The \\textsc{K-SportsSum} dataset is much larger than any other dataset.\n\n\\vspace{1ex}\n\\noindent\\textbf{News Article.} The average number of tokens in \\textsc{K-SportsSum} is 606.80 which is less than the counterpart in \\textsc{SportsSum} (801.11) due to the manual cleaning process.\nFig.~\\ref{fig:deep_statistics}(a) shows the distributions of token length for news articles.\nThis distribution obeys positive skewness distribution which indicates the length of most news articles is less than the average.\n\n\\vspace{1ex}\n\\noindent\\textbf{Live Commentary Document.} The length of commentary document usually reaches thousands of tokens, which makes the task more challenging.\nThe average number of tokens for commentary documents in \\textsc{SportsSum} is 3459.97, which is longer than the counterpart in \\textsc{K-SportsSum}, i.e., 2251.62.\nThis is because \\textsc{SportsSum} considers every commentary sentences in the document, but we only retain the commentary sentences which has timeline information, for the reason that most the commentary sentences without timeline information are irrelevant to the current game.\nThe distributions of token length for live commentary documents in \\textsc{K-SportsSum} are shown in Fig.~\\ref{fig:deep_statistics}(b). This distribution obeys the Gaussian mixture distribution because the live services in Sina Sports Live have been updated once. The length distribution of commentary documents is different before and after the update. Some commentary documents in \\textsc{K-SportsSum} are provided by the old live services, while others are offered by the new one.\n\n\\vspace{1ex}\n\\noindent\\textbf{Word Clouds.}\nFig.~\\ref{fig:deep_statistics}(c) and Fig.~\\ref{fig:deep_statistics}(d) show the word clouds for news article and live commentary document, where is easy to find the different text styles between these two types of text.\n\n\\vspace{1ex}\n\\noindent\\textbf{Knowledge Corpus.}\nThe knowledge corpus contains 523 team articles and 14,724 player passages.\nEach team article has 18.28 sentences or 1341.91 tokens on average. Each player passage contains 15.05 sentences or 283.49 tokens on average.\n\n\\section{Model}\nWe propose knowledge-enhanced summarizer which generates sports news based on both live commentary document and knowledge corpus. Formally, a model is given a live commentary document $C=\\{(t_{1},s_{1},c_{1}),...,(t_{m},s_{m},c_{m})\\}$ together with a knowledge corpus $K$ and outputs a sports news article $R=\\{r_{1},r_{2},...,r_{n}\\}$. $r_{i}$ represents $i$-th news sentence and $(t_{j},s_{j},c_{j})$ is $j$-th commentary, where $t_{j}$ is the timeline information, $s_{j}$ denotes the current scores and $c_{j}$ is the commentary sentence.\n\nThe overview of our knowledge-enhanced summarizer is illustrated in Fig.~\\ref{fig:overview}. Firstly, we utilize a selector to extract key commentary sentences from the original commentary document (Sec.~\\ref{sec:selector}). Secondly, a knowledge retriever is used to obtain related passages and articles from the knowledge corpus for each selected commentary sentence (Sec.~\\ref{sec:retriever}). Lastly, we make use of a seq2seq rewriter to generate news sentences based on the corresponding commentary sentences and retrieved passages/articles (Sec.~\\ref{sec:rewriter}). In order to train our selector and rewriter, we need labels to indicate the importance of commentary sentences and aligned $\\langle$commentary sentence, news sentence$\\rangle$ pairs.\nFollowing Huang et al.~\\cite{Huang2020GeneratingSN}, we obtain these importance labels and sentence pairs through a sentence mapping process (Sec.~\\ref{sec:sentence_pairs}).\n\n\\subsection{Key Commentary Sentence Selection}\n\\label{sec:selector}\nThe selector is used to extract key commentary sentences $\\{c_{i1},c_{i2},...\\\\,c_{ik}\\}$ from all commentary sentences $\\{c_{1},c_{2},...,c_{m}\\}$ of a given live commentary document, which can be modeled as a text classification task.\nDifferent from the previous state-of-the-art two-step framework~\\cite{Huang2020GeneratingSN} purely utilizing TextCNN~\\cite{Kim2014ConvolutionalNN} as the selector and ignores the contexts of a commentary sentence, we make use of RoBERTa~\\cite{Liu2019RoBERTaAR} to extract the contextual representation of a commentary sentence, and then predict its importance.\nSpecifically, we input the target commentary sentence with its context to RoBERTa~\\cite{Liu2019RoBERTaAR} in a sliding window way. The representation of target sentence is obtained by averaging the output embedding of each token belonging to the target sentence.\nFinally, a sigmoid classifier is employed to predict the importance of the target commentary sentence.\nThe cross-entropy loss is used as the training objective for the selector.\n\n\\begin{figure*}[t]\n\\centerline{\\includegraphics[width=0.80\\textwidth]{rewriter.png}}\n\\caption{Our encoder-decoder rewriter architecture.}\n\\label{fig:rewriter}\n\\end{figure*}\n\n\\subsection{Knowledge Retrieval}\n\\label{sec:retriever}\nGiven a selected commentary sentence $c_{ij}$ and the knowledge corpus $K$, the knowledge retriever first recognize the team and player entity mentions in $c_{ij}$, and then link the entity mentions to team articles or player passages from the corpus $K$.\n\n\\subsubsection{Named Entity Recognition Process}\nIn order to recognize the players and teams mentioned in commentary sentence $c_{ij}$, we train a FLAT model~\\cite{Li2020FLATCN} (a state-of-the-art Chinese NER model which could better leverage the lattice information of Chinese characters sequences) on MSRA~\\cite{Levow2006TheTI} (a general Chinese NER dataset), and then make use of the trained FLAT model to predict the entity mentions in $c_{ij}$. We only retain the \\texttt{PER} and \\texttt{ORG} entity mentions predicted by FLAT.\nBecause the \\texttt{PER} entity mentions indicate the players while the \\texttt{ORG} entity mentions hint the teams.\n\n\\subsubsection{Entity Linking Process}\nFor a given game, we can accurately retrieve dozens of candidate player passages and several (2 in most cases) candidate team articles through the link and alignment relations provided by knowledge corpus.\nThe entity linking process needs to link each \\texttt{PER} or \\texttt{ORG} entity mention to the candidate passages/articles. Here, we employ a simple yet effective linking method, we calculate the normalized Levenshtein distance between each entity mention with the title\\footnote{The standard name of each player is regarded as the title of player passages, while the team articles collected from Wikipedia already has the corresponding titles.} of passages/articles:\n\n\\begin{equation}\n\\label{levenshtein}\nNLev(entity,title)=\\frac{Lev(entity,title)}{max(len(entity),len(title))}\n\\end{equation}\nwhere $NLev(\\cdot,\\cdot)$ means the normalized Levenshtein distance, and $Lev(\\cdot,\\cdot)$ represents the standard Levenshtein distance. $len(\\cdot)$ indicates the number of characters within input sequences. \n\nEach \\texttt{PER} (or \\texttt{ORG}) entity mention is linked to the corresponding player passage (or team article) whose title has the nearest normalized Levenshtein distance with the entity mention if the nearest distance within a predefined threshold $\\lambda_{p}$ (or $\\lambda_{o}$). Otherwise, we do not link the entity mention.\nEventually, for a given commentary sentence $c_{ij}$, we obtain a number of $\\langle$linked \\texttt{PER}/\\texttt{ORG} entity mention, corresponding passage/article$\\rangle$ pairs.\n\n\\subsection{Sports News Generation}\n\\label{sec:rewriter}\nAs shown in Fig.~\\ref{fig:rewriter}, we make use of mT5~\\cite{Xue2021mT5AM}, a pre-trained multilingual language model\\footnote{Since there is no Chinese version of T5 for public use, we choose its multilingual version, i.e., mT5.}, as our rewriter to generate a news sentence $r_{ij}$ based on a given commentary sentence $c_{ij}$, its timeline information $t_{ij}$ and the $\\langle$linked \\texttt{PER}/\\texttt{ORG} entity mention, corresponding passage/article$\\rangle$ pairs.\nSpecifically, we tokenize the temporal phrase ``In the $t_{ij}$-minute'' and $c_{ij}$ using mT5's tokenizer, then form the input as \\texttt{<s> temporal phrase </s> commentary sentence </s>}. The input embeddings of each token consist of a segment embedding $z^{seg}$ and a knowledge embedding $z^{know}$ in addition to a token embedding $z^{token}$ and a position embedding $z^{pos}$ within the input sequence:\n\\begin{equation}\n\\label{equa:input_embedding}\nz_{k} = LN(z_{k}^{token}+z_{k}^{pos}+z_{k}^{seg}+z_{k}^{know})\n\\end{equation}\nwhere $z_{k}$ denotes the fused embedding of the $k$-th token in the input sequence. $LN(\\cdot)$ represents layer normalization.\n\nWe explain the two additional embeddings below.\n\n\\noindent\\textbf{Segment embeddings.} To convey the semantics of fine-grained types of tokens, we use four learnable segment embeddings (\\texttt{[Playe\\\\r], [Team], [Time] and [Other]}) to indicate a token belongs to: (1) linked \\texttt{PER} entity mentions; (2) linked \\texttt{ORG} entity mentions; (3) the temporal phrase; (4) none of above.\n\n\\noindent\\textbf{Knowledge embeddings.} For a token belonging to the linked \\texttt{PER}/\\texttt{ORG} entity mentions, we consider the representation of corresponding passage/article as its knowledge embedding. In detail, we first input each sentence of a given passage/article to a pre-trained RoBERTa~\\cite{Liu2019RoBERTaAR} one by one and use the output embedding of \\texttt{[CLS]} token as the sentence embedding. Then, the representation of the whole passage/article is the average of all sentences' embeddings.\n\nThe input embeddings are further passed to the mT5-encoder and generate the news sentence $r_{ij}$ in the sequence-to-sequence learning process with the negative log-likelihood loss.\nAll generated news sentences are concatenated to form final sports news.\n\n\\subsection{Training with Oracle}\n\\label{sec:sentence_pairs}\nTo train the selector and rewriter, we need (1) labels to indicate the importance of each commentary sentence and (2) a large number of $\\langle$commentary sentence, news sentence$\\rangle$ pairs, respectively.\nTo obtain the above labels and pairs, we \\textbf{map each news sentence to its commentary sentence} through the same sentence mapping process as Huang et al.~\\cite{Huang2020GeneratingSN}.\nSpecifically, although there is no explicit timeline information on news articles, we find about 40\\% news sentences in \\textsc{K-SportsSum} begin with ``in the n-th minutes''.\nFor each news sentence $r_{i}$ beginning with ``in the $h_{i}$-th minutes'', we first obtain its time information $h_{i}$. Then, we consider commentary sentences $c_{j}$ whose corresponding timeline $t_{j} \\in [h_{i},h_{i}+3]$ as candidate mapping set of $r_{i}$. Lastly, we calculate BERTScore~\\cite{Zhang2020BERTScoreET} (a metric to measure the sentence similarity) of $r_{i}$ and all the commentary sentences in candidate mapping set. The commentary sentence with the highest score is paired with $r_{i}$.\nWith the above process, we can obtain a large number of pairs of a mapped commentary sentence and a news sentence, which can be used to train our rewriter. Furthermore, the commentary sentence appearing in the pairs will be regarded as important while others are insignificant, which could be used as the training data for our selector.\n\n\\begin{table*}[t]\n  \\centering\n  \\resizebox{0.80\\textwidth}{!}\n  {\n    \\centering\n    \\begin{tabular}{clcccccc}\n      \\cline{1-8}\n      \\multirow{2}{*}{Method}                                                        & \\multirow{2}{*}{Model} & \\multicolumn{3}{c}{\\textsc{K-SportsSum}}                                   & \\multicolumn{3}{c}{\\textsc{SportsSum}}                                      \\\\\n                                                                                     &                        & ROUGE-1              & ROUGE-2              & ROUGE-L              & ROUGE-1              & ROUGE-2              & ROUGE-L              \\\\ \\cline{1-8}\n      \\multirow{3}{*}{Extractive Models}                                             & TextRank               & 21.04                & 6.17                 & 20.64                & 18.37                & 5.69                 & 17.23                \\\\\n      & PacSum                 & 24.33                & 7.28                 & 24.04                & 21.84                & 6.56                 & 20.19  \\\\\n                                                                                     & Roberta-Large                 & 27.79                & 8.14                 & 27.21                & 26.64                & 7.44                 & 25.59                \\\\ \\cline{1-8}\n      \\multirow{2}{*}{Abstractive Models}                                            & Abs-LSTM                   & 31.24                & 11.27                & 30.95                & 29.22                & 10.94                & 28.09                \\\\\n                                                                                     & Abs-PGNet                  & 36.49                & 13.34                & 36.11                & 33.21                & 11.76                & 32.37                \\\\ \\cline{1-8}\n      \\multirow{2}{*}{\\makecell[c]{Two Step Framework \\\\ (Selector + Rewriter)}}                      & SportsSUM~\\cite{Huang2020GeneratingSN}              & 44.89                & 19.04                & 44.16                & 43.17                & 18.66                & 42.27                \\\\\n                                                                                     & KES (Our)                  & \\textbf{48.79}                & \\textbf{21.04}                & \\textbf{47.17}                & \\textbf{47.43}                & \\textbf{20.54}                & \\textbf{47.79}                \\\\ \\cline{1-8}\n                                      \n      \\end{tabular}\n  }\n  \\caption{Experimental results on \\textsc{K-SportsSum} and \\textsc{SportsSum}.}\n  \\label{table:result}\n\\end{table*}\n\n\\begin{table*}[t]\n  \\centering\n    \\begin{tabular}{p{0.25\\textwidth}p{0.03\\textwidth}p{0.30\\textwidth}p{0.32\\textwidth}}\n    \\hline\n    \\multicolumn{1}{c}{Commentary Sentence} & \\multicolumn{1}{c}{T.} & \\multicolumn{1}{c}{KES (w/o know.)} & \\multicolumn{1}{c}{KES} \\\\ \\hline\n    Barcelona attacked on the left side, Semedo passed the ball to the front of the small restricted area, Suarez scored the ball!!! & \\multicolumn{1}{c}{27'}      & Barcelona broke the deadlock in the 27th minute, Semedo crossed from the left and Suarez scored in front of the small restricted area. & \\textbf{Defending champion} broke the deadlock in the 27th minute, Semedo crossed from the left and Suarez scored in front of the small restricted area. \\\\ \\hline\n    De Yang was replaced and Song Boxuan came on as a substitute                                                                     & \\multicolumn{1}{c}{45'}      & In the 45th minute, De Yang was replaced by Song Boxuan                                                                                & In the 45th minute, De Yang was replaced by Song Boxuan, \\textbf{who is 1.7 meters height.}                                                               \\\\ \\hline\n    \\end{tabular}\n  \\caption{Qualitative analysis on \\textsc{K-SportsSum} development set (T.:timeline information)}\n  \\label{table:case_study}\n\\end{table*}\n\n\\section{Experiments}\n\n\\subsection{Implementation Details}\nWe train all models on one 32GB Tesla V100 GPU. Our knowledge-enhanced summarizer is implemented based on RoBERTa~\\cite{Liu2019RoBERTaAR} and mT5~\\cite{Xue2021mT5AM} of huggingface Transformers library~\\cite{wolf-etal-2020-transformers} with default settings.\nIn detail, we utilize RoBERTa-Large (12 layers with 1024 hidden size) to initialize our selector, and mT5-Large (24 layers with 1024 hidden size) as our rewriter. Another fixed RoBERTa-Large in our rewriter is used to calculate the knowledge embedding for a number of input tokens.\nFor knowledge retriever, we train a FLAT~\\cite{Li2020FLATCN} NER model on a general Chinese NER dataset, i.e., MSRA~\\cite{Levow2006TheTI}. The trained FLAT model achieves 94.21 F1 score on the MSRA testing set, which is similar to the original paper. The predefined threshold $\\lambda_{p}$ and $\\lambda_{o}$ used in the entity linking process are 0.2 and 0.25, respectively.\nWe set the hyperparameters based on the preliminary experiments on the development set. We use minimal hyperparameter tuning using Learning Rates (LRs) in [1e-5, 2e-5, 3e-5] and epochs of 3 to 10. We find the selector with LR of 3e-5 and 5 epochs to work best. The best configuration for the mT5 rewriter is 2e-5 lr, 7epochs. For all models, we set the batch size to 32, use Adam optimizer with a default initial momentum and adopt linear warmup in the first 500 steps.\n\n\\subsection{Quantitative Results}\n\\subsubsection{Sports Game Summarization Task.}\nWe compare our knowledge enhanced summarizer (i.e., KES) with several general text summarization models and the current state-of-the-art two-step model proposed by Huang et al.~\\cite{Huang2020GeneratingSN} (i.e., SportsSUM). \nNote that the baseline models do not include the state-of-the-art pre-trained encoder-decoder language models (e.g., T5~\\cite{Raffel2020ExploringTL} and BART~\\cite{Lewis2020BARTDS}) due to their limitation with long text.\nTab.~\\ref{table:result} shows that our model outperforms the baselines on both \\textsc{K-SportsSum} and \\textsc{SportsSum} datasets in terms of ROUGE scores.\nSpecifically, TextRank~\\cite{Mihalcea2004TextRankBO} and PacSum~\\cite{zheng-lapata-2019-sentence} are two typical unsupervised extractive summarization models. Roberta-Large is used as a supervised  extractive summarization model in the same way as our selector. These three models achieve limited performances due to different text styles between commentaries and news. Abs-LSTM and Abs-PGNet~\\cite{See2017GetTT} are two abstractive summarization models which dispose of sports game summarization in an end2end sequence-to-sequence learning way. They outperform extractive models, because they take different text styles into account. Nevertheless, both LSTM and PGNet could not better model the long-distance dependency in the input sequence. Thanks to the appearance of transformer model~\\cite{Vaswani2017AttentionIA}, an encoder-decoder architecture which makes use of self-attention mechanism to model the long-distance dependency, many pre-trained encoder-decoder language models have been proposed one after another such as BART and T5. However, they cannot be direct used for sports game summarization because the input limitations of T5 and BART are 512 tokens and 1,024 tokens, respectively.\nThe state-of-the-art baseline SportsSUM~\\cite{Huang2020GeneratingSN} uses TextCNN selector and PGNet rewriter to achieve better results than the above models, where the selector could effectively handle the long commentaries text while the rewriter alleviates the different styles issue.\nDespite its better performance, SportsSUM neglects the knowledge gap between live commentaries and sports news.\nOur knowledge-enhanced summarizer uses the additional corpus to alleviate the knowledge gap, together with the advanced selector and rewriter to achieve a new state-of-the-art performance. Since \\textsc{K-SportsSum} and \\textsc{SportsSum} are both collected from Sina Sports Live, for each game in \\textsc{SportsSum}, we also can accurately retrieve related passages and articles from the corpus, and then train our knowledge-enhanced summarizer.\n\n\\subsubsection{Ablation Study.}\nWe run 5 ablations, modifying various settings of our knowledge-enhanced summarizer: \n(1) remove segment embeddings in knowledge retriever; (2) remove knowledge embeddings in knowledge retriever; (3) remove both segment embeddings and knowledge embeddings; (4) replace mT5 rewriter with PGN rewriter (it is worth noting that the PGN rewriter is based on LSTM, which cannot utilize segment embeddings and knowledge embeddings); (5) replace Roberta-Large selector with TextCNN selector.\n\nThe effect of these ablations on \\textsc{K-SportsSum} development set is shown in Tab.~\\ref{table:ablations}. In each case, the average ROUGE score is lower than our origin knowledge-enhanced summarizer, which justifies the rationality of our model.\n\n\\subsection{Qualitative Results}\nTab.~\\ref{table:case_study} shows the news sentences generated by (a) our original knowledge-enhanced summarizer, i.e., KES and (b) the variant model which removes knowledge embeddings in knowledge retriever, i.e., KES (w/o know.).\nAs shown, the news sentences generated by original KES are more informative than the counterpart by KES (w/o know.).\nOur knowledge-enhanced summarizer implicitly makes use of additional knowledge by fusing the knowledge embedding into the pre-trained language model (mT5 in our experiments).\nThough this implicit way could help the model to generate informative sports news, we also find that this way may lead to wrong facts. As the second example shown in Tab.~\\ref{table:case_study}, the generated news sentence describes the height of De Yang is 1.7 meters. However, the actual height of De Yang is 1.8 meters.\nThis finding implies that KES has learned the pattern of adding additional knowledge to news sentences but it is still challenging to generate correct descriptions. \n\n\\begin{table}[t]\n  \\centering\n  \\resizebox{0.30\\textwidth}{!}\n  {\n    \\centering\n    \\begin{tabular}{lr}\n        \\hline\n        Model                  & Avg. Rouge/$\\triangle$ \\\\ \\hline\n        KES                    & 39.94      \\\\ \\hline\n        KES (w/o seg.)         & 39.41/-0.53      \\\\\n        KES (w/o know.)        & 39.22/-0.72      \\\\\n        KES (w/o seg.\\&know.)  & 38.23/-1.71      \\\\\n        KES (PGN rewriter)     & 37.02/-2.92      \\\\\n        KES (TextCNN selector) & 37.72/-2.22      \\\\ \\hline\n        \\end{tabular}\n  }\n  \\caption{\\textsc{K-SportsSum} development set ablations (seg.: segment embedding, know.: knowledge embedding).}\n  \\label{table:ablations}\n\\end{table}\n\n\\subsection{Human Study}\nWe conduct human studies to further evaluate the sports news generated by different methods, i.e., KES, KES (w/o know.) and SportsSUM~\\cite{Huang2020GeneratingSN}. Five master students are recruited and each student evaluates 50 samples for each method.\nThe evaluator scores generated sports news in terms of informativeness, fluency and overall quality with a 3-point scale.\n\nFig.~\\ref{fig:huamn_evaluation} shows the human evaluation results. KES outperforms KES (w/o know.) and SportsSUM on all three aspects, which verifies that our original KES performances better on generating sports news. What is more, the fluency of sports news generated by KES is better than the counterpart by KES (w/o know.), which indicates taking the knowledge gap into account when generating news could also improve its fluency.\n\n\\subsection{Discussion}\nWe can conclude from the above experiments and analysis that sports game summarization is more challenging than traditional text summarization.\nWe believe the following research directions are worth following:\n(1) Exploring models explicitly utilizing knowledge;\n(2) Leveraging long text pre-trained model (e.g., Longformer~\\cite{Beltagy2020LongformerTL} and ETC~\\cite{Ainslie2020ETCEL}) to deal with sports game summarization task.\n\n\\begin{figure}[t]\n\\centering\n\\subfigure[Informativeness]{\n  \\includegraphics[width=0.30\\linewidth]{informativity.png}\n}\n\\subfigure[Fluency]{\n  \\includegraphics[width=0.30\\linewidth]{fluency.png}\n}\n\\subfigure[Overall]{\n  \\includegraphics[width=0.30\\linewidth]{overall.png}\n}\n\\caption{Results on human study.}\n\\label{fig:huamn_evaluation}\n\\end{figure}\n\n\\section{Related Work}\nText Summarization aims at preserving the main information of one or multiple documents with a relatively short text~\\cite{rush-etal-2015-neural,chopra-etal-2016-abstractive,Nallapati2016AbstractiveTS}.\nOur paper focuses on sports game summarization, a challenging branch of text summarization.\nEarly literature mainly explores different strategies on limited-scale datasets~\\cite{zhang-etal-2016-towards,Wan2016OverviewOT} to first select key commentary sentences, and then either form the sports news directly~\\cite{zhang-etal-2016-towards,Zhu2016ResearchOS,Yao2017ContentSF} or relies on human-constructed templates to generate final news~\\cite{Liu2016SportsNG,lv2020generate}.\nSpecifically, Zhang et al.~\\cite{zhang-etal-2016-towards} extract different features (e.g., the number of words, keywords and stop words) of commentary sentences, and then utilize a learning to rank (LTR) model to select key commentary sentences so as to form news. Yao et al.~\\cite{Yao2017ContentSF} take the description style and the importance of the described behavior into account during key commentary sentences selection. Zhu et al.~\\cite{Zhu2016ResearchOS} model the sentences selection process as a sequence tagging task and deal with it using Conditional Random Field (CRF).\nLv et al.~\\cite{lv2020generate} make use of Convolutional Neural Network (CNN) to select key commentary sentences and further adopt pre-defined sentence templates to generate final news.\nRecently, Huang et al.~\\cite{Huang2020GeneratingSN} present \\textsc{SportsSum}, the first large-scale sports game summarization dataset. They also discuss a state-of-the-art two-step framework which first selects key commentary sentences, and then rewrites each selected sentence to a news sentence through seq2seq models.\nDespite its great contributions, there are many noises in \\textsc{SportsSum} due to its simple rule-based data cleaning process. %In contrast, \\textsc{K-SportsSum} is constructed through a manual cleaning process. It's currently the highest quality and largest sports game summarization dataset.\n\n  \n  \n\\section{Conclusion}\nIn conclusion, we propose \\textsc{K-SportsSum}, a large-scale human-cleaned sports game summarization benchmark. In order to narrow the knowledge gap between live commentaries and sports news, \\textsc{K-SportsSum} also provides a knowledge corpus containing the information of sports teams and players. Additionally, a  knowledge-enhanced summarizer is presented to harness external knowledge for generating more informative sports news. We have conducted extensive experiments to verify the effectiveness of the proposed method on two datasets compared with current state-of-the-art baselines via quantitative analysis, qualitative analysis and human study.\n\n\\begin{acks}\nZhixu Li and Jianfeng Qu are the corresponding authors.\nWe would like to thank anonymous reviewers for their suggestions and comments. \nThis research is supported by the National Natural Science Foundation of China (Grant No. 62072323, 61632016, 62102276), the Natural Science Foundation of Jiangsu Province (No. BK20191420, BK20210705), the Priority Academic Program Development of Jiangsu Higher Education Institutions, Suda-Toycloud Data Intelligence Joint Laboratory, and the Collaborative Innovation Center of Novel Software Technology and Industrialization.\n\\end{acks}\n\n\\clearpage\n\\balance\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2010.00526v1.tex",
        "arXiv-2110.05750v1.tex",
        "arXiv-2111.12535v1.tex"
    ],
    "group_id": "group_66",
    "response": "### Title: Advances in Sports Game Summarization and Question Answering: A Comparative Analysis of LiveQA, SportsSum2.0, and K-SportsSum\n\n### Introduction\n\nSports game summarization and question answering (QA) have emerged as significant areas of research within the broader field of natural language processing (NLP). These tasks involve generating concise and informative summaries from live commentary texts and answering questions based on the same texts, respectively. The primary objective of sports game summarization is to create high-quality sports news articles from live text commentary scripts, which are often lengthy and colloquial. Conversely, QA tasks aim to test the ability of machine reading comprehension models to understand and reason about the temporal and numerical information embedded within these live commentaries. Both tasks are critical for enhancing the accessibility and comprehension of sports events for both automated and human audiences.\n\nHistorically, research in sports game summarization has focused on developing methods to select and rewrite key commentary sentences to form coherent and informative news articles. Early works, such as Zhang et al. (2016) and Wan et al. (2016), introduced datasets and models that aimed to address the summarization task. However, these early datasets were limited in scale and quality, and the models often failed to account for the different text styles between live commentaries and news articles. More recent datasets, such as SportsSum (Huang et al., 2020), have significantly increased the scale of available data, but they still suffer from noise and a lack of consideration for the knowledge gap between commentaries and news.\n\nSimilarly, QA research has traditionally focused on datasets that test the ability of models to extract answers from specific passages. However, these datasets often neglect the temporal and mathematical reasoning required to answer questions that span across multiple events or require calculations based on the game's numerical data. This limitation has led to the development of new datasets like LiveQA, which specifically target the ability of models to understand and reason about temporal and numerical information.\n\nThe field has seen considerable progress in recent years, with the advent of large-scale datasets and advanced models that incorporate attention mechanisms and multi-hop inference. However, challenges remain, particularly in handling the unique characteristics of live commentary texts, such as their length, colloquial style, and the need for external knowledge to generate accurate and informative summaries. Additionally, QA models struggle with temporal and mathematical reasoning, which are crucial for answering questions that require understanding the game's timeline and numerical data.\n\nThis summary will explore three recent papers that address these challenges: LiveQA, SportsSum2.0, and K-SportsSum. Each paper introduces a new dataset or model that aims to improve the state-of-the-art in sports game summarization and QA. We will discuss the main contributions of each paper, their commonalities, and innovations, as well as the results and implications of their findings.\n\n### LiveQA: A Question Answering Dataset over Sports Live\n\nLiveQA, introduced by Sujian Li and colleagues, is a new QA dataset constructed from play-by-play live broadcasts of NBA games. The dataset contains 117,000 multiple-choice questions written by human commentators for over 1,670 NBA games. These questions are derived from the Chinese Hupu website and are designed to test the ability of QA models to perform reasoning across timeline-based live broadcasts. The dataset is unique in that it requires models to understand series of timeline information and perform mathematical computations to infer answers.\n\nThe LiveQA dataset is structured such that each live-stream of a match includes a timeline of events and questions inserted into corresponding positions. This structure ensures that the temporal context of each question is preserved. Each question has two answer choices and is associated with a specific game event or timeline. The dataset includes a diverse range of questions, such as those requiring addition, subtraction, or comparison operations, as well as questions that necessitate tracking events over time.\n\nTable 1 provides statistics on the dataset, showing that it contains 1,670 documents, with an average of 70.09 quizzes and 1,069.83 sentences per document. Questions are classified into three categories based on their answer locations: those that can be answered by extracting information from neighboring sentences, those that require summarizing all information after the game ends, and those that cannot be answered from the passage.\n\n\\begin{table}[h]\n\\begin{center}\n\\begin{tabular}{|l|l|}\n\\hline\n\\textbf{Element} & \\textbf{Count} \\\\\n\\hline\nDocument & 1,670 \\\\\nSentences in Total & 1,786,616 \\\\\nSentences in Average & 1,069.83 \\\\\nQuizzes in Total & 117,050 \\\\\nQuizzes in Average & 70.09 \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\caption{Statistics of LiveQA dataset.}\n\\label{stat}\n\\end{table}\n\nLiveQA's main characteristics include the requirement for temporal inference and the involvement of mathematical reasoning. These characteristics make the dataset challenging for existing QA models, as they often rely on extractive methods that do not account for temporal context or numerical operations.\n\nTo evaluate the performance of QA models on LiveQA, the authors implemented three baseline models: random selection, dominant option selection, and the Gated-Attention Reader (GA) model. The GA model, which is designed for multi-hop inference, failed to outperform the dominant option method, achieving only 53.1\\% accuracy. This result highlights the limitations of existing models in handling temporal and mathematical reasoning tasks.\n\n### SportsSum2.0: Generating High-Quality Sports News from Live Text Commentary\n\nSportsSum2.0, introduced by Jiaan Wang and colleagues, is an updated version of the SportsSum dataset, which aims to generate sports news articles from live commentary documents. The original SportsSum dataset, while large-scale, suffers from noise and a lack of consideration for lexical overlap between commentaries and news articles. SportsSum2.0 addresses these issues through a manual cleaning process and an advanced pseudo-labeling algorithm that incorporates both semantic and lexical overlap.\n\nThe dataset consists of 5,402 human-cleaned samples, each of which includes a live commentary document and a corresponding news article. The authors collected live commentary documents and news articles from Sina Sports Live and manually cleaned the news articles to remove irrelevant descriptions, advertisements, and history-related information. This cleaning process ensures that the dataset is of high quality and suitable for training sophisticated summarization models.\n\n\\begin{table*}[t]\n\\centering\n\\resizebox{0.80\\textwidth}{!}{\n  \\begin{tabular}{|c|c|cccccc|cccccc|}\n  \\hline\n  \\multicolumn{2}{|c|}{\\multirow{3}{*}{Datasets}} & \\multicolumn{6}{c|}{News article} & \\multicolumn{6}{c|}{Live commentary document} \\\\\n  \\multicolumn{2}{|c|}{} & \\multicolumn{2}{c|}{\\# Tokens} & \\multicolumn{2}{c|}{\\# Words} & \\multicolumn{2}{c|}{\\# Sent.} & \\multicolumn{2}{c|}{\\# Tokens} & \\multicolumn{2}{c|}{\\# Words} & \\multicolumn{2}{c|}{\\# Sent.} \\\\\n  \\multicolumn{2}{|c|}{} & Avg. & 95th pctl. & Avg. & 95th pctl. & Avg. & 95th pctl. & Avg. & 95th pctl. & Avg. & 95th pctl. & Avg. & 95th pctl. \\\\\n  \\hline\n  SportsSum & 5,428 & 801.11 & 1,558 & 427.98 & 924 & 23.80 & 39 & 3,459.97 & 5,354 & 1,825.63 & 3,133 & 193.77 & 379 \\\\\n  \\hline\n  SportsSum2.0 & 5,402 & 606.80 & 1,430 & 351.30 & 845 & 19.22 & 40 & 2,251.62 & 3,581 & 1,200.31 & 1,915 & 187.69 & 388 \\\\\n  \\hline\n  \\end{tabular}\n}\n\\caption{Statistics of SportsSum and SportsSum2.0 datasets.}\n\\label{table:statistic}\n\\end{table*}\n\nThe authors also introduce a reranker-enhanced summarizer framework that extends the two-step model (selector + rewriter) to include a reranker that considers the importance, fluency, and redundancy of rewritten news sentences. This framework utilizes RoBERTa for the selector and mT5 for the rewriter, and achieves state-of-the-art performance on both SportsSum and SportsSum2.0 datasets in terms of ROUGE scores.\n\n### Knowledge Enhanced Sports Game Summarization (K-SportsSum)\n\nK-SportsSum, introduced by Jiaan Wang and colleagues, is a large-scale human-cleaned sports game summarization dataset that includes a knowledge corpus containing information about 523 sports teams and 14,724 sports players. The dataset consists of 7,854 commentary-news pairs, each of which includes a live commentary document and a corresponding news article. The authors collected live commentary documents and news articles from Sina Sports Live and manually cleaned the news articles to remove irrelevant descriptions, advertisements, and history-related information.\n\n\\begin{table*}[t]\n\\centering\n\\resizebox{0.80\\textwidth}{!}{\n  \\begin{tabular}{|c|c|cccccc|cccccc|}\n  \\hline\n  \\multicolumn{2}{|c|}{\\multirow{3}{*}{Datasets}} & \\multicolumn{6}{c|}{News article} & \\multicolumn{6}{c|}{Live commentary document} \\\\\n  \\multicolumn{2}{|c|}{} & \\multicolumn{2}{c|}{\\# Tokens} & \\multicolumn{2}{c|}{\\# Words} & \\multicolumn{2}{c|}{\\# Sent.} & \\multicolumn{2}{c|}{\\# Tokens} & \\multicolumn{2}{c|}{\\# Words} & \\multicolumn{2}{c|}{\\# Sent.} \\\\\n  \\multicolumn{2}{|c|}{} & Avg. & 95th pctl. & Avg. & 95th pctl. & Avg. & 95th pctl. & Avg. & 95th pctl. & Avg. & 95th pctl. & Avg. & 95th pctl. \\\\\n  \\hline\n  SportsSum & 5,428 & 801.11 & 1,558 & 427.98 & 924 & 23.80 & 39 & 3,459.97 & 5,354 & 1,825.63 & 3,133 & 193.77 & 379 \\\\\n  \\hline\n  K-SportsSum & 7,854 & 606.80 & 1,430 & 351.30 & 845 & 19.22 & 40 & 2,251.62 & 3,581 & 1,200.31 & 1,915 & 187.69 & 388 \\\\\n  \\hline\n  \\end{tabular}\n}\n\\caption{Statistics of SportsSum and K-SportsSum datasets.}\n\\label{table:statistic}\n\\end{table*}\n\nThe authors propose a knowledge-enhanced summarizer that utilizes both live commentary documents and the knowledge corpus to generate sports news articles. This model extends the two-step framework (selector + rewriter) by incorporating a knowledge retriever that links entity mentions in commentary sentences to related passages and articles in the knowledge corpus. The knowledge retriever uses a FLAT model for named entity recognition (NER) and a simple yet effective linking method based on normalized Levenshtein distance to retrieve relevant passages and articles.\n\nThe knowledge-enhanced summarizer (KES) is implemented using RoBERTa for the selector and mT5 for the rewriter. The rewriter incorporates segment embeddings and knowledge embeddings into the input sequence to generate more informative and fluent news sentences. The authors conducted extensive experiments on the K-SportsSum and SportsSum datasets, demonstrating that KES achieves state-of-the-art performance in terms of ROUGE scores and human evaluation metrics.\n\n### Commonalities and Innovations\n\nAll three papers address the challenges of sports game summarization and QA by introducing new datasets and models that incorporate unique characteristics of live commentary texts. LiveQA focuses on QA tasks that require temporal inference and mathematical reasoning, while SportsSum2.0 and K-SportsSum address summarization tasks that need to generate high-quality sports news articles from live commentaries.\n\nThe commonalities among these papers include the use of manual cleaning processes to improve the quality of datasets and the adoption of advanced models and techniques to handle the unique characteristics of live commentary texts. Specifically, SportsSum2.0 and K-SportsSum both employ RoBERTa and mT5 models for their summarization tasks, while LiveQA uses a Gated-Attention Reader (GA) model for QA tasks.\n\nThe innovations of these papers lie in their unique approaches to addressing the challenges of sports game summarization and QA. LiveQA introduces a new QA dataset that tests the ability of models to understand temporal and numerical information. SportsSum2.0 improves the quality of the SportsSum dataset through manual cleaning and an advanced pseudo-labeling algorithm that considers both semantic and lexical overlap. K-SportsSum further narrows the knowledge gap between live commentaries and sports news by providing a large-scale knowledge corpus and a knowledge-enhanced summarizer that incorporates external knowledge during the summarization process.\n\n### Comparison of Results\n\nThe results of the three papers demonstrate the effectiveness of their proposed approaches in addressing the challenges of sports game summarization and QA. LiveQA's baseline models, including random selection, dominant option selection, and GA, achieve accuracies of 50.0\\%, 56.4\\%, and 53.1\\%, respectively. These results indicate that existing QA models struggle with temporal and mathematical reasoning tasks, as they cannot outperform the dominant option method.\n\nSportsSum2.0's reranker-enhanced summarizer achieves state-of-the-art performance on both SportsSum and SportsSum2.0 datasets, outperforming previous models in terms of ROUGE scores. The model's performance on SportsSum2.0 is 46.23 ROUGE-1, 19.41 ROUGE-2, and 45.37 ROUGE-L, while its performance on SportsSum is 47.62 ROUGE-1, 19.73 ROUGE-2, and 47.19 ROUGE-L. These results highlight the importance of manual cleaning and advanced techniques in improving the quality of sports game summarization models.\n\nK-SportsSum's knowledge-enhanced summarizer (KES) achieves state-of-the-art performance on both K-SportsSum and SportsSum datasets, outperforming previous models in terms of ROUGE scores. The model's performance on K-SportsSum is 48.79 ROUGE-1, 21.04 ROUGE-2, and 47.17 ROUGE-L, while its performance on SportsSum is 47.43 ROUGE-1, 20.54 ROUGE-2, and 47.79 ROUGE-L. These results demonstrate the effectiveness of incorporating external knowledge in the summarization process.\n\n### Discussion of Differences\n\nThe differences in the results of these papers can be attributed to the unique characteristics and challenges of each task. LiveQA's dataset is specifically designed to test the ability of QA models to understand temporal and numerical information, which is a novel challenge for existing models. The baseline models' poor performance on LiveQA highlights the need for further research on QA models that can handle these challenges.\n\nSportsSum2.0 and K-SportsSum both address the summarization task, but they differ in their approaches to improving the quality of datasets and models. SportsSum2.0 improves the quality of the SportsSum dataset through manual cleaning and an advanced pseudo-labeling algorithm, while K-SportsSum further narrows the knowledge gap between live commentaries and sports news by providing a large-scale knowledge corpus and a knowledge-enhanced summarizer. The superior performance of K-SportsSum's KES model on both datasets suggests that incorporating external knowledge is a promising direction for future research.\n\n### Conclusion\n\nIn conclusion, the three papers introduce new datasets and models that significantly advance the state-of-the-art in sports game summarization and QA. LiveQA's dataset tests the ability of QA models to understand temporal and numerical information, while SportsSum2.0 and K-SportsSum improve the quality of summarization datasets and models through manual cleaning and advanced techniques. The results of these papers demonstrate the effectiveness of their proposed approaches and highlight the challenges and opportunities in the field of sports game summarization and QA.\n\nFuture research directions include exploring models that explicitly utilize external knowledge for summarization tasks and leveraging long text pre-trained models to handle the unique characteristics of live commentary texts. Additionally, further research is needed to develop QA models that can effectively handle temporal and mathematical reasoning tasks, as demonstrated by LiveQA's dataset. These advancements will continue to push the boundaries of NLP and enhance the accessibility and comprehension of sports events for both automated and human audiences."
}