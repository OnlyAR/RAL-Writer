{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Reformer: The Efficient Transformer}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nLarge Transformer models routinely achieve state-of-the-art results on\na number of tasks but training these models can be prohibitively costly,\nespecially on long sequences. We introduce two techniques to improve\nthe efficiency of Transformers. For one, we replace dot-product attention\nby one that uses locality-sensitive hashing, changing its complexity\nfrom O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence.\nFurthermore, we use reversible residual layers instead of the standard\nresiduals, which allows storing activations only once in the training\nprocess instead of $N$ times, where $N$ is the number of layers.\nThe resulting model, the Reformer, performs on par with Transformer models\nwhile being much more memory-efficient and much faster on long sequences.\n\\end{abstract}\n\n\\section{Introduction}\n\nThe Transformer architecture \\citep{transformer} is widely used in natural language processing\nand yields state-of-the-art results on a number of tasks. To obtain these results,\nresearchers have resorted to training ever larger Transformer models. The number of parameters exceeds\n0.5B per layer in the largest configuration reported in \\citep{meshtf} while the number\nof layers goes up to 64 in \\citep{chartransformer}. Transformer models are also used on\nincreasingly long sequences. Up to 11 thousand tokens of text in a single example were\nprocessed in \\citep{wikipedia} and when processing other modalities, like music \\citep{huang2018music}\nand images \\citep{parmar2018imagetransformer}, even longer sequences are commonplace.\nThese large-scale long-sequence models yield great results but strain resources to\nthe point where some argue that this trend is breaking NLP\nresearch\\footnote{\\url{https://hackingsemantics.xyz/2019/leaderboards/}}.\nMany large Transformer models can only realistically be trained in large\nindustrial research laboratories and such models trained with model parallelism\ncannot even be fine-tuned on a single GPU as their memory requirements demand a \nmulti-accelerator hardware setup even for a single training step.\n\nDo large Transformer models fundamentally require such huge resources or are\nthey simply inefficient? Consider the following calculation: the 0.5B parameters used in the largest reported Transformer layer account for 2GB of memory. Activations for 64K tokens with embedding size 1024 and batch size 8 account for $64\\text{K} \\times 1\\text{K} \\times 8 = 0.5$B floats, requiring another 2GB of memory. If our memory use was only per-layer, then we should fairly easily fit a large Transformer even on sequences of length 64K on a single accelerator. Further, the whole corpus used to train BERT only requires 17GB to store. Why is it then that we cannot even fine-tune these models on single machines?\n\nThe above estimate includes only per-layer memory and input activations cost and does \nnot take into account the following major sources of memory use in the Transformer.\n\\begin{itemize}\n\\item Memory in a model with $N$ layers is $N$-times larger than in a single-layer model due to\n  the fact that activations need to be stored for back-propagation.\n\\item Since the depth $d_{ff}$ of intermediate feed-forward layers is often much larger than\n  the depth $d_{model}$ of attention activations, it accounts for a large fraction of memory use.\n\\item Attention on sequences of length $L$ is O($L^2$) in both computational and memory complexity,\n  so even for a single sequence of $64$K tokens can exhaust accelerator memory.\n\\end{itemize}\n\nWe introduce the Reformer model which solves these problems using the following techniques:\n\\begin{itemize}\n\\item Reversible layers, first introduced in \\cite{gomez2017reversible}, enable storing only a single copy of activations in the whole model, so the $N$ factor disappears.\n\\item Splitting activations inside feed-forward layers and processing them in chunks removes the $d_{ff}$ factor and saves memory inside feed-forward layers.\n\\item Approximate attention computation based on locality-sensitive hashing replaces the O($L^2$) factor in attention layers with O($L\\log L$) and so allows operating on long sequences.\n\\end{itemize}\n\nWe study these techniques and show that they have negligible impact on the training process compared to the standard Transformer. \nSplitting activations in fact only affects the implementation; it is numerically identical to the layers used in the Transformer. \nApplying reversible residuals instead of the standard ones does change the model but has a negligible effect on training in all \nconfigurations we experimented with.  Finally, locality-sensitive hashing in attention is a more major change that can influence\nthe training dynamics, depending on the number of concurrent hashes used.  We study this\nparameter and find a value which is both efficient to use and yields results very close to full attention.\n\nWe experiment on a synthetic task, a text task (enwik8) with sequences of length 64K and an image generation task (imagenet-64 generation)\nwith sequences of length 12K. In both cases we show that Reformer matches the results obtained with full Transformer\nbut runs much faster, especially on the text task, and with orders of magnitude better memory efficiency.\n\n\\section{Locality-sensitive Hashing Attention}\n\n\\paragraph{Dot-product attention.}\nThe standard attention used in the Transformer is the scaled dot-product attention \\citep{transformer}. \nThe input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$.  \nThe dot products of the query with all keys are computed, scaled by $\\sqrt{d_k}$, and a softmax function is \napplied to obtain the weights on the values. In practice, the attention function on a set of queries is computed \nsimultaneously, packed together into a matrix $Q$.  Assuming the keys and values are also packed together into \nmatrices $K$ and $V$, the matrix of outputs is defined as:\n\n\\begin{equation}\\label{eq:attn}\n   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n\\end{equation}\n\n\\paragraph{Multi-head attention.}\nIn the Transformer, instead of performing a single attention function with $d_{model}$-dimensional keys, \nvalues and queries, one linearly projects the queries, keys and values $h$ times with different, \nlearned linear projections to $d_k$, $d_k$ and $d_v$ dimensions, respectively.  Attention is applied to \neach of these projected versions of queries, keys and values in parallel, yielding $d_v$-dimensional \noutput values. These are concatenated and once again projected, resulting in the final values. This \nmechanism is known as multi-head attention.\n\n\\paragraph{Memory-efficient attention.}\nTo calculate the memory use of the attention mechanism, let us focus on\nthe attention computation from Equation~\\ref{eq:attn}.\nLet us assume that Q, K and V all have the shape\n$[batch\\_size, length, d_{model}]$. The main issue is\nthe term $QK^T$, which has the shape $[batch\\_size, length, length]$.\nIn the experimental section we train a model on sequences of length\n$64K$ -- in this case, even at batch-size of 1, this is a $64K \\times 64K$\nmatrix, which in 32-bit floats would take 16GB of memory.\nThis is impractical and has hindered the use of the Transformer for long\nsequences. But it is important to note that the $QK^T$ matrix does not\nneed to be fully materialized in memory. The attention can indeed be computed\nfor each query $q_i$ separately, only calculating $\\mathrm{softmax}(\\frac{q_iK^T}{\\sqrt{d_k}})V$ \nonce in memory, and then re-computing it on the backward pass when needed for gradients.\nThis way of computing attention may be less efficient but it only uses\nmemory proportional to $length$. We use this memory-efficient implementation\nof attention to run the full-attention baselines presented in the experimental\nsection.\n\n\\paragraph{Where do Q, K, V come from?}\nThe multi-head attention described above operates on keys, queries and\nvalues, but usually we are only given a single tensor of activations A\nof the shape $[batch\\_size, length, d_{model}]$ -- e.g., coming from\nembedding the tokens in a sentence into vectors. To build Q, K and V\nfrom A, the Transformer uses 3 different linear layers projecting A\ninto Q, K and V with different parameters. For models with LSH attention,\nwe want queries and keys (Q and K) to be identical. This is easily achieved\nby using the same linear layer to go from A to Q and K, and a separate one\nfor V. We call a model that behaves like this a shared-QK Transformer.\nIt turns out that sharing QK does not affect the performance of Transformer,\neven if we additionally normalize the length of the keys K,\nas we show in the experimental Section~\\ref{sec:exp_qk}.\n\n\\paragraph{Hashing attention.}\nFor the LSH attention, we start with two tensors, Q=K and V\nof the shape $[batch\\_size, length, d_{model}]$.\nWe keep the multi-head mechanism intact and focus on the attention\ncomputation from Equation~\\ref{eq:attn}. As already mentioned,\nthe main issue is the term $QK^T$, which has the shape\n$[batch\\_size, length, length]$. But note that we are actually only\ninterested in $\\mathrm{softmax}(QK^T)$. Since softmax is dominated\nby the largest elements, for each query $q_i$ we only need to focus\non the keys in K that are closest to $q_i$. For example, if K is of\nlength 64K, for each $q_i$ we could only consider a small subset of,\nsay, the $32$ or $64$ closest keys. That is much more efficient, but how\ncan we find the nearest neighbors among the keys?\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{figures/angular-LSH.png}\n    \\caption{An angular locality sensitive hash uses random rotations of spherically projected \n    points to establish buckets by an argmax over signed axes projections.  In this highly \n    simplified 2D depiction, two points $x$ and $y$ are unlikely to share the same hash buckets \n    (above) for the three different angular hashes unless their spherical projections are close \n    to one another (below).}\n    \\label{fig:lsh}\n\\end{figure}\n\n\\paragraph{Locality sensitive hashing.}\nThe problem of finding nearest neighbors quickly in high-dimensional spaces\ncan be solved by locality-sensitive hashing (LSH). A hashing scheme that assigns each\nvector $x$ to a hash $h(x)$ is called locality-sensitive if nearby\nvectors get the same hash with high probability and distant ones do not.\nIn our case, we actually only require that nearby vectors get the same\nhash with high probability and that hash-buckets are of similar size with\nhigh probability.\n\nWe achieve this by employing random projections as follows (see Figure \\ref{fig:lsh}).\nTo get $b$ hashes, we first fix a random matrix $R$ of size $[d_k, b/2]$.\nWe then define $h(x) = \\argmax([xR; -xR])$ where $[u; v]$ denotes the concatenation of two vectors.\nThis method is a known LSH scheme \\citep{andoni2015angularLSH} and is easy to implement\nand apply to batches of vectors.\n\n\\begin{figure}\n        \\centering\n        \\includegraphics[width=0.9\\textwidth]{figures/combined-attention-figs.png}\n        \\caption{Simplified depiction of LSH Attention showing the hash-bucketing, sorting, \n                  and chunking steps and the resulting causal attentions. (a-d) Attention matrices for these varieties of attention.}\n        \\label{fig:attnpattern}\n\\end{figure}\n\n\\paragraph{LSH attention.}\nKnowing our LSH scheme and the general idea of hashing attention, we will now formalize the LSH attention we use in this paper. We first rewrite the equation for normal attention, (\\ref{eq:attn}), for a single query position $i$ at a time:\n\\begin{align}\n    &o_i = \\sum_{j \\in \\pair_i} \\exp \\left(q_i \\cdot k_j - z(i, \\pair_i) \\right) v_j\n    &\\text{ where } \\pair_i = \\left\\{j : i \\geq j \\right\\}\n\\end{align}\n\nWe introduce the notation $\\pair_i$ to represent the set that the query at position $i$ attends to, and $z$ to denote the partition function (i.e.\\ the normalizing term in the softmax). For clarity, we also omit scaling by $\\sqrt{d_k}$.\n\nFor batching purposes we typically perform attention over a larger set $\\extpair_i = \\{0, 1, \\ldots, l\\} \\supseteq \\pair_i$ while masking out elements not in $\\pair_i$:\n\\begin{equation}\\label{eq:attn-masked}\n    o_i = \\sum_{j \\in \\extpair_i} \\exp \\left(q_i \\cdot k_j - m(j, \\pair_i) - z(i, \\pair_i) \\right) v_j \\quad\n    \\text{ where } \\ m(j, \\pair_i) = \\begin{cases}\n    \\infty & \\text{if } j \\notin \\pair_i\\\\\n    0 & \\text{otherwise}\n    \\end{cases}\n\\end{equation}\n\nNow we turn to LSH attention, which we can think of in terms of restricting the set $\\pair_i$ of target items a query position $i$ can attend to, by only allowing attention within a single hash bucket.\n\\begin{equation}\n    \\pair_i = \\left\\{ j : h(q_i) = h(k_j) \\right\\}\n\\end{equation}\n\nFigure~\\ref{fig:attnpattern}(a-b) shows a schematic comparison of full-attention with a hashed variant. Part (a) depicts that the attention matrix for full attention is typically sparse, but the computation does not take advantage of this sparsity. In (b), the queries and keys have been sorted according to their hash bucket. Since similar items fall in the same bucket with high probability, the full attention pattern can be approximated by only allowing attention within each bucket.\n\nHash buckets in this formulation tend to be uneven in size, which makes it difficult to batch across buckets. Moreover, the number of queries and the number of keys within a bucket may be unequal -- in fact, it is possible for a bucket to contain many queries but no keys. To alleviate these issues, we first ensure that $h(k_j) = h(q_j)$ by setting $k_j = \\frac{q_j}{\\|q_j\\|}$. Next, we sort the queries by bucket number and, within each bucket, by sequence position; this defines a permutation where $i \\mapsto s_i$ after sorting. In the sorted attention matrix, pairs from the same bucket will cluster near the diagonal (as depicted in Figure~\\ref{fig:attnpattern}c). We can follow a batching approach where chunks of $m$ consecutive queries (after sorting) attend to each other, and one chunk back (Figure~\\ref{fig:attnpattern}d). Following our earlier notation, this corresponds to setting:\n\\begin{equation}\n    \\extpair_i = \\left\\{ j : \\left\\lfloor \\frac{s_i}{m}\\right\\rfloor - 1 \\leq \\left\\lfloor \\frac{s_j}{m}\\right\\rfloor \\leq  \\left\\lfloor \\frac{s_i}{m}\\right\\rfloor \\right\\}\n\\end{equation}\nIf $\\max_i \\left|\\pair_i\\right| < m$, then $\\pair_i \\subseteq \\extpair_i$. In practice we set $m = \\frac{2l}{n_{buckets}}$ (where $l$ is the sequence length). The average bucket size is $\\frac{l}{n_{buckets}}$, and we assume that the probability of a bucket growing to twice that size is sufficiently low.\nThe overall process of LSH attention is summarized in Figure~\\ref{fig:attnpattern}.\n\n\\paragraph{Multi-round LSH attention.}\nWith hashing, there is always a small probability that similar items nevertheless fall in different buckets. This probability can be reduced by doing multiple rounds of hashing with $n_{rounds}$ distinct hash functions $\\{h^{(1)}, h^{(2)}, \\ldots\\}$, such that:\n\\begin{align}\\label{eq:union-pairs}\n    &\\pair_i = \\bigcup_{r=1}^{n_{rounds}} \\pair^{(r)}_i\n    & \\text{ where } \\pair^{(r)}_i = \\left\\{ j : h^{(r)}(q_i) = h^{(r)}(q_j) \\right\\}\n\\end{align}\nThe multi-round case essentially involves performing LSH attention $n_{rounds}$ times in parallel; the details of the procedure are described in in Appendix~\\ref{sec:multi-round-detail}.\n\n\\paragraph{Causal masking for shared-QK attention.}\nIn a Transformer decoder, masking (denoted by $m(j, \\pair_i)$ in Equation~\\ref{eq:attn-masked}) is used to prevent positions from attending into the future. To implement masking in LSH attention, we associate every query/key vector with a position index, re-order the position indices using the same permutations used to sort the query/key vectors, and then use a comparison operation to compute the mask.\n\nWhile attention to the future is not allowed, typical implementations of the Transformer \\emph{do} allow a position to attend to \\emph{itself}. Such behavior is undesirable in a shared-QK formulation because the dot-product of a query vector with itself will almost always be greater than the dot product of a query vector with a vector at another position. We therefore modify the masking to forbid a token from attending to itself, except in situations where a token has no other valid attention targets (e.g.\\ the first token in a sequence).\n\n\\begin{table}\n\\caption{Memory and time complexity of attention variants.\n  We write $l$ for length, $b$ for batch size, $n_h$ for the number of heads,\n  $n_c$ for the number of LSH chunks, $n_r$ for the number of hash repetitions.}\n\\label{tab:complexity}\n\\begin{center}\n\\begin{tabular}{lcc}\nAttention Type & Memory Complexity & Time Complexity  \\\\\n\\hline\nScaled Dot-Product & $\\max(bn_hld_k, bn_hl^2)$ & $\\max(bn_hld_k, bn_hl^2)$ \\\\\nMemory-Efficient & $\\max(bn_hld_k, bn_hl^2)$ & $\\max(bn_hld_k, bn_hl^2)$ \\\\\nLSH Attention & $\\max(bn_hld_k, bn_hln_r(4l/n_c)^2)$ & $\\max(bn_hld_k, bn_hn_rl(4l/n_c)^2)$ \\\\\n\\end{tabular}\n\\end{center}\n\\end{table}\n\n\\subsection{Analysis on a synthetic task}\n\nTo verify the performance of LSH attention and study its behavior,\nwe start with the following synthetic task: duplicate a sequence\nof symbols. In this task, each training and testing example has\nthe form $0w0w$ where $w \\in \\{1, \\dots, N\\}^*$ is a sequence of\nsymbols ranging from $1$ to $N$ (we use $N = 127$ in our experiments).\nAn example with the word $w$ of length $3$ is given below.\n\n\\begin{center}\n\\vspace{0.5em}\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n{\\bf Example:} & 0 & 19 & 113 & 72 & 0  & 19  & 113  & 72  \\\\ \\hline\n\\end{tabular}\n\\vspace{0.5em}\n\\end{center}\n\nTo study LSH attention, we train a language model on examples of\nthe above form where each $w$ is of length $511$ (so the whole input\n$0w0w$ is of length $1024$). As this is a language modeling task,\nwe always predict the next symbol given all the previous ones,\nbut we mask the loss and accuracy to only consider positions in the\nsecond half of the input, i.e., those that can actually be predicted.\n\nThe above task can be solved perfectly (to accuracy 100\\% and loss 0) by\na 1-layer Transformer model. Note though, that it requires non-local\nattention lookups, so it cannot be solved by any model relying on sparse\nattention with a limited span.\nTo make it easy and fast to train but similar\nto models used in NLP, we use a 1-layer Transformer with\n$d_{model} = d_{ff} = 256$ and $4$ heads. We train it for 150K steps\nin $4$ different settings: with full attention, LSH attention with\n$n_{rounds} = 1$, $n_{rounds} = 2$ and $n_{rounds} = 4$.\n\n\\begin{table}\n\\caption{Accuracies on the duplication task of a 1-layer Transformer model\n  with full attention and with locality-sensitive hashing attention using\n  different number of parallel hashes.}\n\\label{tab:dupres}\n\\begin{center}\n\\begin{tabular}{l|c|c|c|c|c}\n\\diagbox{Train}{Eval} & Full Attention & LSH-$8$ & LSH-$4$ & LSH-$2$ & LSH-$1$ \\\\\n\\hline\nFull Attention & 100\\% & 94.8\\% & 92.5\\% & 76.9\\% & 52.5\\% \\\\\nLSH-$4$        & 0.8\\% & 100\\%  & 99.9\\% & 99.4\\% & 91.9\\% \\\\\nLSH-$2$        & 0.8\\% & 100\\%  & 99.9\\% & 98.1\\% & 86.8\\% \\\\\nLSH-$1$        & 0.8\\% & 99.9\\% & 99.6\\% & 94.8\\% & 77.9\\% \\\\ % \\hline\n\\end{tabular}\n\\end{center}\n\\end{table}\n\nFrom the results summarized in Table~\\ref{tab:dupres} we see that\na model trained with full attention can be immediately used with LSH\nattention, but at some loss of accuracy. When trained from scratch\nwith LSH attention, the model trained with 4 hashes achieves almost\nperfect accuracy as well. Interestingly, the accuracy becomes perfect\nwhen evaluated with 8 hashes. It goes down when evaluated with 2 or 1\nhashes. Models trained with less hashes show worse results but even\nthe model trained with just 1 hash performs almost perfectly when\nevaluated with 8 hashes.\n\n\\section{Reversible Transformer} \\label{sec:reversible}\n\nAs the above section shows, the complexity of attention can be reduced\nfrom square in length to linear, provided an approximation is acceptable.\nBut it is clear from Table~\\ref{tab:complexity} that each field starts\nwith a $b\\cdot n_h\\cdot l$ term: the $b\\cdot n_h\\cdot l\\cdot d_k$, \nor alternatively $b\\cdot l\\cdot d_{model}$ cost\ncannot be avoided. Indeed, the activations before each layer are already\nof the size $b\\cdot l\\cdot d_{model}$, so the memory use of the whole model with $n_l$\nlayers is at least $b\\cdot l\\cdot d_{model}\\cdot n_l$. Even worse: inside the feed-forward layers of \nTransformer this goes up to $b\\cdot l\\cdot d_{ff}\\cdot n_l$. In a big Transformer\nit is usual to set $d_{ff}=4K$ and $n_l=16$ so with $l=64K$ this again\nwould use an impractical $16GB$ of memory\n\nIn this section, we show how to reduce this cost by first dealing with the\n$n_l$ part of the term using reversible layers and then showing how chunking\ncan allow us to handle the $d_{ff}$ problem.\nThe effects of each of these approaches on memory and time complexity are summarized in Table~\\ref{tab:tcomplexity}.\n\n\\paragraph{RevNets.}\nReversible residual networks were introduced by \\citet{gomez2017reversible} where it was shown\nthat they can replace ResNets for image classification.\nThe main idea is to allow the activations at any given layer to be recovered from the activations at the following layer, using only the model parameters. Rather than having to checkpoint intermediate values for use in the backward pass, layers can be reversed one-by-one as back-propagation proceeds from the output of the network to its input. Whereas a normal residual layer performs a function $x \\mapsto y$ that operates on a single input and produces a single output and has the form $y = x + F(x)$, a reversible layer works on pairs of inputs/outputs: $(x_1, x_2) \\mapsto (y_1, y_2)$, and follows the equations:\n\\begin{align}\n    y_1 &= x_1 + F(x_2) &\n    y_2 &= x_2 + G(y_1)\n\\end{align}\n\nA layer can be reversed by subtracting (rather than adding) the residuals:\n\\begin{align}\\label{eq:reverse-ff}\n    x_2 &= y_2 - G(y_1) &\n    x_1 &= y_1 - F(x_2)\n\\end{align}\n\n\\paragraph{Reversible Transformer.}\nWe apply the RevNet idea to the Transformer by combining the attention and\nfeed-forward layers inside the revnet block. In the notation above,\nF becomes an attention layer while G becomes the feed-forward layer. Note that Layer Normalization \\citep{layernorm2016} is moved inside the residual blocks.\n\\begin{align}\n    Y_1 &= X_1 + \\mathrm{Attention}(X_2) &\n    Y_2 &= X_2 + \\mathrm{FeedForward}(Y_1)\n\\end{align}\n\nThe reversible Transformer does not need to store activations in each layer\nand so gets rid of the $n_l$ term.  In Section~\\ref{sec:exp_rev} we show that it performs the same as the normal Transformer when using  the same number of parameters; we achieve this by having both $x_1$ and $x_2$ have size $d_{model}$.\n\n\\paragraph{Chunking.}\nWhile reversibility covers the $n_l$ term, the thicker layers can still use\na lot of memory. The feed-forward layer in particular can use intermediate vectors \nof dimensionality $d_{ff}=4K$ or higher. However, computations in feed-forward layers \nare completely independent across positions in a sequence, so the computation can be split \ninto $c$ chunks:\n\\begin{equation}\n    Y_2 = \\left[Y_2^{(1)}; \\ldots; Y_2^{(c)}\\right] = \\left[X_2^{(1)} + \\mathrm{FeedForward}(Y_1^{(1)}); \\ldots; X_2^{(c)} + \\mathrm{FeedForward}(Y_1^{(c)})\\right]\n\\end{equation}\n\nThis layer is typically batched by performing operations for all positions in parallel, but operating on one chunk at a time can reduce memory. The reverse computation in (\\ref{eq:reverse-ff}) and the backward pass are also chunked.\nIn addition to the feed-forward layers, for models with large vocabulary (more than $d_{model}$ word types) we also chunk the log-probabilities at the output and calculate the loss for sections of the sequence at a time.\n\n\\paragraph{Chunking, large batches and parameter reuse.}\nWith chunking and reversible layers the memory we use for activations\nin the whole network is independent of the number of layers. The same is\nnot true for parameters though as their number grows with the number of layers. This problem is remedied though because we can swap layer\nparameters to and from CPU memory when this layer is not computing.\nIn a standard Transformer this would be inefficient because memory\ntransfer to CPU is slow. The batch size multiplied by length in Reformer\nis much larger though and therefore the amount of compute done with the parameters amortizes the cost of their transfer.\n\n\\begin{table}\n\\caption{Memory and time complexity of Transformer variants.\n  We write $d_{model}$ and $d_{ff}$ for model depth and assume $d_{ff} \\geq d_{model}$; $b$ stands for batch size, $l$ for length, $n_l$ for the number of layers.\n  We assume $n_{c} = l/32$ so $4l/n_{c} = 128$ and we write $c = 128^2$.}\n\\label{tab:tcomplexity}\n\\begin{center}\n\\begin{tabular}{lcc}\nModel Type & Memory Complexity & Time Complexity  \\\\\n\\hline\nTransformer & $\\max(bld_{ff}, bn_hl^2)n_l$ & $(bld_{ff} + bn_hl^2)n_l$ \\\\\nReversible Transformer & $\\max(bld_{ff}, bn_hl^2)$ & $(bn_hld_{ff} + bn_hl^2)n_l$ \\\\\nChunked Reversible Transformer & $\\max(bld_{model}, bn_hl^2)$ & $(bn_hld_{ff} + bn_hl^2)n_l$ \\\\\nLSH Transformer & $\\max(bld_{ff}, bn_hln_rc)n_l$ & $(bld_{ff} + bn_hn_rlc)n_l$ \\\\\nReformer & $\\max(bld_{model}, bn_hln_rc)$ & $(bld_{ff} + bn_hn_rlc)n_l$ \\\\\n\n\\end{tabular}\n\\end{center}\n\\end{table}\n\n\\section{Related Work} \\label{sec:relwork}\n\nThe Transformer model introduced in \\citep{transformer} has been used\nwidely in natural language tasks and further extended to model diverse data such as music scores \\citep{huang2018music}, \nand images \\citep{parmar2018imagetransformer, ramachandran2019attentionimage}.  Most notably, this model class \nhas been applied successfully in the self-supervised training of extremely large language models\n\\citep{devlin2018BERT, radford2019GPT2}.\n\nGiven the enormous computational requirements of state of the art sequence models, \nthere has been increasing interest in finding methods to reduce the memory footprint and computational \nrequirements of Transformer models.  In addition to standard methods such as precision reduction and gradient checkpointing \\citep{sohoni2019lowmemory}, more efficient versions of the Transformer model's self-attention mechanism \\citep{sukhbaatar2019adaptiveattn, sukhbaatar2019persistentmemory} have also recently been explored.\n\nIn particular, leveraging sparsity in the attention layers has proved fruitful. OpenAI introduced\nthe sparse Transformer \\citep{child2019sparsetransformer} which exploits a factorized sparse \nrepresentation of attention.  Using product-key attention to increase the key space has also been used to reduce memory requirements in the feed-forward layers with no loss in performance \n\\citep{lample2019productkeys}.\n\nLocality-sensitive hashing (LSH) has, to our knowledge, not been directly applied to \nTransformer attention layers before. But previous work using external memory with neural networks has\ndealt with memories of large sizes.  The original implementation of memory networks~\\citep{mem_nets} and later work on scaling it \\citep{large_mem_nets, hier_mem_nets} used memory with size in the millions.  The cost of doing so is that the memory must be fixed prior to training. Moreover, since during the beginning of training the model is unlikely to query the memory correctly, strong supervision is used to encourage the model to query memory locations that are useful. These hints are either given as additional supervising information by the  task or determined heuristically as in \\citet{goldilocks}.\nThe requirement that the memory be fixed before has been removed in\n\\citet{santoro16} at the cost of memory size and later alleviated by \\citet{jack_rae}. The last paper considered memory lookups with approximate nearest neighbors including both LSH and random kd-trees, but only for lookups in external memory.\n\n\\section{Experiments} \\label{sec:exp}\n\nIn this section we present experimental results demonstrating the techniques\ndescribed above. We analyze the techniques one-by-one to make clear which\ncombinations have impact on performance. We start by showing that reversible\nlayers and shared query-key spaces do not impact performance, then proceed \nto analyze hashing attention and finally the full Reformer model.\n\nWe ran our experiments on the imagenet64 and enwik8-64K tasks, where the latter is a variant of enwik8 that is chunked into subsequences of $2^{16} = 64K$ tokens. We use 3-layer models for our ablations so as to make it tractable to compare with the regular Transformer, which has high memory usage and performs full $O(l^2)$ attention. All experiments have $d_{model}=1024$, $d_{ff}=4096$, $n_{heads}=8$, and a total batch size of 8 sequences. We used the Adafactor optimizer \\citep{adafactor} for training these models. We also evaluate on the WMT 2014 English-to-German translation task, following the hyperparameters of \\citet{transformer}. Training for all experiments was parallelized across 8 devices (8 GPUs or 8 TPU v3 cores). Code for training our models is made publicly available.\\footnote{\\url{https://github.com/google/trax/tree/master/trax/models/reformer}}\n\n\\paragraph{Effect of sharing QK.} \\label{sec:exp_qk}\nWe first consider the effect of shared-QK attention on a regular Transformer model. Shared-QK attention sets $k_j = \\frac{q_j}{\\|q_j\\|}$ and prevents tokens from attending to themselves (except when no other context is available). In the left part of Figure~\\ref{fig:share-qk-rev}, we plot perplexity curves for both regular and shared-QK attention. A shared query-key space does not perform worse than regular attention; in fact, for enwik8 it appears to train slightly faster. In other words, we are not sacrificing accuracy by switching to shared-QK attention. \n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.9\\textwidth]{figures/joint_qk_rev_plot.png}\n    \\caption{Effect of shared query-key space (left) and reversibility (right) on \n    performance on enwik8 and imagenet64 training. The curves show bits per dim on held-out data.}\n    \\label{fig:share-qk-rev}\n\\end{figure}\n\n\\begin{table}\n\\caption{BLEU scores on newstest2014 for WMT English-German (Enâ€“De). We additionally report detokenized BLEU scores as computed by sacreBLEU~\\citep{sacrebleu}.}\n\\label{tab:wmt}\n\\begin{center}\n\\begin{tabular}{lccc}\n&& \\multicolumn{2}{c}{\\textit{sacreBLEU}} \\\\\nModel & BLEU & \\textit{Uncased}\\footnotemark[3] & \\textit{Cased}\\footnotemark[4] \\\\\n\\hline\n\\citet{transformer}, base model & 27.3 \\\\\n\\citet{transformer}, big & 28.4 \\\\\n\\citet{ott2018scaling}, big & 29.3 \\\\\n\\hline\nReversible Transformer (base, 100K steps) & 27.6 & \\textit{27.4} & \\textit{26.9} \\\\\nReversible Transformer (base, 500K steps, no weight sharing) & 28.0 & \\textit{27.9} & \\textit{27.4} \\\\\nReversible Transformer (big, 300K steps, no weight sharing) & 29.1 & \\textit{28.9} & \\textit{28.4}\n\\end{tabular}\n\\end{center}\n\\end{table}\n\n\\footnotetext[3]{\\scriptsize \\texttt{BLEU+case.lc+lang.en-de+numrefs.1+smooth.exp+test.wmt14/full+tok.intl+version.1.4.3}} \n\\footnotetext[4]{\\scriptsize \\texttt{BLEU+case.mixed+lang.en-de+numrefs.1+smooth.exp+test.wmt14/full+tok.intl+version.1.4.3}} \n\n\\paragraph{Effect of reversible layers.} \\label{sec:exp_rev}\nIn the two plots on the right in Figure~\\ref{fig:share-qk-rev}, we compare a regular Transformer per \\citet{transformer} with the reversible one describe in Section~\\ref{sec:reversible}. The two models have identical parameter counts, and the learning curves likewise appear to be nearly the same. These results show that the memory savings in the reversible Transformer do not come at the expense of accuracy.\n\n\\paragraph{Reversible layers in machine translation.}\nWe also evaluate reversible layers in the context of an encoder-decoder Transformer model for machine translation from English to German. We start by making both the encoder and the decoder fully reversible in the Transformer-base architecture, and see that the resulting model performs comparably to \\citet{transformer} when trained for 100K steps. We also evaluate training for a greater number of steps and with a larger model. Reformer models are very memory-efficient, so for the latter two experiments we do not need to save memory by sharing embedding and output projection weight matrices throughout the model. Results are shown in Table~\\ref{tab:wmt}. We do not apply LSH attention in this setting because examples are single sentences, and sentences tend to be relatively short. Our typical LSH attention configuration uses chunks of 128 tokens after hashing and sorting, whereas the examples in the WMT14 test set are all shorter than 128 tokens.\n\n\\paragraph{LSH attention in Transformer.} \\label{sec:exp_lsh}\nLSH attention is an approximation for full attention that, as evidenced in Figure~\\ref{fig:hash-curves}, becomes more accurate as the number of hashes increases. At $n_{rounds}=8$, it already almost matches full attention. The computational cost of a model grows with the number of hashes, so this hyperparameter can be adjusted depending on the available compute budget. Additionally, as in Table 2, the number of hashes can be increased at evaluation time to produce more accurate results.\nOn the right half of Figure~\\ref{fig:layer-curves}, we plot the speed of different attention types vs. the sequence length, while holding the total number of tokens fixed. We see that while regular attention becomes slower at longer sequence length, LSH attention speed remains flat.\n\n\\begin{figure}[!t]\n    \\centering\n    \\includegraphics[width=1.0\\textwidth]{figures/hash_plot.png}\n    \\caption{LSH attention performance as a function of hashing rounds on imagenet64.}\n    \\label{fig:hash-curves}\n\\end{figure}\n\n\\begin{figure}[!t]\n    \\centering\n    \\includegraphics[width=1.0\\textwidth]{figures/merge_depth_speed.png}\n    \\caption{Left: LSH attention performance as a function of number of layers on enwik8. \n    Right: Speed of attention evaluation as a function of input length for full- and LSH- attention.}\n    \\label{fig:layer-curves}\n\\end{figure}\n\n\\paragraph{Large Reformer models.} \\label{sec:exp_large}\nTo verify that the Reformer can indeed fit large models on a single core\nand train fast on long sequences, we train up to 20-layer big Reformers on enwik8 and imagenet64.\nAs can be seen in Figure~\\ref{fig:layer-curves}, these models\nfit into memory and train. We were not able to train Transformer baselines\nin this case as they are too slow and memory-hungry, but we see clear improvement\nwith the number of layers. A 12-layer model on enwik8 trained for 20K steps with a dropout rate of 0.1 achieves 1.19 bits/dim on the test set. We also trained a 12-layer Reformer model for longer with\nfurther tuning and improvements and we reached 1.05 bits/dim on the enwiki8 test\nset.\n\n\\section{Conclusion}\n\nReformer combines the modeling capacity of a Transformer with an architecture that can be executed efficiently on long sequences and with small memory\nuse even for models with a large number of layers.\nWe believe that this will help large, richly-parameterized Transformer models become more widespread and accessible. \nAlso, the ability to handle long sequences opens the way for the use of\nthe Reformer on many generative tasks. In addition to generating very long\ncoherent text, the Reformer can bring the power of Transformer models to other\ndomains like time-series forecasting, music, image and video generation.\n\n\\newpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Informer: Beyond Efficient Transformer for Long Sequence \\\\ Time-Series Forecasting}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n\nMany real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. \nTo address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a \\emph{ProbSparse} self-attention mechanism, which achieves $\\bigO(L \\log L)$ in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.\n\n\\end{abstract}\n\n\\section{Introduction}\n\n\\noindent Time-series forecasting is a critical ingredient across many domains, such as sensor network monitoring \\cite{papadimitriou2006optimal}, energy and smart grid management, economics and finance \\cite{DBLP:conf/vldb/ZhuS02}, and disease propagation analysis \\cite{DBLP:conf/kdd/MatsubaraSPF14}. In these scenarios, we can leverage a substantial amount of time-series data on past behavior to make a forecast in the long run, namely long sequence time-series forecasting (LSTF).\nHowever, existing methods are mostly designed under short-term problem setting, like predicting 48 points or less \\cite{hochreiter1997long, li2017graph, yu2017long, journals/corr/abs-1904-07464, conf/ijcai/QinSCCJC17,wen2017multi}. \nThe increasingly long sequences strain the models' prediction capacity to the point where this trend is holding the research on LSTF.\nAs an empirical example, Fig.(\\ref{fig:intro.4fig}) shows the forecasting results on a real dataset, where the LSTM network predicts the hourly temperature of an electrical transformer station from the short-term period (12 points, 0.5 days) to the long-term period (480 points, 20 days). The overall performance gap is substantial when the prediction length is greater than 48 points (the solid star in Fig.(\\ref{fig:intro.4fig}b)), where the MSE rises to unsatisfactory performance, the inference speed gets sharp drop, and the LSTM model starts to fail.\n\n\\begin{figure}[t]\n\\centering\n\\subfigure[Sequence Forecasting.]{\n\\includegraphics[width=0.46\\linewidth]{figures/intro_short_long.pdf}\n}\n\\hfill\n\\subfigure[Run LSTM on sequences.]{\n\\includegraphics[width=0.48\\linewidth]{figures/intro_d.pdf}\n}\n\\caption{(a) LSTF can cover an extended period than the short sequence predictions, making vital distinction in policy-planning and investment-protecting. (b) The prediction capacity of existing methods limits LSTF's performance. E.g., starting from length=48, MSE rises unacceptably high, and the inference speed drops rapidly.}\n\\label{fig:intro.4fig}\n\\end{figure}\n\nThe major challenge for LSTF is to enhance the prediction capacity to meet the increasingly long sequence demand, which requires (a) extraordinary long-range alignment ability and (b) efficient operations on long sequence inputs and outputs.\nRecently, Transformer models have shown superior performance in capturing long-range dependency than RNN models. The self-attention mechanism can reduce the maximum length of network signals traveling paths into the theoretical shortest $\\bigO(1)$ and avoid the recurrent structure, whereby Transformer shows great potential for the LSTF problem.\nNevertheless, the self-attention mechanism violates requirement (b) due to its $L$-quadratic computation and memory consumption on $L$-length inputs/outputs. Some large-scale Transformer models pour resources and yield impressive results on NLP tasks \\cite{DBLP:journals/corr/abs-2005-14165}, but the training on dozens of GPUs and expensive deploying cost make theses models unaffordable on real-world LSTF problem. The efficiency of the self-attention mechanism and Transformer architecture becomes the bottleneck of applying them to LSTF problems.\nThus, in this paper, we seek to answer the question: \\emph{can we improve Transformer models to be computation, memory, and architecture efficient, as well as maintaining higher prediction capacity?}\n\nVanilla Transformer~\\cite{vaswani2017attention} has three significant limitations when solving the LSTF problem:\n\n\\begin{enumerate}[nosep, leftmargin=0.45cm]\n    \\item \\textbf{The quadratic computation of self-attention.} The atom operation of self-attention mechanism, namely canonical dot-product, causes the time complexity and memory usage per layer to be $\\bigO(L^2)$. \n    \\item \\textbf{The memory bottleneck in stacking layers for long inputs.} The stack of $J$ encoder/decoder layers makes total memory usage to be $\\bigO(J \\cdot L^2)$, which limits the model scalability in receiving long sequence inputs.\n    \\item \\textbf{The speed plunge in predicting long outputs.} Dynamic decoding of vanilla Transformer makes the step-by-step inference as slow as RNN-based model (Fig.(\\ref{fig:intro.4fig}b)).\n\\end{enumerate}\nThere are some prior works on improving the efficiency of self-attention. The Sparse Transformer \\cite{child2019generating}, LogSparse Transformer \\cite{li2019enhancing}, and Longformer \\cite{DBLP:journals/corr/abs-2004-05150} all use a heuristic method to tackle limitation 1 and reduce the complexity of self-attention mechanism to $\\bigO(L \\log L)$, where their efficiency gain is limited \\cite{qiu2019blockwise}. Reformer \\cite{kitaev2019reformer} also achieves $\\bigO(L \\log L)$ with locally-sensitive hashing self-attention, but it only works on extremely long sequences. More recently, Linformer \\cite{wang2020linformer} claims a linear complexity $\\bigO(L)$, but the project matrix can not be fixed for real-world long sequence input, which may have the risk of degradation to $\\bigO(L^2)$. Transformer-XL \\cite{dai2019transformer} and Compressive Transformer \\cite{rae2019compressive} use auxiliary hidden states to capture long-range dependency, which could amplify limitation 1 and be adverse to break the efficiency bottleneck. All these works mainly focus on limitation 1, and the limitation 2\\&3 remains unsolved in the LSTF problem. To enhance the prediction capacity, we tackle all these limitations and achieve improvement beyond efficiency in the proposed \\mn.\n\nTo this end, our work delves explicitly into these three issues.  We investigate the sparsity in the self-attention mechanism, make improvements of network components, and conduct extensive experiments.\nThe contributions of this paper are summarized as follows:\n\n\\begin{itemize}[nosep, leftmargin=0.45cm]\n\\item We propose {\\mn} to successfully enhance the prediction capacity in the LSTF problem, which validates the Transformer-like model's potential value to capture individual long-range dependency between long sequence time-series outputs and inputs.\n\\item We propose \\emph{ProbSparse} self-attention mechanism to efficiently replace the canonical self-attention. It achieves the $\\bigO(L \\log L)$ time complexity and $\\bigO(L \\log L)$ memory usage on dependency alignments.\n\\item We propose self-attention distilling operation to privilege dominating attention scores in $J$-stacking layers and sharply reduce the total space complexity to be $\\bigO((2-\\epsilon)L \\log L)$, which helps receiving long sequence input.\n\\item We propose generative style decoder to acquire long sequence output with only one forward step needed, simultaneously avoiding cumulative error spreading during the inference phase.\n\\end{itemize}\n\n\\section{Preliminary}\n\\label{sec:method}\n\nWe first provide the LSTF problem definition.\nUnder the rolling forecasting setting with a fixed size window, we have the input $\\mathcal{X}^t=\\{\\mb{x}^t_1,\\ldots,\\mb{x}^t_{L_x} ~|~ \\mb{x}^t_{i} \\in \\mathbb{R}^{d_{x}}\\}$ at time $t$, and the output is to predict corresponding sequence $\\mathcal{Y}^t=\\{\\mb{y}^t_1,\\ldots,\\mb{y}^t_{L_y} ~|~ \\mb{y}^t_{i} \\in \\mathbb{R}^{d_{y}} \\}$. The LSTF problem encourages a longer output's length $L_y$ than previous works \\cite{cho2014properties,sutskever2014sequence} and the feature dimension is not limited to univariate case ($d_{y} \\geq 1$).\n\n\\textbf{Encoder-decoder architecture}\nMany popular models are devised to ``encode'' the input representations $\\mathcal{X}^t$ into a hidden state representations $\\mathcal{H}^t$ and ``decode'' an output representations $\\mathcal{Y}^t$ from $\\mathcal{H}^t=\\{\\mb{h}^t_1,\\ldots,\\mb{h}^t_{L_h}\\}$. The inference involves a step-by-step process named ``dynamic decoding'', where the decoder computes a new hidden state $\\mb{h}^t_{k+1}$ from the previous state $\\mb{h}^t_{k}$ and other necessary outputs from $k$-th step then predict the $(k+1)$-th sequence $\\mb{y}^t_{k+1}$.\n\n\\textbf{Input Representation} A uniform input representation is given to enhance the global positional context and local temporal context of the time-series inputs. To avoid trivializing description, we put the details in Appendix B.\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\linewidth]{figures/Encoder_Decoder_all.pdf}\n\\caption{{\\mn} model overview. Left: The encoder receives massive long sequence inputs (green series). We replace canonical self-attention with the proposed \\emph{ProbSparse} self-attention. The blue trapezoid is the self-attention distilling operation to extract dominating attention, reducing the network size sharply. The layer stacking replicas increase robustness. Right: The decoder receives long sequence inputs, pads the target elements into zero, measures the weighted attention composition of the feature map, and instantly predicts output elements (orange series) in a generative style.}\n\\label{fig:method.en.de.all}\n\\end{figure}\n\n\\section{Methodology}\nExisting methods for time-series forecasting can be roughly grouped into two categories\\footnote{Related work is in Appendix A due to space limitation.}. Classical time-series models serve as a reliable workhorse for time-series forecasting \\cite{box2015time,ray1990time,seeger2017approximate,seeger2016bayesian}, and deep learning techniques mainly develop an encoder-decoder prediction paradigm by using RNN and their variants \\cite{hochreiter1997long,li2017graph,yu2017long}.\nOur proposed {\\mn} holds the encoder-decoder architecture while targeting the LSTF problem. Please refer to Fig.(\\ref{fig:method.en.de.all}) for an overview and the following sections for details.\n\n\\subsection{Efficient Self-attention Mechanism}\n\\label{sec:method.transformer.attention}\n\nThe canonical self-attention in \\cite{vaswani2017attention} is defined based on the tuple inputs, i.e, query, key and value, which performs the scaled dot-product as $\\mathcal{A} (\\mb{Q}, \\mb{K}, \\mb{V}) = \\textrm{Softmax} ({\\mb{Q}\\mb{K}^{\\top}}/{\\sqrt{d}})\\mb{V}$,\n\\iffalse\n\\begin{equation}\\label{eq:method.origin.Attention}\n    \\mathcal{A} (\\mb{Q}, \\mb{K}, \\mb{V}) = \\textrm{Softmax} (\\frac{\\mb{Q}\\mb{K}^{\\top}}{\\sqrt{d}})\\mb{V} \\qquad,\n\\end{equation}\n\\fi\nwhere $\\mb{Q} \\in \\mathbb{R}^{L_Q \\times d}$, $\\mb{K} \\in \\mathbb{R}^{L_K \\times d}$, $\\mb{V} \\in \\mathbb{R}^{L_V \\times d}$ and $d$ is the input dimension.\nTo further discuss the self-attention mechanism, let $\\mb{q}_{i}$, $\\mb{k}_{i}$, $\\mb{v}_{i}$ stand for the $i$-th row in $\\mb{Q}$, $\\mb{K}$, $\\mb{V}$ respectively. Following the formulation in \\cite{tsai2019transformer}, the $i$-th query's attention is defined as a kernel smoother in a probability form:\n\\begin{equation}\\label{eq:method.prob.Attention}\n    \\mathcal{A} (\\mb{q}_i, \\mb{K}, \\mb{V}) = \\sum_{j} \\frac{k(\\mb{q}_i, \\mb{k}_j)}{\\sum_{l} k(\\mb{q}_i, \\mb{k}_l)} \\mb{v}_{j} = \\mathbb{E}_{p(\\mb{k}_j|\\mb{q}_i)} [\\mb{v}_j] ~,\n\\end{equation}\nwhere $p(\\mb{k}_j|\\mb{q}_i) = {k(\\mb{q}_i, \\mb{k}_j)}/{\\sum_{l} k(\\mb{q}_i, \\mb{k}_l)}$ and $k(\\mb{q}_i, \\mb{k}_j)$ selects the asymmetric\nexponential kernel $\\exp ({\\mb{q}_i \\mb{k}_j^{\\top}}/{\\sqrt{d}})$.\nThe self-attention combines the values and acquires outputs based on computing the probability $p(\\mb{k}_j|\\mb{q}_i)$. It requires the quadratic times dot-product computation and $\\bigO(L_Q L_K)$ memory usage, which is the major drawback when enhancing prediction capacity.\n\nSome previous attempts have revealed that the distribution of self-attention probability has potential sparsity, and they have designed ``selective'' counting strategies on all $p(\\mb{k}_j|\\mb{q}_i)$ without significantly affecting the performance.\nThe Sparse Transformer \\cite{child2019generating} incorporates both the row outputs and column inputs, in which the sparsity arises from the separated spatial correlation. The LogSparse Transformer \\cite{li2019enhancing} notices the cyclical pattern in self-attention and forces each cell to attend to its previous one by an exponential step size.\nThe Longformer \\cite{DBLP:journals/corr/abs-2004-05150} extends previous two works to more complicated sparse configuration.\nHowever, they are limited to theoretical analysis from following heuristic methods and tackle each multi-head self-attention with the same strategy, which narrows their further improvement.\n\nTo motivate our approach, we first perform a qualitative assessment on the learned attention patterns of the canonical self-attention. The ``sparsity'' self-attention score forms a long tail distribution (see Appendix C for details), i.e., a few dot-product pairs contribute to the major attention, and others generate trivial attention. Then, the next question is how to distinguish them?\n\n\\begin{figure*}[t]\n  \\centering\n  \\includegraphics[width=0.85\\linewidth]{figures/Network_Decoder.pdf}\n  \\caption{The single stack in \\mn's encoder. (1) The horizontal stack stands for an individual one of the encoder replicas in Fig.(\\ref{fig:method.en.de.all}). (2) The presented one is the main stack receiving the whole input sequence. Then the second stack takes half slices of the input, and the subsequent stacks repeat. (3) The red layers are dot-product matrixes, and they get cascade decrease by applying self-attention distilling on each layer. (4) Concatenate all stacks' feature maps as the encoder's output.}\n  \\label{fig:method.encoder}\n\\end{figure*}\n\n\\textbf{Query Sparsity Measurement}\nFrom Eq.(\\ref{eq:method.prob.Attention}), the $i$-th query's attention on all the keys are defined as a probability $p(\\mb{k}_j|\\mb{q}_i)$ and the output is its composition with values $\\mb{v}$. The dominant dot-product pairs encourage the corresponding query's attention probability distribution away from the uniform distribution. If $p(\\mb{k}_j|\\mb{q}_i)$ is close to a uniform distribution $q(\\mb{k}_j|\\mb{q}_i)={1}/{L_{K}}$, the self-attention becomes a trivial sum of values $\\mb{V}$ and is redundant to the residential input. Naturally, the ``likeness'' between distribution $p$ and $q$ can be used to distinguish the ``important'' queries. We measure the ``likeness'' through Kullback-Leibler divergence\n$ KL (q||p) = \\ln \\sum_{l=1}^{L_K} e^{{\\mb{q}_i\\mb{k}_l^{\\top}}/{\\sqrt{d}}} - \\frac{1}{L_K} \\sum_{j=1}^{L_K} {\\mb{q}_i\\mb{k}_j^{\\top}}/{\\sqrt{d}} - \\ln L_K $.\nDropping the constant, we define the $i$-th query's sparsity measurement as\n\\begin{equation}\n\\label{eq.method.measure}\n    M (\\mb{q}_i, \\mb{K}) = \\ln \\sum_{j=1}^{L_K} e^{\\frac{\\mb{q}_i\\mb{k}_j^{\\top}}{\\sqrt{d}}} - \\frac{1}{L_K} \\sum_{j=1}^{L_K} \\frac{\\mb{q}_i\\mb{k}_j^{\\top}}{\\sqrt{d}} \\qquad,\n\\end{equation}\nwhere the first term is the Log-Sum-Exp (LSE) of $\\mb{q}_i$ on all the keys, and the second term is the arithmetic mean on them. If the $i$-th query gains a larger $M(\\mb{q}_i, \\mb{K})$, its attention probability $p$ is more ``diverse'' and has a high chance to contain the dominate dot-product pairs in the header field of the long tail self-attention distribution.\n\n\\textbf{\\emph{ProbSparse} Self-attention}\nBased on the proposed measurement, we have the \\emph{ProbSparse} self-attention by allowing each key to only attend to the $u$ dominant queries:\n\\begin{equation}\n\\label{eq:method.probsparse}\n    \\mathcal{A} (\\mb{Q}, \\mb{K}, \\mb{V}) = \\textrm{Softmax} (\\frac{\\overline{\\mb{Q}}\\mb{K}^{\\top}}{\\sqrt{d}})\\mb{V} \\qquad,\n\\end{equation}\nwhere $\\overline{\\mb{Q}}$ is a sparse matrix of the same size of $\\mb{q}$ and it only contains the Top-$u$ queries under the sparsity measurement $M(\\mb{q},\\mb{K})$. Controlled by a constant sampling factor $c$, we set $u=c \\cdot \\ln{L_Q}$, which makes the \\emph{ProbSparse} self-attention only need to calculate $\\bigO(\\ln{L_Q})$ dot-product for each query-key lookup and the layer memory usage maintains $\\bigO(L_K\\ln{L_Q})$. Under the multi-head perspective, this attention generates different sparse query-key pairs for each head, which avoids severe information loss in return.\n\nHowever, the traversing of all the queries for the measurement $M(\\mb{q}_i,\\mb{K})$ requires calculating each dot-product pairs, i.e., quadratically $\\bigO(L_Q L_K)$, besides the LSE operation has the potential numerical stability issue. Motivated by this, we propose an empirical approximation for the efficient acquisition of the query sparsity measurement.\n\n\\begin{lemma}\n\\label{le.M.bound}\nFor each query $\\mb{q}_i \\in \\mathbb{R}^{d}$ and $\\mb{k}_j \\in \\mathbb{R}^{d}$ in the keys set $\\mb{K}$, we have the bound as\n$\\ln L_K \\leq M (\\mb{q}_i, \\mb{K}) \\leq \\max_{j}\\{{\\mb{q}_i \\mb{k}_j^{\\top}}/{\\sqrt{d}}\\} - \\frac{1}{L_K} \\sum_{j=1}^{L_K} \\{{\\mb{q}_i\\mb{k}_j^{\\top}}/{\\sqrt{d}} \\} + \\ln L_K$.\nWhen $\\mb{q}_i \\in \\mb{K}$, it also holds.\n\\end{lemma}\n\nFrom the Lemma \\ref{le.M.bound} (proof is given in Appendix D.1), we propose the max-mean measurement as\n\\begin{equation}\n\\label{eq.method.measure2}\n    \\overline{M} (\\mb{q}_i, \\mb{K}) = \\max_{j}\\{\\frac{\\mb{q}_i \\mb{k}_j^{\\top}}{\\sqrt{d}}\\} - \\frac{1}{L_K} \\sum_{j=1}^{L_K} \\frac{\\mb{q}_i\\mb{k}_j^{\\top}}{\\sqrt{d}} \\quad.\n\\end{equation}\n\nThe range of Top-$u$ approximately holds in the boundary relaxation with Proposition 1 (refers in Appendix D.2). Under the long tail distribution, we only need to randomly sample $U=L_K \\ln L_Q$ dot-product pairs to calculate the $\\overline{M} (\\mb{q}_i, \\mb{K})$, i.e., filling other pairs with zero. Then, we select sparse Top-$u$ from them as $\\overline{\\mb{Q}}$. The max-operator in $\\overline{M} (\\mb{q}_i, \\mb{K})$ is less sensitive to zero values and is numerical stable. In practice, the input length of queries and keys are typically equivalent in the self-attention computation, i.e $L_Q=L_K=L$ such that the total \\emph{ProbSparse} self-attention time complexity and space complexity are $\\bigO(L \\ln L)$.\n\n\\subsection{Encoder: Allowing for Processing Longer Sequential Inputs under the Memory Usage Limitation}\n\\label{sec:method.transformer.encoder}\nThe encoder is designed to extract the robust long-range dependency of the long sequential inputs.\nAfter the input representation, the $t$-th sequence input $\\mathcal{X}^t$ has been shaped into a matrix $\\mb{X}^t_{\\textrm{en}} \\in \\mathbb{R}^{L_x \\times d_{\\textrm{model}}}$. We give a sketch of the encoder in Fig.(\\ref{fig:method.encoder}) for clarity.\n\n\\textbf{Self-attention Distilling} As the natural consequence of the \\emph{ProbSparse} self-attention mechanism, the encoder's feature map has redundant combinations of value $\\mb{V}$. We use the distilling operation to privilege the superior ones with dominating features and make a focused self-attention feature map in the next layer. It trims the input's time dimension sharply, seeing the $n$-heads weights matrix (overlapping red squares) of Attention blocks in Fig.(\\ref{fig:method.encoder}). Inspired by the dilated convolution \\cite{yu2017dilated, gupta2017dilated}, our ``distilling'' procedure forwards from $j$-th layer into $(j+1)$-th layer as:\n\\begin{equation}\n\\label{eq:method.distilling}\n\\mb{X}^t_{j+1}=\n    \\mathrm{MaxPool}\\left(~\n    \\mathrm{ELU}(~\n    \\mathrm{Conv1d}(\n    [\\mb{X}^t_{j}]_{\\mathrm{AB}}\n    )~)~\\right) \\quad, \n\\end{equation}\nwhere $[\\cdot]_{\\text{{AB}}}$ represents the attention block. It contains the Multi-head \\emph{ProbSparse} self-attention and the essential operations, where $\\textrm{Conv1d}(\\cdot)$ performs an 1-D convolutional filters (kernel width=3) on time dimension with the $\\textrm{ELU}(\\cdot)$ activation function \\cite{clevert2015fast}. We add a max-pooling layer with stride 2 and down-sample $\\mb{X}^t$ into its half slice after stacking a layer, which reduces the whole memory usage to be $\\bigO((2-\\epsilon) L \\log L)$, where $\\epsilon$ is a small number. To enhance the robustness of the distilling operation, we build replicas of the main stack with halving inputs, and progressively decrease the number of self-attention distilling layers by dropping one layer at a time, like a pyramid in Fig.(\\ref{fig:method.en.de.all}), such that their output dimension is aligned. Thus, we concatenate all the stacks' outputs and have the final hidden representation of encoder.\n\n\\subsection{Decoder: Generating Long Sequential Outputs Through One Forward Procedure}\n\\label{sec:method.decoder}\nWe use a standard decoder structure \\cite{vaswani2017attention} in Fig.(\\ref{fig:method.en.de.all}), and it is composed of a stack of two identical multi-head attention layers. However, the generative inference is employed to alleviate the speed plunge in long prediction. We feed the decoder with the following vectors as\n\\begin{equation}\\label{eq.method.de_in}\n    \\mb{X}^t_{\\textrm{de}} = \\textrm{Concat} ( \\mb{X}^t_{\\textrm{token}}, \\mb{X}^t_{\\mb{0}}  ) \\in \\mathbb{R}^{(L_{\\textrm{token}} + L_y) \\times d_{\\textrm{model}}} \\quad,\n\\end{equation}\nwhere $\\mb{X}^t_{\\textrm{token}} \\in \\mathbb{R}^{L_{\\textrm{token}} \\times d_{\\textrm{model}}}$ is the start token, $\\mb{X}^t_{\\mb{0}} \\in \\mathbb{R}^{L_y \\times d_{\\textrm{model}}} $ is a placeholder for the target sequence (set scalar as 0).\nMasked multi-head attention is applied in the \\emph{ProbSparse} self-attention computing by setting masked dot-products to $-\\infty$. It prevents each position from attending to coming positions, which avoids auto-regressive. A fully connected layer acquires the final output, and its outsize $d_y$ depends on whether we are performing a univariate forecasting or a multivariate one.\n\n\\textbf{Generative Inference} Start token is efficiently applied in NLP's ``dynamic decoding'' \\cite{devlin2018bert}, and we extend it into a generative way. Instead of choosing specific flags as the token, we sample a $L_{\\textrm{token}}$ long sequence in the input sequence, such as an earlier slice before the output sequence.\nTake predicting 168 points as an example (7-day temperature prediction in the experiment section), we will take the known 5 days before the target sequence as ``start-token'', and feed the generative-style inference decoder with $\\mb{X}_{\\textrm{de}}=\\{\\mb{X}_{5d},\\mb{X}_{\\mb{0}}\\}$. The $\\mb{X}_{\\mb{0}}$ contains target sequence's time stamp, i.e., the context at the target week.\nThen our proposed decoder predicts outputs by one forward procedure rather than the time consuming ``dynamic decoding'' in the conventional encoder-decoder architecture. A detailed performance comparison is given in the computation efficiency section.\n\n\\textbf{Loss function}\nWe choose the MSE loss function on prediction w.r.t the target sequences, and the loss is propagated back from the decoder's outputs across the entire model.\n\n\\begin{table*}[t]\n\\centering\n\\fontsize{9pt}{9pt}\\selectfont\n\\centering\n\\begin{tabular}{c|c|c|c|c|c|c|c|c|c}\n\\toprule[1.0pt]\n\\multicolumn{2}{c|}{Methods}         & {\\mn} & {\\mn$^{\\dag}$} & {LogTrans} & {Reformer} & {LSTMa} & {DeepAR} & {ARIMA} & {Prophet} \\\\\n\\midrule[0.5pt]\n\\multicolumn{2}{c|}{Metric}          & MSE~~MAE               & MSE~~MAE                    & MSE~~MAE          & MSE~~MAE          & MSE~~MAE    & MSE~~MAE            & MSE~~MAE             & MSE~~MAE          \\\\\n\\midrule[1.0pt]\n\\multirow{5}{*}{\\rotatebox{90}{ETTh$_1$}}\t & \t24\t & \t0.098~~0.247\t & \t\\textbf{0.092}~~\\textbf{0.246}\t & \t0.103~~0.259\t & \t0.222~~0.389\t & \t0.114~~0.272\t & \t0.107~~0.280\t & \t0.108~~0.284\t & \t0.115~~0.275\\\\\n\t & \t48\t & \t\\textbf{0.158}~~\\textbf{0.319}\t & \t0.161~~0.322\t & \t0.167~~0.328\t & \t0.284~~0.445\t & \t0.193~~0.358\t & \t0.162~~0.327\t & \t0.175~~0.424\t & \t0.168~~0.330\\\\\n\t & \t168\t & \t\\textbf{0.183}~~\\textbf{0.346}\t & \t0.187~~0.355\t & \t0.207~~0.375\t & \t1.522~~1.191\t & \t0.236~~0.392\t & \t0.239~~0.422\t & \t0.396~~0.504\t & \t1.224~~0.763\\\\\n\t & \t336\t & \t0.222~~0.387\t & \t\\textbf{0.215}~~\\textbf{0.369}\t & \t0.230~~0.398\t & \t1.860~~1.124\t & \t0.590~~0.698\t & \t0.445~~0.552\t & \t0.468~~0.593\t & \t1.549~~1.820\\\\\n\t & \t720\t & \t0.269~~0.435\t & \t\\textbf{0.257}~~\\textbf{0.421}\t & \t0.273~~0.463\t & \t2.112~~1.436\t & \t0.683~~0.768\t & \t0.658~~0.707\t & \t0.659~~0.766\t & \t2.735~~3.253\\\\\n\\midrule[0.5pt]\n\\multirow{5}{*}{\\rotatebox{90}{ETTh$_2$}}\t & \t24\t & \t\\textbf{0.093}~~\\textbf{0.240}\t & \t0.099~~0.241\t & \t0.102~~0.255\t & \t0.263~~0.437\t & \t0.155~~0.307\t & \t0.098~~0.263\t & \t3.554~~0.445\t & \t0.199~~0.381\\\\\n\t & \t48\t & \t\\textbf{0.155}~~\\textbf{0.314}\t & \t0.159~~0.317\t & \t0.169~~0.348\t & \t0.458~~0.545\t & \t0.190~~0.348\t & \t0.163~~0.341\t & \t3.190~~0.474\t & \t0.304~~0.462\\\\\n\t & \t168\t & \t\\textbf{0.232}~~\\textbf{0.389}\t & \t0.235~~0.390\t & \t0.246~~0.422\t & \t1.029~~0.879\t & \t0.385~~0.514\t & \t0.255~~0.414\t & \t2.800~~0.595\t & \t2.145~~1.068\\\\\n\t & \t336\t & \t0.263~~\\textbf{0.417}\t & \t\\textbf{0.258}~~0.423\t & \t0.267~~0.437\t & \t1.668~~1.228\t & \t0.558~~0.606\t & \t0.604~~0.607\t & \t2.753~~0.738\t & \t2.096~~2.543\\\\\n\t & \t720\t & \t\\textbf{0.277}~~\\textbf{ 0.431}\t & \t0.285~~0.442\t & \t0.303~~0.493\t & \t2.030~~1.721\t & \t0.640~~0.681\t & \t0.429~~0.580\t & \t2.878~~1.044\t & \t3.355~~4.664\\\\\n\\midrule[0.5pt]\n\\multirow{5}{*}{\\rotatebox{90}{ETTm$_1$}}\t & \t24\t & \t\\textbf{0.030}~~\\textbf{0.137}\t & \t0.034~~0.160\t & \t0.065~~0.202\t & \t0.095~~0.228\t & \t0.121~~0.233\t & \t0.091~~0.243\t & \t0.090~~0.206\t & \t0.120~~0.290\\\\\n\t & \t48\t & \t0.069~~0.203\t & \t\\textbf{0.066}~~\\textbf{0.194}\t & \t0.078~~0.220\t & \t0.249~~0.390\t & \t0.305~~0.411\t & \t0.219~~0.362\t & \t0.179~~0.306\t & \t0.133~~0.305\\\\\n\t & \t96\t & \t0.194~~\\textbf{0.372}\t & \t\\textbf{0.187}~~0.384\t & \t0.199~~0.386\t & \t0.920~~0.767\t & \t0.287~~0.420\t & \t0.364~~0.496\t & \t0.272~~0.399\t & \t0.194~~0.396\\\\\n\t & \t288\t & \t\\textbf{0.401}~~0.554\t & \t0.409~~\\textbf{0.548}\t & \t0.411~~0.572\t & \t1.108~~1.245\t & \t0.524~~0.584\t & \t0.948~~0.795\t & \t0.462~~0.558\t & \t0.452~~0.574\\\\\n\t & \t672\t & \t\\textbf{0.512}~~\\textbf{0.644}\t & \t0.519~~0.665\t & \t0.598~~0.702\t & \t1.793~~1.528\t & \t1.064~~0.873\t & \t2.437~~1.352\t & \t0.639~~0.697\t & \t2.747~~1.174\\\\\n\\midrule[0.5pt]\n\\multirow{5}{*}{\\rotatebox{90}{Weather}}\t & \t24\t & \t\\textbf{0.117}~~\\textbf{0.251}\t & \t0.119~~0.256\t & \t0.136~~0.279\t & \t0.231~~0.401\t & \t0.131~~0.254\t & \t0.128~~0.274\t & \t0.219~~0.355\t & \t0.302~~0.433\\\\\n\t & \t48\t & \t\\textbf{0.178}~~0.318\t & \t0.185~~\\textbf{0.316}\t & \t0.206~~0.356\t & \t0.328~~0.423\t & \t0.190~~0.334\t & \t0.203~~0.353\t & \t0.273~~0.409\t & \t0.445~~0.536\\\\\n\t & \t168\t & \t\\textbf{0.266}~~\\textbf{0.398}\t & \t0.269~~0.404\t & \t0.309~~0.439\t & \t0.654~~0.634\t & \t0.341~~0.448\t & \t0.293~~0.451\t & \t0.503~~0.599\t & \t2.441~~1.142\\\\\n\t & \t336\t & \t\\textbf{0.297}~~\\textbf{0.416}\t & \t0.310~~0.422\t & \t0.359~~0.484\t & \t1.792~~1.093\t & \t0.456~~0.554\t & \t0.585~~0.644\t & \t0.728~~0.730\t & \t1.987~~2.468\\\\\n\t & \t720\t & \t\\textbf{0.359}~~\\textbf{0.466}\t & \t0.361~~0.471\t & \t0.388~~0.499\t & \t2.087~~1.534\t & \t0.866~~0.809\t & \t0.499~~0.596\t & \t1.062~~0.943\t & \t3.859~~1.144\\\\\n\\midrule[0.5pt]\n\\multirow{5}{*}{\\rotatebox{90}{ECL}}\t & \t48\t & \t0.239~~0.359\t & \t0.238~~0.368\t & \t0.280~~0.429\t & \t0.971~~0.884\t & \t0.493~~0.539\t & \t\\textbf{0.204}~~\\textbf{0.357}\t & \t0.879~~0.764\t & \t0.524~~0.595\\\\\n\t & \t168\t & \t0.447~~0.503\t & \t0.442~~0.514\t & \t0.454~~0.529\t & \t1.671~~1.587\t & \t0.723~~0.655\t & \t\\textbf{0.315}~~\\textbf{0.436}\t & \t1.032~~0.833\t & \t2.725~~1.273\\\\\n\t & \t336\t & \t0.489~~0.528\t & \t0.501~~0.552\t & \t0.514~~0.563\t & \t3.528~~2.196\t & \t1.212~~0.898\t & \t\\textbf{0.414}~~\\textbf{0.519}\t & \t1.136~~0.876\t & \t2.246~~3.077\\\\\n\t & \t720\t & \t\\textbf{0.540}~~\\textbf{0.571}\t & \t0.543~~0.578\t & \t0.558~~0.609\t & \t4.891~~4.047\t & \t1.511~~0.966\t & \t0.563~~0.595\t & \t1.251~~0.933\t & \t4.243~~1.415\\\\\n\t & \t960\t & \t\\textbf{0.582}~~\\textbf{0.608}\t & \t0.594~~0.638\t & \t0.624~~0.645\t & \t7.019~~5.105\t & \t1.545~~1.006\t & \t0.657~~0.683\t & \t1.370~~0.982\t & \t6.901~~4.264\\\\\n\n\\midrule[1.0pt]\n\\multicolumn{2}{c|}{Count}           & {32}                 & {12}                          & {0}        & {0}        & {0}     & {6}           & {0}     & {0}      \\\\\n\\bottomrule[1.0pt]\n\n\\end{tabular}\n\\caption{Univariate long sequence time-series forecasting results on four datasets (five cases).}\n\\label{tab:exp.mainResults}\n\\end{table*}\n\n\\section{Experiment}\n\\label{sec:exp}\n\n\\subsection{Datasets}\nWe extensively perform experiments on four datasets, including 2 collected real-world datasets for LSTF and 2 public benchmark datasets.\n\n\\textbf{ETT} (Electricity Transformer Temperature)\\footnote{We collected the ETT dataset and published it at \\url{https://github.com/zhouhaoyi/ETDataset}.}:\nThe ETT is a crucial indicator in the electric power long-term deployment.\nWe collected 2-year data from two separated counties in China. To explore the granularity on the LSTF problem, we create separate datasets as \\{ETTh$_1$, ETTh$_2$\\} for 1-hour-level and ETTm$_1$ for 15-minute-level.\nEach data point consists of the target value \"oil temperature\" and 6 power load features.\nThe train/val/test is 12/4/4 months.\n\n\\textbf{ECL} (Electricity Consuming Load)\\footnote{ECL dataset was acquired at \\url{https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014}.}: It collects the electricity consumption (Kwh) of 321 clients. Due to the missing data \\cite{li2019enhancing}, we convert the dataset into hourly consumption of 2 years and set `MT\\_320' as the target value. The train/val/test is 15/3/4 months.\n\n\\textbf{Weather} \\footnote{Weather dataset was acquired at \\url{https://www.ncei.noaa.gov/data/local-climatological-data/}.}: This dataset contains local climatological data for nearly 1,600 U.S. locations, 4 years from 2010 to 2013, where data points are collected every 1 hour. Each data point consists of the target value ``wet bulb\" and 11 climate features. The train/val/test is 28/10/10 months.\n\n\\begin{table*}[t]\n\\centering\n\\fontsize{9pt}{9pt}\\selectfont\n\\begin{tabular}{c|c|cc|cc|cc|cc|cc|cc}\n\\toprule[1.0pt]\n\\multicolumn{2}{c}{Methods}     & \\multicolumn{2}{|c}{\\mn} & \\multicolumn{2}{|c}{\\mn$^{\\dag}$} & \\multicolumn{2}{|c}{LogTrans} & \\multicolumn{2}{|c}{Reformer} & \\multicolumn{2}{|c}{LSTMa} & \\multicolumn{2}{|c}{LSTnet}      \\\\\n\\midrule[0.5pt]\n\\multicolumn{2}{c|}{Metric}      & MSE                & MAE               & MSE                          & MAE              & MSE               & MAE      & MSE           & MAE          & MSE         & MAE         & MSE            & MAE            \\\\\n\\midrule[1.0pt]\n\\multirow{5}{*}{\\rotatebox{90}{ETTh$_1$}} & 24  & \\textbf{0.577}          & \\textbf{0.549}          & 0.620                   & 0.577                   & 0.686                   & 0.604                   & 0.991                   & 0.754                   & 0.650                   & 0.624                   & 1.293                   & 0.901                   \\\\\n                          & 48  & \\textbf{0.685}          & \\textbf{0.625}          & 0.692                   & 0.671                   & 0.766                   & 0.757                   & 1.313                   & 0.906                   & 0.702                   & 0.675                   & 1.456                   & 0.960                   \\\\\n                          & 168 & \\textbf{0.931}          & \\textbf{0.752}          & 0.947                   & 0.797                   & 1.002                   & 0.846                   & 1.824                   & 1.138                   & 1.212                   & 0.867                   & 1.997                   & 1.214                   \\\\\n                          & 336 & 1.128                   & 0.873                   & \\textbf{1.094}          & \\textbf{0.813}          & 1.362                   & 0.952                   & 2.117                   & 1.280                   & 1.424                   & 0.994                   & 2.655                   & 1.369                   \\\\\n                          & 720 & \\textbf{1.215}          & \\textbf{0.896}          & 1.241                   & 0.917                   & 1.397                   & 1.291                   & 2.415                   & 1.520                   & 1.960                   & 1.322                   & 2.143                   & 1.380                   \\\\\n\\midrule[0.5pt]\n\\multirow{5}{*}{\\rotatebox{90}{ETTh$_2$}} & 24  & \\textbf{0.720}          & \\textbf{0.665}          & 0.753                   & 0.727                   & 0.828                   & 0.750                   & 1.531                   & 1.613                   & 1.143                   & 0.813                   & 2.742                   & 1.457                   \\\\\n                          & 48  & \\textbf{1.457}          & \\textbf{1.001}          & 1.461                   & 1.077                   & 1.806                   & 1.034                   & 1.871                   & 1.735                   & 1.671                   & 1.221                   & 3.567                   & 1.687                   \\\\\n                          & 168 & 3.489                   & \\textbf{1.515}          & 3.485                   & 1.612                   & 4.070                   & 1.681                   & 4.660                   & 1.846                   & 4.117                   & 1.674                   & \\textbf{3.242}          & 2.513                   \\\\\n                          & 336 & 2.723                   & 1.340                   & 2.626                   & \\textbf{1.285}          & 3.875                   & 1.763                   & 4.028                   & 1.688                   & 3.434                   & 1.549                   & \\textbf{2.544}          & 2.591                   \\\\\n                          & 720 & \\textbf{3.467}          & \\textbf{1.473}          & 3.548                   & 1.495                   & 3.913                   & 1.552                   & 5.381                   & 2.015                   & 3.963                   & 1.788                   & 4.625                   & 3.709                   \\\\\n\\midrule[0.5pt]\n\\multirow{5}{*}{\\rotatebox{90}{ETTm$_1$}} & 24  & 0.323                   & \\textbf{0.369}          & \\textbf{0.306}          & 0.371                   & 0.419                   & 0.412                   & 0.724                   & 0.607                   & 0.621                   & 0.629                   & 1.968                   & 1.170                   \\\\\n                          & 48  & 0.494                   & 0.503                   & \\textbf{0.465}          & \\textbf{0.470}          & 0.507                   & 0.583                   & 1.098                   & 0.777                   & 1.392                   & 0.939                   & 1.999                   & 1.215                   \\\\\n                          & 96  & \\textbf{0.678}          & 0.614                   & 0.681                   & \\textbf{0.612}          & 0.768                   & 0.792                   & 1.433                   & 0.945                   & 1.339                   & 0.913                   & 2.762                   & 1.542                   \\\\\n                          & 288 & \\textbf{1.056}                   & \\textbf{0.786}          & 1.162                   & 0.879                   & 1.462                   & 1.320                   & 1.820                   & 1.094                   & 1.740                   & 1.124                   & 1.257          & 2.076                   \\\\\n                          & 672 & \\textbf{1.192}          & \\textbf{0.926}          & 1.231                   & 1.103                   & 1.669                   & 1.461                   & 2.187                   & 1.232                   & 2.736                   & 1.555                   & 1.917                   & 2.941                   \\\\\n\\midrule[0.5pt]\n\\multirow{5}{*}{\\rotatebox{90}{Weather}}  & 24  & \\textbf{0.335}          & \\textbf{0.381}          & 0.349                   & 0.397                   & 0.435                   & 0.477                   & 0.655                   & 0.583                   & 0.546                   & 0.570                   & 0.615                   & 0.545                   \\\\\n                          & 48  & 0.395                   & 0.459                   & \\textbf{0.386}          & \\textbf{0.433}          & 0.426                   & 0.495                   & 0.729                   & 0.666                   & 0.829                   & 0.677                   & 0.660                   & 0.589                   \\\\\n                          & 168 & \\textbf{0.608}          & \\textbf{0.567}          & 0.613                   & 0.582                   & 0.727                   & 0.671                   & 1.318                   & 0.855                   & 1.038                   & 0.835                   & 0.748                   & 0.647                   \\\\\n                          & 336 & \\textbf{0.702}          & \\textbf{0.620}          & 0.707                   & 0.634                   & 0.754                   & 0.670                   & 1.930                   & 1.167                   & 1.657                   & 1.059                   & 0.782                   & 0.683                   \\\\\n                          & 720 & \\textbf{0.831}         & \\textbf{0.731}          & 0.834                   & 0.741                   & 0.885                   & 0.773                   & 2.726                   & 1.575                   & 1.536                   & 1.109                   & 0.851                   & 0.757                   \\\\\n\\midrule[0.5pt]\n\\multirow{5}{*}{\\rotatebox{90}{ECL}}      & 48  & 0.344                   & \\textbf{0.393}          & \\textbf{0.334}          & 0.399                   & 0.355                   & 0.418                   & 1.404                   & 0.999                   & 0.486                   & 0.572                   & 0.369                   & 0.445                   \\\\\n                          & 168 & 0.368                   & 0.424          & \\textbf{0.353}          & \\textbf{0.420}                   & 0.368                   & 0.432                   & 1.515                   & 1.069                   & 0.574                   & 0.602                   & 0.394                   & 0.476                   \\\\\n                          & 336 & 0.381                   & \\textbf{0.431}          & 0.381                   & 0.439                   & \\textbf{0.373}          & 0.439                   & 1.601                   & 1.104                   & 0.886                   & 0.795                   & 0.419                   & 0.477                   \\\\\n                          & 720 & 0.406                   & 0.443                   & \\textbf{0.391}          & \\textbf{0.438}          & 0.409                   & 0.454                   & 2.009                   & 1.170                   & 1.676                   & 1.095                   & 0.556                   & 0.565                   \\\\\n                          & 960 & \\textbf{0.460}          & \\textbf{0.548}          & 0.492                   & 0.550                   & 0.477                   & 0.589                   & 2.141                   & 1.387                   & 1.591                   & 1.128                   & 0.605                   & 0.599                   \\\\\n\\midrule[1.0pt]\n\\multicolumn{2}{c}{Count}       & \\multicolumn{2}{|c}{33}                 & \\multicolumn{2}{|c}{14}                          & \\multicolumn{2}{|c}{1}        & \\multicolumn{2}{|c}{0}        & \\multicolumn{2}{|c}{0}     & \\multicolumn{2}{|c}{2}    \\\\\n\\bottomrule[1.0pt]\n\n\\end{tabular}\n\\caption{Multivariate long sequence time-series forecasting results on four datasets (five cases).}\n\\label{tab:exp.multivarResults}\n\\end{table*}\n\n\\subsection{Experimental Details}\nWe briefly summarize basics, and more information on network components and setups are given in Appendix E.\n\n\\textbf{Baselines:}\nWe have selected five time-series forecasting methods as comparison, including ARIMA \\cite{ariyo2014stock}, Prophet \\cite{taylor2018forecasting}, LSTMa \\cite{bahdanau2014neural}, LSTnet \\cite{lai2018modeling} and DeepAR \\cite{flunkert2017deepar}. To better explore the \\emph{ProbSparse} self-attention's performance in our proposed {\\mn}, we incorporate the canonical self-attention variant (\\mn$^{\\dag}$), the efficient variant Reformer \\cite{kitaev2019reformer} and the most related work LogSparse self-attention \\cite{li2019enhancing} in the experiments.\nThe details of network components are given in Appendix E.1. \n\n\\textbf{Hyper-parameter tuning:}\nWe conduct grid search over the hyper-parameters, and detailed ranges are given in Appendix E.3. {\\mn} contains a 3-layer stack and a 1-layer stack (1/4 input) in the encoder, and a 2-layer decoder. Our proposed methods are optimized with Adam optimizer, and its learning rate starts from $1e^{-4}$, decaying two times smaller every epoch. The total number of epochs is 8 with proper early stopping. We set the comparison methods as recommended, and the batch size is 32.\n\\textbf{Setup:}\nThe input of each dataset is zero-mean normalized.\nUnder the LSTF settings, we prolong the prediction windows size $L_y$ progressively, i.e., \\{1d, 2d, 7d, 14d, 30d, 40d\\} in \\{ETTh, ECL, Weather\\}, \\{6h, 12h, 24h, 72h, 168h\\} in ETTm.\n\\textbf{Metrics:} We use two evaluation metrics, including $\\textrm{MSE}=\\frac{1}{n}\\sum_{i=1}^{n}(\\mb{y}-\\hat{\\mb{y}})^2$ and $\\textrm{MAE}=\\frac{1}{n}\\sum_{i=1}^{n}|\\mb{y}-\\hat{\\mb{y}}|$ on each prediction window (averaging for multivariate prediction), and roll the whole set with $\\textrm{stride}=1$.\n\\textbf{Platform:} All the models were trained/tested on a single Nvidia V100 32GB GPU. The source code is available at \\url{https://github.com/zhouhaoyi/Informer2020}.\n\n\\begin{figure*}[t]\n\\centering\n    \\subfigure[Input length.]{\n    \\includegraphics[height=0.23\\linewidth]{figures/parameter_analysis_input_token.pdf}\n    \\label{fig:sensi_length}\n    }\n    \\hfill\n    \\subfigure[Sampling Factor.]{\n    \\includegraphics[height=0.23\\linewidth]{figures/parameter_analysis_factor_c.pdf}\n    \\label{fig:sensi_factor}\n    }\n    \\hfill\n    \\subfigure[Stacking Combination.]{\n    \\includegraphics[height=0.23\\linewidth]{figures/parameter_analysis_dependency.pdf}\n    \\label{fig:sensi_stacks}\n    }\n\\caption{The parameter sensitivity of three components in \\mn. \\qquad\\qquad~~~}\n\\label{fig:sensi}\n\\end{figure*}\n\n\\begin{table*}[t]\n\\begin{minipage}{0.58\\linewidth}\n\\fontsize{9pt}{9pt}\\selectfont\n\\begin{threeparttable}[b]\n\\begin{tabular}{cc|ccc|ccc}\n\\toprule[1.0pt]\n\\multicolumn{2}{l|}{Prediction length} & \\multicolumn{3}{c|}{336} & \\multicolumn{3}{c}{720}  \\\\\n\\midrule[0.5pt]\n\\multicolumn{2}{l|}{Encoder's input}   & 336    & 720    & 1440   & 720    & 1440   & 2880   \\\\\n\\midrule[1.0pt]\n\\multirow{2}{*}{\\mn}       & MSE       & 0.249 & 0.225 & 0.216 & 0.271 & 0.261 & 0.257 \\\\\n                           & MAE       & 0.393 & 0.384 & 0.376 & 0.435 & 0.431 & 0.422 \\\\\n\\midrule[0.5pt]\n    \\multirow{2}{*}{\\mn$^{\\dag}$}       & MSE       & 0.241 & 0.214 & - & 0.259 & - & - \\\\\n                           & MAE       & 0.383 & 0.371 & - & 0.423 & - & - \\\\\n\\midrule[0.5pt]\n\\multirow{2}{*}{LogTrans}       & MSE       & 0.263 & 0.231 & - & 0.273 & - & -      \\\\\n                           & MAE       & 0.418 & 0.398 & - & 0.463 & - & -      \\\\\n\\midrule[0.5pt]\n\\multirow{2}{*}{Reformer}       & MSE       & 1.875 & 1.865 & 1.861 & 2.243 & 2.174 & 2.113      \\\\\n                           & MAE       & 1.144 & 1.129 & 1.125 & 1.536 & 1.497 & 1.434      \\\\\n\\bottomrule[1.0pt]\n\\end{tabular}\n\\begin{tablenotes}\n\\item[1] \\mn$^{\\dag}$ uses the canonical self-attention mechanism.\n\\item[2] The `-' indicates failure for the out-of-memory.\n\\end{tablenotes}\n\\caption{Ablation study of the \\emph{ProbSparse} self-attention mechanism.}\n\\label{tab:exp.ablation.sparse}\n\\end{threeparttable}\n\\end{minipage}\n\\begin{minipage}{0.41\\linewidth}\n\\centering\n\\fontsize{9pt}{9pt}\\selectfont\n\\begin{threeparttable}\n\\begin{tabular}{l|c|c|c}\n\\toprule[1.0pt]\n\\multirow{3}{*}{Methods}  & \\multicolumn{2}{c|}{Training} & Testing     \\\\\n\\cmidrule{2-4}\n         & Time          & Memory  & Steps             \\\\\n\\midrule[1.0pt]\n\\mn   & $\\bigO (L \\log L)$   & $\\bigO (L \\log L)$ & 1                       \\\\\n\\midrule[0.5pt]\nTransformer   & $\\bigO (L^2)$   & $\\bigO (L^2)$ & $L$                       \\\\\n\\midrule[0.5pt]\nLogTrans & $\\bigO (L \\log L)$   & $\\bigO (L^2)$ & 1$^{\\star}$                      \\\\\n\\midrule[0.5pt]\nReformer & $\\bigO (L \\log L)$   & $\\bigO (L \\log L)$ & $L$          \\\\\n\\midrule[0.5pt]\nLSTM     & $\\bigO (L)$   & $\\bigO (L)$ & $L$                     \\\\\n\\bottomrule[1.0pt]\n\\end{tabular}\n\\begin{tablenotes}\n\\item[1] The LSTnet is hard to present in a closed form.\n\\item[2] The ${\\star}$ denotes applying our proposed decoder. \n\\end{tablenotes}\n\\end{threeparttable}\n\\captionsetup{type=table} %% tell latex to change to table\n\\caption{$L$-related computation statics of each layer.}\n\\label{tab:exp.computation.summary}\n\\end{minipage}\n\\end{table*}\n\n\\subsection{Results and Analysis}\n\\label{sec:exp.overallresult}\nTable \\ref{tab:exp.mainResults} and Table \\ref{tab:exp.multivarResults} summarize the univariate/multivariate evaluation results of all the methods on 4 datasets. We gradually prolong the prediction horizon as a higher requirement of prediction capacity, where the LSTF problem setting is precisely controlled to be tractable on one single GPU for each method.\nThe best results are highlighted in boldface.\n\n\\textbf{Univariate Time-series Forecasting}\nUnder this setting, each method attains predictions as a single variable over time series. From Table \\ref{tab:exp.mainResults}, we can observe that:\n\\textbf{(1)} The proposed model {\\mn} significantly improves the inference performance (wining-counts in the last column) across all datasets, and their predict error rises smoothly and slowly within the growing prediction horizon, which demonstrates the success of {\\mn} in enhancing the prediction capacity in the LSTF problem.\n\\textbf{(2)} The {\\mn} beats its canonical degradation \\mn$^{\\dag}$ mostly in wining-counts, i.e., 32$>$12, which supports the query sparsity assumption in providing a comparable attention feature map. Our proposed method also out-performs the most related work LogTrans and Reformer. We note that the Reformer keeps dynamic decoding and performs poorly in LSTF, while other methods benefit from the generative style decoder as nonautoregressive predictors.\n\\textbf{(3)} The {\\mn} model shows significantly better results than recurrent neural networks LSTMa. Our method has a MSE decrease of 26.8\\% (at 168), 52.4\\% (at 336) and 60.1\\% (at 720).\nThis reveals a shorter network path in the self-attention mechanism acquires better prediction capacity than the RNN-based models.\n\\textbf{(4)} The proposed method outperforms DeepAR, ARIMA and Prophet on MSE by decreasing 49.3\\% (at 168), 61.1\\% (at 336), and 65.1\\% (at 720) in average.\nOn the ECL dataset, DeepAR performs better on shorter horizons ($\\leq 336$), and our method surpasses on longer horizons. We attribute this to a specific example, in which the effectiveness of prediction capacity is reflected with the problem scalability.\n\n\\textbf{Multivariate Time-series Forecasting}\nWithin this setting, some univariate methods are inappropriate, and LSTnet is the state-of-art baseline. On the contrary, our proposed {\\mn} is easy to change from univariate prediction to multivariate one by adjusting the final FCN layer.\nFrom Table \\ref{tab:exp.multivarResults}, we observe that: \\textbf{(1)} The proposed model {\\mn} greatly outperforms other methods and the findings 1 \\& 2 in the univariate settings still hold for the multivariate time-series.\n\\textbf{(2)} The {\\mn} model shows better results than RNN-based LSTMa and CNN-based LSTnet, and the MSE decreases 26.6\\% (at 168), 28.2\\% (at 336), 34.3\\% (at 720) in average. Compared with the univariate results, the overwhelming performance is reduced, and such phenomena can be caused by the anisotropy of feature dimensions' prediction capacity. It is beyond the scope of this paper, and we will explore it in the future work.\n\n\\textbf{LSTF with Granularity Consideration}\nWe perform an additional comparison to explore the performance with various granularities. The sequences \\{96, 288, 672\\} of ETTm$_1$ (minutes-level) are aligned with \\{24, 48, 168\\} of ETTh$_1$ (hour-level). The {\\mn} outperforms other baselines even if the sequences are at different granularity levels.\n\n\\subsection{Parameter Sensitivity}\nWe perform the sensitivity analysis of the proposed {\\mn} model on ETTh1 under the univariate setting.\n\\textbf{Input Length:} In Fig.(\\ref{fig:sensi}a), when predicting short sequences (like 48), initially increasing input length of encoder/decoder degrades performance, but further increasing causes the MSE to drop because it brings repeat short-term patterns. However, the MSE gets lower with longer inputs in predicting long sequences (like 168). Because the longer encoder input may contain more dependencies, and the longer decoder token has rich local information.\n\\textbf{Sampling Factor:} The sampling factor controls the information bandwidth of \\emph{ProbSparse} self-attention in Eq.(\\ref{eq:method.probsparse}). We start from the small factor (=3) to large ones, and the general performance increases a little and stabilizes at last in Fig.(\\ref{fig:sensi}b). It verifies our query sparsity assumption that there are redundant dot-product pairs in the self-attention mechanism. We set the sample factor $c=5$ (the red line) in practice.\n\\textbf{The Combination of Layer Stacking:} The replica of Layers is complementary for the self-attention distilling, and we investigate each stack \\{L, L/2, L/4\\}'s behavior in Fig.(\\ref{fig:sensi}c). The longer stack is more sensitive to the inputs, partly due to receiving more long-term information. Our method's selection (the red line), i.e., joining L and L/4, is the most robust strategy.\n\n\\subsection{Ablation Study: How well {\\mn} works?}\nWe also conducted additional experiments on ETTh$_1$ with ablation consideration.\n\n\\begin{table*}[t]\n\\centering\n\\fontsize{9pt}{9pt}\\selectfont\n\\begin{threeparttable}\n\\begin{tabular}{cc|ccccc|ccccc}\n\\toprule[1.0pt]\n\\multicolumn{2}{l|}{Prediction length}      & \\multicolumn{5}{c|}{336}           & \\multicolumn{5}{c}{480} \\\\\n\\midrule[0.5pt]\n\\multicolumn{2}{l|}{Encoder's input} & 336 & 480 & 720 & 960 & 1200 & 336 & 480 & 720 & 960 & 1200\\\\\n\\midrule[1.0pt]\n\\multirow{2}{*}{\\mn$^{\\dag}$} & MSE & 0.249 & 0.208 & 0.225 & 0.199 & 0.186 & 0.197 & 0.243 & 0.213 & 0.192 & 0.174\\\\\n                              & MAE & 0.393 & 0.385 & 0.384 & 0.371 & 0.365 & 0.388 & 0.392 & 0.383 & 0.377 & 0.362 \\\\\n\\midrule[0.5pt]\n\\multirow{2}{*}{\\mn$^{\\ddag}$} & MSE & 0.229 & 0.215 & 0.204 & - & - & 0.224 & 0.208 & 0.197 & - & -  \\\\\n                               & MAE & 0.391 & 0.387 & 0.377 & - & - & 0.381 & 0.376 & 0.370 & - & -\\\\\n\\bottomrule[1.0pt]\n\\end{tabular}\n\\begin{tablenotes}\n\\item[1] \\mn$^{\\ddag}$ removes the self-attention distilling from \\mn$^{\\dag}$.\n\\item[2] The `-' indicates failure for the out-of-memory.\n\\end{tablenotes}\n\\end{threeparttable}\n\\caption{Ablation study of the self-attention distilling.}\n\\label{tab:exp.ablation.distilling}\n\\end{table*}\n\n\\begin{table*}[t]\n\\centering\n\\fontsize{9pt}{9pt}\\selectfont\n\\begin{threeparttable}\n\\begin{tabular}{cc|ccccc|ccccc}\n\\toprule[1.0pt]\n\\multicolumn{2}{l|}{Prediction length} & \\multicolumn{5}{c|}{336} & \\multicolumn{5}{c}{480}  \\\\\n\\midrule[0.5pt]\n\\multicolumn{2}{l|}{Prediction offset} & +0 & +12 & +24 & +48 & +72 & +0 & +48 & +96 & +144 & +168 \\\\ \\midrule[1.0pt]\n\\multirow{2}{*}{\\mn$^{\\ddag}$} & MSE & 0.207 & 0.209 & 0.211 & 0.211 & 0.216 & 0.198 & 0.203 & 0.203 & 0.208 & 0.208 \\\\\n                     & MAE & 0.385 & 0.387 & 0.391 & 0.393 & 0.397 & 0.390 & 0.392 & 0.393 & 0.401 & 0.403\\\\\n\\midrule[0.5pt]\n\\multirow{2}{*}{\\mn$^{\\S}$} & MSE & 0.201 & - & - & - & - & 0.392 & - & - & - & - \\\\\n                     & MAE & 0.393 & - & - & - & - & 0.484 & - & - & - & - \\\\\n\\bottomrule[1.0pt]\n\\end{tabular}\n\\begin{tablenotes}\n\\item[1] \\mn$^{\\S}$ replaces our decoder with dynamic decoding one in \\mn$^{\\ddag}$.\n\\item[2] The `-' indicates failure for the unacceptable metric results.\n\\end{tablenotes}\n\\end{threeparttable}\n\\caption{Ablation study of the generative style decoder.}\n\\label{tab:exp.ablation.decoder}\n\\vspace{-1 ex}\n\\end{table*}\n\n\\textbf{The performance of \\emph{ProbSparse} self-attention mechanism}\nIn the overall results Table \\ref{tab:exp.mainResults} \\& \\ref{tab:exp.multivarResults}, we limited the problem setting to make the memory usage feasible for the canonical self-attention.\nIn this study, we compare our methods with LogTrans and Reformer, and thoroughly explore their extreme performance.\nTo isolate the memory efficient problem, we first reduce settings as \\{batch size=8, heads=8, dim=64\\}, and maintain other setups in the univariate case. %We perform a comparison with LogTrans and Reformer accordingly.\nIn Table \\ref{tab:exp.ablation.sparse}, the \\emph{ProbSparse} self-attention shows better performance than the counterparts. The LogTrans gets OOM in extreme cases because its public implementation is the mask of the full-attention, which still has $\\bigO(L^2)$ memory usage. Our proposed \\emph{ProbSparse} self-attention avoids this from the simplicity brought by the query sparsity assumption in Eq.(\\ref{eq.method.measure2}), referring to the pseudo-code in Appendix E.2, and reaches smaller memory usage.\n\n\\textbf{The performance of self-attention distilling}\nIn this study, we use \\mn$^{\\dag}$ as the benchmark to eliminate additional effects of \\emph{ProbSparse} self-attention.\nThe other experimental setup is aligned with the settings of univariate Time-series.\nFrom Table \\ref{tab:exp.ablation.distilling}, \\mn$^{\\dag}$ has fulfilled all the experiments and achieves better performance after taking advantage of long sequence inputs. The comparison method \\mn$^{\\ddag}$ removes the distilling operation and reaches OOM with longer inputs ($> 720$).\nRegarding the benefits of long sequence inputs in the LSTF problem, we conclude that the self-attention distilling is worth adopting, especially when a longer prediction is required.\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{figures/runtime_new.pdf}\n  \\caption{The total runtime of training/testing phase.}\n  \\label{fig:exp.runtime_new}\n  \\vspace{-2 ex}\n\\end{figure}\n\n\\textbf{The performance of generative style decoder}\nIn this study, we testify the potential value of our decoder in acquiring a ``generative'' results. Unlike the existing methods, the labels and outputs are forced to be aligned in the training and inference, our proposed decoder's predicting relies solely on the time stamp, which can predict with offsets. From Table \\ref{tab:exp.ablation.decoder}, we can see that the general prediction performance of \\mn$^{\\ddag}$ resists with the offset increasing, while the counterpart fails for the dynamic decoding. It proves the decoder's ability to capture individual long-range dependency between arbitrary outputs and avoid error accumulation.\n\n\\subsection{Computation Efficiency}\n\\label{sec:Exp.CompuationEfficiency}\nWith the multivariate setting and all the methods' current finest implement, we perform a rigorous runtime comparison in Fig.(\\ref{fig:exp.runtime_new}). During the training phase, the {\\mn} (red line) achieves the best training efficiency among Transformer-based methods. During the testing phase, our methods are much faster than others with the generative style decoding. The comparisons of theoretical time complexity and memory usage are summarized in Table \\ref{tab:exp.computation.summary}. The performance of {\\mn} is aligned with the runtime experiments. Note that the LogTrans focus on improving the self-attention mechanism, and we apply our proposed decoder in LogTrans for a fair comparison (the $\\star$ in Table \\ref{tab:exp.computation.summary}).\n\n\\section{Conclusion}\nIn this paper, we studied the long-sequence time-series forecasting problem and  proposed {\\mn} to predict long sequences. Specifically, we designed the \\emph{ProbSparse} self-attention mechanism and distilling operation to handle the challenges of quadratic time complexity and quadratic memory usage in vanilla Transformer. Also, the carefully designed generative decoder alleviates the limitation of traditional encoder-decoder architecture. The experiments on real-world data demonstrated the effectiveness of {\\mn} for enhancing the prediction capacity in LSTF problem.\n\n\\begin{appendices}\n\n\\section{Related Work}\n\\label{sec:appendix.relatedwork}\nWe provide a literature review of the long sequence time-series forecasting (LSTF) problem below.\n\n\\textbf{Time-series Forecasting} Existing methods for time-series forecasting can be roughly grouped into two categories: classical models and deep learning based methods.\nClassical time-series models serve as a reliable workhorse for time-series forecasting, with appealing properties such as interpretability and theoretical guarantees \\cite{box2015time,ray1990time}.\nModern extensions include the support for missing data \\cite{seeger2017approximate} and multiple data types \\cite{seeger2016bayesian}.\nDeep learning based methods mainly develop sequence to sequence prediction paradigm by using RNN and their variants, achieving ground-breaking performance \\cite{hochreiter1997long,li2017graph,yu2017long}.\nDespite the substantial progress, existing algorithms still fail to predict long sequence time series with satisfying accuracy. Typical state-of-the-art approaches \\cite{seeger2017approximate,seeger2016bayesian}, especially deep-learning methods \\cite{yu2017long,conf/ijcai/QinSCCJC17,flunkert2017deepar,mukherjee2018armdn,wen2017multi}, remain as a sequence to sequence prediction paradigm with step-by-step process, which have the following limitations: (i) Even though they may achieve accurate prediction for one step forward, they often suffer from accumulated error from the dynamic decoding, resulting in the large errors for LSTF problem \\cite{journals/corr/abs-1904-07464,conf/ijcai/QinSCCJC17}. The prediction accuracy decays along with the increase of the predicted sequence length. (ii) Due to the problem of vanishing gradient and memory constraint \\cite{sutskever2014sequence}, most existing methods cannot learn from the past behavior of the whole history of the time-series. In our work, the {\\mn} is designed to address this two limitations.\n\n\\textbf{Long sequence input problem} From the above discussion, we refer to the second limitation as to the long sequence time-series input (LSTI) problem. We will explore related works and draw a comparison between our LSTF problem. The researchers truncate / summarize / sample the input sequence to handle a very long sequence in practice, but valuable data may be lost in making accurate predictions. Instead of modifying inputs, Truncated BPTT \\cite{aicher2019adaptively} only uses last time steps to estimate the gradients in weight updates, and Auxiliary Losses \\cite{trinh2018learning} enhance the gradients flow by adding auxiliary gradients. Other attempts includes Recurrent Highway Networks \\cite{zilly2017recurrent} and Bootstrapping Regularizer \\cite{cao2019better}. Theses methods try to improve the gradient flows in the recurrent network's long path, but the performance is limited with the sequence length growing in the LSTI problem. CNN-based methods \\cite{stoller2019seq,bai2018convolutional} use the convolutional filter to capture the long term dependency, and their receptive fields grow exponentially with the stacking of layers, which hurts the sequence alignment. In the LSTI problem, the main task is to enhance the model's capacity of receiving long sequence inputs and extract the long-range dependency from these inputs. But the LSTF problem seeks to enhance the model's prediction capacity of forecasting long sequence outputs, which requires establishing the long-range dependency between outputs and inputs. Thus, the above methods are not feasible for LSTF directly.\n\n\\textbf{Attention model} Bahdanau et al. firstly proposed the addictive attention \\cite{bahdanau2014neural} to improve the word alignment of the encoder-decoder architecture in the translation task. Then, its variant \\cite{DBLP:conf/emnlp/LuongPM15} has proposed the widely used location, general, and dot-product attention.\nThe popular self-attention based Transformer \\cite{vaswani2017attention} has recently been proposed as new thinking of sequence modeling and has achieved great success, especially in the NLP field. The ability of better sequence alignment has been validated by applying it to translation, speech, music, and image generation. In our work, the {\\mn} takes advantage of its sequence alignment ability and makes it amenable to the LSTF problem.\n\n\\textbf{Transformer-based time-series model} The most related works \\cite{song2018attend,ma2019cdsa,li2019enhancing} all start from a trail on applying Transformer in time-series data and fail in LSTF forecasting as they use the vanilla Transformer. And some other works \\cite{child2019generating, li2019enhancing} noticed the sparsity in self-attention mechanism and we have discussed them in the main context.\n\n\\section{The Uniform Input Representation}\n\\label{sec:appendix.input}\nThe RNN models \\cite{schuster1997bidirectional,hochreiter1997long,chung2014empirical,sutskever2014sequence,conf/ijcai/QinSCCJC17,chang2018memory} capture the time-series pattern by the recurrent structure itself and barely relies on time stamps. The vanilla transformer \\cite{vaswani2017attention, devlin2018bert} uses point-wise self-attention mechanism and the time stamps serve as local positional context. However, in the LSTF problem, the ability to capture long-range independence requires global information like hierarchical time stamps (week, month and year) and agnostic time stamps (holidays, events). These are hardly leveraged in canonical self-attention and consequent query-key mismatches between the encoder and decoder bring underlying degradation on the forecasting performance. We propose a uniform input representation to mitigate the issue, the Fig.(\\ref{fig:method.embed}) gives an intuitive overview.\n\nAssuming we have $t$-th sequence input $\\mathcal{X}^t$ and $p$ types of global time stamps and the feature dimension after input representation is $d_{\\rm{model}}$. We firstly preserve the local context by using a fixed position embedding:\n\\begin{equation}\n\\begin{aligned}\n   \\textrm{PE}_{(pos, 2j)} = \\sin(pos/(2L_x)^{2j/d_{\\rm{model}}}) \\\\\n   \\textrm{PE}_{(pos, 2j+1)} = \\cos(pos/(2L_x)^{2j/d_{\\rm{model}}})\n\\end{aligned}\\quad,\n\\end{equation}\nwhere $j\\in\\{1, \\dots, \\lfloor {d_{\\rm{model}}}/{2} \\rfloor \\}$. Each global time stamp is employed by a learnable stamp embeddings $\\textrm{SE}_{(pos)}$ with limited vocab size (up to 60, namely taking minutes as the finest granularity). That is, the self-attention's similarity computation can have access to global context and the computation consuming is affordable on long inputs. To align the dimension, we project the scalar context $\\mb{x}^t_{i}$ into $d_{\\rm{model}}$-dim vector $\\mb{u}^t_{i}$ with 1-D convolutional filters (kernel width=3, stride=1). Thus, we have the feeding vector\n\\begin{small}\n\\begin{equation}\\label{eq:method.input.sum}\n  \\mathcal{X}^t_{\\textrm{feed}[i]} = \\alpha \\mb{u}^t_i + \\textrm{PE}_{(L_x \\times (t-1)+i,~)}  \\!+\\! \\sum_{p} [\\textrm{SE}_{(L_x \\times (t-1)+i)}]_p \\quad,\n\\end{equation}\n\\end{small}\nwhere $i\\in\\{1, \\dots, L_x\\}$, and $\\alpha$ is the factor balancing the magnitude between the scalar projection and local/global embeddings. We recommend $\\alpha=1$ if the sequence input has been normalized.\n\n\\begin{figure}[h]\n  \\centering\n  \\includegraphics[width=0.85\\linewidth]{figures/Input_Rep.pdf}\n  \\caption{The input representation of \\mn. The inputs's embedding consists of three separate parts, a scalar projection, the local time stamp (Position) and global time stamp embeddings (Minutes, Hours, Week, Month, Holiday etc.).}\n  \\label{fig:method.embed}\n\\end{figure}\n\n\\section{The long tail distribution in self-attention feature map}\nWe have performed the vanilla Transformer on the \\textbf{ETTh$_1$} dataset to investigate the distribution of self-attention feature map. We select the attention score of \\{Head1,Head7\\} @ Layer1. The blue line in Fig.(\\ref{fig:method.self-attention.score}) forms a long tail distribution, i.e. a few dot-product pairs contribute to the major attention and others can be ignored.\n\\label{sec:appendix.longtail}\n\\begin{figure}[ht]\n  \\centering\n  \\includegraphics[width=0.95\\linewidth]{figures/self-attention_score_2.pdf}\n  \\caption{The Softmax scores in the self-attention from a 4-layer canonical Transformer trained on \\textbf{ETTh$_1$} dataset.}\n  \\label{fig:method.self-attention.score}\n\\end{figure}\n\n\\section{Details of the proof}\n\\subsection{Proof of Lemma 1}\n\\label{sec:appendix.proof1}\n\\begin{proof}\nFor the individual $\\mb{q}_i$, we can relax the discrete keys into the continuous $d$-dimensional variable, i.e. vector $\\mb{k}_j$. The query sparsity measurement is defined as the $M(\\mb{q}_i, \\mb{K}) = \\ln \\sum_{j=1}^{L_K} e^{{\\mb{q}_i\\mb{k}_j^{\\top}}/{\\sqrt{d}}} - \\frac{1}{L_K} \\sum_{j=1}^{L_K} ({\\mb{q}_i\\mb{k}_j^{\\top}}/{\\sqrt{d}})$.\n\nFirstly, we look into the left part of the inequality. For each query $\\mb{q}_i$,  the first term of the $M(\\mb{q}_i, \\mb{K})$ becomes the log-sum-exp of the inner-product of a fixed query $\\mb{q}_i$ and all the keys , and we can define $f_i(\\mb{K})=\\ln \\sum_{j=1}^{L_K} e^{{\\mb{q}_i\\mb{k}_j^{\\top}}/{\\sqrt{d}}}$. From the Eq.(2) in the Log-sum-exp network\\cite{DBLP:journals/corr/abs-1806-07850} and the further analysis, the function $f_i(\\mb{K})$ is convex. Moreover, $f_i(\\mb{K})$ add a linear combination of $\\mb{k}_j$ makes the $M(\\mb{q}_i, \\mb{K})$ to be the convex function for a fixed query. Then we can take the derivation of the measurement with respect to the individual vector $\\mb{k}_j$ as $\\frac{\\partial M(\\mb{q}_i, \\mb{K})}{\\partial \\mb{k}_j} = \\frac{e^{\\mb{q}_i\\mb{k}_j^{\\top}/{\\sqrt{d}}}}{\\sum_{j=1}^{L_K} e^{\\mb{q}_i\\mb{k}_j^{\\top}/{\\sqrt{d}}}} \\cdot \\frac{\\mb{q}_i}{\\sqrt{d}} - \\frac{1}{L_K} \\cdot \\frac{\\mb{q}_i}{\\sqrt{d}}$.\n\\iffalse\n\\begin{equation}\n\\nonumber\n    \\frac{\\partial M(\\mb{q}_i, \\mb{K})}{\\partial \\mb{k}_j} = \\frac{e^{\\mb{q}_i\\mb{k}_j^{\\top}/{\\sqrt{d}}}}{\\sum_{j=1}^{L_K} e^{\\mb{q}_i\\mb{k}_j^{\\top}/{\\sqrt{d}}}} \\cdot \\frac{\\mb{q}_i}{\\sqrt{d}} - \\frac{1}{L_K} \\cdot \\frac{\\mb{q}_i}{\\sqrt{d}} \\qquad,\n\\end{equation}\n\\fi\nTo reach the minimum value, we let $\\vec{\\nabla} M(\\mb{q}_i) = \\vec{0}$ and the following condition is acquired as $\\mb{q}_i\\mb{k}_1^{\\top} + \\ln L_K = \\cdots = \\mb{q}_i\\mb{k}_j^{\\top} + \\ln L_K = \\cdots = \\ln \\sum_{j=1}^{L_K} e^{\\mb{q}_i\\mb{k}_j^{\\top}}$.\n\\iffalse\n\\begin{equation}\n\\nonumber\n    \\mb{q}_i\\mb{k}_1^{\\top} + \\ln L_K = \\cdots = \\mb{q}_i\\mb{k}_j^{\\top} + \\ln L_K = \\cdots = \\ln \\sum_{j=1}^{L_K} e^{\\mb{q}_i\\mb{k}_j^{\\top}} \\quad.\n\\end{equation}\n\\fi\nNaturally, it requires $\\mb{k}_1=\\mb{k}_2=\\cdots=\\mb{k}_{L_K}$, and we have the measurement's minimum as $\\ln L_K$, i.e.\n\\begin{equation}\n\\label{eq.appendix.proof1.left}\n    M(\\mb{q}_i, \\mb{K}) \\geq \\ln L_K \\qquad.\n\\end{equation}\n\nSecondly, we look into the right part of the inequality. If we select the largest inner-product $\\max_{j}\\{{\\mb{q}_i \\mb{k}_j^{\\top}}/{\\sqrt{d}}\\}$, it is easy that\n\n\\begin{small}\n\\begin{equation}\n\\begin{aligned}\n    M(\\mb{q}_i, \\mb{K}) &= \\ln \\sum_{j=1}^{L_K} e^{\\frac{{\\mb{q}_i\\mb{k}_j^{\\top}}}{{\\sqrt{d}}}} \\!-\\!\\frac{1}{L_K} \\sum_{j=1}^{L_K} (\\frac{{\\mb{q}_i\\mb{k}_j^{\\top}}}{\\sqrt{d}}) \\\\\n    & \\leq \\ln ( L_K \\!\\cdot\\! \\max_{j}\\{\\frac{{\\mb{q}_i \\mb{k}_j^{\\top}}}{\\sqrt{d}}\\} )\\!-\\!\\frac{1}{L_K} \\sum_{j=1}^{L_K} (\\frac{\\mb{q}_i\\mb{k}_j^{\\top}}{\\sqrt{d}}) \\\\\n    & = \\ln L_K \\!+\\! \\max_{j}\\{\\frac{\\mb{q}_i \\mb{k}_j^{\\top}}{\\sqrt{d}}\\}\\!-\\!\n    \\frac{1}{L_K} \\sum_{j=1}^{L_K} (\\frac{\\mb{q}_i\\mb{k}_j^{\\top}}{\\sqrt{d}})\n\\end{aligned}.\n\\end{equation}\n\\end{small}\n\nCombine the Eq.(14) and Eq.(15), we have the results of Lemma 1. When the key set is the same with the query set, the above discussion also holds.\n\\end{proof}\n\n\\begin{proposition}\nAssuming $\\mb{k}_j \\sim \\mathcal{N} (\\mu, \\Sigma)$ and we let $\\mb{qk}_{i}$ denote set $\\{({\\mb{q}_i\\mb{k}_j^{\\top}})/{\\sqrt{d}} ~|~ j=1,\\ldots,L_K\\}$, then $\\forall M_{m} = \\max_{i} M(\\mb{q}_i, \\mb{K})$ there exist $\\kappa > 0$ such that: in the interval $\\forall \\mb{q}_1, \\mb{q}_2 \\in \\{ \\mb{q} | M(\\mb{q}, \\mb{K}) \\in [ M_{m},  M_{m} - \\kappa)\\}$, if $\\overline{M} (\\mb{q}_1, \\mb{K}) > \\overline{M} (\\mb{q}_2, \\mb{K})$ and~$\\mathrm{Var}(\\mb{qk}_{1}) > \\mathrm{Var}(\\mb{qk}_{2})$, we have high probability that $M(\\mb{q}_1, \\mb{K}) > M(\\mb{q}_2, \\mb{K})$.\n\\end{proposition}\n\n\\subsection{Proof of Proposition 1}\n\\label{sec:appendix.proof2}\n\\begin{proof}\nTo make the further discussion simplify, we can note $a_{i,j}={q_i k_j^T}/{\\sqrt{d}}$, thus define the array $A_i = [a_{i,1},\\cdots,a_{i,L_k}]$. Moreover, we denote $\\frac{1}{L_{K}} \\sum_{j=1}^{L_{K}} ({\\mathbf{q}_{i} \\mathbf{k}_{j}^{\\top}}/{\\sqrt{d}})=\\mathrm{mean}(A_i)$, then we can denote $\\bar{M}\\left(\\mathbf{q}_{i}, \\mathbf{K}\\right)=\\max(A_i)-\\mathrm{mean}(A_i)$, $i=1,2$.\n\nAs for $M\\left(\\mathbf{q}_{i}, \\mathbf{K}\\right)$, we denote each component $a_{i,j}= \\mathrm{mean}(A_i) + \\Delta a_{i,j},j=1,\\cdots,L_k$, then we have the following:\n\\begin{equation}\n\\nonumber\n\\begin{aligned}\nM\\left(\\mathbf{q}_{i}, \\mathbf{K}\\right)=&\\ln \\sum_{j=1}^{L_{K}} e^{{\\mathbf{q}_{i} \\mathbf{k}_{j}^{\\top}}/{\\sqrt{d}}}-\\frac{1}{L_{K}}\n\\sum_{j=1}^{L_{K}} ({\\mathbf{q}_{i} \\mathbf{k}_{j}^{\\top}}/{\\sqrt{d}})\\\\\n=&\\ln(\\Sigma_{j=1}^{L_k} e^{\\mathrm{mean}(A_i)}e^{\\Delta a_{i,j}})-\\mathrm{mean}(A_i)\\\\\n=&\\ln(e^{\\mathrm{mean}(A_i)} \\Sigma_{j=1}^{L_k} e^{\\Delta a_{i,j}})-\\mathrm{mean}(A_i)\\\\\n=&\\ln(\\Sigma_{j=1}^{L_k}e^{\\Delta a_{i,j}})\n\\end{aligned},\n\\end{equation}\nand it is easy to find $\\Sigma_{j=1}^{L_k}\\Delta a_{i,j} = 0$.\n\nWe define the function $ES(A_i)=\\Sigma_{j=1}^{L_k}\\exp(\\Delta a_{i,j})$, equivalently defines $A_i = [\\Delta a_{i,1},\\cdots,\\Delta a_{i,L_k}]$, and immediately our proposition can be written as the equivalent form:\n\nFor $\\forall A_1,A_2$, if\n\\begin{enumerate}[nosep,  leftmargin=0.85cm]\n\\item $\\max(A_1)-\\mathrm{mean}(A_1)\\geq \\max(A_2)-\\mathrm{mean}(A_2)$\n\\item $\\mathrm{Var}(A_1) > \\mathrm{Var}(A_2)$\n\\end{enumerate}\nThen we rephrase the original conclusion into more general form that $ES(A_1)>ES(A_2)$ with high probability, and the probability have positive correlation with $\\mathrm{Var}(A_1)-\\mathrm{Var}(A_2)$.\\\\\n\nFurthermore, we consider a fine case, $\\forall M_{m} = \\max_{i} M(\\mb{q}_i, \\mb{K})$ there exist $\\kappa > 0$ such that in that interval $\\forall \\mb{q}_i, \\mb{q}_j \\in \\{ \\mb{q} | M(\\mb{q}, \\mb{K}) \\in [ M_{m},  M_{m} - \\kappa)\\}$ if $\\max(A_1)-\\mathrm{mean}(A_1)\\geq \\max(A_2)-\\mathrm{mean}(A_2)$ and~$\\mathrm{Var}(A_1) > \\mathrm{Var}(A_2)$, we have high probability that $M(\\mb{q}_1, \\mb{K}) > M(\\mb{q}_2, \\mb{K})$,which is equivalent to $ES(A_1) > ES(A_2)$.\n\nIn the original proposition, $\\mathbf{k}_{\\mathbf{j}} \\sim \\mathcal{N}(\\mu, \\Sigma)$ follows multivariate Gaussian distribution, which means that $k_1,\\cdots,k_n$ are I.I.D Gaussian distribution, thus defined by the Wiener-khinchin law of large Numbers, $a_{i,j}={q_i k_j^T}/{\\sqrt{d}}$ is one-dimension Gaussian distribution with the expectation of 0 if $n\\rightarrow \\infty$. So back to our definition, $\\Delta a_{1,m} \\sim N(0,\\sigma_1^2),\\Delta a_{2,m} \\sim N(0,\\sigma_2^2),\\forall m\\in 1,\\cdots,L_k$, and our proposition is equivalent to a lognormal-distribution sum problem.\n \nA lognormal-distribution sum problem is equivalent to approximating the distribution of $ES(A_1)$ accurately, whose history is well-introduced in the articles \\cite{dufresne2008sums},\\cite{vargasguzman2005change}. Approximating lognormality of sums of lognormals is a well-known rule of thumb, and no general PDF function can be given for the sums of lognormals. However, \\cite{romeo2003broad} and \\cite{hcine2015approximation} pointed out that in most cases, sums of lognormals is still a lognormal distribution, and by applying central limits theorem in \\cite{beaulieu2011extended}, we can have a good approximation that $ES(A_1)$ is a lognormal distribution, and we have $E(ES(A_1))=ne^{\\frac{\\sigma_1^2}{2}}$, $\\mathrm{Var}(ES(A_1))=ne^{\\sigma_1^2}(e^{\\sigma_1^2}-1)$. Equally, $E(ES(A_2))=ne^{\\frac{\\sigma_2^2}{2}}$, $\\mathrm{Var}(ES(A_2))=ne^{\\sigma_2^2}(e^{\\sigma_2^2}-1)$.\n\nWe denote $B_1=ES(A_1),B_2=ES(A_2)$, and the probability $Pr(B_1-B_2>0)$ is the final result of our proposition in general conditions, with $\\sigma_1^2>\\sigma_2^2$ WLOG. The difference of lognormals is still a hard problem to solve.\n\nBy using the theorem given in\\cite{lo2012sum}, which gives a general approximation of the probability distribution on the sums and difference for the lognormal distribution. Namely $S_1$ and $S_2$ are two lognormal stochastic variables obeying the stochastic differential equations$\\frac{d S_{i}}{S_{i}}=\\sigma_{i} d Z_{i}$, $i=1,2,$ in which $dZ_{1,2}$ presents a standard Weiner process associated with $S_{1,2}$ respectively, and $\\sigma_{i}^{2}=\\operatorname{Var}\\left(\\ln S_{i}\\right)$, $S^{\\pm} \\equiv S_{1} \\pm S_{2}$,$S_0^{\\pm} \\equiv S_{10} \\pm S_{20}$. As for the joint probability distribution function $P\\left(S_{1}, S_{2}, t ; S_{10}, S_{20}, t_{0}\\right)$, the value of $S_1$ and $S_2$ at time $t>t_0$ are provided by their initial value $S_{10}$ and $S_{20}$ at initial time $t_0$. The Weiner process above is equivalent to the lognormal distribution\\cite{weiner1984meaning}, and the conclusion below is written in general form containing both the sum and difference of lognormal distribution approximation denoting $\\pm$ for sum $+$ and difference $-$ respectively.\n \nIn boundary condition \n \\begin{equation*}\n     \\bar{P}_{\\pm}\\left(S^{\\pm}, t ; S_{10}, S_{20}, t_{0} \\longrightarrow t\\right)=\\delta\\left(S_{10} \\pm S_{20}-S^{\\pm}\\right) \\quad,\n \\end{equation*}\ntheir closed-form probability distribution functions are given by \n  \\begin{equation*}\n  \\begin{aligned}\n     &\\qquad f^{\\mathrm{LN}}\\left(\\tilde{S}^{\\pm}, t ; \\tilde{S}_{0}^{\\pm}, t_{0}\\right)\\\\\n     = &\\frac{1}{\\widetilde{S}^{\\pm} \\sqrt{2 \\pi \\tilde{\\sigma}_{\\pm}^{2}\\left(t-t_{0}\\right)}} \\\\\n      &\\cdot \\exp \\left\\{-\\frac{\\left[\\ln \\left(\\tilde{S}^{+} / \\tilde{S}_{0}^{+}\\right)+(1 / 2) \\tilde{\\sigma}_{\\pm}^{2}\\left(t-t_{0}\\right)\\right]^{2}}{2 \\tilde{\\sigma}_{\\pm}^{2}\\left(t-t_{0}\\right)}\\right\\} \\quad.\n \\end{aligned}\n \\end{equation*}\nIt is an approximately normal distribution, and $\\tilde{S}^{+}$, $\\tilde{S}^{-}$ are lognormal random variables, $\\tilde{S}_{0}^{\\pm}$ are initial condition in $t_0$ defined by Weiner process above. (Noticed that $\\tilde{\\sigma}_{\\pm}^{2}\\left(t-t_{0}\\right)$ should be small to make this approximation valid.In our simulation experiment, we set $t-t_0=1$ WLOG.) Since\n\\begin{equation*}\n\\widetilde{S}_{0}^{-}=(S_{10}-S_{20})+\\left(\\frac{\\sigma_{-}^{2}}{\\sigma_{1}^{2}-\\sigma_{2}^{2}}\\right) (S_{10}+S_{20}),\\\\ \n\\end{equation*}\nand\n\\begin{equation*}\n    \\tilde{\\sigma_-}=\\left(\\sigma_{1}^{2}-\\sigma_{2}^{2}\\right) /\\left(2 \\sigma_{-}\\right)\\\\\n\\end{equation*}\n\\begin{equation*}\n    \\sigma_-=\\sqrt{\\sigma_1^2+\\sigma_2^2}\n\\end{equation*}\nNoticed that $E(B_1)>E(B_2)$, $\\mathrm{Var}(B_1)>\\mathrm{Var}(B_2)$, the mean value and the variance of the approximate normal distribution shows positive correlation with $\\sigma_1^2-\\sigma_2^2$.Besides, the closed-form PDF $f^{\\mathrm{LN}}\\left(\\tilde{S}^{\\pm}, t ; \\tilde{S}_{0}^{\\pm}, t_{0}\\right)$ also show positive correlation with $\\sigma_1^2-\\sigma_2^2$. Due to the limitation of $\\tilde{\\sigma}_{\\pm}^{2}\\left(t-t_{0}\\right)$ should be small enough, such positive correlation is not significant in our illustrative numerical experiment.\n\nBy using Lie-Trotter Operator Splitting Method in \\cite{lo2012sum}, we can give illustrative numeral examples for the distribution of $B_1-B_2$,in which the parameters are well chosen to fit for our top-u approximation in actual LLLT experiments. Figure shows that it is of high probability that when $\\sigma_1^2>\\sigma_2^2$, the inequality holds that $B_1>B_2$, $ES(A_1)>ES(A_2)$.\n\nFinishing prooving our proposition in general conditions, we can consider a more specific condition that if $\\mb{q}_1, \\mb{q}_2 \\in \\{ \\mb{q} | M(\\mb{q}, \\mb{K}) \\in [ M_{m},  M_{m} - \\kappa)\\}$, the proposition still holds with high probability.\n\nFirst, we have $M(q_1,\\mb{k})=ln(B_1)>(M_m-\\kappa)$ holds for $\\forall q_1,q_2$ in this interval. Since we have proved that $E(B_1))=ne^{\\frac{\\sigma_1^2}{2}}$, we can conclude that $\\forall q_i$ in the given interval,$\\exists \\alpha, \\sigma_i^2>\\alpha, i=1,2$. Since we have $\\widetilde{S}_{0}^{-}=(S_{10}-S_{20})+\\left(\\frac{\\sigma_{-}^{2}}{\\sigma_{1}^{2}-\\sigma_{2}^{2}}\\right) (S_{10}+S_{20})$, which also shows positive correlation with $\\sigma_1^2+\\sigma_2^2>2\\alpha$, and positive correlation with $\\sigma_1^2-\\sigma_2^2$. So due to the nature of the approximate normal distribution PDF, if $\\sigma_1^2>\\sigma_2^2$ WLOG, $Pr(M(q_1,\\mb{k})>M(q_2,\\mb{k}))\\approx \\Phi(\\frac{\\widetilde{S}_{0}^{-}}{\\tilde{\\sigma_-}})$ also shows positive correlation with $\\sigma_1^2+\\sigma_2^2>2\\alpha$.\n\nWe give an illustrative numerical examples of the approximation above in Fig.(\\ref{fig:appendix.examples}). In our actual LTTnet experiment, we choose Top-k of $A_1,A_2$, not the whole set.Actually, we can make a naive assumption that in choosing $top-\\lfloor\\frac{1}{4}L_k\\rfloor$ variables of $A_1,A_2$ denoted as $A_1^{'} , A_2^{'}$,the variation $\\sigma_1,\\sigma_2$ don't change significantly, but the expectation $E(A_1^{'}) , E(A_2^{'})$ \nascends obviously, which leads to initial condition $S_{10},S_{20}$ ascends significantly, since the initial condition will be sampled from $top-\\lfloor\\frac{1}{4}L_k\\rfloor$ variables, not the whole set.\n\nIn our actual LTTnet experiment, we set $U$, namely choosing around $top-\\lfloor\\frac{1}{4}L_k\\rfloor$ of $A_1$ and $A_2$, it is guaranteed that with over $99\\%$ probability that in the $[ M_{m},  M_{m} - \\kappa)$ interval, as shown in the black curve of Fig.(\\ref{fig:appendix.examples}). Typically the condition 2 can be relaxed, and we can believe that if $q_1,q_2$ fits the condition 1 in our proposition, we have $M(\\mb{q}_1, \\mb{K}) > M(\\mb{q}_2, \\mb{K})$.\n\\end{proof}\n\n\\begin{figure}\n    \\resizebox{0.85\\linewidth}{!}{\\includegraphics{figures/lognormal.pdf}}\n    \\centering\n    \\caption{Probability Density verses $S_1-S_2$ for the approximation of  shifted lognormal distribution.}\n    \\label{fig:appendix.examples}\n\\end{figure}\n\n\\section{Reproducibility}\n\\subsection{Details of the experiments}\n\\label{sec:appendix.network}\nThe details of proposed \\mn~ model is summarized in Table \\ref{tab:method.network.detail}. For the \\emph{ProbSparse} self-attention mechanism, we let $d$=32, $n$=16 and add residual connections, a position-wise feed-forward network layer (inner-layer dimension is 2048) and a dropout layer ($p=0.1$) likewise. Note that we preserves 10\\% validation data for each dataset, so all the experiments are conducted over 5 random train/val shifting selection along time and the results are averaged over the 5 runs. All the datasets are performed standardization such that the mean of variable is 0 and the standard deviation is 1.\n\n\\begin{table}[ht]\n\\caption{The \\mn~ network components in details}\n\\label{tab:method.network.detail}\n\\small\n\\centering\n\\begin{tabular}{c|c|c|c}\n\\hline\n\\hline\n\\multicolumn{3}{l}{\\textbf{Encoder:}} & N \\\\\n\\hline\nInputs & 1x3 Conv1d & Embedding ($d=512$) & \\multirow{7}{*}{4} \\\\\n\\cline{1-3}\n\\multirow{4}{*}{\\shortstack{ProbSparse\\\\ Self-attention\\\\ Block }} & \\multicolumn{2}{|c|}{\\scriptsize{Multi-head ProbSparse Attention} ($h=16$, $d=32$)} & \\\\\n\\cline{2-3}\n & \\multicolumn{2}{|c|}{Add, LayerNorm, Dropout ($p=0.1$)} & \\\\\n\\cline{2-3}\n & \\multicolumn{2}{|c|}{Pos-wise FFN ($d_{\\rm{inner}}=2048$), GELU } & \\\\\n\\cline{2-3}\n & \\multicolumn{2}{|c|}{Add, LayerNorm, Dropout ($p=0.1$)} & \\\\\n\\cline{1-3}\n\\multirow{2}{*}{Distilling} & \\multicolumn{2}{|c|}{1x3 conv1d, ELU} & \\\\\n\\cline{2-3}\n & \\multicolumn{2}{|c|}{Max pooling ($\\textrm{stride}=2$)} & \\\\\n\\hline\n\\multicolumn{3}{l}{\\textbf{Decoder:}} & N \\\\\n\\hline\nInputs & 1x3 Conv1d & Embedding ($d=512$) & \\multirow{6}{*}{2} \\\\\n\\cline{1-3}\nMasked PSB & \\multicolumn{2}{|c|}{add Mask on Attention Block} & \\\\\n\\cline{1-3}\n\\multirow{4}{*}{\\shortstack{Self-attention\\\\ \\\\ Block }} & \\multicolumn{2}{|c|}{Multi-head Attention ($h=8$, $d=64$)} & \\\\\n\\cline{2-3}\n & \\multicolumn{2}{|c|}{Add, LayerNorm, Dropout ($p=0.1$)} & \\\\\n\\cline{2-3}\n & \\multicolumn{2}{|c|}{Pos-wise FFN ($d_{\\rm{inner}}=2048$), GELU } & \\\\\n\\cline{2-3}\n & \\multicolumn{2}{|c|}{Add, LayerNorm, Dropout ($p=0.1$)} & \\\\\n\\hline\n\\multicolumn{4}{l}{\\textbf{Final:}} \\\\\n\\hline\nOutputs & \\multicolumn{2}{|c|}{FCN ($d=d_{\\rm{out}}$)} & \\\\\n\\cline{1-3}\n\\hline\n\\end{tabular}\n\\end{table}\n\n\\subsection{Implement of the \\emph{ProbSparse} self-attention}\n\\label{sec:appendix.algo}\nWe have implemented the \\emph{ProbSparse} self-attention in Python 3.6 with Pytorch 1.0. The pseudo-code is given in Algo.(\\ref{alg:mPegasos.pbAttention}). The source code is available at \\url{https://github.com/zhouhaoyi/Informer2020}. All the procedure can be highly efficient vector operation and maintains logarithmic total memory usage. The masked version can be achieved by applying positional mask on step 6 and using $\\textrm{cmusum}(\\cdot)$ in $\\textrm{mean}(\\cdot)$ of step 7. In the practice, we can use $\\textrm{sum}(\\cdot)$ as the simpler implement of $\\textrm{mean}(\\cdot)$.\n\n\\begin{algorithm}[ht]\n\\caption{ProbSparse self-attention}\n\\label{alg:mPegasos.pbAttention}\n\\begin{algorithmic}[1]\n\\small\n\\REQUIRE\nTensor $\\mb{Q}\\in \\mathbb{R}^{m \\times d}$, $\\mb{K}\\in \\mathbb{R}^{n \\times d}$, $\\mb{V}\\in \\mathbb{R}^{n \\times d}$\\\\\n\\PRINT set hyperparameter $c$, $u= c \\ln m$ and $U= m \\ln n$\n\\STATE randomly select $U$ dot-product pairs from $\\mb{K}$ as $\\bar{\\mb{K}}$\n\\STATE set the sample score $\\bar{\\mathbf{S}} = \\mb{Q} \\bar{\\mb{K}}^{\\top}$\n\\STATE compute the measurement $M= \\textrm{max} (\\bar{\\mathbf{S}}) - \\textrm{mean} (\\bar{\\mathbf{S}})$ by row\n\\STATE set Top-$u$ queries under $M$ as $\\bar{\\mb{Q}}$\n\\STATE set $\\mathbf{S}_1 = \\textrm{softmax}(\\bar{\\mb{Q}} \\mb{K}^{\\top} / \\sqrt{d}) \\cdot \\mb{V}$\n\\STATE set $\\mathbf{S}_0 = \\textrm{mean}(\\mb{V})$\n\\STATE set $\\mathbf{S} = \\{\\mathbf{S}_1,\\mathbf{S}_0\\}$ by their original rows accordingly\n\\ENSURE\nself-attention feature map $\\mathbf{S}$. \\\\\n\\end{algorithmic}\n\\end{algorithm}\n\n\\begin{figure*}[t]\n  \\centering\n  \\includegraphics[width=1\\linewidth]{figures/Exp_DrawCurve_New2.pdf}\n  \\caption{The predicts (len=336) of \\mn, \\mn$^{\\dag}$, LogTrans, Reformer, DeepAR, LSTMa, ARIMA and Prophet on the ETTm dataset. The red / blue curves stand for slices of the prediction / ground truth.}\n  \\label{fig:appendix.exp.curve}\n\\end{figure*}\n\n\\subsection{The hyperparameter tuning range}\n\\label{sec:appendix.range}\nFor all methods, the input length of recurrent component is chosen from \\{24, 48, 96, 168, 336, 720\\} for the ETTh1, ETTh2, Weather and Electricity dataset, and chosen from \\{24, 48, 96, 192, 288, 672\\} for the ETTm dataset. \nFor LSTMa and DeepAR, the size of hidden states is chosen from \\{32, 64, 128, 256\\}. \nFor LSTnet, the hidden dimension of the Recurrent layer and Convolutional layer is chosen from \\{64, 128, 256\\} and \\{32, 64, 128\\} for Recurrent-skip layer, and the skip-length of Recurrent-skip layer is set as 24 for the ETTh1, ETTh2, Weather and ECL dataset, and set as 96 for the ETTm dataset.\nFor {\\mn}, the layer of encoder is chosen from \\{6, 4, 3, 2\\} and the layer of decoder is set as 2. The head number of multi-head attention is chosen from \\{8, 16\\}, and the dimension of multi-head attentionâ€™s output is set as 512. The length of encoderâ€™s input sequence and decoderâ€™s start token is chosen from \\{24, 48, 96, 168, 336, 480, 720\\} for the ETTh1, ETTh2, Weather and ECL dataset, and \\{24, 48, 96, 192, 288, 480, 672\\} for the ETTm dataset. In the experiment, the decoderâ€™s start token is a segment truncated from the encoderâ€™s input sequence, so the length of decoderâ€™s start token must be less than the length of encoderâ€™s input.\n\nThe RNN-based methods perform a dynamic decoding with left shifting on the prediction windows. Our proposed methods \\mn-series and LogTrans (our decoder) perform non-dynamic decoding.\n\n\\section{Extra experimental results}\n\nFig.(\\ref{fig:appendix.exp.curve}) presents a slice of the predicts of 8 models. The most realted work LogTrans and Reformer shows acceptable results. The LSTMa model is not amenable for the long sequence prediction task. The ARIMA and DeepAR can capture the long trend of the long sequences. And the Prophet detects the changing point and fits it with a smooth curve better than the ARIMA and DeepAR. Our proposed model \\mn~ and \\mn$^{\\dag}$ show significantly better results than above methods.\n\n\\section{Computing Infrastructure}\nAll the experiments are conducted on Nvidia Tesla V100 SXM2 GPUs (32GB memory). Other configuration includes 2 * Intel Xeon Gold 6148 CPU, 384GB DDR4 RAM and 2 * 240GB M.2 SSD, which is sufficient for all the baselines. \n\n\\end{appendices}\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Autoformer: Decomposition Transformers with \\\\ Auto-Correlation\nfor Long-Term Series Forecasting}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nExtending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the \\textit{long-term forecasting} problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Going beyond Transformers, we design \\textit{Autoformer} as a novel decomposition architecture with an \\textit{Auto-Correlation} mechanism. We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efficiency and accuracy. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38\\% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease. Code is available at this repository: \\url{https://github.com/thuml/Autoformer}.\n\\end{abstract}\n\n\\section{Introduction}\n\nTime series forecasting has been widely used in energy consumption, traffic and economics planning, weather and disease propagation forecasting. In these real-world applications, one pressing demand is to extend the forecast time into the far future, which is quite meaningful for the long-term planning and early warning.\nThus, in this paper, we study the \\textit{long-term forecasting} problem of time series, characterizing itself by the large length of predicted time series.\nRecent deep forecasting models \\cite{haoyietal-informer-2021,kitaev2020reformer,2019Enhancing,Flunkert2017DeepARPF,oreshkin2019n,Sen2019ThinkGA,2018Modeling,NIPS2017_3f5ee243} have achieved great progress, especially the Transformer-based models. Benefiting from the self-attention mechanism, Transformers obtain great advantage in modeling long-term dependencies for sequential data, which enables more powerful big models \\cite{NEURIPS2020_1457c0d6,Devlin2019BERTPO}.\n\nHowever, the forecasting task is extremely challenging under the long-term setting. First, it is unreliable to discover the temporal dependencies directly from the long-term time series because the dependencies can be obscured by entangled temporal patterns. Second, canonical Transformers with self-attention mechanisms are computationally prohibitive for long-term forecasting because of the quadratic complexity of sequence length. Previous Transformer-based forecasting models \\cite{haoyietal-informer-2021,kitaev2020reformer,2019Enhancing} mainly focus on improving self-attention to a \\emph{sparse} version. While performance is significantly improved, these models still utilize the point-wise representation aggregation. Thus, in the process of efficiency improvement, they will sacrifice the information utilization because of the sparse point-wise connections, resulting in a bottleneck for long-term forecasting of time series.\n\nTo reason about the intricate temporal patterns, we try to take the idea of decomposition, which is a standard method in time series analysis \\cite{Anderson1976TimeSeries2E,Cleveland1990STLAS}. It can be used to process the complex time series and extract more predictable components. However, under the forecasting context, it can only be used as the \\emph{pre-processing} of past series because the future is unknown \\cite{Hyndman2013ForecastingPA}. This common usage limits the capabilities of decomposition and overlooks the potential future interactions among decomposed components. Thus, we attempt to go beyond pre-processing usage of decomposition and propose a generic architecture to empower the deep forecasting models with immanent capacity of progressive decomposition.\nFurther, decomposition can ravel out the entangled temporal patterns and highlight the inherent properties of time series \\cite{Hyndman2013ForecastingPA}. Benefiting from this, we try to take advantage of the series periodicity to renovate the point-wise connection in self-attention. We observe that the sub-series at the same phase position among periods often present similar temporal processes. Thus, we try to construct a series-level connection based on the process similarity derived by series periodicity.\n\nBased on the above motivations, we propose an original \\textbf{Autoformer} in place of the Transformers for long-term time series forecasting.\nAutoformer still follows residual and encoder-decoder structure but renovates Transformer into a decomposition forecasting architecture. By embedding our proposed decomposition blocks as the inner operators, Autoformer can progressively separate the long-term trend information from predicted hidden variables. This design allows our model to alternately decompose and refine the intermediate results during the forecasting procedure. \nInspired by the stochastic process theory \\cite{Chatfield1981TheAO,Papoulis1965ProbabilityRV}, Autoformer introduces an \\textbf{Auto-Correlation} mechanism in place of self-attention, which discovers the sub-series similarity based on the series periodicity and aggregates similar sub-series from underlying periods. This series-wise mechanism achieves $\\mathcal{O}(L\\log L)$ complexity for length-$L$ series and breaks the information utilization bottleneck by expanding the point-wise representation aggregation to sub-series level. \nAutoformer achieves the state-of-the-art accuracy on six benchmarks.\nThe contributions are summarized as follows:\n\\begin{itemize}\n  \\item To tackle the intricate temporal patterns of the long-term future, we present \\textit{Autoformer} as a decomposition architecture and design the inner decomposition block to empower the deep forecasting model with immanent progressive decomposition capacity.\n  \\item We propose an \\textit{Auto-Correlation} mechanism with dependencies discovery and information aggregation at the series level. Our mechanism is beyond previous self-attention family and can simultaneously benefit the computation efficiency and information utilization.\n  \\item Autoformer achieves a 38\\% relative improvement under the long-term setting on six benchmarks, covering five real-world applications: energy, traffic, economics, weather and disease.\n\\end{itemize}\n\n\\vspace{-5pt}\n\\section{Related Work}\n\\vspace{-5pt}\n\\subsection{Models for Time Series Forecasting}\nDue to the immense importance of time series forecasting, various models have been well developed.\nMany time series forecasting methods start from the classic tools \\cite{Sorjamaa2007MethodologyFL,chen2021data}. ARIMA \\cite{Box1968SomeRA,Box1970TimeSA} tackles the forecasting problem by transforming the non-stationary process to stationary through differencing. The filtering method is also introduced for series forecasting \\cite{Kurle2020DeepRP,Bzenac2020NormalizingKF}.\nBesides, recurrent neural networks (RNNs) models are used to model the temporal dependencies for time series \\cite{Wen2017AMQ,Rangapuram2018DeepSS,2017Long,Maddix2018DeepFW}. DeepAR \\cite{Flunkert2017DeepARPF} combines autoregressive methods and RNNs to model the probabilistic distribution of future series. \nLSTNet \\cite{2018Modeling} introduces convolutional neural networks (CNNs) with recurrent-skip connections to capture the short-term and long-term temporal patterns. Attention-based RNNs \\cite{2017A,Shih2019TemporalPA,Song2018AttendAD} introduce the temporal attention to explore the long-range dependencies for prediction.\nAlso, many works based on temporal convolution networks (TCN) \\cite{Oord2016WaveNetAG,2017Conditional,Bai2018AnEE,Sen2019ThinkGA} attempt to model the temporal causality with the causal convolution.\nThese deep forecasting models mainly focus on the temporal relation modeling by recurrent connections, temporal attention or causal convolution. \n\nRecently, Transformers \\cite{NIPS2017_3f5ee243,Wu2020AdversarialST} based on the self-attention mechanism shows great power in sequential data, such as natural language processing \\cite{Devlin2019BERTPO,NEURIPS2020_1457c0d6}, audio processing \\cite{huang2018music} and even computer vision \\cite{dosovitskiy2021an,liu2021Swin}. However, applying self-attention to long-term time series forecasting is computationally prohibitive because of the quadratic complexity of sequence length $L$ in both memory and time. LogTrans \\cite{2019Enhancing} introduces the local convolution to Transformer and proposes the LogSparse attention to select time steps following the exponentially increasing intervals, which reduces the complexity to $\\mathcal{O}(L(\\log L)^2)$. Reformer \\cite{kitaev2020reformer} presents the local-sensitive hashing (LSH) attention and reduces the complexity to $\\mathcal{O}(L\\log L)$. Informer \\cite{haoyietal-informer-2021} extends Transformer with KL-divergence based ProbSparse attention and also achieves $\\mathcal{O}(L\\log L)$ complexity. Note that these methods are based on the vanilla Transformer and try to improve the self-attention mechanism to a \\emph{sparse} version, which still follows the point-wise dependency and aggregation. In this paper, our proposed Auto-Correlation mechanism is based on the inherent periodicity of time series and can provide series-wise connections.\n\n\\subsection{Decomposition of Time Series}\nAs a standard method in time series analysis, time series decomposition \\cite{Anderson1976TimeSeries2E,Cleveland1990STLAS} deconstructs a time series into several components, each representing one of the underlying categories of patterns that are more predictable. It is primarily useful for exploring historical changes over time. For the forecasting tasks, decomposition is always used as the \\emph{pre-processing} of historical series before predicting future series \\cite{Hyndman2013ForecastingPA,Asadi2020ASD}, such as Prophet \\cite{Taylor2017ForecastingAS} with trend-seasonality decomposition and N-BEATS \\cite{oreshkin2019n} with basis expansion and DeepGLO \\cite{Sen2019ThinkGA} with matrix decomposition. However, such pre-processing is limited by the plain decomposition effect of historical series and overlooks the hierarchical interaction between the underlying patterns of series in the long-term future. This paper takes the decomposition idea from a new progressive dimension. Our Autoformer harnesses the decomposition as an inner block of deep models, which can progressively decompose the hidden series throughout the whole forecasting process, including both the past series and the predicted intermediate results.\n\n\\section{Autoformer}\n\nThe time series forecasting problem is to predict the most probable length-$O$ series in the future given the past length-$I$ series, denoting as \\textit{input-$I$-predict-$O$}. The \\textit{long-term forecasting} setting is to predict the long-term future, i.e. larger $O$.\nAs aforementioned, we have highlighted the difficulties of long-term series forecasting: handling intricate temporal patterns and breaking the bottleneck of computation efficiency and information utilization. To tackle these two challenges, we introduce the decomposition as a builtin block to the deep forecasting model and propose \\textit{Autoformer} as a decomposition architecture. Besides, we design the \\textit{Auto-Correlation} mechanism to discover the period-based dependencies and aggregate similar sub-series from underlying periods.\n\n\\subsection{Decomposition Architecture}\nWe renovate Transformer \\cite{NIPS2017_3f5ee243} to a deep decomposition architecture (Figure \\ref{fig:framework}), including the inner series decomposition block, Auto-Correlation mechanism, and corresponding Encoder and Decoder.\n\n\\vspace{-5pt}\n\\paragraph{Series decomposition block}\nTo learn with the complex temporal patterns in long-term forecasting context, we take the idea of decomposition \\cite{Anderson1976TimeSeries2E,Cleveland1990STLAS}, which can separate the series into trend-cyclical and seasonal parts. These two parts reflect the long-term progression and the seasonality of the series respectively. However, directly decomposing is unrealizable for future series because the future is just unknown. To tackle this dilemma, we present a \\textit{series decomposition block} as an inner operation of Autoformer (Figure \\ref{fig:framework}), which can extract the long-term stationary trend from predicted intermediate hidden variables progressively. Concretely, we adapt the moving average to smooth out periodic fluctuations and highlight the long-term trends. For length-$L$ input series $\\mathcal{X}\\in\\mathbb{R}^{L\\times d}$, the process is:\n\\begin{equation}\\label{equ:moving_avg}\n  \\begin{split}\n  \\mathcal{X}_{\\mathrm{t}} & = \\mathrm{AvgPool}(\\mathrm{Padding}(\\mathcal{X})) \\\\\n  \\mathcal{X}_{\\mathrm{s}} & = \\mathcal{X} - \\mathcal{X}_{\\mathrm{t}}, \\\\\n  \\end{split}\n\\end{equation}\nwhere $\\mathcal{X}_{\\mathrm{s}},\\mathcal{X}_{\\mathrm{t}}\\in\\mathbb{R}^{L\\times d}$ denote the seasonal and the extracted trend-cyclical part respectively. We adopt the $\\mathrm{AvgPool}(\\cdot)$ for moving average with the padding operation to keep the series length unchanged. We use $\\mathcal{X}_{\\mathrm{s}},\\mathcal{X}_{\\mathrm{t}}=\\mathrm{SeriesDecomp}(\\mathcal{X})$ to summarize above equations, which is a model inner block. \n\n\\vspace{-5pt}\n\\paragraph{Model inputs} The inputs of encoder part are the past $I$ time steps $\\mathcal{X}_{\\mathrm{en}}\\in\\mathbb{R}^{I\\times d}$.\nAs a decomposition architecture (Figure \\ref{fig:framework}), the input of Autoformer decoder contains both the seasonal part $\\mathcal{X}_{\\mathrm{des}} \\in\\mathbb{R}^{(\\frac{I}{2}+O)\\times d}$ and trend-cyclical part $\\mathcal{X}_{\\mathrm{det}} \\in\\mathbb{R}^{(\\frac{I}{2}+O)\\times d}$ to be refined. Each initialization consists of two parts: the component decomposed from the latter half of encoder's input $\\mathcal{X}_{\\mathrm{en}}$ with length $\\frac{I}{2}$ to provide recent information, placeholders with length $O$ filled by scalars. It's formulized as follows:\\begin{equation}\\label{equ:model_inputs}\n  \\begin{split}\n  \\mathcal{X}_{\\mathrm{ens}}, \\mathcal{X}_{\\mathrm{ent}} &= \\mathrm{SeriesDecomp}({\\mathcal{X}_{\\mathrm{en}}}_{\\frac{I}{2}:I}) \\\\\n  \\mathcal{X}_{\\mathrm{des}} & = \\mathrm{Concat}(\\mathcal{X}_{\\mathrm{ens}}, \\mathcal{X}_{0}) \\\\\n  \\mathcal{X}_{\\mathrm{det}} & = \\mathrm{Concat}(\\mathcal{X}_{\\mathrm{ent}}, \\mathcal{X}_{\\mathrm{Mean}}), \\\\\n  \\end{split}\n\\end{equation}\nwhere $\\mathcal{X}_{\\mathrm{ens}},\\mathcal{X}_{\\mathrm{ent}}\\in\\mathbb{R}^{\\frac{I}{2}\\times d}$ denote the seasonal and trend-cyclical parts of $\\mathcal{X}_{\\mathrm{en}}$ respectively, and $\\mathcal{X}_{0},\\mathcal{X}_{\\mathrm{Mean}}\\in\\mathbb{R}^{O\\times d}$ denote the placeholders filled with zero and the mean of $\\mathcal{X}_{\\mathrm{en}}$ respectively.\n\n\\begin{figure*}\n\\begin{center}\n\t\\centerline{\\includegraphics[width=\\columnwidth]{fig/framework_1_1.pdf}}\n\t\\caption{Autoformer architecture. The encoder eliminates the long-term trend-cyclical part by series decomposition blocks (\\textcolor{blue}{blue} blocks) and focuses on seasonal patterns modeling. The decoder accumulates the trend part extracted from hidden variables progressively. The past seasonal information from encoder is utilized by the encoder-decoder Auto-Correlation (center \\textcolor[rgb]{0.15,0.7,0.15}{green} block in decoder). }\n\t\\label{fig:framework}\n\t\\vspace{-20pt}\n\\end{center}\n\\end{figure*}\n\n\\paragraph{Encoder} \nAs shown in Figure \\ref{fig:framework}, the encoder focuses on the seasonal part modeling. The output of the encoder contains the past seasonal information and will be used as the cross information to help the decoder refine prediction results. \nSuppose we have $N$ encoder layers. The overall equations for $l$-th encoder layer are summarized as $\\mathcal{X}_{\\mathrm{en}}^{l}=\\mathrm{Encoder}(\\mathcal{X}_{\\mathrm{en}}^{l-1})$. Details are shown as follows:\n\\begin{equation}\\label{equ:overall_encoder}\n  \\begin{split}\n  \\mathcal{S}_{\\mathrm{en}}^{l,1},\\underline{~~} & = \\mathrm{SeriesDecomp}\\Big(\\mathrm{Auto\\text{-}Correlation}(\\mathcal{X}_{\\mathrm{en}}^{l-1})+\\mathcal{X}_{\\mathrm{en}}^{l-1}\\Big) \\\\\n  \\mathcal{S}_{\\mathrm{en}}^{l,2},\\underline{~~} & = \\mathrm{SeriesDecomp}\\Big(\\mathrm{FeedForward}(\\mathcal{S}_{\\mathrm{en}}^{l,1})+\\mathcal{S}_{\\mathrm{en}}^{l,1}\\Big), \\\\\n  \\end{split}\n\\end{equation}\nwhere ``$\\underline{~~}$'' is the eliminated trend part. $\\mathcal{X}_{\\mathrm{en}}^{l}=\\mathcal{S}_{\\mathrm{en}}^{l,2},l\\in\\{1,\\cdots,N\\}$ denotes the output of $l$-th encoder layer and $\\mathcal{X}_{\\mathrm{en}}^{0}$ is the embedded $\\mathcal{X}_{\\mathrm{en}}$. $\\mathcal{S}_{\\mathrm{en}}^{l,i},i\\in\\{1,2\\}$ represents the seasonal component after the $i$-th series decomposition block in the $l$-th layer respectively. We will give detailed description of $\\mathrm{Auto\\text{-}Correlation}(\\cdot)$ in the next section, which can seamlessly replace the self-attention.\n\n\\paragraph{Decoder}\nThe decoder contains two parts: the accumulation structure for trend-cyclical components and the stacked Auto-Correlation mechanism for seasonal components (Figure \\ref{fig:framework}). Each decoder layer contains the \\emph{inner} Auto-Correlation and \\emph{encoder-decoder} Auto-Correlation, which can refine the prediction and utilize the past seasonal information respectively. Note that the model extracts the potential trend from the intermediate hidden variables during the decoder, allowing Autoformer to progressively refine the trend prediction and eliminate interference information for period-based dependencies discovery in Auto-Correlation. \nSuppose there are $M$ decoder layers. With the latent variable $\\mathcal{X}_{\\mathrm{en}}^{N}$ from the encoder, the equations of $l$-th decoder layer can be summarized as $\\mathcal{X}_{\\mathrm{de}}^{l}=\\mathrm{Decoder}(\\mathcal{X}_{\\mathrm{de}}^{l-1},\\mathcal{X}_{\\mathrm{en}}^{N})$. The decoder can be formalized as follows:\n\\begin{equation}\\label{equ:overall_decoder}\n  \\begin{split}\n  \\mathcal{S}_{\\mathrm{de}}^{l,1},\\mathcal{T}_{\\mathrm{de}}^{l,1} & = \\mathrm{SeriesDecomp}\\Big(\\mathrm{Auto\\text{-}Correlation}(\\mathcal{X}_{\\mathrm{de}}^{l-1})+\\mathcal{X}_{\\mathrm{de}}^{l-1}\\Big) \\\\\n  \\mathcal{S}_{\\mathrm{de}}^{l,2},\\mathcal{T}_{\\mathrm{de}}^{l,2} & = \\mathrm{SeriesDecomp}\\Big(\\mathrm{Auto\\text{-}Correlation}(\\mathcal{S}_{\\mathrm{de}}^{l,1}, \\mathcal{X}_{\\mathrm{en}}^{N})+\\mathcal{S}_{\\mathrm{de}}^{l,1}\\Big) \\\\\n  \\mathcal{S}_{\\mathrm{de}}^{l,3},\\mathcal{T}_{\\mathrm{de}}^{l,3} & = \\mathrm{SeriesDecomp}\\Big(\\mathrm{FeedForward}(\\mathcal{S}_{\\mathrm{de}}^{l,2})+\\mathcal{S}_{\\mathrm{de}}^{l,2}\\Big) \\\\\n  \\mathcal{T}_{\\mathrm{de}}^{l} & = \\mathcal{T}_{\\mathrm{de}}^{l-1} + \\mathcal{W}_{l,1}\\ast\\mathcal{T}_{\\mathrm{de}}^{l,1}+\\mathcal{W}_{l,2}\\ast\\mathcal{T}_{\\mathrm{de}}^{l,2}+\\mathcal{W}_{l,3}\\ast\\mathcal{T}_{\\mathrm{de}}^{l,3}, \\\\\n  \\end{split}\n\\end{equation}\nwhere $\\mathcal{X}_{\\mathrm{de}}^{l}= \\mathcal{S}_{\\mathrm{de}}^{l,3},l\\in\\{1,\\cdots,M\\}$ denotes the output of $l$-th decoder layer. $\\mathcal{X}_{\\mathrm{de}}^{0}$ is embedded from $\\mathcal{X}_{\\mathrm{des}}$ for deep transform and $\\mathcal{T}_{\\mathrm{de}}^{0}=\\mathcal{X}_{\\mathrm{det}}$ is for accumulation. $\\mathcal{S}_{\\mathrm{de}}^{l,i},\\mathcal{T}_{\\mathrm{de}}^{l,i},i\\in\\{1,2,3\\}$ represent the seasonal component and trend-cyclical component after the $i$-th series decomposition block in the $l$-th layer respectively. $\\mathcal{W}_{l,i},i\\in\\{1,2,3\\}$ represents the projector for the $i$-th extracted trend $\\mathcal{T}_{\\mathrm{de}}^{l,i}$.\n\nThe final prediction is the sum of the two refined decomposed components, as $\\mathcal{W}_{\\mathcal{S}}\\ast\\mathcal{X}_{\\mathrm{de}}^{M}+\\mathcal{T}_{\\mathrm{de}}^{M}$, where $\\mathcal{W}_{\\mathcal{S}}$ is to project the deep transformed seasonal component $\\mathcal{X}_{\\mathrm{de}}^{M}$ to the target dimension.\n\n\\begin{figure*}\n\\begin{center}\n\t\\centerline{\\includegraphics[width=\\columnwidth]{fig/attention.pdf}}\n\t\\vspace{-5pt}\n\t\\caption{Auto-Correlation (left) and Time Delay Aggregation (right). We utilize the Fast Fourier Transform to calculate the autocorrelation $\\mathcal{R}(\\tau)$, which reflects the time-delay similarities. Then the similar sub-processes are rolled to the same index based on selected delay $\\tau$ and aggregated by $\\mathcal{R}(\\tau)$.}\n\t\\label{fig:autocorrelation}\n\t\\vspace{-20pt}\n\\end{center}\n\\end{figure*}\n\\vspace{-5pt}\n\\subsection{Auto-Correlation Mechanism}\\label{autocorrelation_calculation}\n\\vspace{-5pt}\nAs shown in Figure \\ref{fig:autocorrelation}, we propose the Auto-Correlation mechanism with series-wise connections to expand the information utilization. Auto-Correlation discovers the {period-based dependencies} by calculating the series autocorrelation and aggregates similar sub-series by {time delay aggregation}.\n\n\\paragraph{Period-based dependencies} It is observed that the same phase position among periods naturally provides similar sub-processes.\n\nInspired by the stochastic process theory \\cite{Chatfield1981TheAO,Papoulis1965ProbabilityRV}, for a real discrete-time process $\\{\\mathcal{X}_t\\}$, we can obtain the autocorrelation $\\mathcal{R}_{\\mathcal{X}\\mathcal{X}}(\\tau)$ by the following equations:\n\\begin{equation}\\label{equ:autocorrelation} \n  \\begin{split}\n    \\mathcal{R}_{\\mathcal{X}\\mathcal{X}}(\\tau)=\\lim_{L\\to \\infty}\\frac{1}{L}\\sum_{t=1}^{L}\\mathcal{X}_t\\mathcal{X}_{t-\\tau}.\n  \\end{split}\n\\end{equation}\n$\\mathcal{R}_{\\mathcal{X}\\mathcal{X}}(\\tau)$ reflects the time-delay similarity between $\\{\\mathcal{X}_{t}\\}$ and its $\\tau$ lag series $\\{\\mathcal{X}_{t-\\tau}\\}$. As shown in Figure \\ref{fig:autocorrelation}, we use the autocorrelation $\\mathcal{R}(\\tau)$ as the unnormalized confidence of estimated period length $\\tau$. Then, we choose the most possible $k$ period lengths $\\tau_{1},\\cdots,\\tau_{k}$. The period-based dependencies are derived by the above estimated periods and can be weighted by the corresponding autocorrelation. \n\n\\paragraph{Time delay aggregation} The period-based dependencies connect the sub-series among estimated periods. Thus, we present the \\textit{time delay aggregation} block (Figure \\ref{fig:autocorrelation}), which can roll the series based on selected time delay $\\tau_{1},\\cdots,\\tau_{k}$. This operation can align similar sub-series that are at the same phase position of estimated periods, which is different from the point-wise dot-product aggregation in self-attention family. Finally, we aggregate the sub-series by softmax normalized confidences.\n\nFor the single head situation and time series $\\mathcal{X}$ with length-$L$, after the projector, we get query $\\mathcal{Q}$, key $\\mathcal{K}$ and value $\\mathcal{V}$. Thus, it can replace self-attention seamlessly. The Auto-Correlation mechanism is:\n\\begin{equation}\\label{equ:autocorrelation_fusion}\n  \\begin{split}\n  \\tau_{1},\\cdots,\\tau_{k} & = \\mathop{\\arg\\mathrm{Topk}}_{\\tau\\in\\{1,\\cdots,L\\}}\\left(\\mathcal{R}_{\\mathcal{Q},\\mathcal{K}}(\\tau)\\right)  \\\\\n  \\widehat{\\mathcal{R}}_{\\mathcal{Q},\\mathcal{K}}(\\tau_{1}),\\cdots,\\widehat{\\mathcal{R}}_{\\mathcal{Q},\\mathcal{K}}(\\tau_{k}) & = \\mathrm{SoftMax}\\left(\\mathcal{R}_{\\mathcal{Q},\\mathcal{K}}(\\tau_{1}),\\cdots,\\mathcal{R}_{\\mathcal{Q},\\mathcal{K}}(\\tau_{k})\\right) \\\\\n  \\mathrm{Auto\\text{-}Correlation}(\\mathcal{Q},\\mathcal{K},\\mathcal{V})&= \\sum_{i=1}^{k}\\mathrm{Roll}(\\mathcal{V},\\tau_{i})\\widehat{\\mathcal{R}}_{\\mathcal{Q},\\mathcal{K}}(\\tau_{i}),\n  \\end{split}\n\\end{equation}\nwhere $\\arg\\mathrm{Topk}(\\cdot)$ is to get the arguments of the $\\mathrm{Topk}$ autocorrelations and let $k=\\lfloor c\\times\\log L\\rfloor$, $c$ is a hyper-parameter. $\\mathcal{R}_{\\mathcal{Q},\\mathcal{K}}$ is autocorrelation between series $\\mathcal{Q}$ and $\\mathcal{K}$. $\\mathrm{Roll}(\\mathcal{X},\\tau)$ represents the operation to $\\mathcal{X}$ with time delay $\\tau$, during which elements that are shifted beyond the first position are re-introduced at the last position. For the encoder-decoder Auto-Correlation (Figure \\ref{fig:framework}), $\\mathcal{K}, \\mathcal{V}$ are from the encoder $\\mathcal{X}_{\\mathrm{en}}^{N}$ and will be resized to length-$O$, $\\mathcal{Q}$ is from the previous block of the decoder.\n\nFor the multi-head version used in Autoformer, with hidden variables of $d_{\\mathrm{model}}$ channels, $h$ heads, the query, key and value for $i$-th head are $\\mathcal{Q}_{i},\\mathcal{K}_{i},\\mathcal{V}_{i}\\in\\mathbb{R}^{L\\times \\frac{d_{\\mathrm{model}}}{h}}, i\\in\\{1,\\cdots,h\\}$. The process is:\n\\begin{equation}\\label{equ:autocorrelation_multihead}\n  \\begin{split}\n  \\mathrm{MultiHead}(\\mathcal{Q},\\mathcal{K},\\mathcal{V}) &= \\mathcal{W_\\mathrm{output}}\\ast\\mathrm{Concat}(\\mathrm{head}_{1},\\cdots,\\mathrm{head}_{h}) \\\\\n  \\mathrm{where}\\ \\mathrm{head}_{i} &= \\mathrm{Auto\\text{-}Correlation}(\\mathcal{Q}_{i},\\mathcal{K}_{i},\\mathcal{V}_{i}).\n  \\end{split}\n\\end{equation}\n\n\\paragraph{Efficient computation} For period-based dependencies, these dependencies point to sub-processes at the same phase position of underlying periods and are inherently sparse. Here, we select the most possible delays to avoid picking the opposite phases. Because we aggregate $\\mathcal{O}(\\log L)$ series whose length is $L$, the complexity of Equations \\ref{equ:autocorrelation_fusion} and \\ref{equ:autocorrelation_multihead} is $\\mathcal{O}(L\\log L)$. For the autocorrelation computation (Equation \\ref{equ:autocorrelation}), given time series $\\{\\mathcal{X}_t\\}$, $\\mathcal{R}_{\\mathcal{X}\\mathcal{X}}(\\tau)$ can be calculated by Fast Fourier Transforms (FFT) based on the Wienerâ€“Khinchin theorem \\cite{MR1555316}:\n\\begin{equation}\\label{equ:autocorrelation_algo}\n  \\begin{split}\n  \\mathcal{S}_{\\mathcal{X}\\mathcal{X}}(f) & = \\mathcal{F}\\left(\\mathcal{X}_{t}\\right)\\mathcal{F}^\\ast\\left(\\mathcal{X}_{t}\\right) = \\int_{-\\infty}^{\\infty}\\mathcal{X}_{t}e^{-i2\\pi tf}{\\rm d}t\\overline{\\int_{-\\infty}^{\\infty}\\mathcal{X}_{t}e^{-i2\\pi tf}{\\rm d}t} \\\\\n  \\mathcal{R}_{\\mathcal{X}\\mathcal{X}}(\\tau) &= \\mathcal{F}^{-1}\\left(\\mathcal{S}_{\\mathcal{X}\\mathcal{X}}(f)\\right) = \\int_{-\\infty}^{\\infty}\\mathcal{S}_{\\mathcal{X}\\mathcal{X}}(f)e^{i2\\pi f\\tau}{\\rm d}f, \\\\\n  \\end{split}\n\\end{equation}\nwhere $\\tau\\in\\{1,\\cdots,L\\}$, $\\mathcal{F}$ denotes the FFT and $\\mathcal{F}^{-1}$ is its inverse. $\\ast$ denotes the conjugate operation and $\\mathcal{S}_{\\mathcal{X}\\mathcal{X}}(f)$ is in the frequency domain. Note that the series autocorrelation of all lags in $\\{1,\\cdots,L\\}$ can be calculated at once by FFT. Thus, Auto-Correlation achieves the $\\mathcal{O}(L\\log L)$ complexity.\n\\begin{figure*}\n\\begin{center}\n\t\\centerline{\\includegraphics[width=\\columnwidth]{fig/compare_1.2.pdf}}\n\t\\vspace{-5pt}\n\t\\caption{Auto-Correlation vs. self-attention family. Full Attention \\cite{NIPS2017_3f5ee243} (a) adapts the fully connection among all time points. Sparse Attention \\cite{kitaev2020reformer,haoyietal-informer-2021} (b) selects points based on the proposed similarity metrics. LogSparse Attention \\cite{2019Enhancing} (c) chooses points following the exponentially increasing intervals. Auto-Correlation (d) focuses on the connections of sub-series among underlying periods.}\n\t\\label{fig:compare}\n\t\\vspace{-25pt}\n\\end{center}\n\\end{figure*}\n\n\\paragraph{Auto-Correlation vs. self-attention family} Different from the point-wise self-attention family, Auto-Correlation presents the series-wise connections (Figure \\ref{fig:compare}). Concretely, for the temporal dependencies, we find the dependencies among sub-series based on the periodicity. In contrast, the self-attention family only calculates the relation between scattered points. Though some self-attentions \\cite{2019Enhancing,haoyietal-informer-2021} consider the local information, they only utilize this to help point-wise dependencies discovery. For the information aggregation, we adopt the time delay block to aggregate the similar sub-series from underlying periods. In contrast, self-attentions aggregate the selected points by dot-product. \nBenefiting from the inherent sparsity and sub-series-level representation aggregation, Auto-Correlation can simultaneously benefit the computation efficiency and information utilization.\n\n\\section{Experiments}\n\nWe extensively evaluate the proposed Autoformer on six real-world benchmarks, covering five mainstream time series forecasting applications: energy, traffic, economics, weather and disease.\n\n\\paragraph{Datasets} Here is a description of the six experiment datasets: \n(1) \\textit{ETT} \\cite{haoyietal-informer-2021} dataset contains the data collected from electricity transformers, including load and oil temperature that are recorded every 15 minutes between July 2016 and July 2018. \n(2) \\textit{Electricity}\\footnote{\\url{https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014}} dataset contains the hourly electricity consumption of 321 customers from 2012 to 2014.\n(3) \\textit{Exchange} \\cite{2018Modeling} records the daily exchange rates of eight different countries\nranging from 1990 to 2016.\n(4) \\textit{Traffic}\\footnote{\\url{http://pems.dot.ca.gov}} is a collection of hourly data from California Department of Transportation, which describes the road occupancy rates measured by different sensors on San Francisco Bay area freeways. \n(5) \\textit{Weather}\\footnote{\\url{https://www.bgc-jena.mpg.de/wetter/}} is recorded every 10 minutes for 2020 whole year, which contains 21 meteorological indicators, such as air temperature, humidity, etc.\n(6) \\textit{ILI}\\footnote{\\url{https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html}} includes the weekly recorded influenza-like illness (ILI) patients data from Centers for Disease Control and Prevention of the United States between 2002 and 2021, which describes the ratio of patients seen with ILI and the total number of the patients. We follow standard protocol and split all datasets into training, validation and test set in chronological order by the ratio of 6:2:2 for the ETT dataset and 7:1:2 for the other datasets.\n\n\\begin{table}[tbp]\n  \\caption{Multivariate results with different prediction lengths $O \\in \\{96,192,336,720\\}$. We set the input length $I$ as 36 for ILI and 96 for the others. A lower MSE or MAE indicates a better prediction.}\\label{tab:Results}\n  \\centering\n  \\begin{threeparttable}\n  \\begin{small}\n  \\renewcommand{\\multirowsetup}{\\centering}\n  \\setlength{\\tabcolsep}{2.6pt}\n  \\begin{tabular}{c|c|cccccccccccccc}\n    \\toprule\n    \\multicolumn{2}{c}{Models} & \\multicolumn{2}{c}{\\textbf{Autoformer}} &  \\multicolumn{2}{c}{Informer\\cite{haoyietal-informer-2021}} & \\multicolumn{2}{c}{LogTrans\\cite{2019Enhancing}}  & \\multicolumn{2}{c}{Reformer\\cite{kitaev2020reformer}} & \\multicolumn{2}{c}{LSTNet\\cite{2018Modeling}} & \\multicolumn{2}{c}{LSTM\\cite{Hochreiter1997LongSM}} & \\multicolumn{2}{c}{TCN\\cite{Bai2018AnEE}}  \\\\\n    \\cmidrule(lr){3-4} \\cmidrule(lr){5-6}\\cmidrule(lr){7-8} \\cmidrule(lr){9-10}\\cmidrule(lr){11-12}\\cmidrule(lr){13-14}\\cmidrule(lr){15-16}\n    \\multicolumn{2}{c}{Metric} & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE  \\\\\n    \\toprule\n    \\multirow{4}{*}{\\rotatebox{90}{ETT$^\\ast$}} & 96 & \\textbf{0.255} & \\textbf{0.339} & 0.365 & 0.453 & 0.768 & 0.642 & 0.658 & 0.619 & 3.142 & 1.365 & 2.041 & 1.073 & 3.041 & 1.330  \\\\\n    & 192 & \\textbf{0.281} & \\textbf{0.340} & 0.533 & 0.563 & 0.989 & 0.757 & 1.078 & 0.827 & 3.154 & 1.369 & 2.249 & 1.112 & 3.072 & 1.339  \\\\\n    & 336 & \\textbf{0.339} & \\textbf{0.372} & 1.363 & 0.887 & 1.334 & 0.872 & 1.549 & 0.972 & 3.160 & 1.369 & 2.568 & 1.238 & 3.105 & 1.348  \\\\\n    & 720 & \\textbf{0.422} & \\textbf{0.419} & 3.379 & 1.388 & 3.048 & 1.328 & 2.631 & 1.242 & 3.171 & 1.368 & 2.720 & 1.287 & 3.135 & 1.354  \\\\\n    \\midrule\n    \\multirow{4}{*}{\\rotatebox{90}{Electricity}} & 96  & \\textbf{0.201} & \\textbf{0.317} & 0.274 & 0.368 & 0.258 & 0.357 & 0.312 & 0.402 & 0.680 & 0.645 & 0.375 & 0.437 & 0.985 & 0.813  \\\\\n    & 192  & \\textbf{0.222} & \\textbf{0.334} & 0.296 & 0.386 & 0.266 & 0.368 & 0.348 & 0.433 & 0.725 & 0.676 & 0.442 & 0.473 & 0.996 & 0.821  \\\\\n    & 336  & \\textbf{0.231} & \\textbf{0.338} & 0.300 & 0.394 & 0.280 & 0.380 & 0.350 & 0.433 & 0.828 & 0.727 & 0.439 & 0.473 & 1.000 & 0.824  \\\\\n    & 720  & \\textbf{0.254} & \\textbf{0.361} & 0.373 & 0.439 & 0.283 & 0.376 & 0.340 & 0.420 & 0.957 & 0.811 & 0.980 & 0.814 & 1.438 & 0.784  \\\\\n    \\midrule\n    \\multirow{4}{*}{\\rotatebox{90}{Exchange}} & 96 & \\textbf{0.197} & \\textbf{0.323} & 0.847 & 0.752 & 0.968 & 0.812 & 1.065 & 0.829 & 1.551 & 1.058 & 1.453 & 1.049 & 3.004 & 1.432  \\\\\n    & 192 & \\textbf{0.300} & \\textbf{0.369} & 1.204 & 0.895 & 1.040 & 0.851 & 1.188 & 0.906 & 1.477 & 1.028 & 1.846 & 1.179 & 3.048 & 1.444  \\\\\n    & 336 & \\textbf{0.509} & \\textbf{0.524} & 1.672 & 1.036 & 1.659 & 1.081 & 1.357 & 0.976 & 1.507 & 1.031 & 2.136 & 1.231 & 3.113 & 1.459  \\\\\n    & 720 & \\textbf{1.447} & \\textbf{0.941} & 2.478 & 1.310 & 1.941 & 1.127 & 1.510 & 1.016 & 2.285 & 1.243 & 2.984 & 1.427 & 3.150 & 1.458  \\\\\n    \\midrule\n    \\multirow{4}{*}{\\rotatebox{90}{Traffic}}  & 96 & \\textbf{0.613} & \\textbf{0.388} & 0.719 & 0.391 & 0.684 & 0.384 & 0.732 & 0.423 & 1.107 & 0.685 & 0.843 & 0.453 & 1.438 & 0.784  \\\\\n    & 192 & \\textbf{0.616} & \\textbf{0.382} & 0.696 & 0.379 & 0.685 & 0.390 & 0.733 & 0.420 & 1.157 & 0.706 & 0.847 & 0.453 & 1.463 & 0.794  \\\\\n    & 336 & \\textbf{0.622} & \\textbf{0.337} & 0.777 & 0.420 & 0.733 & 0.408 & 0.742 & 0.420 & 1.216 & 0.730 & 0.853 & 0.455 & 1.479 & 0.799  \\\\\n    & 720 & \\textbf{0.660} & \\textbf{0.408} & 0.864 & 0.472 & 0.717 & 0.396 & 0.755 & 0.423 & 1.481 & 0.805 & 1.500 & 0.805 & 1.499 & 0.804  \\\\\n    \\midrule\n    \\multirow{4}{*}{\\rotatebox{90}{Weather}}  & 96 & \\textbf{0.266} & \\textbf{0.336} & 0.300 & 0.384 & 0.458 & 0.490 & 0.689 & 0.596 & 0.594 & 0.587 & 0.369 & 0.406 & 0.615 & 0.589  \\\\\n    & 192 & \\textbf{0.307} & \\textbf{0.367} & 0.598 & 0.544 & 0.658 & 0.589 & 0.752 & 0.638 & 0.560 & 0.565 & 0.416 & 0.435 & 0.629 & 0.600  \\\\\n    & 336 & \\textbf{0.359} & \\textbf{0.395} & 0.578 & 0.523 & 0.797 & 0.652 & 0.639 & 0.596 & 0.597 & 0.587 & 0.455 & 0.454 & 0.639 & 0.608  \\\\\n    & 720 & \\textbf{0.419} & \\textbf{0.428} & 1.059 & 0.741 & 0.869 & 0.675 & 1.130 & 0.792 & 0.618 & 0.599 & 0.535 & 0.520 & 0.639 & 0.610  \\\\\n    \\midrule\n    \\multirow{4}{*}{\\rotatebox{90}{ILI}}  & 24   & \\textbf{3.483} & \\textbf{1.287} & 5.764 & 1.677 & 4.480 & 1.444 & 4.400 & 1.382 & 6.026 & 1.770 & 5.914 & 1.734 & 6.624 & 1.830  \\\\\n    & 36   & \\textbf{3.103} & \\textbf{1.148} & 4.755 & 1.467 & 4.799 & 1.467 & 4.783 & 1.448 & 5.340 & 1.668 & 6.631 & 1.845 & 6.858 & 1.879  \\\\\n    & 48   & \\textbf{2.669} & \\textbf{1.085} & 4.763 & 1.469 & 4.800 & 1.468 & 4.832 & 1.465 & 6.080 & 1.787 & 6.736 & 1.857 & 6.968 & 1.892  \\\\\n    & 60   & \\textbf{2.770} & \\textbf{1.125} & 5.264 & 1.564 & 5.278 & 1.560 & 4.882 & 1.483 & 5.548 & 1.720 & 6.870 & 1.879 & 7.127 & 1.918  \\\\\n    \\bottomrule\n  \\end{tabular}\n  \n  \\end{small}\n   \\begin{tablenotes}\n        \\footnotesize\n        \\item[*] \\emph{ETT} means the ETTm2. See Appendix \\ref{appendix:full_benchmark} for the \\textbf{full benchmark} of ETTh1, ETTh2, ETTm1.\n  \\end{tablenotes}\n  \\end{threeparttable}\n  \\vspace{-15pt}\n\\end{table}\n\n\\vspace{-5pt}\n\\paragraph{Implementation details}\\label{Implementation}\nOur method is trained with the L2 loss, using the ADAM \\cite{DBLP:journals/corr/KingmaB14} optimizer with an initial learning rate of $10^{-4}$. Batch size is set to 32. The training process is early stopped within 10 epochs. All experiments are repeated three times, implemented in PyTorch \\cite{Paszke2019PyTorchAI} and conducted on a single NVIDIA TITAN RTX 24GB GPUs.\nThe hyper-parameter $c$ of Auto-Correlation is in the range of 1 to 3 to trade off performance and efficiency. See Appendix \\ref{appendix:main_results} and \\ref{appendix:parameter} for standard deviations and sensitivity analysis.\nAutoformer contains 2 encoder layers and 1 decoder layer. \n\n\\vspace{-5pt}\n\\paragraph{Baselines} We include 10 baseline methods. For the \\emph{multivariate} setting, we select three latest state-of-the-art transformer-based models: Informer \\cite{haoyietal-informer-2021}, Reformer \\cite{kitaev2020reformer}, LogTrans \\cite{2019Enhancing}, two RNN-based models: LSTNet \\cite{2018Modeling}, LSTM \\cite{Hochreiter1997LongSM} and CNN-based TCN \\cite{Bai2018AnEE} as baselines. \nFor the \\emph{univariate} setting, we include more competitive baselines: N-BEATS\\cite{oreshkin2019n}, DeepAR \\cite{Flunkert2017DeepARPF}, Prophet \\cite{Taylor2017ForecastingAS} and ARMIA \\cite{Anderson1976TimeSeries2E}. \n\n\\subsection{Main Results}\nTo compare performances under different future horizons, we fix the input length and evaluate models with a wide range of prediction lengths: 96, 192, 336, 720. This setting precisely meets the definition of long-term forecasting. Here are results on both the multivariate and univariate settings.\n\n\\vspace{-5pt}\n\\paragraph{Multivariate results}\nAs for the multivariate setting, Autoformer achieves the consistent state-of-the-art performance in all benchmarks and all prediction length settings (Table \\ref{tab:Results}). Especially, under the input-96-predict-336 setting, compared to previous state-of-the-art results, Autoformer gives \\textbf{74\\%} (1.334$\\to$0.339) MSE reduction in ETT, \\textbf{18\\%} (0.280$\\to$0.231) in Electricity, \\textbf{61\\%} (1.357$\\to$0.509) in Exchange, \\textbf{15\\%} (0.733$\\to$0.622) in Traffic and \\textbf{21\\%} (0.455$\\to$0.359) in Weather. For the input-36-predict-60 setting of ILI, Autoformer makes \\textbf{43\\%} (4.882$\\to$2.770) MSE reduction. Overall, Autoformer yields a \\textbf{38\\%} averaged MSE reduction among above settings.\nNote that Autoformer still provides remarkable improvements in the \\textit{Exchange} dataset that is \\textbf{without obvious periodicity}. \nSee Appendix \\ref{appendix:main_results} for detailed showcases.\nBesides, we can also find that the performance of Autoformer changes quite steadily as the prediction length $O$ increases. It means that Autoformer retains better \\textbf{long-term robustness}, which is meaningful for real-world practical applications, such as weather early warning and long-term energy consumption planning.\n\n\\vspace{-5pt}\n\\paragraph{Univariate results} We list the univariate results of two typical datasets in Table \\ref{tab:univarite_results}. Under the comparison with extensive baselines, our Autoformer still achieves state-of-the-art performance for the long-term forecasting tasks. In particular, for the input-96-predict-336 setting, our model achieves \\textbf{14\\%} (0.180$\\to$0.145) MSE reduction on the ETT dataset with obvious periodicity. \nFor the Exchange dataset without obvious periodicity, Autoformer surpasses other baselines by \\textbf{17\\%} (0.611$\\to$0.508) and shows greater long-term forecasting capacity. Also, we find that ARIMA \\cite{Anderson1976TimeSeries2E} performs best in the input-96-predict-96 setting of the Exchange dataset but fails in the long-term setting. This situation of ARIMA can be benefited from its inherent capacity for non-stationary economic data but is limited by the intricate temporal patterns of real-world series.\n\\begin{table}[tbp]\n  \\caption{Univariate results with different prediction lengths $O \\in \\{96,192,336,720\\}$ on typical datasets. We set the input length $I$ as 96. A lower MSE or MAE indicates a better prediction.}\\label{tab:univarite_results}\n  \\centering\n  \\begin{small}\n  \\renewcommand{\\multirowsetup}{\\centering}\n  \\setlength{\\tabcolsep}{1.5pt}\n  \\begin{tabular}{c|c|ccccccccccccccccc}\n    \\toprule\n    \\multicolumn{2}{c}{Models} & \\multicolumn{2}{c}{\\scalebox{0.92}{\\textbf{Autoformer}}}  & \\multicolumn{2}{c}{\\scalebox{0.92}{N-BEATS\\cite{oreshkin2019n}}} &\n    \\multicolumn{2}{c}{\\scalebox{0.92}{Informer\\cite{haoyietal-informer-2021}}} & \\multicolumn{2}{c}{\\scalebox{0.92}{LogTrans\\cite{2019Enhancing}}}  & \\multicolumn{2}{c}{\\scalebox{0.92}{Reformer\\cite{kitaev2020reformer}}}\n     & \\multicolumn{2}{c}{\\scalebox{0.92}{DeepAR\\cite{Flunkert2017DeepARPF}}} & \\multicolumn{2}{c}{\\scalebox{0.92}{Prophet\\cite{Taylor2017ForecastingAS}}} & \\multicolumn{2}{c}{\\scalebox{0.92}{ARIMA\\cite{Anderson1976TimeSeries2E}}}  \\\\\n    \\cmidrule(lr){3-4} \\cmidrule(lr){5-6}\\cmidrule(lr){7-8} \\cmidrule(lr){9-10}\\cmidrule(lr){11-12}\\cmidrule(lr){13-14}\\cmidrule(lr){15-16}\\cmidrule(lr){17-18}\n    \\multicolumn{2}{c}{Metric} & \\scalebox{0.92}{MSE} & \\scalebox{0.92}{MAE} & \\scalebox{0.92}{MSE} & \\scalebox{0.92}{MAE} & \\scalebox{0.92}{MSE} & \\scalebox{0.92}{MAE} & \\scalebox{0.92}{MSE} & \\scalebox{0.92}{MAE} & \\scalebox{0.92}{MSE} & \\scalebox{0.92}{MAE} & \\scalebox{0.92}{MSE} & \\scalebox{0.92}{MAE} & \\scalebox{0.92}{MSE} & \\scalebox{0.92}{MAE} & \\scalebox{0.92}{MSE} & \\scalebox{0.92}{MAE}  \\\\\n    \\toprule\n    \\multirow{4}{*}{\\scalebox{0.92}{\\rotatebox{90}{ETT}}}&  \\scalebox{0.92}{96} & \\scalebox{0.92}{\\textbf{0.065}} & \\scalebox{0.92}{\\textbf{0.189}} & \\scalebox{0.92}{0.082} & \\scalebox{0.92}{0.219} & \\scalebox{0.92}{0.088} & \\scalebox{0.92}{0.225} & \\scalebox{0.92}{0.082} & \\scalebox{0.92}{0.217} & \\scalebox{0.92}{0.131} & \\scalebox{0.92}{0.288}  & \\scalebox{0.92}{0.099} & \\scalebox{0.92}{0.237} & \\scalebox{0.92}{0.287} & \\scalebox{0.92}{0.456} & \\scalebox{0.92}{0.211} & \\scalebox{0.92}{0.362}  \\\\\n    & \\scalebox{0.92}{192} &  \\scalebox{0.92}{\\textbf{0.118}} & \\scalebox{0.92}{\\textbf{0.256}}  & \\scalebox{0.92}{0.120} & \\scalebox{0.92}{0.268} & \\scalebox{0.92}{0.132} & \\scalebox{0.92}{0.283} & \\scalebox{0.92}{0.133} & \\scalebox{0.92}{0.284} & \\scalebox{0.92}{0.186} & \\scalebox{0.92}{0.354} & \\scalebox{0.92}{0.154} & \\scalebox{0.92}{0.310} & \\scalebox{0.92}{0.312} & \\scalebox{0.92}{0.483} & \\scalebox{0.92}{0.261} & \\scalebox{0.92}{0.406} \\\\\n    & \\scalebox{0.92}{336} &  \\scalebox{0.92}{\\textbf{0.154}} & \\scalebox{0.92}{\\textbf{0.305}} & \\scalebox{0.92}{0.226} & \\scalebox{0.92}{0.370} & \\scalebox{0.92}{0.180} & \\scalebox{0.92}{0.336} & \\scalebox{0.92}{0.201} & \\scalebox{0.92}{0.361} & \\scalebox{0.92}{0.220} & \\scalebox{0.92}{0.381} &  \\scalebox{0.92}{0.277} & \\scalebox{0.92}{0.428} & \\scalebox{0.92}{0.331} & \\scalebox{0.92}{0.474} & \\scalebox{0.92}{0.317} & \\scalebox{0.92}{0.448} \\\\\n    & \\scalebox{0.92}{720} &  \\scalebox{0.92}{\\textbf{0.182}} & \\scalebox{0.92}{\\textbf{0.335}} & \\scalebox{0.92}{0.188} & \\scalebox{0.92}{0.338} & \\scalebox{0.92}{0.300} & \\scalebox{0.92}{0.435} & \\scalebox{0.92}{0.268} & \\scalebox{0.92}{0.407} & \\scalebox{0.92}{0.267} & \\scalebox{0.92}{0.430} &  \\scalebox{0.92}{0.332} & \\scalebox{0.92}{0.468} & \\scalebox{0.92}{0.534} & \\scalebox{0.92}{0.593} & \\scalebox{0.92}{0.366} & \\scalebox{0.92}{0.487}  \\\\\n    \\midrule\n    \\multirow{4}{*}{\\scalebox{0.92}{\\rotatebox{90}{Exchange}}} & \\scalebox{0.92}{96} & \\scalebox{0.92}{0.241} & \\scalebox{0.92}{0.387} & \\scalebox{0.92}{0.156} & \\scalebox{0.92}{0.299} & \\scalebox{0.92}{0.591} & \\scalebox{0.92}{0.615} & \\scalebox{0.92}{0.279} & \\scalebox{0.92}{0.441} & \\scalebox{0.92}{1.327} & \\scalebox{0.92}{0.944} & \\scalebox{0.92}{0.417} & \\scalebox{0.92}{0.515} & \\scalebox{0.92}{0.828} & \\scalebox{0.92}{0.762} & \\scalebox{0.92}{\\textbf{0.112}} & \\scalebox{0.92}{\\textbf{0.245}} \\\\\n    & \\scalebox{0.92}{192} & \\scalebox{0.92}{\\textbf{0.273}} & \\scalebox{0.92}{\\textbf{0.403}} & \\scalebox{0.92}{0.669} & \\scalebox{0.92}{0.665} & \\scalebox{0.92}{1.183} & \\scalebox{0.92}{0.912} & \\scalebox{0.92}{1.950} & \\scalebox{0.92}{1.048} & \\scalebox{0.92}{1.258} & \\scalebox{0.92}{0.924} & \\scalebox{0.92}{0.813} & \\scalebox{0.92}{0.735} & \\scalebox{0.92}{0.909} & \\scalebox{0.92}{0.974} & \\scalebox{0.92}{0.304} & \\scalebox{0.92}{0.404} \\\\\n    & \\scalebox{0.92}{336} & \\scalebox{0.92}{\\textbf{0.508}} & \\scalebox{0.92}{\\textbf{0.539}} & \\scalebox{0.92}{0.611} & \\scalebox{0.92}{0.605} & \\scalebox{0.92}{1.367} & \\scalebox{0.92}{0.984} & \\scalebox{0.92}{2.438} & \\scalebox{0.92}{1.262} & \\scalebox{0.92}{2.179} & \\scalebox{0.92}{1.296} & \\scalebox{0.92}{1.331} & \\scalebox{0.92}{0.962} & \\scalebox{0.92}{1.304} & \\scalebox{0.92}{0.988} & \\scalebox{0.92}{0.736} & \\scalebox{0.92}{0.598} \\\\\n    & \\scalebox{0.92}{720} & \\scalebox{0.92}{\\textbf{0.991}} & \\scalebox{0.92}{\\textbf{0.768}} & \\scalebox{0.92}{1.111} & \\scalebox{0.92}{0.860} & \\scalebox{0.92}{1.872} & \\scalebox{0.92}{1.072} & \\scalebox{0.92}{2.010} & \\scalebox{0.92}{1.247} & \\scalebox{0.92}{1.280} & \\scalebox{0.92}{0.953} & \\scalebox{0.92}{1.894} & \\scalebox{0.92}{1.181} & \\scalebox{0.92}{3.238} & \\scalebox{0.92}{1.566} & \\scalebox{0.92}{1.871} & \\scalebox{0.92}{0.935} \\\\\n    \\bottomrule\n  \\end{tabular}\n  \\end{small}\n  \\vspace{-5pt}\n\\end{table}\n\n\\subsection{Ablation studies}\n\n\\begin{table}[hbp]\n    \\caption{Ablation of decomposition in multivariate ETT with MSE metric. \\textbf{Ours} adopts our progressive architecture into other models. \\textbf{Sep} employs two models to forecast pre-decomposed seasonal and trend-cyclical components separately. \\emph{Promotion} is the MSE reduction compared to \\textbf{Origin}. }\\label{tab:ablation_of_decomposition}\n    \\centering\n    \\begin{small}\n    \\renewcommand{\\multirowsetup}{\\centering}\n    \\setlength{\\tabcolsep}{2.2pt}\n    \\begin{tabular}{c|cccccccccccc|cc}\n    \\toprule\n    Input-96 & \\multicolumn{3}{c}{Transformer\\cite{NIPS2017_3f5ee243}} & \\multicolumn{3}{c}{Informer\\cite{haoyietal-informer-2021}} & \\multicolumn{3}{c}{LogTrans\\cite{kitaev2020reformer}} & \\multicolumn{3}{c}{Reformer\\cite{2019Enhancing}} & \\multicolumn{2}{c}{Promotion} \\\\\n    \\cmidrule(lr){2-4}\\cmidrule(lr){5-7}\\cmidrule(lr){8-10}\\cmidrule(lr){11-13}\\cmidrule(lr){14-15}\n    Predict-$O$ & Origin & Sep & Ours & Origin & Sep & Ours & Origin & Sep & Ours & Origin & Sep & Ours & Sep & Ours \\\\\n    \\toprule\n    96 & 0.604 & 0.311 & \\textbf{0.204} & 0.365 &0.490 & \\textbf{0.354} & 0.768 & 0.862 & \\textbf{0.231} & 0.658 &0.445 &  \\textbf{0.218} & 0.069 & 0.347 \\\\\n    192 & 1.060 & 0.760 & \\textbf{0.266} & 0.533 &0.658 & \\textbf{0.432} & 0.989 & 0.533 & \\textbf{0.378} & 1.078 &0.510 & \\textbf{0.336} & 0.300 & 0.562 \\\\\n    336 & 1.413 & 0.665 & \\textbf{0.375} & 1.363 &1.469 & \\textbf{0.481} & 1.334 & 0.762 & \\textbf{0.362} & 1.549 &1.028 & \\textbf{0.366} & 0.434 & 1.019 \\\\\n    720 & 2.672 & 3.200 & \\textbf{0.537} & 3.379 &2.766 & \\textbf{0.822} & 3.048 &2.601 & \\textbf{0.539} & 2.631 & 2.845& \\textbf{0.502} & 0.079 & 2.332 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\end{small}\n    \\vspace{-10pt}\n\\end{table}\n\n\\paragraph{Decomposition architecture}\nWith our proposed progressive decomposition architecture, other models can gain consistent promotion, especially as the prediction length $O$ increases (Table \\ref{tab:ablation_of_decomposition}). This verifies that our method can generalize to other models and release the capacity of other dependencies learning mechanisms, alleviate the distraction caused by intricate patterns. Besides, our architecture outperforms the pre-processing, although the latter employs a bigger model and more parameters. Especially, pre-decomposing may even bring negative effect because it neglects the interaction of \ncomponents during long-term future, such as Transformer \\cite{NIPS2017_3f5ee243} predict-720, Informer \\cite{haoyietal-informer-2021} predict-336.\n\n\\paragraph{Auto-Correlation vs. self-attention family} As shown in Table \\ref{tab:ablation_of_attention}, our proposed Auto-Correlation achieves the best performance under various input-$I$-predict-$O$ settings, which verifies the effectiveness of series-wise connections comparing to point-wise self-attentions (Figure \\ref{fig:compare}). Furthermore, we can also observe that Auto-Correlation is memory efficiency from the last column of Table \\ref{tab:ablation_of_attention}, which can be used in long sequence forecasting, such as input-336-predict-1440.\n\n\\begin{table}[hbp]\n    \\caption{Comparison of Auto-Correlation and self-attention in the multivariate ETT. We \\textbf{replace} the Auto-Correlation in Autoformer with different self-attentions. The ``-'' indicates the out-of-memory.}\\label{tab:ablation_of_attention}\n    \\centering\n    \\begin{small}\n    \\renewcommand{\\multirowsetup}{\\centering}\n    \\setlength{\\tabcolsep}{6.6pt}\n    \\begin{tabular}{c|c|ccccccccc}\n    \\toprule\n    \\multicolumn{2}{c}{Input Length $I$} & \\multicolumn{3}{c}{96}  & \\multicolumn{3}{c}{192} & \\multicolumn{3}{c}{336}   \\\\\n    \\cmidrule(lr){3-5} \\cmidrule(lr){6-8}\\cmidrule(lr){9-11} \n    \\multicolumn{2}{c}{Prediction Length $O$} & 336 & 720 & 1440 & 336 & 720 & 1440 & 336 & 720 & 1440  \\\\\n    \\toprule\n    Auto- & MSE & \\textbf{0.339}&\t\\textbf{0.422}&\t\\textbf{0.555}&\t\\textbf{0.355}&\t\\textbf{0.429}&\t\\textbf{0.503}&\t\\textbf{0.361} &\t\\textbf{0.425} &\t\\textbf{0.574} \\\\\n    Correlation & MAE & \\textbf{0.372} & \\textbf{0.419} & \\textbf{0.496} & \\textbf{0.392} & \\textbf{0.430} & \\textbf{0.484} & \\textbf{0.406} & \t\\textbf{0.440} & \\textbf{0.534} \\\\\n    \\midrule\n    Full & MSE & 0.375\t& 0.537 & 0.667& 0.450 & 0.554 & -  &\t0.501&\t0.647&\t- \\\\\n    Attention\\cite{NIPS2017_3f5ee243} & MAE &0.425&\t0.502&\t0.589&\t0.470&\t0.533 &\t- &\t0.485&\t0.491&\t-  \\\\\n    \\midrule\n    LogSparse & MSE & 0.362 & \t0.539 & \t0.582 & \t0.420 & \t0.552 & \t0.958 & \t0.474 & \t0.601 & - \\\\\n    Attention\\cite{2019Enhancing} & MAE & 0.413\t & 0.522 & \t0.529 & \t0.450 & \t0.513 & \t0.736 & \t0.474 & \t0.524 & - \\\\\n    \\midrule\n    LSH & MSE & 0.366 & \t0.502 & \t0.663 & \t0.407 & \t0.636 & \t1.069 & \t0.442 & \t0.615 & - \\\\\n    Attention\\cite{kitaev2020reformer} & MAE & 0.404\t & 0.475 & \t0.567 & \t0.421 & \t0.571 & \t0.756 & \t0.476 & \t0.532 & - \\\\\n    \\midrule\n    ProbSparse & MSE & 0.481 & \t0.822 & 0.715 & 0.404 & 1.148 & \t0.732 & 0.417 & 0.631 & 1.133\\\\\n    Attention\\cite{haoyietal-informer-2021} & MAE &0.472 &\t0.559\t& 0.586 &\t0.425\t&0.654&\t0.602&\t0.434\t&0.528 & 0.691 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\end{small}\n    \\vspace{-5pt}\n\\end{table}\n\n\\subsection{Model Analysis}\n\\paragraph{Time series decomposition} As shown in Figure \\ref{fig:visual_decomp}, without our series decomposition block, the forecasting model cannot capture the increasing trend and peaks of the seasonal part. By adding the series decomposition blocks, Autoformer can aggregate and refine the trend-cyclical part from series progressively. This design also facilitates the learning of the seasonal part, especially the peaks and troughs. This verifies the necessity of our proposed progressive decomposition architecture.\n\n\\begin{figure*}[hbp]\n\\begin{center}\n\t\\centerline{\\includegraphics[width=\\columnwidth]{fig/decomp.pdf}}\n\t\\caption{Visualization of learned seasonal $\\mathcal{X}_{\\mathrm{de}}^{M}$ and trend-cyclical $\\mathcal{T}_{\\mathrm{de}}^{M}$ of the last decoder layer. We gradually add the decomposition blocks in decoder from left to right. This case is from ETT dataset under input-96-predict-720 setting. For clearness, we add the linear growth to raw data additionally.}\n\t\\label{fig:visual_decomp}\n\t\\vspace{-20pt}\n\\end{center}\n\\end{figure*}\n\n\\begin{figure*}[tbp]\n  \\begin{center}\n    \\centerline{\\includegraphics[width=\\columnwidth]{fig/attention_visual.pdf}}\n    \\vspace{-5pt}\n    \\caption{Visualization of learned dependencies. For clearness, we select the top-6 time delay sizes $\\tau_{1},\\cdots,\\tau_{6}$ of Auto-Correlation and mark them in raw series (\\textcolor{red}{red} lines). For self-attentions, top-6 similar points with respect to the last time step (\\textcolor{red}{red} stars) are also marked by \\textcolor{orange}{orange} points.}\n    \\label{fig:visual_periodicity}\n    \\vspace{-15pt}\n  \\end{center}\n  \\end{figure*}\n\n\\paragraph{Dependencies learning} The marked time delay sizes in Figure \\ref{fig:visual_periodicity}(a) indicate the most likely periods. Our learned periodicity can guide the model to aggregate the sub-series from the same or neighbor phase of periods by $\\mathrm{Roll}(\\mathcal{X},\\tau_{i}),\\ i\\in\\{1,\\cdots,6\\}$. For the last time step (declining stage), Auto-Correlation fully utilizes all similar sub-series without omissions or errors compared to self-attentions. This verifies that Autoformer can discover the relevant information more sufficiently and precisely.\n\n\\paragraph{Complex seasonality modeling} As shown in Figure \\ref{fig:seasonality_learning}, the lags that Autoformer learns from deep representations can indicate the real seasonality of raw series. For example, the learned lags of the daily recorded Exchange dataset present the monthly, quarterly and yearly periods (Figure \\ref{fig:seasonality_learning} (b)). For the hourly recorded Traffic dataset (Figure \\ref{fig:seasonality_learning} (c)), the learned lags show the intervals as 24-hours and 168-hours, which match the daily and weekly periods of real-world scenarios. These results show that Autoformer can capture the complex seasonalities of real-world series from deep representations and further provide a human-interpretable prediction.\n\n\\begin{figure*}[tbp]\n  \\begin{center}\n    \\centerline{\\includegraphics[width=\\columnwidth]{fig/seasonality.pdf}}\n    \\vspace{-5pt}\n    \\caption{Statistics of learned lags. For each time series in the test set, we count the top 10 lags learned by decoder for the input-96-predict-336 task. Figure (a)-(d) are the density histograms.}\n    \\label{fig:seasonality_learning}\n    \\vspace{-20pt}\n  \\end{center}\n  \\end{figure*}\n\n\\paragraph{Efficiency analysis} We compare the running memory and time among Auto-Correlation-based and self-attention-based models (Figure \\ref{fig:model_analysis}) during the training phase. The proposed Autoformer shows $\\mathcal{O}(L\\log L)$ complexity in both memory and time and achieves better long-term sequences efficiency.\n\n\\begin{figure*}[thbp]\n\\begin{center}\n\t\\centerline{\\includegraphics[width=\\columnwidth]{fig/analysis_final.pdf}}\n\t\\vspace{-5pt}\n\t\\caption{Efficiency Analysis. For memory, we replace Auto-Correlation with self-attention family in Autoformer and record the memory with input 96. For running time, we run the Auto-Correlation or self-attentions $10^3$ times to get the execution time per step. The output length increases exponentially.}\n\t\\label{fig:model_analysis}\n\t\\vspace{-20pt}\n\\end{center}\n\\end{figure*}\n\n\\section{Conclusions}\n\nThis paper studies the long-term forecasting problem of time series, which is a pressing demand for real-world applications. However, the intricate temporal patterns prevent the model from learning reliable dependencies. We propose the Autoformer as a decomposition architecture by embedding the series decomposition block as an inner operator, which can progressively aggregate the long-term trend part from intermediate prediction. Besides, we design an efficient Auto-Correlation mechanism to conduct dependencies discovery and information aggregation at the series level, which contrasts clearly from the previous self-attention family. Autoformer can naturally achieve $\\mathcal{O}(L\\log L)$ complexity and yield consistent state-of-the-art performance in extensive real-world datasets. \n\n\\begin{ack}\nThis work was supported by the National Natural Science Foundation of China under Grants 62022050 and 62021002, Beijing Nova Program under Grant Z201100006820041, China's Ministry of Industry and Information Technology, the MOE Innovation Plan and the BNRist Innovation Fund.\n\\end{ack}\n\n\\small\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2001.04451v2.tex",
        "arXiv-2012.07436v3.tex",
        "arXiv-2106.13008v5.tex"
    ],
    "group_id": "group_51",
    "response": "### Title: Innovations in Transformer Models for Long Sequence Time-Series Forecasting\n\n### Introduction\n\nThe field of time-series forecasting has seen significant advancements driven by the need to predict future trends in various domains such as energy consumption, traffic management, economics, weather forecasting, and disease propagation. Traditional methods, such as ARIMA and Prophet, have been foundational but are increasingly challenged by the complexity and length of modern datasets. These methods often struggle with capturing long-range dependencies and maintaining computational efficiency, especially for long-term forecasting tasks. The introduction of deep learning models, particularly those based on Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs), has brought substantial improvements in accuracy and modeling capacity. However, these models still face limitations in handling intricate temporal patterns and achieving computational efficiency for long sequences.\n\nRecently, Transformer models have emerged as a powerful alternative due to their ability to capture long-range dependencies effectively. However, the standard Transformer architecture suffers from quadratic time and memory complexity, making it impractical for long sequence time-series forecasting (LSTF). To address these challenges, researchers have developed various modifications to the Transformer architecture, including sparse attention mechanisms and reversible layers. This summary focuses on three recent papers that introduce novel approaches to improve the efficiency and performance of Transformer models for LSTF tasks.\n\n### Main Content of Each Paper\n\n#### Reformer: The Efficient Transformer\n\nThe Reformer paper introduces a model that aims to reduce the computational and memory complexity of the standard Transformer architecture, making it more suitable for long sequences. The key innovation is the use of locality-sensitive hashing (LSH) to approximate the attention mechanism, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the sequence length. Additionally, the Reformer employs reversible residual layers, which allow the model to store only a single copy of activations across layers rather than multiple copies, thereby reducing memory usage. The reversible layers are particularly useful in training deep models, as they eliminate the need to checkpoint activations for backpropagation.\n\nThe Reformer model is evaluated on a synthetic task, a text task (enwik8) with sequences of length 64K, and an image generation task (Imagenet-64 generation) with sequences of length 12K. The results show that the Reformer matches the performance of the full Transformer but with significantly better memory efficiency and faster execution, especially on long sequences. The model's performance is also tested on the WMT English-to-German translation task, demonstrating its utility in natural language processing (NLP) tasks.\n\n#### Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting\n\nInformer addresses the challenges of long sequence time-series forecasting (LSTF) by proposing a novel architecture that enhances the prediction capacity of Transformer models. The primary contributions include the introduction of a ProbSparse self-attention mechanism, self-attention distilling, and a generative style decoder. The ProbSparse self-attention mechanism achieves O($L\\log L$) complexity in both time and memory usage, which is a significant improvement over the standard O($L^2$) complexity. This mechanism leverages the sparsity in the attention scores to reduce the number of computations required.\n\nThe self-attention distilling operation is designed to extract the dominating attention scores from the attention mechanism, thereby reducing the overall memory usage of the model. This operation is inspired by dilated convolutions and involves a max-pooling layer to downsample the input sequence. The generative style decoder predicts the entire output sequence in a single forward pass, avoiding the step-by-step inference process that is common in traditional encoder-decoder architectures. This approach significantly improves the inference speed for long sequences.\n\nInformer is evaluated on four large-scale datasets, including ETT (Electricity Transformer Temperature), ECL (Electricity Consuming Load), Weather, and ETTm (Electricity Transformer Temperature at a minute level). The results demonstrate that Informer outperforms existing methods, including ARIMA, Prophet, LSTMa, LSTnet, and DeepAR, across various prediction lengths. The model's performance is consistent even as the prediction length increases, indicating its robustness for long-term forecasting tasks.\n\n#### Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting\n\nAutoformer proposes a novel architecture that integrates time series decomposition into the Transformer framework to enhance long-term forecasting performance. The architecture includes an Auto-Correlation mechanism, which discovers dependencies based on the periodicity of the time series and aggregates similar sub-series from underlying periods. This mechanism operates on the series level rather than the point level, providing a more efficient and accurate way to model temporal dependencies.\n\nThe Autoformer architecture consists of an encoder and a decoder, where the encoder focuses on modeling the seasonal patterns of the time series, and the decoder progressively refines the trend-cyclical components. The model is evaluated on six real-world benchmarks, covering applications such as energy, traffic, economics, weather, and disease forecasting. The results show that Autoformer achieves state-of-the-art accuracy, with a 38% relative improvement on average across the benchmarks.\n\n### Commonalities and Innovations\n\nAll three papers focus on improving the efficiency and performance of Transformer models for long sequence tasks, whether in natural language processing or time-series forecasting. They address the quadratic complexity of the standard Transformer by introducing novel mechanisms to reduce the computational and memory requirements. \n\n- **Reformer** uses locality-sensitive hashing to approximate the attention mechanism and reversible layers to reduce memory usage.\n- **Informer** introduces the ProbSparse self-attention mechanism and self-attention distilling to enhance efficiency, and a generative style decoder to improve inference speed.\n- **Autoformer** integrates time series decomposition into the Transformer architecture, using an Auto-Correlation mechanism to model dependencies at the series level.\n\n### Comparison of Results\n\nThe results from the three papers highlight the effectiveness of their respective innovations in improving the efficiency and accuracy of Transformer models for long sequence tasks. \n\n- **Reformer** demonstrates its ability to handle long sequences efficiently, achieving comparable performance to the full Transformer on tasks like enwik8 and Imagenet-64 generation. However, it performs poorly in long-term forecasting tasks due to its reliance on dynamic decoding.\n- **Informer** shows superior performance in long-term forecasting tasks, outperforming existing methods like ARIMA, Prophet, and DeepAR across various datasets and prediction lengths. The model's generative style decoder and self-attention distilling contribute significantly to its performance.\n- **Autoformer** achieves state-of-the-art accuracy in long-term forecasting tasks, with a 38% relative improvement on average. The model's progressive decomposition architecture and Auto-Correlation mechanism enable it to capture complex temporal patterns effectively, even in datasets without obvious periodicity.\n\n### Conclusion\n\nThe three papers introduce innovative approaches to enhance the efficiency and performance of Transformer models for long sequence tasks. **Reformer** focuses on reducing the computational and memory complexity of the attention mechanism and layer activations, making it suitable for long sequences in NLP and image generation tasks. **Informer** addresses the specific challenges of long-term time-series forecasting by introducing the ProbSparse self-attention mechanism and a generative style decoder, achieving significant improvements in accuracy and efficiency. **Autoformer** integrates time series decomposition into the Transformer architecture, using an Auto-Correlation mechanism to model dependencies at the series level, demonstrating robust performance in various real-world applications.\n\nThese advancements not only improve the efficiency of Transformer models but also enhance their ability to capture long-range dependencies, making them more practical for real-world applications. Future research could explore further optimizations of these mechanisms, such as combining them with other techniques to achieve even better performance. Additionally, applying these models to more diverse and complex datasets could provide further insights into their effectiveness and potential limitations."
}